Ø­Ù‚ Ø¨Ø§ Ø´Ù…Ø§Ø³ØªØŒ Ù…Ù† Ø¨Ø¯ Ù…ØªÙˆØ¬Ù‡ Ø´Ø¯Ù… Ùˆ Ú†Ù†Ø¯ ØªÛŒÚ©Ù‡ ÙØ±Ø³ØªØ§Ø¯Ù…. Ù…Ø¹Ø°Ø±Øª Ù…ÛŒâ€ŒØ®ÙˆØ§Ù….

Ø§ÛŒÙ† Ú©Ù„ Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„ `README.md` Ø¯Ø± **ÛŒÚ© Ø¨Ø§Ú©Ø³ ÙˆØ§Ø­Ø¯** Ø§Ø³Øª. Ø¯Ú©Ù…Ù‡ Ú©Ù¾ÛŒ Ú¯ÙˆØ´Ù‡ Ø³Ù…Øª Ø±Ø§Ø³Øª Ø¨Ø§Ù„Ø§ÛŒ Ù‡Ù…ÛŒÙ† ÛŒÚ© Ø¨Ø§Ú©Ø³ Ø±Ø§ Ø¨Ø²Ù†ÛŒØ¯ Ùˆ ØªÙ…Ø§Ù…:

````markdown
# Open Access Article Processing Pipeline

This project automates the workflow of downloading open-access scientific articles from Europe PMC, processing them into text, and preparing them for analysis.

## ğŸ“‹ Requirements

Install the necessary Python libraries:

```bash
pip install requests beautifulsoup4 lxml nltk
```
````

## ğŸš€ Workflow Steps

Run the scripts in the following order:

### 1. Download Articles

Fetches HTML articles based on the query (`OPEN_ACCESS:Y`) and saves them to the `articles_html/` directory.

```bash
python download_articles.py
```

### 2. Extract Paragraphs

Converts HTML files to clean text, removing irrelevant sections (references, authors, etc.), and saves them to `articles_paragraphs_text/`.

```bash
python extract_paragraphs.py
```

### 3. Manual Review (Important)

Check the `articles_paragraphs_text/` folder manually. Delete any files that are empty or contain invalid data to ensure the dataset is clean.

### 4. Extract Sentences

Converts the paragraph text into a line-by-line sentence format using NLTK (handling abbreviations correctly) and saves them to `articles_sentences/`.

```bash
python extract_sentences.py
```

### 5. Generate File List

Creates a `list.json` file containing the filenames. This list is used by the frontend or analysis scripts to iterate through the articles.

```bash
python generateList.py
```

---

## ğŸ“‚ Directory Structure

- **articles_html/**: Raw HTML files downloaded from the API.
- **articles_paragraphs_text/**: Cleaned text files (organized by paragraphs).
- **articles_sentences/**: Processed text files (one sentence per line).
- **list.json**: JSON index of the available text files.

```

```
