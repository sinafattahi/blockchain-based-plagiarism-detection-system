Perceptual evaluation of speech disorders produces scores that poorly predict the consequences of speech impairment on the communication abilities of patients treated for oral/oropharyngeal cancer.
This may be mitigated by automatic speech analysis.
To measure communication and speech impairment using automatic analyses of spontaneous speech and self‐administered questionnaires in patients treated for oral cavity or oropharyngeal cancer.
The spontaneous speech of 25 patients was recorded during a semistructured interview.
Various acoustic and automatic tools were applied to the speech signal to obtain scores relating to the different linguistic levels.
Reduction of dimensionality was applied to retain only relevant and nonredundant parameters.
Self‐administered questionnaires assessing communication and associated factors (associated deficits, anxiety/depression, cognitive status, communication needs relating to social circles, self‐perception of speech impairment and quality of life) were conducted.
A predictive modelling of communication and speech impairment by LASSO regression was performed using the scores from the automatic tools alone, which were then combined with the scores arising from the questionnaires.
A total of 149 automatic parameters were extracted from the speech signal, of which 75 were retained after dimensional reduction.
Predictive modelling of communication and speech impairment [Holistic Communication Score (HoCoS)] using the selected automatic parameters (number of sonants and occlusives recognised per second) provides a correlation of 0.83 between the predicted and actual score.
This modelling is reliable (rS= 0.82 between five‐fold cross‐validation and HoCoS).
The correlation reaches 0.89 when including associated factors in the modelling, while maintaining a high reliability (rS= 0.70 between five‐fold cross‐validation and HoCoS).
The use of automatic speech analysis allows a reliable prediction of the communication and speech impairment experienced by the patients.
This study opens up new perspectives for the use of automatic speech recognition systems in clinical evaluation and for the consideration of functional and psychosocial needs expressed by the patients during their follow‐up.
Automatic and acoustic analyses compensate for the biases of perceptual speech assessment in clinical practice.
Although they are used to assess speech severity, few studies have examined their contribution to the measurement of communication and speech impairment, which is yet an essential objective of clinical intervention.
Acoustic and automatic analyses (and in particular the use of automatic speech recognition systems) enable good prediction of communication and speech impairment reported by patients, with a limited number of tools and parameters.
This prediction is even improved by adding to the models scores from self‐questionnaires measuring functional and psychosocial dimensions that may be impacted by the speech disorder.
This study provides reliable new tools for measuring impaired communication.
Acoustic and automatic tools can be used in routine clinical care to obtain a valid and reliable measure of communication and speech impairment.
This prediction leads to a follow‐up score to quantify the level of communication and speech impairment at a given time and the evolution of patients from a speech sample.
Cancers of the oral cavity and oropharynx are common and have a high incidence – over 475 000 new cases were reported worldwide in 20201.
However, their relative mortality is decreasing: patients are living longer with the effects of cancer and treatment (Borggreven et al.2007).
Several studies have focused on the functional consequences of speech disorders, the most common symptom in cancers of the oral cavity and oropharynx, in terms of communication.
Speech disorders in the context of cancer generally contribute to changes in perceived speech impairment (i.e., the perception of functional impairment of speech disorder) to the social dynamics of patients, of which communication is an essential part.
In ENT oncology, for example, a correlation of 0.29 is found between speech acceptability experienced by total laryngectomised patients and their participation in communication (Eadie et al.2016).
Studies conducted specifically with patients treated for oral cavity or oropharyngeal cancer primarily target the psychosocial and nonfunctional impact, and find links between quality of life and speech deficits (Borggreven et al.2007).
In clinical practice, the assessment of speech production is a crucial step in determining therapeutic strategies.
Objectives include ensuring that activities are as functional as possible, that is, to promote the recovery of intelligibility, comprehensibility and level of communication, and/or implement compensations (augmentative and alternative communication) where necessary.
Participation restrictions should also be minimised by adapting communication strategies to the communication needs of the patient, depending on the composition of the respective social circles and entourage, social life (family and professional) and leisure activities, etc. of the patient.
However, these dimensions are still not systematically evaluated in current practice (Pommée et al.2021).
The current clinical assessments, primarily by speech‐language pathologists, remain primarily perceptual (Pommée et al.2021).
Therapists rely on a range of measurement methods to quantify speech perception: visual analogue scales, Likert scales, direct magnitude estimates (Carmichael2007), stimulus transcription, or assessment of speech comprehension (Hustad2008) or the severity of such (Wu et al.2019b).
These perceptual elements have many limitations: on the one hand, the lack of standardisation of this assessment (in terms of tools used to assess speech and resulting measures) means that the measures cannot be easily reproduced and are not particularly stable over time or between clinicians (Pommée et al.2021).
Moreover, the quantitative perceptual assessment of a speech disorder varies depending on the assessor of such disorder owing to their respective expertise or differences in internal referents (Kuo and Tjaden2016), but effects on individual variability can also be found: the same assessor may assign different scores depending on the assessment context, mental capacity, or habituation to pathological speech (Fex1992).
Finally, the perceptual assessment performed by an auditor (listener), even an expert auditor, is not always representative of the patients’ own perception of their speech disorder (van der Molen et al.2012).
Indeed, the percentage of the precise agreement between the external perceptual assessment and the self‐report of the speech impairment is only moderate (Gray et al.2012).
Indeed, these authors found an exact agreement of 44.9% between the communication levels and needs estimated by the participants (grouped into five categories of communication levels) and the external assessments carried out by expert SLPs, with a tendency for the experts to slightly increase the communication needs expressed by the participants.
New, automatic and instrumental speech evaluation tools are being developed to overcome the biases arising from a perceptual evaluation.
It is therefore essential to use tools and systems based on scores reported by the patients themselves to obtain a measure as close as possible to the participants’ experiences and needs.
The use of an instrumental system trained on multiple human judges eliminates inter‐judge bias, the trained system therefore reproducing the result of an average expert.
Besides, the deterministic nature of automatic systems (the same input, e.g., a speech extract, will always give the same result) compensates for intra‐judge bias, which is the main issue in communication needs reporting.
If these systems aim essentially to extract parameters of characterisation of the acoustic degradations (Middag et al.2008; Van Nuffelen et al.2009; Zhang et al.2023) from the speech signal, few tools enable clinicians to measure the functional consequences of the speech disorder in the daily communication activities of the patient.
Moreover, if the link between speech and communication is strong, then speech disorders will be part of a complex context.
As presented in Wilson's model (Wilson and Cleary1995), multiple factors, such as cognitive or anxio‐depressive status, health perceptions or characteristics of the environment, may be associated with both speech disorder (symptomatic status) and communication abilities (functional status).
A comprehensive assessment of the speech disorder must therefore be multidimensional.
The aim of this study is to measure communication and speech impairment using automatic analyses of spontaneous speech and self‐administered questionnaires that assess the biopsychosocial factors involved in speech in patients treated for oral cavity or oropharyngeal cancer.
All procedures were performed in accordance with the 1964 Helsinki declaration and its amendments or comparable ethical standards.
Each subject was told in advance of the purpose of this study and was given an information sheet.
Participants gave their written informed consent (nonopposition).
All patients coming for consultation or hospitalised in an ENT department between October 2019 and December 2020 and who could be included during the period were invited to participate in this study.
A systematic review (Balaguer et al.2020b) showed that 50% of the studies carried out on acoustic and automatic speech analysis after cancer included fewer than 20 participants.
Thus, for this study, all eligible patients during the recruitment period were included in our study, with a target of at least 21 participants.
Our inclusion criteria were: legal age (at least 18 years old) and having been treated for oral cavity or oropharyngeal cancer (surgical treatment and/or radiotherapy and/or chemotherapy) for at least 6 months (stable disease).
In order to maximise recruitment and given that the number of participants in previous studies was low, all participants meeting the inclusion criteria were asked to participate and their consent was obtained.
All of them agreed and were included in the study, except for those with an associated chronic disease that could influence the results of this study, such as apraxia of speech or stuttering, which induce effects other than speech impairment in the communication dynamic.
A total of 25 patients were included (median age: 67 years, IQR: 12; 15 men and 10 women; oral cavity: 14, oropharynx: 10, two locations: 1).
The majority of patients were treated by a sequence including surgery and radiotherapy ± chemotherapy (84%).
The average time since treatment was 87.2 months (standard deviation = 121.8; median = 40; interquartile deviation = 123).
The details of the characteristics of the participants are shown in Table1.
Note: Missing data: for some patients, TNM classification or information about speech and language therapy was not retrieved from the participants’ files.
In order to obtain a speech sample as close as possible to natural speech, the participants were recorded during a semidirected interview conducted during the completion of a questionnaire that focused on communication problems (ECVB, Mazaux et al.2006).
Thus, the recorded speech is spontaneous and representative of everyday speech (Prins and Bastiaanse2004) and may contain cues regarding communication abilities (Knuijt et al.2017).
The speech recordings were made in a nonanechoic interview room to remain as close as possible to the standard clinical assessments.
However, no external or internal noise (air conditioning, ventilation, etc.) could be present to avoid disrupting the quality of the recording.
The speech samples were made using a ZOOM H4N Pro digital recorder (48 kHz sampling rate, 16‐bit quantisation, mono mode).
The sound files were recorded in linear PCM format (exported into WAVE format).
The headset microphone (Thomann T Bone HC 444 TWS) was placed 6 cm from the subject's mouth, positioned in front of the subject below the level of the lower lip and at the level of the right labial commissure to maintain the distance between the microphone and the mouth to ensure the reliability of the intensity measurements.
For processing, the audio files were resampled to 16 kHz for acoustic speech analysis and for use by speech recognition systems in the format typically used for automatic speech transcription.
A preliminary segmentation of the files was necessary, as the spontaneous speech samples correspond to the recording of a semidirected interview between an examiner and a subject.
The segments of the subject's speech were determined along with the ‘nonuseful’ segments relating to pauses or to the examiner's speech segments (characterised by a low energy with the use of the headset microphone).
The voice activity detector (VAD) Google WebRTC‐VAD2was therefore used owing to its high‐performance, speed and freedom.
The aggressiveness setting of the VAD was set to 3, its maximum value (the higher the score, the more the VAD will cut utterances as soon as a short blank occurs).
This setting excludes all the speech segments of the assessor as this speaker is far from the microphone and then has a very low recording sound level.
According to Wilson's and Cleary's model (Wilson and Cleary1995), all associated deficits and other alterations of the aerodigestive tract that influence the individual's characteristics should be considered.
The Carcinologic Handicap Index questionnaire (CHI, Balaguer et al.2021) was used for this purpose.
Cognitive status assessed in this study by the Montreal Cognitive Assessment (MoCA, Rambeau et al.2019) and the anxiety status (Eadie et al.2018) – assessed by the Hospital Anxiety and Depression scale (HAD, Zigmond and Snalth1983) and Échelle de Détresse Psychologique (Psychological Distress Scale, EDP, Nordmann‐Dolbeault2009) – are also a major factor in the personal motivation of patients.
The characteristics of the environment, notably social and psychological support, are to be related to the constitution of social circles that impact the communication needs of the participants (Eadie et al.2018).
The Evaluation of the Constitution of Social Circles (ECSC) scale was validated and used in this study (Balaguer et al.2022b), alongside with the QFS (Social Functioning Questionnaire, Zanello et al.2006).
Health perceptions relating to the speech disorder are linked to the subject's self‐perception of the disorder.
They are assessed in this study by the Phonation Handicap Index (PHI, Balaguer et al.2020a), and by the European Organisation for Research and Treatment of Cancer (EORTC) questionnaires (Aaronson et al.1993; Bjordal et al.1999).
Finally, individual data, such as the socio‐professional category, are related to economic and social support, clinical and treatment data correspond to symptomatology (individual characteristics).
All source of information on biopsychosocial factors are given in Table2.
Caron's psycholinguistic model (François and Nespoulous2011) was used to construct the research methodology for the parameters.
Parameters in the three main production dimensions of this model were extracted to focus on speech production: conceptual (pragmasemantic component), categorical (morpho‐syntactic component) and articulatory (phonetic and phonemic components), as well as at the level of the speech signal.
These analyses were conducted using the Praat3software.
The script used was adapted from the study on the validation of the Acoustic Voice Quality Index (Maryn et al.2010) to retrieve spectral (long‐term average spectrum [LTAS]: slope, tilt) and cepstral (smoothed cepstral peak prominence) parameters.
The Praat script ‘Syllable nuclei’ (de Jong and Wempe2009) was used to retrieve new temporal indicators (speech and articulation rate, average duration of a syllable).
The script detects syllabic nuclei to enable the automatic measurement of articulation and speech rates.
Finally, SpeechTools, an optimised version of a pseudo‐syllable extractor (Farinas2002), was used to evaluate the structure of the speech signal in vowel and nonvowel segments.
An automatic speech recognition (ASR) system was used to obtain phoneme sequences, by subject.
Given the limited amount of training data (few French‐speaking corpora of post‐cancer speech currently exist), we chose to use systems trained on speech from adults without speech disorders but which have already been used in the decoding to that captured in the training set, such as the speech of children or speakers with a speech disorder.
For this, we opted for a Time‐Delay Neural Network factorised—Hidden Markov Model (TDNN‐HMM) (Povey et al.2018) previously used in children's speech (Wu et al.2019).
The TDNNf‐HMM model used in this study was developed by Gelin to model monophones (Gelin et al.2021) on 147 h of typical adult speech (CommonVoice corpus) using the Kaldi toolkit (Povey et al.2011): the TDNN‐f models the acoustic units, while the HMM models the concatenations.
This model has 12 hidden layers and presents a phoneme error rate (PER) of 23.5% on a corpus of typical adults (Gelin et al.2021).
The Common Voice4online database was used to train this TDNNf‐HMM.
Decoding was performed on the speech segments determined by WebRTC‐VAD to limit the effects of examiner's speech residuals that could alter the sequence of recognised phonemes.
The phone (or way of pronouncing a sound) that most closely approximates the acoustic features carried by the signal was retained by the TDNNf‐HMM in each 25 ms frame (with a 10 ms step) from among 33 possibilities: 18 consonants, 12 vowels and three semiconsonants.
Each recognised phoneme is also assigned a confidence score obtained using a Minimum Bayes Risk (MBR) method.
This enabled the user to create a phoneme inventory per subject, and phonetic statistics could be computed based on the outputs of the ASR system.
The same ASR system was used (TDNNf‐HMM) to perform the lexical level analyses.
The operation of the system used here (hereinafter ‘lexical’ TDNNf) is almost identical to the one described previously, except for the use of whole files, not segmented by the VAD.
Indeed, a pretest showed that the use of segments defined by the VAD limited the work of the recognition system, which could then no longer rely on the linguistic context to adjust the outputs (action of the language model also learned from the Common Voice ngram corpus: the probability of recognising a word depends on the context, i.e., on the words around it).
The performances of this ‘lexical’ TDNNf are good, with a WER (Word Error Rate) at 14.93% on typical adult speech.
The lexical outputs (words recognised per subject) were used to build a lexical inventory per subject.
The per‐word confidence score is used to analyse lexical features using the French ‘Lexique’ online database (New et al.2001), version 3.83, but also to establish lexical diversity and density scores (Guiraud1954).
A grammatical analysis of the ‘lexical’ TDNNf output was conducted, again using the resources listed in the Lexique database.
A top‐down hierarchical classification analysis (Reinert1990) was performed with the Iramuteq5free software to study the thematic variety.
The principle of this analysis, which is also based on the outputs of the lexical TDNNf, is to iteratively partition the sequence of words recognised for each subject into two parts by maximising a specific criterion at each partition.
The words regularly appearing in the same close lexical context will thus be grouped.
Finally, a sentiment analysis of the ‘lexical’ TDNNf output was conducted using the Microsoft Azure6cognitive API (application programming interface).
This API generates a value between 0 (negative sentiment) and 1 (positive sentiment), a value of 0.5 corresponding to a neutral sentiment (factual statement).
Following the extraction process carried out on spontaneous speech, a first selection of parameters was made to reduce the final number of parameters that could be used to fulfil the main objective (Badjio and Poulet2005).
Along with (2) thenonredundantparameters, that is, those are not correlated with more thanr= 0.90 with another parameter.
In the case of over‐correlation, the expert committee proceeded to select the inter‐correlated parameters on a principle of streamlining: the parameters found in the greatest number of blocks of inter‐correlated parameters were retained as a priority.
In the other cases, the choice of the experts was based on the most easily interpretable parameter.
This prediction was carried out in several stages.
The prediction target is the Holistic Communication Score (HoCoS, Balaguer et al.2022a), a valid and reliable score generated from items from different self‐administered questionnaires.
The HoCoS is a score for patients’ perception of communication and speech impairment.
It includes 24 items relating to the expression of needs, use of the telephone and psychosocial experience of communication.
This tool, validated in French, enables to get patients’ own perceptions, which is essential in clinical care for adapting therapeutic strategies.
The HoCoS is centred on 100, and the higher the score, the more competent and confident the subject feels about their communication skills.
A bivariate analysis was first conducted, which compared each of the parameters retained after the selection process with the HoCoS as previously described.
In a second step, an LASSO regression approach was conducted to select the standardised variables relating to the automatic parameters to minimise the error to make a prediction.
The LASSO regression is an estimation method that constrains its coefficients not to explode in the case of many dependent variables (i.e., in high dimensions).
It allows us to estimate a lambda coefficient that minimises the average prediction error in cross‐validation (here set to five‐fold) during the parameter selection stage, in order to choose the most suitable parameters (Tibshirani1996).
and then full modelling was carried out using a linear regression including the associated factors retained in (i) and the automatic parameters retained in (2).
Statistical analyses were performed using Stata 16.1 software (StataCorp.
Stata statistical software: release 16.
College Station, TX: StataCorp LLC).
A 5% level of significance was chosen in all the analyses.
The normality of the dispersion of the quantitative variables was tested using a Shapiro–Wilk test.
All correlation coefficients are Spearman's correlation coefficient, due to the nonparametric nature of the data.
Correlation analyses were carried out using the following thresholds: greater than 0.9 (very high correlation), 0.7–0.9 (high correlation), 0.5–0.7 (moderate correlation), 0.3–0.5 (low correlation), less than 0.3 (negligible correlation).
A total of 149 parameters were extracted from the various speech analysis tools.
A synthesis of these parameters is shown in Table3(the full description is available in ‘Supporting Information’).
Characteristics of the extracted automatic parameters.
The overall selection process is shown in Figure1.
The details of the parameters are provided in ‘Supporting Information’.
Selection process of parameters from automatic analyses of spontaneous speech.
A committee of two academic experts in computer science and signal processing and a speech‐language pathologist conducted the relevance analysis.
Two parameters measure effects that are absent in the sample: the total number of euphonious links and the proportion of euphonious links from the lexical recognition system, and were therefore excluded from the remainder of the analyses.
Thirty‐one nonstandard parameters were also excluded to avoid an effect of duration of recording that could influence the target measure, giving nonnormalised values.
At the speech signal level, three measures relating to the fundamental frequency may depend on the speaker's gender (the maximum, minimum and range) and were therefore excluded.
Fifteen parameters are considered complex to interpret (mainly constructed from other variables).
Among them, regarding the measures arising from the detection of vowel segments and pauses, 13 parameters relating to pseudo‐consonants and pseudo‐plosives were excluded owing to their ‘exclusionary’ construction (nonvowel segments, but which are not pauses for pseudo‐consonants, and silences lower than 150 ms for pseudo‐plosives).
One more is the total duration of silences was not recorded, as the measurement of such is derived from a subtraction between two other parameters (total duration minus phonation duration).
Finally, the total number of grammatical categories associated with the recognised words was used to calculate the proportion of grammatical class differences, but this number has no direct and obvious interpretation in itself and was therefore also excluded.
Finally, 51 parameters were excluded at this stage, which results in 98 parameters (66.4%) that fulfil the relevance criterion.
A Spearman inter‐item correlation matrix was carried out with a threshold of 0.90 to eliminate redundant items: none of the parameters must be correlated at 0.90 or more with another parameter, which is ultimately retained.
Again, the expert committee proceeded to select inter‐correlated parameters per block.
All parameters inter‐correlated at 0.90 or more were included in a block, and only one of the parameters in this block was retained.
The selection was based on a principle of streamlining: the parameters found in the greatest number of blocks were retained as a priority; the choice of the experts was based on the most easily interpretable parameter for the other blocks.
As a result of this process, 23 additional parameters were not retained, as they were considered too redundant.
A first bivariate analysis was conducted that compared each of the 75 automatic parameters selected with the HoCoS (Step 1 in Section2.5).
From the 75 parameters, four (5%) have a Spearman correlation greater than or equal to 0.70 with the HoCoS, 13 (17%) between 0.50 and 0.70, and 16 (21%) between 0.40 and 0.50.
Note that all 33 parameters (44%) with correlation coefficients greater than or equal to 0.40 have a significantpvalue at the 5% threshold (Table4).
Bivariate analyses between the holistic communication score and the automatic parameters with significant Spearman correlation coefficients at the 5% level (number of participants:n= 24).
To predict the communication and speech impairment (corresponding at the Step ii in Section2.5), the LASSO regression with the HoCoS as a dependent variable and the automatic parameters as explanatory variables (after standardisation) results in the selection of a model with two explanatory variables (lowest average error of prediction in cross‐validation at 108.56 for a lambda penalty coefficient at 6.91): the number of occlusives per second (occs) and the number of sonants per second (sonants).
This linear model with two explanatory variables predicting the HoCoS observes all the conditions of application (notably the homoscedasticity of the residuals:p= 0.29, Breusch–Pagan test; the Gaussian distribution of the residuals:p= 0.84, Shapiro–Wilk test; and the absence of a collinearity problem: inflation factor of the variance at 2.20 lower than the threshold of 5).
It has an root mean square error (RMSE) of 6.38.
This model predictsR2= 62.7% of the total variance of the HoCoS.
The predicted values from this regression model show high correlations with the HoCoS.
The Spearman correlation between HoCoS and the predicted values on the 25 participants in our sample isrS= 0.83 (see Figure2).
Scatter plot between Holistic Communication Score (HoCoS) and predicted values on the 25 participants in the sample.
The cross‐validation shows good outcomes regarding this prediction.
The correlations are high between HoCoS and, on the one hand, the values predicted by the ‘leave one out’ cross‐validation (training with the two automatic parameters selected in 24 participants and prediction on the last subject,rS= 0.78) and on the other, the values predicted by the five‐fold cross‐validation (rS= 0.82, see Figure3).
Scatter plot between Holistic Communication Score (HoCoS) and predicted values by five‐fold cross‐validation.
In this final step, the associated factors from the questionnaires of anxiety/depression, assessment of associated deficits, communication requirements relating to social circles and cognitive status were included in predictive models of the communication and speech impairment score (HoCoS).
All 227 items measuring biopsychosocial characteristics were included (CHI: 50 items, MoCA: 14; HAD: 16; EDP: 1; ECSC: 27; QFS: 18; PHI: 3 specifically relating to self‐perception; EORTC QLQ‐C30: 45; EORTC QLQ‐H&N35: 53).
First, an LASSO regression, including only the items of the self‐administered questionnaires relating to the associated factors, leads us to retain nine of them (lowest average error of prediction in cross‐validation at 48.89 for a lambda penalty coefficient at 0.88).
Four of these relate to the composition of the respective social circles: average satisfaction with the frequency of contact with the members of the individual's personal circle (eccs1c), average satisfaction with the quality of the conversations with the members of the personal circle (eccs1d), satisfaction with the relationships with the members of extra‐familial circle (qfs52), satisfaction with the way in which financial and administrative aspects of the individual's life are managed (qfs62).
Two relate to the anxious‐depressive state: ‘I enjoy the same things as I did before’ (had2), overall anxiety score (had2).
One item relates to the deficits associated with olfaction: ‘Are you afraid of having an accident due to issues with your sense of smell?’(chiog2).
Finally, the last two items relate to the self‐perceived discomfort and handicap arising from the speech disorder: discomfort in producing understandable speech (phigene), daily handicap experienced due to the speech problem (phihcp).
Then, a linear regression was used to predict the HoCoS with the two automatic parameters (occsandsonants) and the nine items and scores relating to the associated factors.
A check of the regression conditions led to the exclusion of the variable relating to the discomfort in the production of comprehensible speech (phigene) because it presents a very high variance inflation factor (at 38.83), which poses a multicollinearity problem.
The conditions for applying the regression are fulfilled after excluding this variable (and modelling the HoCoS by 10 explanatory variables: two automatic parameters and eight associated factors).
The modelling of the HoCoS is shown in Table5.
Results of HoCoS modelling using automatic parameters and associated factors.
This modelling has anR2of 0.836 (explains 83.6% of the total variance of the HoCoS), anR2adjusted to 0.720 and an average RMSE of 5.30.
Five‐fold cross‐validation shows a correlationrS= 0.70 between the predicted score and the HoCoS.
The ‘leave one out’ cross‐validation has a higher correlation atrS= 0.78.
Finally, the predicted score on the 25 participants correlates atrS= 0.89 with the ‘ground truth’ HoCoS.
Table6represents the most significant results of the modelling.
The modelling including the associated factors explains more of the variance of the HoCoS (R2, 84% vs. 63% for the automatic parameters alone), due to an increase of explanatory variables.
These eight additional explanatory variables corresponding to associated factors come from 8 items for specific questionnaires: it seems relevant to keep all of them because of the increase of explained variance and the low their filling represents for the participants.
Comparison of performance between HoCoS models (n= 25).
The prediction of communication and speech impairment is more accurate when the model is trained on the entire sample (correlation at 0.89 vs 0.83 for the automatic parameters alone).
However, this gain is linked to poorer performance in five‐fold cross‐validation, which limits the reliability of the results of the modelling with associated factors.
We also compared the performance of the modelling with the perceptual severity score of the speech disorder assessed by a panel of six experts.
In all cases, the use of parameters from automatic analyses of spontaneous speech shows slightly better results than a perceptual score of severity of speech disorder assigned to participants by expert speech therapists.
Although the latter is not intended to quantify the communicative impairments of patients per se, it remains the reference in routine clinical practice.
The results of this study suggest that the impairment of communication can be accurately predicted by two automatic parameters derived from an automatic phonemic analysis of the speech signal (number of sonants recognised per second and number of occlusives recognised per second).
This measurement can be further enhanced by adding to the predictive models other indicators such as psychosocial indicators, relating to social circles, the anxio‐depressive status, consequences on other functions of the aerodigestive tract or the perceived handicap.
With these additional parameters, the prediction of communication and speech impairment is of better quality, while maintaining parsimonious models in terms of the number of variables included.
At present, automatic systems do not seem to be able to capture in the speech signal this information relating to a person's overall dynamics.
Thus, the integration of nonverbal cues that are not captured by the acoustic analyses to the modelling of the communication and speech impairment could be a way to complement the modelling.
In other words, there is potential to integrate information about nonverbal cues and signals to give a broader picture of overall communication and speech impairment/ability.
This type of study could be proposed by coupling the acoustic analysis of the speech signal with an analysis of the visual signal during the production of spontaneous speech (e.g., during a semidirected interview).
Studies have shown that pointing gestures or facial expressions (Dhingra et al.2020) can be accurately captured and analysed using colour‐depth cameras, type RGB‐D (red blue green depth).
An exploration of the links between these nonverbal cues and factors associated with communication or even quality of life remains to be carried out and would complement the analysis.
This analysis requires the creation of a new corpus in which the semidirected interview would be recorded in both audio and video.
Another way of automatically accessing psychosocial information would be to carry out a linguistic content analysis.
In our study, at the lexical level, the performance of the ASR system is poor, with a very high word error rate (at 84.51%).
These systems, trained on healthy speech, give bad recognition performances because of the comparison of altered acoustic speech models in pathological cases and typical acoustic models in healthy speech.
It is therefore possible that the variability of lexical output does not allow the lexical and discourse parameters to be significantly related to the HoCoS through a ‘bottom‐up’ ricochet effect, where low‐level phonemic impairments impact the results of the higher‐level linguistic components.
Then, training the system for pathological speech would result in more adapted and specific acoustic models with fewer recognition errors.
This type of analysis needs large corpora to train acoustic models that are relatively more stable, as described earlier by Middag et al. (2009). As no large French cancer speech corpora exist to date, some new systems could overcome this limitation: transfer learning techniques to adapt existing healthy corpora to pathological speech based on relatively little data (Gelin et al.2021), Listen, Attend and Spell (LAS) architectures (Chan et al.2016) or transformers‐type (Lu et al.2020).
These latest end‐to‐end systems are recent and could now improve the quality of speech recognition by incorporating less ‘a priori’ associated with the absence of a per se language model.
These various adaptations and the use of new speech recognition systems could lead to obtain outputs that are closer to the linguistic elements actually produced.
If the analysis of therecognisedelements carried out in this study allows a comparison with the standard production, the analysis of the elements actuallyproducedwould be more precise in terms of content analysis and would allow to find new clues at other linguistic levels in the speech signal.
Moreover, due to the complexity of phonemic transcription of spontaneous speech after cancer, we were unable to collect ground truth at the phonemic level.
A PER calculation was therefore not possible in this study.
However, given the discrepancy observed at the word level between post‐cancer speech and speech from speakers without speech disorder in the WER, our phonemic ASR probably shows an equally high PER when the phonemic ASR is applied to post‐cancer speech.
This could explain why the best predictors are low‐level linguistic parameters whose articulatory mode is particularly problematic in patients treated for oral or oropharyngeal cancer.
Cancer treatments can result in anatomical changes that limit the amplitude and strength of articulatory movements, and thus the quality of occlusion.
The positive correlation between the number of occlusives and sonants recognised per second and communication performance is therefore consistent with clinical data.
We can also hypothesise that the use of better‐performing ASR systems with reduced WER could limit this effect and highlight new parameters at different linguistic levels, all the more so if ASR systems with reduced WER are used.
Such systems do not yet exist, but their development could open up new fields of research about the diagnostic and therapeutic utility of ASR in clinical practice.
Of the 19 participants in the sample for whom tumour data could be retrieved, the majority (11/19, 58%) were patients who had been treated for large tumours (T3 or T4).
However, large tumours are the most deleterious for speech quality.
Low‐level alterations (at the phoneme level) will therefore be in the foreground with these patients.
Hence, our communication and speech impairment modelling highlights two phonemic parameters, the quality of recognition at the lexical level and the results obtained at higher levels on lexical output being weak.
Regarding the role of psychosocial factors on communication participation, our results are ultimately fairly close to the findings of Eadie (Eadie et al.2018).
This study shows an influence of the severityof self‐assessed speech impairment(phihcp),perceived social support(eccs1c, eccs1d, qfs52, qfs62) andperceived depression and anxiety(had2, hada).
However unlike Eadie et al. (2018), no influence ofcognitive statusortime since the end of treatmentis statistically shown by our study.
The effect of these variables can actually be mixed in other significant variables represented in the model.
Another explanation can be that cognitive status is assessed using a test, whereas Eadie's study (Eadie et al.2018) consisted of the self‐assessment of participants regarding their cognitive status: it is therefore possible that the feeling of cognitive functioning modifies communication skills more than the actual status.
Concerning the time since treatment [corresponding to the time since diagnosis in Eadie's study (Eadie et al.2018), diagnosis and treatment being close in time in cancer pathologies in any case], the average time is lower in this study (7.27 vs. 12.2 years).
It is therefore possible that the functional dynamics of communication have a different impact based on the period after treatment, with functioning becoming increasingly unsatisfactory.
Two final points remain to be analysed.
The resilience capacities of the participants were not measured, yet these are significant factors in Eadie et al. (2018), which would perhaps enable the results to be adjusted, particularly in terms of the time since treatment.
Finally, a significant link was found between communication and smell function (more precisely, the alert function relating to the fear of having an accident).
This point had not been explored by Eadie et al. (2018) in his study, in which the majority of participants had undergone a total laryngectomy (53.7%), a procedure that totally cancels the olfactory functions of treated patients.
This significant link is in line with the results of Stockhorst's study (Stockhorst and Pietrowski2004), which highlighted a link between olfaction and communication in general, and that of Oleszkiewicz et al. (2020), which showed a link between olfaction and perceived well‐being.
Thus, our study shows that the greater the fear of an accident linked to loss of smell, the lower the communication score, and thus the greater the communication and speech impairment: olfaction therefore seems to be a factor in functional and psychosocial processes relating to speech, communication and quality of life.
As this study was conducted on 25 participants, enriching the corpus with the study of a larger sample size would allow us to specify and complete the exploratory analyses.
This increase in sample size would then allow for a greater generalisability of the results, or for additional subgroup analyses such as: the location of the tumour (acoustic and automatic parameters used in speech analysis are different according to it, Balaguer et al.2020b) or the tumour size (Borggreven et al.2007).
Another avenue to explore is therefore to determine groups according to the severity of speech disorder.
Recent studies have shown a good performance of automatic systems (based on X‐vectors) in predicting intelligibility after ENT cancer (Quintas et al.2020).
Different predictive models could then be used depending on the degree of intelligibility of the participants, with a focus on phonetic and phonemic parameters for all participants, and a study of parameters of a higher linguistic level for participants of higher intelligibility and for whom ASR systems would perform better.
Afterwards, a range of communication needs could be highlighted according to the degree of impairment.
The links between the factors associated with speech and communication and quality of life have yet to be explored in more detail.
Although the model proposed by Wilson (Wilson and Cleary1995) allows us to target the factors involved in the functional and psychosocial dynamics more generally, it is likely that the links between each of these factors (speech disorder, associated factors, communication and quality of life) are not as linear or direct in ENT cancer.
Theoretical modelling specific to patients treated for oral cavity or oropharyngeal cancer would enable clinicians to explain the relationships between each level more specifically and to better explain and predict the quality of life.
Finally, the automatic analysis of spontaneous speech is a line of research to be developed, based on technological developments in speech recognition systems, to enable the most comprehensive measurement of communication and speech impairment in patients treated for oral or oropharyngeal cancer.
The results of this study suggest that the use of automatic speech analysis allows a reliable prediction of the communication and speech impairment experienced by the patients, with limited resources (the prediction results from two phonemic parameters given by a unique ASR system).
The prediction is improved by adding to the predictive model scores from biopsychosocial autoquestionnaires filled by the patients.
This prediction, based on an ASR system and data from self‐administered questionnaires, represents a clinical breakthrough in the assessment and follow‐up of patients with speech disorders following oral or oropharyngeal cancer.
This study opens up new perspectives for the use and optimisation of ASR systems in clinical assessments, on the one hand, and, on the other, the consideration of functional and psychosocial needs expressed by patients during their therapeutic follow‐up.