Inappropriate manipulations of digital images pose significant risks to research integrity.
Here we assessed the capability of students and researchers to detect image duplications in biomedical images.
We conducted a pen-and-paper survey involving medical students who had been exposed to research paper images during their studies, as well as active researchers.
We asked them to identify duplications in images of Western blots, cell cultures, and histological sections and evaluated their performance based on the number of correctly and incorrectly detected duplications.
A total of 831 students and 26 researchers completed the survey during 2023/2024 academic year.
Out of 34 duplications of 21 unique image parts, the students correctly identified a median of 10 duplications (interquartile range [IQR] = 8–13), and made 2 mistakes (IQR = 1–4), whereas the researchers identified a median of 11 duplications (IQR = 8–14) and made 1 mistake (IQR = 1–3).
There were no significant differences between the two groups in either the number of correctly detected duplications (p= .271, Cliff’s δ = 0.126) or the number of mistakes (p= .731, Cliff’s δ = 0.039).
Both students and researchers identified higer percentage of duplications in the Western blot images than cell or tissue images (p< .005 and Cohen’s d = 0.72;p< .005 and Cohen’s d = 1.01, respectively).
For students, gender was a weak predictor of performance, with female participants finding slightly more duplications (p< .005, Cliff's δ = 0.158), but making more mistakes (p< .005, Cliff's δ = 0.239).
The study year had no significant impact on student performance (p= .209; Cliff's δ = 0.085).
Despite differences in expertise, both students and researchers demonstrated limited proficiency in detecting duplications in digital images.
Digital image manipulation may be better detected by automated screening tools, and researchers should have clear guidance on how to prepare digital images in scientific publications.
The online version contains supplementary material available at 10.1186/s41073-025-00172-0.
As a method of presenting data, digital images in scientific literature are often modified using software to improve their clarity and quality [1].
Appropriate modifications, such as cropping to draw focus to key areas, adjusting saturation to enhance colours or contrast to highlight details, and enhancing brightness for better visibility, are generally accepted practices [2].
However, such modifications should be done with care, should not remove any information from the image, and should be transparently reported in the methods section or figure legends in the manuscript [3].
This practice is endorsed by organizations such as the United States Office of Research Integrity and the Committee on Publication Ethics [4,5].
The practice of modifying digital images as a way of misrepresenting or manipulating data has pervaded science for some time [1], with any such act being considered a serious form of research misconduct [6].
In 2016, image manipulations likely accounted for approximately 70% of the cases investigated by the United States Office of Research Integrity [7].
Recent studies have shown that this phenomenon is more prevalent than previously thought.
For example, a large-scale analysis of over 20,000 papers published between 1995 and 2014 found an increase in inappropriate image duplication over time [8].
According to the study, the percentage of papers containing inappropriate image duplications was less than 2% from 1995 to 2002, but it rose in 2003 and has remained close to or above 4% since then.
Another study found that around 6% of 960 papers from 2009 to 2016 contained inappropriate image duplications; they were inversely correlated with journal impact factors, while the authors who included duplicated images in one of their papers were more likely to do so in others as well [9].
The consequences of image manipulation can be severe, especially in biomedicine [10].
This was seen in the cases of Alzheimer’s research, where the authors of a seminal paper manipulated Western blots to exaggerate the presence of Aβ oligomers and present them as a primary cause of the illness, misleading research efforts and leading to wasted research funding and ineffective therapeutic approaches for nearly two decades [11].
Manipulated images have also been identified in cancer research; for example, investigations into the Dana-Farber Cancer Institute, a Harvard Medical School’s affiliate, found that Western blot images had been manipulated in 38 of their publications [12].
Moreover, as of January 2025, nine Nobel prize winners have had 27 papers retracted, with 17 papers co-authored by four laurates being retracted due to inappropriate image manipulations [13,14].
Image manipulations in scientific publications are usually identified by experts or specialized software.
However, there is a lack of research on the ability of humans to detect such manipulations.
To address this gap, we assessed the ability of medical students with general knowledge of biomedical images and experts who encounter such images daily to detect image alterations, focusing specifically on image duplications.
In this cross-sectional survey study, we designed and delivered a pen-and-paper survey to the students and academic researchers at the University of Split School of Medicine (USSM) in Croatia.
The participants were drawn from all study years at all four study programs at the USSM: Medicine, Dental Medicine, Pharmacy, and Medical Studies in English.
The students enrolled in the first three programs were given the Croatian version of the questionnaire, while the students of Medicine in English received the English version.
We also surveyed academic researchers from three research fields: anatomy and histology, pharmacy and pharmacology, and immunology.
All researchers completed the Croatian version of the questionnaire.
The survey was conducted during the 2023/2024 academic year.
To determine the minimum number of participants needed to reflect the entire student population of USSM, we used an online sample size calculator (www.calculator.net/sample-size-calculator.html) with a 95% confidence level and a 5% margin of error.
The required sample size varied across the four study programs: 227 participants for Medicine (552 enrolled students), 126 participants for Dental Medicine (186 enrolled students), 116 participants for Pharmacy (164 enrolled students), and 173 participants for Medicine in English (312 enrolled students).
At the time of the study, there were 12 active researchers at the Departments of Anatomy and Histology, 12 at the Departments of Pharmacy and Pharmacology, and 8 at the Department of Immunology and Medical Genetics.
We designed a questionnaire with six tasks, each featuring images of Western blots, cell cultures, or histological sections (Table2).
The questionnaire included randomly-arranged tasks with varying levels of difficulty, featuring three Western blot images, one histological section, and two cell culture images.
These presented challenges of identifying duplicated bands, duplications within tissue structure, or manipulations in cellular arrangements, respectively.
All images were previously flagged for image manipulation, either in an article published on this topic [15], or on the PubPeer database, an online platform where researchers can discuss potential issues with published research (https://pubpeer.com).
We also manually screened each image to identify any additional duplications and uploaded the images to theForensicallyphoto forensics tool (https://29a.ch/photo-forensics).
Through this process, we found one additional duplication in the first and second tasks, and two additional duplications in the fourth task.
Simple duplication – reuse of the same image across different groups or experiments.
Duplication with repositioning – duplicated images and moved within the frame.
Duplication with alteration – parts of an image are duplicated and modified.
Due to copyright restrictions on certain images, we are unable to share the complete questionnaire.
Two examples, sharable by under the Creative Commons Attribution 4.0 International (CC BY 4.0) license, are presented in the Supplement (Part S1), one depicting a Western blot and the other a cell culture.
There were 21 unique image parts, which could be duplicated more than once, so that the total score of possible duplication events was 34, which was the maximum score in the survey.
The participants were instructed to mark detected duplications using coloured pencils, assigning a unique colour to each instance of duplication.
They were informed that each image contained at least one instance of image duplication.
To ensure understanding, we provided the participants with a completed example (different from those in the survey) as a reference and encouraged them to ask questions at any point if clarification was needed.
The survey had a time cap of 25 min for all participants, with an additional five minutes allocated for instructions.
We collected the participants’ basic demographic information, including gender, year of birth, study year, and study program.
All research team members participated in the survey grading.
Specifically, a single survey was graded independently by two researchers, who assigned one point for each correctly identified set of duplications and (separately) one point for each ‘mistake’ (areas marked as duplications that were, in fact, not duplicated).
In cases of discrepancies, the two researchers consulted a third researcher for resolution.
For the overall scores, we counted the total number of identified duplications and total number of mistakes.
To compare duplications between individual images, values were expressed as percentages, since the total number of duplications differed between examples in the survey.
We summarized the results as medians and interquartile ranges (IQRs), or as percentages of correctly and incorrectly detected duplications across various tasks.
We used the Shapiro–Wilk test to assess the normality of data distributions, Mann–Whitney test to compare two independent groups, and Kruskal–Wallis test with Dunn’spost-hoctest to compare more than two groups.
Effect sizes were calculated using Cliff’s delta for comparisons between two independent groups, and were interpreted as negligible (< 0.147), small (0.147–0.330), medium (0.330–0.474), and large (≥ 0.474).
Eta-squared (η2) was used as the effect size measure for Kruskal–Wallis tests, and was interpreted as small (~ 0.01), medium (~ 0.06), and large (≥ 0.14).
To compare paired data, we used the Wilcoxon signed-rank test and Cohen’s d with 95% confidence intervals as a measure of effect size.
Cohen’s d was interpreted as small for ~ 0.20, medium for ~ 0.50, and large for ≥ 0.80 effect size.
No correction for multiple comparisons was applied.
We also used linear regression to investigate the relationship between demographic predictors and the number of correctly and incorrectly marked duplications.
Statistical analysis was performed using MedCalc (version 20.218), JASP (version 0.18.3.0) and Python (version 3.8.19).
All visualizations were created in Python using the Matplotlib library (https://matplotlib.org/).
The Ethics Committee of the USSM approved the study protocol (Class No.: 029–01/25–02/0001; Reg.
No.: 2181–198-03–04–25–0020).
At the beginning of the survey, the participants were informed in writing about the study’s objectives, measures taken to maintain their anonymity, voluntary nature of their participation, and their right to withdraw at any time.
They were also informed that, by continuing the survey, they give their informed consent.
We did not collect any identifying information aside from basic demographic data, and only aggregate results are presented.
A total of 840 out of 1,214 students (69.2% response rate) and 26 out of 32 researchers (81.3% response rate) completed the survey.
We excluded nine incomplete responses (all from students), leaving 857 surveys for analysis.
In cases where a participant failed to answer the specific tasks or marked no areas on an image, those responses were treated as missing.
Such missing data were excluded from task-level analyses and visualizations but did not lead to exclusion of the entire participant record.
Similarly, we excluded the participants who identified as nonbinary or chose not to disclose their gender when comparing results across male and female participants.
Baseline characteristics of the student and researchers are summarized in Table1.
Among 831 students, the median age was 23 years (IQR = 21–24), and 75% were female.
Students were drawn from all four academic programs.
Among 26 researchers, 69% were female and were evenly distributed across three research fields.
Age data were not collected from researchers due to the small sample size, in order to preserve anonymity.
Students correctly identified a median of 10 duplications out of maximum 34 duplications (IQR = 8–13), which corresponds to 29.4% detection rate (IQR = 23.53%–38.24%).
They incorrectly marked a median of 2 duplications (IQR = 1–4).
Students found more duplications relating to gel images (median = 33.3%, IQR = 20.2–42.9%) than cell/tissue images (median = 30.8%, IQR = 23.1–30.8%) (p< 0.005, Cohen’s d = 0.72; 95% CI = 0.66–0.79) (Supplement, Figure S1A).
These values reflect the proportion of total possible gel or cell/tissue duplications that were correctly identified, as the number of duplications differed between these two image types.
There was no statistically significant difference in incorrectly identified manipulations between the two types of images (p=0.991; Cohen’s d = 0.01; 95% CI = –0.072, 0.059) (Supplement, Figure S1A).
There was no difference in correctly (p= 0.209; Cliff's δ = 0.085; 95% CI = −0.221, 0.051) and incorrectly (p= 0.097; Cliff's δ = 0.112; 95% CI = −0.024, 0.245) marked duplications between first- and fifth-year students.
Gender was a statistically significant but weak predictor of both the number of correctly marked duplications (R2= 1.6%;p< 0.005) and the number of mistakes (R2= 2.22%;p< 0.005).
Female students found more duplications compared to male students (p< 0.005; Cliff's δ = 0.158; 95% CI = 0.069, 0.252), but they also had more mistakes (p< 0.005; Cliff's δ = 0.239, 95% CI = 0.156, 0.324) (Supplement, Figure S2).
The percentage of correctly identified duplications varied across the tasks (Table2).
The sixth task proved the most challenging for students, as the majority failed to detect any duplication.
In contrast, the third task was the easiest, with most students identifying one duplication.
The highest number of mistakes was found in the fourth task (median = 1, IQR = 0–1) and the sixth task (median = 0, IQR = 0–2), with the latter having the widest range of correctly identified duplications (IQR = 0–16) (Supplement, Figure S3).
Researchers correctly identified a median of 11 duplications (IQR = 8–14) (32.4% of total correct answers) and incorrectly marked a median of 1 duplication (IQR = 1–3).
They also found more duplications relating to gel images (median = 35.7%, IQR = 23.8–51.2%) than cell/tissue images (median = 30.8%, IQR = 23.1–30.8%) (p< 0.005, Cohen’s d = 1.01; 95% CI = 0.70, 1.41).
However, we found no significant differences between students and researchers, either in the number of correctly (p= 0.271: Cliff's δ = 0.126; 95% CI = −0.343, 0.109) or incorrectly marked duplications (p= 0.731; Cliff's δ = 0.039; 95% CI = −0.17, 0.217) (Supplement, Figure S4), or across the four departments (p= 0.305; η2= 0.016; 95% CI = −0.072, 0.412 for correctly marked duplications andp= 0.792; η2= 0.067; 95% CI = −0.083, 0.303 for incorrectly marked duplications) (Supplement, Figure S5).
As with the students, the sixth task was also the most challenging for researchers.
Due to a negligible difference between the two groups, the results were combined to show pooled detection rates for duplications (Table2).
This study provides new insights into the ability of students and researchers to detect duplications in digital images.
Our results suggest that students at USSM have poor skills in detecting image duplications, regardless of their study level.
This conclusion is compounded by the findings from researchers whose skills were comparable to the students’, despite their expertise in viewing similar images in their day-to-day work, as both groups identified less than one-third of all duplications.
These findings suggest that the ability to detect image manipulations is distinct from factors such as familiarity with similar images, medical education, or gender, and may instead be associated with other, yet unexplored factors.
However, these results should be considered preliminary, and further validation in other academic contexts is needed to ensure generalizability.
It is also important to note that participants in our study were told that every image contained at least one duplication.
Therefore, the detection rates observed in our study likely represent an upper bound on real-world performance.
Future studies could assess detection ability when the presence of manipulation is uncertain, representing real-world conditions where reviewers are not told to look for a duplication in every figure.
There was a variation in detection rates across tasks, pointing to a key role of duplication geometry in visual detectability.
Simple duplications, where the duplicated region is an exact copy and spatially close to the original, were detected more often than duplications with repositioning or alteration.
This supports the proposition that humans are sensitive to visual symmetry and rely on it when scanning for irregularities [16].
Our findings on the students’ and researchers’ ability to identify duplications in research images are similar to those from studies on the general population and common images.
In a study by Nightingale et al., over 15,000 adults were asked to detect manipulations within images of real-world scenes – photographs depicting everyday environments, such as an indoor room, a city street, or a natural landscape [17].
Three-quarters of the images were digitally altered using techniques such as airbrushing to remove wrinkles and through the addition or removal of image elements.
The participants in that study demonstrated poor ability to detect manipulations, despite being shown a short video describing these techniques.
Similarly, in another study, participants’ performance in identifying manipulations was poor despite the intentional errors in reflections and shadows serving as visual cues to identify inconsistencies within the images [18].
Moreover, a recent study with over 800 participants and involving AI-generated images of histological samples showed that both experts and naive participants struggled to distinguish real images from AI-generated ones [19].
These findings, along with the finding from our study, emphasize the need for improved training and tools to enhance image manipulation detection.
Detecting and preventing image manipulation is essential for maintaining scientific reliability, as it helps uncover cases of fraudulent research and ensures that future work is not based on false data [20].
This is why it is crucial to ensure that researchers are aware of best practices for handling digital images.
Further studies could investigate whether certain types of interventions could sensitize individuals to be more successful at detecting image manipulations, or whether certain educational packages or tools could be applied to raise awareness about this issue.
They could also explore other variables that may be associated with the ability to recognize digital image manipulations, including awareness of key research integrity issues, and general visual orientational skills.
It would be useful to delineate these factors, as they would inform on whether, for example, research integrity education could target this gap in skills.
However, instead of relying solely on human detection, it would be useful to develop AI tools that outperform humans in identifying manipulated images in scientific papers.
While human oversight is essential, AI-based solutions are capable of detecting different image editing operations, such as duplications, selective image modifications, and other forms of tampering that may go unnoticed by the human eye [21].
The adoption of AI-driven systems, such asProofig,ImaCheck, andImageTwinhas already helped publishers detect problematic images before publication [22].
UsingProofigsoftware, a study of 1,300 papers submitted to the American Association for Cancer Research journals between 2021 and 2022 found that 15% of the articles contained image duplications, with 28% of these being intentional [23].
In addition to AI detection, a research image integrity system incorporating hashing and public key cryptography could offer a comprehensive solution to address fraudulent image practices [24].
In the context of image verification, public key cryptography can be used to create a digital signature for image authentication [23].
This is achieved by first generating a cryptographic hash of an image, which acts as a unique digital fingerprint [25].
The hash is then encrypted with a private key, creating a digital signature attached to the image metadata [25].
To verify authenticity, the recipient retrieves the public key, decrypts the digital signature, and compares the revealed hash with the one of the received images [25].
A match confirms the image’s authenticity, while a mismatch indicates potential manipulation [25].
Publishers also play a pivotal role in ensuring the integrity of scientific literature.
They should take accountability for the content they publish by implementing proper measures, such as establishing clear journal guidelines on image editing, mandating the submission of raw data, and utilizing automated screening tools to detect potential manipulations.
The primary limitation of this study was its focus on students and researchers from a single university, which limits the generalizability of the findings to other student populations and academic institutions.
Additionally, the sample size for the researcher group was small because not all members of the departments were available for the study.
Finally, the last image in the questionnaire contained intricate details and small elements, which might have been clearer in an online format which would have allowed for better resolution and magnification.
Many of the participants responded incorrectly to the last task or simply did not attempt to solve it; less variation in the difficulty of the surveyed tasks would be more favourable for future studies conducting similar tests.
Our findings confirm that image duplications are difficult to detect, particularly with the naked eye.
This is true for readers with little research knowledge or experience, such as medical students with basic image knowledge, and research experts who regularly work with biomedical images.
These results demonstrate the importance of raising awareness about image integrity to prevent image manipulation occurring in the first place.
It also opens the need for the development of AI tools and hashing techniques that outperform humans in identifying manipulated images in scientific publications.
Additionally, publishers play an important role in ensuring the integrity of scientific literature by implementing clear journal guidelines on image editing and implementing automated screening tools.
Finally, promoting a culture of integrity within research institutions can further reduce instances of image manipulation and ensure the credibility of scientific research.
We thank Vladimir Ercegović, a student of the sixth year of Medicine at the University of Split School of Medicine, for his help with data collection.
All authors participated in the conceptualization, development of methodology and the questionnaire design.
All authors participated in applying the questionnaire, collecting data, and conducting formal analysis and investigation.
AMij performed statistical analysis and wrote computer code for visualization in Python, and was a major contributor in writing the manuscript.
MFŽ, LU and AM had a major role in reviewing and editing the manuscript.
AM was also responsible for project supervision, administration and funding acquisition.
All authors read and approved the final manuscript.
This study was funded by the Croatian Science Foundation “Professionalism in Health – Decision making in practice and research” (ProDeM) under grant agreement No. IP-2019–04-4882.
The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.