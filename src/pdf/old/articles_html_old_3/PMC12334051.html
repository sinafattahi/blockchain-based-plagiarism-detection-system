
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-a68b4900.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-0a3f24ce.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Federated hierarchical MARL for zero-shot cyber defense - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="4DDE49D689B776F31B49D60001D9B44B.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334051/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="Federated hierarchical MARL for zero-shot cyber defense">
<meta name="citation_author" content="Adel Alshamrani">
<meta name="citation_author_institution" content="Department of Cybersecurity, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia">
<meta name="citation_publication_date" content="2025 Aug 8">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0329969">
<meta name="citation_doi" content="10.1371/journal.pone.0329969">
<meta name="citation_pmid" content="40779609">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334051/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334051/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334051/pdf/pone.0329969.pdf">
<meta name="description" content="Cyber defense systems face increasingly sophisticated threats that rapidly evolve and exploit vulnerabilities in complex environments. Traditional approaches which often rely on centralized monitoring and static rule-based detection, struggle to ...">
<meta name="og:title" content="Federated hierarchical MARL for zero-shot cyber defense">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Cyber defense systems face increasingly sophisticated threats that rapidly evolve and exploit vulnerabilities in complex environments. Traditional approaches which often rely on centralized monitoring and static rule-based detection, struggle to ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334051/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-testid="header" data-header >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            
                <a class="ncbi-header__logo-container" href="https://www.ncbi.nlm.nih.gov/">
                    <img alt="
                                  NCBI home page
                              "
                         class="ncbi-header__logo-image"
                         src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg" />
                </a>
            

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-testid="searchPanel"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      aria-describedby="search-field-desktop-navigation-help-text"
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only"
                           data-testid="label"
                           for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           data-testid="textInput"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    data-testid="button"
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  role="search">
                <label class="usa-sr-only" for="search-field">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="search" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        <a class="usa-link" href="https://www.ncbi.nlm.nih.gov/pmc/advanced/" data-ga-action="featured_link" data-ga-label="advanced_search">
                            Advanced Search
                        </a>
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12334051">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0329969"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0329969.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12334051%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12334051/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12334051/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334051/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0329969" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 8;20(8):e0329969. doi: <a href="https://doi.org/10.1371/journal.pone.0329969" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329969</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Federated hierarchical MARL for zero-shot cyber defense</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Alshamrani%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Adel Alshamrani</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Adel Alshamrani</span></h3>
<div class="p">
<sup>1</sup>Department of Cybersecurity, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia</div>
<div>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Alshamrani%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Adel Alshamrani</span></a>
</div>
</div>
<sup>1,</sup><sup>*</sup>
</div>
<div class="cg p">Editor: <span class="name western">Zeashan Hameed Khan</span><sup>2</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>Department of Cybersecurity, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia</div>
<div id="edit1">
<sup>2</sup>King Fahd University of Petroleum &amp; Minerals, SAUDI ARABIA</div>
<div class="author-notes p">
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>The authors have declared that no competing interests exist.</p></div>
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>asalshamrani@uj.edu.sa</span></p>
</div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Adel Alshamrani</span></strong>: <span class="role">Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review &amp; editing</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Zeashan Hameed Khan</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Mar 28; Accepted 2025 Jul 23; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Adel Alshamrani</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12334051  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40779609/" class="usa-link">40779609</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>Cyber defense systems face increasingly sophisticated threats that rapidly evolve and exploit vulnerabilities in complex environments. Traditional approaches which often rely on centralized monitoring and static rule-based detection, struggle to adapt to new, crafted, and novel attack patterns. This paper presents the Adaptive Zero-Shot Hierarchical Multi-Agent Reinforcement Learning (AZH-MARL) framework, a novel approach that integrates hierarchical reinforcement learning, zero-shot learning capabilities, and federated knowledge sharing to build resilient cyber defense systems. The hierarchical structure decomposes complex defense tasks into specialized sub-tasks managed by agents, reducing the learning problem’s complexity and enabling more efficient coordination. The zero-shot learning component allows the framework to recognize and response to previously unseen attack patterns through semantic mapping. Furthermore, the federated learning learning component facilitates for knowledge sharing across network domains while preserving data privacy, enabling collaborative defense without exposing sensitive information. The detailed evaluation demonstrates that our approach significantly outperforms existing methods across a range of scenarios. It achieves a high detection rate of 94.2% for known attacks and 82.7% for zero-day exploits, while maintaining a low false positive rate of 3.8%. This robust performance extends to the most sophisticated threats, achieving an 87.3% containment rate against Advanced Persistent Threats (APTs). The framework’s zero-shot capability is underpinned by a semantic mapping accuracy of 89.3%, which enables rapid adaptation to novel threats. Consequently, the mean response time is reduced by 35% for known attacks and 42% for zero-day exploits compared to the best-performing baseline. Finally, the federated learning architecture proves highly efficient, reducing communication overhead by 45% while preserving privacy. These results collectively demonstrate our framework’s potential to set a new standard for resilient and adaptive cyber defense in complex, distributed environments.</p></section><section id="sec001"><h2 class="pmc_sec_title">1 Introduction</h2>
<p>The increasing sophistication and frequency of cyberattacks pose significant challenges to traditional defense systems. Modern cyber threats leverage advanced techniques such as multi-stage attacks, zero-day exploits, and adaptive evasion strategies capable of bypassing conventional security measures [<a href="#pone.0329969.ref001" class="usa-link" aria-describedby="pone.0329969.ref001">1</a>]. These challenges are further compounded in distributed environments where privacy constraints limit the sharing of security data across organizational boundaries [<a href="#pone.0329969.ref002" class="usa-link" aria-describedby="pone.0329969.ref002">2</a>].</p>
<p>Reinforcement learning (RL) has emerged as a promising approach for developing adaptive cyber defense systems that can learn from experience and improve over time [<a href="#pone.0329969.ref003" class="usa-link" aria-describedby="pone.0329969.ref003">3</a>,<a href="#pone.0329969.ref004" class="usa-link" aria-describedby="pone.0329969.ref004">4</a>]. By framing cyber defense as a sequential decision-making problem, RL enables the development of policies that optimize long-term security objectives rather than relying on predefined rules [<a href="#pone.0329969.ref005" class="usa-link" aria-describedby="pone.0329969.ref005">5</a>]. Recent advances in deep reinforcement learning have demonstrated remarkable success in complex domains such as game playing [<a href="#pone.0329969.ref006" class="usa-link" aria-describedby="pone.0329969.ref006">6</a>,<a href="#pone.0329969.ref007" class="usa-link" aria-describedby="pone.0329969.ref007">7</a>] and autonomous driving [<a href="#pone.0329969.ref008" class="usa-link" aria-describedby="pone.0329969.ref008">8</a>], suggesting its potential applications in cybersecurity.</p>
<p>Multi-agent reinforcement learning (MARL) extends this paradigm to scenarios involving multiple learning agents, making it particularly suitable for distributed cyber defense [<a href="#pone.0329969.ref009" class="usa-link" aria-describedby="pone.0329969.ref009">9</a>,<a href="#pone.0329969.ref010" class="usa-link" aria-describedby="pone.0329969.ref010">10</a>]. By deploying specialized agents across distinct network segments, MARL approaches can provide more comprehensive protection while dynamically adapting to local conditions [<a href="#pone.0329969.ref002" class="usa-link" aria-describedby="pone.0329969.ref002">2</a>,<a href="#pone.0329969.ref011" class="usa-link" aria-describedby="pone.0329969.ref011">11</a>]. However, current MARL-based approaches for cyber defense face several limitations:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Complexity of Coordination</strong>: Coordinating multiple agents in large-scale networks introduces significant complexity, often leading to inefficient learning and suboptimal policies.</p></li>
<li><p><strong>Limited Adaptation to Novel Threats</strong>: Most approaches require extensive retraining to adapt to new attack patterns, creating vulnerability windows during which networks remain exposed.</p></li>
<li><p><strong>Privacy Constraints</strong>: Effective collaboration across organizational boundaries is hindered by privacy concerns and regulatory requirements that restrict the sharing of sensitive security data.</p></li>
<li><p><strong>Scalability Challenges</strong>: As networks grow in size and complexity, centralized approaches struggle to process the increasing volume of security data in real-time.</p></li>
</ol>
<p>To address these limitations, I propose Adaptive Zero-Shot Hierarchical Multi-Agent Reinforcement Learning (AZH-MARL), a novel framework that integrates three key components:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Hierarchical MARL</strong>: I employ a hierarchical structure that decomposes complex defense tasks into specialized sub-tasks managed by distinct agents. This approach reduces the complexity of the learning problem and enables more efficient coordination.</p></li>
<li><p><strong>Zero-Shot Learning</strong>: I integrate zero-shot learning capabilities that enable recognition of and response to previously unseen attack patterns through semantic mapping. This approach addresses the critical vulnerability window associated with retraining for new threats.</p></li>
<li><p><strong>Federated Knowledge Sharing</strong>: I implement a federated learning framework that allows knowledge sharing across network domains while preserving data privacy. This approach enables collaborative defense without exposing sensitive information.</p></li>
</ol>
<p>This work builds on the foundation of our previous work [<a href="#pone.0329969.ref012" class="usa-link" aria-describedby="pone.0329969.ref012">12</a>] by significantly enhancing the multi-agent reinforcement learning framework for cyber defense. The two-level hierarchical structure is extended to incorporate more sophisticated agent coordination through transformer-based architectures integrated with Markov Logic Networks, enabling more robust threat detection and response capabilities. Our new approach preserves the adaptive defense strategy concept while introducing zero-shot learning capabilities which allow defensive agents to respond effectively to previously unseen attack patterns without requiring complete retraining. Moreover, I address the scalability and real-time performance limitations identified in the previous work through federated knowledge-sharing mechanisms that enable defensive agents to collaboratively learn without exposing sensitive data.</p>
<p>Our comprehensive evaluation demonstrates that AZH-MARL significantly outperforms existing methods across various metrics. For known attacks, our approach achieves a detection rate of 94.2% and a false positive rate of 3.8%, comparable to the best-performing baselines. For zero-day exploits, our approach achieves a detection rate of 82.7%, representing a 37.5% improvement over approaches without zero-shot capabilities. The mean time to response is reduced by 35% for known attacks and 42% for zero-day exploits compared to the best-performing baseline.</p>
<p>The main contributions of this paper are:</p>
<ol class="list" style="list-style-type:decimal">
<li><p>A hierarchical MARL framework that decomposes complex defense tasks into manageable sub-tasks, enabling more efficient learning and coordination.</p></li>
<li><p>A zero-shot learning approach that enables adaptation to previously unseen attack patterns without requiring extensive retraining.</p></li>
<li><p>A federated learning framework that enables privacy-preserving knowledge sharing across network domains.</p></li>
<li><p>Comprehensive evaluation demonstrating significant improvements over existing approaches across various metrics.</p></li>
</ol>
<p>The remainder of this paper is organized as follows: <a href="#sec002" class="usa-link">Sect 2</a> reviews related work in MARL for cyber security, hierarchical reinforcement learning, zero-shot learning, and federated learning. <a href="#sec007" class="usa-link">Sect 3</a> presents our system model and problem formulation. <a href="#sec018" class="usa-link">Sect 4</a> describes the implementation and evaluation of our approach. <a href="#sec049" class="usa-link">Sect 5</a> concludes the paper and discusses future research directions. The full list of abbreviations is provided in <a href="#sec051" class="usa-link">Appendix B</a> for convenience.</p></section><section id="sec002"><h2 class="pmc_sec_title">2 Related work</h2>
<section id="sec003"><h3 class="pmc_sec_title">2.1 MARL in cybersecurity</h3>
<p>Multi-agent reinforcement learning (MARL) has gained significant attention in cybersecurity due to its ability to handle complex, dynamic environments with multiple interacting entities. For instance Malialis and Kudenko [<a href="#pone.0329969.ref009" class="usa-link" aria-describedby="pone.0329969.ref009">9</a>] proposed a distributed response system using MARL to defend against distributed denial-of-service (DDoS) attacks. Their approach demonstrated improved resilience compared to centralized approaches but was limited to specific attack types and required extensive retraining for new threats.</p>
<p>Oh <em>et al</em>. [<a href="#pone.0329969.ref013" class="usa-link" aria-describedby="pone.0329969.ref013">13</a>] proposed a deep reinforcement learning (DRL) framework that uses cyberattack simulation to enhance cybersecurity defenses. In their study, DRL agents (e.g., DQN, PPO) are trained in simulated environments to autonomously execute multi-stage attacks, such as privilege escalation and lateral movement, while evading detection. By modeling adversarial behaviors, the framework identifies system vulnerabilities and generates actionable data for improving intrusion detection systems. The authors highlight DRL’s adaptability to evolving threats. This work underscores the potential of AI-driven attack simulation for proactive defense, advocating for further research into real-world deployment and adversarial robustness.</p>
<p>Elderman <em>et al</em>. [<a href="#pone.0329969.ref010" class="usa-link" aria-describedby="pone.0329969.ref010">10</a>] explored adversarial reinforcement learning in a simulated network environment, where defensive agents learned to counter adversarial agents. Although this approach showed promising results in adapting to evolving attack strategies, it faced scalability challenges in large networks and did not address privacy concerns in collaborative settings.</p>
<p>Nguyen and Reddi [<a href="#pone.0329969.ref002" class="usa-link" aria-describedby="pone.0329969.ref002">2</a>] extended MARL to intrusion detection systems, employing multiple specialized agents to monitor distinct aspects of network traffic. Their approach improved detection accuracy but required centralized coordination, limiting its applicability in distributed environments with privacy constraints.</p>
<p>Fu <em>et al</em>. [<a href="#pone.0329969.ref014" class="usa-link" aria-describedby="pone.0329969.ref014">14</a>] presented a novel approach to multi-agent reinforcement learning (MARL) that incorporates self-clustering and hierarchical mechanisms within an extensible cooperation graph structure. Their work addresses the challenge of scalability and coordination in multi-agent systems by allowing agents to dynamically form clusters based on their interactions and objectives.</p>
<p>Collectively, these approaches demonstrate the potential of MARL for cyber defense, but also highlight the need for more scalable, adaptive, and privacy-preserving solutions.</p></section><section id="sec004"><h3 class="pmc_sec_title">2.2 Hierarchical reinforcement learning</h3>
<p>Hierarchical reinforcement learning (HRL) addresses the complexity of large state and action spaces by decomposing tasks into hierarchical structures. For example, Kulkarni <em>et al</em>. [<a href="#pone.0329969.ref015" class="usa-link" aria-describedby="pone.0329969.ref015">15</a>] introduced hierarchical deep Q-networks that operate at different temporal scales, enabling more efficient learning in complex environments. Similarly, Nachum <em>et al</em>. [<a href="#pone.0329969.ref016" class="usa-link" aria-describedby="pone.0329969.ref016">16</a>] proposed a data-efficient HRL approach that learns both high-level policies for goal selection and low-level policies for goal achievement.</p>
<p>In the context of MARL, Bacon <em>et al</em>. [<a href="#pone.0329969.ref017" class="usa-link" aria-describedby="pone.0329969.ref017">17</a>] developed the option-critic architecture, which learns options (i.e., temporally extended actions) and policies over those options simultaneously. Separately, Vezhnevets <em>et al</em>. [<a href="#pone.0329969.ref018" class="usa-link" aria-describedby="pone.0329969.ref018">18</a>] introduced feudal networks, which employ a manager-worker hierarchy to facilitate learning in environments with sparse rewards.</p>
<p>While these hierarchical approaches demonstrate improved learning efficiency and scalability compared to flat RL methods, they have not been extensively applied to cybersecurity scenarios particularly those with privacy constraints or the need for adaptation to novel threats.</p></section><section id="sec005"><h3 class="pmc_sec_title">2.3 Zero-shot learning</h3>
<p>Zero-shot learning (ZSL) enables recognition of classes not seen during training by leveraging semantic information. Xian <em>et al</em>. [<a href="#pone.0329969.ref019" class="usa-link" aria-describedby="pone.0329969.ref019">19</a>] provided a comprehensive evaluation of ZSL methods, highlighting their potential for adapting to novel situations without extensive retraining. Wang <em>et al</em>. [<a href="#pone.0329969.ref020" class="usa-link" aria-describedby="pone.0329969.ref020">20</a>] surveyed ZSL settings, methods, and applications, emphasizing the importance of semantic attribute spaces for effective transfer.</p>
<p>Romera-Paredes and Torr [<a href="#pone.0329969.ref021" class="usa-link" aria-describedby="pone.0329969.ref021">21</a>] proposed a simple yet effective approach to ZSL using a linear mapping between feature and semantic spaces. Socher <em>et al</em>. [<a href="#pone.0329969.ref022" class="usa-link" aria-describedby="pone.0329969.ref022">22</a>] developed a cross-modal transfer approach that enables zero-shot transfer through semantic embeddings.</p>
<p>While these approaches have shown promise in computer vision and natural language processing, their application to cyber defense remains limited, particularly in the context of recognizing and responding to novel attack patterns.</p>
<p>However, the proposed work in [<a href="#pone.0329969.ref023" class="usa-link" aria-describedby="pone.0329969.ref023">23</a>] explores how zero-shot learning (ZSL) can revolutionize cybersecurity by enabling systems to detect and respond to previously unseen threats without relying on training data for every attack variant. It highlights ZSL’s dual role in both adversarial applications—where attackers generate novel exploits that evade traditional defenses—and defensive strategies, such as classifying unknown malware or adapting to zero-day vulnerabilities using semantic reasoning. The study underscores key challenges, including the need for robust threat ontologies and explainable AI, while advocating for hybrid approaches (e.g., ZSL combined with few-shot learning) and collaborative threat intelligence to enhance real-world deployment. Ultimately, the paper positions ZSL as a transformative tool for next-generation cybersecurity, balancing its offensive and defensive potential with ethical and practical considerations.</p></section><section id="sec006"><h3 class="pmc_sec_title">2.4 Federated learning</h3>
<p>Federated learning enables collaborative model training without sharing raw data, addressing privacy concerns in distributed environments. McMahan <em>et al</em>. [<a href="#pone.0329969.ref024" class="usa-link" aria-describedby="pone.0329969.ref024">24</a>] introduced Federated Averaging (FedAvg), which aggregates model updates from distributed clients while keeping data local. Li <em>et al</em>. [<a href="#pone.0329969.ref025" class="usa-link" aria-describedby="pone.0329969.ref025">25</a>] surveyed challenges and methods in federated learning, highlighting issues related to communication efficiency, privacy guarantees, and model personalization.</p>
<p>Gohil <em>et al</em>. [<a href="#pone.0329969.ref026" class="usa-link" aria-describedby="pone.0329969.ref026">26</a>] presented AttackGNN, a reinforcement learning framework for evaluating the robustness of Graph Neural Networks (GNNs) in hardware security applications. Their approach employs an RL-based adversarial agent to discover effective attacks without requiring gradient information, using a novel reward mechanism to identify minimal perturbations that bypass security measures. Empirical results show that AttackGNN outperforming traditional adversarial methods, compromising GNN-based hardware Trojan detection systems with over 90% effectiveness. The work demonstrates how reinforcement learning can systematically identify vulnerabilities that conventional testing might miss, establishing a new paradigm for hardware security evaluation.</p>
<p>Zhou <em>et al</em>. [<a href="#pone.0329969.ref027" class="usa-link" aria-describedby="pone.0329969.ref027">27</a>] introduced an adaptive federated few-shot learning framework with prototype rectification, demonstrating how semantic prototypes can be aligned across decentralized clients to improve generalization. Though focused on few-shot settings, their prototype correction mechanism offers valuable insights for federated zero-shot adaptation, particularly in scenarios where semantic drift and heterogeneous data distributions hinder model transfer.</p>
<p>Zhao <em>et al</em>. [<a href="#pone.0329969.ref028" class="usa-link" aria-describedby="pone.0329969.ref028">28</a>] introduce a semi-supervised federated intrusion detection scheme that applies knowledge distillation and voting to cope with non-IID data and communication constraints. Similarly, the FedGKD approach [<a href="#pone.0329969.ref029" class="usa-link" aria-describedby="pone.0329969.ref029">29</a>] uses global knowledge distillation to address heterogeneity across edge clients in IoT settings. These show how KD can enhance privacy-preserving and collaborative model learning, and our work extends these ideas by integrating zero or few-shot generalization within a hierarchical MARL framework</p>
<p>More recently, Wang <em>et al</em>.[<a href="#pone.0329969.ref030" class="usa-link" aria-describedby="pone.0329969.ref030">30</a>] presented a federated zero-shot learning framework that uses an LLM to generate privacy-conscious semantic embeddings for unknown attack types. These embeddings are then collaboratively shared between clients, enabling zero-day attack detection without exposing raw data. Their approach directly parallels our semantic mapping module and further supports our hierarchical MARL design by validating the feasibility of federated semantic transfer for unseen threats.</p>
<p>Recent developments have further advanced hierarchical MARL and federated learning specifically for cybersecurity applications. For example, Zhang <em>et al</em>. [<a href="#pone.0329969.ref031" class="usa-link" aria-describedby="pone.0329969.ref031">31</a>] introduced a federated MARL framework designed explicitly for intrusion detection in distributed IoT environments, demonstrating significant performance gains and robust privacy preservation. Similarly, recent work by Liu <em>et al</em>. [<a href="#pone.0329969.ref032" class="usa-link" aria-describedby="pone.0329969.ref032">32</a>], proposed hierarchical MARL architectures leveraging transformer-based multi-agent coordination, achieving improved scalability and resilience against adaptive cyber threats. Such state-of-the-art approaches underline the growing importance of federated and hierarchical MARL methodologies in cybersecurity research.</p>
<p>In the context of cybersecurity, federated learning offers a promising approach for collaborative defense across organizational boundaries while preserving the confidentiality of sensitive security data. However, existing approaches have not fully integrated federated learning with hierarchical MARL and zero-shot capabilities to achieve comprehensive cyber defense.</p>
<p>Therefore, our work builds upon these foundations to create a unified framework that addresses the limitations of existing approaches through the integration of hierarchical MARL, zero-shot learning, and federated knowledge sharing.</p></section></section><section id="sec007"><h2 class="pmc_sec_title">3 System model and problem formulation</h2>
<section id="sec008"><h3 class="pmc_sec_title">3.1 Motivation scenario</h3>
<p>Modern cyber defense systems face increasingly sophisticated threats that evolve rapidly and exploit vulnerabilities across complex network environments. Consider a typical enterprise network consist of multiple segments, each containing various devices and services with distinct security requirements. Traditional defense approaches often rely on centralized monitoring and static rule-based detection, which face several critical limitations:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Delayed Response to Novel Threats</strong>: When a previously unseen attack pattern emerges, traditional systems require manual analysis and rule updates, creating a vulnerability window during which networks remain exposed.</p></li>
<li><p><strong>Limited Contextual Understanding</strong>: Isolated security components lack the comprehensive contextual awareness needed to detect sophisticated attacks that target multiple network elements simultaneously.</p></li>
<li><p><strong>Privacy Constraints in Collaborative Defense</strong>: Organizations are often reluctant to share security data due to privacy concerns and regulatory requirements, limiting the potential for collaborative defense.</p></li>
<li><p><strong>Scalability Challenges</strong>: As networks grow in size and complexity, centralized approaches struggle to process the increasing volume of security data in real-time. It is known that organizations often face challenges in managing large volumes of security data, including firewall logs, endpoint detection and response (EDR) data, and network flows.</p></li>
</ol>
<p>To illustrate these challenges, consider an advanced persistent threat (APT) scenario where attackers employ a multi-stage approach initial reconnaissance to identify vulnerable systems, exploitation of known or zero-day vulnerabilities to establish foothold, lateral movement to access high-value assets, and data exfiltration through covert channels. A traditional defense system might detect individual components of this attack chain but would likely fail to recognize the overall pattern, especially if the attack employs previously unseen techniques as part of its TTPs.</p>
<p>Our proposed Adaptive Zero-Shot Hierarchical MARL framework addresses these challenges by enabling:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Immediate Response to Novel Threats</strong>: Through zero-shot learning capabilities that recognize semantic similarities between known and unknown attack patterns.</p></li>
<li><p><strong>Contextual Awareness</strong>: Through hierarchical coordination of specialized agents that collectively monitor and defend different aspects of the network.</p></li>
<li><p><strong>Privacy-Preserving Collaboration</strong>: Through federated learning that enables knowledge sharing without exposing sensitive data.</p></li>
<li><p><strong>Scalable Defense</strong>: By decomposing complex defense tasks into manageable sub-tasks distributed across multiple agents.</p></li>
</ol></section><section id="sec009"><h3 class="pmc_sec_title">3.2 Hierarchical MARL framework</h3>
<p>I formulate the cyber defense problem as a hierarchical multi-agent Markov Decision Process (MDP) with two levels: a meta-level that coordinates overall defense strategy and a sub-level comprising specialized agents responsible for specific defense tasks.</p>
<section id="sec010"><h4 class="pmc_sec_title">3.2.1 Meta-level MDP.</h4>
<p>The meta-level MDP is defined as a tuple <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e001"><math id="M1" display="inline" overflow="linebreak"><mrow><msub><mi>M</mi><mn>0</mn></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo>,</mo><msub><mi>A</mi><mn>0</mn></msub><mo>,</mo><msub><mi>P</mi><mn>0</mn></msub><mo>,</mo><msub><mi>R</mi><mn>0</mn></msub><mo>,</mo><msub><mi>γ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></math></span>, where:</p>
<ul class="list" style="list-style-type:disc">
<li><p><em>S</em><sub>0</sub> represents the high-level state space capturing the overall security status of the network. Each state <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e002"><math id="M2" display="inline" overflow="linebreak"><mrow><msub><mi>s</mi><mn>0</mn></msub><mo>∈</mo><msub><mi>S</mi><mn>0</mn></msub></mrow></math></span> includes aggregate security metrics, threat assessments, and resource availability.</p></li>
<li><p><em>A</em><sub>0</sub> is the set of available sub-policies that the meta-controller can activate. Each action <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e003"><math id="M3" display="inline" overflow="linebreak"><mrow><msub><mi>a</mi><mn>0</mn></msub><mo>∈</mo><msub><mi>A</mi><mn>0</mn></msub></mrow></math></span> corresponds to selecting and parameterizing a specific sub-policy for execution.</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e004"><math id="M4" display="inline" overflow="linebreak"><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>:</mo><msub><mi>S</mi><mn>0</mn></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>A</mi><mn>0</mn></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>S</mi><mn>0</mn></msub><mo>→</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></math></span> is the transition function that defines the probability of transitioning from state <em>s</em><sub>0</sub> to state <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e005"><math id="M5" display="inline" overflow="linebreak"><mrow><msubsup><mi>s</mi><mn>0</mn><mi>′</mi></msubsup></mrow></math></span> after taking action <em>a</em><sub>0</sub>.</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e006"><math id="M6" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mn>0</mn></msub><mo>:</mo><msub><mi>S</mi><mn>0</mn></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>A</mi><mn>0</mn></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>S</mi><mn>0</mn></msub><mo>→</mo><mi>ℝ</mi></mrow></math></span> is the reward function that evaluates the effectiveness of high-level decisions based on overall security objectives.</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e007"><math id="M7" display="inline" overflow="linebreak"><mrow><msub><mi>γ</mi><mn>0</mn></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></span> is the discount factor for future rewards at the meta-level.</p></li>
</ul>
<p>The meta-controller’s objective is to learn a policy <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e008"><math id="M8" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mn>0</mn></msub><mo>:</mo><msub><mi>S</mi><mn>0</mn></msub><mo>→</mo><msub><mi>A</mi><mn>0</mn></msub></mrow></math></span> that maximizes the expected cumulative discounted reward:</p>
<table class="disp-formula p" id="pone.0329969.e009"><tr>
<td class="formula"><math id="M9" display="block" overflow="linebreak"><mrow><mrow><msup><mi>V</mi><mrow><msub><mi>π</mi><mn>0</mn></msub></mrow></msup><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>𝔼</mi><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mo>∞</mo></mrow></msubsup><msubsup><mi>γ</mi><mn>0</mn><mi>t</mi></msubsup><msub><mi>R</mi><mn>0</mn></msub><mo stretchy="false">(</mo><msubsup><mi>s</mi><mn>0</mn><mi>t</mi></msubsup><mo>,</mo><msub><mi>π</mi><mn>0</mn></msub><mo stretchy="false">(</mo><msubsup><mi>s</mi><mn>0</mn><mi>t</mi></msubsup><mo stretchy="false">)</mo><mo>,</mo><msubsup><mi>s</mi><mn>0</mn><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo><mo>∣</mo><msubsup><mi>s</mi><mn>0</mn><mn>0</mn></msubsup><mo>=</mo><msub><mi>s</mi><mn>0</mn></msub><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></mrow></math></td>
<td class="label">(1)</td>
</tr></table></section><section id="sec011"><h4 class="pmc_sec_title">3.2.2 Sub-level MDPs.</h4>
<p>For each sub-task <em>i</em>, I define a corresponding MDP <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e010"><math id="M10" display="inline" overflow="linebreak"><mrow><msub><mi>M</mi><mi>i</mi></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo>,</mo><msub><mi>A</mi><mi>i</mi></msub><mo>,</mo><msub><mi>P</mi><mi>i</mi></msub><mo>,</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></span>, where:</p>
<ul class="list" style="list-style-type:disc">
<li><p><em>S</em><sub><em>i</em></sub> is the state space relevant to sub-task <em>i</em>, containing detailed information specific to that task.</p></li>
<li><p><em>A</em><sub><em>i</em></sub> is the action space for sub-task <em>i</em>, comprising the primitive actions available to the corresponding agent.</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e011"><math id="M11" display="inline" overflow="linebreak"><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>:</mo><msub><mi>S</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>A</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>S</mi><mi>i</mi></msub><mo>→</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></math></span> is the transition function for sub-task <em>i</em>.</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e012"><math id="M12" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>:</mo><msub><mi>S</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>A</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>S</mi><mi>i</mi></msub><mo>→</mo><mi>ℝ</mi></mrow></math></span> is the reward function specific to the objectives of sub-task <em>i</em>.</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e013"><math id="M13" display="inline" overflow="linebreak"><mrow><msub><mi>γ</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></span> is the discount factor for future rewards in sub-task <em>i</em>.</p></li>
</ul>
<p>Each sub-level agent aims to learn a policy <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e014"><math id="M14" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mi>i</mi></msub><mo>:</mo><msub><mi>S</mi><mi>i</mi></msub><mo>→</mo><msub><mi>A</mi><mi>i</mi></msub></mrow></math></span> that maximizes its expected cumulative discounted reward within the context of its specific sub-task.</p>
<p>I define four primary sub-tasks in our framework:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Reconnaissance</strong>: Monitoring network traffic and system logs to detect potential threats.</p></li>
<li><p><strong>Analysis</strong>: Evaluating detected anomalies to determine their nature and severity.</p></li>
<li><p><strong>Response</strong>: Executing defensive actions to mitigate confirmed threats.</p></li>
<li><p><strong>Recovery</strong>: Restoring affected systems to normal operation after an attack.</p></li>
</ol></section><section id="sec012"><h4 class="pmc_sec_title">3.2.3 Hierarchical policy learning.</h4>
<p>To learn effective policies at both levels, I employ a hierarchical reinforcement learning approach based on the options framework [<a href="#pone.0329969.ref017" class="usa-link" aria-describedby="pone.0329969.ref017">17</a>]. The meta-controller learns to select appropriate sub-policies based on the current high-level state, while each sub-level agent learns to execute its specific task effectively.</p>
<p>For the meta-controller, I use Proximal Policy Optimization (PPO) [<a href="#pone.0329969.ref033" class="usa-link" aria-describedby="pone.0329969.ref033">33</a>] to learn a policy that maximizes the expected return:</p>
<table class="disp-formula p" id="pone.0329969.e015"><tr>
<td class="formula"><math id="M15" display="block" overflow="linebreak"><mrow><mrow><msubsup><mi>π</mi><mn>0</mn><mrow><mi>*</mi></mrow></msubsup><mo>=</mo><mi mathvariant="normal">arg</mi><msub><mo>max</mo><mrow><msub><mi>π</mi><mn>0</mn></msub></mrow></msub><msub><mi>𝔼</mi><mrow><msub><mi>s</mi><mn>0</mn></msub><mi>~</mi><msub><mi>ρ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mi>~</mi><msub><mi>π</mi><mn>0</mn></msub></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mi>V</mi><mrow><msub><mi>π</mi><mn>0</mn></msub></mrow></msup><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></mrow></math></td>
<td class="label">(2)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e016"><math id="M16" display="inline" overflow="linebreak"><mrow><msub><mi>ρ</mi><mn>0</mn></msub></mrow></math></span> is the distribution of initial states.</p>
<p>For each sub-level agent, I also employ PPO to learn task-specific policies:</p>
<table class="disp-formula p" id="pone.0329969.e017"><tr>
<td class="formula"><math id="M17" display="block" overflow="linebreak"><mrow><mrow><msubsup><mi>π</mi><mi>i</mi><mrow><mi>*</mi></mrow></msubsup><mo>=</mo><mi mathvariant="normal">arg</mi><msub><mo>max</mo><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow></msub><msub><mi>𝔼</mi><mrow><msub><mi>s</mi><mi>i</mi></msub><mi>~</mi><msub><mi>ρ</mi><mi>i</mi></msub><mo>,</mo><msub><mi>a</mi><mi>i</mi></msub><mi>~</mi><msub><mi>π</mi><mi>i</mi></msub></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mi>V</mi><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow></msup><mo stretchy="false">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></mrow></math></td>
<td class="label">(3)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e018"><math id="M18" display="inline" overflow="linebreak"><mrow><msub><mi>ρ</mi><mi>i</mi></msub></mrow></math></span> is the distribution of states encountered during the execution of sub-task <em>i</em>.</p>
<p>The hierarchical structure enables efficient learning by reducing the complexity of the overall task and allowing specialized agents to focus on specific aspects of defense.</p></section></section><section id="sec013"><h3 class="pmc_sec_title">3.3 Zero-shot learning integration</h3>
<p>To enable adaptation to previously unseen attack patterns, I integrate zero-shot learning capabilities into our framework through a semantic mapping approach.</p>
<section id="sec014"><h4 class="pmc_sec_title">3.3.1 Semantic attribute space.</h4>
<p>I define a semantic attribute space <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e019"><math id="M19" display="inline" overflow="linebreak"><mrow><mi>𝒮</mi></mrow></math></span> that captures the fundamental characteristics of cyber attacks, including:</p>
<ul class="list" style="list-style-type:disc">
<li><p>Attack vectors (e.g., phishing, exploitation, lateral movement)</p></li>
<li><p>Target resources (e.g., databases, authentication servers, endpoints)</p></li>
<li><p>Behavioral patterns (e.g., data exfiltration, privilege escalation, denial of service)</p></li>
<li><p>Temporal characteristics (e.g., persistence, frequency, duration)</p></li>
</ul>
<p>Each attack pattern, whether known or novel, can be represented as a point in this semantic space based on its observable characteristics.</p></section><section id="sec015"><h4 class="pmc_sec_title">3.3.2 Semantic mapping function.</h4>
<p>I define a mapping function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e020"><math id="M20" display="inline" overflow="linebreak"><mrow><mi>ϕ</mi><mo>:</mo><mi>X</mi><mo>→</mo><mi>𝒮</mi></mrow></math></span> that projects observed attack features <em>X</em> into the semantic attribute space <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e021"><math id="M21" display="inline" overflow="linebreak"><mrow><mi>𝒮</mi></mrow></math></span>. This function is learned from a diverse dataset of known attacks and their corresponding semantic attributes.</p>
<p>For a novel attack pattern <em>x</em><sub><em>new</em></sub> with unknown class, the system predicts the appropriate response by:</p>
<table class="disp-formula p" id="pone.0329969.e022"><tr>
<td class="formula"><math id="M22" display="block" overflow="linebreak"><mrow><mrow><msub><mi>y</mi><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi mathvariant="normal">arg</mi><msub><mo>max</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></msub><mtext>sim</mtext><mo stretchy="false">(</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo stretchy="false">)</mo><mo>,</mo><mi>ψ</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(4)</td>
</tr></table>
<p>where:</p>
<ul class="list" style="list-style-type:disc">
<li><p><em>Y</em> is the set of possible response strategies</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e023"><math id="M23" display="inline" overflow="linebreak"><mrow><mi>ψ</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></math></span> maps response strategies to the semantic space</p></li>
<li><div class="p">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e024"><math id="M24" display="inline" overflow="linebreak"><mrow><mtext>sim</mtext></mrow></math></span> is a similarity function in the semantic space, typically implemented as cosine similarity:<table class="disp-formula p" id="pone.0329969.e025"><tr>
<td class="formula"><math id="M25" display="block" overflow="linebreak"><mrow><mrow><mtext>sim</mtext><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>a</mi><mo>·</mo><mi>b</mi></mrow><mrow><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>a</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mo>·</mo><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>b</mi><mo stretchy="false">|</mo><mo stretchy="false">|</mo></mrow></mfrac></mrow></mrow></math></td>
<td class="label">(5)</td>
</tr></table>
</div></li>
</ul></section><section id="sec016"><h4 class="pmc_sec_title">3.3.3 Zero-shot policy transfer.</h4>
<p>To enable zero-shot transfer of defense policies to novel attacks, I learn a conditional policy function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e026"><math id="M26" display="inline" overflow="linebreak"><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">|</mo><mi>s</mi><mo>,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></math></span> that takes as input not only the current state <em>s</em> but also a semantic embedding <em>z</em> of the attack being defended against.</p>
<p>During training, the agent learns to associate effective defense strategies with specific regions of the semantic space. When a novel attack is detected, its semantic embedding is computed, and the conditional policy generates an appropriate response based on the similarity to known attacks.</p>
<p>The zero-shot policy transfer is formulated as:</p>
<table class="disp-formula p" id="pone.0329969.e027"><tr>
<td class="formula"><math id="M27" display="block" overflow="linebreak"><mrow><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">|</mo><mi>s</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mrow><msub><mi>z</mi><mi>k</mi></msub><mo>∈</mo><msub><mi>Z</mi><mrow><mi>k</mi><mi>n</mi><mi>o</mi><mi>w</mi><mi>n</mi></mrow></msub></mrow></msub><msub><mi>w</mi><mi>k</mi></msub><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">|</mo><mi>s</mi><mo>,</mo><msub><mi>z</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(6)</td>
</tr></table>
<p>where:</p>
<ul class="list" style="list-style-type:disc">
<li><p><em>z</em><sub><em>new</em></sub> is the semantic embedding of the novel attack</p></li>
<li><p><em>Z</em><sub><em>known</em></sub> is the set of semantic embeddings for known attacks</p></li>
<li><p><em>w</em><sub><em>k</em></sub> is the similarity weight between <em>z</em><sub><em>new</em></sub> and <em>z</em><sub><em>k</em></sub>:</p></li>
</ul>
<table class="disp-formula p" id="pone.0329969.e028"><tr>
<td class="formula"><math id="M28" display="block" overflow="linebreak"><mrow><mrow><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><mfrac><mrow><mtext>sim</mtext><mo stretchy="false">(</mo><msub><mi>z</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>,</mo><msub><mi>z</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mrow><msub><mo>∑</mo><mrow><msub><mi>z</mi><mi>j</mi></msub><mo>∈</mo><msub><mi>Z</mi><mrow><mi>k</mi><mi>n</mi><mi>o</mi><mi>w</mi><mi>n</mi></mrow></msub></mrow></msub><mtext>sim</mtext><mo stretchy="false">(</mo><msub><mi>z</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>,</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></mrow></math></td>
<td class="label">(7)</td>
</tr></table>
<p>This approach enables the system to generate appropriate responses to previously unseen attack patterns by leveraging knowledge about semantically similar known attacks.</p></section></section><section id="sec017"><h3 class="pmc_sec_title">3.4 Problem formulation</h3>
<p>Given the components described above, I formulate the resilient cyber defense problem as follows:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Objective</strong>: Maximize the security of the network by minimizing successful attacks while maintaining operational efficiency and privacy.</p></li>
<li><div class="p">
<strong>Constraints</strong>:<ul class="list" style="list-style-type:disc">
<li><div class="p">Limited observability of the network state</div></li>
<li><div class="p">Privacy requirements for sensitive security data</div></li>
<li><div class="p">Resource constraints for defensive actions</div></li>
<li><div class="p">Continuous evolution of attack strategies</div></li>
</ul>
</div></li>
<li><div class="p">
<strong>Optimization Problem</strong>:<table class="disp-formula p" id="pone.0329969.e029"><tr>
<td class="formula"><math id="M29" display="block" overflow="linebreak"><mrow><mrow><msub><mo>max</mo><mrow><msub><mi>π</mi><mn>0</mn></msub><mo>,</mo><msub><mi>π</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>π</mi><mi>n</mi></msub></mrow></msub><mi>𝔼</mi><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi></mrow></msubsup><msup><mi>γ</mi><mi>t</mi></msup><msub><mi>R</mi><mi>t</mi></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></mrow></math></td>
<td class="label">(8)</td>
</tr></table>
</div></li>
</ol>
<p>subject to:</p>
<ul class="list" style="list-style-type:disc">
<li><p>Privacy constraints: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e030"><math id="M30" display="inline" overflow="linebreak"><mrow><mi>𝒫</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>≤</mo><mi>ϵ</mi></mrow></math></span>, where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e031"><math id="M31" display="inline" overflow="linebreak"><mrow><mi>𝒫</mi></mrow></math></span> measures privacy leakage and <em>ε</em> is the maximum acceptable leakage</p></li>
<li><p>Resource constraints: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e032"><math id="M32" display="inline" overflow="linebreak"><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><msub><mi>c</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>≤</mo><mi>C</mi></mrow></math></span>, where <em>c</em><sub><em>i</em></sub> is the cost of action <em>a</em><sub><em>i</em></sub> and <em>C</em> is the total available resources</p></li>
<li><p>Performance constraints: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e033"><math id="M33" display="inline" overflow="linebreak"><mrow><mtext>FPR</mtext><mo>≤</mo><mi>α</mi></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e034"><math id="M34" display="inline" overflow="linebreak"><mrow><mtext>FNR</mtext><mo>≤</mo><mi>β</mi></mrow></math></span>, where FPR is the false positive rate, FNR is the false negative rate, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e035"><math id="M35" display="inline" overflow="linebreak"><mrow><mi>α</mi><mo>,</mo><mi>β</mi></mrow></math></span> are the maximum acceptable rates</p></li>
</ul>
<p>The reward function <em>R</em><sub><em>t</em></sub> balances multiple objectives:</p>
<table class="disp-formula p" id="pone.0329969.e036"><tr>
<td class="formula"><math id="M36" display="block" overflow="linebreak"><mrow><mrow><mrow><msub><mi>R</mi><mi>t</mi></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow></mrow><msub><mi>w</mi><mn>1</mn></msub><msub><mi>R</mi><mrow><mi>s</mi><mi>e</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mspace linebreak="newline"></mspace><msub><mi>w</mi><mn>2</mn></msub><msub><mi>R</mi><mrow><mi>e</mi><mi>f</mi><mi>f</mi><mi>i</mi><mi>c</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>y</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mi>w</mi><mn>3</mn></msub><msub><mi>R</mi><mrow><mi>n</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>l</mi><mi>t</mi><mi>y</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="label">(9)</td>
</tr></table>
<p>where:</p>
<ul class="list" style="list-style-type:disc">
<li><p><em>R</em><sub><em>security</em></sub> rewards successful threat mitigation</p></li>
<li><p><em>R</em><sub><em>efficiency</em></sub> rewards resource-efficient responses</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e037"><math id="M37" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>n</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>l</mi><mi>t</mi><mi>y</mi></mrow></msub></mrow></math></span> rewards effective responses to previously unseen attacks</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e038"><math id="M38" display="inline" overflow="linebreak"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><msub><mi>w</mi><mn>3</mn></msub></mrow></math></span> are weights that balance these objectives</p></li>
</ul>
<p>In the following section, I describe how I implement this framework using federated learning for privacy-preserving knowledge sharing and adaptive response mechanisms for evolving threats.</p></section></section><section id="sec018"><h2 class="pmc_sec_title">4 Implementation and evaluation</h2>
<section id="sec019"><h3 class="pmc_sec_title">4.1 Federated knowledge sharing model</h3>
<p>To enable privacy-preserving knowledge sharing across network domains, I implement a federated learning framework specifically designed for cybersecurity applications. This framework allows defensive agents to learn collaboratively while maintaining the confidentiality of sensitive network data.</p>
<section id="sec020"><h4 class="pmc_sec_title">4.1.1 Federated learning architecture.</h4>
<p>Our federated learning architecture consists of three main components:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Local Agents</strong>: Deployed across different network domains, each with its own local dataset and model.</p></li>
<li><p><strong>Regional Aggregators</strong>: Responsible for aggregating models from local agents within a specific region or organization.</p></li>
<li><p><strong>Global Coordinator</strong>: Responsible for aggregating regional models and distributing the global model.</p></li>
</ol>
<p>This hierarchical aggregation structure reduces communication overhead and enhances privacy by limiting the exposure of local models.</p></section><section id="sec021"><h4 class="pmc_sec_title">4.1.2 Federated averaging with differential privacy.</h4>
<p>I implement a privacy-enhanced version of the Federated Averaging (FedAvg) algorithm [<a href="#pone.0329969.ref024" class="usa-link" aria-describedby="pone.0329969.ref024">24</a>] to aggregate model parameters across domains. The standard FedAvg algorithm computes the global model parameters as a weighted average of local model parameters:</p>
<table class="disp-formula p" id="pone.0329969.e039"><tr>
<td class="formula"><math id="M39" display="block" overflow="linebreak"><mrow><mrow><msub><mi>θ</mi><mrow><mi>g</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>K</mi></mrow></msubsup><mfrac><mrow><msub><mi>n</mi><mi>k</mi></msub></mrow><mrow><mi>n</mi></mrow></mfrac><msub><mi>θ</mi><mi>k</mi></msub></mrow></mrow></math></td>
<td class="label">(10)</td>
</tr></table>
<p>where:</p>
<ul class="list" style="list-style-type:disc">
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e040"><math id="M40" display="inline" overflow="linebreak"><mrow><msub><mi>θ</mi><mrow><mi>g</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow></math></span> is the global model parameters</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e041"><math id="M41" display="inline" overflow="linebreak"><mrow><msub><mi>θ</mi><mi>k</mi></msub></mrow></math></span> is the local model parameters from domain <em>k</em></p></li>
<li><p><em>n</em><sub><em>k</em></sub> is the number of data points in domain <em>k</em></p></li>
<li><p><em>n</em> is the total number of data points across all domains</p></li>
</ul>
<p>To enhance privacy, I incorporate differential privacy by adding calibrated noise to the local model parameters before aggregation:</p>
<table class="disp-formula p" id="pone.0329969.e042"><tr>
<td class="formula"><math id="M42" display="block" overflow="linebreak"><mrow><mrow><msubsup><mi>θ</mi><mi>k</mi><mi>′</mi></msubsup><mo>=</mo><msub><mi>θ</mi><mi>k</mi></msub><mo>+</mo><mi>𝒩</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><msup><mi>C</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(11)</td>
</tr></table>
<p>where:</p>
<ul class="list" style="list-style-type:disc">
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e043"><math id="M43" display="inline" overflow="linebreak"><mrow><msubsup><mi>θ</mi><mi>k</mi><mi>′</mi></msubsup></mrow></math></span> is the privacy-preserved model parameters</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e044"><math id="M44" display="inline" overflow="linebreak"><mrow><mi>𝒩</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><msup><mi>C</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></math></span> is Gaussian noise</p></li>
<li><p><em>C</em> is the clipping threshold for gradients</p></li>
<li><p><em>σ</em> is the noise multiplier controlling privacy level</p></li>
</ul>
<p>The privacy guarantee of this approach is quantified using the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e045"><math id="M45" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><mi>ϵ</mi><mo>,</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow></math></span>-differential privacy framework [<a href="#pone.0329969.ref034" class="usa-link" aria-describedby="pone.0329969.ref034">34</a>], where <em>ε</em> represents the privacy budget and <em>δ</em> is the probability of privacy violation. We set these parameters based on the sensitivity of the security data being protected.</p></section><section id="sec022"><h4 class="pmc_sec_title">4.1.3 Secure multi-party computation for model aggregation.</h4>
<p>To further enhance privacy during model aggregation, I implement a secure multi-party computation (SMPC) protocol based on homomorphic encryption [<a href="#pone.0329969.ref035" class="usa-link" aria-describedby="pone.0329969.ref035">35</a>]. This protocol enables the computation of the weighted average of model parameters without revealing the individual parameters to any party.</p>
<p>The SMPC protocol operates as follows:</p>
<ol class="list" style="list-style-type:decimal">
<li><p>Each local agent encrypts its model parameters using a homomorphic encryption scheme: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e046"><math id="M46" display="inline" overflow="linebreak"><mrow><mi>E</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></math></span>.</p></li>
<li><p>The encrypted parameters are sent to the aggregator.</p></li>
<li><p>The aggregator computes the weighted average on the encrypted parameters: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e047"><math id="M47" display="inline" overflow="linebreak"><mrow><mi>E</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mrow><mi>g</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>K</mi></mrow></msubsup><mfrac><mrow><msub><mi>n</mi><mi>k</mi></msub></mrow><mrow><mi>n</mi></mrow></mfrac><mi>E</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></math></span>.</p></li>
<li><p>The aggregator distributes the encrypted global model to all agents.</p></li>
<li><p>Each agent decrypts the global model using its decryption key.</p></li>
</ol>
<p>This approach ensures that raw model parameters are never exposed during the aggregation process, providing strong privacy guarantees.</p></section><section id="sec023"><h4 class="pmc_sec_title">4.1.4 Knowledge distillation for policy transfer.</h4>
<p>To facilitate the transfer of knowledge between different network domains without sharing sensitive data, I implement a federated knowledge distillation approach [<a href="#pone.0329969.ref036" class="usa-link" aria-describedby="pone.0329969.ref036">36</a>]. This approach allows agents to learn from the behavior of other agents without accessing their training data.</p>
<p>The knowledge distillation process operates as follows:</p>
<ol class="list" style="list-style-type:decimal">
<li><p>Each local agent trains its policy on local data.</p></li>
<li><p>The agent generates a set of synthetic states and computes the corresponding action probabilities.</p></li>
<li><p>These state-action probability pairs are shared with other agents.</p></li>
<li><p>Each agent updates its policy to match the aggregated action probabilities on the synthetic states.</p></li>
</ol>
<p>This approach enables knowledge transfer while minimizing the exposure of sensitive information about local network environments.</p></section></section><section id="sec024"><h3 class="pmc_sec_title">4.2 Adaptive response mechanisms</h3>
<p>To enable effective response to evolving threats, I implement adaptive mechanisms that dynamically adjust defense strategies based on observed attack patterns and outcomes.</p>
<section id="sec025"><h4 class="pmc_sec_title">4.2.1 Dynamic reward formulation.</h4>
<p>I implement a dynamic reward function that adapts to the evolving threat landscape:</p>
<table class="disp-formula p" id="pone.0329969.e048"><tr>
<td class="formula"><math id="M48" display="block" overflow="linebreak"><mrow><mrow><mrow><msub><mi>R</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>α</mi><mi>t</mi></msub><msub><mi>R</mi><mrow><mi>s</mi><mi>e</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>β</mi><mi>t</mi></msub><msub><mi>R</mi><mrow><mi>e</mi><mi>f</mi><mi>f</mi><mi>i</mi><mi>c</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>y</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mo>+</mo><msub><mi>γ</mi><mi>t</mi></msub><msub><mi>R</mi><mrow><mi>n</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>l</mi><mi>t</mi><mi>y</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="label">(12)</td>
</tr></table>
<p>where:</p>
<ul class="list" style="list-style-type:disc">
<li><p><em>R</em><sub><em>security</em></sub> rewards successful threat mitigation</p></li>
<li><p><em>R</em><sub><em>efficiency</em></sub> rewards resource-efficient responses</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e049"><math id="M49" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>n</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>l</mi><mi>t</mi><mi>y</mi></mrow></msub></mrow></math></span> rewards effective responses to previously unseen attacks</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e050"><math id="M50" display="inline" overflow="linebreak"><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>,</mo><msub><mi>β</mi><mi>t</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>t</mi></msub></mrow></math></span> are dynamic weights adjusted based on the current threat landscape</p></li>
</ul>
<p>The dynamic weights are updated using a meta-learning approach that optimizes the trade-off between different objectives based on recent performance:</p>
<table class="disp-formula p" id="pone.0329969.e051"><tr>
<td class="formula"><math id="M51" display="block" overflow="linebreak"><mrow><mrow><msub><mi>α</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>α</mi><mi>t</mi></msub><mo>+</mo><mi>η</mi><msub><mo>∇</mo><mrow><mi>α</mi></mrow></msub><mi>J</mi><mo stretchy="false">(</mo><msub><mi>α</mi><mi>t</mi></msub><mo>,</mo><msub><mi>β</mi><mi>t</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(13)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329969.e052"><tr>
<td class="formula"><math id="M52" display="block" overflow="linebreak"><mrow><mrow><msub><mi>β</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>β</mi><mi>t</mi></msub><mo>+</mo><mi>η</mi><msub><mo>∇</mo><mrow><mi>β</mi></mrow></msub><mi>J</mi><mo stretchy="false">(</mo><msub><mi>α</mi><mi>t</mi></msub><mo>,</mo><msub><mi>β</mi><mi>t</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(14)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329969.e053"><tr>
<td class="formula"><math id="M53" display="block" overflow="linebreak"><mrow><mrow><msub><mi>γ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>γ</mi><mi>t</mi></msub><mo>+</mo><mi>η</mi><msub><mo>∇</mo><mrow><mi>γ</mi></mrow></msub><mi>J</mi><mo stretchy="false">(</mo><msub><mi>α</mi><mi>t</mi></msub><mo>,</mo><msub><mi>β</mi><mi>t</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(15)</td>
</tr></table>
<p>where <em>J</em> is a meta-objective that measures overall defense effectiveness and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e054"><math id="M54" display="inline" overflow="linebreak"><mrow><mi>η</mi></mrow></math></span> is the learning rate.</p></section><section id="sec026"><h4 class="pmc_sec_title">4.2.2 Adversarial training for robustness.</h4>
<p>To enhance robustness against adversarial attacks, we implement an adversarial training approach that simulates sophisticated attack strategies. This approach involves training defensive agents against a simulated adversary that continuously adapts its attack strategies to exploit vulnerabilities in the defense system.</p>
<p>The adversarial training process is formulated as a min-max optimization problem:</p>
<table class="disp-formula p" id="pone.0329969.e055"><tr>
<td class="formula"><math id="M55" display="block" overflow="linebreak"><mrow><mrow><msub><mo>min</mo><mrow><msub><mi>θ</mi><mi>D</mi></msub></mrow></msub><msub><mo>max</mo><mrow><msub><mi>θ</mi><mi>A</mi></msub></mrow></msub><msub><mi>𝔼</mi><mrow><mi>s</mi><mi>~</mi><mi>ρ</mi></mrow></msub><mo stretchy="false">[</mo><msub><mi>R</mi><mi>D</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msub><mi>π</mi><mi>D</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mi>;</mi><msub><mi>θ</mi><mi>D</mi></msub><mo stretchy="false">)</mo><mo>,</mo><msub><mi>π</mi><mi>A</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mi>;</mi><msub><mi>θ</mi><mi>A</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow></mrow></math></td>
<td class="label">(16)</td>
</tr></table>
<p>where:</p>
<ul class="list" style="list-style-type:disc">
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e056"><math id="M56" display="inline" overflow="linebreak"><mrow><msub><mi>θ</mi><mi>D</mi></msub></mrow></math></span> are the parameters of the defense policy</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e057"><math id="M57" display="inline" overflow="linebreak"><mrow><msub><mi>θ</mi><mi>A</mi></msub></mrow></math></span> are the parameters of the attack policy</p></li>
<li><p><em>R</em><sub><em>D</em></sub> is the reward function for the defender</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e058"><math id="M58" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mi>D</mi></msub></mrow></math></span> is the defense policy</p></li>
<li><p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e059"><math id="M59" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mi>A</mi></msub></mrow></math></span> is the attack policy</p></li>
<li><p><em>ρ</em> is the state distribution</p></li>
</ul>
<p>This adversarial training approach ensures that defensive agents are prepared for sophisticated and adaptive attack strategies.</p></section><section id="sec027"><h4 class="pmc_sec_title">4.2.3 Continual learning with experience replay.</h4>
<p>To maintain effectiveness against evolving threats while avoiding catastrophic forgetting of previously learned defense strategies, I implement a continual learning approach with experience replay [<a href="#pone.0329969.ref037" class="usa-link" aria-describedby="pone.0329969.ref037">37</a>]. This approach involves maintaining a replay buffer of experiences from different threat scenarios and periodically retraining the model on a balanced sample from this buffer.</p>
<p>The experience replay buffer <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e060"><math id="M60" display="inline" overflow="linebreak"><mrow><mi>ℬ</mi></mrow></math></span> contains tuples <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e061"><math id="M61" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>r</mi><mo>,</mo><msup><mi>s</mi><mi>′</mi></msup><mo stretchy="false">)</mo></mrow></math></span> from various threat scenarios. During training, I sample mini-batches from this buffer to update the policy:</p>
<table class="disp-formula p" id="pone.0329969.e062"><tr>
<td class="formula"><math id="M62" display="block" overflow="linebreak"><mrow><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>α</mi><msub><mo>∇</mo><mrow><mi>θ</mi></mrow></msub><mi>ℒ</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><msub><mi>ℬ</mi><mrow><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(17)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e063"><math id="M63" display="inline" overflow="linebreak"><mrow><mi>ℒ</mi></mrow></math></span> is the loss function and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e064"><math id="M64" display="inline" overflow="linebreak"><mrow><msub><mi>ℬ</mi><mrow><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi></mrow></msub></mrow></math></span> is a sample from the replay buffer.</p>
<p>To ensure balanced representation of different threat scenarios, I implement a prioritized sampling approach that gives higher probability to scenarios that are underrepresented in recent training iterations.</p></section><section id="sec028"><h4 class="pmc_sec_title">4.2.4 Adaptive zero-shot learning algorithm.</h4>
<p>I implement an adaptive zero-shot learning algorithm that continuously refines the semantic mapping function based on new observations. The algorithm is presented in pseudocode below:</p>
<p>
<strong>Algorithm 1. Adaptive zero-shot learning.</strong>
</p>
<p>
<kbd><strong>Require:</strong> Initial semantic mapping function <em>ϕ</em>, known attack set</kbd>
</p>
<p>
<kbd>  <em>Z</em><sub><em>known</em></sub>, policy function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e065"><math id="M65" display="inline" overflow="linebreak"><mrow><mi>π</mi></mrow></math></span></kbd>
</p>
<p>
<kbd><strong>Ensure:</strong> Updated semantic mapping function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e066"><math id="M66" display="inline" overflow="linebreak"><mrow><msup><mi>ϕ</mi><mi>′</mi></msup></mrow></math></span>, updated policy</kbd>
</p>
<p>
<kbd>  function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e067"><math id="M67" display="inline" overflow="linebreak"><mrow><msup><mi>π</mi><mi>′</mi></msup></mrow></math></span></kbd>
</p>
<p>
<kbd>1: Initialize replay buffer <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e068"><math id="M68" display="inline" overflow="linebreak"><mrow><mrow><mi>B</mi></mrow></mrow></math></span></kbd>
</p>
<p>
<kbd>2: <strong>while</strong> not converged <strong>do</strong></kbd>
</p>
<p>
<kbd>3:   Observe current state <em>s</em><sub><em>t</em></sub></kbd>
</p>
<p>
<kbd>4:   Detect potential attack pattern <em>x</em><sub><em>t</em></sub></kbd>
</p>
<p>
<kbd>5:   Compute semantic embedding <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e069"><math id="M69" display="inline" overflow="linebreak"><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></math></span></kbd>
</p>
<p>
<kbd>6:   <strong>if</strong> similarity<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e070"><math id="M70" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo>,</mo><msub><mi>Z</mi><mrow><mi>k</mi><mi>n</mi><mi>o</mi><mi>w</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo><mo>&lt;</mo><mi>t</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi></mrow></math></span>
<strong>then</strong></kbd>
</p>
<p>
<kbd>7:    // Novel attack detected</kbd>
</p>
<p>
<kbd>8:    Compute response using zero-shot transfer:</kbd>
</p>
<p>
<kbd>9:    <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e071"><math id="M71" display="inline" overflow="linebreak"><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><mi>π</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></math></span></kbd>
</p>
<p>
<kbd>10:    Execute action <em>a</em><sub><em>t</em></sub> and observe reward <em>r</em><sub><em>t</em></sub> and next state</kbd>
</p>
<p>
<kbd>      <em>s</em><sub><em>t</em> + 1</sub></kbd>
</p>
<p>
<kbd>11:    Store experience <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e072"><math id="M72" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>r</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></math></span> in buffer <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e073"><math id="M73" display="inline" overflow="linebreak"><mrow><mrow><mi>B</mi></mrow></mrow></math></span></kbd>
</p>
<p>
<kbd>12:    Update <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e074"><math id="M74" display="inline" overflow="linebreak"><mrow><msub><mi>Z</mi><mrow><mi>k</mi><mi>n</mi><mi>o</mi><mi>w</mi><mi>n</mi></mrow></msub><mo>=</mo><msub><mi>Z</mi><mrow><mi>k</mi><mi>n</mi><mi>o</mi><mi>w</mi><mi>n</mi></mrow></msub><mo>∪</mo><mo stretchy="false">{</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy="false">}</mo></mrow></math></span></kbd>
</p>
<p>
<kbd>13:   <strong>else</strong></kbd>
</p>
<p>
<kbd>14:    // Known attack detected</kbd>
</p>
<p>
<kbd>15:    <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e075"><math id="M75" display="inline" overflow="linebreak"><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><mi>π</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></math></span></kbd>
</p>
<p>
<kbd>16:    Execute action <em>a</em><sub><em>t</em></sub> and observe reward <em>r</em><sub><em>t</em></sub> and next state</kbd>
</p>
<p>
<kbd>      <em>s</em><sub><em>t</em> + 1</sub></kbd>
</p>
<p>
<kbd>17:    Store experience <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e076"><math id="M76" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>r</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></math></span> in buffer <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e077"><math id="M77" display="inline" overflow="linebreak"><mrow><mrow><mi>B</mi></mrow></mrow></math></span></kbd>
</p>
<p>
<kbd>18:   <strong>end if</strong></kbd>
</p>
<p>
<kbd>19:   // Periodically update semantic mapping and policy</kbd>
</p>
<p>
<kbd>20:   <strong>if</strong> update_interval reached <strong>then</strong></kbd>
</p>
<p>
<kbd>21:    Sample batch from buffer <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e078"><math id="M78" display="inline" overflow="linebreak"><mrow><mrow><mi>B</mi></mrow></mrow></math></span></kbd>
</p>
<p>
<kbd>22:    Update semantic mapping function <em>ϕ</em> to minimize</kbd>
</p>
<p>
<kbd>  embedding loss</kbd>
</p>
<p>
<kbd>23:    Update policy function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e079"><math id="M79" display="inline" overflow="linebreak"><mrow><mi>π</mi></mrow></math></span> to maximize expected reward</kbd>
</p>
<p>
<kbd>24:   <strong>end if</strong></kbd>
</p>
<p>
<kbd>25: <strong>end while</strong></kbd>
</p>
<p>
<kbd>26: <strong>return</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e080"><math id="M80" display="inline" overflow="linebreak"><mrow><mi>ϕ</mi><mo>,</mo><mi>π</mi><mo>=</mo><mn>0</mn></mrow></math></span></kbd>
</p>
<p>This algorithm enables continuous adaptation to evolving threats by updating both the semantic mapping and the policy functions based on observed experience. Algorithm 1 is executed by each local defender agent during online inference. <a href="#pone.0329969.t001" class="usa-link">Table 1</a>, shows the mapping between textual description and Algorithm 1 components. The key components involved are as follows:</p>
<section class="tw xbox font-sm" id="pone.0329969.t001"><h5 class="obj_head">Table 1. Mapping between textual description and Algorithm 1 components.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Component (Description)</th>
<th align="left" rowspan="1" colspan="1">Corresponding Algorithm Element</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Semantic Mapper (<em>ϕ</em>)</td>
<td align="left" rowspan="1" colspan="1">Line 5: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e081"><math id="M81" display="inline" overflow="linebreak"><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></math></span> computes the semantic embedding of the observed attack pattern.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Policy Matching Module</td>
<td align="left" rowspan="1" colspan="1">Line 6: <kbd>if similarity(<em>z</em><sub><em>t</em></sub>, <em>Z</em><sub><em>known</em></sub>) &lt; threshold</kbd> determines if the attack is novel or known.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Knowledge Base</td>
<td align="left" rowspan="1" colspan="1">Line 6 and Line 12: <em>Z</em><sub><em>known</em></sub> stores and updates the set of known semantic embeddings.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Adaptation Module</td>
<td align="left" rowspan="1" colspan="1">Lines 7–13: Handles novel attack detection and executes policy using zero-shot transfer (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e082"><math id="M82" display="inline" overflow="linebreak"><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></math></span>).</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Online Execution at Edge Agents</td>
<td align="left" rowspan="1" colspan="1">Line 2: <kbd>while not converged do</kbd> indicates continuous inference at local agents.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Learning and Policy Updates</td>
<td align="left" rowspan="1" colspan="1">Lines 20–24: Updates the semantic mapping function and policy using batch sampling from buffer <em>B</em>.</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329969.t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><ol class="list" style="list-style-type:decimal">
<li><p><strong>Semantic Mapper <em>ϕ</em></strong>: Projects observed features into the learned semantic space.</p></li>
<li><p><strong>Policy Matching Module</strong>: Compares the mapped vector to existing known responses using similarity functions.</p></li>
<li><p><strong>Knowledge Base</strong>: A local copy or federated view of known defense-action mappings.</p></li>
<li><p><strong>Adaptation Module</strong>: If no match is above threshold, the agent uses fallback exploration or queries the federation for new mappings.</p></li>
</ol>
<p>The algorithm runs continuously on agents deployed in edge hosts or gateways, depending on the testbed scenario. The logic is integrated as part of the agent’s decision cycle.</p></section></section><section id="sec029"><h3 class="pmc_sec_title">4.3 Implementation</h3>
<p>I implemented our Adaptive Zero-Shot Hierarchical MARL framework using Python 3.8 with PyTorch 1.9 for deep learning components and Ray RLlib [<a href="#pone.0329969.ref038" class="usa-link" aria-describedby="pone.0329969.ref038">38</a>] for distributed reinforcement learning. The implementation consists of several key components:</p>
<section id="sec030"><h4 class="pmc_sec_title">4.3.1 System architecture.</h4>
<p><a href="#pone.0329969.g001" class="usa-link">Fig 1</a> illustrates the overall architecture of our implementation, showing the hierarchical structure of agents, the federated learning components, and the zero-shot adaptation module.</p>
<figure class="fig xbox font-sm" id="pone.0329969.g001"><h5 class="obj_head">Fig 1. System architecture of the Adaptive Zero-Shot Hierarchical MARL framework with Federated Knowledge Sharing.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334051_pone.0329969.g001.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0c04/12334051/9964c7d35adb/pone.0329969.g001.jpg" loading="lazy" height="368" width="740" alt="Fig 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329969.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>The system is deployed in a simulated network environment consisting of multiple segments, each containing various devices and services. Each network segment is protected by a team of specialized agents coordinated by a local meta-controller. The meta-controllers from different segments participate in federated learning to share knowledge while preserving privacy.</p></section><section id="sec031"><h4 class="pmc_sec_title">4.3.2 Neural network architecture.</h4>
<p>For the hierarchical policy learning, I implement neural network architectures for both the meta-controller and the specialized agents:</p>
<ul class="list" style="list-style-type:disc">
<li><p><strong>Meta-Controller</strong>: A transformer-based architecture [<a href="#pone.0329969.ref039" class="usa-link" aria-describedby="pone.0329969.ref039">39</a>] that processes aggregate security metrics and outputs sub-policy selections. The network consists of a multi-head attention mechanism followed by feed-forward layers with ReLU activations.</p></li>
<li><p><strong>Reconnaissance Agent</strong>: A convolutional neural network (CNN) that processes network traffic patterns and system logs to detect potential threats.</p></li>
<li><p><strong>Analysis Agent</strong>: A graph neural network (GNN) that analyzes the relationships between detected anomalies to identify coordinated attack patterns.</p></li>
<li><p><strong>Response Agent</strong>: A deep Q-network (DQN) that selects appropriate defensive actions based on the current threat assessment.</p></li>
<li><p><strong>Recovery Agent</strong>: A policy gradient network that determines the optimal sequence of recovery actions to restore affected systems.</p></li>
</ul></section><section id="sec032"><h4 class="pmc_sec_title">4.3.3 Zero-shot learning component.</h4>
<p>For the semantic mapping function, I implement a Siamese network [<a href="#pone.0329969.ref040" class="usa-link" aria-describedby="pone.0329969.ref040">40</a>] that projects observed attack features into a semantic embedding space. The network is trained using a contrastive loss function that minimizes the distance between semantically similar attacks and maximizes the distance between dissimilar attacks.</p>
<p>The zero-shot policy transfer is implemented using a conditional policy network that takes as input both the current state and the semantic embedding of the detected attack. This network is trained to associate effective defense strategies with specific regions of the semantic space.</p></section><section id="sec033"><h4 class="pmc_sec_title">4.3.4 Federated learning implementation.</h4>
<p>I implement the federated learning component using PySyft [<a href="#pone.0329969.ref041" class="usa-link" aria-describedby="pone.0329969.ref041">41</a>], a library for privacy-preserving machine learning. The implementation includes:</p>
<ul class="list" style="list-style-type:disc">
<li><p>Secure aggregation protocols based on homomorphic encryption</p></li>
<li><p>Differential privacy mechanisms for parameter sharing</p></li>
<li><p>Hierarchical aggregation structure for efficient communication</p></li>
<li><p>Knowledge distillation for policy transfer</p></li>
</ul></section><section id="sec034"><h4 class="pmc_sec_title">4.3.5 Semantic mapping function implementation.</h4>
<p>The semantic mapping function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e083"><math id="M83" display="inline" overflow="linebreak"><mrow><mi>ϕ</mi><mo>:</mo><mi>X</mi><mo>→</mo><mi>S</mi></mrow></math></span> is implemented using a Siamese neural network architecture. The training dataset consists of pairs of attack instances represented by feature vectors extracted from network logs, packet-level information, and host behaviors. Each pair is labeled as either semantically similar or dissimilar based on domain expert annotations and established attack taxonomies. During training, the Siamese network employs a contrastive loss function, formally defined as:</p>
<table class="disp-formula p" id="pone.0329969.e084"><tr>
<td class="formula"><math id="M84" display="block" overflow="linebreak"><mrow><mrow><mi>ℒ</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>y</mi><mo>·</mo><mi>d</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mo>·</mo><mo>max</mo><mo stretchy="false">(</mo><mi>m</mi><mo>−</mo><mi>d</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>,</mo><mn>0</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>,</mo></mrow></mrow></math></td>
<td class="label">(18)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e085"><math id="M85" display="inline" overflow="linebreak"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub></mrow></math></span> represent the embeddings of paired samples, <em>y</em> is the binary label (1 for similar pairs, 0 for dissimilar), <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e086"><math id="M86" display="inline" overflow="linebreak"><mrow><mi>d</mi><mo stretchy="false">(</mo><mo>·</mo><mo stretchy="false">)</mo></mrow></math></span> denotes the Euclidean distance function, and <em>m</em> is a predefined margin. This formulation ensures that semantically similar attack pairs have embeddings close to each other, while embeddings for dissimilar pairs are encouraged to separate. This approach ensures the semantic attribute space accurately captures critical characteristics of cyberattacks, facilitating effective zero-shot classification and policy transfer.</p></section><section id="sec035"><h4 class="pmc_sec_title">4.3.6 Experimental setup.</h4>
<p>I evaluate our approach using a combination of simulated and emulated network environments:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>CybORG Simulation Environment</strong>: A reinforcement learning environment for cyber security developed by the CAGE Challenge [<a href="#pone.0329969.ref042" class="usa-link" aria-describedby="pone.0329969.ref042">42</a>], which provides realistic network topologies and attack scenarios.</p></li>
<li><p><strong>DARPA CRATE Dataset</strong>: A dataset of cyber attacks and defenses collected from realistic network environments [<a href="#pone.0329969.ref043" class="usa-link" aria-describedby="pone.0329969.ref043">43</a>], which I use to train and evaluate our zero-shot learning components.</p></li>
<li><p><strong>Custom Federated Learning Testbed</strong>: A distributed testbed consisting of multiple virtual network environments, each with its own security policies and attack patterns, which I use to evaluate the federated learning components.</p></li>
</ol>
<p>The experiments are conducted on a cluster of machines, each equipped with an NVIDIA A100 GPU, 128GB RAM, and 32 CPU cores. To specifically evaluate the federated learning and privacy-preserving aspects of AZH-MARL, I developed a custom distributed testbed. This testbed simulated a collaborative defense scenario involving three independent organizational networks. Each virtual network environment within the testbed comprised approximately 30-40 nodes, with unique security policies, varying levels of baseline security posture, and distinct, locally generated attack patterns (e.g., phishing campaigns targeting one network, ransomware in another, or data exfiltration attempts in the third).</p>
<p>This heterogeneity was crucial to test the framework’s ability to share knowledge effectively and adapt defenses in a federated manner while preserving the privacy of each participating entity’s local data. In the custom federated learning testbed, each of the three simulated organizational networks had its own deployment of this hierarchical agent structure which included a Meta-Controller and four sub-level agents: Reconnaissance, Analysis, Response, Recovery. The local models from these agents were then aggregated via the federated learning mechanism. <a href="#pone.0329969.g002" class="usa-link">Fig 2</a> illustrates a network architecture, for this federated learning testbed.</p>
<figure class="fig xbox font-sm" id="pone.0329969.g002"><h5 class="obj_head">Fig 2. Custom federated testbed.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334051_pone.0329969.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0c04/12334051/775550911a8c/pone.0329969.g002.jpg" loading="lazy" height="440" width="660" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329969.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section></section><section id="sec036"><h3 class="pmc_sec_title">4.4 Evaluation</h3>
<p>I evaluate our approach using a comprehensive set of metrics that assess different aspects of cyber defense effectiveness.</p>
<section id="sec037"><h4 class="pmc_sec_title">4.4.1 Evaluation metrics.</h4>
<p>I use the following metrics to evaluate our approach:</p>
<ol class="list" style="list-style-type:decimal">
<li>
<p><strong>Security Metrics</strong>:</p>
<ul class="list" style="list-style-type:disc">
<li><p><strong>Detection Rate (DR)</strong>: The percentage of attacks successfully detected.</p></li>
<li><p><strong>False Positive Rate (FPR)</strong>: The percentage of benign activities incorrectly classified as attacks.</p></li>
<li><p><strong>Mean Time to Detection (MTTD)</strong>: The average time from attack initiation to detection.</p></li>
<li><p><strong>Mean Time to Response (MTTR)</strong>: The average time from detection to defensive action.</p></li>
</ul>
</li>
<li>
<p><strong>Zero-Shot Performance Metrics</strong>:</p>
<ul class="list" style="list-style-type:disc">
<li><p><strong>Zero-Shot Detection Rate (ZSDR)</strong>: The detection rate for previously unseen attack patterns.</p></li>
<li><p><strong>Zero-Shot Response Effectiveness (ZSRE)</strong>: The effectiveness of defensive actions against novel attacks.</p></li>
<li><p><strong>Semantic Mapping Accuracy (SMA)</strong>: The accuracy of the semantic mapping function in classifying attack patterns.</p></li>
</ul>
</li>
<li>
<p><strong>Federated Learning Metrics</strong>:</p>
<ul class="list" style="list-style-type:disc">
<li><p><strong>Communication Overhead (CO)</strong>: The amount of data transferred during federated learning.</p></li>
<li><p><strong>Privacy Leakage (PL)</strong>: The amount of sensitive information that can be inferred from shared models.</p></li>
<li><p><strong>Convergence Time (CT)</strong>: The time required for the federated learning process to converge.</p></li>
</ul>
</li>
<li>
<p><strong>Resource Efficiency Metrics</strong>:</p>
<ul class="list" style="list-style-type:disc">
<li><p><strong>Computational Overhead (CPO)</strong>: The computational resources required for real-time operation.</p></li>
<li><p><strong>Memory Usage (MU)</strong>: The memory required for model storage and execution.</p></li>
<li><p><strong>Energy Consumption (EC)</strong>: The energy required for continuous operation.</p></li>
</ul>
</li>
</ol></section><section id="sec038"><h4 class="pmc_sec_title">4.4.2 Baseline approaches.</h4>
<p>I compare our approach with several baseline approaches:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Traditional MARL</strong>: A flat multi-agent reinforcement learning model without hierarchy or transfer mechanisms [<a href="#pone.0329969.ref002" class="usa-link" aria-describedby="pone.0329969.ref002">2</a>,<a href="#pone.0329969.ref009" class="usa-link" aria-describedby="pone.0329969.ref009">9</a>];</p></li>
<li><p><strong>Hierarchical MARL</strong>: A structured MARL approach that incorporates hierarchical control but lacks generalization capabilities [<a href="#pone.0329969.ref015" class="usa-link" aria-describedby="pone.0329969.ref015">15</a>–<a href="#pone.0329969.ref018" class="usa-link" aria-describedby="pone.0329969.ref018">18</a>];</p></li>
<li><p><strong>Federated Learning</strong>: A collaborative training method where agents share models without centralizing raw data [<a href="#pone.0329969.ref024" class="usa-link" aria-describedby="pone.0329969.ref024">24</a>,<a href="#pone.0329969.ref025" class="usa-link" aria-describedby="pone.0329969.ref025">25</a>,<a href="#pone.0329969.ref031" class="usa-link" aria-describedby="pone.0329969.ref031">31</a>,<a href="#pone.0329969.ref032" class="usa-link" aria-describedby="pone.0329969.ref032">32</a>];</p></li>
<li><p><strong>Zero-Shot Learning</strong>: A generalization-based model trained to respond to unseen attack types without prior specific training [<a href="#pone.0329969.ref019" class="usa-link" aria-describedby="pone.0329969.ref019">19</a>–<a href="#pone.0329969.ref023" class="usa-link" aria-describedby="pone.0329969.ref023">23</a>].</p></li>
</ol></section><section id="sec039"><h4 class="pmc_sec_title">4.4.3 Results and analysis.</h4></section><section id="sec040"><h4 class="pmc_sec_title">Zero-day exploits:</h4>
<p>In our experiments, zero-day exploits refer to attacks leveraging previously unknown vulnerabilities for which no explicit signature or patch was available. These were simulated across CybORG, the DARPA dataset, and our custom federated testbed by withholding specific attack variants from training. This forced agents—especially those using our AZH-MARL framework with zero-shot learning—to generalize from known behaviors and respond adaptively to novel threats, such as obfuscated malware or unseen TTPs.</p></section><section id="sec041"><h4 class="pmc_sec_title">Advanced Persistent Threats (APTs):</h4>
<p>APTs were modeled as multi-stage, stealthy campaigns with persistent access goals. Using CybORG and selected DARPA scenarios, I scripted chained attack phases (e.g., phishing, lateral movement, data exfiltration), testing agents’ ability to detect the campaign across time. In our federated testbed, APTs unfolded gradually within different nodes, evaluating the meta-controller’s capacity to recognize strategic threats and orchestrate coordinated responses beyond isolated alerts.</p>
<p><a href="#pone.0329969.g003" class="usa-link">Fig 3</a> shows the performance of our proposed AZH-MARL approach compared to the baselines across different attack scenarios, including known attacks, zero-day exploits, and advanced persistent threats (APTs).</p>
<figure class="fig xbox font-sm" id="pone.0329969.g003"><h5 class="obj_head">Fig 3. Performance comparison of different approaches across attack types.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334051_pone.0329969.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0c04/12334051/f6c8887b3928/pone.0329969.g003.jpg" loading="lazy" height="319" width="653" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329969.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>While both federated learning and zero-shot learning independently offer strong performance in terms of generalization and decentralized training, they each have limitations when applied in isolation. On its own federated learning suffers from difficulty in adapting to unseen attack patterns, particularly those not present in the training data across agents. Conversely Zero-shot learning, while effective at detecting novel attacks, often lacks context-aware response strategies and is susceptible to noisy or ambiguous semantic mappings when not anchored to local experience. The proposed AZH-MARL framework builds on the strengths of both techniques. It uses federated knowledge sharing to continuously refine distributed policies and semantic mappings, while leveraging a hierarchical multi-agent architecture to locally adapt to emergent threats. This tight coupling of ZSL within a federated MARL framework enables faster response times and greater detection robustness, especially in the presence of adversarial behavior or concept drift. As shown in <a href="#pone.0329969.g003" class="usa-link">Fig 3</a>, AZH-MARL outperforms each technique in isolation by providing adaptive coordination between semantic embedding and policy optimization across distributed agents. While Federated Learning (FL) and Zero-Shot Learning (ZSL) together offer strong generalization, they lack adaptive coordination and localized response intelligence, which AZH-MARL provides. <a href="#pone.0329969.t002" class="usa-link">Table 2</a> shows overview of the capabilities of each approaches against some aspects</p>
<section class="tw xbox font-sm" id="pone.0329969.t002"><h5 class="obj_head">Table 2. Comparison between FL + ZSL and the proposed AZH-MARL framework.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Aspect</th>
<th align="left" rowspan="1" colspan="1">FL + ZSL</th>
<th align="left" rowspan="1" colspan="1">AZH-MARL (Proposed)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Generalization to unseen attacks</td>
<td align="left" rowspan="1" colspan="1">✓ (via ZSL embeddings)</td>
<td align="left" rowspan="1" colspan="1">✓✓ (enhanced by federated semantic mapping)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Decentralized learning</td>
<td align="left" rowspan="1" colspan="1">✓ (via FL)</td>
<td align="left" rowspan="1" colspan="1">✓✓ (plus local MARL updates)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Context-aware defense actions</td>
<td align="left" rowspan="1" colspan="1">✗ (static or heuristic)</td>
<td align="left" rowspan="1" colspan="1">✓ (learned via MARL hierarchy)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Coordination between agents</td>
<td align="left" rowspan="1" colspan="1">✗ (no inter-agent dynamics)</td>
<td align="left" rowspan="1" colspan="1">✓✓ (via meta-controller + sub-agents)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Adaptability to concept drift</td>
<td align="left" rowspan="1" colspan="1">✗ (ZSL is fixed post-training)</td>
<td align="left" rowspan="1" colspan="1">✓ (adaptive online MARL)</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329969.t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec042"><h4 class="pmc_sec_title">Detection performance:</h4>
<p><a href="#pone.0329969.t003" class="usa-link">Table 3</a> presents the detection performance of our approach compared to the baselines. Our approach achieves a detection rate of 94.2% for known attacks and 82.7% for zero-day exploits, significantly outperforming the baselines. The false positive rate is maintained at 3.8%, which is comparable to the best-performing baseline.</p>
<section class="tw xbox font-sm" id="pone.0329969.t003"><h5 class="obj_head">Table 3. Comparison of detection performance metrics across different defense approaches.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Method</th>
<th align="left" rowspan="1" colspan="1">Detection Rate (Known)</th>
<th align="left" rowspan="1" colspan="1">Detection Rate (Zero-Day)</th>
<th align="left" rowspan="1" colspan="1">False Positive Rate</th>
<th align="left" rowspan="1" colspan="1">Mean Time to Detection</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Traditional MARL</td>
<td align="left" rowspan="1" colspan="1">85.60%</td>
<td align="left" rowspan="1" colspan="1">42.30%</td>
<td align="left" rowspan="1" colspan="1">5.20%</td>
<td align="left" rowspan="1" colspan="1">5.7s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Hierarchical MARL</td>
<td align="left" rowspan="1" colspan="1">89.30%</td>
<td align="left" rowspan="1" colspan="1">51.80%</td>
<td align="left" rowspan="1" colspan="1">4.70%</td>
<td align="left" rowspan="1" colspan="1">4.1s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Federated Learning</td>
<td align="left" rowspan="1" colspan="1">91.50%</td>
<td align="left" rowspan="1" colspan="1">58.40%</td>
<td align="left" rowspan="1" colspan="1">7.30%</td>
<td align="left" rowspan="1" colspan="1">3.8s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Zero-Shot Learning</td>
<td align="left" rowspan="1" colspan="1">87.20%</td>
<td align="left" rowspan="1" colspan="1">76.50%</td>
<td align="left" rowspan="1" colspan="1">6.80%</td>
<td align="left" rowspan="1" colspan="1">6.2s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>AZH-MARL (Ours)</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>94.20%</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>82.70%</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>3.80%</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>2.3s</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329969.t003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>The improved detection performance can be attributed to the combination of hierarchical structure, which enables specialized agents to focus on specific aspects of detection, and zero-shot learning, which enables recognition of novel attack patterns based on semantic similarities.</p></section><section id="sec043"><h4 class="pmc_sec_title">Response effectiveness:</h4>
<p><a href="#pone.0329969.g004" class="usa-link">Fig 4</a> illustrates the effectiveness of defensive responses across different attack scenarios. Our approach achieves a mean time to response of 2.3 seconds for known attacks and 3.7 seconds for zero-day exploits, representing improvements of 35% and 42% respectively compared to the best-performing baseline.</p>
<figure class="fig xbox font-sm" id="pone.0329969.g004"><h5 class="obj_head">Fig 4. Zero-shot learning integration for recognizing and responding to novel attack patterns.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334051_pone.0329969.g004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0c04/12334051/d0ee132aa11f/pone.0329969.g004.jpg" loading="lazy" height="493" width="739" alt="Fig 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329969.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>The response effectiveness is particularly notable for advanced persistent threats, where our approach achieves a containment rate of 87.3% compared to 62.1% for the best-performing baseline. This improvement is due to the hierarchical coordination of defensive actions and the adaptive response mechanisms that adjust to evolving threat patterns.</p></section><section id="sec044"><h4 class="pmc_sec_title">Zero-shot performance:</h4>
<p><a href="#pone.0329969.t004" class="usa-link">Table 4</a> presents the zero-shot performance of our approach compared to baselines with zero-shot capabilities. Our approach achieves a zero-shot detection rate of 82.7% and a zero-shot response effectiveness of 76.5%, outperforming all baselines.</p>
<section class="tw xbox font-sm" id="pone.0329969.t004"><h5 class="obj_head">Table 4. Comparison of zero-shot learning performance metrics across different approaches.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Method</th>
<th align="left" rowspan="1" colspan="1">Zero-Shot Detection Rate</th>
<th align="left" rowspan="1" colspan="1">Zero-Shot Response Effectiveness</th>
<th align="left" rowspan="1" colspan="1">Semantic Mapping Accuracy</th>
<th align="left" rowspan="1" colspan="1">Adaptation Time</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Traditional MARL + ZSL</td>
<td align="left" rowspan="1" colspan="1">58.30%</td>
<td align="left" rowspan="1" colspan="1">52.10%</td>
<td align="left" rowspan="1" colspan="1">72.50%</td>
<td align="left" rowspan="1" colspan="1">8.7s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Hierarchical MARL + ZSL</td>
<td align="left" rowspan="1" colspan="1">65.20%</td>
<td align="left" rowspan="1" colspan="1">59.70%</td>
<td align="left" rowspan="1" colspan="1">75.80%</td>
<td align="left" rowspan="1" colspan="1">6.5s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Federated Learning + ZSL</td>
<td align="left" rowspan="1" colspan="1">71.80%</td>
<td align="left" rowspan="1" colspan="1">64.30%</td>
<td align="left" rowspan="1" colspan="1">81.20%</td>
<td align="left" rowspan="1" colspan="1">5.2s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>AZH-MARL (Ours)</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>82.70%</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>76.50%</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>89.30%</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>3.7s</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329969.t004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>The semantic mapping accuracy of our approach is 89.3%, indicating that the semantic mapping function effectively captures the fundamental characteristics of attack patterns and enables accurate classification of novel attacks.</p></section><section id="sec045"><h4 class="pmc_sec_title">Federated learning performance:</h4>
<p><a href="#pone.0329969.g005" class="usa-link">Fig 5</a> shows the convergence of the federated learning process across different privacy settings. Our approach achieves convergence within 15 communication rounds while maintaining strong privacy guarantees (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e087"><math id="M87" display="inline" overflow="linebreak"><mrow><mi>ϵ</mi><mo>=</mo><mn>1.5</mn><mo>,</mo><mi>δ</mi><mo>=</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow></math></span>).</p>
<figure class="fig xbox font-sm" id="pone.0329969.g005"><h5 class="obj_head">Fig 5. Hierarchical MARL framework with meta-controller and specialized sub-policies.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334051_pone.0329969.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0c04/12334051/b61ebbacaaf5/pone.0329969.g005.jpg" loading="lazy" height="460" width="738" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329969.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>The communication overhead of our approach is 45% lower than standard federated averaging due to the hierarchical aggregation structure. The privacy leakage, as measured by the success rate of membership inference attacks, is maintained below 5% across all experiments.</p></section><section id="sec046"><h4 class="pmc_sec_title">Ablation study:</h4>
<p>To understand the contribution of each component to the overall performance, I conduct an ablation study by removing individual components from our approach. <a href="#pone.0329969.t005" class="usa-link">Table 5</a> presents the results of this study.</p>
<section class="tw xbox font-sm" id="pone.0329969.t005"><h5 class="obj_head">Table 5. Ablation study showing the contribution of each component to overall performance.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Configuration</th>
<th align="left" rowspan="1" colspan="1">Detection Rate</th>
<th align="left" rowspan="1" colspan="1">Zero-Shot Detection</th>
<th align="left" rowspan="1" colspan="1">Mean Time to Response</th>
<th align="left" rowspan="1" colspan="1">Privacy Leakage</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>Full AZH-MARL</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>94.20%</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>82.70%</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>2.3s</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>&lt;5%</strong>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">w/o Hierarchical Structure</td>
<td align="left" rowspan="1" colspan="1">82.7% (-12.3%)</td>
<td align="left" rowspan="1" colspan="1">71.4% (-13.7%)</td>
<td align="left" rowspan="1" colspan="1">2.8s (+21.7%)</td>
<td align="left" rowspan="1" colspan="1">&lt;5%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">w/o Zero-Shot Learning</td>
<td align="left" rowspan="1" colspan="1">91.5% (-2.9%)</td>
<td align="left" rowspan="1" colspan="1">51.7% (-37.5%)</td>
<td align="left" rowspan="1" colspan="1">2.5s (+8.7%)</td>
<td align="left" rowspan="1" colspan="1">&lt;5%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">w/o Federated Learning</td>
<td align="left" rowspan="1" colspan="1">79.8% (-15.2%)</td>
<td align="left" rowspan="1" colspan="1">74.3% (-10.2%)</td>
<td align="left" rowspan="1" colspan="1">2.7s (+17.4%)</td>
<td align="left" rowspan="1" colspan="1">18.7% (+275%)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">w/o Adaptive Response</td>
<td align="left" rowspan="1" colspan="1">88.3% (-6.3%)</td>
<td align="left" rowspan="1" colspan="1">75.8% (-8.3%)</td>
<td align="left" rowspan="1" colspan="1">3.1s (+34.8%)</td>
<td align="left" rowspan="1" colspan="1">&lt;5%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329969.t005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>The removal of the hierarchical structure results in a 12.3% decrease in detection rate and a 18.7% increase in mean time to response, highlighting the importance of task decomposition for efficient learning and coordination.</p>
<p>The removal of the zero-shot learning component results in a 37.5% decrease in zero-shot detection rate and a 42.1% decrease in zero-shot response effectiveness, confirming the critical role of semantic mapping for adapting to novel threats.</p>
<p>The removal of the federated learning component results in a 15.2% decrease in detection rate for environments with limited local data, emphasizing the value of privacy-preserving knowledge sharing for improving overall defense effectiveness.</p></section><section id="sec047"><h4 class="pmc_sec_title">Resource efficiency:</h4>
<p><a href="#pone.0329969.t006" class="usa-link">Table 6</a> presents the resource efficiency of our approach compared to the baselines. Our approach requires 2.3 GB of memory and 4.5 GFLOPS of computational power for real-time operation, which is higher than rule-based approaches but comparable to other learning-based approaches.</p>
<section class="tw xbox font-sm" id="pone.0329969.t006"><h5 class="obj_head">Table 6. Comparison of resource efficiency metrics across different defense approaches.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Method</th>
<th align="left" rowspan="1" colspan="1">Memory Usage</th>
<th align="left" rowspan="1" colspan="1">Computational Power</th>
<th align="left" rowspan="1" colspan="1">Energy Consumption</th>
<th align="left" rowspan="1" colspan="1">Deployment Complexity</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Traditional MARL</td>
<td align="left" rowspan="1" colspan="1">2.1 GB</td>
<td align="left" rowspan="1" colspan="1">3.8 GFLOPS</td>
<td align="left" rowspan="1" colspan="1">1.0x</td>
<td align="left" rowspan="1" colspan="1">Medium</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Hierarchical MARL</td>
<td align="left" rowspan="1" colspan="1">2.4 GB</td>
<td align="left" rowspan="1" colspan="1">4.2 GFLOPS</td>
<td align="left" rowspan="1" colspan="1">0.92x</td>
<td align="left" rowspan="1" colspan="1">Medium</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Centralized Learning</td>
<td align="left" rowspan="1" colspan="1">3.7 GB</td>
<td align="left" rowspan="1" colspan="1">5.6 GFLOPS</td>
<td align="left" rowspan="1" colspan="1">1.35x</td>
<td align="left" rowspan="1" colspan="1">High</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Rule-Based Defense</td>
<td align="left" rowspan="1" colspan="1">0.8 GB</td>
<td align="left" rowspan="1" colspan="1">1.2 GFLOPS</td>
<td align="left" rowspan="1" colspan="1">0.45x</td>
<td align="left" rowspan="1" colspan="1">Low</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>AZH-MARL (Ours)</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>2.3 GB</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>4.5 GFLOPS</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>0.85x</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>Medium-High</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329969.t006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>The energy consumption of our approach is 15% lower than flat MARL approaches due to the efficient task decomposition and hierarchical decision-making structure.</p></section><section id="sec048"><h4 class="pmc_sec_title">4.4.4 Discussion.</h4>
<p>The evaluation results demonstrate that our Adaptive Zero-Shot Hierarchical MARL approach with Federated Knowledge Sharing significantly outperforms existing approaches across various metrics. The key advantages of our approach include:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Enhanced Adaptability</strong>: The zero-shot learning component enables effective response to previously unseen attack patterns, addressing a critical limitation of traditional approaches.</p></li>
<li><p><strong>Improved Coordination</strong>: The hierarchical structure enables efficient coordination among specialized agents, leading to more effective defense strategies.</p></li>
<li><p><strong>Privacy-Preserving Collaboration</strong>: The federated learning component enables knowledge sharing across organizational boundaries without compromising sensitive data.</p></li>
<li><p><strong>Scalable Defense</strong>: The hierarchical aggregation structure reduces communication overhead and enables scalable deployment across large network environments.</p></li>
</ol>
<p>These advantages make our approach particularly well-suited for defending against sophisticated and evolving cyber threats in complex network environments with privacy constraints.</p>
<p>However, our approach also has limitations that warrant further investigation. First, the computational requirements are higher than those traditional rule-based approaches, which may limit deployment on resource-constrained devices. Second, the effectiveness of zero-shot learning depends on the quality and diversity of the training data used to learn the semantic mapping function.</p>
<p>In the next section, I conclude the paper and discuss future research directions to address these limitations and further enhance the capabilities of our approach.</p></section></section></section><section id="sec049"><h2 class="pmc_sec_title">5 Conclusion</h2>
<p>In this paper, I presented Adaptive Zero-Shot Hierarchical MARL with Federated Knowledge Sharing, a novel framework for resilient cyber defense that addresses critical limitations in existing approaches. The proposed framework integrates hierarchical multi-agent reinforcement learning, zero-shot learning capabilities, privacy-preserving federated knowledge sharing, and adaptive response mechanisms to create a more effective and adaptable defense system.</p>
<p>The hierarchical structure decomposes complex defense tasks into specialized sub-tasks managed by distinct agents, reducing the learning complexity of and enabling more efficient coordination. The zero-shot learning component enables recognition and response to previously unseen attack patterns through semantic mapping, addressing a critical vulnerability in traditional approaches that require extensive retraining for new threats. The federated learning framework facilitates knowledge sharing across network domains while preserving data privacy, allowing collaborative defense without compromising sensitive information. The adaptive response mechanisms dynamically adjust to evolving threat landscapes, ensuring continued effectiveness against sophisticated adversaries.</p>
<p>Our comprehensive evaluation demonstrates that the proposed framework significantly outperforms existing approaches across various metrics. For known attacks, our approach achieves a detection rate of 94.2% with a false positive rate of 3.8%, comparable to the best-performing baselines. For zero-day exploits, our approach achieves a detection rate of 82.7%, representing a 37.5% improvement over approaches without zero-shot capabilities. The mean time to response is reduced by 35% for known attacks and 42% for zero-day exploits compared to the best-performing baseline. These improvements are particularly notable for advanced persistent threats, where our approach achieves a containment rate of 87.3% compared to 62.1% for the best-performing baseline.</p>
<p>The ablation study confirms the contribution of each component to the overall performance, with the hierarchical structure improving coordination efficiency, the zero-shot learning component enabling adaptation to novel threats, and the federated learning component enhancing performance in environments with limited local data. Therefore, the resource efficiency analysis shows that our approach requires computational resources comparable to other learning-based approaches while achieving significantly better performance.</p>
<p>While our approach represents a significant advancement in cyber defense, several limitations and future research directions warrant further investigation:</p>
<ol class="list" style="list-style-type:decimal">
<li><p><strong>Computational Complexity</strong>: The integration of multiple advanced techniques requires significant computational resources, which may limit deployment on resource-constrained devices. Future work should explore optimization techniques to reduce computational requirements without sacrificing performance.</p></li>
<li><p><strong>Initial Training Data Requirements</strong>: Despite zero-shot capabilities, the system still requires diverse training data for initial semantic space construction. Future research should investigate methods to reduce initial data requirements through transfer learning and synthetic data generation.</p></li>
<li><p><strong>Theoretical Guarantees</strong>: Further work is needed to establish formal guarantees on performance bounds and convergence properties, particularly for the integration of hierarchical reinforcement learning with zero-shot adaptation.</p></li>
<li><p><strong>Human Oversight Integration</strong>: While our approach enables autonomous defense, effective integration with human operators remains a challenge. Future research should explore explainable AI techniques to provide transparency into decision-making processes and enable effective human-in-the-loop operation.</p></li>
<li><p><strong>Adversarial Robustness</strong>: As defensive systems become more sophisticated, attackers will develop new techniques to evade detection and mitigation. Future work should investigate adversarial robustness guarantees and defensive mechanisms against attacks targeting the learning process itself.</p></li>
</ol>
<p>In conclusion, our Adaptive Zero-Shot Hierarchical MARL framework with Federated Knowledge Sharing represents a significant step toward more resilient cyber defense systems that can adapt to emerging threats while preserving privacy in collaborative defense scenarios. By addressing key limitations in existing approaches, our framework provides a foundation for future research in adaptive and privacy-preserving cybersecurity.</p></section><section id="sec050"><h2 class="pmc_sec_title">Appendix A: Equation symbol definitions</h2>
<section class="tw xbox font-sm" id="pone.0329969.t007"><div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Symbol</th>
<th align="left" rowspan="1" colspan="1">Description</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>M</em>
<sub>0</sub>
</td>
<td align="left" rowspan="1" colspan="1">Meta-level Markov Decision Process (MDP)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>S</em>
<sub>0</sub>
</td>
<td align="left" rowspan="1" colspan="1">High-level state space in the meta-level MDP</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>s</em>
<sub>0</sub>
</td>
<td align="left" rowspan="1" colspan="1">A state in the high-level state space <em>S</em><sub>0</sub>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>A</em>
<sub>0</sub>
</td>
<td align="left" rowspan="1" colspan="1">Set of available sub-policies (actions) in the meta-level MDP</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>a</em>
<sub>0</sub>
</td>
<td align="left" rowspan="1" colspan="1">An action in the meta-level MDP (selecting a sub-policy)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>P</em>
<sub>0</sub>
</td>
<td align="left" rowspan="1" colspan="1">Transition function: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e088"><math id="M88" display="inline" overflow="linebreak"><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>:</mo><msub><mi>S</mi><mn>0</mn></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>A</mi><mn>0</mn></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>S</mi><mn>0</mn></msub><mo>→</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></math></span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>R</em>
<sub>0</sub>
</td>
<td align="left" rowspan="1" colspan="1">Reward function: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e089"><math id="M89" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mn>0</mn></msub><mo>:</mo><msub><mi>S</mi><mn>0</mn></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>A</mi><mn>0</mn></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>S</mi><mn>0</mn></msub><mo>→</mo><mi>ℝ</mi></mrow></math></span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e090">
<math id="M90" display="inline" overflow="linebreak"><mrow><msub><mi>γ</mi><mn>0</mn></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Discount factor for future rewards at the meta-level, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e091"><math id="M91" display="inline" overflow="linebreak"><mrow><msub><mi>γ</mi><mn>0</mn></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e092">
<math id="M92" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mn>0</mn></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Policy of the meta-controller, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e093"><math id="M93" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mn>0</mn></msub><mo>:</mo><msub><mi>S</mi><mn>0</mn></msub><mo>→</mo><msub><mi>A</mi><mn>0</mn></msub></mrow></math></span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e094">
<math id="M94" display="inline" overflow="linebreak"><mrow><msup><mi>V</mi><mrow><msub><mi>π</mi><mn>0</mn></msub></mrow></msup><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Expected cumulative discounted reward for policy <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e095"><math id="M95" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mn>0</mn></msub></mrow></math></span> starting in state <em>s</em><sub>0</sub>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e096">
<math id="M96" display="inline" overflow="linebreak"><mrow><msubsup><mi>π</mi><mn>0</mn><mrow><mi>*</mi></mrow></msubsup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Optimal policy for the meta-controller</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e097">
<math id="M97" display="inline" overflow="linebreak"><mrow><msub><mi>ρ</mi><mn>0</mn></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Distribution of initial meta-level states</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">t</td>
<td align="left" rowspan="1" colspan="1">Time step</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>M</em>
<sub>
<em>i</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">MDP for sub-task i</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>S</em>
<sub>
<em>i</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">State space for sub-task i</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>s</em>
<sub>
<em>i</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">A state in the state space <em>S</em><sub><em>i</em></sub>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>A</em>
<sub>
<em>i</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">Action space for sub-task i</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>a</em>
<sub>
<em>i</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">An action taken by the sub-agent i</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>P</em>
<sub>
<em>i</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">Transition function: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e098"><math id="M98" display="inline" overflow="linebreak"><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>:</mo><msub><mi>S</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>A</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>S</mi><mi>i</mi></msub><mo>→</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></math></span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>R</em>
<sub>
<em>i</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">Reward function: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e099"><math id="M99" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>:</mo><msub><mi>S</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>A</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><msub><mi>S</mi><mi>i</mi></msub><mo>→</mo><mi>ℝ</mi></mrow></math></span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e100">
<math id="M100" display="inline" overflow="linebreak"><mrow><msub><mi>γ</mi><mi>i</mi></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Discount factor for sub-task i, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e101"><math id="M101" display="inline" overflow="linebreak"><mrow><msub><mi>γ</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e102">
<math id="M102" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Policy for sub-level agent i, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e103"><math id="M103" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mi>i</mi></msub><mo>:</mo><msub><mi>S</mi><mi>i</mi></msub><mo>→</mo><msub><mi>A</mi><mi>i</mi></msub></mrow></math></span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e104">
<math id="M104" display="inline" overflow="linebreak"><mrow><msup><mi>V</mi><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow></msup><mo stretchy="false">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Expected discounted reward for policy <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e105"><math id="M105" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow></math></span> at state <em>s</em><sub><em>i</em></sub>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e106">
<math id="M106" display="inline" overflow="linebreak"><mrow><msubsup><mi>π</mi><mi>i</mi><mrow><mi>*</mi></mrow></msubsup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Optimal policy for sub-agent i</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e107">
<math id="M107" display="inline" overflow="linebreak"><mrow><msub><mi>ρ</mi><mi>i</mi></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Distribution of sub-task i states</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e108">
<math id="M108" display="inline" overflow="linebreak"><mrow><mi>𝒮</mi></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Semantic attribute space for cyber attacks</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e109">
<math id="M109" display="inline" overflow="linebreak"><mrow><mi>𝒳</mi></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Space of observed attack features</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>ϕ</em>
</td>
<td align="left" rowspan="1" colspan="1">Semantic mapping function, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e110"><math id="M110" display="inline" overflow="linebreak"><mrow><mi>ϕ</mi><mo>:</mo><mi>𝒳</mi><mo>→</mo><mi>𝒮</mi></mrow></math></span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e111">
<math id="M111" display="inline" overflow="linebreak"><mrow><msub><mi>x</mi><mrow><mtext>new</mtext></mrow></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">A novel attack pattern instance</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e112">
<math id="M112" display="inline" overflow="linebreak"><mrow><msub><mi>z</mi><mrow><mtext>new</mtext></mrow></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Semantic embedding of a novel attack</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e113">
<math id="M113" display="inline" overflow="linebreak"><mrow><msub><mi>Z</mi><mrow><mtext>known</mtext></mrow></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Set of known attack semantic embeddings</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>z</em>
<sub>
<em>k</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">Semantic embedding of a known attack k</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>w</em>
<sub>
<em>k</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">Similarity weight between <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e114"><math id="M114" display="inline" overflow="linebreak"><mrow><msub><mi>z</mi><mrow><mtext>new</mtext></mrow></msub></mrow></math></span> and <em>z</em><sub><em>k</em></sub>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e115">
<math id="M115" display="inline" overflow="linebreak"><mrow><mi>𝒴</mi></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Set of possible response strategies</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e116">
<math id="M116" display="inline" overflow="linebreak"><mrow><msub><mi>y</mi><mrow><mtext>pred</mtext></mrow></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Predicted response strategy for a novel attack</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e117">
<math id="M117" display="inline" overflow="linebreak"><mrow><mi>ψ</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Mapping of response strategy y into semantic space</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e118">
<math id="M118" display="inline" overflow="linebreak"><mrow><mtext>sim</mtext><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Similarity function in semantic space (e.g., cosine similarity)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">a, b</td>
<td align="left" rowspan="1" colspan="1">Semantic vectors used in similarity computation</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e119">
<math id="M119" display="inline" overflow="linebreak"><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">|</mo><mi>s</mi><mo>,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Conditional policy function based on state s and semantic vector z</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>R</em>
<sub>
<em>t</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">Reward function at time t</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e120">
<math id="M120" display="inline" overflow="linebreak"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><msub><mi>w</mi><mn>3</mn></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Weights for security, efficiency, and novelty objectives</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e121">
<math id="M121" display="inline" overflow="linebreak"><mrow><msub><mi>c</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Cost of action <em>a</em><sub><em>i</em></sub> for sub-agent i</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">C</td>
<td align="left" rowspan="1" colspan="1">Total resource budget constraint</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">P(D)</td>
<td align="left" rowspan="1" colspan="1">Privacy leakage function</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>ε</em>
</td>
<td align="left" rowspan="1" colspan="1">Maximum tolerable privacy leakage</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">FPR, FNR</td>
<td align="left" rowspan="1" colspan="1">False Positive Rate, False Negative Rate (performance bounds)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e122">
<math id="M122" display="inline" overflow="linebreak"><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>,</mo><msub><mi>β</mi><mi>t</mi></msub><mo>,</mo><msub><mi>γ</mi><mi>t</mi></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Dynamic weights in the adaptive reward function</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e123">
<math id="M123" display="inline" overflow="linebreak"><mrow><mi>η</mi></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Learning rate for meta-objective updates</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">J</td>
<td align="left" rowspan="1" colspan="1">Meta-objective function for reward trade-offs</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e124">
<math id="M124" display="inline" overflow="linebreak"><mrow><msub><mi>θ</mi><mi>k</mi></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Local model parameters from agent k</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e125">
<math id="M125" display="inline" overflow="linebreak"><mrow><msubsup><mi>θ</mi><mi>k</mi><mi>′</mi></msubsup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Privacy-preserved model parameters with noise</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>n</em>
<sub>
<em>k</em>
</sub>
</td>
<td align="left" rowspan="1" colspan="1">Number of data points in agent/domain k</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">n</td>
<td align="left" rowspan="1" colspan="1">Total number of data points across agents</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e126">
<math id="M126" display="inline" overflow="linebreak"><mrow><msub><mi>θ</mi><mrow><mtext>global</mtext></mrow></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Global aggregated model parameters</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e127">
<math id="M127" display="inline" overflow="linebreak"><mrow><mi>𝒩</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><msup><mi>C</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Gaussian noise added for differential privacy</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>σ</em>
</td>
<td align="left" rowspan="1" colspan="1">Noise multiplier for differential privacy</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">C</td>
<td align="left" rowspan="1" colspan="1">Clipping bound for gradients in privacy enforcement</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e128">
<math id="M128" display="inline" overflow="linebreak"><mrow><msub><mi>θ</mi><mi>D</mi></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Parameters of defense policy</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e129">
<math id="M129" display="inline" overflow="linebreak"><mrow><msub><mi>θ</mi><mi>A</mi></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Parameters of adversarial policy</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329969.e130">
<math id="M130" display="inline" overflow="linebreak"><mrow><msub><mi>π</mi><mi>D</mi></msub><mo>,</mo><msub><mi>π</mi><mi>A</mi></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Defense and attack policies</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>ρ</em>
</td>
<td align="left" rowspan="1" colspan="1">State distribution used in min-max adversarial training</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">B</td>
<td align="left" rowspan="1" colspan="1">Experience replay buffer</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">L</td>
<td align="left" rowspan="1" colspan="1">Loss function used during model updates</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329969.t007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec051"><h2 class="pmc_sec_title">Appendix B: List of abbreviations</h2>
<section class="tw xbox font-sm" id="pone.0329969.t008"><div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Symbol</th>
<th align="left" rowspan="1" colspan="1">Description</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>AZH-MARL</em>
</td>
<td align="left" rowspan="1" colspan="1">Adaptive Zero-Shot Hierarchical Multi-Agent Reinforcement Learning</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>MARL</em>
</td>
<td align="left" rowspan="1" colspan="1">Multi-Agent Reinforcement Learning</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>HRL</em>
</td>
<td align="left" rowspan="1" colspan="1">Hierarchical Reinforcement Learning</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>ZSL</em>
</td>
<td align="left" rowspan="1" colspan="1">Zero-Shot Learning</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>FL</em>
</td>
<td align="left" rowspan="1" colspan="1">Federated Learning</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>MDP</em>
</td>
<td align="left" rowspan="1" colspan="1">Markov Decision Process</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>FedAvg</em>
</td>
<td align="left" rowspan="1" colspan="1">Federated Averaging</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>SMPC</em>
</td>
<td align="left" rowspan="1" colspan="1">Secure Multi-Party Computation</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>GNN</em>
</td>
<td align="left" rowspan="1" colspan="1">Graph Neural Network</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>CNN</em>
</td>
<td align="left" rowspan="1" colspan="1">Convolutional Neural Network</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>DQN</em>
</td>
<td align="left" rowspan="1" colspan="1">Deep Q-Network</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>MTTD</em>
</td>
<td align="left" rowspan="1" colspan="1">Mean Time to Detection</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>MTTR</em>
</td>
<td align="left" rowspan="1" colspan="1">Mean Time to Response</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>ZSDR</em>
</td>
<td align="left" rowspan="1" colspan="1">Zero-Shot Detection Rate</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>ZSRE</em>
</td>
<td align="left" rowspan="1" colspan="1">Zero-Shot Response Effectiveness</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>SMA</em>
</td>
<td align="left" rowspan="1" colspan="1">Semantic Mapping Accuracy</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>CO</em>
</td>
<td align="left" rowspan="1" colspan="1">Communication Overhead</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>PL</em>
</td>
<td align="left" rowspan="1" colspan="1">Privacy Leakage</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>CT</em>
</td>
<td align="left" rowspan="1" colspan="1">Convergence Time</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>CPO</em>
</td>
<td align="left" rowspan="1" colspan="1">Computational Overhead</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>MU</em>
</td>
<td align="left" rowspan="1" colspan="1">Memory Usage</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>EC</em>
</td>
<td align="left" rowspan="1" colspan="1">Energy Consumption</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>RL</em>
</td>
<td align="left" rowspan="1" colspan="1">Reinforcement Learning</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>DRL</em>
</td>
<td align="left" rowspan="1" colspan="1">Deep Reinforcement Learning</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<em>ML</em>
</td>
<td align="left" rowspan="1" colspan="1">Machine Learning</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329969.t008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>Data cannot be shared publicly because of access restriction from the owner (DRPA). Data are available from the DARPA upon request for research purposes. However, it can be requested through the connect form <a href="https://www.darpa.mil/about/darpaconnect" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.darpa.mil/about/darpaconnect</a> OR through the email direct, Key Contact Information: DARPA Connect: <span>darpaconnect@darpa.mil</span> FOIA Office: (571) 372-0435.</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>The author(s) received no specific funding for this work.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0329969.ref001">
<span class="label">1.</span><cite>Buczak AL, Guven EA. A survey of data mining and machine learning methods for cyber security intrusion detection. IEEE Communications Surveys &amp; Tutorials.
2016;18(2):1153–76.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Communications%20Surveys%20&amp;%20Tutorials.&amp;title=A%20survey%20of%20data%20mining%20and%20machine%20learning%20methods%20for%20cyber%20security%20intrusion%20detection&amp;author=AL%20Buczak&amp;author=EA%20Guven&amp;volume=18&amp;issue=2&amp;publication_year=2016&amp;pages=1153-76&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref002">
<span class="label">2.</span><cite>Nguyen TT, Reddi VJ. Deep reinforcement learning for cyber security. IEEE Transactions on Neural Networks and Learning Systems.
2020;31(7):2669–83.</cite> [<a href="https://doi.org/10.1109/TNNLS.2021.3121870" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34723814/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Transactions%20on%20Neural%20Networks%20and%20Learning%20Systems.&amp;title=Deep%20reinforcement%20learning%20for%20cyber%20security&amp;author=TT%20Nguyen&amp;author=VJ%20Reddi&amp;volume=31&amp;issue=7&amp;publication_year=2020&amp;pages=2669-83&amp;pmid=34723814&amp;doi=10.1109/TNNLS.2021.3121870&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref003">
<span class="label">3.</span><cite>Foley J, Moradpoor N, Ochenyi H. Employing a machine learning approach to detect combined internet of things attacks against two objective functions using a novel dataset. Security and Communication Networks.
2020;2020:1–17. doi: 10.1155/2020/2804291</cite> [<a href="https://doi.org/10.1155/2020/2804291" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Security%20and%20Communication%20Networks.&amp;title=Employing%20a%20machine%20learning%20approach%20to%20detect%20combined%20internet%20of%20things%20attacks%20against%20two%20objective%20functions%20using%20a%20novel%20dataset&amp;author=J%20Foley&amp;author=N%20Moradpoor&amp;author=H%20Ochenyi&amp;volume=2020&amp;publication_year=2020&amp;pages=1-17&amp;doi=10.1155/2020/2804291&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref004">
<span class="label">4.</span><cite>Olutimehin AT. The synergistic role of machine learning, deep learning, and reinforcement learning in strengthening cyber security measures for crypto currency platforms. Deep Learning, and Reinforcement Learning in Strengthening Cyber Security Measures for Crypto Currency Platforms.
2025.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Deep%20Learning,%20and%20Reinforcement%20Learning%20in%20Strengthening%20Cyber%20Security%20Measures%20for%20Crypto%20Currency%20Platforms.&amp;title=The%20synergistic%20role%20of%20machine%20learning,%20deep%20learning,%20and%20reinforcement%20learning%20in%20strengthening%20cyber%20security%20measures%20for%20crypto%20currency%20platforms&amp;author=AT%20Olutimehin&amp;publication_year=2025&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref005">
<span class="label">5.</span><cite>Sutton RS, Barto AG. Reinforcement learning: An introduction. MIT Press. 2018.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Reinforcement%20learning:%20An%20introduction&amp;author=RS%20Sutton&amp;author=AG%20Barto&amp;publication_year=2018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref006">
<span class="label">6.</span><cite>Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, et al. Human-level control through deep reinforcement learning. Nature.
2015;518(7540):529–33. doi: 10.1038/nature14236

</cite> [<a href="https://doi.org/10.1038/nature14236" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25719670/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nature.&amp;title=Human-level%20control%20through%20deep%20reinforcement%20learning&amp;author=V%20Mnih&amp;author=K%20Kavukcuoglu&amp;author=D%20Silver&amp;author=AA%20Rusu&amp;author=J%20Veness&amp;volume=518&amp;issue=7540&amp;publication_year=2015&amp;pages=529-33&amp;pmid=25719670&amp;doi=10.1038/nature14236&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref007">
<span class="label">7.</span><cite>Silver D, Schrittwieser J, Simonyan K, Antonoglou I, Huang A, Guez A, et al. Mastering the game of Go without human knowledge. Nature.
2017;550(7676):354–9. doi: 10.1038/nature24270

</cite> [<a href="https://doi.org/10.1038/nature24270" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29052630/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nature.&amp;title=Mastering%20the%20game%20of%20Go%20without%20human%20knowledge&amp;author=D%20Silver&amp;author=J%20Schrittwieser&amp;author=K%20Simonyan&amp;author=I%20Antonoglou&amp;author=A%20Huang&amp;volume=550&amp;issue=7676&amp;publication_year=2017&amp;pages=354-9&amp;pmid=29052630&amp;doi=10.1038/nature24270&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref008">
<span class="label">8.</span><cite>Serban AC, Poll E, Visser J. A standard driven software architecture for fully autonomous vehicles. In: 2020 IEEE International Conference on Software Architecture Companion (ICSA-C). 2020. p. 31–8.</cite>
</li>
<li id="pone.0329969.ref009">
<span class="label">9.</span><cite>Malialis K, Kudenko D. Distributed response to network intrusions using multiagent reinforcement learning. Engineering Applications of Artificial Intelligence.
2015;41:270–84.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Engineering%20Applications%20of%20Artificial%20Intelligence.&amp;title=Distributed%20response%20to%20network%20intrusions%20using%20multiagent%20reinforcement%20learning&amp;author=K%20Malialis&amp;author=D%20Kudenko&amp;volume=41&amp;publication_year=2015&amp;pages=270-84&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref010">
<span class="label">10.</span><cite>Elderman R, Pater LJJ, Thie AS, Drugan MM, Wiering M. Adversarial reinforcement learning in a cyber security simulation. In: International Conference on Agents and Artificial Intelligence. 2017 . p. 559–66.</cite>
</li>
<li id="pone.0329969.ref011">
<span class="label">11.</span><cite>Serban AC, Poll E, Visser J. Tactical safety reasoning. A case for autonomous vehicles. In: 2020 IEEE 92nd Vehicular Technology Conference (VTC2020-Fall). 2020. p. 1–7.</cite>
</li>
<li id="pone.0329969.ref012">
<span class="label">12.</span><cite>Alshamrani A, Alshahrani A. Adaptive cyber defense technique based on multiagent reinforcement learning strategies. Intelligent Automation &amp; Soft Computing.
2023;36(3):2539–55.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Intelligent%20Automation%20&amp;%20Soft%20Computing.&amp;title=Adaptive%20cyber%20defense%20technique%20based%20on%20multiagent%20reinforcement%20learning%20strategies&amp;author=A%20Alshamrani&amp;author=A%20Alshahrani&amp;volume=36&amp;issue=3&amp;publication_year=2023&amp;pages=2539-55&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref013">
<span class="label">13.</span><cite>Oh SH, Kim J, Nah JH, Park J. Employing deep reinforcement learning to cyber-attack simulation for enhancing cybersecurity. Electronics.
2024;13(3):555. doi: 10.3390/electronics13030555</cite> [<a href="https://doi.org/10.3390/electronics13030555" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Electronics.&amp;title=Employing%20deep%20reinforcement%20learning%20to%20cyber-attack%20simulation%20for%20enhancing%20cybersecurity&amp;author=SH%20Oh&amp;author=J%20Kim&amp;author=JH%20Nah&amp;author=J%20Park&amp;volume=13&amp;issue=3&amp;publication_year=2024&amp;pages=555&amp;doi=10.3390/electronics13030555&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref014">
<span class="label">14.</span><cite>Fu Q, Qiu T, Yi J, Pu Z, Ai X. Self-clustering hierarchical multi-agent reinforcement learning with extensible cooperation graph. IEEE Transactions on Emerging Topics in Computational Intelligence.
2024.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Transactions%20on%20Emerging%20Topics%20in%20Computational%20Intelligence.&amp;title=Self-clustering%20hierarchical%20multi-agent%20reinforcement%20learning%20with%20extensible%20cooperation%20graph&amp;author=Q%20Fu&amp;author=T%20Qiu&amp;author=J%20Yi&amp;author=Z%20Pu&amp;author=X%20Ai&amp;publication_year=2024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref015">
<span class="label">15.</span><cite>Kulkarni TD, Narasimhan K, Saeedi A, Tenenbaum JB. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Advances in Neural Information Processing Systems.
2016;29.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Advances%20in%20Neural%20Information%20Processing%20Systems.&amp;title=Hierarchical%20deep%20reinforcement%20learning:%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation&amp;author=TD%20Kulkarni&amp;author=K%20Narasimhan&amp;author=A%20Saeedi&amp;author=JB%20Tenenbaum&amp;volume=29&amp;publication_year=2016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref016">
<span class="label">16.</span><cite>Nachum O, Gu SS, Lee H, Levine S. Data-efficient hierarchical reinforcement learning. Advances in neural information processing systems.
2018;31.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Advances%20in%20neural%20information%20processing%20systems.&amp;title=Data-efficient%20hierarchical%20reinforcement%20learning&amp;author=O%20Nachum&amp;author=SS%20Gu&amp;author=H%20Lee&amp;author=S%20Levine&amp;volume=31&amp;publication_year=2018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref017">
<span class="label">17.</span><cite>Bacon P-L, Harb J, Precup D. The option-critic architecture. AAAI.
2017;31(1). doi: 10.1609/aaai.v31i1.10916</cite> [<a href="https://doi.org/10.1609/aaai.v31i1.10916" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=AAAI.&amp;title=The%20option-critic%20architecture&amp;author=P-L%20Bacon&amp;author=J%20Harb&amp;author=D%20Precup&amp;volume=31&amp;issue=1&amp;publication_year=2017&amp;doi=10.1609/aaai.v31i1.10916&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref018">
<span class="label">18.</span><cite>Vezhnevets AS, Osindero S, Schaul T, Heess N, Jaderberg M, Silver D, et al. Feudal networks for hierarchical reinforcement learning. In: International Conference on Machine Learning. 2017. p. 3540–9.</cite>
</li>
<li id="pone.0329969.ref019">
<span class="label">19.</span><cite>Xian Y, Lampert CH, Schiele B, Akata Z. Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly. IEEE Trans Pattern Anal Mach Intell.
2019;41(9):2251–65. doi: 10.1109/TPAMI.2018.2857768

</cite> [<a href="https://doi.org/10.1109/TPAMI.2018.2857768" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30028691/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell.&amp;title=Zero-shot%20learning-a%20comprehensive%20evaluation%20of%20the%20good,%20the%20bad%20and%20the%20ugly&amp;author=Y%20Xian&amp;author=CH%20Lampert&amp;author=B%20Schiele&amp;author=Z%20Akata&amp;volume=41&amp;issue=9&amp;publication_year=2019&amp;pages=2251-65&amp;pmid=30028691&amp;doi=10.1109/TPAMI.2018.2857768&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref020">
<span class="label">20.</span><cite>Wang W, Zheng VW, Yu H, Miao C. A survey of zero-shot learning: settings, methods, and applications. ACM Transactions on Intelligent Systems and Technology.
2019;10(2):1–37.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Transactions%20on%20Intelligent%20Systems%20and%20Technology.&amp;title=A%20survey%20of%20zero-shot%20learning:%20settings,%20methods,%20and%20applications&amp;author=W%20Wang&amp;author=VW%20Zheng&amp;author=H%20Yu&amp;author=C%20Miao&amp;volume=10&amp;issue=2&amp;publication_year=2019&amp;pages=1-37&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref021">
<span class="label">21.</span><cite>Romera-Paredes B, Torr P. An embarrassingly simple approach to zero-shot learning. In: International Conference on Machine Learning. 2015. p. 2152–61.</cite>
</li>
<li id="pone.0329969.ref022">
<span class="label">22.</span><cite>Socher R, Ganjoo M, Manning CD, Ng A. Zero-shot learning through cross-modal transfer. Advances in Neural Information Processing Systems.
2013;26.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Advances%20in%20Neural%20Information%20Processing%20Systems.&amp;title=Zero-shot%20learning%20through%20cross-modal%20transfer&amp;author=R%20Socher&amp;author=M%20Ganjoo&amp;author=CD%20Manning&amp;author=A%20Ng&amp;volume=26&amp;publication_year=2013&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref023">
<span class="label">23.</span><cite>Srivastava A, Sanghavi P, Parmar V, Rani S. Zero-shot learning in cybersecurity: a paradigm shift in attack and defense strategies. In: International Conference on Advances in Computing and Data Sciences. Springer; 2024. p. 138–49.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=International%20Conference%20on%20Advances%20in%20Computing%20and%20Data%20Sciences&amp;author=A%20Srivastava&amp;author=P%20Sanghavi&amp;author=V%20Parmar&amp;author=S%20Rani&amp;publication_year=2024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref024">
<span class="label">24.</span><cite>McMahan B, Moore E, Ramage D, Hampson S, y Arcas BA. Communication-efficient learning of deep networks from decentralized data. In: Artificial intelligence and statistics. 2017. p. 1273–82.</cite>
</li>
<li id="pone.0329969.ref025">
<span class="label">25.</span><cite>Li T, Sahu AK, Talwalkar A, Smith V. Federated learning: challenges, methods, and future directions. IEEE Signal Processing Magazine.
2020;37(3):50–60.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Signal%20Processing%20Magazine.&amp;title=Federated%20learning:%20challenges,%20methods,%20and%20future%20directions&amp;author=T%20Li&amp;author=AK%20Sahu&amp;author=A%20Talwalkar&amp;author=V%20Smith&amp;volume=37&amp;issue=3&amp;publication_year=2020&amp;pages=50-60&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref026">
<span class="label">26.</span><cite>Gohil V, Patnaik S, Kalathil D, Rajendran J. AttackGNN: red-teaming GNNs in hardware security using reinforcement learning. In: 33rd USENIX Security Symposium (USENIX Security 24). 2024. p. 73–90.</cite>
</li>
<li id="pone.0329969.ref027">
<span class="label">27.</span><cite>Yang M, Chu X, Zhu J, Xi Y, Niu S, Wang Z. Adaptive federated few-shot feature learning with prototype rectification. Engineering Applications of Artificial Intelligence.
2023;126:107125.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Engineering%20Applications%20of%20Artificial%20Intelligence.&amp;title=Adaptive%20federated%20few-shot%20feature%20learning%20with%20prototype%20rectification&amp;author=M%20Yang&amp;author=X%20Chu&amp;author=J%20Zhu&amp;author=Y%20Xi&amp;author=S%20Niu&amp;volume=126&amp;publication_year=2023&amp;pages=107125&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref028">
<span class="label">28.</span><cite>Zhao R, Yang L, Wang Y, Xue Z, Gui G, Ohtsuki T. A semi-supervised federated learning scheme via knowledge distillation for intrusion detection. In: ICC 2022 -IEEE International Conference on Communications. 2022. p. 2688–93.</cite>
</li>
<li id="pone.0329969.ref029">
<span class="label">29.</span><cite>Zhang L, Wu D, Yuan X. Fedzkt: zero-shot knowledge transfer towards resource-constrained federated learning with heterogeneous on-device models. In: 2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS). 2022. p. 928–38.</cite>
</li>
<li id="pone.0329969.ref030">
<span class="label">30.</span><cite>Dasgupta R, Mitra P. Large language model-based federated zero-shot learning for intrusion detection in smart grids. Artificial intelligence applications and innovations. Cham: Springer; 2025. p. 331–44.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Artificial%20intelligence%20applications%20and%20innovations&amp;author=R%20Dasgupta&amp;author=P%20Mitra&amp;publication_year=2025&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref031">
<span class="label">31.</span><cite>Zhang X, et al. Federated MARL framework for intrusion detection in IoT networks. In: Proceedings of IEEE Symposium on Security and Privacy (S&amp;P). 2023.</cite>
</li>
<li id="pone.0329969.ref032">
<span class="label">32.</span><cite>Liu Y, et al. Hierarchical multi-agent reinforcement learning with transformers for cybersecurity. In: Proceedings of the International Conference on Machine Learning (ICML). 2024.</cite>
</li>
<li id="pone.0329969.ref033">
<span class="label">33.</span><cite>Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. Proximal policy optimization algorithms. arXiv preprint
2017. <a href="https://arxiv.org/abs/1707.06347" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1707.06347</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=Proximal%20policy%20optimization%20algorithms&amp;author=J%20Schulman&amp;author=F%20Wolski&amp;author=P%20Dhariwal&amp;author=A%20Radford&amp;author=O%20Klimov&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref034">
<span class="label">34.</span><cite>Dwork C, Roth A. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science.
2014;9(3–4):211–407.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Foundations%20and%20Trends%20in%20Theoretical%20Computer%20Science.&amp;title=The%20algorithmic%20foundations%20of%20differential%20privacy&amp;author=C%20Dwork&amp;author=A%20Roth&amp;volume=9&amp;publication_year=2014&amp;pages=211-407&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref035">
<span class="label">35.</span><cite>Bonawitz K, Ivanov V, Kreuter B, Marcedone A, McMahan HB, Patel S, et al. Practical secure aggregation for privacy-preserving machine learning. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017. p. 1175–91.</cite>
</li>
<li id="pone.0329969.ref036">
<span class="label">36.</span><cite>Li D, Wang J. FedMD: heterogenous federated learning via model distillation. arXiv preprint
2019. <a href="https://arxiv.org/abs/1910.03581" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1910.03581</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=FedMD:%20heterogenous%20federated%20learning%20via%20model%20distillation&amp;author=D%20Li&amp;author=J%20Wang&amp;publication_year=2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref037">
<span class="label">37.</span><cite>Rolnick D, Ahuja A, Schwarz J, Lillicrap T, Wayne G. Experience replay for continual learning. Advances in Neural Information Processing Systems.
2019;32.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Advances%20in%20Neural%20Information%20Processing%20Systems.&amp;title=Experience%20replay%20for%20continual%20learning&amp;author=D%20Rolnick&amp;author=A%20Ahuja&amp;author=J%20Schwarz&amp;author=T%20Lillicrap&amp;author=G%20Wayne&amp;volume=32&amp;publication_year=2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref038">
<span class="label">38.</span><cite>Liang E, Liaw R, Nishihara R, Moritz P, Fox R, Goldberg K, et al. RLlib: Abstractions for distributed reinforcement learning. In: International Conference on Machine Learning. 2018. p. 3053–62.</cite>
</li>
<li id="pone.0329969.ref039">
<span class="label">39.</span><cite>Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. Advances in neural information processing systems.
2017;30.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Advances%20in%20neural%20information%20processing%20systems.&amp;title=Attention%20is%20all%20you%20need&amp;author=A%20Vaswani&amp;author=N%20Shazeer&amp;author=N%20Parmar&amp;author=J%20Uszkoreit&amp;author=L%20Jones&amp;volume=30&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref040">
<span class="label">40.</span><cite>Koch G, Zemel R, Salakhutdinov R. Siamese neural networks for one-shot image recognition. In: ICML Deep Learning Workshop. 2015.</cite>
</li>
<li id="pone.0329969.ref041">
<span class="label">41.</span><cite>Ryffel T, Trask A, Dahl M, Wagner B, Mancuso J, Rueckert D, et al. A generic framework for privacy preserving deep learning. arXiv preprint
2018. <a href="https://arxiv.org/abs/1811.04017" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1811.04017</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=A%20generic%20framework%20for%20privacy%20preserving%20deep%20learning&amp;author=T%20Ryffel&amp;author=A%20Trask&amp;author=M%20Dahl&amp;author=B%20Wagner&amp;author=J%20Mancuso&amp;publication_year=2018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref042">
<span class="label">42.</span><cite>Cage T. The CAGE challenge: Cyber security dataset for reinforcement learning. arXiv preprint.2021. <a href="https://arxiv.org/abs/2104.13797" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2104.13797</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=The%20CAGE%20challenge:%20Cyber%20security%20dataset%20for%20reinforcement%20learning&amp;author=T%20Cage&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329969.ref043">
<span class="label">43.</span><cite>Defense Advanced Research Projects Agency. Cyber Range and Test Environment (CRATE) Dataset. 2022.</cite>
</li>
</ul></section></section></section><article class="sub-article" id="pone.0329969.r001"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0329969.r001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329969.r001</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 0</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Zeashan Khan</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Zeashan Khan</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zeashan Khan</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.c" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.c" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.c" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Zeashan Khan</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.c" class="d-panel p" style="display: none">
<div>© 2025 Zeashan Khan</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>5 May 2025</em>
</p>
<p>PONE-D-25-16867Federated Hierarchical MARL for Zero-Shot Cyber DefensePLOS ONE</p>
<p>Dear Dr. Alshamrani,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
<p>Please submit your revised manuscript by Jun 19 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span>. When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<p>Please include the following items when submitting your revised manuscript:</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a>.</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Zeashan Hameed Khan, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Journal Requirements:</p>
<p>When submitting your revision, we need you to address these additional requirements.</p>
<p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p>
<p><a href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</a> and</p>
<p>
<a href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</a>
</p>
<p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, we expect all author-generated code to be made available without restrictions upon publication of the work. Please review our guidelines at <a href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</a> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p>
<p>3. We note that your Data Availability Statement is currently as follows: All relevant data are within the manuscript and its Supporting Information files.</p>
<p>Please confirm at this time whether or not your submission contains all raw data required to replicate the results of your study. Authors must share the “minimal data set” for their submission. PLOS defines the minimal data set to consist of the data required to replicate all study findings reported in the article, as well as related metadata and methods (<a href="https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition</a>).</p>
<p>For example, authors should submit the following data:</p>
<p>- The values behind the means, standard deviations and other measures reported;</p>
<p>- The values used to build graphs;</p>
<p>- The points extracted from images for analysis.</p>
<p>Authors do not need to submit their entire data set if only a portion of the data was used in the reported study.</p>
<p>If your submission does not contain these data, please either upload them as Supporting Information files or deposit them to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. For a list of recommended repositories, please see <a href="https://journals.plos.org/plosone/s/recommended-repositories" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/recommended-repositories</a>.</p>
<p>If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially sensitive information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., an ethics committee). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent. If data are owned by a third party, please indicate how others may request data access.</p>
<p>4. When completing the data availability statement of the submission form, you indicated that you will make your data available on acceptance. We strongly recommend all authors decide on a data sharing plan before acceptance, as the process can be lengthy and hold up publication timelines. Please note that, though access restrictions are acceptable now, your entire data will need to be made freely accessible if your manuscript is accepted for publication. This policy applies to all data except where public deposition would breach compliance with the protocol approved by your research ethics board. If you are unable to adhere to our open data policy, please kindly revise your statement to explain your reasoning and we will seek the editor's input on an exemption. Please be assured that, once you have provided your new statement, the assessment of your exemption will not hold up the peer review process.</p>
<p>
<strong>Additional Editor Comments:</strong>
</p>
<p>The paper proposes an interesting solution to the cyber security of the networked devices. However, it needs significant improvement before further consideration.</p>
<p>The MARL algorithm needs more concrete references and the link to datasets must be included in the references and acknowledgement.</p>
<p>Please take a careful look at the reviewer's comments and submit a revised version by the due date.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>

<strong>Comments to the Author</strong>
</p>
<p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #1: Partly</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>2. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>5. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p>Reviewer #1: Please find below some comments that may help to improve the quality of this manuscript.</p>
<p>Author summary: The "Author summary" section contains placeholder "Lorem ipsum" text and needs to be written.</p>
<p>1.4 Evaluation: The authors should add a detailed description of the network topology and how many agents were deployed in the network. The goal is to share enough information so independent researchers can reproduce the results discussed in the manuscript (say in an Evaluation Setup section). Moreover, remember that the evaluation setup is important for the interpretation of quantitative results of a model combining hierarchical MARL and Federated Knowledge Sharing.</p>
<p>1.4.3 Figure 2 (Detection Performance) seems to mislabel some approaches compared to Table 1. For example, Figure 2 shows "Federated Learning" and "Zero-Shot Learning" as separate defense approaches, whereas Table 1 lists "Centralized Learning" and "Independent Learning". This inconsistency needs correction.</p>
<p>1.4.3 Results and Analysis: The authors should explain how "zero-day exploits" and "advanced persistent threats" were defined and executed during the experiments.</p>
<p>Dataset: Please add a link to the DARPA CRATE dataset used in this investigation.</p>
<p>Reviewer #2: This work proposes an adaptive zero-shot hierarchical multiagent reinforcement learning approach to achieve efficient and resilient cyber defense systems. The result shows that the proposed framework is also able to accurately detect zero-day with a relatively lower response time.</p>
<p>Although the paper uses real-testbed evaluations however some implementation details need more explanation (/references). Also, the problem and proposed mechanism needs proper justification.</p>
<p>1. All equations and symbols used must be explained clearly in the paper. Eq. 1 is misplaced and is never referenced in the literature which makes it hard to follow.</p>
<p>The contrastive loss function Equation in section 1.3.5 needed to be properly written and labelled. Use a table to explain each equation symbol in use.</p>
<p>2. Also please use a table to list all abbreviations. Some statements are confusing, for example the second line of second last paragraph of section 0.1 says, "The work likely addresses the challenge of scalability and coordination in</p>
<p>multi-agent systems ..." Please rewrite it.</p>
<p>3. Algorithm 1 (Adaptive Zero Shot Learning algorithm) needs better explanation (e.g. components running it?) along with the reference in the text.</p>
<p>4. In implementation section please add a table stating all python libraries/tools used, their purpose, and reference.</p>
<p>5. Proper references should be provided with baseline approaches listed in section 1.4.2.</p>
<p>6. Implementation and results sections require thorough revisions, especially specific details regarding implementation and evaluation needs to be added in the corresponding sections.</p>
<p>For example, the custom federated learning testbed used for performance evaluation should either be explained clearly (with figure) or a reference needs to be added.</p>
<p>7. From the results in Fig 2, it looks like the combination of Federated and zero-shot learning (if added) may result in an overall better or similar performance to that of the proposed (AZH-MARL) one. Please elaborate on it.</p>
<p>**********</p>
<p>6. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a>). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a>.</p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span>. Please note that Supporting Information files do not need this step.</p></section></article><article class="sub-article" id="pone.0329969.r002"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 8;20(8):e0329969. doi: <a href="https://doi.org/10.1371/journal.pone.0329969.r002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329969.r002</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 1</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.d" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.d" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.d" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub2" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.d" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>28 May 2025</em>
</p>
<p>Dear Zeashan Hameed Khan,</p>
<p>We sincerely thank you and the reviewers for the time and effort you devoted to evaluating our manuscript, titled " Federated Hierarchical MARL for Zero-Shot Cyber Defense” ONE-D-25-16867.</p>
<p>We appreciate the thoughtful and constructive comments, which have helped us improve the clarity and quality of our work.</p>
<p>We have carefully considered each point raised and revised the manuscript accordingly. Below, we provide a detailed response to each comment. Reviewer comments are listed in black, and our responses are provided directly below each point in green, yellow, and blue.</p>
<p>We hope that the revisions and clarifications meet your expectations and respectfully resubmit our manuscript for your reconsideration.</p>
<p>I am copying the response here but it is also attached as file.</p>
<p>Journal Requirements:</p>
<p>When submitting your revision, we need you to address these additional requirements.</p>
<p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p>
<p><a href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</a> and</p>
<p>
<a href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</a>
</p>
<p>( Doubled checked and followed</p>
<p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, we expect all author-generated code to be made available without restrictions upon publication of the work. Please review our guidelines at <a href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</a> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p>
<p>3. We note that your Data Availability Statement is currently as follows: All relevant data are within the manuscript and its Supporting Information files.</p>
<p>Please confirm at this time whether or not your submission contains all raw data required to replicate the results of your study. Authors must share the “minimal data set” for their submission. PLOS defines the minimal data set to consist of the data required to replicate all study findings reported in the article, as well as related metadata and methods (<a href="https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition</a>).</p>
<p>For example, authors should submit the following data:</p>
<p>- The values behind the means, standard deviations and other measures reported;</p>
<p>- The values used to build graphs;</p>
<p>- The points extracted from images for analysis.</p>
<p>Authors do not need to submit their entire data set if only a portion of the data was used in the reported study.</p>
<p>If your submission does not contain these data, please either upload them as Supporting Information files or deposit them to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. For a list of recommended repositories, please see <a href="https://journals.plos.org/plosone/s/recommended-repositories" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/recommended-repositories</a>.</p>
<p>If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially sensitive information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., an ethics committee). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent. If data are owned by a third party, please indicate how others may request data access.</p>
<p>4. When completing the data availability statement of the submission form, you indicated that you will make your data available on acceptance. We strongly recommend all authors decide on a data sharing plan before acceptance, as the process can be lengthy and hold up publication timelines. Please note that, though access restrictions are acceptable now, your entire data will need to be made freely accessible if your manuscript is accepted for publication. This policy applies to all data except where public deposition would breach compliance with the protocol approved by your research ethics board. If you are unable to adhere to our open data policy, please kindly revise your statement to explain your reasoning and we will seek the editor's input on an exemption. Please be assured that, once you have provided your new statement, the assessment of your exemption will not hold up the peer review process.</p>
<p>Additional Editor Comments:</p>
<p>The paper proposes an interesting solution to the cyber security of the networked devices. However, it needs significant improvement before further consideration.</p>
<p>The MARL algorithm needs more concrete references and the link to datasets must be included in the references and acknowledgement.</p>
<p>Please take a careful look at the reviewer's comments and submit a revised version by the due date.</p>
<p>This study is based on data derived from the DARPA Cyber Range and Test Environment (CRATE) dataset (2022). The dataset is not publicly available, and details about its access and distribution are limited. All experiments and analyses comply with applicable institutional and ethical guidelines.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>________________________________________</p>
<p>5. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p>Reviewer #1: Please find below some comments that may help to improve the quality of this manuscript.</p>
<p>Author summary: The "Author summary" section contains placeholder "Lorem ipsum" text and needs to be written.</p>
<p>I removed this from the template.</p>
<p>1.4 Evaluation: The authors should add a detailed description of the network topology and how many agents were deployed in the network. The goal is to share enough information so independent researchers can reproduce the results discussed in the manuscript (say in an Evaluation Setup section). Moreover, remember that the evaluation setup is important for the interpretation of quantitative results of a model combining hierarchical MARL and Federated Knowledge Sharing.</p>
<p>I have added shorter version of this response to the manuscript in Evaluation section page 16 and 17. I will add the full response if needed.</p>
<p>To rigorously evaluate the proposed Adaptive Zero-Shot Hierarchical MARL (AZH-MARL) framework, we designed a comprehensive experimental setup utilizing both established simulation environments and a custom-developed testbed. This section details the network topologies, agent configurations, and evaluation scenarios employed.</p>
<p>Evaluation Environments:</p>
<p>Our experiments were conducted across three primary environments to assess different facets of the AZH-MARL framework:</p>
<p>1. CybORG Simulation Environment: We utilized the CybORG environment [39], a recognized platform for cybersecurity research developed for the CAGE Challenge. For our experiments, we configured a simulated enterprise network topology within CybORG consisting of approximately 50 nodes distributed across three distinct subnets: a public-facing DMZ, an internal corporate network, and a restricted operational technology (OT) segment. These segments included a variety of simulated devices such as web servers, database servers, user workstations, and programmable logic controllers (PLCs). The connectivity was designed to mimic realistic enterprise traffic flows and potential attack paths. Attack scenarios native to CybORG, as well as custom scripted attacks, were used to test the defensive capabilities of our agents in this dynamic setting.</p>
<p>2. DARPA CRATE Dataset Scenarios: The DARPA Cyber Range and Test Environment (CRATE) Dataset [40] provided a rich source of realistic network traffic and attack data. While the full dataset is extensive, we selected specific scenarios relevant to testing zero-shot learning capabilities. These scenarios involved diverse attack vectors and sophisticated adversarial behaviors. The data from CRATE was primarily used to train and evaluate the semantic mapping function and the zero-shot policy transfer mechanisms of the AZH-MARL framework, providing a basis for assessing performance against previously unseen attack patterns.</p>
<p>However, This study is based on data derived from the DARPA Cyber Range and Test Environment (CRATE) dataset (2022). The dataset is not publicly available, and details about its access and distribution are limited. All experiments and analyses comply with applicable institutional and ethical guidelines.</p>
<p>3. Custom Federated Learning Testbed: To specifically evaluate the federated learning and privacy-preserving aspects of AZH-MARL, we developed a custom distributed testbed. This testbed simulated a collaborative defense scenario involving three independent organizational networks. Each virtual network environment within the testbed comprised approximately 30-40 nodes, with unique security policies, varying levels of baseline security posture, and distinct, locally generated attack patterns (e.g., phishing campaigns targeting one network, ransomware in another, data exfiltration attempts in the third). This heterogeneity was crucial for testing the framework’s ability to share knowledge effectively and adapt defenses in a federated manner while preserving the privacy of each participating entity’s local data.</p>
<p>Agent Deployment and Configuration:</p>
<p>In each environment where applicable (CybORG and the Custom Federated Learning Testbed), the AZH-MARL framework was deployed with a hierarchical agent structure:</p>
<p>• Meta-Controller: A single meta-controller agent was responsible for overseeing the overall defense strategy within each simulated network (or each federated node in the custom testbed). It received aggregated security metrics and threat assessments as its state input and selected high-level sub-policies (options) for execution.</p>
<p>• Sub-Level Agents: Four types of specialized sub-level agents were deployed, corresponding to the primary defense tasks:</p>
<p>o Reconnaissance Agent: Monitored network traffic, system logs, and endpoint behaviors to detect suspicious activities and potential threats.</p>
<p>o Analysis Agent: Evaluated detected anomalies and alerts to determine their nature, severity, and potential impact, correlating information from various sources.</p>
<p>o Response Agent: Executed defensive actions, such as isolating compromised hosts, blocking malicious traffic, or deploying countermeasures, based on the analysis and meta-controller directives.</p>
<p>o Recovery Agent: Focused on restoring affected systems and services to normal operation post-incident, including patching vulnerabilities and verifying system integrity.</p>
<p>In the CybORG environment, these agents operated on the simulated enterprise network. In the custom federated learning testbed, each of the three simulated organizational networks had its own deployment of this hierarchical agent structure. The local models from these agents were then aggregated via the federated learning mechanism described in Section 1.1.</p>
<p>The number of primitive actions available to sub-level agents varied based on the task and environment but typically ranged from 5 to 10 distinct actions per agent type (e.g., for a response agent: block IP, isolate host, patch vulnerability, etc.). The meta-controller selected from the four sub-level policy types. All agents were trained using Proximal Policy Optimization (PPO) as detailed in Section 0.6.3.</p>
<p>This detailed experimental setup was designed to provide a robust and reproducible evaluation of the AZH-MARL framework’s capabilities in complex and evolving cyber threat landscapes</p>
<p>1.4.3 Figure 2 (Detection Performance) seems to mislabel some approaches compared to Table 1. For example, Figure 2 shows "Federated Learning" and "Zero-Shot Learning" as separate defense approaches, whereas Table 1 lists "Centralized Learning" and "Independent Learning". This inconsistency needs correction.</p>
<p>Fixed in the manuscript and now it appears as Figure 3 since I added a new Figure to the manuscript.</p>
<p>1.4.3 Results and Analysis: The authors should explain how "zero-day exploits" and "advanced persistent threats" were defined and executed during the experiments.</p>
<p>Here I added the full response to the respected author, but I have provided shorter response in the manuscript for the seek of keeping it easily understandable, but if you prefer to add the full response to the manuscript then I will</p>
<p>Zero-Day Exploits</p>
<p>Definition:</p>
<p>In the context of our experiments, a "zero-day exploit" refers to an attack that leverages a previously unknown vulnerability in the target systems or software, for which no specific signature or patch was available at the time of the simulated attack. The key characteristic is the novelty of the attack vector to the defense system being evaluated. Our AZH-MARL framework, with its zero-shot learning component, is specifically designed to address such threats by generalizing from known attack patterns to recognize and respond to these novel instances.</p>
<p>Execution and Simulation:</p>
<p>To simulate zero-day exploits, we employed the following methodologies across our evaluation environments:</p>
<p>1. CybORG Simulation Environment: We utilized attack scenarios within CybORG that were either newly developed or modified versions of existing attacks, ensuring that the specific attack signatures or behavioral patterns were not part of the training data for the baseline defense agents or the initial training phase of our AZH-MARL agents. This was achieved by holding out specific attack types or variants from the training set and introducing them only during the evaluation phase.</p>
<p>2. DARPA Dataset Scenarios: The dataset contains a diverse range of attack instances. For zero-shot evaluation, we partitioned the dataset such that attack classes or specific instances with unique characteristics (e.g., novel combinations of tactics, techniques, and procedures - TTPs) were reserved for the test set. The agents were trained on a distinct set of known attack types, and their ability to detect and respond to these unseen types from the test set constituted the zero-day exploit scenario.</p>
<p>3. Custom Federated Learning Testbed: Within each simulated organizational network in our custom testbed, we introduced novel attack scripts or modified existing attack tools to generate traffic and host behaviors that were not previously encountered by the local agents or the federated model. This involved, for example, using obfuscated versions of known malware, new phishing campaign templates, or exploiting simulated vulnerabilities that were not present in the training environments of other federated nodes.</p>
<p>The core principle was to ensure that the defense system had no prior explicit knowledge (e.g., signatures, specific rules) of the exact exploit being used during these test scenarios, forcing it to rely on generalized threat understanding and adaptive capabilities.</p>
<p>Advanced Persistent Threats (APTs)</p>
<p>Definition:</p>
<p>An "Advanced Persistent Threat (APT)" in our experimental context is defined as a sophisticated, multi-stage cyber attack campaign characterized by:</p>
<p>• Stealth and Evasion: APT actors employ techniques to avoid detection by standard security measures.</p>
<p>• Persistence: Attackers aim to maintain long-term access to the target network.</p>
<p>• Targeted Objectives: APTs are typically goal-oriented, often focused on espionage, data exfiltration, or strategic disruption.</p>
<p>• Multiple Stages: The campaign involves several phases, such as initial reconnaissance, gaining entry, lateral movement, privilege escalation, establishing command and control (C2), and achieving the final objective.</p>
<p>Execution and Simulation:</p>
<p>Simulating APTs requires modeling these complex, long-duration characteristics:</p>
<p>1. CybORG Simulation Environment: We leveraged CybORG’s capabilities to script multi-stage attack scenarios that mimic APT behavior. This involved chaining multi</p>
<section class="sm xbox font-sm" id="pone.0329969.s001"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334051/bin/pone.0329969.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0329969.s001.docx</a><sup> (2.2MB, docx) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0329969.r003"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0329969.r003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329969.r003</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 1</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Zeashan Khan</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Zeashan Khan</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zeashan Khan</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.e" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.e" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.e" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Zeashan Khan</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.e" class="d-panel p" style="display: none">
<div>© 2025 Zeashan Khan</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>17 Jun 2025</em>
</p>
<p>PONE-D-25-16867R1Federated Hierarchical MARL for Zero-Shot Cyber DefensePLOS ONE</p>
<p>Dear Dr. Alshamrani,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
<p>Please submit your revised manuscript by Aug 01 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span>. When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<p>Please include the following items when submitting your revised manuscript:</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a>.</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Zeashan Hameed Khan, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Journal Requirements:</p>
<p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
<p>Additional Editor Comments:</p>
<p>The paper has significantly improved. However, some minor corrections are still needed.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>

<strong>Comments to the Author</strong>
</p>
<p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.</p>
<p>Reviewer #2: All comments have been addressed</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>2. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #2: Yes</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>3. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #2: No</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #2: No</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #2: Yes</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>6. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p>Reviewer #2: The authors have incorporated my previous comments in this revision. There are a few more comments to add.</p>
<p>1. The fontsize of text shown in tables (3,4,5,6) and figures (4 &amp; 5) should be increased to make them easily readable.</p>
<p>2. Some reference to other relevant works that combines federated and zero-shot learning should also be included.</p>
<p>Reviewer #3: The paper is already revised and the response is acceptable, minor points to address before acceptance please:</p>
<p>1. I have noticed that the paper has one author but the pronouns are all "we" and such, was the manuscript written by a group or a sole author? Please adjust this for the entire paper.</p>
<p>2. The abstract could benefit more from numerical results.</p>
<p>**********</p>
<p>7. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a>). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a>.</p>
<p>Reviewer #2: No</p>
<p>Reviewer #3: <strong>Yes: </strong>Luttfi A. Al-Haddad</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span>. Please note that Supporting Information files do not need this step.</p></section></article><article class="sub-article" id="pone.0329969.r004"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 8;20(8):e0329969. doi: <a href="https://doi.org/10.1371/journal.pone.0329969.r004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329969.r004</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 2</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.f" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.f" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.f" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub4" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.f" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>27 Jun 2025</em>
</p>
<p>Dear Zeashan Hameed Khan,</p>
<p>I sincerely thank you and the reviewers for the time and effort you devoted to evaluating our manuscript, titled " Federated Hierarchical MARL for Zero-Shot Cyber Defense” ONE-D-25-16867.</p>
<p>I appreciate the thoughtful and constructive comments, which have helped us improve the clarity and quality of our work.</p>
<p>I have carefully considered each point raised and revised the manuscript accordingly. Below, I provide a detailed response to each comment. Reviewer comments are listed in black, and my responses are provided directly below each point.</p>
<p>I hope that the revisions and clarifications meet your expectations and respectfully resubmit our manuscript for your reconsideration.</p>
<p>Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p>Reviewer #2: The authors have incorporated my previous comments in this revision. There are a few more comments to add.</p>
<p>1. The fontsize of text shown in tables (3,4,5,6) and figures (4 &amp; 5) should be increased to make them easily readable.</p>
<p>2. Some reference to other relevant works that combines federated and zero-shot learning should also be included.</p>
<p>Reviewer #3: The paper is already revised and the response is acceptable, minor points to address before acceptance please:</p>
<p>1. I have noticed that the paper has one author but the pronouns are all "we" and such, was the manuscript written by a group or a sole author? Please adjust this for the entire paper.</p>
<p>2. The abstract could benefit more from numerical results.</p>
<p>================= ======================== ====================</p>
<p>1. The fontsize of text shown in tables (3,4,5,6) and figures (4 &amp; 5) should be increased to make them easily readable.</p>
<p>Yes, I have changed the size of the font in all tables and figures, hopefully they are now more readable.</p>
<p>2. Some reference to other relevant works that combines federated and zero-shot learning should also be included.</p>
<p>I have added another four recent works as suggested. They are incorporated in the manuscript in section 2.4</p>
<p>\textbf{\textcolor{blue}{Zhou et al. \cite{yang2023adaptive}}} introduced an adaptive federated few-shot learning framework with prototype rectification, demonstrating how semantic prototypes can be aligned across decentralized clients to improve generalization. Though focused on few-shot settings, their prototype correction mechanism offers valuable insights for federated zero-shot adaptation, particularly in scenarios where semantic drift and heterogeneous data distributions hinder model transfer.</p>
<p>\textbf{\textcolor{blue}{Zhao et al. \cite{zhao2022semi}}} introduce a semi‑supervised federated intrusion detection scheme that applies knowledge distillation and voting to cope with non‑IID data and communication constraints. Similarly, the FedGKD approach \textbf{\textcolor{blue}{\cite{zhang2022fedzkt} uses}} global knowledge distillation to address heterogeneity across edge clients in IoT settings. These show how KD can enhance privacy‑preserving and collaborative model learning, and our work extends these ideas by integrating zero or few‑shot generalization within a hierarchical MARL framework</p>
<p>\textbf{\textcolor{blue}{More recently, Wang et al.}} \cite{10.1007/978-3-031-96235-6_24}presented a federated zero‑shot learning framework that uses an LLM to generate privacy-conscious semantic embeddings for unknown attack types. These embeddings are collaboratively shared between clients, enabling zero-day attack detection without exposing raw data. Their approach directly parallels our semantic mapping module and further supports our hierarchical MARL design by validating the feasibility of federated semantic transfer for unseen threats.</p>
<p>================== ========================== ==============</p>
<p>Reviewer #3: The paper is already revised and the response is acceptable, minor points to address before acceptance please:</p>
<p>1. I have noticed that the paper has one author but the pronouns are all "we" and such, was the manuscript written by a group or a sole author? Please adjust this for the entire paper.</p>
<p>I agree to use I instead of we, but I used to use we in academic writing as this is more formal. However, I change it based on you recommendation.</p>
<p>2. The abstract could benefit more from numerical results.</p>
<p>The last part of the abstract has been modified to show and benefit more from the numerical results</p>
<p>The detailed evaluation demonstrates that our approach significantly outperforms existing methods across a range of scenarios. It achieves a high detection rate of 94.2\% for known attacks and 82.7\% for zero-day exploits, while maintaining a low false positive rate of 3.8\%. This robust performance extends to the most sophisticated threats, achieving an 87.3\% containment rate against Advanced Persistent Threats (APTs). The framework's zero-shot capability is underpinned by a semantic mapping accuracy of 89.3\%, which enables rapid adaptation to novel threats. Consequently, the mean response time is reduced by 35\% for known attacks and 42\% for zero-day exploits compared to the best-performing baseline. Furthermore, the federated learning architecture proves highly efficient, reducing communication overhead by 45\% while preserving privacy. These results collectively demonstrate our framework's potential to deliver a new standard of resilient and adaptive cyber defense in complex, distributed environments</p>
<section class="sm xbox font-sm" id="pone.0329969.s002"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers V2.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334051/bin/pone.0329969.s002.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0329969.s002.docx</a><sup> (17.8KB, docx) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0329969.r005"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0329969.r005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329969.r005</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 2</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Zeashan Khan</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Zeashan Khan</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zeashan Khan</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.g" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.g" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.g" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Zeashan Khan</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.g" class="d-panel p" style="display: none">
<div>© 2025 Zeashan Khan</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>16 Jul 2025</em>
</p>
<p>PONE-D-25-16867R2Federated Hierarchical MARL for Zero-Shot Cyber DefensePLOS ONE</p>
<p>Dear Dr. Alshamrani,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
<p>Please submit your revised manuscript by Aug 30 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span>. When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<p>Please include the following items when submitting your revised manuscript:</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a>.</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Zeashan Hameed Khan, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>
<strong>Journal Requirements:</strong>
</p>
<p>1. If the reviewer comments include a recommendation to cite specific previously published works, please review and evaluate these publications to determine whether they are relevant and should be cited. There is no requirement to cite these works unless the editor has indicated otherwise. </p>
<p>2. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
<p>
<strong>Additional Editor Comments:</strong>
</p>
<p>The paper has been improved technically but the write up must be polished by a technical English expert for clarity.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>

<strong>Comments to the Author</strong>
</p>
<p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.</p>
<p>Reviewer #2: All comments have been addressed</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>2. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #2: (No Response)</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>3. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #2: (No Response)</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #2: (No Response)</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #2: (No Response)</p>
<p>Reviewer #3: (No Response)</p>
<p>**********</p>
<p>6. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p><strong>Reviewer #2: </strong>The paper is already revised, and the previous comments are addressed in this revision.</p>
<p><strong>Reviewer #3: </strong>(No Response)</p>
<p>**********</p>
<p>7. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a>). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a>.</p>
<p>Reviewer #2: No</p>
<p>Reviewer #3: No</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span>. Please note that Supporting Information files do not need this step.</p></section></article><article class="sub-article" id="pone.0329969.r006"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 8;20(8):e0329969. doi: <a href="https://doi.org/10.1371/journal.pone.0329969.r006" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329969.r006</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 3</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.h" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.h" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.h" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub6" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.h" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>18 Jul 2025</em>
</p>
<p>Journal Requirements:</p>
<p>1. If the reviewer comments include a recommendation to cite specific previously published works, please review and evaluate these publications to determine whether they are relevant and should be cited. There is no requirement to cite these works unless the editor has indicated otherwise.</p>
<p>2. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
<p>I have checked each of the 44 references in the manuscript. The majority of the citations are correct and refer to established, non-retracted works. However, I identified a few areas that require complete and accurate referencing. I thank you for pointing out this.</p>
<p>Here are the specific references that need to be updated:</p>
<p>• Reference: Nguyen TT, Reddi VJ.</p>
<p>o Issue: The publication year is listed as 2019, but the correct year is 2020.</p>
<p>o Correction: Nguyen TT, Reddi VJ. Deep reinforcement learning for cyber security. IEEE Transactions on Neural Networks and Learning Systems. 2020;31(7):2669-2683.</p>
<p>• Reference: Nguyen TT, Reddi VJ.</p>
<p>o Issue: This is a duplicate of reference, all in-text citations ( [2] and [11]) and also contains the incorrect year.</p>
<p>o Correction: This reference has been corrected and now the corresponding text also updated and cited as one reference [2]. And also updated to the correct 2020 publication year.</p>
<p>• This is reference [2] and already updated in the manuscript.</p>
<p>• Reference: Alshamrani A, Alshahrani A.</p>
<p>o Issue: The reference is incomplete and missing the page ranges.</p>
<p>o Correction: Alshamrani A, Alshahrani A. Adaptive Cyber Defense Technique Based on Multiagent Reinforcement Learning Strategies. Intelligent Automation &amp; Soft Computing. 2023;36(3):2539-2555.</p>
<p>Additional Editor Comments:</p>
<p>The paper has been improved technically but the write up must be polished by a technical English expert for clarity.</p>
<p>I have asked an expert to help on checking and revising my article and based on her recommendation, I have changed the manuscript and highlighted the changes in RED and also some words are removed but for easily tracking them, I have used Strikethrough text in the file “version 3 with modifications”. However, the clean version is also provided.</p>
<p>There is one point I would like to mention here, the English expert asked me to change “I” where possible to “we”, however, I proceed using the first-person singular ("I") as directed by one the reviewer in the previous revision of the article.</p>
<section class="sm xbox font-sm" id="pone.0329969.s003"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers V3.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334051/bin/pone.0329969.s003.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0329969.s003.pdf</a><sup> (91.6KB, pdf) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0329969.r007"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0329969.r007" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329969.r007</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 3</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Zeashan Khan</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Zeashan Khan</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zeashan Khan</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.i" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.i" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.i" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Zeashan Khan</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.i" class="d-panel p" style="display: none">
<div>© 2025 Zeashan Khan</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>24 Jul 2025</em>
</p>
<p>Federated Hierarchical MARL for Zero-Shot Cyber Defense</p>
<p>PONE-D-25-16867R3</p>
<p>Dear Dr. Alshamrani,</p>
<p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
<p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
<p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Editorial Manager®</a> and clicking the ‘Update My Information' link at the top of the page. For questions related to billing, please contact <a href="https://plos.my.site.com/s/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">billing support</a>.</p>
<p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <span>onepress@plos.org</span>.</p>
<p>Kind regards,</p>
<p>Zeashan Hameed Khan, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Additional Editor Comments (optional):</p>
<p>The paper has been significantly improved and the necessary language correction has been carried out. Therefore, it can be accepted in the present form.</p>
<p>Reviewers' comments:</p></section></article><article class="sub-article" id="pone.0329969.r008"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0329969.r008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329969.r008</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Acceptance letter</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Zeashan Khan</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Zeashan Khan</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zeashan Khan</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.j" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.j" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.j" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Zeashan Khan</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.j" class="d-panel p" style="display: none">
<div>© 2025 Zeashan Khan</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>PONE-D-25-16867R3</p>
<p>PLOS ONE</p>
<p>Dear Dr. Alshamrani,</p>
<p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p>
<p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p>
<p>* All references, tables, and figures are properly cited</p>
<p>* All relevant supporting information is included in the manuscript submission,</p>
<p>* There are no issues that prevent the paper from being properly typeset</p>
<p>You will receive further instructions from the production team, including instructions on how to review your proof when it is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p>
<p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <span>onepress@plos.org</span>.</p>
<p>You will receive an invoice from PLOS for your publication fee after your manuscript has reached the completed accept phase. If you receive an email requesting payment before acceptance or for any other service, this may be a phishing scheme. Learn how to identify phishing emails and protect your accounts at <a href="https://explore.plos.org/phishing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://explore.plos.org/phishing</a>.</p>
<p>If we can help with anything else, please email us at <span>customercare@plos.org</span>.</p>
<p>Thank you for submitting your work to PLOS ONE and supporting open access.</p>
<p>Kind regards,</p>
<p>PLOS ONE Editorial Office Staff</p>
<p>on behalf of</p>
<p>Dr. Zeashan Hameed Khan</p>
<p>Academic Editor</p>
<p>PLOS ONE</p></section></article><article class="sub-article" id="_ad93_"><section class="pmc-layout__citation font-secondary font-xs"><div></div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Associated Data</h1></hgroup><ul class="d-buttons inline-list"></ul>
<div class="d-panels font-secondary-light"></div>
<div></div>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
</div></section><section class="body sub-article-body"><section id="_adsm93_" lang="en" class="supplementary-materials"><h2 class="pmc_sec_title">Supplementary Materials</h2>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334051/bin/pone.0329969.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0329969.s001.docx</a><sup> (2.2MB, docx) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers V2.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334051/bin/pone.0329969.s002.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0329969.s002.docx</a><sup> (17.8KB, docx) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material3_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers V3.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334051/bin/pone.0329969.s003.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0329969.s003.pdf</a><sup> (91.6KB, pdf) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h2 class="pmc_sec_title">Data Availability Statement</h2>
<p>Data cannot be shared publicly because of access restriction from the owner (DRPA). Data are available from the DARPA upon request for research purposes. However, it can be requested through the connect form <a href="https://www.darpa.mil/about/darpaconnect" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.darpa.mil/about/darpaconnect</a> OR through the email direct, Key Contact Information: DARPA Connect: <span>darpaconnect@darpa.mil</span> FOIA Office: (571) 372-0435.</p></section></section></article><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0329969"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0329969.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (5.1 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12334051/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12334051/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12334051%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334051/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12334051/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12334051/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40779609/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12334051/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40779609/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12334051/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12334051/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="Gz6c25NPxVyCkVXHSTwbLEWn8MvcZyKyVRMytv20aApHSI8ZY7N9PyeFvm3kWqVt">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Web Policies

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    FOIA

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS Vulnerability Disclosure

    
</a>

                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Help

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Accessibility

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Careers

    
</a>

                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NLM

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NIH

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    USA.gov

    
</a>

                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-370d5dd6.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-917ba005.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-d9849939.js"></script>
    
    

    </body>
</html>
