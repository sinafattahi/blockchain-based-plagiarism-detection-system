
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-a68b4900.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-0a3f24ce.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            LGD_Net: Capsule network with extreme learning machine for classification of lung diseases using CT scans - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="FF2083E989B770231A83E9000E5A143D.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334061/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="LGD_Net: Capsule network with extreme learning machine for classification of lung diseases using CT scans">
<meta name="citation_author" content="Ali Haider Khan">
<meta name="citation_author_institution" content="College of Computer Science, Beijing University of Technology, Beijing, China">
<meta name="citation_author" content="Jianqiang Li">
<meta name="citation_author_institution" content="College of Computer Science, Beijing University of Technology, Beijing, China">
<meta name="citation_author" content="Muhammad Nabeel Asghar">
<meta name="citation_author_institution" content="Department of Information Systems, College of Computer Sciences and Information Technology, King Faisal University, Al Hofuf, Saudi Arabia">
<meta name="citation_author" content="Sajid Iqbal">
<meta name="citation_author_institution" content="Department of Information Systems, College of Computer Sciences and Information Technology, King Faisal University, Al Hofuf, Saudi Arabia">
<meta name="citation_publication_date" content="2025 Aug 8">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0327419">
<meta name="citation_doi" content="10.1371/journal.pone.0327419">
<meta name="citation_pmid" content="40779565">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334061/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334061/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334061/pdf/pone.0327419.pdf">
<meta name="description" content="Lung diseases (LGDs) are related to an extensive range of lung disorders, including pneumonia (PNEUM), lung cancer (LC), tuberculosis (TB), and COVID-19 etc. The diagnosis of LGDs is performed by using different medical imaging such as X-rays, CT ...">
<meta name="og:title" content="LGD_Net: Capsule network with extreme learning machine for classification of lung diseases using CT scans">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Lung diseases (LGDs) are related to an extensive range of lung disorders, including pneumonia (PNEUM), lung cancer (LC), tuberculosis (TB), and COVID-19 etc. The diagnosis of LGDs is performed by using different medical imaging such as X-rays, CT ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334061/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-testid="header" data-header >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            
                <a class="ncbi-header__logo-container" href="https://www.ncbi.nlm.nih.gov/">
                    <img alt="
                                  NCBI home page
                              "
                         class="ncbi-header__logo-image"
                         src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg" />
                </a>
            

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-testid="searchPanel"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      aria-describedby="search-field-desktop-navigation-help-text"
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only"
                           data-testid="label"
                           for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           data-testid="textInput"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    data-testid="button"
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  role="search">
                <label class="usa-sr-only" for="search-field">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="search" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        <a class="usa-link" href="https://www.ncbi.nlm.nih.gov/pmc/advanced/" data-ga-action="featured_link" data-ga-label="advanced_search">
                            Advanced Search
                        </a>
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12334061">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0327419"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0327419.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12334061%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12334061/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12334061/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334061/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0327419" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 8;20(8):e0327419. doi: <a href="https://doi.org/10.1371/journal.pone.0327419" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0327419</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>LGD_Net: Capsule network with extreme learning machine for classification of lung diseases using CT scans</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20AH%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Ali Haider Khan</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Ali Haider Khan</span></h3>
<div class="p">
<sup>1</sup>College of Computer Science, Beijing University of Technology, Beijing, China</div>
<div>Conceptualization, Data curation, Formal analysis, Writing – original draft</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khan%20AH%22%5BAuthor%5D" class="usa-link"><span class="name western">Ali Haider Khan</span></a>
</div>
</div>
<sup>1,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Jianqiang Li</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Jianqiang Li</span></h3>
<div class="p">
<sup>1</sup>College of Computer Science, Beijing University of Technology, Beijing, China</div>
<div>Conceptualization, Methodology</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Jianqiang Li</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Asghar%20MN%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Muhammad Nabeel Asghar</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Muhammad Nabeel Asghar</span></h3>
<div class="p">
<sup>2</sup>Department of Information Systems, College of Computer Sciences and Information Technology, King Faisal University, Al Hofuf, Saudi Arabia</div>
<div>Conceptualization, Supervision</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Asghar%20MN%22%5BAuthor%5D" class="usa-link"><span class="name western">Muhammad Nabeel Asghar</span></a>
</div>
</div>
<sup>2,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Iqbal%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Sajid Iqbal</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Sajid Iqbal</span></h3>
<div class="p">
<sup>2</sup>Department of Information Systems, College of Computer Sciences and Information Technology, King Faisal University, Al Hofuf, Saudi Arabia</div>
<div>Conceptualization, Formal analysis, Funding acquisition</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Iqbal%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Sajid Iqbal</span></a>
</div>
</div>
<sup>2</sup>
</div>
<div class="cg p">Editor: <span class="name western">Hadeel K Aljobouri,</span><sup>3</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>College of Computer Science, Beijing University of Technology, Beijing, China</div>
<div id="aff002">
<sup>2</sup>Department of Information Systems, College of Computer Sciences and Information Technology, King Faisal University, Al Hofuf, Saudi Arabia</div>
<div id="edit1">
<sup>3</sup>Al-Nahrain University, IRAQ</div>
<div class="author-notes p">
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>ali.khan@bjut.edu.cn</span> (AHK); <span>masghar@kfu.edu.sa</span> (MNA)</p>
</div>
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>The authors have declared that no competing interests exist.</p></div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Ali Haider Khan</span></strong>: <span class="role">Conceptualization, Data curation, Formal analysis, Writing – original draft</span>
</div>
<div>
<strong class="contrib"><span class="name western">Jianqiang Li</span></strong>: <span class="role">Conceptualization, Methodology</span>
</div>
<div>
<strong class="contrib"><span class="name western">Muhammad Nabeel Asghar</span></strong>: <span class="role">Conceptualization, Supervision</span>
</div>
<div>
<strong class="contrib"><span class="name western">Sajid Iqbal</span></strong>: <span class="role">Conceptualization, Formal analysis, Funding acquisition</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Hadeel K Aljobouri,</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Nov 27; Accepted 2025 Jun 15; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Khan et al</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12334061  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40779565/" class="usa-link">40779565</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>Lung diseases (LGDs) are related to an extensive range of lung disorders, including pneumonia (PNEUM), lung cancer (LC), tuberculosis (TB), and COVID-19 etc. The diagnosis of LGDs is performed by using different medical imaging such as X-rays, CT scans, and MRI. However, LGDs contain similar symptoms such as fever, cough, and sore throat, making it challenging for radiologists to classify these LGDs. If LGDs are not diagnosed at their initial phase, they may produce severe complications or even death. An automated classifier is required for the classification of LGDs. Thus, this study aims to propose a novel model named lung diseases classification network (LGD_Net) based on the combination of a capsule network (CapsNet) with the extreme learning machine (ELM) for the classification of five different LGDs such as PNEUM, LC, TB, COVID-19 omicron (COO), and normal (NOR) using CT scans. The LGD_Net model is trained and tested on the five publicly available benchmark datasets. The datasets contain an imbalanced distribution of images; therefore, a borderline SMOTE (BL_SMT) approach is applied to handle this problem. Additionally, the affine transformation methods are used to enhance LGD datasets. The performance of the LGD_Net is compared with four CNN-based baseline models such as Vgg-19 (D<sup>1</sup>), ResNet-101 (D<sup>2</sup>), Inception-v3 (D<sup>3</sup>), and DenseNet-169 (D<sup>4</sup>). The LGD_Net model achieves an accuracy of 99.71% in classifying LGDs using CT scans. While the other models such as D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> attains an accuracy of 91.21%, 94.39%, 93.96%, and 93.82%, respectively. The findings demonstrate that the LGD_Net model works significantly as compared to D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> as well as state-of-the-art (SOTA). Thus, this study concludes that the LGD_Net model provides significant assistance to radiologists in classifying several LGDs.</p></section><section id="sec001"><h2 class="pmc_sec_title">1. Introduction</h2>
<p>Lung diseases (LGDs) such as pneumonia (PNEUM), tuberculosis (TB), and COVID-19, etc., are infectious or transmissible diseases that can travel from one person to another [<a href="#pone.0327419.ref001" class="usa-link" aria-describedby="pone.0327419.ref001">1</a>–<a href="#pone.0327419.ref004" class="usa-link" aria-describedby="pone.0327419.ref004">4</a>]. These diseases are caused by numerous agents of infection, the most common of which are bacteria, fungi, and viruses, amongst others. The bacteria that triggered the infection [<a href="#pone.0327419.ref002" class="usa-link" aria-describedby="pone.0327419.ref002">2</a>], these symptoms can vary significantly. Some infections cause serious threats to one’s life, but many other infections do not. In December 2019, a new coronavirus known as COVID-19 first manifested itself in Wuhan, China, as an outbreak of fatal and severe infections, and it quickly spread throughout the country [<a href="#pone.0327419.ref001" class="usa-link" aria-describedby="pone.0327419.ref001">1</a>]. The COVID-19 is the pathogen that leads to the sickness. On March 11, 2020, the WHO designated COVID-19 as a global epidemic [<a href="#pone.0327419.ref002" class="usa-link" aria-describedby="pone.0327419.ref002">2</a>,<a href="#pone.0327419.ref003" class="usa-link" aria-describedby="pone.0327419.ref003">3</a>]. The COVID-19 pandemic resulted in the infection of millions of people all over the world [<a href="#pone.0327419.ref004" class="usa-link" aria-describedby="pone.0327419.ref004">4</a>–<a href="#pone.0327419.ref007" class="usa-link" aria-describedby="pone.0327419.ref007">7</a>]. Medical imaging techniques, including CXR, ultrasonography, and CT scans, have emerged as promising diagnostic tools for LGDs.</p>
<p>Malaise, migraine, headache, difficulty breathing, difficulty breathing, muscular soreness, parched mouth, backache, vomiting, and abdominal cramps were some of the clinical symptoms that were shown in the early stages of COVID-19 [<a href="#pone.0327419.ref008" class="usa-link" aria-describedby="pone.0327419.ref008">8</a>–<a href="#pone.0327419.ref010" class="usa-link" aria-describedby="pone.0327419.ref010">10</a>]. The absence of taste and fragrance was one of the most often-seen symptoms of COVID-19 [<a href="#pone.0327419.ref010" class="usa-link" aria-describedby="pone.0327419.ref010">10</a>]. Governments and regulatory bodies around the world have implemented a strict lockdown to maintain social isolation and stop the epidemic’s spread as daily reports of new breakouts have been arriving at an unprecedented rate [<a href="#pone.0327419.ref011" class="usa-link" aria-describedby="pone.0327419.ref011">11</a>]. The countries that have been hit the worst by COVID-19 have instituted travel and transit bans in trying to stop the virus from spreading further. In light of the current public health crisis, the medical community was making concerted efforts to identify and implement innovative methods, technologies, and approaches that allow them to monitor and curb the development of the COVID-19 outbreak [<a href="#pone.0327419.ref012" class="usa-link" aria-describedby="pone.0327419.ref012">12</a>]. AI [<a href="#pone.0327419.ref013" class="usa-link" aria-describedby="pone.0327419.ref013">13</a>] is now one of the most effective approaches that track the progress of COVID-19, assess its risk and severity [<a href="#pone.0327419.ref014" class="usa-link" aria-describedby="pone.0327419.ref014">14</a>], and predict its rate of increase.</p>
<p>In the beginning, an assay known as RT-PCR was utilized to identify COVID-19 [<a href="#pone.0327419.ref015" class="usa-link" aria-describedby="pone.0327419.ref015">15</a>]. To carry out this test, it is necessary to first extract the infected person’s DNA using reverse transcription. This DNA is then passed on to PCR to undergo DNA amplification and analysis. Because only RNA sequences are conveyed by this virus, it is, therefore, able to detect COVID-19 [<a href="#pone.0327419.ref016" class="usa-link" aria-describedby="pone.0327419.ref016">16</a>]. However, the PCR test does have certain drawbacks and has false-positive and false-negative outputs [<a href="#pone.0327419.ref017" class="usa-link" aria-describedby="pone.0327419.ref017">17</a>]. The diagnosis of COVID-19 at its initial phase is challenging for radiologists because the symptoms of COVID-19 are similar to LGDs. Recent studies are seeking to devise a categorization system that is both useful and effective for patients who are infected with COVID-19 and other LGDs. For the identification of the LGDs, CT scans are used [<a href="#pone.0327419.ref018" class="usa-link" aria-describedby="pone.0327419.ref018">18</a>]. By examining specific features, such as opacity or unclear patterns in the airways, CT imaging can play a significant role in the diagnosis of LGDs and COVID-19.</p>
<p>Deep learning (DL) models have opened a new door that helps in diagnosing several diseases [<a href="#pone.0327419.ref019" class="usa-link" aria-describedby="pone.0327419.ref019">19</a>]. The healthcare system has made effective diagnostics advancements due to the collaboration of CNNs. These advancements include the detection of chest diseases [<a href="#pone.0327419.ref020" class="usa-link" aria-describedby="pone.0327419.ref020">20</a>], the identification of cancer cells [<a href="#pone.0327419.ref021" class="usa-link" aria-describedby="pone.0327419.ref021">21</a>], the classification of brain and breast tumors [<a href="#pone.0327419.ref022" class="usa-link" aria-describedby="pone.0327419.ref022">22</a>], and the analysis of genomic sequences [<a href="#pone.0327419.ref023" class="usa-link" aria-describedby="pone.0327419.ref023">23</a>]. For this study, a novel model named lung diseases classification network (LGD_Net) based on the combination of a capsule network (CapsNet) [<a href="#pone.0327419.ref024" class="usa-link" aria-describedby="pone.0327419.ref024">24</a>] with the extreme learning machine (ELM) [<a href="#pone.0327419.ref025" class="usa-link" aria-describedby="pone.0327419.ref025">25</a>] for the classification of five different LGDs such as pneumonia (PNEUM), lung cancer (LC), tuberculosis (TB), COVID-19 omicron (COO), and normal (NOR) using CT scans. Additionally, we segmented CT scan images using a convolutional-deconvolutional capsule network called SegCaps [<a href="#pone.0327419.ref026" class="usa-link" aria-describedby="pone.0327419.ref026">26</a>] and a CapsNet [<a href="#pone.0327419.ref027" class="usa-link" aria-describedby="pone.0327419.ref027">27</a>] with ELM that had been trained for enhanced generalization. The reason for choosing CapsNet is that traditional CNN uses a pooling layer that loses the spatial relationship between features [<a href="#pone.0327419.ref028" class="usa-link" aria-describedby="pone.0327419.ref028">28</a>–<a href="#pone.0327419.ref030" class="usa-link" aria-describedby="pone.0327419.ref030">30</a>], however, the CapsNet encodes both the probability of features [<a href="#pone.0327419.ref031" class="usa-link" aria-describedby="pone.0327419.ref031">31</a>] and their pose such as orientation, scale, and position. The five publicly available benchmark datasets are used for training and testing the LGD_Net model. The imbalance distribution of LGD images in datasets is handled by using borderline SMOTE (BL-SMT) [<a href="#pone.0327419.ref032" class="usa-link" aria-describedby="pone.0327419.ref032">32</a>]. Additionally, affine transformation methods are also used to enhance the size of the dataset. To the best of our knowledge, this is primary work for the classification of PNEUM, LC, TB, COO, and NOR using CT scans. It alleviates some of the pressure placed on medical professionals to apply various kinds of tests to diagnose each chest condition individually. Additionally, the LGD_Net model is also compared with four state-of-the-art (SOTA) methods such as Vgg-19 (D<sup>1</sup>) [<a href="#pone.0327419.ref028" class="usa-link" aria-describedby="pone.0327419.ref028">28</a>], ResNet-101 (D<sup>2</sup>) [<a href="#pone.0327419.ref029" class="usa-link" aria-describedby="pone.0327419.ref029">29</a>], Inception-v3 (D<sup>3</sup>), and DenseNet-169 (D<sup>4</sup>) [<a href="#pone.0327419.ref030" class="usa-link" aria-describedby="pone.0327419.ref030">30</a>]. The major contribution of this work is presented below:</p>
<ol class="list" style="list-style-type:decimal">
<li><p>A novel LGD_Net model, which is based on the combination of CapsNet and ELM for the classification of LGDs using CT scans. Additionally, SegCaps is used with the LGD_Net to perform the segmentation process.</p></li>
<li><p>The imbalance distribution of LGD images in each class of the dataset is balanced by using BL-SMT. Additionally, an affine transformation method is applied to enhance the size of the datasets.</p></li>
<li><p>The GRAD-CAM highlights the infected area of LGDs by using the proposed LGD_Net model.</p></li>
<li><p>In this present work, we also perform the ablation study to demonstrate the efficacy of the LGD_Net.</p></li>
<li><p>The performance of the proposed LGD_Net model is compared with four baseline models such as D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> in terms of many metrics such as accuracy (Z<sup>1</sup>), precision (Z<sup>2</sup>), recall (Z<sup>3</sup>), F1-score (Z<sup>4</sup>), dice similarity coefficient (Z<sup>5</sup>), and an AUC (Z<sup>6</sup>).</p></li>
<li><p>The proposed LGD_Net model attains the classification accuracy of 99.17% which is superior to baseline models and SOTA methods.</p></li>
</ol>
<p>This study is divided into the following sections: Section II presents the modern literature on LGDs using different medical imaging modalities. The dataset description, dataset preprocessing, and proposed LGD_Net model are discussed in section III. The findings of the present study are discussed in section IV. The conclusion and future work are described in section V.</p></section><section id="sec002"><h2 class="pmc_sec_title">2. Literature review</h2>
<p>Recent studies used different imaging methods with AI for the classification of LGDs such as PNEUM, TB, LC, etc. <a href="#pone.0327419.t001" class="usa-link">Table 1</a> presents the latest studies that use AI approaches for the diagnosis of LGDs. Wang et al., [<a href="#pone.0327419.ref031" class="usa-link" aria-describedby="pone.0327419.ref031">31</a>] designed a novel TL-based model for the identification of COVID-19 and PNEUM using CXR. Before training their model, they applied several image enhancement methods to prevent their model from overfitting. They achieved an accuracy of 94.52%.</p>
<section class="tw xbox font-sm" id="pone.0327419.t001"><h3 class="obj_head">Table 1. Comparative analysis of AI methods used by recent studies for the classification of LGDs using different medical imaging.</h3>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">Ref</th>
<th align="left" rowspan="2" colspan="1">Year</th>
<th align="left" rowspan="2" colspan="1">Models</th>
<th align="left" colspan="2" rowspan="1">Image Type</th>
<th align="left" rowspan="1" colspan="1">Disease Classification</th>
<th align="left" rowspan="2" colspan="1">No. of Diseases</th>
<th align="left" rowspan="1" colspan="1">Accuracy</th>
</tr>
<tr>
<th align="left" rowspan="1" colspan="1">CT scans</th>
<th align="left" rowspan="1" colspan="1">X-Rays</th>
<th align="left" rowspan="1" colspan="1"></th>
<th align="left" rowspan="1" colspan="1"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref033" class="usa-link" aria-describedby="pone.0327419.ref033">33</a>]</td>
<td align="left" rowspan="1" colspan="1">2025</td>
<td align="left" rowspan="1" colspan="1">CTCovid19</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 &amp; NOR.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">97.00%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref034" class="usa-link" aria-describedby="pone.0327419.ref034">34</a>]</td>
<td align="left" rowspan="1" colspan="1">2025</td>
<td align="left" rowspan="1" colspan="1">VGG19  +  Attention CNN</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 &amp; NOR.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">97.19%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref001" class="usa-link" aria-describedby="pone.0327419.ref001">1</a>]</td>
<td align="left" rowspan="1" colspan="1">2024</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">PNEUM, LC &amp; NOR</td>
<td align="left" rowspan="1" colspan="1">03</td>
<td align="left" rowspan="1" colspan="1">96.35%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref002" class="usa-link" aria-describedby="pone.0327419.ref002">2</a>]</td>
<td align="left" rowspan="1" colspan="1">2024</td>
<td align="left" rowspan="1" colspan="1">FL-DCNN</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 &amp; NOR</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">96.23%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref003" class="usa-link" aria-describedby="pone.0327419.ref003">3</a>]</td>
<td align="left" rowspan="1" colspan="1">2024</td>
<td align="left" rowspan="1" colspan="1">02-Layers CNN</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">PNEUM, TB, &amp; NOR</td>
<td align="left" rowspan="1" colspan="1">03</td>
<td align="left" rowspan="1" colspan="1">94.69%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref035" class="usa-link" aria-describedby="pone.0327419.ref035">35</a>]</td>
<td align="left" rowspan="1" colspan="1">2022</td>
<td align="left" rowspan="1" colspan="1">CDC_Net</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">COVID-19, PNEUM, &amp; NOR.</td>
<td align="left" rowspan="1" colspan="1">03</td>
<td align="left" rowspan="1" colspan="1">97.99%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref036" class="usa-link" aria-describedby="pone.0327419.ref036">36</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">CapsNet with IELM</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 &amp; NOR.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">98.00%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref037" class="usa-link" aria-describedby="pone.0327419.ref037">37</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 and NOR.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">96.00%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref038" class="usa-link" aria-describedby="pone.0327419.ref038">38</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">Fuzzy Logic</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">COVID-19, PNEUM, and NOR</td>
<td align="left" rowspan="1" colspan="1">03</td>
<td align="left" rowspan="1" colspan="1">81.01%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref039" class="usa-link" aria-describedby="pone.0327419.ref039">39</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 and non-COVID-19.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">98.00%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref040" class="usa-link" aria-describedby="pone.0327419.ref040">40</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 and NOR.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">96.00%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref041" class="usa-link" aria-describedby="pone.0327419.ref041">41</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">FL + CNN</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 and NOR.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">95.21%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref042" class="usa-link" aria-describedby="pone.0327419.ref042">42</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">Random Forest</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 and NOR.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">81.80%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref043" class="usa-link" aria-describedby="pone.0327419.ref043">43</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">ResNet-18</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 and non-COVID-19.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">97.78%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref044" class="usa-link" aria-describedby="pone.0327419.ref044">44</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">COVID-19 and non-COVID-19.</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">94.00%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>Ioannis et al., [<a href="#pone.0327419.ref045" class="usa-link" aria-describedby="pone.0327419.ref045">45</a>] developed a CNN based on COVID-19 and PNEUM classification. They performed two experiments, firstly, they applied their model for binary classification such as COVID-19 and NOR cases. Secondly, they used their model for multi-classification such as COVID-19, NOR, and PNEUM. They attained an accuracy of 98.75% and 93.48% for binary and multiclass classification, respectively. The CoVNet was designed by [<a href="#pone.0327419.ref046" class="usa-link" aria-describedby="pone.0327419.ref046">46</a>] for LGD classification and they achieved a classification accuracy of 98.28%.</p>
<p>A CDC_Net model was designed by Malik et al., [<a href="#pone.0327419.ref035" class="usa-link" aria-describedby="pone.0327419.ref035">35</a>] for the classification of several LGDs i.e., LC, PNEUM, and COVID-19 using LGD CXR. The CDC_Net contains residual thoughts and they achieved an accuracy rate of 97.99%. The two models, i.e., Vgg-16 and ResNet-34 were used by Choudary et al., [<a href="#pone.0327419.ref047" class="usa-link" aria-describedby="pone.0327419.ref047">47</a>] for LGDs classification. The ResNet-34 attained the highest classification accuracy of 95.74% which is superior to Vgg-16. Goyal et al., [<a href="#pone.0327419.ref048" class="usa-link" aria-describedby="pone.0327419.ref048">48</a>] fined-tuned the Inception-v3 for the detection of LGDs by using CT scan images. Their model achieved an accuracy of 92.63%. The BDCNet model was made by Malik et al., [<a href="#pone.0327419.ref049" class="usa-link" aria-describedby="pone.0327419.ref049">49</a>] for the diagnosis of several lung disorders by using CXR. They applied several augmentation methods and attained the F1 score of 97.21%.</p>
<p>Another study by Malik et al., [<a href="#pone.0327419.ref050" class="usa-link" aria-describedby="pone.0327419.ref050">50</a>] designed a DMFL_Net with the combination of FL &amp; CNN for the classification of LGDs. Additionally, their objective is to handle patient data privacy with LGD classification and they achieved significant outcomes in identifying lung disorders.</p>
<p>Sufian et al., [<a href="#pone.0327419.ref051" class="usa-link" aria-describedby="pone.0327419.ref051">51</a>] compared the performance of 06 TL models for the classification of LGDs using CT scans. They used the HE method to enhance the contrast of CT scans. Additionally, the data augmentation methods were also used before training the TL models. The highest accuracy of 96.78% was achieved by the InceptionNet-v3 model.</p>
<p>The study [<a href="#pone.0327419.ref052" class="usa-link" aria-describedby="pone.0327419.ref052">52</a>] proposed a MobileNet with the amalgamation of residual DCNN for the classification of lung diseases using CT scans. They attained a remarkable accuracy of 94.48%.</p>
<p>A ResNet model was used by the study [<a href="#pone.0327419.ref053" class="usa-link" aria-describedby="pone.0327419.ref053">53</a>] for the detection of lung disorders using CXR. They trained and tested the ResNet model on 02 public datasets. Additionally, 70% of training data were used for training, and the rest CXR data were used for testing the model. The ResNet model gained an accuracy of 97.00% in the classification of LGDs.</p>
<p>In [<a href="#pone.0327419.ref054" class="usa-link" aria-describedby="pone.0327419.ref054">54</a>], they ensembled the Vgg-16 with DenseNet-169 for LGD classification using CXR and CT scans. Their model achieved a recall of 95.50%. Malik et al., [<a href="#pone.0327419.ref036" class="usa-link" aria-describedby="pone.0327419.ref036">36</a>] proposed a CNN model for the diagnosis of COVID-19. They also applied BCT and FL to secure the patient data at the time of training. The remarkable results of 98.00% were achieved in terms of accuracy. Additionally, the study [<a href="#pone.0327419.ref055" class="usa-link" aria-describedby="pone.0327419.ref055">55</a>] also used the adversarial model to annotate lung disease using a CT scan.</p>
<p>Malik et al., [<a href="#pone.0327419.ref056" class="usa-link" aria-describedby="pone.0327419.ref056">56</a>] used the CNN model for the classification of several chest disorders. Initially, the SURF and ORB were applied to extract the features, and then these features were fed to the proposed CNN model. The ORB + Vgg-19 model attained a significant precision value of 97.15%. Kermany et al., [<a href="#pone.0327419.ref057" class="usa-link" aria-describedby="pone.0327419.ref057">57</a>] applied the InceptionNet-v3 model for the diagnosis of PNEUM using CXR. The InceptionNet-v3 model attained an accuracy of 92.80% in finding the PNEUM.</p>
<p>The lung disorders identified by ResNet-50 in the study [<a href="#pone.0327419.ref058" class="usa-link" aria-describedby="pone.0327419.ref058">58</a>]. The 5-fold cross-validation method was used to train the ResNet-50 model and achieved the appropriate accuracy of 98.00%. Furthermore, the 3D CT scans were used by Zheng et al [<a href="#pone.0327419.ref059" class="usa-link" aria-describedby="pone.0327419.ref059">59</a>] for training their proposed ResNet-18 [<a href="#pone.0327419.ref013" class="usa-link" aria-describedby="pone.0327419.ref013">13</a>] model to identify COVID-19 and pneumonia. They achieved the classification accuracy of 95.60%,</p>
<p>In [<a href="#pone.0327419.ref060" class="usa-link" aria-describedby="pone.0327419.ref060">60</a>], they proposed a three-layer CNN model for the PNEUM classification from NOR cases. They compared the outcomes of the three layers of the CNN model with ResNet-18, DenseNet-169, and Vgg-16. Additionally, a small dataset of lung diseases was used for training the proposed model and they achieved the classification accuracy of 92.40% in the identification of pneumonia.</p>
<p>Bernheim et al. [<a href="#pone.0327419.ref061" class="usa-link" aria-describedby="pone.0327419.ref061">61</a>] developed a ResNet-18 model with SVM for the identification of PNEUM. Initially, they applied the ResNet-18 for the identification of lung disorders. After that, they fused the SVM with the last convolutional layers of ResNet-18 for the classification of LGDs. The ResNet-18 model with SVM achieved a classification precision of 95.30%.</p>
<p>The classification of PNEUM and NOR cases were performed by Harmon et al., [<a href="#pone.0327419.ref062" class="usa-link" aria-describedby="pone.0327419.ref062">62</a>] by using DenseNet-121. They trained and tested the DenseNet-121 model on several publicly available databases of LGDs. They achieved significant results in the identification of LGDs.</p>
<p>The work [<a href="#pone.0327419.ref034" class="usa-link" aria-describedby="pone.0327419.ref034">34</a>] used the ResNet-18 model for the identification of viral PNEUM, NOR, and COVID-19 using 3D scan slices. The 3D was applied to train and test the ResNet-18 model. Their performance was compared with Vgg-16, DenseNet, and InceptionNet. The ResNet-18 outperforms these models and obtained an accuracy of 96.57%.</p>
<p>Another study was conducted by Bhandary et al., [<a href="#pone.0327419.ref063" class="usa-link" aria-describedby="pone.0327419.ref063">63</a>] for the classification of LGDs by combination of AlexNet and SVM. They applied the image enhancement techniques to enhance the contrast of the CXR images. Then, they applied data augmentation before training the proposed model. Their AlexNet and SVM models attained an accuracy of 95.63%.</p>
<p>Previous studies [<a href="#pone.0327419.ref008" class="usa-link" aria-describedby="pone.0327419.ref008">8</a>–<a href="#pone.0327419.ref013" class="usa-link" aria-describedby="pone.0327419.ref013">13</a>, <a href="#pone.0327419.ref015" class="usa-link" aria-describedby="pone.0327419.ref015">15</a>, <a href="#pone.0327419.ref017" class="usa-link" aria-describedby="pone.0327419.ref017">17</a>–<a href="#pone.0327419.ref019" class="usa-link" aria-describedby="pone.0327419.ref019">19</a>, <a href="#pone.0327419.ref021" class="usa-link" aria-describedby="pone.0327419.ref021">21</a>] employed DL models for the classification of COVID-19 and NOR cases using CT scans. Additionally, the studies [<a href="#pone.0327419.ref022" class="usa-link" aria-describedby="pone.0327419.ref022">22</a>–<a href="#pone.0327419.ref026" class="usa-link" aria-describedby="pone.0327419.ref026">26</a>, <a href="#pone.0327419.ref028" class="usa-link" aria-describedby="pone.0327419.ref028">28</a>–<a href="#pone.0327419.ref031" class="usa-link" aria-describedby="pone.0327419.ref031">31</a>, <a href="#pone.0327419.ref046" class="usa-link" aria-describedby="pone.0327419.ref046">46</a>] also used CT scan images for the identification of PNEUM. Several studies [<a href="#pone.0327419.ref001" class="usa-link" aria-describedby="pone.0327419.ref001">1</a>–<a href="#pone.0327419.ref003" class="usa-link" aria-describedby="pone.0327419.ref003">3</a>, <a href="#pone.0327419.ref005" class="usa-link" aria-describedby="pone.0327419.ref005">5</a>, <a href="#pone.0327419.ref023" class="usa-link" aria-describedby="pone.0327419.ref023">23</a>, <a href="#pone.0327419.ref053" class="usa-link" aria-describedby="pone.0327419.ref053">53</a>, <a href="#pone.0327419.ref036" class="usa-link" aria-describedby="pone.0327419.ref036">36</a>] conclude that the symptoms of LGDs are similar to each other and their classification becomes challenging for the doctor. No proper pre-processing methods were used by the previous studies [<a href="#pone.0327419.ref005" class="usa-link" aria-describedby="pone.0327419.ref005">5</a>–<a href="#pone.0327419.ref009" class="usa-link" aria-describedby="pone.0327419.ref009">9</a>]. Major studies [<a href="#pone.0327419.ref001" class="usa-link" aria-describedby="pone.0327419.ref001">1</a>–<a href="#pone.0327419.ref003" class="usa-link" aria-describedby="pone.0327419.ref003">3</a>] trained DL on an imbalanced dataset of the LGDs. However, these studies also used the deep CNN model which produces the vanishing gradient problem and might affect the classification performance of the models. Therefore, to handle these problems, this study uses spatial and signal normalization methods as data preprocessing approaches. Additionally, BL-SMT is used to resolve the problem of the imbalance number of images. Then, the LGD_Net model is used to perform the classification of the LGDs.</p></section><section id="sec003"><h2 class="pmc_sec_title">3. Materials and methods</h2>
<p>The experimental methodology of the LGD_Net model and four baseline models such as D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> are discussed in this section.</p>
<section id="sec004"><h3 class="pmc_sec_title">3.1 Workflow of the LGD_Net model</h3>
<p>The initial diagnosis of LGDs such as PNEUM, COO, LC, TB, and NOR can help doctors stop the progression of the illness and limit its impact. There is a possibility that the manual categorization of LGDs is inefficient and time-consuming. Because of this, there is a need for a diagnostic method for this condition that is more cutting-edge and has more automated components. Thus, this study proposed a model that combines CapsNet and ELM for classifying LGDs such as PNEUM, COO, LC, TB, and NOR through the use of CT scans. For training the LGD_Net model and D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup>, we fixed the resolution of the input CT scan image at 224 × 224 pixels. The spatial and signal normalization methods were also used to prevent the model from overfitting. To deal with the problem of datasets being unevenly distributed and to equalize the number of samples within each category, we additionally used the technique BL-SMT [<a href="#pone.0327419.ref064" class="usa-link" aria-describedby="pone.0327419.ref064">64</a>]. Then, we applied affine transformation methods to increase the size of the LGDs dataset. The LGDs dataset is divided into three portions such as training, testing, and validation. <a href="#pone.0327419.g001" class="usa-link">Fig 1</a> presents the study flow diagram for the classification of LGDs using CT scan images. The experimental technique spanned 30 training epochs. Based on the confusion matrix, the performance of the LGD_Net was compared with D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> in terms of Z<sup>1</sup>, Z<sup>2</sup>, Z<sup>3</sup>, Z<sup>4</sup>, Z<sup>5</sup>, and Z<sup>6</sup>. Furthermore, the Grad-CAM heat map was generated with LGD_Net to depict the visual features of LGDs.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g001"><h4 class="obj_head">Fig 1. Study flow diagram for the classification of LGDs using CT scan.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g001.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/b1bacbc20f2f/pone.0327419.g001.jpg" loading="lazy" height="441" width="682" alt="Fig 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec005"><h3 class="pmc_sec_title">3.2 Datasets descriptions</h3>
<p>This section presents an extensive description of five publicly available benchmark datasets that are used for training and testing the proposed LGD_Net and four baseline models such as D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup>. Due to the global prevalence of COVID-19, this investigation focused on CT scans of COO, PNEUM, LC, TB, and NOR diseases. Initially, we accessed a Kaggle repository that had been established by Gupta et al., [<a href="#pone.0327419.ref043" class="usa-link" aria-describedby="pone.0327419.ref043">43</a>] and downloaded a total of 800 CT scan images of the COO and 1540 images of NOR. Then, the LC images were collected from the study [<a href="#pone.0327419.ref065" class="usa-link" aria-describedby="pone.0327419.ref065">65</a>]. This dataset contains a total of 20,000 CXR and CT scans of which 5000 images are CT scans and the rest are chest X-rays. Additionally, for this study, CXR images are not considered in this study. Stefanini et al., [<a href="#pone.0327419.ref066" class="usa-link" aria-describedby="pone.0327419.ref066">66</a>] provide a multiclass CT scan image dataset of SARS-CoV-2 and PNEUM. The dataset contains a total of 4173 CT scans from 210 distinct patients, of which 2168 relate to 80 individuals who have been verified to have SARS-CoV-2 infection using RT-PCR and the rest were of PNEUM. Both the Public Hospital of the Government Employees of Sao Paulo (HSPM) and the Metropolitan Hospital of Lapa, both located in the Brazilian state of Sao Paulo, contributed to the collection of this lung dataset. Furthermore, 1247 CT scans of PNEUM were collected from [<a href="#pone.0327419.ref057" class="usa-link" aria-describedby="pone.0327419.ref057">57</a>]. TB CT scans were collected from the study [<a href="#pone.0327419.ref067" class="usa-link" aria-describedby="pone.0327419.ref067">67</a>]. The TB dataset contains a total of 4200 CT scans of which 3500 are of NOR images while the 700 images were of TB. A detailed summary of the LGDs dataset is demonstrated in <a href="#pone.0327419.t002" class="usa-link">Table 2</a>. Additionally, a few sample CT scan images of LGDs are depicted in <a href="#pone.0327419.g002" class="usa-link">Fig 2</a>.</p>
<section class="tw xbox font-sm" id="pone.0327419.t002"><h4 class="obj_head">Table 2. A comprehensive summary of LGDs datasets.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Classes</th>
<th align="left" rowspan="1" colspan="1">LGDs</th>
<th align="left" rowspan="1" colspan="1">Abbreviations</th>
<th align="left" rowspan="1" colspan="1">No. of CT scans</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">COVID-19 Omicron</td>
<td align="left" rowspan="1" colspan="1">COO</td>
<td align="left" rowspan="1" colspan="1">2968</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">Pneumonia</td>
<td align="left" rowspan="1" colspan="1">PNEUM</td>
<td align="left" rowspan="1" colspan="1">1247</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">Lung Cancer</td>
<td align="left" rowspan="1" colspan="1">LC</td>
<td align="left" rowspan="1" colspan="1">5000</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">Tuberculosis</td>
<td align="left" rowspan="1" colspan="1">TB</td>
<td align="left" rowspan="1" colspan="1">700</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">Normal</td>
<td align="left" rowspan="1" colspan="1">NOR</td>
<td align="left" rowspan="1" colspan="1">5040</td>
</tr>
<tr>
<td align="left" colspan="3" rowspan="1">Total</td>
<td align="left" rowspan="1" colspan="1">14955</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="pone.0327419.g002"><h4 class="obj_head">Fig 2. Sample CT scan images of LGDs used for training and testing the proposed LGD_Net model and baseline models.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/0272f62047df/pone.0327419.g002.jpg" loading="lazy" height="398" width="682" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec006"><h3 class="pmc_sec_title">3.3 Data pre-processing</h3>
<p>This section contains a comprehensive discussion of the data-preprocessing methods used in this study. Initially, data normalization methods are discussed. Secondly, the BL-SMT method is described to handle the imbalanced distribution of LGD CT scan images. Lastly, affine transformation methods are briefly explained which are used to enhance the dataset before training the LGD_Net model and D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup>.</p>
<section id="sec007"><h4 class="pmc_sec_title">3.3.1 Data normalization (D_Norm).</h4>
<p>The DL model faces challenges while processing the input CT scan image data [<a href="#pone.0327419.ref068" class="usa-link" aria-describedby="pone.0327419.ref068">68</a>]. Because the input data is coming from different CT scan machines, and every machine has its measuring parameters [<a href="#pone.0327419.ref069" class="usa-link" aria-describedby="pone.0327419.ref069">69</a>]. Therefore, in this study, two different D_Norm methods such as spatial normalization (SPN) and signal normalization (SGN) are used. The purpose of using these two methods is to make the quality of CT scan images consistent, regardless they are generated from different CT scan machines [<a href="#pone.0327419.ref068" class="usa-link" aria-describedby="pone.0327419.ref068">68</a>,<a href="#pone.0327419.ref069" class="usa-link" aria-describedby="pone.0327419.ref069">69</a>]. Initially, we applied SPN to maintain the resolution of the LGD CT scan [<a href="#pone.0327419.ref015" class="usa-link" aria-describedby="pone.0327419.ref015">15</a>], while SGN was used to adjust the brightness of each voxel to the lung window [<a href="#pone.0327419.ref070" class="usa-link" aria-describedby="pone.0327419.ref070">70</a>].</p>
<ul class="list" style="list-style-type:disc"><li><p>
<strong>SPN</strong>
</p></li></ul>
<p>SPN is an image processing method used to align anatomical structure into standardized coordinate space of medical imaging such as CT scan, MRI, etc. The purpose is to ensure that CT scan images from different scanners are comparable by removing variations due to individual differences in size, shape, or orientation. This study uses SPN to determine the resolution of LGD CT scan images. Several studies [<a href="#pone.0327419.ref015" class="usa-link" aria-describedby="pone.0327419.ref015">15</a>,<a href="#pone.0327419.ref066" class="usa-link" aria-describedby="pone.0327419.ref066">66</a>–<a href="#pone.0327419.ref069" class="usa-link" aria-describedby="pone.0327419.ref069">69</a>] conclude that CT scans of human lungs have a standardized volume of 332 × 332 × 512 mm<sup>3</sup>. Thus, SPN is applied to the five LGD datasets used in this study. It converts the CT scan images of different formats into a single standard format [<a href="#pone.0327419.ref071" class="usa-link" aria-describedby="pone.0327419.ref071">71</a>] to make LGD_Net a viable option. As a direct result of this, improvements may be seen in both performance and learning.</p>
<ul class="list" style="list-style-type:disc"><li><p>
<strong>SGN</strong>
</p></li></ul>
<p>This study uses the SGN method to adjust the pixel intensity values to a standardized scale, improving consistency across CT scan images of LGDs and enhancing the performance of the proposed LGD_Net model. Thus, the SGN is applied to get the final value by computing the intensity of each voxel with the LGDs window. The Hounsfield unit (HU) is required for measuring the window of the CT scan. The HU is obtained by using the window level (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e001"><math id="M1" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>W</mi><mi>i</mi><mi>n</mi><mi>_</mi><mi>L</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></math></span>) and window width (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e002"><math id="M2" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>W</mi><mi>i</mi><mi>n</mi><mi>_</mi><mi>W</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow></msub></mrow></math></span>). Eq (1) is applied to get the final normalized value by utilizing the current window size as the parameter.</p>
<table class="disp-formula p" id="pone.0327419.e003"><tr>
<td class="formula"><math id="M3" display="block" overflow="linebreak"><mrow><msub><mi>I</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mi>_</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>I</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mi>_</mi><mi>O</mi><mi>r</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>−</mo><msub><mi>H</mi><mrow><mi>W</mi><mi>i</mi><mi>n</mi><mi>_</mi><mi>L</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><mrow><msub><mi>H</mi><mrow><mi>W</mi><mi>i</mi><mi>n</mi><mi>_</mi><mi>W</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow></msub></mrow></mfrac></mrow></math></td>
<td class="label">(1)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e004"><math id="M4" display="inline" overflow="linebreak"><mrow><msub><mi>I</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mi>_</mi><mi>O</mi><mi>r</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow></math></span> represents the LGD images. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e005"><math id="M5" display="inline" overflow="linebreak"><mrow><msub><mi>I</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mi>_</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi></mrow></msub></mrow></math></span> shows the normalized intensity of LGD CT scan images. For this study, we set the lower limit of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e006"><math id="M6" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>W</mi><mi>i</mi><mi>n</mi><mi>_</mi><mi>L</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></math></span> between −0.05 and 0.5.</p></section><section id="sec008"><h4 class="pmc_sec_title">3.3.2 Borderline SMOTE (BL-SMT).</h4>
<p>In this study, the BL-SMT upsampling method is employed to solve the problem of the dataset having an uneven distribution of LGD images. Most of the recent work [<a href="#pone.0327419.ref015" class="usa-link" aria-describedby="pone.0327419.ref015">15</a>,<a href="#pone.0327419.ref067" class="usa-link" aria-describedby="pone.0327419.ref067">67</a>–<a href="#pone.0327419.ref072" class="usa-link" aria-describedby="pone.0327419.ref072">72</a>] believes that providing an uneven distribution of images at the time of training the DL model can affect its performance. Thus, the BL-SMT model is used to balance the distribution of LGD images. The BL-SMT up-sampled the LGD images by adding the zero values in between the original LGD samples so that the sampling rate may be increased. The SMT produced the CT scan images of LGD by using Euclidean distance of each minority class of LGD data and BL focused on minority instances that are near the boundary with the majority class. Thus, we combine the BL-SMT to create the synthetic images of LGD from the original LGD CT scan images as shown in <a href="#pone.0327419.g003" class="usa-link">Fig 3</a>. The pseudocode of BL-SMT is presented in Algorithm 1. Furthermore, <a href="#pone.0327419.t003" class="usa-link">Table 3</a> presents a detailed summary of the LGD image generated synthetically after using BL-SMT.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g003"><h5 class="obj_head">Fig 3. Synthetic LGD image generated after applying BL-SMT.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/68f51d71743a/pone.0327419.g003.jpg" loading="lazy" height="536" width="682" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><section class="tw xbox font-sm" id="pone.0327419.t003"><h5 class="obj_head">Table 3. A comprehensive summary of LGDs datasets after applying BL-SMT.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Classes</th>
<th align="left" rowspan="1" colspan="1">LGDs</th>
<th align="left" rowspan="1" colspan="1">Abbreviations</th>
<th align="left" rowspan="1" colspan="1">Original CT scans</th>
<th align="left" rowspan="1" colspan="1">Synthetic Images using BL-SMT</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">COVID-19 Omicron</td>
<td align="left" rowspan="1" colspan="1">COO</td>
<td align="left" rowspan="1" colspan="1">2968</td>
<td align="left" rowspan="1" colspan="1">4879</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">Pneumonia</td>
<td align="left" rowspan="1" colspan="1">PNEUM</td>
<td align="left" rowspan="1" colspan="1">1247</td>
<td align="left" rowspan="1" colspan="1">4899</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">Lung Cancer</td>
<td align="left" rowspan="1" colspan="1">LC</td>
<td align="left" rowspan="1" colspan="1">5000</td>
<td align="left" rowspan="1" colspan="1">5000</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">Tuberculosis</td>
<td align="left" rowspan="1" colspan="1">TB</td>
<td align="left" rowspan="1" colspan="1">700</td>
<td align="left" rowspan="1" colspan="1">4912</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">Normal</td>
<td align="left" rowspan="1" colspan="1">NOR</td>
<td align="left" rowspan="1" colspan="1">5040</td>
<td align="left" rowspan="1" colspan="1">5040</td>
</tr>
<tr>
<td align="left" colspan="3" rowspan="1">Total</td>
<td align="left" rowspan="1" colspan="1">14955</td>
<td align="left" rowspan="1" colspan="1">24,730</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p><strong>Algorithm 1:</strong> BL-SMT for generating synthetic LGD images</p>
<p><strong>Input Parameters:</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e007"><math id="M7" display="inline" overflow="linebreak"><mrow><msub><mi>T</mi><mrow><mi>T</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span>: Set LGD images for training, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e008"><math id="M8" display="inline" overflow="linebreak"><mrow><msub><mi>M</mi><mrow><mi>M</mi><mi>i</mi><mi>n</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span>: Instances of minority set of LGD dataset, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e009"><math id="M9" display="inline" overflow="linebreak"><mrow><msub><mi>K</mi><mrow><mi>N</mi><mi>N</mi></mrow></msub></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e010"><math id="M10" display="inline" overflow="linebreak"><mrow><msub><mi>P</mi><mrow><mi>N</mi><mi>N</mi></mrow></msub><mi>:</mi></mrow></math></span> No. of nearest neighbors, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e011"><math id="M11" display="inline" overflow="linebreak"><mrow><msub><mi>U</mi><mrow><mi>O</mi><mi>r</mi><mi>i</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub><mo> </mo></mrow></math></span>: Quantity of synthetic LGD images required to compensate for the original LGD images in the specified class.</p>
<p><strong>Output</strong>: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e012"><math id="M12" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub><mo> </mo></mrow></math></span>: A group of synthetic LGD images from the minority class.</p>
<p><strong>1.</strong> I = ɸ              <strong>// I represents a collection of LGD images that are considered BL</strong>.</p>
<p><strong>2. For</strong> all <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e013"><math id="M13" display="inline" overflow="linebreak"><mrow><msub><mi>r</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span> in <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e014"><math id="M14" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span>
<strong>Do:</strong></p>
<p><strong>3.</strong>    <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e015"><math id="M15" display="inline" overflow="linebreak"><mrow><msub><mi>K</mi><mrow><mi>N</mi><mi>N</mi></mrow></msub></mrow></math></span> ← Nearest Neighbors of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e016"><math id="M16" display="inline" overflow="linebreak"><mrow><msub><mi>r</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span> in <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e017"><math id="M17" display="inline" overflow="linebreak"><mrow><msub><mi>T</mi><mrow><mi>T</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span></p>
<p><strong>4</strong>.    r ← the number of samples in <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e018"><math id="M18" display="inline" overflow="linebreak"><mrow><msub><mi>r</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span> and not in <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e019"><math id="M19" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span></p>
<p><strong>5.</strong>      <strong>IF</strong> L/2 ≤ r &lt; L then <strong>//</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e020"><math id="M20" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">r</mi></mrow></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">i</mi></mrow></mrow></mrow></msub></mrow></math></span>
<strong>is a borderline sample of LGDs.</strong></p>
<p><strong>6.</strong>      add <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e021"><math id="M21" display="inline" overflow="linebreak"><mrow><msub><mi>r</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span> to I</p>
<p><strong>7.</strong>      <strong>End IF</strong></p>
<p>
<strong>8. End FOR</strong>
</p>
<p><strong>9.</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e022"><math id="M22" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span> = ɸ              <strong>//</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e023"><math id="M23" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">R</mi></mrow></mrow><mrow><mrow><mi mathvariant="bold">S</mi><mi mathvariant="bold">Y</mi><mi mathvariant="bold">N</mi><mi mathvariant="bold">_</mi><mi mathvariant="bold">L</mi><mi mathvariant="bold">G</mi><mi mathvariant="bold">D</mi></mrow></mrow></msub></mrow></math></span>
<strong>is a set containing synthetic samples of LGD images.</strong></p>
<p><strong>10. For</strong> all <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e024"><math id="M24" display="inline" overflow="linebreak"><mrow><msub><mi>s</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span> in <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e025"><math id="M25" display="inline" overflow="linebreak"><mrow><mi>I</mi></mrow></math></span>
<strong>Do:</strong></p>
<p><strong>11</strong>.      <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e026"><math id="M26" display="inline" overflow="linebreak"><mrow><msub><mi>s</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span> ← <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e027"><math id="M27" display="inline" overflow="linebreak"><mrow><msub><mi>P</mi><mrow><mi>N</mi><mi>N</mi></mrow></msub></mrow></math></span> nearest neighbors of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e028"><math id="M28" display="inline" overflow="linebreak"><mrow><msub><mi>s</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span> in <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e029"><math id="M29" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span></p>
<p><strong>12.</strong>        <strong>For</strong> i=1 to <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e030"><math id="M30" display="inline" overflow="linebreak"><mrow><msub><mi>U</mi><mrow><mi>O</mi><mi>r</mi><mi>i</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span>
<strong>Do:</strong></p>
<p><strong>13</strong>.        i ← choose a random sample from <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e031"><math id="M31" display="inline" overflow="linebreak"><mrow><msub><mi>s</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span></p>
<p><strong>14</strong>.        <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e032"><math id="M32" display="inline" overflow="linebreak"><mrow><msub><mi>s</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span>′ ← <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e033"><math id="M33" display="inline" overflow="linebreak"><mrow><msub><mi>s</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span>+ j <sub>*</sub> (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e034"><math id="M34" display="inline" overflow="linebreak"><mrow><msub><mi>s</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span> – i) <strong>//j is a random number in(0, 1),</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e035"><math id="M35" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">s</mi></mrow></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">i</mi></mrow></mrow></mrow></msub></mrow></math></span>
<strong>is a synthetic sample of LGD images.</strong></p>
<p><strong>15.</strong>        add <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e036"><math id="M36" display="inline" overflow="linebreak"><mrow><msub><mi>s</mi><mrow><mi>i</mi></mrow></msub></mrow></math></span> to <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e037"><math id="M37" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span></p>
<p>
<strong><strong>16.</strong>        <strong>End FOR</strong></strong>
</p>
<p>
<strong>17. End FOR</strong>
</p>
<p><strong>18.</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e038"><math id="M38" display="inline" overflow="linebreak"><mrow><msup><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow><mi>′</mi></msup><mo> </mo></mrow></math></span>= <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e039"><math id="M39" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow></math></span> U <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e040"><math id="M40" display="inline" overflow="linebreak"><mrow><msup><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow><mi>′</mi></msup></mrow></math></span>
<strong>//</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e041"><math id="M41" display="inline" overflow="linebreak"><mrow><msup><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">R</mi></mrow></mrow><mrow><mrow><mi mathvariant="bold">S</mi><mi mathvariant="bold">Y</mi><mi mathvariant="bold">N</mi><mi mathvariant="bold">_</mi><mi mathvariant="bold">L</mi><mi mathvariant="bold">G</mi><mi mathvariant="bold">D</mi></mrow></mrow></msub></mrow><mi>′</mi></msup></mrow></math></span>
<strong>is the union of minority samples and synthetic samples.</strong></p>
<p><strong>19. Return</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e042"><math id="M42" display="inline" overflow="linebreak"><mrow><msup><mrow><msub><mi>R</mi><mrow><mi>S</mi><mi>Y</mi><mi>N</mi><mi>_</mi><mi>L</mi><mi>G</mi><mi>D</mi></mrow></msub></mrow><mi>′</mi></msup></mrow></math></span></p></section><section id="sec009"><h4 class="pmc_sec_title">3.3.3 Affine transformation.</h4>
<p>In this study, several affine transformation approaches (ATP) such as rotation (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e043"><math id="M43" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>O</mi><mi>T</mi></mrow></msub></mrow></math></span>), scaling (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e044"><math id="M44" display="inline" overflow="linebreak"><mrow><msub><mi>S</mi><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi></mrow></msub></mrow></math></span>), horizontal shift (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e045"><math id="M45" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span>), vertical shift (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e046"><math id="M46" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span>), and random translation (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e047"><math id="M47" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>A</mi><mi>N</mi><mi>_</mi><mi>T</mi><mi>R</mi><mi>A</mi><mi>N</mi><mi>S</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math></span> to enhance the LGD datasets. The purpose of enhancing the LGD dataset is to prevent the LGD_Net model from overfitting. The dataset (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e048"><math id="M48" display="inline" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math></span> generated after applying BL-SMT is split into three steps as presented in Eq (2).</p>
<table class="disp-formula p" id="pone.0327419.e049"><tr>
<td class="formula"><math id="M49" display="block" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo> </mo><mo>→</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><mo> </mo><msup><mi>T</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo>,</mo><mo> </mo><msup><mi>T</mi><mrow><mi>V</mi><mi>a</mi><mi>l</mi></mrow></msup><mo>,</mo><mo> </mo><msup><mi>T</mi><mrow><mi>T</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></td>
<td class="label">(2)</td>
</tr></table>
<p>Here <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e050"><math id="M50" display="inline" overflow="linebreak"><mrow><msup><mi>T</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo>=</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mi>t</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mn>1</mn><mi>l</mi><mi>d</mi><mi>o</mi><mi>t</mi><mi>s</mi><mo> </mo><msup><mi>t</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mi>…</mi><mo> </mo><msup><mi>t</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msup><mi>T</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></span> shows the training set of LGD images, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e051"><math id="M51" display="inline" overflow="linebreak"><mrow><msup><mi>T</mi><mrow><mi>V</mi><mi>a</mi><mi>l</mi></mrow></msup><mo>=</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mi>t</mi><mrow><mi>V</mi><mi>a</mi><mi>l</mi></mrow></msup><mo stretchy="false">(</mo><mn>1</mn><mi>l</mi><mi>d</mi><mi>o</mi><mi>t</mi><mi>s</mi><mo> </mo><msup><mi>t</mi><mrow><mi>V</mi><mi>a</mi><mi>l</mi></mrow></msup><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mi>…</mi><mo> </mo><msup><mi>t</mi><mrow><mi>V</mi><mi>a</mi><mi>l</mi></mrow></msup><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msup><mi>T</mi><mrow><mi>V</mi><mi>a</mi><mi>l</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></span> represents the validation set and the testing set is represented as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e052"><math id="M52" display="inline" overflow="linebreak"><mrow><msup><mi>T</mi><mrow><mi>T</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo>=</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mi>t</mi><mrow><mi>T</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo stretchy="false">(</mo><mn>1</mn><mi>l</mi><mi>d</mi><mi>o</mi><mi>t</mi><mi>s</mi><msup><mi>t</mi><mrow><mi>T</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mi>…</mi><msup><mi>t</mi><mrow><mi>T</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msup><mi>T</mi><mrow><mi>T</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></span>. Thus, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e053"><math id="M53" display="inline" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow></math></span> divided into <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e054"><math id="M54" display="inline" overflow="linebreak"><mrow><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><mo> </mo><msup><mi>T</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo>,</mo><mo> </mo><msup><mi>T</mi><mrow><mi>V</mi><mi>a</mi><mi>l</mi></mrow></msup><mo>,</mo><mo> </mo><msup><mi>T</mi><mrow><mi>T</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></span> as mentioned in Eq (3).</p>
<table class="disp-formula p" id="pone.0327419.e055"><tr>
<td class="formula"><math id="M55" display="block" overflow="linebreak"><mrow><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msup><mi>T</mi><mrow><mi>T</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mo>+</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msup><mi>T</mi><mrow><mi>V</mi><mi>a</mi><mi>l</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mo>+</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msup><mi>T</mi><mrow><mi>T</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mo> </mo><mo>=</mo><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow></math></td>
<td class="label">(3)</td>
</tr></table>
<p>The ATPs are applied to the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e056"><math id="M56" display="inline" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo> </mo><mo>→</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><mo> </mo><msup><mi>T</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></span> which enhances the LGD images dataset. The enhanced image dataset <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e057"><math id="M57" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><msub><mi>D</mi><mrow><mi>N</mi><mi>E</mi><mi>W</mi><mo>−</mo><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math></span> is obtained by using Eq (4).</p>
<table class="disp-formula p" id="pone.0327419.e058"><tr>
<td class="formula"><math id="M58" display="block" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>N</mi><mi>E</mi><mi>W</mi><mo>−</mo><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>=</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></td>
<td class="label">(4)</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p><strong>Rotation (</strong><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e059"><math id="M59" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">R</mi></mrow></mrow><mrow><mrow><mi mathvariant="bold-italic">R</mi><mi mathvariant="bold-italic">O</mi><mi mathvariant="bold-italic">T</mi></mrow></mrow></msub></mrow></math></span>)</p></li></ul>
<p>Initially, we applied the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e060"><math id="M60" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>O</mi><mi>T</mi></mrow></msub></mrow></math></span> the function of ATP, which rotates the angle vector (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e061"><math id="M61" display="inline" overflow="linebreak"><mrow><msup><mi>T</mi><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>O</mi><mi>T</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo></mrow></math></span> by skipping the value of zero. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e062"><math id="M62" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>O</mi><mi>T</mi></mrow></msub></mrow></math></span> is performed by using the Eqs (5–6).</p>
<table class="disp-formula p" id="pone.0327419.e063"><tr>
<td class="formula"><math id="M63" display="block" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>N</mi><mi>E</mi><mi>W</mi><mo>−</mo><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>01</mn></mrow></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>R</mi><mi>O</mi><mi>T</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>01</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></td>
<td class="label">(5)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e064"><tr>
<td class="formula"><math id="M64" display="block" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>O</mi><mi>T</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>01</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow><mo>=</mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>01</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mrow><mo> </mo><mi>A</mi></mrow><mrow><mn>1</mn></mrow><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>O</mi><mi>T</mi></mrow></msub></mrow></msubsup><mo> </mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><mo> </mo><mi>…</mi><mo> </mo><msubsup><mi>T</mi><mrow><mi>n</mi></mrow><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>O</mi><mi>T</mi></mrow></msub></mrow></msubsup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mrow><mo> </mo><mi>A</mi></mrow><mrow><mi>n</mi></mrow><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>O</mi><mi>T</mi></mrow></msub></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></td>
<td class="label">(6)</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p><strong>Scaling (</strong><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e065"><math id="M65" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">S</mi></mrow></mrow><mrow><mrow><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">C</mi><mi mathvariant="bold-italic">A</mi><mi mathvariant="bold-italic">L</mi></mrow></mrow></msub></mrow></math></span>)</p></li></ul>
<p>The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e066"><math id="M66" display="inline" overflow="linebreak"><mrow><msub><mi>S</mi><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi></mrow></msub></mrow></math></span> operation is performed on the LGD dataset by applying the <em>T</em>
<sup><em>SCAL</em></sup> factor by skipping the value of <em>T</em>
<sup><em>SCAL</em></sup> = 1. The following Eqs (7–8) are used to perform the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e067"><math id="M67" display="inline" overflow="linebreak"><mrow><msub><mi>S</mi><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi></mrow></msub></mrow></math></span>.</p>
<table class="disp-formula p" id="pone.0327419.e068"><tr>
<td class="formula"><math id="M68" display="block" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>N</mi><mi>E</mi><mi>W</mi><mo>−</mo><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>02</mn></mrow></msub><mo>=</mo><msub><mi>S</mi><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>02</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></td>
<td class="label">(7)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e069"><tr>
<td class="formula"><math id="M69" display="block" overflow="linebreak"><mrow><msub><mi>S</mi><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>02</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow><mo>=</mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>02</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mrow><mo> </mo><mi>A</mi></mrow><mrow><mn>1</mn></mrow><mrow><msub><mi>S</mi><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi></mrow></msub></mrow></msubsup><mo> </mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><mo> </mo><mi>…</mi><mo> </mo><msubsup><mi>T</mi><mrow><mi>n</mi></mrow><mrow><msub><mi>S</mi><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi></mrow></msub></mrow></msubsup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mrow><mo> </mo><mi>A</mi></mrow><mrow><mi>n</mi></mrow><mrow><msub><mi>S</mi><mrow><mi>S</mi><mi>C</mi><mi>A</mi><mi>L</mi></mrow></msub></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></td>
<td class="label">(8)</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p><strong>Horizontal shift (</strong><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e070"><math id="M70" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">H</mi></mrow></mrow><mrow><mrow><mi mathvariant="bold-italic">H</mi><mi mathvariant="bold-italic">_</mi><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">h</mi><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">f</mi><mi mathvariant="bold-italic">t</mi></mrow></mrow></msub></mrow></math></span>)</p></li></ul>
<p>For this study, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e071"><math id="M71" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span> is used. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e072"><math id="M72" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span> shift the X-coordinates of each point of the CT scan image to a certain amount that is proportional to the Y-coordinate. However, the points of the Y-coordinate remain unchanged. After using <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e073"><math id="M73" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span> for a value (<em>P, R</em>) the new coordinates are produced (<em>P′, R′</em>) with a shift value of (<em>L</em>) measured using Eqs (9–10).</p>
<table class="disp-formula p" id="pone.0327419.e074"><tr>
<td class="formula"><math id="M74" display="block" overflow="linebreak"><mrow><msup><mi>P</mi><mrow><mi>′</mi></mrow></msup><mo>=</mo><mi>P</mi><mo>+</mo><mi>L</mi><mi>×</mi><mi>R</mi></mrow></math></td>
<td class="label">(9)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e075"><tr>
<td class="formula"><math id="M75" display="block" overflow="linebreak"><mrow><msup><mi>R</mi><mrow><mi>′</mi></mrow></msup><mo>=</mo><mi>R</mi></mrow></math></td>
<td class="label">(10)</td>
</tr></table>
<p>However, Eq (11) and (12) is used to create the new image by using <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e076"><math id="M76" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span>.</p>
<table class="disp-formula p" id="pone.0327419.e077"><tr>
<td class="formula"><math id="M77" display="block" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>N</mi><mi>E</mi><mi>W</mi><mo>−</mo><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>03</mn></mrow></msub><mo>=</mo><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>03</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></td>
<td class="label">(11)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e078"><tr>
<td class="formula"><math id="M78" display="block" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>03</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow><mo>=</mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>03</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mrow><mo> </mo><mi>A</mi></mrow><mrow><mn>1</mn></mrow><mrow><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></msubsup><mo> </mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><mo> </mo><mi>…</mi><mo> </mo><msubsup><mi>T</mi><mrow><mi>n</mi></mrow><mrow><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></msubsup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mrow><mo> </mo><mi>A</mi></mrow><mrow><mi>n</mi></mrow><mrow><msub><mi>H</mi><mrow><mi>H</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></td>
<td class="label">(12)</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p><strong>Vertical shift (</strong><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e079"><math id="M79" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">H</mi></mrow></mrow><mrow><mrow><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">_</mi><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">h</mi><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">f</mi><mi mathvariant="bold-italic">t</mi></mrow></mrow></msub></mrow></math></span>)</p></li></ul>
<p>For this study, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e080"><math id="M80" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span> is used. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e081"><math id="M81" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span> shift the Y-coordinates of each point of the CT scan image to a certain amount that is proportional to the X-coordinate. However, the points of the X-coordinate remain unchanged. After using <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e082"><math id="M82" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span> for a value (<em>P, R</em>) the new coordinates are produced (<em>P′, R′</em>) with a shift value of (<em>M</em>) measured using Eqs (13–14).</p>
<table class="disp-formula p" id="pone.0327419.e083"><tr>
<td class="formula"><math id="M83" display="block" overflow="linebreak"><mrow><msup><mi>P</mi><mrow><mi>′</mi></mrow></msup><mo>=</mo><mi>P</mi><mo>+</mo><mi>M</mi><mi>×</mi><mi>R</mi></mrow></math></td>
<td class="label">(13)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e084"><tr>
<td class="formula"><math id="M84" display="block" overflow="linebreak"><mrow><msup><mi>R</mi><mrow><mi>′</mi></mrow></msup><mo>=</mo><mi>R</mi></mrow></math></td>
<td class="label">(14)</td>
</tr></table>
<p>However, Eq (15) and (16) is used to create the new image by using <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e085"><math id="M85" display="inline" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></math></span>.</p>
<table class="disp-formula p" id="pone.0327419.e086"><tr>
<td class="formula"><math id="M86" display="block" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>N</mi><mi>E</mi><mi>W</mi><mo>−</mo><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>04</mn></mrow></msub><mo>=</mo><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>04</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></td>
<td class="label">(15)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e087"><tr>
<td class="formula"><math id="M87" display="block" overflow="linebreak"><mrow><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>04</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow><mo>=</mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>04</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mrow><mo> </mo><mi>A</mi></mrow><mrow><mn>1</mn></mrow><mrow><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></msubsup><mo> </mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><mo> </mo><mi>…</mi><mo> </mo><msubsup><mi>T</mi><mrow><mi>n</mi></mrow><mrow><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></msubsup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mrow><mo> </mo><mi>A</mi></mrow><mrow><mi>n</mi></mrow><mrow><msub><mi>H</mi><mrow><mi>V</mi><mi>_</mi><mi>S</mi><mi>h</mi><mi>i</mi><mi>f</mi><mi>t</mi></mrow></msub></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></td>
<td class="label">(16)</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p>
<strong>Random translation (</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e088">
<math id="M88" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">R</mi></mrow></mrow><mrow><mrow><mi mathvariant="bold-italic">R</mi><mi mathvariant="bold-italic">A</mi><mi mathvariant="bold-italic">N</mi><mi mathvariant="bold-italic">_</mi><mi mathvariant="bold-italic">T</mi><mi mathvariant="bold-italic">R</mi><mi mathvariant="bold-italic">A</mi><mi mathvariant="bold-italic">N</mi><mi mathvariant="bold-italic">S</mi></mrow></mrow></msub><mrow><mo stretchy="false">)</mo></mrow></mrow></math>
</span>
</p></li></ul>
<p>By using <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e089"><math id="M89" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>A</mi><mi>N</mi><mi>_</mi><mi>T</mi><mi>R</mi><mi>A</mi><mi>N</mi><mi>S</mi></mrow></msub></mrow></math></span>, we transformed the LGD dataset by <em>i</em> times using random horizontal movement (<em>R</em><sub><em>HM</em></sub>) and random vertical movements (<em>R</em><sub><em>VM</em></sub>) keeping the value in a range between <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e090"><math id="M90" display="inline" overflow="linebreak"><mrow><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mo>−</mo><msub><mi>H</mi><mrow><mi>R</mi><mi>a</mi><mi>n</mi><mi>T</mi></mrow></msub><mo>,</mo><mo> </mo><msub><mi>H</mi><mrow><mi>R</mi><mi>a</mi><mi>n</mi><mi>T</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></span> for equal distribution as discussed in Eq (17). However, the symbols R<sup>X</sup> and R<sup>Y</sup> show the <em>R</em><sub><em>HM</em></sub> and <em>R</em><sub><em>VM</em></sub>, respectively.</p>
<table class="disp-formula p" id="pone.0327419.e091"><tr>
<td class="formula"><math id="M91" display="block" overflow="linebreak"><mrow><msubsup><mi>ε</mi><mrow><mi>i</mi></mrow><mrow><mi>G</mi></mrow></msubsup><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mo>−</mo><msub><mi>H</mi><mrow><mi>R</mi><mi>a</mi><mi>n</mi><mi>T</mi></mrow></msub><mo>,</mo><mo> </mo><msub><mi>H</mi><mrow><mi>R</mi><mi>a</mi><mi>n</mi><mi>T</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">∀</mo></mrow><mo> </mo><mi>i</mi><mo> </mo><mo>∈</mo><mo> </mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mi>i</mi><mo stretchy="false">]</mo><mo> </mo><mo>∀</mo><mo> </mo><mi>G</mi><mo> </mo><mo>∈</mo><mo> </mo><mo stretchy="false">[</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy="false">]</mo></mrow></math></td>
<td class="label">(17)</td>
</tr></table>
<p>The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e092"><math id="M92" display="inline" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>A</mi><mi>N</mi><mi>_</mi><mi>T</mi><mi>R</mi><mi>A</mi><mi>N</mi><mi>S</mi></mrow></msub><mo> </mo></mrow></math></span>is applied to produce the new LGD images by using Eqs (18–19).</p>
<table class="disp-formula p" id="pone.0327419.e093"><tr>
<td class="formula"><math id="M93" display="block" overflow="linebreak"><mrow><msub><mi>D</mi><mrow><mi>N</mi><mi>E</mi><mi>W</mi><mo>−</mo><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>05</mn></mrow></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>R</mi><mi>A</mi><mi>N</mi><mi>_</mi><mi>T</mi><mi>R</mi><mi>A</mi><mi>N</mi><mi>S</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>05</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></td>
<td class="label">(18)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e094"><tr>
<td class="formula"><math id="M94" display="block" overflow="linebreak"><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>A</mi><mi>N</mi><mi>_</mi><mi>T</mi><mi>R</mi><mi>A</mi><mi>N</mi><mi>S</mi></mrow></msub><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>05</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo> </mo><mo fence="true" form="postfix" stretchy="true">}</mo></mrow><mo>=</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mrow><mo> </mo><msub><mi>D</mi><mrow><mi>L</mi><mi>G</mi><mi>D</mi><mo>−</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>05</mn></mrow></msub></mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mi>T</mi><mi>P</mi><mo stretchy="false">)</mo></mrow></msup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mi>ε</mi><mrow><mn>1</mn></mrow><mrow><mi>x</mi></mrow></msubsup><mo>,</mo><mo> </mo><msubsup><mi>ε</mi><mrow><mn>1</mn></mrow><mrow><mi>y</mi></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><mo> </mo><mi>…</mi><mo> </mo><msubsup><mi>T</mi><mrow><mi>n</mi></mrow><mrow><msub><mi>R</mi><mrow><mi>R</mi><mi>A</mi><mi>N</mi><mi>_</mi><mi>T</mi><mi>R</mi><mi>A</mi><mi>N</mi><mi>S</mi></mrow></msub></mrow></msubsup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>i</mi><mo>,</mo><mo> </mo><msubsup><mi>ε</mi><mrow><mn>1</mn></mrow><mrow><mi>x</mi></mrow></msubsup><mo>,</mo><mo> </mo><msubsup><mi>ε</mi><mrow><mn>1</mn></mrow><mrow><mi>y</mi></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></td>
<td class="label">(19)</td>
</tr></table></section></section><section id="sec010"><h3 class="pmc_sec_title">3.4 Proposed model</h3>
<p>This section is divided into two parts. Initially, we used SegCaps to perform the CT scan image segmentation of LGDs. After that, the segmented CT scan images are used with a proposed LGD_Net model based on CapsNet with ELM for the classification of LGDs.</p>
<section id="sec011"><h4 class="pmc_sec_title">3.4.1 CT scans-based segmentation using SegCaps.</h4>
<p>We used the SegCaps method to perform the segmentation process of several LGDs such as COO, PNEUM, LC, TB, and NOR. The procedure of segmenting the CT scan images was carried out with the assistance of SegCaps, as described in [<a href="#pone.0327419.ref071" class="usa-link" aria-describedby="pone.0327419.ref071">71</a>]. In addition, segmented CT scan images are utilized by the proposed model during the entirety of the training process in conjunction with ELM for the identification of LGDs. When it comes to the process of chest CT scam image segmentation, the strategy that is recommended makes use of 2D slices as the input. When segmenting the CT scan of the lungs, a volume with the dimensions 332 × 332 × 512 mm<sup>3</sup> is used. S<sub>X</sub>, S<sub>Y</sub>, and S<sub>Z</sub> are the three planes that make up each volume of a three-dimensional (3D) CT scan. We established these planes to make it easier to differentiate between different types of LGDs. The Eq (20) is applied to quantify these CT scans of LGDs.</p>
<table class="disp-formula p" id="pone.0327419.e095"><tr>
<td class="formula"><math id="M95" display="block" overflow="linebreak"><mrow><msup><mi>C</mi><mrow><mi>L</mi></mrow></msup><mo>=</mo><mi>E</mi><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msubsup><mi>P</mi><mrow><mi>S</mi><mi>X</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo>,</mo><mo> </mo><msubsup><mi>P</mi><mrow><mi>S</mi><mi>Y</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo>,</mo><mo> </mo><msubsup><mi>P</mi><mrow><mi>S</mi><mi>Z</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow></mrow></math></td>
<td class="label">(20)</td>
</tr></table>
<p>where <em>C</em><sup><em>I</em></sup> refers to the likelihood of chest diseases and <em>L</em> stands for the location of the disease. The letter <em>E</em> represents the technique that is applied while defining voxel views in three-dimensional space. Method <em>L</em> was used to predict the <em>P</em><sub><em>SX</em></sub>, <em>P</em><sub><em>SY</em></sub>, and <em>P</em><sub><em>SZ</em></sub> voxels. The conventional method takes a long time; therefore, we made the following change to Eq (20).</p>
<table class="disp-formula p" id="pone.0327419.e096"><tr>
<td class="formula"><math id="M96" display="block" overflow="linebreak"><mrow><msup><mrow><mover><mrow><mi>C</mi></mrow><mo stretchy="false">^</mo></mover></mrow><mrow><mi>L</mi></mrow></msup><mo>=</mo><mi>E</mi><mo> </mo><mo stretchy="false">(</mo><msubsup><mrow><mover><mrow><mi>P</mi></mrow><mo stretchy="false">^</mo></mover></mrow><mrow><mi>S</mi><mi>X</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo>,</mo><mo> </mo><msubsup><mrow><mover><mrow><mo> </mo><mi>P</mi></mrow><mo stretchy="false">^</mo></mover></mrow><mrow><mi>S</mi><mi>Y</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo>,</mo><mo> </mo><msubsup><mrow><mo> </mo><mover><mrow><mi>P</mi></mrow><mo stretchy="false">^</mo></mover></mrow><mrow><mi>S</mi><mi>Z</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></math></td>
<td class="label">(21)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e097"><tr>
<td class="formula"><math id="M97" display="block" overflow="linebreak"><mrow><msup><mrow><mover><mrow><mi>C</mi></mrow><mo stretchy="false">^</mo></mover></mrow><mrow><mi>L</mi></mrow></msup><mo>=</mo><mi>E</mi><mo> </mo><mo stretchy="false">(</mo><mo> </mo><msubsup><mi>f</mi><mrow><mi>S</mi><mi>X</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msubsup><mi>P</mi><mrow><mi>S</mi><mi>X</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><mo> </mo><msubsup><mi>f</mi><mrow><mi>S</mi><mi>Y</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msubsup><mi>P</mi><mrow><mi>S</mi><mi>Y</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><mo> </mo><msubsup><mi>f</mi><mrow><mi>S</mi><mi>Z</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msubsup><mi>P</mi><mrow><mi>S</mi><mi>Z</mi></mrow><mrow><mi>L</mi></mrow></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo stretchy="false">)</mo></mrow></math></td>
<td class="label">(22)</td>
</tr></table></section><section id="sec012"><h4 class="pmc_sec_title">3.4.2 LGDs classification using LGD_Net.</h4>
<p>In recent years, CNN has seen an increase in the number of medical applications for CT scan image classification [<a href="#pone.0327419.ref071" class="usa-link" aria-describedby="pone.0327419.ref071">71</a>–<a href="#pone.0327419.ref073" class="usa-link" aria-describedby="pone.0327419.ref073">73</a>]. This results in a significant increase in the amount of complexity that must be computed and even has an effect on the degree of performance that can be attained by the classifier. This is because the pooling layers (P_lay) of a CNN do not process the spatial relation that is present between the features of LGD images [<a href="#pone.0327419.ref074" class="usa-link" aria-describedby="pone.0327419.ref074">74</a>]. The LGD_Net model overcomes the previously described issues and achieves improvements in the diagnostic accuracy of LGDs such as COO, PNUEM, LC, TB, and NOR by fusing the potent capabilities of CapsNet [<a href="#pone.0327419.ref024" class="usa-link" aria-describedby="pone.0327419.ref024">24</a>] with ELM [<a href="#pone.0327419.ref025" class="usa-link" aria-describedby="pone.0327419.ref025">25</a>]. In LGD_Net, the CapsNet has provided the extraction of strong features [<a href="#pone.0327419.ref071" class="usa-link" aria-describedby="pone.0327419.ref071">71</a>] while ELM [<a href="#pone.0327419.ref072" class="usa-link" aria-describedby="pone.0327419.ref072">72</a>] has been effectively substituted for the more conventional dense classification layers to enhance LGD predictions [<a href="#pone.0327419.ref075" class="usa-link" aria-describedby="pone.0327419.ref075">75</a>]. In contrast to traditional ANN [<a href="#pone.0327419.ref075" class="usa-link" aria-describedby="pone.0327419.ref075">75</a>,<a href="#pone.0327419.ref076" class="usa-link" aria-describedby="pone.0327419.ref076">76</a>] methods, the proposed LGD_Net model calculates its inputs and then summarizes the outcome into a small vector of final outputs. <a href="#pone.0327419.t004" class="usa-link">Table 4</a> presents the comparison between LGD_Net and ANN.</p>
<section class="tw xbox font-sm" id="pone.0327419.t004"><h5 class="obj_head">Table 4. Comparison of the proposed LGD_Net with ANN.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Sr#</th>
<th align="left" rowspan="1" colspan="1">Parameters</th>
<th align="left" rowspan="1" colspan="1">LGD_Net</th>
<th align="left" rowspan="1" colspan="1">ANN</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">Activation Function</td>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e098">
<math id="M98" display="inline" overflow="linebreak"><mrow><msub><mi>F</mi><mrow><mi>N</mi></mrow></msub><mo>=</mo><mo> </mo><mfrac><mrow><msup><mrow><msub><mrow><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>F</mi></mrow><mrow><mi>N</mi></mrow></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow><mrow><mn>1</mn><mo>+</mo><mo> </mo><msup><mrow><msub><mrow><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>F</mi></mrow><mrow><mi>N</mi></mrow></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mfrac><mo> </mo><mfrac><mrow><msub><mi>F</mi><mrow><mi>N</mi></mrow></msub></mrow><mrow><msub><mrow><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>F</mi></mrow><mrow><mi>N</mi></mrow></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo></mrow></mfrac></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e099">
<math id="M99" display="inline" overflow="linebreak"><mrow><msub><mi>P</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo><mo>=</mo><mi>A</mi><mo stretchy="false">(</mo><msub><mi>B</mi><mrow><mi>o</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math>
</span>
</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">ATP</td>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e100">
<math id="M100" display="inline" overflow="linebreak"><mrow><msub><mrow><mover><mrow><mi>T</mi></mrow><mo stretchy="false">^</mo></mover></mrow><mrow><mi>a</mi><mo stretchy="false">|</mo><mi>b</mi></mrow></msub><mo>=</mo><mo> </mo><msub><mi>K</mi><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub><mo>,</mo><mo> </mo><msub><mi>C</mi><mrow><mi>a</mi></mrow></msub></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">×</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">Output_Layer</td>
<td align="left" rowspan="1" colspan="1">Vector (H<sub>i</sub>)</td>
<td align="left" rowspan="1" colspan="1">Scaler (M<sub>i</sub>)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">Weighted Sum</td>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e101">
<math id="M101" display="inline" overflow="linebreak"><mrow><msub><mi>W</mi><mrow><mi>x</mi></mrow></msub><mo>=</mo><msub><mo>∑</mo><mrow><mi>j</mi></mrow></msub><mrow><msub><mi>C</mi><mrow><mi>x</mi><mo>,</mo><mi>a</mi></mrow></msub><mo>*</mo><mo> </mo><msub><mrow><mover><mrow><mi>T</mi></mrow><mo stretchy="false">^</mo></mover></mrow><mrow><mi>a</mi><mo stretchy="false">|</mo><mi>b</mi></mrow></msub></mrow></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e102">
<math id="M102" display="inline" overflow="linebreak"><mrow><msub><mi>Z</mi><mrow><mi>x</mi></mrow></msub><mo>=</mo><mo> </mo><msubsup><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mn>3</mn></mrow></msubsup><mrow><msub><mi>C</mi><mrow><mi>x</mi></mrow></msub><msub><mi>Z</mi><mrow><mi>a</mi></mrow></msub><mo>+</mo><mi>Q</mi></mrow></mrow></math>
</span>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>In LGD_Net, each layer of the CapsNet design has its encoder and decoder. Encoders typically consist of three layers: the convolutional layer (ConvL), the PrimaryCaps (PCAP) layer, and the DigitCaps (DCAP) layer [<a href="#pone.0327419.ref065" class="usa-link" aria-describedby="pone.0327419.ref065">65</a>]. Decoders, on the other hand, typically consist of three FCLs. The structure of LGD_Net used for the classification of LGDs is illustrated in <a href="#pone.0327419.g004" class="usa-link">Fig 4</a>.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g004"><h5 class="obj_head">Fig 4. Structure of LGD_Net model for the classification of LGDs using CT scans.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/9614b281f1dc/pone.0327419.g004.jpg" loading="lazy" height="511" width="775" alt="Fig 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>In the proposed LGD_Net model, the initial ConvL consists of 512 kernels that are 5 × 5 × 1 in size and have a ReLU function. The value of stride to set to 01 for the initial ConvL of the LGD_Net model. This layer is responsible for converting the individual pixel intensity data into the activity levels of nearby feature detectors. The translated output is subsequently passed to the PCAP layer. PCAP layer is the name given to a ConvL, and it features 32 channels of 8-D ConvL capsules. This represents that the PCAP layer of 8D ConvL capsules contains a 5 × 5 kernel with a stride having a value of 2. The responsibility of the PCAP layer is to generate the original CT scan images of LGDs. To do this, it performs inverse graphics [<a href="#pone.0327419.ref076" class="usa-link" aria-describedby="pone.0327419.ref076">76</a>,<a href="#pone.0327419.ref077" class="usa-link" aria-describedby="pone.0327419.ref077">77</a>] and reverse engineering [<a href="#pone.0327419.ref078" class="usa-link" aria-describedby="pone.0327419.ref078">78</a>,<a href="#pone.0327419.ref079" class="usa-link" aria-describedby="pone.0327419.ref079">79</a>].</p>
<p>Following this, the LGD_Net consists of 5 × 5 × 256 kernels to an input volume that is 20 × 20 × 256, the capsule results in the production of a tensor with an output of 8-D that is 6 × 6. Considering that there are 8-D capsules included within the proposed model, the output would have the dimensions 6 × 6, 8 × 8, and 32 kernels. The input for each of the 16-D capsules that go into making up a class in the DCAP layer is taken from the capsule that is positioned immediately before it in the stack. The 8 × 16 weight matrix, denoted by the notation <em>W</em><sub><em>(s,t)</em></sub><em>,</em> is utilized to carry out ATPs on each 8-D capsule. The Eq (23) is used to encode the (<em>E</em><sub><em>(s,t)</em></sub>) the spatial relation that exists between the CT scan images of LGD.</p>
<table class="disp-formula p" id="pone.0327419.e103"><tr>
<td class="formula"><math id="M103" display="block" overflow="linebreak"><mrow><msub><mi>E</mi><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mo> </mo><msub><mi>W</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msub><mo>*</mo><mo> </mo><msub><mi>H</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msub><mo>*</mo><mo> </mo><msub><mi>V</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msub></mrow></math></td>
<td class="label">(23)</td>
</tr></table>
<p>whereas the weight matrix is denoted by <em>W</em><sub><em>(s,t)</em></sub> and <em>H</em><sub><em>(s,t)</em></sub> shows the component vector. Additionally, the input vector of the LGD CT scan is represented by <em>V</em><sub><em>t</em></sub><em>.</em> To get the current value of the capsule (<em>C</em>), the following <a href="#pone.0327419.e104" class="usa-link">Eq (24)</a> is used to calculate the sum input vector and its weight.</p>
<table class="disp-formula p" id="pone.0327419.e104"><tr>
<td class="formula"><math id="M104" display="block" overflow="linebreak"><mrow><mtable columnspacing="2pt" displaystyle="true" rowspacing="3pt"><mtr><mtd columnalign="right"><msub><mi>A</mi><mrow><mi>i</mi></mrow></msub><mo>=</mo><mo> </mo><mo>∑</mo><msub><mi>\nolimits</mi><mrow><mi>t</mi></mrow></msub><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msub><mo>*</mo><mo> </mo><msub><mi>C</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msub></mtd></mtr></mtable></mrow></math></td>
<td class="label">(24)</td>
</tr></table>
<p>After that, we put the encode (<em>E</em><sub><em>(s,t)</em></sub>) the spatial relation as mentioned in Eq (23) to (24), and the following Eq (25) is obtained for measuring the weighted sum of the input vector of CT scan of LGDs.</p>
<table class="disp-formula p" id="pone.0327419.e105"><tr>
<td class="formula"><math id="M105" display="block" overflow="linebreak"><mrow><mtable columnspacing="2pt" displaystyle="true" rowspacing="3pt"><mtr><mtd columnalign="right"><msub><mi>A</mi><mrow><mi>i</mi></mrow></msub><mo>=</mo><mo> </mo><mo>∑</mo><msub><mi>\nolimits</mi><mrow><mi>t</mi></mrow></msub><mrow><mo>*</mo><mo> </mo><msub><mi>H</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msub><mo>*</mo><mo> </mo><msub><mi>V</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">)</mo></mrow><mo>*</mo><mo> </mo><msub><mi>C</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msub></mtd></mtr></mtable></mrow></math></td>
<td class="label">(25)</td>
</tr></table>
<p>The <em>C</em><sub><em>(i,j)</em></sub> represents the routing softmax method (RSM). The RSM is calculated by using the Eq (26).</p>
<table class="disp-formula p" id="pone.0327419.e106"><tr>
<td class="formula"><math id="M106" display="block" overflow="linebreak"><mrow><msub><mi>C</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msub></mrow></msup></mrow><mrow><msub><mo>∑</mo><mrow><mi>G</mi></mrow></msub><msup><mi>e</mi><mrow><msub><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></msub></mrow></msup></mrow></mfrac></mrow></math></td>
<td class="label">(26)</td>
</tr></table>
<p>Then, the value of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e107"><math id="M107" display="inline" overflow="linebreak"><mrow><msub><mi>C</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msub></mrow></math></span> is entered in the Eq (25). The final Eq (27) is used by the LGD_Net model for calculating the weighted sum of the input vector.</p>
<table class="disp-formula p" id="pone.0327419.e108"><tr>
<td class="formula"><math id="M108" display="block" overflow="linebreak"><mrow><mtable columnspacing="2pt" displaystyle="true" rowspacing="3pt"><mtr><mtd columnalign="right"><msub><mi>A</mi><mrow><mi>i</mi></mrow></msub><mo>=</mo><mo> </mo><mo>∑</mo><msub><mi>\nolimits</mi><mrow><mi>y</mi></mrow></msub><mrow><msub><mrow><mo stretchy="false">(</mo><mi>W</mi></mrow><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></msub><mo>*</mo><mo> </mo><msub><mi>H</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></msub><mo>*</mo><mo> </mo><msub><mi>V</mi><mrow><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">)</mo></mrow><mo>*</mo><mo stretchy="false">(</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msub></mrow></msup></mrow><mrow><msub><mo>∑</mo><mrow><mi>G</mi></mrow></msub><msup><mi>e</mi><mrow><msub><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></msub></mrow></msup></mrow></mfrac><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow></math></td>
<td class="label">(27)</td>
</tr></table>
<p>This study uses the LGD_Net model for the multiclassification of LGDs, their probability ranges from 0 to 1. So, the squashing function is applied, which is measured by using Eq (28).</p>
<table class="disp-formula p" id="pone.0327419.e109"><tr>
<td class="formula"><math id="M109" display="block" overflow="linebreak"><mrow><msub><mi>F</mi><mrow><mi>N</mi></mrow></msub><mo>=</mo><mo> </mo><mfrac><mrow><msup><mrow><msub><mrow><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>F</mi></mrow><mrow><mi>N</mi></mrow></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow><mrow><mn>1</mn><mo>+</mo><mo> </mo><msup><mrow><msub><mrow><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>F</mi></mrow><mrow><mi>N</mi></mrow></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mfrac><mo> </mo><mfrac><mrow><msub><mi>F</mi><mrow><mi>N</mi></mrow></msub></mrow><mrow><msub><mrow><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>F</mi></mrow><mrow><mi>N</mi></mrow></msub><mo stretchy="false">|</mo><mo stretchy="false">|</mo></mrow></mfrac></mrow></math></td>
<td class="label">(28)</td>
</tr></table>
<p>To gain an appropriate ratio, the low-level capsule (LLC) to the high-level capsule (HLC) ratio is gradually modified based on the experiment’s results. Algorithm 2 presents the pseudocode of the LGD_Net model by distributing the LLC to HLC using CapsNet. The reason for using Algorithm 2 is to process the several iterations at the time of distributing the LLC to HLC is is progressively updated depending on the output of the HLC and the finest outcome is achieved.</p>
<p>
<strong>Algorithm 2: Proposed LGD_Net for processing all capsules.</strong>
</p>
<p><strong>Input Parameters</strong>: Capsule Network = C<sub>CAP_NET</sub>; Layers in CapsNet = L<sub><em>CAP_</em>LAYERS</sub>; Weighted Sum = W<sub>CAP_SUM</sub>.</p>
<p><strong>Output:</strong> Distributing the output of LLC to HLC</p>
<p>1 <strong>For</strong> C<sub>CAP_NET</sub> in L<sub><em>CAP_</em>LAYERS</sub>:</p>
<p>2    <strong>do:</strong></p>
<p>3      C<sub>CAP_NET</sub> (L<sub><em>CAP_</em>LAYERS</sub> + 1) <strong>then</strong></p>
<p>4    <strong>Set</strong> W<sub>CAP_SUM</sub> = 0</p>
<p>5    <strong>while</strong> K = 1:</p>
<p>6       C<sub>CAP_NET</sub> in L<sub><em>CAP_</em>LAYERS</sub>:</p>
<p>7       C<sub><em>(i,j)</em></sub>// Calculating by using Eq (26)</p>
<p>8    <strong>End</strong></p>
<p>9 <strong>End</strong></p>
<p>10 <strong>For</strong> C<sub>CAP_NET</sub> in L<sub><em>CAP_</em>LAYERS</sub> + 1:</p>
<p>11    <strong>do:</strong></p>
<p>12      A<sub>i</sub>, F<sub>N</sub>// Calculating by using Eqs (25, 28)</p>
<p>13 <strong>For</strong> C<sub>CAP_NET</sub> of <em>L</em> in L<sub><em>CAP_</em>LAYERS</sub> + 1:</p>
<p>14    E<sub><em>(s,t)</em></sub>// Calculating by using Eq (23)</p>
<p>15 <strong>For</strong> C<sub>CAP_NET</sub> of K in L<sub><em>CAP_</em>LAYERS</sub> AND L in L<sub><em>CAP_</em>LAYERS</sub>:</p>
<p>16    <strong>do:</strong></p>
<p>17      T<sub><em>(i,j)</em></sub> ← T<sub><em>(i,j)</em></sub> + <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e110"><math id="M110" display="inline" overflow="linebreak"><mrow><mover><mrow><mi>T</mi></mrow><mo stretchy="false">^</mo></mover><mi>i</mi></mrow></math></span>|j</p>
<p>18    <strong>return</strong> T<sub><em>(i,j)</em></sub></p>
<p>19    <strong>End</strong></p>
<p>20 <strong>End</strong></p>
<p>The LGD_Net model extracts the dominant features from CT scan images with the help of the CapsNet model and feeds these features to ELM for the classification of several LGDs such as COO, PNEUM, NOR, TB, and NOR. For this study, ELM contains a single hidden unit. The purpose of this layer is to auto-tune the features obtained from CapsNet as shown in <a href="#pone.0327419.g004" class="usa-link">Fig 4</a>. The ELM only makes use of a single hidden layer, thus there is no need to make any modifications to that particular component of the network. To achieve a high level of accuracy and increase its overall speed, ELM makes use of the kernel function. ELM uses weight-biased autotuning in conjunction with non-zero activation functions; thus, the key benefits are enhanced approximation and reduced training error [<a href="#pone.0327419.ref080" class="usa-link" aria-describedby="pone.0327419.ref080">80</a>,<a href="#pone.0327419.ref081" class="usa-link" aria-describedby="pone.0327419.ref081">81</a>]. To measure the output of ELM, the following Eqs (29–30) are used.</p>
<table class="disp-formula p" id="pone.0327419.e111"><tr>
<td class="formula"><math id="M111" display="block" overflow="linebreak"><mrow><mtable columnspacing="2pt" displaystyle="true" rowspacing="3pt"><mtr><mtd columnalign="right"><msub><mi>M</mi><mrow><mi>G</mi></mrow></msub><mo>=</mo><mo>∑</mo><msubsup><mi>\nolimits</mi><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>G</mi></mrow></msubsup><mrow><msub><mi>E</mi><mrow><mi>a</mi></mrow></msub><mi>×</mi><msub><mi>L</mi><mrow><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mtd></mtr></mtable></mrow></math></td>
<td class="label">(29)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e112"><tr>
<td class="formula"><math id="M112" display="block" overflow="linebreak"><mrow><mtable columnspacing="2pt" displaystyle="true" rowspacing="3pt"><mtr><mtd columnalign="right"><msub><mi>M</mi><mrow><mi>G</mi></mrow></msub><mo>=</mo><mo>∑</mo><msubsup><mi>\nolimits</mi><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>G</mi></mrow></msubsup><mrow><msub><mi>E</mi><mrow><mi>a</mi></mrow></msub><mi>×</mi><mi>L</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msub><mi>J</mi><mrow><mi>a</mi></mrow></msub><mi>×</mi><msub><mi>K</mi><mrow><mi>b</mi></mrow></msub><mo>+</mo><mo> </mo><msub><mi>L</mi><mrow><mi>a</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><mo> </mo><mi>b</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>…</mi><mo>,</mo><mo> </mo><mi>N</mi><mo>.</mo></mrow></mtd></mtr></mtable></mrow></math></td>
<td class="label">(30)</td>
</tr></table>
<p>Here, the variable <em>L</em> is used to measure the bias vector [<a href="#pone.0327419.ref082" class="usa-link" aria-describedby="pone.0327419.ref082">82</a>]. The total training samples used for the LGD_Net model are presented by the parameter <em>N</em>. Additionally, the input vector and hidden units are represented as <em>K</em> and <em>G</em>, respectively [<a href="#pone.0327419.ref083" class="usa-link" aria-describedby="pone.0327419.ref083">83</a>–<a href="#pone.0327419.ref085" class="usa-link" aria-describedby="pone.0327419.ref085">85</a>]. Additionally, the value of <em>b</em> displays the weight vector between the hidden and output layers, while <em>J</em> is the vector that lies between the input layer and the hidden layer. Like ANN which uses backpropagation, for this study, we used <em>β</em>-matrix as a pseudo-inverse. Eq (31) is used to perform this operation.</p>
<table class="disp-formula p" id="pone.0327419.e113"><tr>
<td class="formula"><math id="M113" display="block" overflow="linebreak"><mrow><mi>R</mi><mo>=</mo><mi>H</mi><mo>*</mo><mi>β</mi></mrow></math></td>
<td class="label">(31)</td>
</tr></table>
<p>The values of <em>R, H</em>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e114"><math id="M114" display="inline" overflow="linebreak"><mrow><mi>β</mi></mrow></math></span> are calculated using Eq (32) and (33), respectively. Additionally, the resultant matrix is described in Eq (34).</p>
<table class="disp-formula p" id="pone.0327419.e115"><tr>
<td class="formula"><math id="M115" display="block" overflow="linebreak"><mrow><mi>H</mi><mo>=</mo><mo>[</mo><mtable><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi>J</mi><mrow><mn>1</mn></mrow><mo>*</mo><mo> </mo><mi>K</mi><mrow><mn>1</mn></mrow><mo>+</mo><mo> </mo><mi>L</mi><mrow><mn>1</mn></mrow><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>s</mi><mo> </mo><mo stretchy="false">(</mo><mi>W</mi><mrow><mi>G</mi></mrow><mo>*</mo><mo> </mo><mi>X</mi><mrow><mn>1</mn></mrow><mo>+</mo><mo> </mo><mi>V</mi><mrow><mi>G</mi></mrow><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi>J</mi><mrow><mi>i</mi></mrow><mo>*</mo><mo> </mo><mi>K</mi><mrow><mi>N</mi></mrow><mo>+</mo><mo> </mo><mi>L</mi><mrow><mi>N</mi></mrow><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>s</mi><mo> </mo><mo stretchy="false">(</mo><mi>W</mi><mrow><mi>G</mi></mrow><mo>*</mo><mo> </mo><mi>X</mi><mrow><mi>N</mi></mrow><mo>+</mo><mo> </mo><mi>V</mi><mrow><mi>G</mi></mrow><mo stretchy="false">)</mo></mtd></mtr></mtable><mo stretchy="false">]</mo><mrow><mi>N</mi><mo>*</mo><mi>G</mi></mrow></mrow></math></td>
<td class="label">(32)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e116"><tr>
<td class="formula"><math id="M116" display="block" overflow="linebreak"><mrow><mi>β</mi><mo>=</mo><mo> </mo><mo>[</mo><mtable><mtr><mtd><msubsup><mi>β</mi><mrow><mn>1</mn></mrow><mrow><mi>T</mi></mrow></msubsup></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msubsup><mi>β</mi><mrow><mi>G</mi></mrow><mrow><mi>T</mi></mrow></msubsup></mtd></mtr></mtable><mo>]</mo><mrow><mi>G</mi><mo>*</mo><mi>L</mi></mrow><mo> </mo><mo> </mo><mo>,</mo><mo> </mo><mo> </mo><mi>R</mi><mo>=</mo><mo> </mo><mo>[</mo><mtable><mtr><mtd><msubsup><mi>R</mi><mrow><mn>1</mn></mrow><mrow><mi>T</mi></mrow></msubsup></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msubsup><mi>R</mi><mrow><mi>N</mi></mrow><mrow><mi>T</mi></mrow></msubsup></mtd></mtr></mtable><mo>]</mo><mrow><mi>N</mi><mo>*</mo><mi>L</mi></mrow><mo> </mo><mo> </mo><mo> </mo></mrow></math></td>
<td class="label">(33)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e117"><tr>
<td class="formula"><math id="M117" display="block" overflow="linebreak"><mrow><mo>[</mo><mtable><mtr><mtd><msubsup><mi>R</mi><mrow><mn>1</mn></mrow><mrow><mi>T</mi></mrow></msubsup></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msubsup><mi>R</mi><mrow><mi>N</mi></mrow><mrow><mi>T</mi></mrow></msubsup></mtd></mtr></mtable><mo>]</mo><mrow><mi>N</mi><mo>*</mo><mi>L</mi></mrow><mo>=</mo><mo>[</mo><mtable><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi>J</mi><mrow><mn>1</mn></mrow><mo>*</mo><mo> </mo><mi>K</mi><mrow><mn>1</mn></mrow><mo>+</mo><mo> </mo><mi>L</mi><mrow><mn>1</mn></mrow><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>s</mi><mo> </mo><mo stretchy="false">(</mo><mi>W</mi><mrow><mi>G</mi></mrow><mo>*</mo><mo> </mo><mi>X</mi><mrow><mn>1</mn></mrow><mo>+</mo><mo> </mo><mi>V</mi><mrow><mi>G</mi></mrow><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋱</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi>J</mi><mrow><mi>i</mi></mrow><mo>*</mo><mo> </mo><mi>K</mi><mrow><mi>N</mi></mrow><mo>+</mo><mo> </mo><mi>L</mi><mrow><mi>N</mi></mrow><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>s</mi><mo> </mo><mo stretchy="false">(</mo><mi>W</mi><mrow><mi>G</mi></mrow><mo>*</mo><mo> </mo><mi>X</mi><mrow><mi>N</mi></mrow><mo>+</mo><mo> </mo><mi>V</mi><mrow><mi>G</mi></mrow><mo stretchy="false">)</mo></mtd></mtr></mtable><mo>]</mo><mrow><mi>N</mi><mo>*</mo><mi>G</mi></mrow><mo>*</mo><mo> </mo><mo> </mo><mo>[</mo><mtable><mtr><mtd><msubsup><mi>β</mi><mrow><mn>1</mn></mrow><mrow><mi>T</mi></mrow></msubsup></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msubsup><mi>β</mi><mrow><mi>G</mi></mrow><mrow><mi>T</mi></mrow></msubsup></mtd></mtr></mtable><mo>]</mo><mrow><mi>G</mi><mo>*</mo><mi>L</mi></mrow></mrow></math></td>
<td class="label">(34)</td>
</tr></table>
<p>Here, the variables <em>H, R,</em> and <em>L</em> present the hidden layer, training data, and no. of output, respectively. The pseudocode of the LGD_Net with ELM is presented in Algorithm 3.</p>
<p><strong>Algorithm 3:</strong> Proposed LGD_Net model with CapsNet and ELM.</p>
<p><strong>Input:</strong> LGDs Image  = C<sub><em>CT_SCAN</em></sub>; Epoch = E<sub><em>EPOCH</em></sub>; Capsule Network = C<sub><em>CAPS-NET</em></sub>; Features = C<sub><em>CAPS_FE</em></sub>; ELMs = E<sub><em>LGD_ELM</em></sub>.</p>
<p><strong>Output:</strong> Classification of several LGDs i.e., COO, PNEUM, NOR, TB, and NOR.</p>
<p><strong>1.</strong> C<sub><em>CT_SCAN</em></sub> ≠ {<strong>∅</strong>}, <strong>then</strong></p>
<p>2.    <strong>For</strong> a = 1:</p>
<p>3.     C<sub><em>CAPS_FE</em> </sub>= C<sub><em>CAPS-NET</em></sub> (C<sub><em>CT_SCAN</em></sub>):</p>
<p>4.     <strong>Then</strong>: Execute <strong>Algorithm 2</strong></p>
<p>5. <strong>End</strong></p>
<p>6. O = E<sub><em>LGD_ELM</em></sub> (C<sub><em>CAPS_FE</em></sub>)</p>
<p>7.    <strong>If</strong> (O<sub><em> </em></sub>= = T):</p>
<p>8.     <strong>return</strong> D<sub>LGDs</sub> (<strong>True</strong>)</p>
<p>9.    <strong>Else:</strong></p>
<p>10.     <strong>False//Goto Step 7</strong></p>
<p>11. <strong>End</strong></p></section></section><section id="sec013"><h3 class="pmc_sec_title">3.5 Performance evaluation</h3>
<p>By using the confusion matrix (C_Mat), the performance of the proposed LGD_Net model and D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> are measured. The C_Mat shows the original <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e118"><math id="M118" display="inline" overflow="linebreak"><mrow><msup><mrow><mo stretchy="false">(</mo><mi>γ</mi></mrow><mrow><mi>O</mi><mi>r</mi><mi>i</mi></mrow></msup><mo stretchy="false">)</mo></mrow></math></span> and predicted (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e119"><math id="M119" display="inline" overflow="linebreak"><mrow><msup><mi>γ</mi><mrow><mi>P</mi><mi>r</mi><mi>e</mi></mrow></msup></mrow></math></span>) value obtained by these models. For this study, we executed the LGD_Net and D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> models up to 30 epochs to get the output. The C_Mat for each epoch <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e120"><math id="M120" display="inline" overflow="linebreak"><mrow><mi>e</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo> </mo><mi>…</mi><mi>E</mi></mrow></math></span> are presented in Eq (35).</p>
<table class="disp-formula p" id="pone.0327419.e121"><tr>
<td class="formula"><math id="M121" display="block" overflow="linebreak"><mrow><msup><mi>γ</mi><mrow><mi>O</mi><mi>r</mi><mi>i</mi></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mo> </mo><mo>[</mo><mtable><mtr><mtd><mfrac><mrow><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msup><mi>C</mi><mrow><mi>t</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">|</mo></mrow></mrow><mrow><mn>2</mn></mrow></mfrac></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><mfrac><mrow><msup><mi>C</mi><mrow><mi>t</mi></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac><mo fence="true" form="postfix" stretchy="true">|</mo></mrow></mtd></mtr></mtable><mo>]</mo><mo>,</mo><mo> </mo><mo>∀</mo><mo> </mo><mi>e</mi><mo> </mo><mo>∈</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>…</mi><mo>,</mo><mo> </mo><mi>E</mi></mrow></math></td>
<td class="label">(35)</td>
</tr></table>
<p>where <em>C</em><sup><em>t</em></sup> shows the test set of the LGDs data, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e122"><math id="M122" display="inline" overflow="linebreak"><mrow><mfrac><mrow><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msup><mi>C</mi><mrow><mi>t</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">|</mo></mrow></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></math></span> represent a balanced enhanced LGDs dataset obtained after applying the ATPs. In this study, when the LGD_Net model is applied to <em>C</em><sup><em>t</em></sup>, then the diagonal elements change into <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e123"><math id="M123" display="inline" overflow="linebreak"><mrow><mfrac><mrow><mo stretchy="false">|</mo><mi>c</mi><mo stretchy="false">|</mo></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></math></span>. The following Eq (36) is the operation of C_Mat used in this study.</p>
<table class="disp-formula p" id="pone.0327419.e124"><tr>
<td class="formula"><math id="M124" display="block" overflow="linebreak"><mrow><msup><mrow><mi>C</mi><mi>_</mi><mi>M</mi><mi>a</mi><mi>t</mi></mrow><mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mo> </mo><mo>[</mo><mtable><mtr><mtd><msub><mi>l</mi><mrow><mn>1</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mtd><mtd><msub><mi>l</mi><mrow><mn>2</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><msub><mi>l</mi><mrow><mn>3</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mtd><mtd><msub><mi>l</mi><mrow><mn>4</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mtd></mtr></mtable><mo>]</mo><mo>,</mo><mo> </mo><mo>∀</mo><mo> </mo><mi>e</mi><mo> </mo><mo>∈</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mo> </mo><mi>e</mi></mrow></math></td>
<td class="label">(36)</td>
</tr></table>
<p>From above Eq (36), the four variables <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e125"><math id="M125" display="inline" overflow="linebreak"><mrow><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msub><mi>l</mi><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>,</mo><mo> </mo><msub><mi>l</mi><mrow><mn>2</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>,</mo><mo> </mo><msub><mi>l</mi><mrow><mn>3</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>,</mo><mo> </mo><msub><mi>l</mi><mrow><mn>4</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></span> Represent true positive (<em>TP</em>), false positive (<em>FP</em>), false negative (<em>FN</em>), and true negative (<em>TN</em>) at the e<sup>th</sup> run. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e126"><math id="M126" display="inline" overflow="linebreak"><mrow><msub><mrow><mo stretchy="false">{</mo><mi>l</mi></mrow><mrow><mn>1</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mi>}</mi></mrow></math></span> shows that the LC, is accurately COO, NOR, PNEUM, and TB is correctly classified. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e127"><math id="M127" display="inline" overflow="linebreak"><mrow><msub><mrow><mo stretchy="false">{</mo><mi>l</mi></mrow><mrow><mn>3</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mi>}</mi></mrow></math></span> shows that the COO is wrongly classified as NOR or vice versa. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e128"><math id="M128" display="inline" overflow="linebreak"><mrow><msub><mrow><mo stretchy="false">{</mo><mi>l</mi></mrow><mrow><mn>2</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mi>}</mi></mrow></math></span> presents that NOR CT scans are inaccurately classified as COO, LC, PNEUM, and TB. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e129"><math id="M129" display="inline" overflow="linebreak"><mrow><msub><mrow><mo stretchy="false">{</mo><mi>l</mi></mrow><mrow><mn>4</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mi>}</mi></mrow></math></span> the PNEUM images are accurately classified as PNEUM. Several metrics such as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e130"><math id="M130" display="inline" overflow="linebreak"><mrow><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msup><mi>Z</mi><mrow><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>,</mo><mo> </mo><msup><mi>Z</mi><mrow><mn>2</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>,</mo><mo> </mo><msup><mi>Z</mi><mrow><mn>3</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>,</mo><mo> </mo><msup><mi>Z</mi><mrow><mn>4</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>,</mo><msup><mi>Z</mi><mrow><mn>5</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><msup><mrow><mo>,</mo><mi>Z</mi></mrow><mrow><mn>6</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></span> were used to measure the efficacy of LGD_Net and D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> model. Where accuracy is denoted by (Z<sup>1</sup>), precision is denoted by (Z<sup>2</sup>), recall is denoted by (Z<sup>3</sup>), F1-score is denoted by (Z<sup>4</sup>), dice similarity coefficient (DSC) is denoted by (Z<sup>5</sup>), and an AUC is denoted by (Z<sup>6</sup>) and these are mathematically written as Eqs (37–42).</p>
<table class="disp-formula p" id="pone.0327419.e131"><tr>
<td class="formula"><math id="M131" display="block" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mo> </mo><mfrac><mrow><msub><mi>l</mi><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>l</mi><mrow><mn>4</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>l</mi><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>+</mo><mo> </mo><msub><mi>l</mi><mrow><mn>2</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>+</mo><mo> </mo><msub><mi>l</mi><mrow><mn>3</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>+</mo><mo> </mo><msub><mi>l</mi><mrow><mn>4</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></td>
<td class="label">(37)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e132"><tr>
<td class="formula"><math id="M132" display="block" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>2</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mo> </mo><mfrac><mrow><msub><mi>l</mi><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>l</mi><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>+</mo><mo> </mo><msub><mi>l</mi><mrow><mn>3</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></td>
<td class="label">(38)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e133"><tr>
<td class="formula"><math id="M133" display="block" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>3</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mo> </mo><mfrac><mrow><msub><mi>l</mi><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>l</mi><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>+</mo><mo> </mo><msub><mi>l</mi><mrow><mn>2</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow></mfrac><mo> </mo></mrow></math></td>
<td class="label">(39)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327419.e134"><tr>
<td class="formula"><math id="M134" display="block" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>4</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mo> </mo><mfrac><mrow><mn>2</mn><mo> </mo><mi>×</mi><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mi>z</mi><mrow><mn>3</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo> </mo><mi>×</mi><mo> </mo><msup><mi>z</mi><mrow><mn>2</mn></mrow></msup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow><mrow><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><msup><mi>z</mi><mrow><mn>3</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>+</mo><mo> </mo><msup><mi>z</mi><mrow><mn>2</mn></mrow></msup><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></mfrac></mrow></math></td>
<td class="label">(40)</td>
</tr></table>
<p>In addition, the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e135"><math id="M135" display="inline" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>5</mn></mrow></msup></mrow></math></span> is applied to evaluate the effectiveness of the segmentation performed by the SegCaps. The Eq (41) is used to measure the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e136"><math id="M136" display="inline" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>5</mn></mrow></msup></mrow></math></span>.</p>
<table class="disp-formula p" id="pone.0327419.e137"><tr>
<td class="formula"><math id="M137" display="block" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>5</mn></mrow></msup><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mo> </mo><mfrac><mrow><mn>2</mn><mi>×</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∩</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><mrow><mi>A</mi><mo>+</mo><mi>B</mi></mrow></mfrac></mrow></math></td>
<td class="label">(41)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e138"><math id="M138" display="inline" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>5</mn></mrow></msup><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>B</mi></mrow></math></span> is used to assess the amount of spatial overlap between two different segmentations, with A and B standing for the target regions. Lastly, the Eq (42) is used to calculate the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327419.e139"><math id="M139" display="inline" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>6</mn></mrow></msup></mrow></math></span> of the LGD_Net and four D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> models.</p>
<table class="disp-formula p" id="pone.0327419.e140"><tr>
<td class="formula"><math id="M140" display="block" overflow="linebreak"><mrow><msup><mi>Z</mi><mrow><mn>6</mn></mrow></msup><mo stretchy="false">(</mo><mi>T</mi><mi>P</mi><mi>R</mi><mo>,</mo><mo> </mo><mi>F</mi><mi>P</mi><mi>R</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mo> </mo><mfrac><mrow><msub><mi>l</mi><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>l</mi><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>+</mo><mo> </mo><msub><mi>l</mi><mrow><mn>3</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true" form="postfix" stretchy="true">]</mo></mrow><mo>,</mo><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mo> </mo><mfrac><mrow><msub><mi>l</mi><mrow><mn>2</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>l</mi><mrow><mn>2</mn></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo><mo>+</mo><mo> </mo><msub><mi>l</mi><mrow><mn>4</mn><mo> </mo></mrow></msub><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></td>
<td class="label">(42)</td>
</tr></table></section></section><section id="sec014"><h2 class="pmc_sec_title">4. Results and discussions</h2>
<p>This section contains the outcomes produced by the LGD_Net, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> in classifying the several LGDs using CT scan images.</p>
<section id="sec015"><h3 class="pmc_sec_title">4.1 Experimental configuration and hyperparameters</h3>
<p>In this study, an open-source TensorFlow (TF) library version 2.15.0 was used to implement the proposed LGD_Net model. The TF version is also used for the development of four models such as D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup>. However, the Keras library is also used to execute the backend operations. The implementation that has no link with DL models was programmed in Python 3.12.5v. The Imblearn library having version 0.12.4 was used to implement the BL_SMT. Additionally, the affine library was used to perform the ATP operations. This experiment was performed on a PC having Windows 10. The specifications of the PC are core i8 11<sup>th</sup> generation, having 02 RAMs of 16 GB, and 11 GB NVIDIA GPU. Several hyperparameters of the LGD_Net model are fine-tuned. The detailed description of the hyperparameters is demonstrated in <a href="#pone.0327419.t005" class="usa-link">Table 5</a>.</p>
<section class="tw xbox font-sm" id="pone.0327419.t005"><h4 class="obj_head">Table 5. Hyperparameters of LGD_Net.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Sr#</th>
<th align="left" rowspan="1" colspan="1">Hyperparameters</th>
<th align="left" rowspan="1" colspan="1">Value</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">Learning_Rate</td>
<td align="left" rowspan="1" colspan="1">0.00001</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">Dropout_Value</td>
<td align="left" rowspan="1" colspan="1">0.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">Batch_size</td>
<td align="left" rowspan="1" colspan="1">32</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">Momentum_CapsNet</td>
<td align="left" rowspan="1" colspan="1">0.9</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">Activation Function</td>
<td align="left" rowspan="1" colspan="1">ReLU</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">6</td>
<td align="left" rowspan="1" colspan="1">Epochs</td>
<td align="left" rowspan="1" colspan="1">30</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec016"><h3 class="pmc_sec_title">4.2 Results analysis</h3>
<p>For evaluating the performance in terms of Z<sup>1</sup>, Z<sup>2</sup>, Z<sup>3</sup>, Z<sup>4</sup>, Z<sup>5</sup>, and Z<sup>6</sup>, we compared the outcomes of the LGD_Net model with D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> on enhanced LGDs dataset obtained after applying BL-SMT and ATPs. Additionally, we also evaluate the performance of the LGD_Net without using ATPs and BL-SMT. The comprehensive results are presented below:</p>
<section id="sec017"><h4 class="pmc_sec_title">4.2.1 Performance of LGD_Net in terms of accuracy (Z<sup>1</sup>).</h4>
<p><a href="#pone.0327419.t006" class="usa-link">Table 6</a> contains the comprehensive obtained by using LGD_Net, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> for the classification of several LGDs using CT scans. The results reveal that the proposed LGD_Net model achieved the highest Z<sup>1</sup> of 99.71% as compared to D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup>. However, the LGD_Net model without using ATPs and BL-SMT achieved a Z<sup>1</sup> of 90.64%. The use of ATPs and BL-SMT with LGD_Net increases the Z<sup>1</sup> by 9.07%. The D<sup>1</sup> model achieved the classification Z<sup>1</sup> of 91.21%. The D<sup>2</sup> and D<sup>3</sup> achieved the Z<sup>1</sup> of 94.39% and 93.96%, respectively for classifying the LGDs using CT scans. Additionally, D<sup>4</sup> achieved a Z<sup>1</sup> of 93.82% in the classification of LGDs. The graphical representation of the LGD_Net, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> in terms of Z<sup>1</sup> is illustrated in <a href="#pone.0327419.g005" class="usa-link">Fig 5</a>.</p>
<section class="tw xbox font-sm" id="pone.0327419.t006"><h5 class="obj_head">Table 6. Extensive comparison of LGD_Net with D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> in terms Z<sup>1</sup>, Z<sup>2</sup>, Z<sup>3</sup>, Z<sup>4</sup>, and Z<sup>6</sup>.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Models</th>
<th align="left" rowspan="1" colspan="1">Loss</th>
<th align="left" rowspan="1" colspan="1">Z<sup>1</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>2</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>3</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>4</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>6</sup>
</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">D<sup>1</sup>
</td>
<td align="left" rowspan="1" colspan="1">0.161</td>
<td align="left" rowspan="1" colspan="1">91.21%</td>
<td align="left" rowspan="1" colspan="1">92.77%</td>
<td align="left" rowspan="1" colspan="1">95.39%</td>
<td align="left" rowspan="1" colspan="1">93.83%</td>
<td align="left" rowspan="1" colspan="1">99.59%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">D<sup>2</sup>
</td>
<td align="left" rowspan="1" colspan="1">0.095</td>
<td align="left" rowspan="1" colspan="1">94.39%</td>
<td align="left" rowspan="1" colspan="1">96.25%</td>
<td align="left" rowspan="1" colspan="1">96.39%</td>
<td align="left" rowspan="1" colspan="1">96.34%</td>
<td align="left" rowspan="1" colspan="1">99.72%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">D<sup>3</sup>
</td>
<td align="left" rowspan="1" colspan="1">0.193</td>
<td align="left" rowspan="1" colspan="1">93.96%</td>
<td align="left" rowspan="1" colspan="1">96.53%</td>
<td align="left" rowspan="1" colspan="1">96.51%</td>
<td align="left" rowspan="1" colspan="1">96.86%</td>
<td align="left" rowspan="1" colspan="1">99.88%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">D<sup>4</sup>
</td>
<td align="left" rowspan="1" colspan="1">0.115</td>
<td align="left" rowspan="1" colspan="1">93.82%</td>
<td align="left" rowspan="1" colspan="1">96.54%</td>
<td align="left" rowspan="1" colspan="1">96.09%</td>
<td align="left" rowspan="1" colspan="1">96.78%</td>
<td align="left" rowspan="1" colspan="1">99.62%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LGD_Net without ATPs &amp; BL-SMT</td>
<td align="left" rowspan="1" colspan="1">0.267</td>
<td align="left" rowspan="1" colspan="1">90.64%</td>
<td align="left" rowspan="1" colspan="1">86.69%</td>
<td align="left" rowspan="1" colspan="1">94.14%</td>
<td align="left" rowspan="1" colspan="1">75.37%</td>
<td align="left" rowspan="1" colspan="1">99.13%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>Proposed LGD_Net Model</strong>
</td>
<td align="left" rowspan="1" colspan="1">0.023</td>
<td align="left" rowspan="1" colspan="1">99.71%</td>
<td align="left" rowspan="1" colspan="1">99.71%</td>
<td align="left" rowspan="1" colspan="1">99.69%</td>
<td align="left" rowspan="1" colspan="1">99.70%</td>
<td align="left" rowspan="1" colspan="1">99.90%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="pone.0327419.g005"><h5 class="obj_head">Fig 5. Accuracy (Z<sup>1</sup>) graph executed up to 30 epochs; (a) D<sup>1</sup>, (b) D<sup>2</sup>, (c) D<sup>3</sup>, (d) D<sup>4</sup>, (e) LGD_Net without ATPs &amp; BL-SMT, and (f) Proposed LGD_Net.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/7ca8e165251e/pone.0327419.g005.jpg" loading="lazy" height="885" width="682" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec018"><h4 class="pmc_sec_title">4.2.2 Recall (Z<sup>2</sup>) of LGD_Net model.</h4>
<p>The Z<sup>2</sup> is used to measure the proportion of TP values from actual LGD image sample data [<a href="#pone.0327419.ref086" class="usa-link" aria-describedby="pone.0327419.ref086">86</a>]. In this study, the TP means the individuals have PNEUM, and the proposed LGD_Net and other models recognize it as PNEUM. The higher Z<sup>2</sup> shows that models accurately predicted the TP LGD image samples. The Z<sup>2</sup> of LGD_Net, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> are depicted in <a href="#pone.0327419.g006" class="usa-link">Fig 6</a>. From <a href="#pone.0327419.t006" class="usa-link">Table 6</a>, the results of Z<sup>2</sup> demonstrate that the proposed LGD_Net achieved the highest results. The Z<sup>2</sup> of the LGD_Net model is 99.71%. However, LGD_Net without ATPs &amp; BL-SMT gained the Z<sup>2</sup> of 86.69%. The D<sup>1</sup> attained the Z<sup>2</sup> of 92.77%. The D<sup>2</sup> achieved the Z<sup>2</sup> of 96.25%. Additionally, Z<sup>2</sup> of 96.53% and 96.54% were achieved by D<sup>3</sup> and D<sup>4</sup>, respectively.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g006"><h5 class="obj_head">Fig 6. Recall (Z<sup>2</sup>) graph executed up to 30 epochs; (a) D<sup>1</sup>, (b) D<sup>2</sup>, (c) D<sup>3</sup>, (d) D<sup>4</sup>, (e) LGD_Net without ATPs &amp; BL-SMT, and (f) Proposed LGD_Net.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g006.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/15df9db03159/pone.0327419.g006.jpg" loading="lazy" height="895" width="694" alt="Fig 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec019"><h4 class="pmc_sec_title">4.2.3 Precision (Z<sup>3</sup>) of LGD_Net model.</h4>
<p>For this study, the Z<sup>3</sup> denotes the precision as discussed in Eq (39). The Z<sup>3</sup> of the LGD_Net, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> are compared with each other. The highest Z<sup>3</sup> of 99.69% was achieved by the LGD_Net model. The results show that LGD_Net has the capability of identifying the correct TP cases from all positive predicted cases [<a href="#pone.0327419.ref087" class="usa-link" aria-describedby="pone.0327419.ref087">87</a>–<a href="#pone.0327419.ref089" class="usa-link" aria-describedby="pone.0327419.ref089">89</a>]. The detailed results of the LGD_Net model and other models are shown in <a href="#pone.0327419.g007" class="usa-link">Fig 7</a>. The D<sup>1</sup> has achieved a significant Z<sup>3</sup> value of 95.39%. The D<sup>2</sup> has gained 96.39% of Z<sup>3</sup>. The LGD_Net model without ATPs &amp; BL_SMT has gained the Z<sup>3</sup> of 94.14%. The Z<sup>3</sup> of 96.51% and 96.09% was gained by the D<sup>3</sup> and D<sup>4</sup>.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g007"><h5 class="obj_head">Fig 7. Precision (Z<sup>3</sup>) graph executed up to 30 epochs; (a) D<sup>1</sup>, (b) D<sup>2</sup>, (c) D<sup>3</sup>, (d) D<sup>4</sup>, (e) LGD_Net without ATPs &amp; BL-SMT, and (f) Proposed LGD_Net.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g007.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/9fa15ab196a6/pone.0327419.g007.jpg" loading="lazy" height="879" width="682" alt="Fig 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec020"><h4 class="pmc_sec_title">4.2.4 F1-score (Z<sup>4</sup>) of LGD_Net model.</h4>
<p>The Z<sup>4</sup> is used to measure the harmonic mean of Z<sup>2</sup> and Z<sup>3</sup> [<a href="#pone.0327419.ref089" class="usa-link" aria-describedby="pone.0327419.ref089">89</a>–<a href="#pone.0327419.ref091" class="usa-link" aria-describedby="pone.0327419.ref091">91</a>]. The high value of Z<sup>4</sup> shows that models are efficient in classifying the LGDs using CT scan images. The results show that the LGD_Net model has attained the highest Z<sup>4</sup> value of 99.70% in identifying the LGDs as compared to other models used in this study. Without ATPs and BL-SMT, the LGD_Net model achieved the Z<sup>4</sup> of 75.37%. The use of ATPs and BL-SMT has shown a significant impact and raised the Z<sup>4</sup> to 24.33%. The D<sup>1</sup> has gained the Z<sup>4</sup> of 93.83%. The D<sup>2</sup> model attained the Z<sup>4</sup> of 96.34%. Additionally, the Z<sup>4</sup> of 96.86% and 96.78% were attained by the D<sup>3</sup> and D<sup>4</sup>, respectively. The results of Z<sup>4</sup> are depicted in <a href="#pone.0327419.g008" class="usa-link">Fig 8</a>.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g008"><h5 class="obj_head">Fig 8. F1-score (Z<sup>4</sup>) graph executed up to 30 epochs; (a) D<sup>1</sup>, (b) D<sup>2</sup>, (c) D<sup>3</sup>, (d) D<sup>4</sup>, (e) LGD_Net without ATPs &amp; BL-SMT, and (f) Proposed LGD_Net.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g008.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/211c19abadf7/pone.0327419.g008.jpg" loading="lazy" height="856" width="682" alt="Fig 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec021"><h4 class="pmc_sec_title">4.2.5 AUC (Z<sup>6</sup>) of LGD_Net model.</h4>
<p>For this study, we used Z<sup>6</sup> to represent the model capability in distinguishing the LGDs. The high AUC shows that models perform appropriately in distinguishing the TP and TN cases. The results from <a href="#pone.0327419.t006" class="usa-link">Table 6</a> show that the proposed LGD_Net attained the highest AUC of 99.90%. However, other models used in this study also perform quite well in terms of Z<sup>6</sup>. The 99.59% Z<sup>6</sup> was achieved by D<sup>1</sup>. The D2 and D3 models achieved the Z<sup>6</sup> of 99.72% and 99.88%, respectively. In last, D<sup>4</sup> and LGD_Net without ATPs &amp; BL-SMT achieved the Z<sup>6</sup> of 99.62% and 99.13%, respectively. The Z<sup>6</sup> of these models is graphically illustrated in <a href="#pone.0327419.g009" class="usa-link">Fig 9</a>.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g009"><h5 class="obj_head">Fig 9. AUC (Z<sup>6</sup>) graph executed up to 30 epochs; (a) D<sup>1</sup>, (b) D<sup>2</sup>, (c) D<sup>3</sup>, (d) D<sup>4</sup>, (e) LGD_Net without ATPs &amp; BL-SMT, and (f) Proposed LGD_Net.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g009.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/4ec3db5bf45c/pone.0327419.g009.jpg" loading="lazy" height="871" width="682" alt="Fig 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g009/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec022"><h4 class="pmc_sec_title">4.2.6 Comparison of LGD_Net with D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, D<sup>4</sup> in terms of loss.</h4>
<p>In this work, the loss represents measures of LGD_Net, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> prediction aligning with actual results. Therefore, we used a cross-entropy loss function for this work. A very low loss value of 0.023 was attained by the LGD_Net as compared to the other models such as D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup>. It shows that LGD_Net correctly predicts the LGDs from the actual LGDs CT scan images. Moreover, the LGD_Net without ATPs and BL-SMT gained a loss of 0.267. The D<sup>1</sup> has a loss of 0.161, while the D<sup>2</sup> and D<sup>3</sup> have a loss value of 0.095 and 0.193, respectively. The loss curves achieved by these models are presented in <a href="#pone.0327419.g010" class="usa-link">Fig 10</a>.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g010"><h5 class="obj_head">Fig 10. Loss curve executed up to 30 epochs; (a) D<sup>1</sup>, (b) D<sup>2</sup>, (c) D<sup>3</sup>, (d) D<sup>4</sup>, (e) LGD_Net without ATPs &amp; BL-SMT, and (f) Proposed LGD_Net.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g010.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/6160d0612e6c/pone.0327419.g010.jpg" loading="lazy" height="870" width="682" alt="Fig 10"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g010/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec023"><h4 class="pmc_sec_title">4.2.7 ROC of LGD_Net.</h4>
<p>For this study, we used ROC to represent the LGD_Net and D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> efficacy and reliability of LGD classification. The higher the ROC means that the model performance in LGDs classification is more correct. The ROC results of LGD_Net, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> are presented in <a href="#pone.0327419.g011" class="usa-link">Fig 11</a>. The results show that LGD_Net attained ROC of 0.9967 and outperforms the other models used in this work. The LGD_Net without ATPs and BL-SMT model achieved an ROC of 0.9859. The ROC values of 0.9889, 0.9959, 0.9919, and 0.9962 were achieved by the D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup>. The ROC of these models is graphically presented in <a href="#pone.0327419.g011" class="usa-link">Fig 11</a>.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g011"><h5 class="obj_head">Fig 11. ROC graphs show the performance of the models in classifying different lung diseases; (a) D<sup>1</sup>, (b) D<sup>2</sup>, (c) D<sup>3</sup>, (d) D<sup>4</sup>, (e) LGD_Net without ATPs &amp; BL-SMT, and (f) Proposed LGD_Net.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g011.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/e5293854c455/pone.0327419.g011.jpg" loading="lazy" height="946" width="744" alt="Fig 11"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g011/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec024"><h4 class="pmc_sec_title">4.2.8 AU (ROC) of LGD_Net.</h4>
<p>The AU (ROC) is used to represent the classwise performance of the LGD_Net, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> in classifying the LGDs using CT scan images as illustrated in <a href="#pone.0327419.g012" class="usa-link">Fig 12</a>. In <a href="#pone.0327419.g012" class="usa-link">Fig 12</a>, class 0 represents the COO, class 1 shows the PNEUM, class 2 shows the LC, class 3 represents the TB, and class 4 shows the NOR.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g012"><h5 class="obj_head">Fig 12. AU (ROC) graphs for classwise evaluation; (a) D<sup>1</sup>, (b) D<sup>2</sup>, (c) D<sup>3</sup>, (d) D<sup>4</sup>, (e) LGD_Net without ATPs &amp; BL-SMT, and (f) Proposed LGD_Net.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g012.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/7f73f009780f/pone.0327419.g012.jpg" loading="lazy" height="947" width="744" alt="Fig 12"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g012/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section></section><section id="sec025"><h3 class="pmc_sec_title">4.3 Segmentation of LGDs</h3>
<p>To assess the viability of the LGD_Net and other models, a total of 692 LGD CT scans were used. <a href="#pone.0327419.g013" class="usa-link">Fig 13</a> provides a segmentation of several LGDs such as COO, PNEUM, LC, and TB, which illustrates that the LGD_Net with SegCaps model works more effectively than UNET and UNET++. The results from the LGD_Net are quite close to ground truth. <a href="#pone.0327419.t007" class="usa-link">Table 7</a> presents a detailed presentation of the findings.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g013"><h4 class="obj_head">Fig 13. Segmentation of several LGDs such as COO, TB, LC, and TB using CT scan images.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g013.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/534e0ee41ba2/pone.0327419.g013.jpg" loading="lazy" height="649" width="682" alt="Fig 13"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g013/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The original image of LGD CT scans is illustrated in the first row. The second row presents the original mask of the original image LGDs. From third to fifth, predicted masks are shown that were produced by the UNET, UNET++, and LGD_Net with SegCaps, respectively.</p></figcaption></figure><section class="tw xbox font-sm" id="pone.0327419.t007"><h4 class="obj_head">Table 7. Segmenting the LGDs by using proposed LGD_Net with SegCaps, UNET, &amp; UNET++.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Models</th>
<th align="left" rowspan="1" colspan="1">Z<sup>1</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>2</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>3</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>4</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>5</sup>
</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">UNET</td>
<td align="left" rowspan="1" colspan="1">85.16%</td>
<td align="left" rowspan="1" colspan="1">85.43%</td>
<td align="left" rowspan="1" colspan="1">85.98%</td>
<td align="left" rowspan="1" colspan="1">85.32%</td>
<td align="left" rowspan="1" colspan="1">85.29%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">UNET++</td>
<td align="left" rowspan="1" colspan="1">87.20%</td>
<td align="left" rowspan="1" colspan="1">87.88%</td>
<td align="left" rowspan="1" colspan="1">87.29%</td>
<td align="left" rowspan="1" colspan="1">87.92%</td>
<td align="left" rowspan="1" colspan="1">87.11%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>LGD_Net with SegCaps</strong>
</td>
<td align="left" rowspan="1" colspan="1">96.89%</td>
<td align="left" rowspan="1" colspan="1">96.52%</td>
<td align="left" rowspan="1" colspan="1">96.55%</td>
<td align="left" rowspan="1" colspan="1">96.62%</td>
<td align="left" rowspan="1" colspan="1">96.69%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>From <a href="#pone.0327419.t007" class="usa-link">Table 7</a>, it has been determined that UNET has a Z<sup>5</sup> of 85.29%, a Z<sup>2</sup> of 85.43%, and an Z<sup>1</sup> of 85.16%. Likewise, UNET++ manages to attain a Z<sup>5</sup> of 87.11%, Z<sup>3</sup> of 87.29%, and Z<sup>1</sup> of 87.20%. The LGD_Net with SegCaps yields a Z<sup>5</sup> of 96.69%, which is superior by 11.40% and 9.58%, respectively, to both UNET and UNET++.</p></section><section id="sec026"><h3 class="pmc_sec_title">4.4 Confusion matrix (C_Mat) and GRAD-CAM of LGD_Net</h3>
<p>The C_Mat was developed to evaluate the performance of the LGD_Net model and the other four models used in this work for the classification of LGDs. The LGD_Net achieved the highest result by accurately classifying the 690 LGD of CT scans out of 692, including 143 cases as LC, 143 cases as COO, 131 cases PNEUM, 150 cases Nor, and 123 cases as TB. The LGD_Net model misclassifies the two cases as TB and LC as presented in <a href="#pone.0327419.g014" class="usa-link">Fig 14(f)</a>. The LGD_Net without ATPs and BL-SMT shows poor performance in classifying the LGDs. Additionally, 74 cases of LC, 18 cases of COO, 06 cases of NOR, and 11 cases of TB were correctly classified as illustrated in <a href="#pone.0327419.g014" class="usa-link">Fig 14(e)</a>. The D<sup>1</sup> correctly predicts the 153 cases as LC. While, 110, 145, 134, and 110 cases of COO, PNEUM, NOR, and TB, respectively, were correctly classified. Furthermore, the 14 cases of TB are misclassified as COO. The D<sup>2</sup> produced significant results as compared to D<sup>3</sup> and D<sup>4</sup>. The D<sup>2</sup> correctly classifies the 146 cases of LC, 123 cases of COO, and 122 cases of PNEUM. However, the D<sup>2</sup> misclassified the 3 cases of NOR as COO and 10 cases of TB as COO. The detailed C_Mat results are depicted in <a href="#pone.0327419.g014" class="usa-link">Fig 14</a>.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g014"><h4 class="obj_head">Fig 14. Performance evaluation by using C_Mat; (a) D<sup>1</sup>, (b) D<sup>2</sup>, (c) D<sup>3</sup>, (d) D<sup>4</sup>, (e) LGD_Net without ATPs &amp; BL-SMT, and (f) Proposed LGD_Net.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g014.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/4eeb28f90ae9/pone.0327419.g014.jpg" loading="lazy" height="388" width="707" alt="Fig 14"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g014/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>In addition, this study uses the Grad-CAM [<a href="#pone.0327419.ref086" class="usa-link" aria-describedby="pone.0327419.ref086">86</a>–<a href="#pone.0327419.ref090" class="usa-link" aria-describedby="pone.0327419.ref090">90</a>] heatmap technique to provide a graphical illustration of the results produced by LGD_Net. The purpose of the heatmap is to illustrate the particular region of interest that is being focused on by the LGD_Net. <a href="#pone.0327419.g015" class="usa-link">Fig 15</a> epresents the GRAD-CAM of the LGD_Net model for highlighting the infection that occurred due to the LGDs.</p>
<figure class="fig xbox font-sm" id="pone.0327419.g015"><h4 class="obj_head">Fig 15. GRAD-CAM of LGD_Net for highlighting the infected area of LGDs using CT scan images.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334061_pone.0327419.g015.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/67bf/12334061/697f6cb78f53/pone.0327419.g015.jpg" loading="lazy" height="466" width="660" alt="Fig 15"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327419.g015/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec027"><h3 class="pmc_sec_title">4.5 Ablation study</h3>
<p>For this work, the LGD_Net model was developed by combining the ATPs and BL-SMT. The control variable strategy, which was also utilized to concurrently adjust a variable to determine whether the LGD_Net relates to LGDs, was used to statistically determine the experimental outcomes. This study examines the LGD_Net model classified the several LGDs using ATPs &amp; BL-SMT and without it. To ascertain the significance of the new module to the model, we compared the models using the current techniques. The outcomes of LGD_Net are shown in Experiment 1, and the ATPs &amp; BL-SMT approach is shown alongside the LGD_Net in Experiment 2. The experiments’ detailed findings are shown in <a href="#pone.0327419.t008" class="usa-link">Table 8</a>.</p>
<section class="tw xbox font-sm" id="pone.0327419.t008"><h4 class="obj_head">Table 8. Outcomes produced by LGD_Net with and without ATPs and BL-SMT.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Experiments</th>
<th align="left" rowspan="1" colspan="1">LGD_Net</th>
<th align="left" rowspan="1" colspan="1">ATPs</th>
<th align="left" rowspan="1" colspan="1">BL-SMT</th>
<th align="left" rowspan="1" colspan="1">Image Size</th>
<th align="left" rowspan="1" colspan="1">Accuracy</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">224 × 224 × 3</td>
<td align="left" rowspan="1" colspan="1">90.64%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">224 × 224 × 3</td>
<td align="left" rowspan="1" colspan="1">99.71%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>Comparing the results of Experiment 1 and Experiment 2, it is clear that adding ATPs and BL-SMT enhanced the classification accuracy of LGD_Net by 9.07%. There are two reasons for producing the remarkable outcomes in Experiment 2, initially, ATPs produced effective synthetic image data by preserving geometric relationships while modifying feature sizes and locations. Additionally, the ATPs in LGD_Net facilitate the efficient learning of intricate patterns. Secondly, the use of SMOTE resolves the class imbalance problem of the LGDs by generating synthetic LGD CT scan images for the minority class. While BL generates the synthetic LGD CT scan samples that are near the decision boundary between minority and majority class [<a href="#pone.0327419.ref092" class="usa-link" aria-describedby="pone.0327419.ref092">92</a>–<a href="#pone.0327419.ref094" class="usa-link" aria-describedby="pone.0327419.ref094">94</a>]. Thus, the use of ATPs and BL-SMT supports the LGD_Net model to improve performance and lowering bias towards the majority class [<a href="#pone.0327419.ref045" class="usa-link" aria-describedby="pone.0327419.ref045">45</a>,<a href="#pone.0327419.ref095" class="usa-link" aria-describedby="pone.0327419.ref095">95</a>–<a href="#pone.0327419.ref098" class="usa-link" aria-describedby="pone.0327419.ref098">98</a>].</p></section><section id="sec028"><h3 class="pmc_sec_title">4.6 Analysis of computational cost</h3>
<p>To assess the applied viability of the LGD_Net model in clinical environments, we conducted a detailed computational performance evaluation. The computational analysis is performed between the proposed LGD_Net model and D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> in terms of training time, latency, complexity, and storage requirement. The detailed results of the computational cost analysis are presented in <a href="#pone.0327419.t009" class="usa-link">Table 9</a>.</p>
<section class="tw xbox font-sm" id="pone.0327419.t009"><h4 class="obj_head">Table 9. Summary of computational cost analysis. Training time represents the time taken to train the model in minutes (min), inference time represents the average time to classify the single image in milliseconds (ms), and model complexity is measured in terms of parameters (Millions = M) and model size (Megabytes = MB).</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Models</th>
<th align="left" rowspan="1" colspan="1">Epochs</th>
<th align="left" rowspan="1" colspan="1">Batch Size</th>
<th align="left" rowspan="1" colspan="1">Training Time</th>
<th align="left" rowspan="1" colspan="1">Inference Time per Image</th>
<th align="left" rowspan="1" colspan="1">Testing Time</th>
<th align="left" rowspan="1" colspan="1">Parameters</th>
<th align="left" rowspan="1" colspan="1">Model Size</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">D<sup>1</sup>
</td>
<td align="left" rowspan="1" colspan="1">30</td>
<td align="left" rowspan="1" colspan="1">32</td>
<td align="left" rowspan="1" colspan="1">55 min</td>
<td align="left" rowspan="1" colspan="1">41 ms</td>
<td align="left" rowspan="1" colspan="1">19 sec</td>
<td align="left" rowspan="1" colspan="1">78.2 M</td>
<td align="left" rowspan="1" colspan="1">35.10 MB</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">D<sup>2</sup>
</td>
<td align="left" rowspan="1" colspan="1">30</td>
<td align="left" rowspan="1" colspan="1">32</td>
<td align="left" rowspan="1" colspan="1">65 min</td>
<td align="left" rowspan="1" colspan="1">36 ms</td>
<td align="left" rowspan="1" colspan="1">23 sec</td>
<td align="left" rowspan="1" colspan="1">75.1 M</td>
<td align="left" rowspan="1" colspan="1">33.90 MB</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">D<sup>3</sup>
</td>
<td align="left" rowspan="1" colspan="1">30</td>
<td align="left" rowspan="1" colspan="1">32</td>
<td align="left" rowspan="1" colspan="1">62 min</td>
<td align="left" rowspan="1" colspan="1">39 ms</td>
<td align="left" rowspan="1" colspan="1">29 sec</td>
<td align="left" rowspan="1" colspan="1">95.0 M</td>
<td align="left" rowspan="1" colspan="1">42.50 MB</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">D<sup>4</sup>
</td>
<td align="left" rowspan="1" colspan="1">30</td>
<td align="left" rowspan="1" colspan="1">32</td>
<td align="left" rowspan="1" colspan="1">69 min</td>
<td align="left" rowspan="1" colspan="1">46 ms</td>
<td align="left" rowspan="1" colspan="1">32 sec</td>
<td align="left" rowspan="1" colspan="1">138.2 M</td>
<td align="left" rowspan="1" colspan="1">62.40 MB</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>Proposed LGD_Net Model</strong>
</td>
<td align="left" rowspan="1" colspan="1">30</td>
<td align="left" rowspan="1" colspan="1">32</td>
<td align="left" rowspan="1" colspan="1">25 min</td>
<td align="left" rowspan="1" colspan="1">12 ms</td>
<td align="left" rowspan="1" colspan="1">11 sec</td>
<td align="left" rowspan="1" colspan="1">1.5 M</td>
<td align="left" rowspan="1" colspan="1">7.90 MB</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t009/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>From <a href="#pone.0327419.t009" class="usa-link">Table 9</a>, the results show that the LGD_Net model trained significantly faster than D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> due to its lighter architecture and fewer parameters. Additionally, the LGD_Net model outperformed these four models in terms of latency, an essential criterion for real-time clinical diagnosis. Thus, the study concludes that LGD_Net offers a balanced trade-off between accuracy, speed, and resource usage, showing strong potential for real-world clinical settings.</p></section><section id="sec029"><h3 class="pmc_sec_title">4.7 Comparison of the LGD_Net with modern SOTA</h3>
<p>For this study, we compare the LGD_Net with recent modern studies [<a href="#pone.0327419.ref045" class="usa-link" aria-describedby="pone.0327419.ref045">45</a>, <a href="#pone.0327419.ref036" class="usa-link" aria-describedby="pone.0327419.ref036">36</a>, <a href="#pone.0327419.ref037" class="usa-link" aria-describedby="pone.0327419.ref037">37</a>, <a href="#pone.0327419.ref043" class="usa-link" aria-describedby="pone.0327419.ref043">43</a>, <a href="#pone.0327419.ref091" class="usa-link" aria-describedby="pone.0327419.ref091">91</a>–<a href="#pone.0327419.ref097" class="usa-link" aria-describedby="pone.0327419.ref097">97</a>, <a href="#pone.0327419.ref099" class="usa-link" aria-describedby="pone.0327419.ref099">99</a>]. Furthermore, <a href="#pone.0327419.t010" class="usa-link">Table 10</a> presents the comprehensive analysis of the LGD_Net with SOTA in terms of Z<sup>1</sup>, Z<sup>2</sup>, Z<sup>3</sup>, and Z<sup>4</sup>.</p>
<section class="tw xbox font-sm" id="pone.0327419.t010"><h4 class="obj_head">Table 10. Performance analysis of LGD_Net with SOTA in classifying LGDs.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Ref</th>
<th align="left" rowspan="1" colspan="1">Year</th>
<th align="left" rowspan="1" colspan="1">Models</th>
<th align="left" rowspan="1" colspan="1">No. of Classes</th>
<th align="left" rowspan="1" colspan="1">Z<sup>1</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>2</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>3</sup>
</th>
<th align="left" rowspan="1" colspan="1">Z<sup>4</sup>
</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref100" class="usa-link" aria-describedby="pone.0327419.ref100">100</a>]</td>
<td align="left" rowspan="1" colspan="1">2025</td>
<td align="left" rowspan="1" colspan="1">LungXpertAI</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">95.62%</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">×</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref101" class="usa-link" aria-describedby="pone.0327419.ref101">101</a>]</td>
<td align="left" rowspan="1" colspan="1">2025</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">83.04%</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">×</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref001" class="usa-link" aria-describedby="pone.0327419.ref001">1</a>]</td>
<td align="left" rowspan="1" colspan="1">2024</td>
<td align="left" rowspan="1" colspan="1">P4-CNN</td>
<td align="left" rowspan="1" colspan="1">03</td>
<td align="left" rowspan="1" colspan="1">92.37%</td>
<td align="left" rowspan="1" colspan="1">92.88%</td>
<td align="left" rowspan="1" colspan="1">92.67%</td>
<td align="left" rowspan="1" colspan="1">92.58%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref002" class="usa-link" aria-describedby="pone.0327419.ref002">2</a>]</td>
<td align="left" rowspan="1" colspan="1">2024</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">93.87%</td>
<td align="left" rowspan="1" colspan="1">93.62%</td>
<td align="left" rowspan="1" colspan="1">93.59%</td>
<td align="left" rowspan="1" colspan="1">93.58%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref091" class="usa-link" aria-describedby="pone.0327419.ref091">91</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">KNN</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">94.05%</td>
<td align="left" rowspan="1" colspan="1">94.02%</td>
<td align="left" rowspan="1" colspan="1">94.02%</td>
<td align="left" rowspan="1" colspan="1">93.99%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref036" class="usa-link" aria-describedby="pone.0327419.ref036">36</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">BCT &amp; FL + CNN</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">98.00%</td>
<td align="left" rowspan="1" colspan="1">97.96%</td>
<td align="left" rowspan="1" colspan="1">96.89%</td>
<td align="left" rowspan="1" colspan="1">97.92%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref037" class="usa-link" aria-describedby="pone.0327419.ref037">37</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">03</td>
<td align="left" rowspan="1" colspan="1">96.87%</td>
<td align="left" rowspan="1" colspan="1">96.38%</td>
<td align="left" rowspan="1" colspan="1">96.37%</td>
<td align="left" rowspan="1" colspan="1">96.29%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref043" class="usa-link" aria-describedby="pone.0327419.ref043">43</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">MobileNet + DarkNet</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">98.71%</td>
<td align="left" rowspan="1" colspan="1">98.68%</td>
<td align="left" rowspan="1" colspan="1">97.97%</td>
<td align="left" rowspan="1" colspan="1">97.98%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref064" class="usa-link" aria-describedby="pone.0327419.ref064">64</a>]</td>
<td align="left" rowspan="1" colspan="1">2023</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">97.61%</td>
<td align="left" rowspan="1" colspan="1">97.52%</td>
<td align="left" rowspan="1" colspan="1">97.96%</td>
<td align="left" rowspan="1" colspan="1">97.82%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref099" class="usa-link" aria-describedby="pone.0327419.ref099">99</a>]</td>
<td align="left" rowspan="1" colspan="1">2022</td>
<td align="left" rowspan="1" colspan="1">LWSNet</td>
<td align="left" rowspan="1" colspan="1">03</td>
<td align="left" rowspan="1" colspan="1">96.98%</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">×</td>
<td align="left" rowspan="1" colspan="1">96.92%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref092" class="usa-link" aria-describedby="pone.0327419.ref092">92</a>]</td>
<td align="left" rowspan="1" colspan="1">2022</td>
<td align="left" rowspan="1" colspan="1">VMD</td>
<td align="left" rowspan="1" colspan="1">03</td>
<td align="left" rowspan="1" colspan="1">97.38%</td>
<td align="left" rowspan="1" colspan="1">96.98%</td>
<td align="left" rowspan="1" colspan="1">96.97%</td>
<td align="left" rowspan="1" colspan="1">96.92%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref093" class="usa-link" aria-describedby="pone.0327419.ref093">93</a>]</td>
<td align="left" rowspan="1" colspan="1">2022</td>
<td align="left" rowspan="1" colspan="1">EfficienctNet B1</td>
<td align="left" rowspan="1" colspan="1">03</td>
<td align="left" rowspan="1" colspan="1">92.13%</td>
<td align="left" rowspan="1" colspan="1">92.39%</td>
<td align="left" rowspan="1" colspan="1">92.18%</td>
<td align="left" rowspan="1" colspan="1">92.19%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref094" class="usa-link" aria-describedby="pone.0327419.ref094">94</a>]</td>
<td align="left" rowspan="1" colspan="1">2022</td>
<td align="left" rowspan="1" colspan="1">Dual_Pachi</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">96.65%</td>
<td align="left" rowspan="1" colspan="1">93.13%</td>
<td align="left" rowspan="1" colspan="1">93.19%</td>
<td align="left" rowspan="1" colspan="1">93.21%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref095" class="usa-link" aria-describedby="pone.0327419.ref095">95</a>]</td>
<td align="left" rowspan="1" colspan="1">2021</td>
<td align="left" rowspan="1" colspan="1">MG-SD</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">92.35%</td>
<td align="left" rowspan="1" colspan="1">92.50%</td>
<td align="left" rowspan="1" colspan="1">92.20%</td>
<td align="left" rowspan="1" colspan="1">×</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref096" class="usa-link" aria-describedby="pone.0327419.ref096">96</a>]</td>
<td align="left" rowspan="1" colspan="1">2020</td>
<td align="left" rowspan="1" colspan="1">CNN</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">72.77%</td>
<td align="left" rowspan="1" colspan="1">73.83%</td>
<td align="left" rowspan="1" colspan="1">71.70%</td>
<td align="left" rowspan="1" colspan="1">×</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref045" class="usa-link" aria-describedby="pone.0327419.ref045">45</a>]</td>
<td align="left" rowspan="1" colspan="1">2020</td>
<td align="left" rowspan="1" colspan="1">Inception-v3</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">76.00%</td>
<td align="left" rowspan="1" colspan="1">87.00%</td>
<td align="left" rowspan="1" colspan="1">93.00%</td>
<td align="left" rowspan="1" colspan="1">×</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">[<a href="#pone.0327419.ref097" class="usa-link" aria-describedby="pone.0327419.ref097">97</a>]</td>
<td align="left" rowspan="1" colspan="1">2020</td>
<td align="left" rowspan="1" colspan="1">GoogleNet</td>
<td align="left" rowspan="1" colspan="1">02</td>
<td align="left" rowspan="1" colspan="1">80.56%</td>
<td align="left" rowspan="1" colspan="1">84.17%</td>
<td align="left" rowspan="1" colspan="1">80.56%</td>
<td align="left" rowspan="1" colspan="1">82.32%</td>
</tr>
<tr>
<td align="left" colspan="3" rowspan="1">
<strong>LGD_Net</strong>
</td>
<td align="left" rowspan="1" colspan="1">05</td>
<td align="left" rowspan="1" colspan="1">99.71%</td>
<td align="left" rowspan="1" colspan="1">99.71%</td>
<td align="left" rowspan="1" colspan="1">99.69%</td>
<td align="left" rowspan="1" colspan="1">99.70%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327419.t010/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec030"><h3 class="pmc_sec_title">4.8 Discussions</h3>
<p>LGDs are a cluster of lung disorders. Different medical imaging such as CT scans, CXR, etc., is used to diagnose and classify a wide variety of LGDs [<a href="#pone.0327419.ref001" class="usa-link" aria-describedby="pone.0327419.ref001">1</a>–<a href="#pone.0327419.ref003" class="usa-link" aria-describedby="pone.0327419.ref003">3</a>]. Most of the studies [<a href="#pone.0327419.ref016" class="usa-link" aria-describedby="pone.0327419.ref016">16</a>–<a href="#pone.0327419.ref017" class="usa-link" aria-describedby="pone.0327419.ref017">17</a>, <a href="#pone.0327419.ref023" class="usa-link" aria-describedby="pone.0327419.ref023">23</a>] conclude that CT scans are the most accurate [<a href="#pone.0327419.ref011" class="usa-link" aria-describedby="pone.0327419.ref011">11</a>] and efficient [<a href="#pone.0327419.ref019" class="usa-link" aria-describedby="pone.0327419.ref019">19</a>–<a href="#pone.0327419.ref021" class="usa-link" aria-describedby="pone.0327419.ref021">21</a>] method for detecting lung disorders [<a href="#pone.0327419.ref098" class="usa-link" aria-describedby="pone.0327419.ref098">98</a>]. Thus, for this study, the proposed LGD_Net has been developed with the combination of CapsNet and ELM. Additionally, the SegCaps were used with LGD_Net for the segmentation of the LGDs using CT scan images. In this study, five publicly benchmark LGD datasets were applied for training and testing the LGD_Net model. The D_Norm method such as SPN and SGN were applied to improve the LGDs CT scan image data integrity. The LGDs datasets contain an imbalanced number of images, so, BL-SMT was applied to handle this. After that, ATPs were applied to enhance the LGD dataset to prevent the LGD_Net and baseline models D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> from overfitting. Algorithm 1 outlined the pseudocode of BL-SMT.</p>
<p>The performance of LGD_Net with and without using ATPs and BL-SMT was compared with D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> in classifying LGD using CT scans. The LGD_Net with ATPs and BL-SMT attained the remarkable classification Z<sup>1</sup> of 99.71%. While other models, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> were achieves the Z<sup>1</sup> of 91.21%, 94.39%, 93.96%, and 93.82%. The comprehensive outcomes of the LGD_Net, D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> are presented in <a href="#pone.0327419.t006" class="usa-link">Table 6</a>. The results of <a href="#pone.0327419.t006" class="usa-link">Table 6</a> demonstrate that LGD_Net is more capable of identifying the LGDs and extracting dominant discriminative patterns from CT scans. Moreover, the outcomes of D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> are also discussed in <a href="#pone.0327419.t006" class="usa-link">Table 6</a>. The D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> perform less in classifying LGDs as compared to LGD_Net. The reason is that D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> have been restricted by their FCL. Because the neurons attached to the input are so big, the filter size in D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, and D<sup>4</sup> is insufficient, ignoring the key components of the LGD CT scan images. Furthermore, because these models contain a lot of layers, the vanishing gradient issue arose during training. Thus, all these problems are overcome by using LGD_Net which contains CapsNet capsules that capture spatial interactions and increase the robustness of LGD classification using CT scans. Furthermore, the single-layer architecture of ELMs with randomly assigned hidden nodes enables the LGD_Net to train quickly by determining the output weight analytically.</p>
<p>The results of the LGD_Net were also compared with SOTA as presented in <a href="#pone.0327419.t009" class="usa-link">Table 9</a>. Huyut [<a href="#pone.0327419.ref091" class="usa-link" aria-describedby="pone.0327419.ref091">91</a>] proposed a model based on KNN for the classification of COVID-19 and NOR cases. They achieved a classification Z<sup>1</sup> of 94.05%. Lasker et al., [<a href="#pone.0327419.ref099" class="usa-link" aria-describedby="pone.0327419.ref099">99</a>] designed a novel LWSNet model for the identification of LGDs such as COO, NOR, and PNEUM. Their LWSNet model achieved the Z<sup>1</sup> and Z<sup>4</sup> of 96.98% and 96.92%, respectively. The study [<a href="#pone.0327419.ref092" class="usa-link" aria-describedby="pone.0327419.ref092">92</a>] proposed a CNN-based model for lung diseases and achieved a remarkable Z 1 of 97.38%. Khan et al., [<a href="#pone.0327419.ref093" class="usa-link" aria-describedby="pone.0327419.ref093">93</a>] designed an EfficienctNet-B1 model for classifying chest diseases using CT scans. Their model scored a Z<sup>1</sup> of 92.13%. Zhang et al., [<a href="#pone.0327419.ref096" class="usa-link" aria-describedby="pone.0327419.ref096">96</a>] proposed CNN model COO classification and achieved 72.77% of Z<sup>1</sup>. From <a href="#pone.0327419.t009" class="usa-link">Table 9</a>, the results reveal that LGD_Net achieved the highest classification Z<sup>1</sup> of 99.71% in diagnosing the five LGDs such as COO, LC, TB, NOR, and PNEUM as compared to SOTA methods.</p></section></section><section id="sec031"><h2 class="pmc_sec_title">5. Conclusion and future work</h2>
<p>For this study, a novel LGD_Net was designed for the classification of LGDs using CT scan images. Currently, lung disorders associated with LGDs are rapidly expanding and harming communities all over the world. There have been a significant number of fatalities as a result of ineffective and time-consuming testing processes and a failure to diagnose lung disorders at an earlier stage. Thus, this study suggested an LGD_Net model to classify several LGDs such as COO, PNEUM, TB, LC, and NOR using CT scan images. Five publicly available benchmark LGD datasets were used for training the LGD_Net model. The datasets contain imbalanced classes of LGDs, therefore, BL-SMT was applied to handle this problem. Additionally, ATPs were also applied to enhance the datasets and prevent the LGD_Net from overfitting. The SegCaps were used to process the segmentation of the lung disorder’s CT scan images. The GRAD-CAM was produced with LGD_Net to highlight the infected region. The results of the ablation study also demonstrate the effectiveness of the LGD_Net in classifying LGDs. The highest classification accuracy of 99.71% was achieved by the LGD_Net. Hence, this study concludes that the LGD_Net model produced significant outcomes as compared to the baseline models such as D<sup>1</sup>, D<sup>2</sup>, D<sup>3</sup>, D<sup>4</sup>, and SOTA classifiers. Additionally, the LGD_Net can be of great assistance to radiologists. The limitation of the study is that the LGD_Net is not effective for other medical imaging modalities such as sonography, MRI, etc. However, in the future, we evaluate the LGD_Net by integrating the blockchain with federated learning for patients’ data privacy.</p></section><section id="sec032"><h2 class="pmc_sec_title">Supporting information</h2>
<section class="sm xbox font-sm" id="pone.0327419.s001"><div class="caption p">
<span>S1 File. Dataset details.</span><p>(DOCX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334061/bin/pone.0327419.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0327419.s001.docx</a><sup> (15.9KB, docx) </sup>
</div></div></section></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>The dataset used in this study is from a third-party source, and its direct link has been provided below. <a href="https://www.kaggle.com/datasets/mehradaria/covid19-lung-ct-scans" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/datasets/mehradaria/covid19-lung-ct-scans</a>.</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>This work was supported by the Deanship of Scientific Research, Vice Presidency for Graduate Studies and Scientific Research, King Faisal University, Saudi Arabia Grant No. KFU252304. The funder, Muhammad Nabeel Asghar, is a co-author of this manuscript and contributed equally to its development. He played an equal role in designing the methodology and writing the draft.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0327419.ref001">
<span class="label">1.</span><cite>Malik H, Anees T. Multi-modal deep learning methods for classification of chest diseases using different medical imaging and cough sounds. PLoS One. 2024;19(3):e0296352. doi: 10.1371/journal.pone.0296352

</cite> [<a href="https://doi.org/10.1371/journal.pone.0296352" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10931489/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38470893/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Multi-modal%20deep%20learning%20methods%20for%20classification%20of%20chest%20diseases%20using%20different%20medical%20imaging%20and%20cough%20sounds&amp;author=H%20Malik&amp;author=T%20Anees&amp;volume=19&amp;issue=3&amp;publication_year=2024&amp;pmid=38470893&amp;doi=10.1371/journal.pone.0296352&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref002">
<span class="label">2.</span><cite>Malik H, Anees T. Federated learning with deep convolutional neural networks for the detection of multiple chest diseases using chest x-rays. Multimed Tools Appl. 2024:1–29.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Multimed%20Tools%20Appl&amp;title=Federated%20learning%20with%20deep%20convolutional%20neural%20networks%20for%20the%20detection%20of%20multiple%20chest%20diseases%20using%20chest%20x-rays&amp;author=H%20Malik&amp;author=T%20Anees&amp;publication_year=2024&amp;pages=1-29&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref003">
<span class="label">3.</span><cite>Tahir A, Malik H, Chaudhry MU. Multi-classification deep learning models for detecting multiple chest infection using cough and breath sounds. In: Deep learning for multimedia processing applications. CRC Press; 2024. p. 216–49.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20for%20multimedia%20processing%20applications&amp;author=A%20Tahir&amp;author=H%20Malik&amp;author=MU%20Chaudhry&amp;publication_year=2024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref004">
<span class="label">4.</span><cite>MCA SGJ, Singh J. The evolutionary machine learning model to detect the COVID 19 infections due in medical images. TURCOMAT. 2019;10(3):955–61.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=TURCOMAT&amp;title=The%20evolutionary%20machine%20learning%20model%20to%20detect%20the%20COVID%2019%20infections%20due%20in%20medical%20images&amp;author=SGJ%20MCA&amp;author=J%20Singh&amp;volume=10&amp;issue=3&amp;publication_year=2019&amp;pages=955-61&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref005">
<span class="label">5.</span><cite>Saxena A, Singh SP, Gaidhane VH. A deep learning approach for the detection of COVID-19 from chest X-ray images using convolutional neural networks. Adv Mach Lear Art Inte. 2019;3(2):52–65.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Mach%20Lear%20Art%20Inte&amp;title=A%20deep%20learning%20approach%20for%20the%20detection%20of%20COVID-19%20from%20chest%20X-ray%20images%20using%20convolutional%20neural%20networks&amp;author=A%20Saxena&amp;author=SP%20Singh&amp;author=VH%20Gaidhane&amp;volume=3&amp;issue=2&amp;publication_year=2019&amp;pages=52-65&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref006">
<span class="label">6.</span><cite>He X, Yang X, Zhang S, Zhao J, Zhang Y, Xing E, et al. Sample-efficient deep learning for COVID-19 diagnosis based on CT scans. medrxiv, 2020-04. 2020.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=medrxiv&amp;title=Sample-efficient%20deep%20learning%20for%20COVID-19%20diagnosis%20based%20on%20CT%20scans&amp;author=X%20He&amp;author=X%20Yang&amp;author=S%20Zhang&amp;author=J%20Zhao&amp;author=Y%20Zhang&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref007">
<span class="label">7.</span><cite>Amyar A, Modzelewski R, Li H, Ruan S. Multi-task deep learning based CT imaging analysis for COVID-19 pneumonia: classification and segmentation. Comput Biol Med. 2020;126:104037. doi: 10.1016/j.compbiomed.2020.104037

</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2020.104037" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7543793/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33065387/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Biol%20Med&amp;title=Multi-task%20deep%20learning%20based%20CT%20imaging%20analysis%20for%20COVID-19%20pneumonia:%20classification%20and%20segmentation&amp;author=A%20Amyar&amp;author=R%20Modzelewski&amp;author=H%20Li&amp;author=S%20Ruan&amp;volume=126&amp;publication_year=2020&amp;pages=104037&amp;pmid=33065387&amp;doi=10.1016/j.compbiomed.2020.104037&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref008">
<span class="label">8.</span><cite>Sharma S. Drawing insights from COVID-19-infected patients using CT scan images and machine learning techniques: a study on 200 patients. Environ Sci Pollut Res Int. 2020;27(29):37155–63. doi: 10.1007/s11356-020-10133-3

</cite> [<a href="https://doi.org/10.1007/s11356-020-10133-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7375456/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32700269/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Environ%20Sci%20Pollut%20Res%20Int&amp;title=Drawing%20insights%20from%20COVID-19-infected%20patients%20using%20CT%20scan%20images%20and%20machine%20learning%20techniques:%20a%20study%20on%20200%20patients&amp;author=S%20Sharma&amp;volume=27&amp;issue=29&amp;publication_year=2020&amp;pages=37155-63&amp;pmid=32700269&amp;doi=10.1007/s11356-020-10133-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref009">
<span class="label">9.</span><cite>Panwar H, Gupta PK, Siddiqui MK, Morales-Menendez R, Bhardwaj P, Singh V. A deep learning and grad-CAM based color visualization approach for fast detection of COVID-19 cases using chest X-ray and CT-scan images. Chaos Soliton Fract. 2020;140:110190. doi: 10.1016/j.chaos.2020.110190

</cite> [<a href="https://doi.org/10.1016/j.chaos.2020.110190" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7413068/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32836918/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Chaos%20Soliton%20Fract&amp;title=A%20deep%20learning%20and%20grad-CAM%20based%20color%20visualization%20approach%20for%20fast%20detection%20of%20COVID-19%20cases%20using%20chest%20X-ray%20and%20CT-scan%20images&amp;author=H%20Panwar&amp;author=PK%20Gupta&amp;author=MK%20Siddiqui&amp;author=R%20Morales-Menendez&amp;author=P%20Bhardwaj&amp;volume=140&amp;publication_year=2020&amp;pages=110190&amp;pmid=32836918&amp;doi=10.1016/j.chaos.2020.110190&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref010">
<span class="label">10.</span><cite>Zhang H-T, Zhang J-S, Zhang H-H, Nan Y-D, Zhao Y, Fu E-Q, et al. Automated detection and quantification of COVID-19 pneumonia: CT imaging analysis by a deep learning-based software. Eur J Nucl Med Mol Imaging. 2020;47(11):2525–32. doi: 10.1007/s00259-020-04953-1

</cite> [<a href="https://doi.org/10.1007/s00259-020-04953-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7358997/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32666395/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eur%20J%20Nucl%20Med%20Mol%20Imaging&amp;title=Automated%20detection%20and%20quantification%20of%20COVID-19%20pneumonia:%20CT%20imaging%20analysis%20by%20a%20deep%20learning-based%20software&amp;author=H-T%20Zhang&amp;author=J-S%20Zhang&amp;author=H-H%20Zhang&amp;author=Y-D%20Nan&amp;author=Y%20Zhao&amp;volume=47&amp;issue=11&amp;publication_year=2020&amp;pages=2525-32&amp;pmid=32666395&amp;doi=10.1007/s00259-020-04953-1&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref011">
<span class="label">11.</span><cite>Afshar P, Heidarian S, Enshaei N, Naderkhani F, Rafiee MJ, Oikonomou A, et al. COVID-CT-MD, COVID-19 computed tomography scan dataset applicable in machine learning and deep learning. Sci Data. 2021;8(1):121. doi: 10.1038/s41597-021-00900-3

</cite> [<a href="https://doi.org/10.1038/s41597-021-00900-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8085195/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33927208/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Data&amp;title=COVID-CT-MD,%20COVID-19%20computed%20tomography%20scan%20dataset%20applicable%20in%20machine%20learning%20and%20deep%20learning&amp;author=P%20Afshar&amp;author=S%20Heidarian&amp;author=N%20Enshaei&amp;author=F%20Naderkhani&amp;author=MJ%20Rafiee&amp;volume=8&amp;issue=1&amp;publication_year=2021&amp;pages=121&amp;pmid=33927208&amp;doi=10.1038/s41597-021-00900-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref012">
<span class="label">12.</span><cite>Shah V, Keniya R, Shridharani A, Punjabi M, Shah J, Mehendale N. Diagnosis of COVID-19 using CT scan images and deep learning techniques. Emerg Radiol. 2021;28(3):497–505. doi: 10.1007/s10140-020-01886-y

</cite> [<a href="https://doi.org/10.1007/s10140-020-01886-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7848247/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33523309/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Emerg%20Radiol&amp;title=Diagnosis%20of%20COVID-19%20using%20CT%20scan%20images%20and%20deep%20learning%20techniques&amp;author=V%20Shah&amp;author=R%20Keniya&amp;author=A%20Shridharani&amp;author=M%20Punjabi&amp;author=J%20Shah&amp;volume=28&amp;issue=3&amp;publication_year=2021&amp;pages=497-505&amp;pmid=33523309&amp;doi=10.1007/s10140-020-01886-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref013">
<span class="label">13.</span><cite>Serte S, Demirel H. Deep learning for diagnosis of COVID-19 using 3D CT scans. Comput Biol Med. 2021;132:104306. doi: 10.1016/j.compbiomed.2021.104306

</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2021.104306" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7943389/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33780867/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Biol%20Med&amp;title=Deep%20learning%20for%20diagnosis%20of%20COVID-19%20using%203D%20CT%20scans&amp;author=S%20Serte&amp;author=H%20Demirel&amp;volume=132&amp;publication_year=2021&amp;pages=104306&amp;pmid=33780867&amp;doi=10.1016/j.compbiomed.2021.104306&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref014">
<span class="label">14.</span><cite>Lassau N, Ammari S, Chouzenoux E, Gortais H, Herent P, Devilder M, et al. Integrating deep learning CT-scan model, biological and clinical variables to predict severity of COVID-19 patients. Nat Commun. 2021;12(1):1–11.
</cite> [<a href="https://doi.org/10.1038/s41467-020-20657-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7840774/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33504775/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat%20Commun&amp;title=Integrating%20deep%20learning%20CT-scan%20model,%20biological%20and%20clinical%20variables%20to%20predict%20severity%20of%20COVID-19%20patients&amp;author=N%20Lassau&amp;author=S%20Ammari&amp;author=E%20Chouzenoux&amp;author=H%20Gortais&amp;author=P%20Herent&amp;volume=12&amp;issue=1&amp;publication_year=2021&amp;pages=1-11&amp;pmid=33504775&amp;doi=10.1038/s41467-020-20657-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref015">
<span class="label">15.</span><cite>Kumar R, Khan AA, Kumar J, Golilarz NA, Zhang S, Ting Y, et al. Blockchain-federated-learning and deep learning models for COVID-19 detection using CT imaging. IEEE Sens J. 2021;21(14):16301–14. doi: 10.1109/JSEN.2021.3076767

</cite> [<a href="https://doi.org/10.1109/JSEN.2021.3076767" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8791443/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35789224/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Sens%20J&amp;title=Blockchain-federated-learning%20and%20deep%20learning%20models%20for%20COVID-19%20detection%20using%20CT%20imaging&amp;author=R%20Kumar&amp;author=AA%20Khan&amp;author=J%20Kumar&amp;author=NA%20Golilarz&amp;author=S%20Zhang&amp;volume=21&amp;issue=14&amp;publication_year=2021&amp;pages=16301-14&amp;pmid=35789224&amp;doi=10.1109/JSEN.2021.3076767&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref016">
<span class="label">16.</span><cite>Rohila VS, Gupta N, Kaul A, Sharma DK. Deep learning assisted COVID-19 detection using full CT-scans. IoT. 2021;14:100377. doi: 10.1016/j.iot.2021.100377
</cite> [<a href="https://doi.org/10.1016/j.iot.2021.100377" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7903153/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38620521/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IoT&amp;title=Deep%20learning%20assisted%20COVID-19%20detection%20using%20full%20CT-scans&amp;author=VS%20Rohila&amp;author=N%20Gupta&amp;author=A%20Kaul&amp;author=DK%20Sharma&amp;volume=14&amp;publication_year=2021&amp;pages=100377&amp;pmid=38620521&amp;doi=10.1016/j.iot.2021.100377&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref017">
<span class="label">17.</span><cite>Malik H, Anees T. Deep learning-based classification of COVID-19 variants and lung cancer using CT scans. JCBI. 2023;6(01):238–69.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=JCBI&amp;title=Deep%20learning-based%20classification%20of%20COVID-19%20variants%20and%20lung%20cancer%20using%20CT%20scans&amp;author=H%20Malik&amp;author=T%20Anees&amp;volume=6&amp;issue=01&amp;publication_year=2023&amp;pages=238-69&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref018">
<span class="label">18.</span><cite>Khan MA, Hussain N, Majid A, Alhaisoni M, Chan Bukhari SA, Kadry S, et al. Classification of positive COVID-19 CT scans using deep learning. Comput Mater Contin. 2021;66(3).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Mater%20Contin&amp;title=Classification%20of%20positive%20COVID-19%20CT%20scans%20using%20deep%20learning&amp;author=MA%20Khan&amp;author=N%20Hussain&amp;author=A%20Majid&amp;author=M%20Alhaisoni&amp;author=SA%20Chan%20Bukhari&amp;volume=66&amp;issue=3&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref019">
<span class="label">19.</span><cite>Hussain E, Hasan M, Rahman MA, Lee I, Tamanna T, Parvez MZ. CoroDet: A deep learning based classification for COVID-19 detection using chest X-ray images. Chaos Soliton Fract. 2021;142:110495. doi: 10.1016/j.chaos.2020.110495

</cite> [<a href="https://doi.org/10.1016/j.chaos.2020.110495" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7682527/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33250589/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Chaos%20Soliton%20Fract&amp;title=CoroDet:%20A%20deep%20learning%20based%20classification%20for%20COVID-19%20detection%20using%20chest%20X-ray%20images&amp;author=E%20Hussain&amp;author=M%20Hasan&amp;author=MA%20Rahman&amp;author=I%20Lee&amp;author=T%20Tamanna&amp;volume=142&amp;publication_year=2021&amp;pages=110495&amp;pmid=33250589&amp;doi=10.1016/j.chaos.2020.110495&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref020">
<span class="label">20.</span><cite>Alshazly H, Linse C, Barth E, Martinetz T. Explainable COVID-19 detection using chest CT scans and deep learning. Sensors (Basel). 2021;21(2):455. doi: 10.3390/s21020455

</cite> [<a href="https://doi.org/10.3390/s21020455" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7828058/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33440674/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sensors%20(Basel)&amp;title=Explainable%20COVID-19%20detection%20using%20chest%20CT%20scans%20and%20deep%20learning&amp;author=H%20Alshazly&amp;author=C%20Linse&amp;author=E%20Barth&amp;author=T%20Martinetz&amp;volume=21&amp;issue=2&amp;publication_year=2021&amp;pages=455&amp;pmid=33440674&amp;doi=10.3390/s21020455&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref021">
<span class="label">21.</span><cite>Rahimzadeh M, Attar A, Sakhaei SM. A fully automated deep learning-based network for detecting COVID-19 from a new and large lung CT scan dataset. Biomed Signal Process Control. 2021;68:102588. doi: 10.1016/j.bspc.2021.102588

</cite> [<a href="https://doi.org/10.1016/j.bspc.2021.102588" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8011666/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33821166/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Biomed%20Signal%20Process%20Control&amp;title=A%20fully%20automated%20deep%20learning-based%20network%20for%20detecting%20COVID-19%20from%20a%20new%20and%20large%20lung%20CT%20scan%20dataset&amp;author=M%20Rahimzadeh&amp;author=A%20Attar&amp;author=SM%20Sakhaei&amp;volume=68&amp;publication_year=2021&amp;pages=102588&amp;pmid=33821166&amp;doi=10.1016/j.bspc.2021.102588&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref022">
<span class="label">22.</span><cite>Roberts M, Driggs D, Thorpe M, Gilbey J, Yeung M, Ursprung S, et al. Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans. Nat Mach Intell. 2021;3(3):199–217. doi: 10.1038/s42256-021-00307-0</cite> [<a href="https://doi.org/10.1038/s42256-021-00307-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat%20Mach%20Intell&amp;title=Common%20pitfalls%20and%20recommendations%20for%20using%20machine%20learning%20to%20detect%20and%20prognosticate%20for%20COVID-19%20using%20chest%20radiographs%20and%20CT%20scans&amp;author=M%20Roberts&amp;author=D%20Driggs&amp;author=M%20Thorpe&amp;author=J%20Gilbey&amp;author=M%20Yeung&amp;volume=3&amp;issue=3&amp;publication_year=2021&amp;pages=199-217&amp;doi=10.1038/s42256-021-00307-0&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref023">
<span class="label">23.</span><cite>Zhao W, Jiang W, Qiu X. Deep learning for COVID-19 detection based on CT images. Sci Rep. 2021;11(1):14353. doi: 10.1038/s41598-021-93832-2

</cite> [<a href="https://doi.org/10.1038/s41598-021-93832-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8275612/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34253822/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Rep&amp;title=Deep%20learning%20for%20COVID-19%20detection%20based%20on%20CT%20images&amp;author=W%20Zhao&amp;author=W%20Jiang&amp;author=X%20Qiu&amp;volume=11&amp;issue=1&amp;publication_year=2021&amp;pages=14353&amp;pmid=34253822&amp;doi=10.1038/s41598-021-93832-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref024">
<span class="label">24.</span><cite>Kwabena Patrick M, Adekoya FA, Mighty A, Edward BY. Capsule networks–a survey. J King Saud Univ Comput Inf Sci. 2019.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20King%20Saud%20Univ%20Comput%20Inf%20Sci&amp;title=Capsule%20networks%E2%80%93a%20survey&amp;author=M%20Kwabena%20Patrick&amp;author=FA%20Adekoya&amp;author=A%20Mighty&amp;author=BY%20Edward&amp;publication_year=2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref025">
<span class="label">25.</span><cite>Ellahyani A, Jaafari IE, Charfi S, Ansari ME. Detection of abnormalities in wireless capsule endoscopy based on extreme learning machine. Signal Image Video P. 2021;15(5):877–84.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Signal%20Image%20Video%20P&amp;title=Detection%20of%20abnormalities%20in%20wireless%20capsule%20endoscopy%20based%20on%20extreme%20learning%20machine&amp;author=A%20Ellahyani&amp;author=IE%20Jaafari&amp;author=S%20Charfi&amp;author=ME%20Ansari&amp;volume=15&amp;issue=5&amp;publication_year=2021&amp;pages=877-84&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref026">
<span class="label">26.</span><cite>Mukherjee H, Ghosh S, Dhar A, Obaidullah SM, Santosh KC, Roy K. Deep neural network to detect COVID-19: one architecture for both CT scans and chest X-rays. Appl Intell (Dordr). 2021;51(5):2777–89. doi: 10.1007/s10489-020-01943-6

</cite> [<a href="https://doi.org/10.1007/s10489-020-01943-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7646727/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34764562/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Appl%20Intell%20(Dordr)&amp;title=Deep%20neural%20network%20to%20detect%20COVID-19:%20one%20architecture%20for%20both%20CT%20scans%20and%20chest%20X-rays&amp;author=H%20Mukherjee&amp;author=S%20Ghosh&amp;author=A%20Dhar&amp;author=SM%20Obaidullah&amp;author=KC%20Santosh&amp;volume=51&amp;issue=5&amp;publication_year=2021&amp;pages=2777-89&amp;pmid=34764562&amp;doi=10.1007/s10489-020-01943-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref027">
<span class="label">27.</span><cite>Voulodimos A, Protopapadakis E, Katsamenis I, Doulamis A, Doulamis N. Deep learning models for COVID-19 infected area segmentation in CT images. Proceedings of the 14th PErvasive Technologies Related to Assistive Environments Conference; 2021. p. 404–11.</cite> [<a href="https://scholar.google.com/scholar_lookup?Voulodimos%20A,%20Protopapadakis%20E,%20Katsamenis%20I,%20Doulamis%20A,%20Doulamis%20N.%20Deep%20learning%20models%20for%20COVID-19%20infected%20area%20segmentation%20in%20CT%20images.%20Proceedings%20of%20the%2014th%20PErvasive%20Technologies%20Related%20to%20Assistive%20Environments%20Conference;%202021.%20p.%20404%E2%80%9311." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref028">
<span class="label">28.</span><cite>Kumari S, Ranjith E, Gujjar A, Narasimman S, Aadil Sha Zeelani HS. Comparative analysis of deep learning models for COVID-19 detection. Glob Transit Proc. 2021;2(2):559–65. doi: 10.1016/j.gltp.2021.08.030</cite> [<a href="https://doi.org/10.1016/j.gltp.2021.08.030" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Glob%20Transit%20Proc&amp;title=Comparative%20analysis%20of%20deep%20learning%20models%20for%20COVID-19%20detection&amp;author=S%20Kumari&amp;author=E%20Ranjith&amp;author=A%20Gujjar&amp;author=S%20Narasimman&amp;author=HS%20Aadil%20Sha%20Zeelani&amp;volume=2&amp;issue=2&amp;publication_year=2021&amp;pages=559-65&amp;doi=10.1016/j.gltp.2021.08.030&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref029">
<span class="label">29.</span><cite>Purohit K, Kesarwani A, Ranjan Kisku D, Dalui M. Covid-19 detection on chest x-ray and ct scan images using multi-image augmented deep learning model. Proceedings of the Seventh International Conference on Mathematics and Computing: ICMC 2021. Singapore: Springer Singapore; 2022. p. 395–413.</cite> [<a href="https://scholar.google.com/scholar_lookup?Purohit%20K,%20Kesarwani%20A,%20Ranjan%20Kisku%20D,%20Dalui%20M.%20Covid-19%20detection%20on%20chest%20x-ray%20and%20ct%20scan%20images%20using%20multi-image%20augmented%20deep%20learning%20model.%20Proceedings%20of%20the%20Seventh%20International%20Conference%20on%20Mathematics%20and%20Computing:%20ICMC%202021.%20Singapore:%20Springer%20Singapore;%202022.%20p.%20395%E2%80%93413." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref030">
<span class="label">30.</span><cite>Singh VK, Kolekar MH. Deep learning empowered COVID-19 diagnosis using chest CT scan images for collaborative edge-cloud computing platform. Multimed Tools Appl. 2022;81(1):3–30. doi: 10.1007/s11042-021-11158-7

</cite> [<a href="https://doi.org/10.1007/s11042-021-11158-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8236565/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34220289/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Multimed%20Tools%20Appl&amp;title=Deep%20learning%20empowered%20COVID-19%20diagnosis%20using%20chest%20CT%20scan%20images%20for%20collaborative%20edge-cloud%20computing%20platform&amp;author=VK%20Singh&amp;author=MH%20Kolekar&amp;volume=81&amp;issue=1&amp;publication_year=2022&amp;pages=3-30&amp;pmid=34220289&amp;doi=10.1007/s11042-021-11158-7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref031">
<span class="label">31.</span><cite>Wang S, Zha Y, Li W, Wu Q, Li X, Niu M, et al. A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis. Eur Respir J. 2020;56(2):2000775. doi: 10.1183/13993003.00775-2020

</cite> [<a href="https://doi.org/10.1183/13993003.00775-2020" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7243395/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32444412/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eur%20Respir%20J&amp;title=A%20fully%20automatic%20deep%20learning%20system%20for%20COVID-19%20diagnostic%20and%20prognostic%20analysis&amp;author=S%20Wang&amp;author=Y%20Zha&amp;author=W%20Li&amp;author=Q%20Wu&amp;author=X%20Li&amp;volume=56&amp;issue=2&amp;publication_year=2020&amp;pages=2000775&amp;pmid=32444412&amp;doi=10.1183/13993003.00775-2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref032">
<span class="label">32.</span><cite>Han H, Wang WY, Mao BH. Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning. International conference on intelligent computing. Berlin, Heidelberg: Springer Berlin Heidelberg; 2005. p. 878–87.</cite> [<a href="https://scholar.google.com/scholar_lookup?Han%20H,%20Wang%20WY,%20Mao%20BH.%20Borderline-SMOTE:%20a%20new%20over-sampling%20method%20in%20imbalanced%20data%20sets%20learning.%20International%20conference%20on%20intelligent%20computing.%20Berlin,%20Heidelberg:%20Springer%20Berlin%20Heidelberg;%202005.%20p.%20878%E2%80%9387." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref033">
<span class="label">33.</span><cite>Antunes C, Rodrigues J, Cunha A. CTCovid19: automatic Covid-19 model for computed tomography scans using deep learning. Intell Based Med. 2025;11:100190. doi: 10.1016/j.ibmed.2024.100190</cite> [<a href="https://doi.org/10.1016/j.ibmed.2024.100190" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Intell%20Based%20Med&amp;title=CTCovid19:%20automatic%20Covid-19%20model%20for%20computed%20tomography%20scans%20using%20deep%20learning&amp;author=C%20Antunes&amp;author=J%20Rodrigues&amp;author=A%20Cunha&amp;volume=11&amp;publication_year=2025&amp;pages=100190&amp;doi=10.1016/j.ibmed.2024.100190&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref034">
<span class="label">34.</span><cite>Afif M, Ayachi R, Said Y, Atri M. Deep learning-based technique for lesions segmentation in CT scan images for COVID-19 prediction. Multimed Tools Appl. 2023:1–15. doi: 10.1007/s11042-023-14941-w

</cite> [<a href="https://doi.org/10.1007/s11042-023-14941-w" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9986667/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37362746/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Multimed%20Tools%20Appl&amp;title=Deep%20learning-based%20technique%20for%20lesions%20segmentation%20in%20CT%20scan%20images%20for%20COVID-19%20prediction&amp;author=M%20Afif&amp;author=R%20Ayachi&amp;author=Y%20Said&amp;author=M%20Atri&amp;publication_year=2023&amp;pages=1-15&amp;pmid=37362746&amp;doi=10.1007/s11042-023-14941-w&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref035">
<span class="label">35.</span><cite>Malik H, Anees T, Din M, Naeem A. CDC_Net: multi-classification convolutional neural network model for detection of COVID-19, pneumothorax, pneumonia, lung Cancer, and tuberculosis using chest X-rays. Multimed Tools Appl. 2023;82(9):13855–80. doi: 10.1007/s11042-022-13843-7

</cite> [<a href="https://doi.org/10.1007/s11042-022-13843-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9485026/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36157356/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Multimed%20Tools%20Appl&amp;title=CDC_Net:%20multi-classification%20convolutional%20neural%20network%20model%20for%20detection%20of%20COVID-19,%20pneumothorax,%20pneumonia,%20lung%20Cancer,%20and%20tuberculosis%20using%20chest%20X-rays&amp;author=H%20Malik&amp;author=T%20Anees&amp;author=M%20Din&amp;author=A%20Naeem&amp;volume=82&amp;issue=9&amp;publication_year=2023&amp;pages=13855-80&amp;pmid=36157356&amp;doi=10.1007/s11042-022-13843-7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref036">
<span class="label">36.</span><cite>Malik H, Anees T, Naeem A, Naqvi RA, Loh W-K. Blockchain-federated and deep-learning-based ensembling of capsule network with incremental extreme learning machines for classification of COVID-19 using CT scans. Bioengineering (Basel). 2023;10(2):203. doi: 10.3390/bioengineering10020203

</cite> [<a href="https://doi.org/10.3390/bioengineering10020203" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9952069/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36829697/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Bioengineering%20(Basel)&amp;title=Blockchain-federated%20and%20deep-learning-based%20ensembling%20of%20capsule%20network%20with%20incremental%20extreme%20learning%20machines%20for%20classification%20of%20COVID-19%20using%20CT%20scans&amp;author=H%20Malik&amp;author=T%20Anees&amp;author=A%20Naeem&amp;author=RA%20Naqvi&amp;author=W-K%20Loh&amp;volume=10&amp;issue=2&amp;publication_year=2023&amp;pages=203&amp;pmid=36829697&amp;doi=10.3390/bioengineering10020203&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref037">
<span class="label">37.</span><cite>Aslani S, Jacob J. Utilisation of deep learning for COVID-19 diagnosis. Clin Radiol. 2023;78(2):150–7. doi: 10.1016/j.crad.2022.11.006

</cite> [<a href="https://doi.org/10.1016/j.crad.2022.11.006" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9831845/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36639173/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Clin%20Radiol&amp;title=Utilisation%20of%20deep%20learning%20for%20COVID-19%20diagnosis&amp;author=S%20Aslani&amp;author=J%20Jacob&amp;volume=78&amp;issue=2&amp;publication_year=2023&amp;pages=150-7&amp;pmid=36639173&amp;doi=10.1016/j.crad.2022.11.006&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref038">
<span class="label">38.</span><cite>Al-Waisy AS, Al-Fahdawi S, Mohammed MA, Abdulkareem KH, Mostafa SA, Maashi MS, et al. COVID-CheXNet: hybrid deep learning framework for identifying COVID-19 virus in chest X-rays images. Soft Comput. 2023;27(5):2657–72. doi: 10.1007/s00500-020-05424-3

</cite> [<a href="https://doi.org/10.1007/s00500-020-05424-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7679792/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33250662/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Soft%20Comput&amp;title=COVID-CheXNet:%20hybrid%20deep%20learning%20framework%20for%20identifying%20COVID-19%20virus%20in%20chest%20X-rays%20images&amp;author=AS%20Al-Waisy&amp;author=S%20Al-Fahdawi&amp;author=MA%20Mohammed&amp;author=KH%20Abdulkareem&amp;author=SA%20Mostafa&amp;volume=27&amp;issue=5&amp;publication_year=2023&amp;pages=2657-72&amp;pmid=33250662&amp;doi=10.1007/s00500-020-05424-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="/articles/PMC11753127/" class="text-red">Retracted</a>]</li>
<li id="pone.0327419.ref039">
<span class="label">39.</span><cite>Bassiouni MM, Chakrabortty RK, Hussain OK, Rahman HF. Advanced deep learning approaches to predict supply chain risks under COVID-19 restrictions. Expert Syst Appl. 2023;211:118604. doi: 10.1016/j.eswa.2022.118604

</cite> [<a href="https://doi.org/10.1016/j.eswa.2022.118604" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9389854/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35999828/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Expert%20Syst%20Appl&amp;title=Advanced%20deep%20learning%20approaches%20to%20predict%20supply%20chain%20risks%20under%20COVID-19%20restrictions&amp;author=MM%20Bassiouni&amp;author=RK%20Chakrabortty&amp;author=OK%20Hussain&amp;author=HF%20Rahman&amp;volume=211&amp;publication_year=2023&amp;pages=118604&amp;pmid=35999828&amp;doi=10.1016/j.eswa.2022.118604&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref040">
<span class="label">40.</span><cite>Vinod DN, Prabaharan SRS. COVID-19-the role of artificial intelligence, machine learning, and deep learning: a newfangled. Arch Comput Methods Eng. 2023;30(4):2667–82. doi: 10.1007/s11831-023-09882-4

</cite> [<a href="https://doi.org/10.1007/s11831-023-09882-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9843670/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36685135/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Arch%20Comput%20Methods%20Eng&amp;title=COVID-19-the%20role%20of%20artificial%20intelligence,%20machine%20learning,%20and%20deep%20learning:%20a%20newfangled&amp;author=DN%20Vinod&amp;author=SRS%20Prabaharan&amp;volume=30&amp;issue=4&amp;publication_year=2023&amp;pages=2667-82&amp;pmid=36685135&amp;doi=10.1007/s11831-023-09882-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref041">
<span class="label">41.</span><cite>Mercaldo F, Belfiore MP, Reginelli A, Brunese L, Santone A. Coronavirus covid-19 detection by means of explainable deep learning. Sci Rep. 2023;13(1):462. doi: 10.1038/s41598-023-27697-y

</cite> [<a href="https://doi.org/10.1038/s41598-023-27697-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9830129/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36627339/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Rep&amp;title=Coronavirus%20covid-19%20detection%20by%20means%20of%20explainable%20deep%20learning&amp;author=F%20Mercaldo&amp;author=MP%20Belfiore&amp;author=A%20Reginelli&amp;author=L%20Brunese&amp;author=A%20Santone&amp;volume=13&amp;issue=1&amp;publication_year=2023&amp;pages=462&amp;pmid=36627339&amp;doi=10.1038/s41598-023-27697-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref042">
<span class="label">42.</span><cite>Wathore R, Rawlekar S, Anjum S, Gupta A, Bherwani H, Labhasetwar N, et al. Improving performance of deep learning predictive models for COVID-19 by incorporating environmental parameters. Gondwana Res. 2023;114:69–77.
</cite> [<a href="https://doi.org/10.1016/j.gr.2022.03.014" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8990533/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35431596/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Gondwana%20Res&amp;title=Improving%20performance%20of%20deep%20learning%20predictive%20models%20for%20COVID-19%20by%20incorporating%20environmental%20parameters&amp;author=R%20Wathore&amp;author=S%20Rawlekar&amp;author=S%20Anjum&amp;author=A%20Gupta&amp;author=H%20Bherwani&amp;volume=114&amp;publication_year=2023&amp;pages=69-77&amp;pmid=35431596&amp;doi=10.1016/j.gr.2022.03.014&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref043">
<span class="label">43.</span><cite>Gupta K, Bajaj V. Deep learning models-based CT-scan image classification for automated screening of COVID-19. Biomed Signal Process Control. 2023;80:104268. doi: 10.1016/j.bspc.2022.104268

</cite> [<a href="https://doi.org/10.1016/j.bspc.2022.104268" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9556167/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36267466/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Biomed%20Signal%20Process%20Control&amp;title=Deep%20learning%20models-based%20CT-scan%20image%20classification%20for%20automated%20screening%20of%20COVID-19&amp;author=K%20Gupta&amp;author=V%20Bajaj&amp;volume=80&amp;publication_year=2023&amp;pages=104268&amp;pmid=36267466&amp;doi=10.1016/j.bspc.2022.104268&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref044">
<span class="label">44.</span><cite>Zhao Z, Wu J, Cai F, Zhang S, Wang Y-G. A hybrid deep learning framework for air quality prediction with spatial autocorrelation during the COVID-19 pandemic. Sci Rep. 2023;13(1):1015. doi: 10.1038/s41598-023-28287-8

</cite> [<a href="https://doi.org/10.1038/s41598-023-28287-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9848720/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36653488/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Rep&amp;title=A%20hybrid%20deep%20learning%20framework%20for%20air%20quality%20prediction%20with%20spatial%20autocorrelation%20during%20the%20COVID-19%20pandemic&amp;author=Z%20Zhao&amp;author=J%20Wu&amp;author=F%20Cai&amp;author=S%20Zhang&amp;author=Y-G%20Wang&amp;volume=13&amp;issue=1&amp;publication_year=2023&amp;pages=1015&amp;pmid=36653488&amp;doi=10.1038/s41598-023-28287-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref045">
<span class="label">45.</span><cite>Apostolopoulos ID, Mpesiana TA. Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks. Phys Eng Sci Med. 2020;43(2):635–40. doi: 10.1007/s13246-020-00865-4

</cite> [<a href="https://doi.org/10.1007/s13246-020-00865-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7118364/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32524445/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Phys%20Eng%20Sci%20Med&amp;title=Covid-19:%20automatic%20detection%20from%20X-ray%20images%20utilizing%20transfer%20learning%20with%20convolutional%20neural%20networks&amp;author=ID%20Apostolopoulos&amp;author=TA%20Mpesiana&amp;volume=43&amp;issue=2&amp;publication_year=2020&amp;pages=635-40&amp;pmid=32524445&amp;doi=10.1007/s13246-020-00865-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref046">
<span class="label">46.</span><cite>Agrawal S, Honnakasturi V, Nara M, Patil N. Utilizing deep learning models and transfer learning for COVID-19 detection from X-ray images. SN Comput Sci. 2023;4(4):326. doi: 10.1007/s42979-022-01655-3

</cite> [<a href="https://doi.org/10.1007/s42979-022-01655-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10105354/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37089895/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=SN%20Comput%20Sci&amp;title=Utilizing%20deep%20learning%20models%20and%20transfer%20learning%20for%20COVID-19%20detection%20from%20X-ray%20images&amp;author=S%20Agrawal&amp;author=V%20Honnakasturi&amp;author=M%20Nara&amp;author=N%20Patil&amp;volume=4&amp;issue=4&amp;publication_year=2023&amp;pages=326&amp;pmid=37089895&amp;doi=10.1007/s42979-022-01655-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref047">
<span class="label">47.</span><cite>Choudhary T, Gujar S, Goswami A, Mishra V, Badal T. Deep learning-based important weights-only transfer learning approach for COVID-19 CT-scan classification. Appl Intell (Dordr). 2023;53(6):7201–15. doi: 10.1007/s10489-022-03893-7

</cite> [<a href="https://doi.org/10.1007/s10489-022-03893-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9289654/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35875199/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Appl%20Intell%20(Dordr)&amp;title=Deep%20learning-based%20important%20weights-only%20transfer%20learning%20approach%20for%20COVID-19%20CT-scan%20classification&amp;author=T%20Choudhary&amp;author=S%20Gujar&amp;author=A%20Goswami&amp;author=V%20Mishra&amp;author=T%20Badal&amp;volume=53&amp;issue=6&amp;publication_year=2023&amp;pages=7201-15&amp;pmid=35875199&amp;doi=10.1007/s10489-022-03893-7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref048">
<span class="label">48.</span><cite>Goyal L, Dhull A, Singh A, Kukreja S, Singh KK. VGG-COVIDNet: a novel model for COVID detection from X-Ray and CT Scan images. Procedia Comput Sci. 2023;218:1926–35. doi: 10.1016/j.procs.2023.01.169

</cite> [<a href="https://doi.org/10.1016/j.procs.2023.01.169" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9886333/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36743790/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Procedia%20Comput%20Sci&amp;title=VGG-COVIDNet:%20a%20novel%20model%20for%20COVID%20detection%20from%20X-Ray%20and%20CT%20Scan%20images&amp;author=L%20Goyal&amp;author=A%20Dhull&amp;author=A%20Singh&amp;author=S%20Kukreja&amp;author=KK%20Singh&amp;volume=218&amp;publication_year=2023&amp;pages=1926-35&amp;pmid=36743790&amp;doi=10.1016/j.procs.2023.01.169&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref049">
<span class="label">49.</span><cite>Malik H, Anees T, Mui-Zzud-Din. BDCNet: multi-classification convolutional neural network model for classification of COVID-19, pneumonia, and lung cancer from chest radiographs. Multimed Syst. 2022;28(3):815–29. doi: 10.1007/s00530-021-00878-3

</cite> [<a href="https://doi.org/10.1007/s00530-021-00878-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8763428/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35068705/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Multimed%20Syst&amp;title=BDCNet:%20multi-classification%20convolutional%20neural%20network%20model%20for%20classification%20of%20COVID-19,%20pneumonia,%20and%20lung%20cancer%20from%20chest%20radiographs&amp;author=H%20Malik&amp;author=T%20Anees&amp;author=%20Mui-Zzud-Din&amp;volume=28&amp;issue=3&amp;publication_year=2022&amp;pages=815-29&amp;pmid=35068705&amp;doi=10.1007/s00530-021-00878-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref050">
<span class="label">50.</span><cite>Malik H, Naeem A, Naqvi RA, Loh W-K. DMFL_Net: a federated learning-based framework for the classification of COVID-19 from multiple chest diseases using X-rays. Sensors (Basel). 2023;23(2):743. doi: 10.3390/s23020743

</cite> [<a href="https://doi.org/10.3390/s23020743" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9864925/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36679541/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sensors%20(Basel)&amp;title=DMFL_Net:%20a%20federated%20learning-based%20framework%20for%20the%20classification%20of%20COVID-19%20from%20multiple%20chest%20diseases%20using%20X-rays&amp;author=H%20Malik&amp;author=A%20Naeem&amp;author=RA%20Naqvi&amp;author=W-K%20Loh&amp;volume=23&amp;issue=2&amp;publication_year=2023&amp;pages=743&amp;pmid=36679541&amp;doi=10.3390/s23020743&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref051">
<span class="label">51.</span><cite>Sufian MM, Moung EG, Hijazi MHA, Yahya F, Dargham JA, Farzamnia A, et al. COVID-19 classification through deep learning models with three-channel grayscale CT images. BDCC. 2023;7(1):36. doi: 10.3390/bdcc7010036</cite> [<a href="https://doi.org/10.3390/bdcc7010036" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=BDCC&amp;title=COVID-19%20classification%20through%20deep%20learning%20models%20with%20three-channel%20grayscale%20CT%20images&amp;author=MM%20Sufian&amp;author=EG%20Moung&amp;author=MHA%20Hijazi&amp;author=F%20Yahya&amp;author=JA%20Dargham&amp;volume=7&amp;issue=1&amp;publication_year=2023&amp;pages=36&amp;doi=10.3390/bdcc7010036&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref052">
<span class="label">52.</span><cite>Arora A, Chakraborty P, Bhatia MPS. Problematic use of digital technologies and its impact on mental health during COVID-19 pandemic: assessment using machine learning. In: Emerging technologies during the era of COVID-19 pandemic; 2021. p. 197–221.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Emerging%20technologies%20during%20the%20era%20of%20COVID-19%20pandemic&amp;author=A%20Arora&amp;author=P%20Chakraborty&amp;author=MPS%20Bhatia&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref053">
<span class="label">53.</span><cite>Kwekha-Rashid AS, Abduljabbar HN, Alhayani B. Coronavirus disease (COVID-19) cases analysis using machine-learning applications. Appl Nanosci. 2023;13(3):2013–25. doi: 10.1007/s13204-021-01868-7

</cite> [<a href="https://doi.org/10.1007/s13204-021-01868-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8138510/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34036034/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Appl%20Nanosci&amp;title=Coronavirus%20disease%20(COVID-19)%20cases%20analysis%20using%20machine-learning%20applications&amp;author=AS%20Kwekha-Rashid&amp;author=HN%20Abduljabbar&amp;author=B%20Alhayani&amp;volume=13&amp;issue=3&amp;publication_year=2023&amp;pages=2013-25&amp;pmid=34036034&amp;doi=10.1007/s13204-021-01868-7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref054">
<span class="label">54.</span><cite>Jangam E, Barreto AAD, Annavarapu CSR. Automatic detection of COVID-19 from chest CT scan and chest X-Rays images using deep learning, transfer learning and stacking. Appl Intell. 2022:1–17.</cite> [<a href="https://doi.org/10.1007/s10489-021-02393-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8180385/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34764605/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Appl%20Intell&amp;title=Automatic%20detection%20of%20COVID-19%20from%20chest%20CT%20scan%20and%20chest%20X-Rays%20images%20using%20deep%20learning,%20transfer%20learning%20and%20stacking&amp;author=E%20Jangam&amp;author=AAD%20Barreto&amp;author=CSR%20Annavarapu&amp;publication_year=2022&amp;pages=1-17&amp;pmid=34764605&amp;doi=10.1007/s10489-021-02393-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref055">
<span class="label">55.</span><cite>Oluwasanmi A, Aftab MU, Qin Z, Ngo ST, Doan TV, Nguyen SB, et al. Transfer learning and semisupervised adversarial detection and classification of COVID‐19 in CT images. Complexity. 2021;2021(1):6680455.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Complexity&amp;title=Transfer%20learning%20and%20semisupervised%20adversarial%20detection%20and%20classification%20of%20COVID%E2%80%9019%20in%20CT%20images&amp;author=A%20Oluwasanmi&amp;author=MU%20Aftab&amp;author=Z%20Qin&amp;author=ST%20Ngo&amp;author=TV%20Doan&amp;volume=2021&amp;issue=1&amp;publication_year=2021&amp;pages=6680455&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref056">
<span class="label">56.</span><cite>Malik H, Anees T, Chaudhry MU, Gono R, Jasiński M, Leonowicz Z, et al. A novel fusion model of hand-crafted features with deep convolutional neural networks for classification of several chest diseases using X-ray images. IEEE Access. 2023;11:39243–68. doi: 10.1109/access.2023.3267492</cite> [<a href="https://doi.org/10.1109/access.2023.3267492" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=A%20novel%20fusion%20model%20of%20hand-crafted%20features%20with%20deep%20convolutional%20neural%20networks%20for%20classification%20of%20several%20chest%20diseases%20using%20X-ray%20images&amp;author=H%20Malik&amp;author=T%20Anees&amp;author=MU%20Chaudhry&amp;author=R%20Gono&amp;author=M%20Jasi%C5%84ski&amp;volume=11&amp;publication_year=2023&amp;pages=39243-68&amp;doi=10.1109/access.2023.3267492&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref057">
<span class="label">57.</span><cite>Kermany DS, Goldbaum M, Cai W, Valentim CCS, Liang H, Baxter SL, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell. 2018;172(5):1122-1131.e9. doi: 10.1016/j.cell.2018.02.010

</cite> [<a href="https://doi.org/10.1016/j.cell.2018.02.010" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29474911/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cell&amp;title=Identifying%20medical%20diagnoses%20and%20treatable%20diseases%20by%20image-based%20deep%20learning&amp;author=DS%20Kermany&amp;author=M%20Goldbaum&amp;author=W%20Cai&amp;author=CCS%20Valentim&amp;author=H%20Liang&amp;volume=172&amp;issue=5&amp;publication_year=2018&amp;pmid=29474911&amp;doi=10.1016/j.cell.2018.02.010&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref058">
<span class="label">58.</span><cite>Narin A, Kaya C, Pamuk Z. Automatic detection of coronavirus disease (COVID-19) using X-ray images and deep convolutional neural networks. Pattern Anal Appl. 2021;24(3):1207–20. doi: 10.1007/s10044-021-00984-y

</cite> [<a href="https://doi.org/10.1007/s10044-021-00984-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8106971/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33994847/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Pattern%20Anal%20Appl&amp;title=Automatic%20detection%20of%20coronavirus%20disease%20(COVID-19)%20using%20X-ray%20images%20and%20deep%20convolutional%20neural%20networks&amp;author=A%20Narin&amp;author=C%20Kaya&amp;author=Z%20Pamuk&amp;volume=24&amp;issue=3&amp;publication_year=2021&amp;pages=1207-20&amp;pmid=33994847&amp;doi=10.1007/s10044-021-00984-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref059">
<span class="label">59.</span><cite>Zheng C, Deng X, Fu Q, Zhou Q, Feng J, Ma H, et al. Deep learning-based detection for COVID-19 from chest CT using weak label. MedRxiv, 2020–03. 2020.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=MedRxiv&amp;title=Deep%20learning-based%20detection%20for%20COVID-19%20from%20chest%20CT%20using%20weak%20label&amp;author=C%20Zheng&amp;author=X%20Deng&amp;author=Q%20Fu&amp;author=Q%20Zhou&amp;author=J%20Feng&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref060">
<span class="label">60.</span><cite>Xue S, Abhayaratne C. Region-of-interest aware 3D ResNet for classification of COVID-19 chest computerised tomography scans. IEEE Access. 2023;11:28856–72. doi: 10.1109/access.2023.3260632</cite> [<a href="https://doi.org/10.1109/access.2023.3260632" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=Region-of-interest%20aware%203D%20ResNet%20for%20classification%20of%20COVID-19%20chest%20computerised%20tomography%20scans&amp;author=S%20Xue&amp;author=C%20Abhayaratne&amp;volume=11&amp;publication_year=2023&amp;pages=28856-72&amp;doi=10.1109/access.2023.3260632&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref061">
<span class="label">61.</span><cite>Bernheim A, Mei X, Huang M, Yang Y, Fayad ZA, Zhang N, et al. Chest CT findings in coronavirus disease-19 (COVID-19): relationship to duration of infection. Radiology. 2020;295(3):200463. doi: 10.1148/radiol.2020200463

</cite> [<a href="https://doi.org/10.1148/radiol.2020200463" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7233369/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32077789/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Radiology&amp;title=Chest%20CT%20findings%20in%20coronavirus%20disease-19%20(COVID-19):%20relationship%20to%20duration%20of%20infection&amp;author=A%20Bernheim&amp;author=X%20Mei&amp;author=M%20Huang&amp;author=Y%20Yang&amp;author=ZA%20Fayad&amp;volume=295&amp;issue=3&amp;publication_year=2020&amp;pages=200463&amp;pmid=32077789&amp;doi=10.1148/radiol.2020200463&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref062">
<span class="label">62.</span><cite>Harmon SA, Sanford TH, Xu S, Turkbey EB, Roth H, Xu Z, et al. Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets. Nat Commun. 2020;11(1):4080. doi: 10.1038/s41467-020-17971-2

</cite> [<a href="https://doi.org/10.1038/s41467-020-17971-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7429815/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32796848/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat%20Commun&amp;title=Artificial%20intelligence%20for%20the%20detection%20of%20COVID-19%20pneumonia%20on%20chest%20CT%20using%20multinational%20datasets&amp;author=SA%20Harmon&amp;author=TH%20Sanford&amp;author=S%20Xu&amp;author=EB%20Turkbey&amp;author=H%20Roth&amp;volume=11&amp;issue=1&amp;publication_year=2020&amp;pages=4080&amp;pmid=32796848&amp;doi=10.1038/s41467-020-17971-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref063">
<span class="label">63.</span><cite>Upadhyay AK, Bhandari AK. Semi-supervised modified-UNet for lung infection image segmentation. IEEE Trans Radiat Plasma Med Sci. 2023;7(6):638–49. doi: 10.1109/trpms.2023.3272209</cite> [<a href="https://doi.org/10.1109/trpms.2023.3272209" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Radiat%20Plasma%20Med%20Sci&amp;title=Semi-supervised%20modified-UNet%20for%20lung%20infection%20image%20segmentation&amp;author=AK%20Upadhyay&amp;author=AK%20Bhandari&amp;volume=7&amp;issue=6&amp;publication_year=2023&amp;pages=638-49&amp;doi=10.1109/trpms.2023.3272209&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref064">
<span class="label">64.</span><cite>Albataineh Z, Aldrweesh F, Alzubaidi MA. COVID-19 CT-images diagnosis and severity assessment using machine learning algorithm. Cluster Comput. 2023:1–16. doi: 10.1007/s10586-023-03972-5

</cite> [<a href="https://doi.org/10.1007/s10586-023-03972-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9871425/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36712413/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cluster%20Comput&amp;title=COVID-19%20CT-images%20diagnosis%20and%20severity%20assessment%20using%20machine%20learning%20algorithm&amp;author=Z%20Albataineh&amp;author=F%20Aldrweesh&amp;author=MA%20Alzubaidi&amp;publication_year=2023&amp;pages=1-16&amp;pmid=36712413&amp;doi=10.1007/s10586-023-03972-5&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref065">
<span class="label">65.</span><cite>Shiraishi J, Katsuragawa S, Ikezoe J, Matsumoto T, Kobayashi T, Komatsu K, et al. Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists’ detection of pulmonary nodules. AJR Am J Roentgenol. 2000;174(1):71–4. doi: 10.2214/ajr.174.1.1740071

</cite> [<a href="https://doi.org/10.2214/ajr.174.1.1740071" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/10628457/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=AJR%20Am%20J%20Roentgenol&amp;title=Development%20of%20a%20digital%20image%20database%20for%20chest%20radiographs%20with%20and%20without%20a%20lung%20nodule:%20receiver%20operating%20characteristic%20analysis%20of%20radiologists%E2%80%99%20detection%20of%20pulmonary%20nodules&amp;author=J%20Shiraishi&amp;author=S%20Katsuragawa&amp;author=J%20Ikezoe&amp;author=T%20Matsumoto&amp;author=T%20Kobayashi&amp;volume=174&amp;issue=1&amp;publication_year=2000&amp;pages=71-4&amp;pmid=10628457&amp;doi=10.2214/ajr.174.1.1740071&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref066">
<span class="label">66.</span><cite>Stefanini AM, Fidelis TO, Penna GM, Pessanha GRG, Marques RAG, de Oliveira DC. Tomographic identification and evaluation of pulmonary involvement due to SARS-CoV-2 infection using artificial intelligence and image segmentation technique. Bioengineering and Biomedical Signal and Image Processing: First International Conference, BIOMESIP 2021, Proceedings 1; 2021 July19–21; Meloneras, Gran Canaria, Spain; 2021. p. 405–16.</cite> [<a href="https://scholar.google.com/scholar_lookup?Stefanini%20AM,%20Fidelis%20TO,%20Penna%20GM,%20Pessanha%20GRG,%20Marques%20RAG,%20de%20Oliveira%20DC.%20Tomographic%20identification%20and%20evaluation%20of%20pulmonary%20involvement%20due%20to%20SARS-CoV-2%20infection%20using%20artificial%20intelligence%20and%20image%20segmentation%20technique.%20Bioengineering%20and%20Biomedical%20Signal%20and%20Image%20Processing:%20First%20International%20Conference,%20BIOMESIP%202021,%20Proceedings%201;%202021%20July19%E2%80%9321;%20Meloneras,%20Gran%20Canaria,%20Spain;%202021.%20p.%20405%E2%80%9316." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref067">
<span class="label">67.</span><cite>Zhang K, Qi S, Cai J, Zhao D, Yu T, Yue Y, et al. Content-based image retrieval with a Convolutional Siamese Neural Network: Distinguishing lung cancer and tuberculosis in CT images. Comput Biol Med. 2022;140:105096. doi: 10.1016/j.compbiomed.2021.105096

</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2021.105096" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34872010/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Biol%20Med&amp;title=Content-based%20image%20retrieval%20with%20a%20Convolutional%20Siamese%20Neural%20Network:%20Distinguishing%20lung%20cancer%20and%20tuberculosis%20in%20CT%20images&amp;author=K%20Zhang&amp;author=S%20Qi&amp;author=J%20Cai&amp;author=D%20Zhao&amp;author=T%20Yu&amp;volume=140&amp;publication_year=2022&amp;pages=105096&amp;pmid=34872010&amp;doi=10.1016/j.compbiomed.2021.105096&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref068">
<span class="label">68.</span><cite>Singh J, Tripathy A, Garg P, Kumar A. Lung tuberculosis detection using anti-aliased convolutional networks. Procedia Comput Sci. 2020;173:281–90.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Procedia%20Comput%20Sci&amp;title=Lung%20tuberculosis%20detection%20using%20anti-aliased%20convolutional%20networks&amp;author=J%20Singh&amp;author=A%20Tripathy&amp;author=P%20Garg&amp;author=A%20Kumar&amp;volume=173&amp;publication_year=2020&amp;pages=281-90&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref069">
<span class="label">69.</span><cite>Yan C, Wang L, Lin J, Xu J, Zhang T, Qi J, et al. A fully automatic artificial intelligence–based CT image analysis system for accurate detection, diagnosis, and quantitative severity evaluation of pulmonary tuberculosis. Eur Radiol. 2022:1–12.</cite> [<a href="https://doi.org/10.1007/s00330-021-08365-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8628489/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34842959/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eur%20Radiol&amp;title=A%20fully%20automatic%20artificial%20intelligence%E2%80%93based%20CT%20image%20analysis%20system%20for%20accurate%20detection,%20diagnosis,%20and%20quantitative%20severity%20evaluation%20of%20pulmonary%20tuberculosis&amp;author=C%20Yan&amp;author=L%20Wang&amp;author=J%20Lin&amp;author=J%20Xu&amp;author=T%20Zhang&amp;publication_year=2022&amp;pages=1-12&amp;pmid=34842959&amp;doi=10.1007/s00330-021-08365-z&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref070">
<span class="label">70.</span><cite>López-González FJ, Silva-Rodríguez J, Paredes-Pacheco J, Niñerola-Baizán A, Efthimiou N, Martín-Martín C, et al. Intensity normalization methods in brain FDG-PET quantification. Neuroimage. 2020;222:117229. doi: 10.1016/j.neuroimage.2020.117229

</cite> [<a href="https://doi.org/10.1016/j.neuroimage.2020.117229" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32771619/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neuroimage&amp;title=Intensity%20normalization%20methods%20in%20brain%20FDG-PET%20quantification&amp;author=FJ%20L%C3%B3pez-Gonz%C3%A1lez&amp;author=J%20Silva-Rodr%C3%ADguez&amp;author=J%20Paredes-Pacheco&amp;author=A%20Ni%C3%B1erola-Baiz%C3%A1n&amp;author=N%20Efthimiou&amp;volume=222&amp;publication_year=2020&amp;pages=117229&amp;pmid=32771619&amp;doi=10.1016/j.neuroimage.2020.117229&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref071">
<span class="label">71.</span><cite>Bhatia P, Sinha A, Joshi SP, Sarkar R, Ghosh R, Jana S. Automated quantification of inflamed lung regions in chest CT by UNET and SegCaps: a comparative analysis in COVID-19 cases. 2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC). IEEE; 2022. p. 3785–8.</cite> [<a href="https://doi.org/10.1109/EMBC48229.2022.9870901" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36086503/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Bhatia%20P,%20Sinha%20A,%20Joshi%20SP,%20Sarkar%20R,%20Ghosh%20R,%20Jana%20S.%20Automated%20quantification%20of%20inflamed%20lung%20regions%20in%20chest%20CT%20by%20UNET%20and%20SegCaps:%20a%20comparative%20analysis%20in%20COVID-19%20cases.%202022%2044th%20Annual%20International%20Conference%20of%20the%20IEEE%20Engineering%20in%20Medicine%20&amp;%20Biology%20Society%20(EMBC).%20IEEE;%202022.%20p.%203785%E2%80%938." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref072">
<span class="label">72.</span><cite>Javaid M, Khan IH. Internet of Things (IoT) enabled healthcare helps to take the challenges of COVID-19 Pandemic. J Oral Biol Craniofac Res. 2021;11(2):209–14. doi: 10.1016/j.jobcr.2021.01.015

</cite> [<a href="https://doi.org/10.1016/j.jobcr.2021.01.015" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7897999/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33665069/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Oral%20Biol%20Craniofac%20Res&amp;title=Internet%20of%20Things%20(IoT)%20enabled%20healthcare%20helps%20to%20take%20the%20challenges%20of%20COVID-19%20Pandemic&amp;author=M%20Javaid&amp;author=IH%20Khan&amp;volume=11&amp;issue=2&amp;publication_year=2021&amp;pages=209-14&amp;pmid=33665069&amp;doi=10.1016/j.jobcr.2021.01.015&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref073">
<span class="label">73.</span><cite>Bokolo AJ. Exploring the adoption of telemedicine and virtual software for care of outpatients during and after COVID-19 pandemic. Ir J Med Sci. 2021;190(1):1–10. doi: 10.1007/s11845-020-02299-z

</cite> [<a href="https://doi.org/10.1007/s11845-020-02299-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7340859/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32642981/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ir%20J%20Med%20Sci&amp;title=Exploring%20the%20adoption%20of%20telemedicine%20and%20virtual%20software%20for%20care%20of%20outpatients%20during%20and%20after%20COVID-19%20pandemic&amp;author=AJ%20Bokolo&amp;volume=190&amp;issue=1&amp;publication_year=2021&amp;pages=1-10&amp;pmid=32642981&amp;doi=10.1007/s11845-020-02299-z&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref074">
<span class="label">74.</span><cite>Koralnik IJ, Tyler KL. COVID‐19: a global threat to the nervous system. Ann Neurol. 2020;88(1):1–11.
</cite> [<a href="https://doi.org/10.1002/ana.25807" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7300753/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32506549/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ann%20Neurol&amp;title=COVID%E2%80%9019:%20a%20global%20threat%20to%20the%20nervous%20system&amp;author=IJ%20Koralnik&amp;author=KL%20Tyler&amp;volume=88&amp;issue=1&amp;publication_year=2020&amp;pages=1-11&amp;pmid=32506549&amp;doi=10.1002/ana.25807&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref075">
<span class="label">75.</span><cite>Tang C, Wang Y, Lv H, Guan Z, Gu J. Caution against corticosteroid-based COVID-19 treatment. Lancet. 2020;395(10239):1759–60. doi: 10.1016/S0140-6736(20)30749-2

</cite> [<a href="https://doi.org/10.1016/S0140-6736(20)30749-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7247780/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32464115/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Lancet&amp;title=Caution%20against%20corticosteroid-based%20COVID-19%20treatment&amp;author=C%20Tang&amp;author=Y%20Wang&amp;author=H%20Lv&amp;author=Z%20Guan&amp;author=J%20Gu&amp;volume=395&amp;issue=10239&amp;publication_year=2020&amp;pages=1759-60&amp;pmid=32464115&amp;doi=10.1016/S0140-6736(20)30749-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref076">
<span class="label">76.</span><cite>Zhang W, Zhou D, Li L, Gu Q. Neural thompson sampling. arXiv:2010.00827 [Preprint]. 2020.</cite>
</li>
<li id="pone.0327419.ref077">
<span class="label">77.</span><cite>Yildirim I, Belledonne M, Freiwald W, Tenenbaum J. Efficient inverse graphics in biological face processing. Sci Adv. 2020;6(10):eaax5979. doi: 10.1126/sciadv.aax5979

</cite> [<a href="https://doi.org/10.1126/sciadv.aax5979" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7056304/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32181338/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Adv&amp;title=Efficient%20inverse%20graphics%20in%20biological%20face%20processing&amp;author=I%20Yildirim&amp;author=M%20Belledonne&amp;author=W%20Freiwald&amp;author=J%20Tenenbaum&amp;volume=6&amp;issue=10&amp;publication_year=2020&amp;pmid=32181338&amp;doi=10.1126/sciadv.aax5979&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref078">
<span class="label">78.</span><cite>Helle RH, Lemu HG. A case study on use of 3D scanning for reverse engineering and quality control. Mater Today: Proc. 2021;45:5255–62. doi: 10.1016/j.matpr.2021.01.828</cite> [<a href="https://doi.org/10.1016/j.matpr.2021.01.828" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Mater%20Today:%20Proc&amp;title=A%20case%20study%20on%20use%20of%203D%20scanning%20for%20reverse%20engineering%20and%20quality%20control&amp;author=RH%20Helle&amp;author=HG%20Lemu&amp;volume=45&amp;publication_year=2021&amp;pages=5255-62&amp;doi=10.1016/j.matpr.2021.01.828&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref079">
<span class="label">79.</span><cite>Botero UJ, Wilson R, Lu H, Rahman MT, Mallaiyan MA, Ganji F, et al. Hardware trust and assurance through reverse engineering: a tutorial and outlook from image analysis and machine learning perspectives. J Emerg Technol Comput Syst. 2021;17(4):1–53. doi: 10.1145/3464959</cite> [<a href="https://doi.org/10.1145/3464959" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Emerg%20Technol%20Comput%20Syst&amp;title=Hardware%20trust%20and%20assurance%20through%20reverse%20engineering:%20a%20tutorial%20and%20outlook%20from%20image%20analysis%20and%20machine%20learning%20perspectives&amp;author=UJ%20Botero&amp;author=R%20Wilson&amp;author=H%20Lu&amp;author=MT%20Rahman&amp;author=MA%20Mallaiyan&amp;volume=17&amp;issue=4&amp;publication_year=2021&amp;pages=1-53&amp;doi=10.1145/3464959&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref080">
<span class="label">80.</span><cite>Zhang Y, Attique Khan M, Zhu Z, Wang S. SNELM: SqueezeNet-guided ELM for COVID-19 recognition. Comput Syst Sci Eng. 2023;46(1):13–26. doi: 10.32604/csse.2023.034172

</cite> [<a href="https://doi.org/10.32604/csse.2023.034172" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7614503/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37155222/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Syst%20Sci%20Eng&amp;title=SNELM:%20SqueezeNet-guided%20ELM%20for%20COVID-19%20recognition&amp;author=Y%20Zhang&amp;author=M%20Attique%20Khan&amp;author=Z%20Zhu&amp;author=S%20Wang&amp;volume=46&amp;issue=1&amp;publication_year=2023&amp;pages=13-26&amp;pmid=37155222&amp;doi=10.32604/csse.2023.034172&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref081">
<span class="label">81.</span><cite>Turkoglu M. COVID-19 detection system using chest CT images and multiple kernels-extreme learning machine based on deep neural network. Ing Rech Biomed. 2021;42(4):207–14. doi: 10.1016/j.irbm.2021.01.004

</cite> [<a href="https://doi.org/10.1016/j.irbm.2021.01.004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7839628/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33527035/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ing%20Rech%20Biomed&amp;title=COVID-19%20detection%20system%20using%20chest%20CT%20images%20and%20multiple%20kernels-extreme%20learning%20machine%20based%20on%20deep%20neural%20network&amp;author=M%20Turkoglu&amp;volume=42&amp;issue=4&amp;publication_year=2021&amp;pages=207-14&amp;pmid=33527035&amp;doi=10.1016/j.irbm.2021.01.004&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref082">
<span class="label">82.</span><cite>Dey N, Zhang Y-D, Rajinikanth V, Pugalenthi R, Raja NSM. Customized VGG19 architecture for pneumonia detection in chest X-rays. Pattern Recognit Lett. 2021;143:67–74. doi: 10.1016/j.patrec.2020.12.010</cite> [<a href="https://doi.org/10.1016/j.patrec.2020.12.010" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Pattern%20Recognit%20Lett&amp;title=Customized%20VGG19%20architecture%20for%20pneumonia%20detection%20in%20chest%20X-rays&amp;author=N%20Dey&amp;author=Y-D%20Zhang&amp;author=V%20Rajinikanth&amp;author=R%20Pugalenthi&amp;author=NSM%20Raja&amp;volume=143&amp;publication_year=2021&amp;pages=67-74&amp;doi=10.1016/j.patrec.2020.12.010&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref083">
<span class="label">83.</span><cite>Allingham JU, Wenzel F, Mariet ZE, Mustafa B, Puigcerver J, Houlsby N, et al. Sparse MoEs meet efficient ensembles. arXiv:2110.03360 [Preprint]. 2021.</cite>
</li>
<li id="pone.0327419.ref084">
<span class="label">84.</span><cite>Bharati S, Podder P, Mondal MRH, Prasath VBS. CO-ResNet: optimized ResNet model for COVID-19 diagnosis from X-ray images. HIS. 2021;17(1–2):71–85. doi: 10.3233/his-210008</cite> [<a href="https://doi.org/10.3233/his-210008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=HIS&amp;title=CO-ResNet:%20optimized%20ResNet%20model%20for%20COVID-19%20diagnosis%20from%20X-ray%20images&amp;author=S%20Bharati&amp;author=P%20Podder&amp;author=MRH%20Mondal&amp;author=VBS%20Prasath&amp;volume=17&amp;publication_year=2021&amp;pages=71-85&amp;doi=10.3233/his-210008&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref085">
<span class="label">85.</span><cite>Dalvi PP, Edla DR, Purushothama BR. Diagnosis of coronavirus disease from chest X-ray images using DenseNet-169 architecture. SN Comput Sci. 2023;4(3):214. doi: 10.1007/s42979-022-01627-7

</cite> [<a href="https://doi.org/10.1007/s42979-022-01627-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9936468/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36811126/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=SN%20Comput%20Sci&amp;title=Diagnosis%20of%20coronavirus%20disease%20from%20chest%20X-ray%20images%20using%20DenseNet-169%20architecture&amp;author=PP%20Dalvi&amp;author=DR%20Edla&amp;author=BR%20Purushothama&amp;volume=4&amp;issue=3&amp;publication_year=2023&amp;pages=214&amp;pmid=36811126&amp;doi=10.1007/s42979-022-01627-7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref086">
<span class="label">86.</span><cite>Ridnik T, Ben-Baruch E, Noy A, Zelnik-Manor L. Imagenet-21k pretraining for the masses. arXiv:2104.10972 [Preprint]. 2021.</cite>
</li>
<li id="pone.0327419.ref087">
<span class="label">87.</span><cite>Morid MA, Borjali A, Del Fiol G. A scoping review of transfer learning research on medical image analysis using ImageNet. Comput Biol Med. 2021;128:104115. doi: 10.1016/j.compbiomed.2020.104115

</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2020.104115" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33227578/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Biol%20Med&amp;title=A%20scoping%20review%20of%20transfer%20learning%20research%20on%20medical%20image%20analysis%20using%20ImageNet&amp;author=MA%20Morid&amp;author=A%20Borjali&amp;author=G%20Del%20Fiol&amp;volume=128&amp;publication_year=2021&amp;pages=104115&amp;pmid=33227578&amp;doi=10.1016/j.compbiomed.2020.104115&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref088">
<span class="label">88.</span><cite>Wang C, Chen D, Hao L, Liu X, Zeng Y, Chen J, et al. Pulmonary image classification based on inception-v3 transfer learning model. IEEE Access. 2019;7:146533–41. doi: 10.1109/access.2019.2946000</cite> [<a href="https://doi.org/10.1109/access.2019.2946000" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=Pulmonary%20image%20classification%20based%20on%20inception-v3%20transfer%20learning%20model&amp;author=C%20Wang&amp;author=D%20Chen&amp;author=L%20Hao&amp;author=X%20Liu&amp;author=Y%20Zeng&amp;volume=7&amp;publication_year=2019&amp;pages=146533-41&amp;doi=10.1109/access.2019.2946000&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref089">
<span class="label">89.</span><cite>Dong N, Zhao L, Wu CH, Chang JF. Inception v3 based cervical cell classification combined with artificially extracted features. Appl Soft Comput. 2020;93:106311. doi: 10.1016/j.asoc.2020.106311</cite> [<a href="https://doi.org/10.1016/j.asoc.2020.106311" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Appl%20Soft%20Comput&amp;title=Inception%20v3%20based%20cervical%20cell%20classification%20combined%20with%20artificially%20extracted%20features&amp;author=N%20Dong&amp;author=L%20Zhao&amp;author=CH%20Wu&amp;author=JF%20Chang&amp;volume=93&amp;publication_year=2020&amp;pages=106311&amp;doi=10.1016/j.asoc.2020.106311&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref090">
<span class="label">90.</span><cite>Fränti P, Mariescu-Istodor R. Soft precision and recall. Pattern Recognit Lett. 2023;167:115–21. doi: 10.1016/j.patrec.2023.02.005</cite> [<a href="https://doi.org/10.1016/j.patrec.2023.02.005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Pattern%20Recognit%20Lett&amp;title=Soft%20precision%20and%20recall&amp;author=P%20Fr%C3%A4nti&amp;author=R%20Mariescu-Istodor&amp;volume=167&amp;publication_year=2023&amp;pages=115-21&amp;doi=10.1016/j.patrec.2023.02.005&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref091">
<span class="label">91.</span><cite>Huyut MT. Automatic detection of severely and mildly infected COVID-19 patients with supervised machine learning models. Ing Rech Biomed. 2023;44(1):100725. doi: 10.1016/j.irbm.2022.05.006

</cite> [<a href="https://doi.org/10.1016/j.irbm.2022.05.006" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9158375/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35673548/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ing%20Rech%20Biomed&amp;title=Automatic%20detection%20of%20severely%20and%20mildly%20infected%20COVID-19%20patients%20with%20supervised%20machine%20learning%20models&amp;author=MT%20Huyut&amp;volume=44&amp;issue=1&amp;publication_year=2023&amp;pages=100725&amp;pmid=35673548&amp;doi=10.1016/j.irbm.2022.05.006&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref092">
<span class="label">92.</span><cite>Gupta H, Bansal N, Garg S, Mallik H, Prabha A, Yadav J. A hybrid convolutional neural network model to detect COVID‐19 and pneumonia using chest X‐ray images. Int J Imaging Syst Tech. 2022;33(1):39–52. doi: 10.1002/ima.22829</cite> [<a href="https://doi.org/10.1002/ima.22829" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Imaging%20Syst%20Tech&amp;title=A%20hybrid%20convolutional%20neural%20network%20model%20to%20detect%20COVID%E2%80%9019%20and%20pneumonia%20using%20chest%20X%E2%80%90ray%20images&amp;author=H%20Gupta&amp;author=N%20Bansal&amp;author=S%20Garg&amp;author=H%20Mallik&amp;author=A%20Prabha&amp;volume=33&amp;issue=1&amp;publication_year=2022&amp;pages=39-52&amp;doi=10.1002/ima.22829&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref093">
<span class="label">93.</span><cite>Khan E, Rehman MZU, Ahmed F, Alfouzan FA, Alzahrani NM, Ahmad J. Chest X-ray classification for the detection of COVID-19 using deep learning techniques. Sensors (Basel). 2022;22(3):1211. doi: 10.3390/s22031211

</cite> [<a href="https://doi.org/10.3390/s22031211" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8838072/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35161958/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sensors%20(Basel)&amp;title=Chest%20X-ray%20classification%20for%20the%20detection%20of%20COVID-19%20using%20deep%20learning%20techniques&amp;author=E%20Khan&amp;author=MZU%20Rehman&amp;author=F%20Ahmed&amp;author=FA%20Alfouzan&amp;author=NM%20Alzahrani&amp;volume=22&amp;issue=3&amp;publication_year=2022&amp;pages=1211&amp;pmid=35161958&amp;doi=10.3390/s22031211&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref094">
<span class="label">94.</span><cite>Ukwuoma CC, Qin Z, Agbesi VK, Cobbinah BM, Yussif SB, Abubakar HS, et al. Dual_Pachi: attention-based dual path framework with intermediate second order-pooling for Covid-19 detection from chest X-ray images. Comput Biol Med. 2022;151(Pt A):106324. doi: 10.1016/j.compbiomed.2022.106324

</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2022.106324" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9671873/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36423531/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Biol%20Med&amp;title=Dual_Pachi:%20attention-based%20dual%20path%20framework%20with%20intermediate%20second%20order-pooling%20for%20Covid-19%20detection%20from%20chest%20X-ray%20images&amp;author=CC%20Ukwuoma&amp;author=Z%20Qin&amp;author=VK%20Agbesi&amp;author=BM%20Cobbinah&amp;author=SB%20Yussif&amp;volume=151&amp;publication_year=2022&amp;pages=106324&amp;pmid=36423531&amp;doi=10.1016/j.compbiomed.2022.106324&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref095">
<span class="label">95.</span><cite>Li J, Wang Y, Wang S, Wang J, Liu J, Jin Q, et al. Multiscale attention guided network for COVID-19 diagnosis using chest X-ray images. IEEE J Biomed Health Inform. 2021;25(5):1336–46. doi: 10.1109/JBHI.2021.3058293

</cite> [<a href="https://doi.org/10.1109/JBHI.2021.3058293" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8545167/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33560995/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20J%20Biomed%20Health%20Inform&amp;title=Multiscale%20attention%20guided%20network%20for%20COVID-19%20diagnosis%20using%20chest%20X-ray%20images&amp;author=J%20Li&amp;author=Y%20Wang&amp;author=S%20Wang&amp;author=J%20Wang&amp;author=J%20Liu&amp;volume=25&amp;issue=5&amp;publication_year=2021&amp;pages=1336-46&amp;pmid=33560995&amp;doi=10.1109/JBHI.2021.3058293&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref096">
<span class="label">96.</span><cite>Zhang J, Xie Y, Pang G, Liao Z, Verjans J, Li W, et al. Viral pneumonia screening on chest X-rays using confidence-aware anomaly detection. IEEE Trans Med Imaging. 2021;40(3):879–90. doi: 10.1109/TMI.2020.3040950

</cite> [<a href="https://doi.org/10.1109/TMI.2020.3040950" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8544953/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33245693/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Med%20Imaging&amp;title=Viral%20pneumonia%20screening%20on%20chest%20X-rays%20using%20confidence-aware%20anomaly%20detection&amp;author=J%20Zhang&amp;author=Y%20Xie&amp;author=G%20Pang&amp;author=Z%20Liao&amp;author=J%20Verjans&amp;volume=40&amp;issue=3&amp;publication_year=2021&amp;pages=879-90&amp;pmid=33245693&amp;doi=10.1109/TMI.2020.3040950&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref097">
<span class="label">97.</span><cite>Loey M, Smarandache F, Khalifa NE. Within the lack of chest COVID-19 X-ray dataset: a novel detection model based on GAN and deep transfer learning. Symmetry. 2020;12(4):651.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Symmetry&amp;title=Within%20the%20lack%20of%20chest%20COVID-19%20X-ray%20dataset:%20a%20novel%20detection%20model%20based%20on%20GAN%20and%20deep%20transfer%20learning&amp;author=M%20Loey&amp;author=F%20Smarandache&amp;author=NE%20Khalifa&amp;volume=12&amp;issue=4&amp;publication_year=2020&amp;pages=651&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref098">
<span class="label">98.</span><cite>Moujahid H, Cherradi B, Al-Sarem M, Bahatti L, Eljialy ABAMY, Alsaeedi A, et al. Combining CNN and Grad-Cam for COVID-19 disease prediction and visual explanation. Intell Autom Soft Comput. 2022;32(2).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Intell%20Autom%20Soft%20Comput&amp;title=Combining%20CNN%20and%20Grad-Cam%20for%20COVID-19%20disease%20prediction%20and%20visual%20explanation&amp;author=H%20Moujahid&amp;author=B%20Cherradi&amp;author=M%20Al-Sarem&amp;author=L%20Bahatti&amp;author=ABAMY%20Eljialy&amp;volume=32&amp;issue=2&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref099">
<span class="label">99.</span><cite>Lasker A, Ghosh M, Obaidullah SM, Chakraborty C, Roy K. LWSNet-a novel deep-learning architecture to segregate Covid-19 and pneumonia from x-ray imagery. Multimed Tools Appl. 2023;82(14):21801–23.
</cite> [<a href="https://doi.org/10.1007/s11042-022-14247-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9734972/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36532598/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Multimed%20Tools%20Appl&amp;title=LWSNet-a%20novel%20deep-learning%20architecture%20to%20segregate%20Covid-19%20and%20pneumonia%20from%20x-ray%20imagery&amp;author=A%20Lasker&amp;author=M%20Ghosh&amp;author=SM%20Obaidullah&amp;author=C%20Chakraborty&amp;author=K%20Roy&amp;volume=82&amp;issue=14&amp;publication_year=2023&amp;pages=21801-23&amp;pmid=36532598&amp;doi=10.1007/s11042-022-14247-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref100">
<span class="label">100.</span><cite>Kordnoori S, Sabeti M, Mostafaei H, Banihashemi SSA. LungXpertAI: a deep multi-task learning model for chest CT scan analysis and COVID-19 detection. Biomed Signal Process Control. 2025;99:106866. doi: 10.1016/j.bspc.2024.106866</cite> [<a href="https://doi.org/10.1016/j.bspc.2024.106866" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Biomed%20Signal%20Process%20Control&amp;title=LungXpertAI:%20a%20deep%20multi-task%20learning%20model%20for%20chest%20CT%20scan%20analysis%20and%20COVID-19%20detection&amp;author=S%20Kordnoori&amp;author=M%20Sabeti&amp;author=H%20Mostafaei&amp;author=SSA%20Banihashemi&amp;volume=99&amp;publication_year=2025&amp;pages=106866&amp;doi=10.1016/j.bspc.2024.106866&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327419.ref101">
<span class="label">101.</span><cite>Liu H, Zhao M, She C, Peng H, Liu M, Li B. Classification of CT scan and X-ray dataset based on deep learning and particle swarm optimization. PLoS One. 2025;20(1):e0317450. doi: 10.1371/journal.pone.0317450

</cite> [<a href="https://doi.org/10.1371/journal.pone.0317450" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11771893/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39869555/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Classification%20of%20CT%20scan%20and%20X-ray%20dataset%20based%20on%20deep%20learning%20and%20particle%20swarm%20optimization&amp;author=H%20Liu&amp;author=M%20Zhao&amp;author=C%20She&amp;author=H%20Peng&amp;author=M%20Liu&amp;volume=20&amp;issue=1&amp;publication_year=2025&amp;pmid=39869555&amp;doi=10.1371/journal.pone.0317450&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adsm93_" lang="en" class="supplementary-materials"><h3 class="pmc_sec_title">Supplementary Materials</h3>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="caption p">
<span>S1 File. Dataset details.</span><p>(DOCX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334061/bin/pone.0327419.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0327419.s001.docx</a><sup> (15.9KB, docx) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The dataset used in this study is from a third-party source, and its direct link has been provided below. <a href="https://www.kaggle.com/datasets/mehradaria/covid19-lung-ct-scans" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/datasets/mehradaria/covid19-lung-ct-scans</a>.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0327419"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0327419.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (4.1 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12334061/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12334061/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12334061%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334061/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12334061/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12334061/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40779565/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12334061/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40779565/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12334061/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12334061/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="IvXKpwRuVGqCaiIURApyW4youE5Bd06jdKoZmIollmdYpcpiJtUG02mmlO2MrK88">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Web Policies

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    FOIA

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS Vulnerability Disclosure

    
</a>

                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Help

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Accessibility

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Careers

    
</a>

                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NLM

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NIH

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    USA.gov

    
</a>

                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-370d5dd6.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-917ba005.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-d9849939.js"></script>
    
    

    </body>
</html>
