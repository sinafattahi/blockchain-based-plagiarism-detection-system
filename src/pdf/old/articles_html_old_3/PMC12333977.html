
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-a68b4900.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-0a3f24ce.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Developmental differences in perceiving arousal and valence from dynamically unfolding emotional expressions - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="4DDE4A0889B778831B4A080006685B1E.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12333977/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="Developmental differences in perceiving arousal and valence from dynamically unfolding emotional expressions">
<meta name="citation_author" content="Nikol Tsenkova">
<meta name="citation_author_institution" content="Psychology and Sports Science, Department of Developmental Psychology, Justus Liebig University, Giessen, Germany">
<meta name="citation_author" content="Daniela Bahn">
<meta name="citation_author_institution" content="Institute of German Linguistics, Clinical Linguistics, Philipps University, Marburg, Germany">
<meta name="citation_author" content="Christina Kauschke">
<meta name="citation_author_institution" content="Institute of German Linguistics, Clinical Linguistics, Philipps University, Marburg, Germany">
<meta name="citation_author" content="Gudrun Schwarzer">
<meta name="citation_author_institution" content="Psychology and Sports Science, Department of Developmental Psychology, Justus Liebig University, Giessen, Germany">
<meta name="citation_publication_date" content="2025 Aug 8">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0329554">
<meta name="citation_doi" content="10.1371/journal.pone.0329554">
<meta name="citation_pmid" content="40779505">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12333977/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12333977/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12333977/pdf/pone.0329554.pdf">
<meta name="description" content="The development of emotion perception has predominantly been studied using static, unimodal stimuli featuring the faces of young adults. Most findings indicate a processing advantage for positive emotions in children (positivity bias) and a ...">
<meta name="og:title" content="Developmental differences in perceiving arousal and valence from dynamically unfolding emotional expressions">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="The development of emotion perception has predominantly been studied using static, unimodal stimuli featuring the faces of young adults. Most findings indicate a processing advantage for positive emotions in children (positivity bias) and a ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12333977/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-testid="header" data-header >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            
                <a class="ncbi-header__logo-container" href="https://www.ncbi.nlm.nih.gov/">
                    <img alt="
                                  NCBI home page
                              "
                         class="ncbi-header__logo-image"
                         src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg" />
                </a>
            

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-testid="searchPanel"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      aria-describedby="search-field-desktop-navigation-help-text"
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only"
                           data-testid="label"
                           for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           data-testid="textInput"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    data-testid="button"
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  role="search">
                <label class="usa-sr-only" for="search-field">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="search" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        <a class="usa-link" href="https://www.ncbi.nlm.nih.gov/pmc/advanced/" data-ga-action="featured_link" data-ga-label="advanced_search">
                            Advanced Search
                        </a>
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12333977">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0329554"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0329554.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12333977%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12333977/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12333977/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12333977/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0329554" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 8;20(8):e0329554. doi: <a href="https://doi.org/10.1371/journal.pone.0329554" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329554</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Developmental differences in perceiving arousal and valence from dynamically unfolding emotional expressions</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Tsenkova%20N%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Nikol Tsenkova</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Nikol Tsenkova</span></h3>
<div class="p">
<sup>1</sup>Psychology and Sports Science, Department of Developmental Psychology, Justus Liebig University, Giessen, Germany</div>
<div>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Tsenkova%20N%22%5BAuthor%5D" class="usa-link"><span class="name western">Nikol Tsenkova</span></a>
</div>
</div>
<sup>1,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Bahn%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Daniela Bahn</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Daniela Bahn</span></h3>
<div class="p">
<sup>2</sup>Institute of German Linguistics, Clinical Linguistics, Philipps University, Marburg, Germany</div>
<div>Conceptualization, Methodology, Supervision, Validation, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Bahn%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">Daniela Bahn</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kauschke%20C%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Christina Kauschke</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Christina Kauschke</span></h3>
<div class="p">
<sup>2</sup>Institute of German Linguistics, Clinical Linguistics, Philipps University, Marburg, Germany</div>
<div>Conceptualization, Funding acquisition, Methodology, Supervision</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kauschke%20C%22%5BAuthor%5D" class="usa-link"><span class="name western">Christina Kauschke</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Schwarzer%20G%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Gudrun Schwarzer</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Gudrun Schwarzer</span></h3>
<div class="p">
<sup>1</sup>Psychology and Sports Science, Department of Developmental Psychology, Justus Liebig University, Giessen, Germany</div>
<div>Conceptualization, Formal analysis, Funding acquisition, Project administration, Resources, Supervision, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Schwarzer%20G%22%5BAuthor%5D" class="usa-link"><span class="name western">Gudrun Schwarzer</span></a>
</div>
</div>
<sup>1</sup>
</div>
<div class="cg p">Editor: <span class="name western">Irving A Cruz-Albarran</span><sup>3</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>Psychology and Sports Science, Department of Developmental Psychology, Justus Liebig University, Giessen, Germany</div>
<div id="aff002">
<sup>2</sup>Institute of German Linguistics, Clinical Linguistics, Philipps University, Marburg, Germany</div>
<div id="edit1">
<sup>3</sup>Autonomous University of Queretaro: Universidad Autonoma de Queretaro, MEXICO</div>
<div class="author-notes p">
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>The authors have declared that no competing interests exist.</p></div>
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>nikol.tsenkova@psychol.uni-giessen.de</span></p>
</div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Nikol Tsenkova</span></strong>: <span class="role">Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Daniela Bahn</span></strong>: <span class="role">Conceptualization, Methodology, Supervision, Validation, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Christina Kauschke</span></strong>: <span class="role">Conceptualization, Funding acquisition, Methodology, Supervision</span>
</div>
<div>
<strong class="contrib"><span class="name western">Gudrun Schwarzer</span></strong>: <span class="role">Conceptualization, Formal analysis, Funding acquisition, Project administration, Resources, Supervision, Writing – review &amp; editing</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Irving A Cruz-Albarran</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Oct 9; Accepted 2025 Jul 17; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Tsenkova et al</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12333977  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40779505/" class="usa-link">40779505</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>The development of emotion perception has predominantly been studied using static, unimodal stimuli featuring the faces of young adults. Most findings indicate a processing advantage for positive emotions in children (positivity bias) and a negativity bias in adults, although these results are usually task-dependent. We created a new stimulus database comprising digital avatars from four age groups, dynamically expressing happiness, happy-surprise, anger, and sadness in visual (face only) and visual-verbal (face and voice) conditions. To determine whether previously found biases would re-emerge with this new database, we tested the arousal and valence perception of positive and negative emotions in 6- and 7-year-old children and young adults. Our results revealed high correlations between children’s and adults’ responses but also significant differences: children rated negative expressions as more arousing compared to adults and positive emotions as more positive than adults. Additionally, visual-verbal presentations were perceived as more arousing than visual across both age groups. In terms of valence, all participants found positive emotions as more positive in the visual condition, whereas negative emotions were perceived as more negative in the visual-verbal condition. As one of the first studies to employ dynamically multimodal emotional expressions, our findings underscore the relevance of studying developmental differences in emotion perception using naturalistic stimuli.</p></section><section id="sec001"><h2 class="pmc_sec_title">Introduction</h2>
<p>Effectively perceiving emotions is crucial for successful interactions and cultivating lasting relationships [<a href="#pone.0329554.ref001" class="usa-link" aria-describedby="pone.0329554.ref001">1</a>]. Understanding the development of emotional perception is imperative, given its role in relationship quality [<a href="#pone.0329554.ref002" class="usa-link" aria-describedby="pone.0329554.ref002">2</a>], life satisfaction [<a href="#pone.0329554.ref003" class="usa-link" aria-describedby="pone.0329554.ref003">3</a>], and academic achievements [<a href="#pone.0329554.ref004" class="usa-link" aria-describedby="pone.0329554.ref004">4</a>] throughout the different stages of our lifespan. Consequently, numerous studies have focused on exploring the developmental trajectory of emotion perception across various age groups – mostly in children [<a href="#pone.0329554.ref005" class="usa-link" aria-describedby="pone.0329554.ref005">5</a>,<a href="#pone.0329554.ref006" class="usa-link" aria-describedby="pone.0329554.ref006">6</a>] and adults [<a href="#pone.0329554.ref007" class="usa-link" aria-describedby="pone.0329554.ref007">7</a>], but also in adolescents [<a href="#pone.0329554.ref008" class="usa-link" aria-describedby="pone.0329554.ref008">8</a>,<a href="#pone.0329554.ref009" class="usa-link" aria-describedby="pone.0329554.ref009">9</a>], and older adults [<a href="#pone.0329554.ref010" class="usa-link" aria-describedby="pone.0329554.ref010">10</a>].</p>
<p>To examine how emotion perception develops over time, researchers employ various tools such as rating scales based on established affective models like the Russell’s Circumplex Model of Affect [<a href="#pone.0329554.ref011" class="usa-link" aria-describedby="pone.0329554.ref011">11</a>]. Within it, emotions are plotted in a coordinate system based on their valence (x-axis) and arousal (y-axis) values. Valence denotes whether an emotional stimulus is perceived as positive/pleasant or negative/unpleasant, while arousal, ranging from high to low, can be conceptualized in two distinct ways. It can refer to an individual’s subjective feeling about a stimulus (internal arousal) or to the perceived level of arousal of the stimulus itself, such as when viewing facial expressions of emotion (external arousal). Some studies indicate that children and adults rate emotions similarly in terms of valence and arousal [<a href="#pone.0329554.ref012" class="usa-link" aria-describedby="pone.0329554.ref012">12</a>,<a href="#pone.0329554.ref013" class="usa-link" aria-describedby="pone.0329554.ref013">13</a>], as shown by the high correlations between their responses. Although these rating studies reveal high correlations between both group’s perceptions, some uncover notable age-dependent biases. For instance, Vesker and colleagues [<a href="#pone.0329554.ref014" class="usa-link" aria-describedby="pone.0329554.ref014">14</a>] had participants rate arousal and valence of positive and negative facial expressions using two Self-Assessment Manikin (SAM) scales [<a href="#pone.0329554.ref015" class="usa-link" aria-describedby="pone.0329554.ref015">15</a>]. Children rated positive expressions as significantly more arousing and positive as compared to adults, indicating a positivity bias. Similarly, in a subsequent study, Vesker et al. [<a href="#pone.0329554.ref016" class="usa-link" aria-describedby="pone.0329554.ref016">16</a>] discovered that children exhibit a positivity bias for valence by categorizing positive expressions faster and more accurately than negative ones. On the other hand, adults displayed the opposite bias, showing greater accuracy for negative expressions than for positive ones. Biases have also emerged when using different emotional modalities, for example from emotional words or body movements. For instance, 5- and 6-year-old children were found to be faster and more accurate in categorizing positive words compared to older children and adults [<a href="#pone.0329554.ref017" class="usa-link" aria-describedby="pone.0329554.ref017">17</a>]. Additionally, 5-year-old children were found to be more accurate with happy body movements, while adults with angry movements [<a href="#pone.0329554.ref018" class="usa-link" aria-describedby="pone.0329554.ref018">18</a>]. However, findings on positivity and negativity biases tend to be inconsistent, likely due to the specificity of the experimental tasks. While some tasks are more likely to produce a positivity bias (e.g., identification tasks: children [<a href="#pone.0329554.ref019" class="usa-link" aria-describedby="pone.0329554.ref019">19</a>], adults [<a href="#pone.0329554.ref020" class="usa-link" aria-describedby="pone.0329554.ref020">20</a>]; intensity and arousal ratings: children [<a href="#pone.0329554.ref021" class="usa-link" aria-describedby="pone.0329554.ref021">21</a>]), others have shown to induce a negativity bias (e.g., visual search tasks: both children and adults [<a href="#pone.0329554.ref022" class="usa-link" aria-describedby="pone.0329554.ref022">22</a>]; recognition tasks: younger and older adults [<a href="#pone.0329554.ref023" class="usa-link" aria-describedby="pone.0329554.ref023">23</a>]; for a comprehensive review on infants and children, see [<a href="#pone.0329554.ref024" class="usa-link" aria-describedby="pone.0329554.ref024">24</a>,<a href="#pone.0329554.ref025" class="usa-link" aria-describedby="pone.0329554.ref025">25</a>]).</p>
<p>Such inconsistencies highlight some limitations in traditional stimulus design, calling into question the ecological validity of these findings. Two recent meta-analyses have made similar observations in studies with young adults [<a href="#pone.0329554.ref026" class="usa-link" aria-describedby="pone.0329554.ref026">26</a>] and with children [<a href="#pone.0329554.ref027" class="usa-link" aria-describedby="pone.0329554.ref027">27</a>], underscoring the importance of employing multimodal dynamic stimuli to detect actual emotion perception age-dependent changes across the lifespan. In contrary, the majority of emotion perception studies rely on static images, such as still images of facial and bodily expressions [<a href="#pone.0329554.ref028" class="usa-link" aria-describedby="pone.0329554.ref028">28</a>]. Furthermore, emotion modalities are often studied in isolation from each other, which is rare in everyday life [<a href="#pone.0329554.ref029" class="usa-link" aria-describedby="pone.0329554.ref029">29</a>], and many existing emotion databases feature only high-intensity levels of emotion expressions [<a href="#pone.0329554.ref030" class="usa-link" aria-describedby="pone.0329554.ref030">30</a>,<a href="#pone.0329554.ref031" class="usa-link" aria-describedby="pone.0329554.ref031">31</a>], which could be perceived as unnatural. Finally, most emotional stimuli databases lack the facial representation diversity encountered in everyday life and are typically expressed by young adults [<a href="#pone.0329554.ref032" class="usa-link" aria-describedby="pone.0329554.ref032">32</a>]. Several databases have addressed the issues of static presentation and isolated modalities – GEMEP [<a href="#pone.0329554.ref033" class="usa-link" aria-describedby="pone.0329554.ref033">33</a>], the CREMA-D [<a href="#pone.0329554.ref034" class="usa-link" aria-describedby="pone.0329554.ref034">34</a>], and the RAVDESS [<a href="#pone.0329554.ref035" class="usa-link" aria-describedby="pone.0329554.ref035">35</a>]. In all three, the highest emotion recognition accuracy was achieved in the multimodal audio-video condition (73%, 64%, and 80%, respectively) compared to the video-only (59%, 58%, and 75%) and the audio-only conditions (44%, 41%, and 60%). It appears that multimodal presentation enhances emotion recognition accuracy by at least 5% [<a href="#pone.0329554.ref036" class="usa-link" aria-describedby="pone.0329554.ref036">36</a>], while dynamic presentation, especially at low intensities, improves recognition compared to static presentation [<a href="#pone.0329554.ref037" class="usa-link" aria-describedby="pone.0329554.ref037">37</a>]. A handful of studies have dealt with the issue of age variation of presented faces, albeit using static stimuli like the Radboud Faces Database [<a href="#pone.0329554.ref038" class="usa-link" aria-describedby="pone.0329554.ref038">38</a>] which comprises photographs of emotional expressions from children and young adults. The participants’ responses revealed that happiness was the most accurately recognized emotion (98% for adults’ faces, 97% for children’s faces). Zsido and colleagues [<a href="#pone.0329554.ref039" class="usa-link" aria-describedby="pone.0329554.ref039">39</a>] conducted a visual detection task using photographs of expressions displayed by child and adult models from various databases and further confirmed the happiness superiority effect across diverse age groups and stimuli. One study [<a href="#pone.0329554.ref040" class="usa-link" aria-describedby="pone.0329554.ref040">40</a>] using a dynamic database with unposed, spontaneous facial expressions of children showed once again that the highest accuracy came from identifying happiness (72%). Similarly, a recent study by Negrão et al. [<a href="#pone.0329554.ref041" class="usa-link" aria-describedby="pone.0329554.ref041">41</a>] utilizing dynamic presentations of children’s facial expressions reaffirmed that happiness is the most easily recognized emotion, as agreed upon by four judges. We are aware of only one experiment [<a href="#pone.0329554.ref042" class="usa-link" aria-describedby="pone.0329554.ref042">42</a>] that explored how children and adults differed in labeling dynamic emotional expressions in three modalities separately (facial, vocal, and postural) and in a combined multimodal condition. Overall, children had the highest accuracy for sadness, followed by happiness and anger, with fear having the lowest recognition score. In contrast, adults were most accurate with happy expressions. Regarding the modalities, both age groups were most accurate with face-only and multimodal presentations, while the lowest accuracy came from the voice-only condition. This is consistent with previous research [<a href="#pone.0329554.ref043" class="usa-link" aria-describedby="pone.0329554.ref043">43</a>,<a href="#pone.0329554.ref044" class="usa-link" aria-describedby="pone.0329554.ref044">44</a>], which suggests that facial expressions alone are sufficient—or at least comparable—to bi- or multimodal information for emotion detection.</p>
<p>To date, and to the best of our knowledge, no study on the development of emotion perception has incorporated all relevant aspects of natural emotion perception: dynamic and simultaneous multimodal presentation (facial, postural, verbal information), lower emotion intensity, and diversity of facial age identities (childhood to late adulthood). A promising solution to address this gap involves utilizing technologies like the digital avatars called MetaHumans [<a href="#pone.0329554.ref045" class="usa-link" aria-describedby="pone.0329554.ref045">45</a>], which offer flexibility and realism in avatar creation and animation. MetaHumans offer several advantages in emotion research, including lifelike appearance, customizable characteristics (e.g., gender, age), precise expression intensity, and most importantly, full control and replicability of emotional expressions across different characters and conditions. Using MetaHumans allows for the creation of highly realistic digital characters while maintaining a great level of experimental control. However, the Uncanny Valley effect [<a href="#pone.0329554.ref046" class="usa-link" aria-describedby="pone.0329554.ref046">46</a>] poses a significant challenge when implementing digital avatars to study emotion perception [<a href="#pone.0329554.ref047" class="usa-link" aria-describedby="pone.0329554.ref047">47</a>]. This effect can produce feelings of discomfort and uneasiness upon encountering humanoid robots or digital humans. Despite this drawback, several studies featuring MetaHumans and other digital avatars have shown promising outcomes. Participants have reported higher levels of perceived attractiveness and lower levels of eeriness [<a href="#pone.0329554.ref048" class="usa-link" aria-describedby="pone.0329554.ref048">48</a>,<a href="#pone.0329554.ref049" class="usa-link" aria-describedby="pone.0329554.ref049">49</a>] and have experienced empathy when viewing emotional expressions [<a href="#pone.0329554.ref050" class="usa-link" aria-describedby="pone.0329554.ref050">50</a>]. Other studies have demonstrated the efficacy of digital avatars in eliciting emotional responses from participants despite their lower level of naturalness [<a href="#pone.0329554.ref051" class="usa-link" aria-describedby="pone.0329554.ref051">51</a>–<a href="#pone.0329554.ref053" class="usa-link" aria-describedby="pone.0329554.ref053">53</a>].</p>
<p>The current online rating study aims to investigate children’s and adults’ perceptions of valence and arousal in emotional expressions using stimuli that embody natural characteristics of emotions. To this end, we utilized a newly created stimulus database comprising digital avatars (MetaHumans), which features dynamic uni- and multimodal presentation, lower intensity, and a diverse range of facial ages. Specifically, we aim to explore the extent to which the previously reported positivity and negativity biases in the development of emotion perception are evident when using such natural stimulus material.</p></section><section id="sec002"><h2 class="pmc_sec_title">Methods</h2>
<section id="sec003"><h3 class="pmc_sec_title">Participants</h3>
<p>We tested two separate groups of German children, one for each of the two dependent measures (arousal and valence), and one group of adults who performed both measures simultaneously. In order to avoid fatigue and confusion regarding the difference between the scales, we separated the children sample into two groups. We did not expect such issues with the adult sample; hence, there was only one adult group.</p>
<p>The arousal group consisted of 26 children (age range: 6–7 years, <em>M</em><sub><em>age</em></sub><em> </em>= 6.4 years, <em>SD </em>= 4 months, 16 females), and the valence group consisted of 28 children (age range: 6–7 years, <em>M</em><sub><em>age</em></sub><em> </em>= 6.4 years, <em>SD</em> = 4 months, 13 females). The adult sample comprised 28 German young adults (age range: 19–35 years, <em>M</em><sub><em>age</em></sub> = 25 years, <em>SD</em> = 4.5 years, 16 females).</p>
<p>Children were recruited via the contact database of the Department of Developmental Psychology at the Justus Liebig University Giessen. This database consists of children of all ages (from infancy to adolescence), whose parents have given their consent to be contacted for experiments done in the Department of Developmental Psychology. The specific lists from which participants were contacted were children born in the months between May 2017 and February 2018, aged 6 and 7 years. For their participation, children received a 10-euro toyshop voucher. Adults were recruited via mailing lists of different universities (Giessen, Frankfurt, and Marburg) and were given student credits or a 15-euro digital voucher.</p>
<p>This study was approved by the local Ethics Committee of the Department of Psychology, Justus Liebig University of Giessen, Germany (#2021−0037). Consent for participation was obtained in written form before the beginning of the experiment—either from the adult participants themselves or, for the child participants, from their parents or caretakers.</p>
<p>We conducted an a priori G*Power analysis [<a href="#pone.0329554.ref054" class="usa-link" aria-describedby="pone.0329554.ref054">54</a>] to determine the required sample size for robust statistical power, with the criterion set at <em>α</em> = .05 and power = .95. A mixed analysis of variance (ANOVA) was selected as the statistical test, with two age groups as the between-subjects factor and four within-subjects measurements (positive visual, positive visual-verbal, negative visual, and negative visual-verbal). The results indicated that a minimum total sample size of 36 participants (18 per group) was sufficient to test the hypotheses of the current study.</p></section><section id="sec004"><h3 class="pmc_sec_title">Stimuli</h3>
<p>We created a new stimulus database of dynamic emotion expressions, incorporating both visual and verbal information. We named it Meta–MED (<strong>Meta</strong>Human <strong>M</strong>ultimodal <strong>E</strong>motion <strong>D</strong>ynamic database, <a href="https://jlubox.uni-giessen.de/getlink/fiAYjs8XxdV9nPkFLLvSPpB7/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">link</a>). For its creation, we used high-fidelity digital avatars called MetaHumans owned by Epic Games [<a href="#pone.0329554.ref045" class="usa-link" aria-describedby="pone.0329554.ref045">45</a>]. Our objectives during the creation process were the following:</p>
<ul class="list" style="list-style-type:none">
<li><p>I. Generating characters from four different age groups: young children (visually representing 6–7-year-olds), adolescents (visually representing 14–15-year-olds), young adults (visually representing individuals in their late 20s), and older adults (visually representing individuals in their 70s and above).</p></li>
<li><p>II. Including both female and male characters in each age group.</p></li>
<li><p>III. Creating two sets of visually distinct characters per age group and gender to investigate whether emotion perception is affected by the specific design of the characters.</p></li>
<li><p>IV. Producing dynamic, low–intensity animations for each character expressing four different emotions: two positive (happiness and happy-surprise) and two negative emotions (anger and sadness).</p></li>
<li><p>V. Developing two conditions for each emotion expression: one featuring only visual information (pure facial expression, from now on “visual condition”), and one combining facial and coinciding verbal information (from now on “visual-verbal condition”).</p></li>
</ul></section><section id="sec005"><h3 class="pmc_sec_title">Creation of the Metahumans</h3>
<p>To achieve objectives I, II, and III, we modified the appearance of existing character templates in the MetaHuman Creator (<a href="https://metahuman.unrealengine.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://metahuman.unrealengine.com/</a>), an online cloud-based tool for designing characters. We utilized the available templates for adolescents, young adults, and older adults. No templates were available for 6–7-year-old children, so we first created these using the MakeHuman software [<a href="#pone.0329554.ref055" class="usa-link" aria-describedby="pone.0329554.ref055">55</a>]. We then imported and stylized them in the MetaHuman Creator tool.</p>
<p>In total, we developed 16 characters, comprising two genders, four age groups, and two character versions, all of White origin. Finally, we animated these characters in Unreal Engine v. 5.1 [<a href="#pone.0329554.ref045" class="usa-link" aria-describedby="pone.0329554.ref045">45</a>].</p></section><section id="sec006"><h3 class="pmc_sec_title">Dynamic presentation of the Metahumans</h3>
<p>To achieve objectives IV and V—animating the four emotions and creating the visual and visual-verbal conditions—we used Face Control Rig within Unreal Engine. This in-built tool is based on the Facial Action Coding System (FACS) developed by Ekman and Friesen [<a href="#pone.0329554.ref056" class="usa-link" aria-describedby="pone.0329554.ref056">56</a>], which provides detailed information on the precise movements of facial muscles associated with different emotions. The Face Control Rig enables the creation of nuanced facial expressions, such as furrowing the brows to convey anger or raising the corners of the mouth to indicate happiness. Furthermore, to ensure a natural depiction of these emotions, we observed videos of actors showing the same facial expressions using FACS (ADFES database [<a href="#pone.0329554.ref057" class="usa-link" aria-describedby="pone.0329554.ref057">57</a>]) to fine-tune the animated expressions accordingly.</p>
<p>Additionally, we used the Face Control Rig to animate lip movements for the visual-verbal condition, where emotional expressions were synchronized with spoken sentences. We selected simple and clear sentences that align with the expressed emotions to ensure they are suitable for children as young as six years. The sentence structure across all emotions was: “Ich bin + specific word for emotion”, resulting in the following sentences: “Ich bin glücklich” (“I am happy”), “Ich bin traurig” (“I am sad”), “Ich bin überrascht” (“I am surprised”), and “Ich bin wütend” (“I am angry”). All verbal information accompanies the visual expression, with the exception of happy-surprise, where the characters first display the emotion and then speak.</p>
<p>Once a certain condition was successfully animated for one MetaHuman (e.g., happiness in a child character), the facial movements were replicated across all other characters. Each emotion animation was limited to two seconds, reflecting the quick onset and brief duration typically associated with natural emotional expressions [<a href="#pone.0329554.ref058" class="usa-link" aria-describedby="pone.0329554.ref058">58</a>]. Research shows that shorter displays of emotional expressions generally yield higher accuracy in recognition [<a href="#pone.0329554.ref059" class="usa-link" aria-describedby="pone.0329554.ref059">59</a>] and are perceived as more realistic when lasting under 1,500 milliseconds [<a href="#pone.0329554.ref060" class="usa-link" aria-describedby="pone.0329554.ref060">60</a>]. Each emotional expression begins with a neutral face and gradually unfolds into the full emotion, maintaining a lower intensity to ensure a natural appearance [<a href="#pone.0329554.ref061" class="usa-link" aria-describedby="pone.0329554.ref061">61</a>,<a href="#pone.0329554.ref062" class="usa-link" aria-describedby="pone.0329554.ref062">62</a>].</p>
<p>For the vocalizations, we obtained the recordings from individuals in the same age groups as the characters in the stimuli (children, adolescents, young adults, and older adults). They were asked to record their voices at home with a phone recorder and were instructed to “feel the emotion” when pronouncing the sentences for a more realistic representation as opposed to neutral pronunciation. Their approval to use their voices for the stimuli was successfully received via consent forms.</p>
<p>The final dataset consist of 128 individual video clips (2 dataset versions x 8 characters x 4 emotions x 2 conditions). For static examples of the stimuli, see <a href="#pone.0329554.g001" class="usa-link">Fig 1</a>. Dynamic examples are available at this <a href="https://jlubox.uni-giessen.de/getlink/fiAYjs8XxdV9nPkFLLvSPpB7/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">link</a>.</p>
<figure class="fig xbox font-sm" id="pone.0329554.g001"><h4 class="obj_head">Fig 1. Examples of the stimuli.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12333977_pone.0329554.g001.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9f6f/12333977/c160e3656750/pone.0329554.g001.jpg" loading="lazy" height="169" width="660" alt="Fig 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329554.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Still images of the digital avatars from the four age groups (from youngest to oldest) expressing all emotions (left to right: sadness, happiness, happy-surprise, anger). For the dynamic version, please refer to the link provided in the Methods/Stimuli section.</p></figcaption></figure></section><section id="sec007"><h3 class="pmc_sec_title">Procedure</h3>
<p>For the online rating study, we used the SoSci survey platform [<a href="#pone.0329554.ref063" class="usa-link" aria-describedby="pone.0329554.ref063">63</a>], which participants accessed from their personal computers. The presentation of stimuli was pseudo-randomized in four different sequences to avoid repeating the same emotion, character, or age group consecutively. Each of the four presentation versions started with a different emotion, age group, and character. Participants were randomly assigned to one of the four presentation sequences using the randomization function available on SoSci. An optional one-minute break was provided halfway through.</p>
<p>At the end of the experiment, we included three additional scales from the Godspeed Questionnaire [<a href="#pone.0329554.ref064" class="usa-link" aria-describedby="pone.0329554.ref064">64</a>] to test for Uncanny Valley effects related to the overall impression of the new stimulus dataset. The Godspeed Questionnaire (GQ) is the most frequently used tool in the field of Human-Robot Interaction [<a href="#pone.0329554.ref065" class="usa-link" aria-describedby="pone.0329554.ref065">65</a>], with over 3,000 citations as of April 2025, and has been translated into 19 languages. It consists of five scales, which can be used independently: Anthropomorphism (α = 0.87), Animacy (α = 0.92), Likeability (α = 0.70), Perceived Intelligence (α = 0.75), and Perceived Safety (α = 0.91) [<a href="#pone.0329554.ref066" class="usa-link" aria-describedby="pone.0329554.ref066">66</a>]. The German version of the GQ [<a href="#pone.0329554.ref067" class="usa-link" aria-describedby="pone.0329554.ref067">67</a>] has been reported to have good internal reliability (α = 0.70). In terms of validity, only the Polish version [<a href="#pone.0329554.ref068" class="usa-link" aria-describedby="pone.0329554.ref068">68</a>] has undergone factor analysis, which yielded a total variance of 74.24% for the four-factor solution.</p>
<p>We took two items from the scale Likeability and one item from the scale Anthropomorphism. We included a rating scale for each ranging from 1 to 5, with the midpoint (3) labeled as “I don’t know”. The first item from the scale Likeability (also called Likeability in our experiment) measured participants’ perception of how nice or scary the MetaHumans appeared, ranging from “very scary” to “very nice”. The second item from the scale Likeability (Friendliness) determined the extent to which the MetaHumans seemed friendly, with responses ranging from “very unfriendly” to “very friendly”. The third item from the scale Anthropomorphism (Realism) evaluated how realistic the characters seemed, ranging from “very unrealistic” to “very realistic”.</p>
<p>Participants rated arousal and valence perceived in the emotional expression videos by using two SAM scales [<a href="#pone.0329554.ref015" class="usa-link" aria-describedby="pone.0329554.ref015">15</a>]. The valence scale features figures with facial expressions ranging from frowning to smiling. The arousal scale includes figures depicting states from calm and relaxed to excited. Participants selected the figure that best represented the emotion in the video, rather than how they subjectively felt about it.</p>
<p>To achieve this in our children sample, parents received detailed instructions at the beginning of the experiment. They assisted their children to access the online study, understand the task, and complete the ratings. Parents were advised not to influence their children’s responses to ensure the collection of their subjective perceptions. Children were randomly assigned to one of the two character database versions and completed 64 trials for either valence or arousal.</p>
<p>At the start of the experiment, participants completed two practice trials with characters not featured in the main dataset. Each experimental trial presented a video of an emotional expression on a single page, with the SAM scales and the following prompts displayed underneath:</p>
<ul class="list" style="list-style-type:none">
<li><p>I. For valence: “Geben Sie bitte an, wie unangenehm/angenehm das gezeigte Gefühl wirkt.” (“Please indicate how unpleasant/pleasant the shown feeling appears.”).</p></li>
<li><p>II. For arousal: “Geben Sie bitte an, wie ruhig/aufgeregt das gezeigte Gefühl wirkt.” (“Please indicate how calm/excited the shown feeling appears.”). For an example of a trial page and the SAM scales, see <a href="#pone.0329554.s001" class="usa-link">S1_Trial example</a>.</p></li>
</ul>
<p>Videos could be played only twice; after this, the play button was disabled and the videos faded to black to prevent replays. Selecting an answer was mandatory to proceed to the next page/trial.</p>
<p>Adults received a SoSci link including both arousal and valence scales shown simultaneously, one above the other. They completed both dataset versions for a total of 128 trials. The procedure and task were otherwise identical to those used for the children.</p>
<p>Responses were collected in the period between 01/06/2023 and 31/10/2023.</p></section></section><section id="sec008"><h2 class="pmc_sec_title">Results</h2>
<section id="sec009"><h3 class="pmc_sec_title">Preliminary analyses</h3>
<p>To test the normality of our arousal and valence data, we performed Shapiro–Wilk tests, which confirmed that the data met the assumptions for parametric analysis. Furthermore, we found no significant gender effects for either of the two measures – for arousal, F(1, 50) =.11, p = .73, η2p = .002, for valence, F(1, 52) =.23, p = .63, η2p = .004. Thus, we did not include gender in all further analyses. Correlational heatmaps can be found in <a href="#pone.0329554.s002" class="usa-link">S2_Correlational heatmaps</a>.</p></section><section id="sec010"><h3 class="pmc_sec_title">Effects of different stimuli versions</h3>
<p>To investigate whether the visual appearance of the MetaHuman characters influenced the participants’ responses, we compared the ratings given for the two datasets, each featuring distinct sets of MetaHuman characters.</p>
<p>For the child sample, given that each child saw only one version, we performed independent-sample <em>t</em>-tests. For arousal, we found no significant differences, <em>t</em>(24) = 0.01, <em>p</em> = .98. For valence, the results showed no significant difference between children’s ratings of the two versions, <em>t</em>(26) = 0.35, <em>p</em> = .72.</p>
<p>For the adult sample, where all participants saw both versions, we used paired-sample t-tests. For arousal, we found no significant difference, <em>t</em>(27) = −1.16, <em>p</em> = .25, comparing the two versions.</p>
<p>A significant result emerged for valence, <em>t</em>(27) = 2.30, <em>p</em> = .02. To further investigate, we compared the positive and negative emotion ratings between version one and version two. The significant difference stemmed from the positive emotions, <em>t</em>(27) = 2.40, <em>p</em> = .02, with version one (<em>M</em><sub>1</sub> = 4.18, <em>SD</em> = 0.29) being rated slightly higher in valence compared to version two (<em>M</em><sub><em>2</em></sub> = 4.13, <em>SD</em> = 0.28). To identify which specific positive emotion (happiness or happy-surprise) in which stimulus condition (visual or visual-verbal) contributed to this difference, we performed four additional paired-sample t-tests. A significant difference was found in the visual-verbal happiness ratings, <em>t</em>(27) = 2.51, <em>p</em> = .01, with version one (<em>M</em><sub><em>1</em></sub> = 4.20, <em>SD</em> = 0.29) being higher than version two (<em>M</em><sub><em>2</em></sub> = 4.08, <em>SD</em> = 0.34). Given the high number of comparisons (6), we applied a Bonferroni correction to adjust the alpha level to 0.008. After correction, the initially significant difference in visual-verbal happiness was no longer significant..</p>
<p>Thus, while the visual appearance of the MetaHumans had a small influence on emotional responses in adults, the effects were not consistent across all measures. Therefore, for subsequent analyses, we used the mean ratings of the adults’ responses from both versions.</p></section><section id="sec011"><h3 class="pmc_sec_title">Main results: arousal</h3>
<section id="sec012"><h4 class="pmc_sec_title">Correlations between children and adults.</h4>
<p>To explore whether children and adults’ arousal responses were similar, we performed two Spearman correlations. These correlations were based on the averaged arousal ratings for each Stimulus Type (visual and visual-verbal) across the two groups. As seen in <a href="#pone.0329554.g002" class="usa-link">Fig 2</a> there is a high overlap between the two groups for the visual condition, which was confirmed by a strong correlation, <em>r</em><sub><em>s</em></sub>(30) =.75, <em><em>p</em> </em>&lt; .001.</p>
<figure class="fig xbox font-sm" id="pone.0329554.g002"><h5 class="obj_head">Fig 2. Arousal ratings of both age groups across all stimuli in the visual condition.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12333977_pone.0329554.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9f6f/12333977/58a94a43600d/pone.0329554.g002.jpg" loading="lazy" height="299" width="700" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329554.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The y-axis denotes the figure scale numerically (1 = low arousal; 5 = high arousal). The x-axis denotes all trials with 1-16 for positive expressions and 17-32 for negative expressions.</p></figcaption></figure><p><a href="#pone.0329554.g003" class="usa-link">Fig 3</a> also shows an overlap between the arousal ratings of both age groups in the visual-verbal condition supported by a significant correlation in the visual-verbal condition, <em>r</em><sub><em>s</em></sub>(30) = 0.70, <em>p</em> &lt; .001. Overall, these findings suggest that both age groups had comparable arousal responses to both the visual and visual-verbal stimuli.</p>
<figure class="fig xbox font-sm" id="pone.0329554.g003"><h5 class="obj_head">Fig 3. Arousal ratings of both age groups across all stimuli in the visual-verbal condition.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12333977_pone.0329554.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9f6f/12333977/ff988702c28f/pone.0329554.g003.jpg" loading="lazy" height="299" width="700" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329554.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The y-axis denotes the figure scale numerically (1 = low arousal; 5 = high arousal). The x-axis denotes all trials with 1-16 for positive expressions and 17-32 for negative expressions.</p></figcaption></figure></section><section id="sec013"><h4 class="pmc_sec_title">Role of age group, age of faces, valence category, and stimulus type.</h4>
<p>To get a better understanding of the factors that influenced participants’ arousal ratings, we conducted a repeated measures ANOVA using SPSS version 28 [<a href="#pone.0329554.ref069" class="usa-link" aria-describedby="pone.0329554.ref069">69</a>] examining the role of participants’ age group, the age of the MetaHuman faces, the emotional valence of the facial expressions, and the type of stimulus presentation.</p>
<p>The analysis comprised four factors: (i) the participants’ Age Group (children and adults), (ii) the Age of Faces of the digital characters (children, adolescents, young adults, older adults), (iii) the Valence Category (positive or negative), and (iv) the Stimulus Type (visual or visual-verbal). Age Group served as a between-subjects factor, whereas the Age of Faces, Valence Category and Stimulus Type were treated as within-subject factors. The reported post-hoc analyses are Bonferroni-corrected pairwise comparisons.</p>
<p>A significant main effect of Age Group was found, <em>F</em>(1, 52) = 5.55, <em>p</em> = .02, <em>η</em><sup><em>2</em></sup><em>p</em> = .096, with children giving higher arousal ratings compared to adults (<em>M</em><sub><em>child </em></sub>= 2.80, <em>M</em><sub><em>adult </em></sub>= 2.45).</p>
<p>Another significant main effect was observed for the Age of Faces, <em>F</em>(3, 156) = 2.60, <em><em>p</em> </em>= .05, <em>η</em><sup><em>2</em></sup><em><em>p</em> </em>= .05, where faces of older adults rated as more arousing compared to faces of young adults (<em>M</em><sub><em>oa </em></sub>= 2.68, <em>M</em><sub><em>ya </em></sub>= 2.59). However, this effect was not significant after Bonferroni correction (<em><em>p</em> </em>= .10).</p>
<p>Furthermore, we found a significant main effect of Stimulus Type, <em>F</em>(1, 52) = 17.25, <em><em>p</em> </em>&lt; .001, <em>η</em><sup><em>2</em></sup><em><em>p</em> </em>= .25. Post-hoc comparisons revealed that the visual-verbal condition had higher arousal ratings than the visual condition (<em>M</em><sub><em>vv </em></sub>= 2.73, <em>M</em><sub><em>v </em></sub>= 2.51, <em>p</em> &lt; .001).</p>
<p>A significant interaction between Valence Category and Age Group emerged, <em>F</em>(1, 52) = 3.87, <em>p</em> = .05, <em>η</em><sup><em>2</em></sup><em>p</em> = .07 (see <a href="#pone.0329554.g004" class="usa-link">Fig 4</a>). Post-hoc comparisons revealed that children rated negative emotions as more arousing than adults (<em>M</em><sub><em>child</em></sub> = 2.92, <em>M</em><sub><em>adult</em> </sub>= 2.27, <em>p</em> &lt; .001).</p>
<figure class="fig xbox font-sm" id="pone.0329554.g004"><h5 class="obj_head">Fig 4. Children’s and adults’ arousal ratings.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12333977_pone.0329554.g004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9f6f/12333977/42c630c9ec1d/pone.0329554.g004.jpg" loading="lazy" height="415" width="700" alt="Fig 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329554.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Mean rating values are for positive and negative expressions across both stimulus versions (visual and visual-verbal). Error bars represent standard error. *p &lt; .05 **p &lt; .01. ***p &lt; .001.</p></figcaption></figure><p>We observed a significant three-way interaction between Age of Faces, Valence Category, and Stimulus Type, <em>F</em>(3,156) = 6.81, <em><em>p</em> </em>&lt; .001, <em>η</em><sup><em>2</em></sup><em>p</em> = .12. First, faces of adolescents and older adults were rated as more arousing (<em>M</em><sub><em>ado </em></sub>= 2.46, <em>p</em> = .02; <em>M</em><sub><em>oa</em> </sub>= 2.49, <em>p</em> = .02) than children’s faces in the negative visual condition (<em>M</em><sub><em>child</em> </sub>= 2.25). Second, in the positive visual-verbal condition, older adult faces were rated higher (<em>M</em><sub><em>oa</em> </sub>= 2.89) than adolescent faces (<em>M</em><sub><em>ado</em> </sub>= 2.53, <em><em>p</em> </em>&lt; .001) and young adult faces (<em>M</em><sub><em>ya </em></sub>= 2.59, <em>p</em> = .004). Additionally, children faces were rated as more arousing in the positive visual condition than in the negative visual condition (<em>M</em><sub><em>posV </em></sub>= 2.65, <em>M</em><sub><em>negV</em> </sub>= 2.25, <em>p</em> = .01). Finally, faces of all ages were rated as more arousing in the negative visual-verbal than in the negative visual condition: children (<em>M</em><sub><em>negV</em> </sub>= 2.25, <em>M</em><sub><em>negVV</em> </sub>= 2.84, <em>p</em> &lt; .001), adolescents (<em>M</em><sub><em>negV</em> </sub>= 2.46, <em>M</em><sub><em>negVV </em></sub>= 2.79, <em>p</em> = .02), young adults (<em>M</em><sub><em>negV</em> </sub>= 2.39, <em>M</em><sub><em>negVV</em> </sub>= 2.76, <em><em>p</em> </em>&lt; . 001), and older adults (<em>M</em><sub><em>negV </em></sub>= 2.49, <em>M</em><sub><em>negVV </em></sub>= 2.77, <em>p</em> = .003). Adolescent faces were rated as more arousing in the positive visual condition compared to the positive visual-verbal condition (<em>M</em><sub><em>posV</em> </sub>= 2.70, <em>M</em><sub><em>posVV </em></sub>= 2.53, <em>p</em> = .02), while older adult faces had higher arousal in the positive visual-verbal condition than in the positive visual condition (<em>M</em><sub><em>posV</em></sub> = 2.58, <em>M</em><sub><em>posVV</em></sub> = 2.89, <em>p</em> =.001).</p>
<p>Thus, the findings show that children found emotional stimuli—specifically negative ones—as more arousing than adults. Additionally, visual-verbal stimuli were generally more arousing than visual-only stimuli, and character age significantly influenced responses depending on context: older adult faces elicited higher arousal than other face ages in the positive visual-verbal condition, while adolescent and older faces were more arousing than children’s in the negative visual condition.</p></section></section><section id="sec014"><h3 class="pmc_sec_title">Emotion-specific results</h3>
<p>To determine which negative emotion elicited stronger arousal responses in children, we compared their ratings of anger and sadness. To do so, we conducted a paired-sample t-test comparing the two negative emotions. Results confirmed that anger elicited significantly higher arousal than sadness, (<em>M</em><sub><em>anger </em></sub>= 3.33, <em>M</em><sub><em>sadness </em></sub>= 2.50), <em>t</em>(25) = 5.20, <em>p</em> &lt; .001.</p></section><section id="sec015"><h3 class="pmc_sec_title">Observed power</h3>
<p>We performed a post-hoc G-Power analysis for arousal (8 measurements, 54 participants) revealed observed power of 71.9%, slightly below the conventional threshold of.80.</p></section><section id="sec016"><h3 class="pmc_sec_title">Main results: valence</h3>
<p>To determine whether the valence responses of children and adults were similar, we performed two Spearman correlations. These correlations were based on the averaged valence ratings for each Stimulus Type (visual and visual-verbal) across the two groups. As shown by the overlap in <a href="#pone.0329554.g005" class="usa-link">Fig 5</a>, the two age groups’ responses were highly correlated, <em>r</em><sub><em>s</em></sub>(30) = 0.97, <em>p</em> &lt; .001.</p>
<figure class="fig xbox font-sm" id="pone.0329554.g005"><h4 class="obj_head">Fig 5. Valence ratings of both age groups across all stimuli in the visual-verbal condition.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12333977_pone.0329554.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9f6f/12333977/dc943d28bd61/pone.0329554.g005.jpg" loading="lazy" height="290" width="660" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329554.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The y-axis denotes the figure scale numerically (1 = low valence; 5 = high valence). The x-axis denotes all trials with 1-16 for positive expressions and 17-32 for negative expressions.</p></figcaption></figure><p>In the visual-verbal condition, there was a very strong correlation between the responses of the two groups, <em>r</em><sub><em>s</em></sub>(30) = 0.94, <em>p</em> &lt; .001, as seen in the overlap in <a href="#pone.0329554.g006" class="usa-link">Fig 6</a>. Similarly to the arousal results, these findings point to highly comparable valence responses of the two age groups to both the visual and visual-verbal stimuli.</p>
<figure class="fig xbox font-sm" id="pone.0329554.g006"><h4 class="obj_head">Fig 6. Valence ratings of both age groups across all stimuli in the visual-verbal condition.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12333977_pone.0329554.g006.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9f6f/12333977/881f644b7954/pone.0329554.g006.jpg" loading="lazy" height="290" width="660" alt="Fig 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329554.g006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The y-axis denotes the figure scale numerically (1 = low valence; 5 = high valence). The x-axis denotes all trials with 1-16 for positive expressions and 17-32 for negative expressions.</p></figcaption></figure></section><section id="sec017"><h3 class="pmc_sec_title">Role of age group, age of faces, valence category, and stimulus type</h3>
<p>To grasp the influence of the different factors that influenced participants’ valence ratings, we conducted a repeated measures ANOVA where we explored the role of participants’ age group, the age of the MetaHuman faces, the emotional valence of the facial expressions, and the type of stimulus presentation. The reported post-hoc analyses are Bonferroni-corrected pairwise comparisons.</p>
<p>We found a significant main effect of Age Group, <em>F</em>(1, 54) = 17.99, <em>p</em> &lt; .001, <em>η</em><sup><em>2</em></sup><em>p</em> = .25, with children giving higher valence ratings compared to adults (<em>M</em><sub><em>child </em></sub>= 3.32, <em>M</em><sub><em>adult </em></sub>= 3.14).</p>
<p>There was also a significant main effect of Age of Faces, <em>F</em>(3, 162) = 3.99, <em><em>p</em> </em>= .009, <em>η</em><sup><em>2</em></sup><em>p</em> = .07. Faces of older adults received higher valence ratings compared to those of adolescents (<em>M</em><sub><em>ado</em> </sub>= 3.19, <em>M</em><sub><em>oa</em> </sub>= 3.27, <em>p</em> = .01).</p>
<p>This finding is better understood with the significant interaction between Age of Faces and Valence Category, <em>F</em>(3, 162) = 4.68, <em>p</em> = .004, <em>η</em><sup><em>2</em></sup><em>p</em> = .08, which revealed that this effect comes from positive emotions (<em>M</em><sub><em>ado</em> </sub>= 4.27, <em>M</em><sub><em>oa </em></sub>= 4.43, <em>p</em> &lt; .001). Additionally, the post-hoc comparisons revealed that older adults’ faces were perceived as more positive than those of children (<em>M</em><sub><em>child </em></sub>= 4.33, <em>M</em><sub><em>oa</em> </sub>= 4.43, <em>p</em> = .007).</p>
<p>A main effect of Valence Category emerged, <em>F</em>(1, 54) = 1198.54, <em>p</em> &lt; .001, <em>η</em><sup><em>2</em></sup><em>p</em> =.96, which is unsurprising considering that valence responses for negative and positive emotions should represent two extremes (i.e., low scores for negative emotions, high scores for positive).</p>
<p>In addition, there was a significant main effect of Stimulus Type, <em>F</em>(1, 54)=100.16, <em>p</em> &lt;.001, <em>η2p</em> =.65, showing that the visual condition was rated higher in valence as compared to the visual-verbal condition (<em>M</em><sub><em>v</em></sub> = 3.34, <em>M</em><sub><em>vv</em></sub> = 3.12, <em>p</em> &lt;.001).</p>
<p>An interaction between Age of Faces and Stimulus Type was found, <em>F</em>(1, 162) = 3.30, <em>p</em> =.02, <em>η</em><sup><em>2</em></sup><em>p</em> =.06. All faces were rated higher in valence in the visual compared to the visual-verbal condition: children (<em>M</em><sub><em>v</em></sub> = 3.34, <em>M</em><sub><em>vv</em></sub> = 3.10, <em>p</em> &lt;.001), adolescents (<em>M</em><sub><em>v</em></sub> = 3.30, <em>M</em><sub><em>vv</em></sub> = 3.09, <em>p</em> &lt; .001), young adults (<em>M</em><sub><em>v </em></sub>= 3.37, <em>M</em><sub><em>vv </em></sub>= 3.10, <em>p</em> &lt; .001), and older adults (<em>M</em><sub><em>v</em> </sub>= 3.34, <em>M</em><sub><em>vv</em> </sub>= 3.19, <em>p</em> &lt; .001).</p>
<p>Finally, we discovered a three-way interaction between Age Group, Valence Category, and Stimulus Type (<em>F</em>(1, 54) = 12.66, <em>p</em> &lt; .001, <em>η</em><sup><em>2</em></sup><em><em>p</em> </em>= .19) as seen in <a href="#pone.0329554.g007" class="usa-link">Fig 7</a>. Post-hoc pairwise comparisons revealed that children rated positive emotions as more positive compared to adults in both the visual (<em>M</em><sub><em>childV </em></sub>= 4.55, <em>M</em><sub><em>adultV </em></sub>= 4.31, <em>p</em> = .004) and visual-verbal conditions (<em>M</em><sub><em>childVV</em> </sub>= 4.45, <em>M</em><sub><em>adultVV </em></sub>= 4.10, <em>p</em> &lt; .001). Additionally, adults rated negative emotions as more negative compared to children only in the visual condition (<em>M</em><sub><em>adultV</em> </sub>= 2.15, <em>M</em><sub><em>childV</em> </sub>= 2.32, <em>p</em> = .04). Furthermore, children rated positive emotions as more positive in the visual condition compared to the visual-verbal (<em>M</em><sub><em>childVP</em> </sub>= 4.55, <em>M</em><sub><em>childVVP</em> </sub>= 4.45, <em>p</em> = .01), whereas negative emotions were perceived as more negative in the visual-verbal condition compared to the visual (<em>M</em><sub><em>childVVN</em> </sub>= 1.94, <em>M</em><sub><em>childVN</em> </sub>= 2.32, <em>p</em> &lt; .001). Adult participants exhibited the same tendencies: positive emotions were rated higher in valence in the visual (<em>M</em><sub><em>adultVP </em></sub>= 4.31, <em>M</em><sub><em>adultVVP</em> </sub>= 4.10, <em>p</em> &lt; .001), while negative emotions were rated lower in valence in the negative visual-verbal condition (<em>M</em><sub><em>adultVVN</em> </sub>= 2.02, <em>M</em><sub><em>adultVN</em> </sub>= 2.15, <em>p</em> = .02).</p>
<figure class="fig xbox font-sm" id="pone.0329554.g007"><h4 class="obj_head">Fig 7. Children’s and adults’ valence ratings.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12333977_pone.0329554.g007.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9f6f/12333977/5832701bf536/pone.0329554.g007.jpg" loading="lazy" height="477" width="700" alt="Fig 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329554.g007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Mean rating values are for positive and negative expressions across both stimulus versions (visual and visual-verbal). Error bars represent standard error. *p &lt; .05 **p &lt; .01. ***p &lt; .001.</p></figcaption></figure><p>Thus, the valence ratings revealed that children generally perceived positive facial expressions as more positive than adults. Visual stimuli produced stronger valence responses than visual-verbal ones, while faces of older adults received higher positivity ratings (in the visual condition). Furthermore, emotions were perceived as more positively in the visual than in the visual-verbal condition, and more negatively in the visual-verbal condition.</p></section><section id="sec018"><h3 class="pmc_sec_title">Emotion-specific results</h3>
<p>To better understand how specific emotions influenced participants’ valence ratings, we conducted emotion-specific analyses within each age group. For children, we explored which positive emotion (happiness or happy-surprise) yielded higher positive ratings by conducting a paired samples <em>t</em>-test. The results showed that surprise and happiness were rated equally positive (<em>M</em><sub><em>surprise</em> </sub>= 4.52, <em>M</em><sub><em>happy</em> </sub>= 4.48, <em>p</em> = .65). Additionally, we investigated which negative emotion (anger or sadness) was perceived as more negative by the adult sample. A paired samples <em>t</em>-test revealed that sadness was rated significantly lower in valence than anger (<em>M</em><sub><em>sad </em></sub>= 1.82, <em>M</em><sub><em>anger</em> </sub>= 2.34, <em>p</em> &lt; .001).</p>
<p>Thus, while children did not differentiate between the two positive emotions, adults showed a clearer distinction between negative emotions, perceiving sadness as significantly more negative than anger.</p></section><section id="sec019"><h3 class="pmc_sec_title">Observed power</h3>
<p>We performed a post-hoc G-Power analysis (4 measurements, 56 participants) which revealed observed power of 93.1%, indicating ample power for this measure.</p></section><section id="sec020"><h3 class="pmc_sec_title">Perception of stimuli: Uncanny Valley measures</h3>
<p>In addition, we explored how participants from both age groups perceived the characters in terms of the Uncanny Valley (UV) effect. Using the responses of 14 children from the arousal sample and another 14 from the valence sample, we compared them to the responses of the full adult sample (<em><em>N</em> </em>= 28) for the three UV measures: Likeability, Friendliness, and Realism. <a href="#pone.0329554.g008" class="usa-link">Fig 8</a> illustrates the frequency of responses (in percentages). Overall, most children and adults rated the stimuli as likable and friendly. A notable difference between the groups emerged in the Realism scale: while more than half of the children rated the stimuli as low in realism, the majority of the adults considered them highly realistic. We performed three chi-square analyses to determine whether these frequency differences were significant, one for each UV measure.</p>
<figure class="fig xbox font-sm" id="pone.0329554.g008"><h4 class="obj_head">Fig 8. Frequency of children’s and adults’ responses for the UV measures.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12333977_pone.0329554.g008.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9f6f/12333977/d63ce69f6c92/pone.0329554.g008.jpg" loading="lazy" height="153" width="700" alt="Fig 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329554.g008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Left to right: likeability, friendliness and realism. The ratings are of the whole stimulus set across both valence category and stimulus types. The bar graphs show percentages. Error bars represent standard error.</p></figcaption></figure><p>For the Likeability scale, we found no significant difference between the groups, <em>X</em><sup><em>2</em></sup> (2, <em>N</em> = 56) = 1.37, <em>p</em> = .50. Similarly, the Friendliness scale did not yield significant results, <em>X</em><sup><em>2</em></sup> (2, <em>N</em> = 56) = 5.14, <em>p</em> = .07. The only significant difference was revealed for the Realism scale, <em>X</em><sup><em>2</em></sup> (2, <em>N</em> = 56) = 9.16, <em>p</em> = .01, with children rating the stimuli as less realistic compared to the adults.</p></section><section id="sec021"><h3 class="pmc_sec_title">Negativity bias and Uncanny Valley: correlations</h3>
<p>Considering that more than half (61%) of the children perceived the stimuli as low in realism, we investigated whether this is connected to the negativity arousal bias found in their emotional ratings. We correlated children’s rating values for the Realism scale with their ratings for the negative emotions in the arousal condition. No significant correlation was revealed, <em>r</em><sub><em>s</em></sub>(24) = −.008, <em>p</em> = .97, indicating that their higher arousal ratings for negative emotions are not associated with the perceived low character realism. Additionally, we correlated children’s ratings for the Realism scale and their ratings for positive emotions. Again, no significant correlation was found, <em>r</em><sub><em>s</em></sub>(24) = −.004, <em>p</em> = .99, suggesting no link between perceived realism and children’s ratings of positive emotions.</p></section></section><section id="sec022"><h2 class="pmc_sec_title">Discussion</h2>
<p>Uncovering the mechanisms behind age-related changes in emotion perception from childhood to adulthood is essential for understanding the complexities of emotional development. While existing studies have contributed to this field using various emotional stimuli [<a href="#pone.0329554.ref014" class="usa-link" aria-describedby="pone.0329554.ref014">14</a>,<a href="#pone.0329554.ref017" class="usa-link" aria-describedby="pone.0329554.ref017">17</a>,<a href="#pone.0329554.ref018" class="usa-link" aria-describedby="pone.0329554.ref018">18</a>] and different presentation methods [<a href="#pone.0329554.ref042" class="usa-link" aria-describedby="pone.0329554.ref042">42</a>,<a href="#pone.0329554.ref043" class="usa-link" aria-describedby="pone.0329554.ref043">43</a>], there has been a notable lack of implementing naturalistic emotion stimuli.</p>
<p>The aim of this online rating study was to address this gap in the literature by exploring how children and adults perceive valence and arousal with a newly designed database that closely mirrors real-life emotion experiences. Our Meta-MED database features naturalistic emotion characteristics such as four dynamic, low-intensity emotional expressions (happiness, happy-surprise, sadness, anger) presented by faces of different age groups (children, adolescents, young adults, and older adults) in two modalities (pure facial expressions and combined facial-vocal expressions). It is crucial to note that the participants assessed the arousal and valence states of the digital avatars, rather than their subjective emotional reactions during the observation.</p>
<section id="sec023"><h3 class="pmc_sec_title">Arousal ratings of children and adults</h3>
<p>Our findings on arousal showed a strong similarity between children’s and adults’ ratings, as shown by the high correlation mirroring the results of Vesker et al. [<a href="#pone.0329554.ref014" class="usa-link" aria-describedby="pone.0329554.ref014">14</a>]. Despite this, there were significant developmental differences between the two age groups.</p>
<p>First, children exhibited a negativity bias, rating negative expressions as more arousing than adults. This heightened perception of arousal for negative emotions was consistent across both visual and visual-verbal conditions, suggesting a strong negative emotional component regardless of stimulus presentation. This contrasts with previous findings [<a href="#pone.0329554.ref014" class="usa-link" aria-describedby="pone.0329554.ref014">14</a>], which reported a positivity bias in children’s arousal ratings. We suspect that this difference could stem from the dynamic nature of our stimuli. On the one hand, static stimuli showing emotions at their peak could lead to an almost immediate detection of a negative expression, thus potentially resulting in lower arousal ratings. On the other hand, dynamically unfolding expressions from neutral to negative might be perceived as unanticipated, and therefore more arousing upon reaching their peak. Such observations have been made by other researchers regarding static versus dynamic presentation of stimuli [<a href="#pone.0329554.ref070" class="usa-link" aria-describedby="pone.0329554.ref070">70</a>].</p>
<p>Additionally, we observed that anger, not sadness, elicited higher ratings in arousal, which is in line with previous research [<a href="#pone.0329554.ref011" class="usa-link" aria-describedby="pone.0329554.ref011">11</a>]. Based on this, we hypothesize one possible explanation for our findings. As discussed by Vaish [<a href="#pone.0329554.ref024" class="usa-link" aria-describedby="pone.0329554.ref024">24</a>] in their literature review, infants may attend to negative faces (such as anger or fear) more frequently than happy ones, given that these emotions signal potential danger, which is more relevant to the infant. From an evolutionarily perspective, we suspect that children, being less capable of defending themselves, could be more reactive to expressions of anger due to the perceived higher threat value. Interestingly, a similar framework was proposed by Vesker and team [<a href="#pone.0329554.ref014" class="usa-link" aria-describedby="pone.0329554.ref014">14</a>], where they found a positivity bias for arousal from static images. They suggest that children might seek positive information from their surrounding as a means of protection from threats. While their findings revealed a positivity bias, both studies align with the evolutionary mechanisms driving these biases, although the direction may vary based on the stimulus material. Considering that our stimuli, we suspect that the difference in the observed biases in our study and Vesker’s study [<a href="#pone.0329554.ref014" class="usa-link" aria-describedby="pone.0329554.ref014">14</a>] could be attributed to the nature of our stimuli – dynamically unfolding and containing verbal information that might have enhanced the perception of emotions for both age groups due to the additional cues from the voice (e.g., an angry/sad tone).</p>
<p>Finally, we did not find consistent effects regarding arousal elicited by the characters’ age groups. Older faces generally produced higher arousal ratings across all participants, yet this effect was not stable and varied across conditions. Nevertheless, incorporating emotional expressions from various facial ages is crucial for maintaining an ecological approach to studying emotion perception.</p></section><section id="sec024"><h3 class="pmc_sec_title">Valence ratings of children and adults</h3>
<p>One key highlight of the valence findings is the validation of our new stimulus database, demonstrated by the consistent and accurate valence categorization across all participants. This validation is crucial for its future applications, as it emphasizes the reliable construction of the emotional expressions.</p>
<p>Similar to the arousal ratings, the responses from both age groups were nearly identical across all stimuli. However, several significant differences emerged between and within the two age groups. Contrary to their negativity bias in arousal, children exhibited a positivity bias, with higher valence ratings for positive emotions in both stimulus conditions than adults. This aligns with two previous findings [<a href="#pone.0329554.ref014" class="usa-link" aria-describedby="pone.0329554.ref014">14</a>,<a href="#pone.0329554.ref016" class="usa-link" aria-describedby="pone.0329554.ref016">16</a>], where children rated and categorized positive expressions similarly in an experimental design featuring static facial expressions. This suggests a robust overall effect of positive emotions, regardless of the presentation type (visual/static or visual-verbal/bimodal). Regarding the emotions presented, children did not differentiate between surprise and happiness, which is expected considering we constructed both emotions as similar as possible (e.g., surprise ends with a smile) to be considered both positive, resulting in similar valence ratings.</p>
<p>A new discovery in the adult group showed a negativity bias in the visual condition compared to children. This effect was not observed in the visual-verbal condition, possibly because adults do not need additional cues to judge the valence of negative emotions. Interestingly, adults rated sadness as significantly more negative than anger, further aligning with Russell’s model [<a href="#pone.0329554.ref011" class="usa-link" aria-describedby="pone.0329554.ref011">11</a>]. According to it, while anger has high arousal ratings and a rather higher valence score, sadness is a low-arousing emotion with a much lower (i.e., negative) valence score.</p>
<p>The results from both age groups suggest inherent differences in the processing of specific emotions in terms of valence. Since our happy and happy-surprise expressions were constructed similarly (i.e., with a smile), we did not expect significant differences just as our findings support. This would have probably been different had surprise been left ambiguous. However, the distinction in valence between anger and sadness, which we found in adults, was more pronounced, with sadness having a stronger negative perceptual impact. These findings align with the notion that “negative emotions are more richly differentiated than positive emotions” [<a href="#pone.0329554.ref071" class="usa-link" aria-describedby="pone.0329554.ref071">71</a>].</p>
<p>Finally, regarding the age variation of faces, an effect of the older faces was observed. Still, since it was limited to specific conditions and comparisons, drawing definitive conclusions regarding the influence of facial age on emotion perception is challenging.</p></section><section id="sec025"><h3 class="pmc_sec_title">Effect of Metahumans on emotion perception</h3>
<p>Another topic of interest was to determine whether the development and presentation of our stimuli had an impact on emotion perception. To determine this, we compared the two visually distinct character sets and asked our participants to rate their overall impression of the stimuli to measure potential Uncanny Valley effects [<a href="#pone.0329554.ref046" class="usa-link" aria-describedby="pone.0329554.ref046">46</a>].</p>
<p>We found that the two versions were rated similarly, showing that the MetaHuman character design did not affect emotion perception. This contributes to the strength of our findings by displaying the robustness of our stimuli construction. Furthermore, children and adults perceived the digital characters as pleasant and friendly, indicating an overall positive impression of the MetaHumans. The only notable difference observed was in perceived realism, with adults rating the avatars as more realistic than children did. This discrepancy may be related to differences in media exposure and realism familiarity. Adults are generally more accustomed to lifelike digital representations through experiences with video games, films, and other forms of digital media. In contrast, children are typically exposed to more stylized, caricatured animations, which could influence their standard of realism. Supporting this idea, gaming statistics from 2018 [<a href="#pone.0329554.ref072" class="usa-link" aria-describedby="pone.0329554.ref072">72</a>] indicated that the average age of gamers was 34, with only 17% under the age of 18—suggesting that adults are more frequently engaged with realistic digital environments. Additionally, research has shown that adults tend to prefer realistic over stylized characters [<a href="#pone.0329554.ref073" class="usa-link" aria-describedby="pone.0329554.ref073">73</a>], whereas children often favor cartoon-like presenters in educational content [<a href="#pone.0329554.ref074" class="usa-link" aria-describedby="pone.0329554.ref074">74</a>].</p></section><section id="sec026"><h3 class="pmc_sec_title">Future directions</h3>
<p>The findings of the current online study not only validate our newly developed Meta-MED database but also highlight the importance of employing realistic and natural stimuli when examining age-related differences in emotion perception. There is growing consensus among researchers regarding the need for ecologically valid stimuli that closely resemble real-life experiences. Furthermore, drawing conclusions from studies using static stimuli should be taken with a grain of salt, as these may not accurately represent dynamic emotional expressions [<a href="#pone.0329554.ref075" class="usa-link" aria-describedby="pone.0329554.ref075">75</a>–<a href="#pone.0329554.ref077" class="usa-link" aria-describedby="pone.0329554.ref077">77</a>].</p>
<p>Our new database emphasizes the effectiveness of using highly realistic avatars to generate stimuli. These avatars are cost-effective, highly controllable, and offer flexibility in representing various aspects of emotional expression (such as intensity and duration) and identity (including race and gender). Recent advancements in MetaHuman technology, which enable real-time facial tracking, have significantly improved emotional expressiveness and can be accessed with just a smartphone tap. Needless to say, this is not easily replicated with hired actors, who often provide posed expressions that resemble static images, resulting in a similarly unrealistic emotion production.</p>
<p>Furthermore, an increasing number of studies have explored emotion perception using digital avatars, particularly in areas such as developing social and communication skills for autistic individuals [<a href="#pone.0329554.ref078" class="usa-link" aria-describedby="pone.0329554.ref078">78</a>] and examining empathy and facial mimicry [<a href="#pone.0329554.ref079" class="usa-link" aria-describedby="pone.0329554.ref079">79</a>,<a href="#pone.0329554.ref080" class="usa-link" aria-describedby="pone.0329554.ref080">80</a>], as well as comfort level and perceived realism of digital avatars [<a href="#pone.0329554.ref081" class="usa-link" aria-describedby="pone.0329554.ref081">81</a>]. These studies are paving the way for avatars to be used in various settings, including virtual reality (e.g., patient simulation: [<a href="#pone.0329554.ref082" class="usa-link" aria-describedby="pone.0329554.ref082">82</a>,<a href="#pone.0329554.ref083" class="usa-link" aria-describedby="pone.0329554.ref083">83</a>]), educational experiences [<a href="#pone.0329554.ref084" class="usa-link" aria-describedby="pone.0329554.ref084">84</a>,<a href="#pone.0329554.ref085" class="usa-link" aria-describedby="pone.0329554.ref085">85</a>], and interventions to enhance performance in individuals with Attention Deficit Hyperactivity Disorder [<a href="#pone.0329554.ref086" class="usa-link" aria-describedby="pone.0329554.ref086">86</a>–<a href="#pone.0329554.ref088" class="usa-link" aria-describedby="pone.0329554.ref088">88</a>].</p>
<p>In terms of emotional development, the differences observed in the child sample may have practical implications across various domains such as education, parenting, and overall cognitive development. For instance, educators could incorporate more positive reinforcement into their teaching methods to enhance students’ understanding of educational content [<a href="#pone.0329554.ref089" class="usa-link" aria-describedby="pone.0329554.ref089">89</a>]. Interestingly, a case study has demonstrated that both positive and negative reinforcement can lead to improved academic performance and reduced problematic behaviors [<a href="#pone.0329554.ref090" class="usa-link" aria-describedby="pone.0329554.ref090">90</a>]. Additionally, this knowledge can be applied when developing support programs for youth navigating challenging and negative experiences. Such programs could focus on reframing these experiences with a more positive perspective, acknowledging consequences, fostering responsibility, and promoting growth [<a href="#pone.0329554.ref091" class="usa-link" aria-describedby="pone.0329554.ref091">91</a>].</p>
<p>Furthermore, parents and caregivers can benefit from creating a balanced emotional environment in which children learn to better perceive and regulate both their own and others’ emotions. This can be achieved through encouragement, praise, and rewards [<a href="#pone.0329554.ref092" class="usa-link" aria-describedby="pone.0329554.ref092">92</a>].</p>
<p>Lastly, a deeper understanding of children’s emotional development and needs could inform improvements in early childhood education and care practices. By reframing challenging and negative experiences in a positive light and encouraging responsibility, we can better support children’s development and help them understand the role of these experiences in shaping their growth [<a href="#pone.0329554.ref091" class="usa-link" aria-describedby="pone.0329554.ref091">91</a>,<a href="#pone.0329554.ref093" class="usa-link" aria-describedby="pone.0329554.ref093">93</a>].</p></section></section><section id="sec027"><h2 class="pmc_sec_title">Limitations</h2>
<p>While we carefully planned and controlled all aspects of our experiment, there are several limitations. First, as an online study, we had no experimental control over how the stimuli were presented, meaning that the setting might have varied from one participant to another. This applies to the need for parental support, and while parents were instructed not to influence their children’s rating decisions, we cannot be certain they complied. Still, considering that the developmental differences we observed would not have emerged had this been the case, we regard this as an unlikely scenario. Second, for the construction of the stimuli, we chose a balanced presentation of emotional expressions – two positive and two negative – however, we acknowledge that nearly all research so far includes most, if not all, six basic emotions. Third, the sentences used for the visual-verbal condition were constructed to be simplistic; still, it is perhaps uncommon for people to express their feelings in such a straightforward manner in everyday life. Finally, while MetaHumans appear quite realistic, their complexity remains artificial, as reflected in the children’s low realism responses. All these factors could have influenced how natural and real-life-like emotional expressions were perceived to be.</p></section><section id="sec028"><h2 class="pmc_sec_title">Conclusion</h2>
<p>The present online rating study featuring dynamic and multimodal emotion expressions revealed that children maintained their previously found positivity bias in valence even with dynamic stimuli, whereas adults exhibited a negativity bias. Contrary to previous findings, children rated negative expressions as more arousing. Furthermore, the type of presentation matters: multimodal presentation not only elevates arousal ratings but also enhances the valence perception of negative emotions, whereas unimodal presentation strengthens the positivity perceived in positive emotions. No consistent effects were detected regarding the ages of the characters’ faces. Overall, our findings highlight the fundamental importance of using ecologically valid stimuli when exploring developmental differences in emotion perception.</p></section><section id="sec029"><h2 class="pmc_sec_title">Supporting information</h2>
<section class="sm xbox font-sm" id="pone.0329554.s001"><div class="caption p">
<span>S1_Trial example. The video with the specific stimulus is presented with either one of the two scales (for the children sample) or with both scales (for the adult sample).</span><p>(TIF)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12333977/bin/pone.0329554.s001.tif" data-ga-action="click_feat_suppl" class="usa-link">pone.0329554.s001.tif</a><sup> (348KB, tif) </sup>
</div></div></section><section class="sm xbox font-sm" id="pone.0329554.s002"><div class="caption p">
<span>S2_Correlational heatmaps. The figures show the correlations separately for the visual and the visual verbal condition of children’ and adults’ responses for arousal and valence. The legend explains how to read the trials properly.</span><p>(PDF)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12333977/bin/pone.0329554.s002.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0329554.s002.pdf</a><sup> (1.7MB, pdf) </sup>
</div></div></section></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>Data are available from the OSF repository [URL: <a href="https://osf.io/3rd7m/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://osf.io/3rd7m/</a>]</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>This work was supported by the DFG (Deutsche Forschungsgemeinschaft) [grant number SFB/TRR 540 135/3 2014]. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0329554.ref001">
<span class="label">1.</span><cite>Van Kleef GA. The Emerging View of Emotion as Social Information. Soc Personal Psychology Compass. 2010;4(5):331–43. doi: 10.1111/j.1751-9004.2010.00262.x</cite> [<a href="https://doi.org/10.1111/j.1751-9004.2010.00262.x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Soc%20Personal%20Psychology%20Compass&amp;title=The%20Emerging%20View%20of%20Emotion%20as%20Social%20Information&amp;author=GA%20Van%20Kleef&amp;volume=4&amp;issue=5&amp;publication_year=2010&amp;pages=331-43&amp;doi=10.1111/j.1751-9004.2010.00262.x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref002">
<span class="label">2.</span><cite>Gregory AJP, Anderson JF, Gable SL. You don’t know how it feels: Accuracy in emotion perception predicts responsiveness of support. Emotion. 2020;20(3):343–52. doi: 10.1037/emo0000608

</cite> [<a href="https://doi.org/10.1037/emo0000608" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31169372/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Emotion&amp;title=You%20don%E2%80%99t%20know%20how%20it%20feels:%20Accuracy%20in%20emotion%20perception%20predicts%20responsiveness%20of%20support&amp;author=AJP%20Gregory&amp;author=JF%20Anderson&amp;author=SL%20Gable&amp;volume=20&amp;issue=3&amp;publication_year=2020&amp;pages=343-52&amp;pmid=31169372&amp;doi=10.1037/emo0000608&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref003">
<span class="label">3.</span><cite>Cohn MA, Fredrickson BL, Brown SL, Mikels JA, Conway AM. Happiness unpacked: positive emotions increase life satisfaction by building resilience. Emotion. 2009;9(3):361–8. doi: 10.1037/a0015952

</cite> [<a href="https://doi.org/10.1037/a0015952" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3126102/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19485613/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Emotion&amp;title=Happiness%20unpacked:%20positive%20emotions%20increase%20life%20satisfaction%20by%20building%20resilience&amp;author=MA%20Cohn&amp;author=BL%20Fredrickson&amp;author=SL%20Brown&amp;author=JA%20Mikels&amp;author=AM%20Conway&amp;volume=9&amp;issue=3&amp;publication_year=2009&amp;pages=361-8&amp;pmid=19485613&amp;doi=10.1037/a0015952&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref004">
<span class="label">4.</span><cite>Izard C, Fine S, Schultz D, Mostow A, Ackerman B, Youngstrom E. Emotion knowledge as a predictor of social behavior and academic competence in children at risk. Psychol Sci. 2001;12(1):18–23. doi: 10.1111/1467-9280.00304

</cite> [<a href="https://doi.org/10.1111/1467-9280.00304" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11294223/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol%20Sci&amp;title=Emotion%20knowledge%20as%20a%20predictor%20of%20social%20behavior%20and%20academic%20competence%20in%20children%20at%20risk&amp;author=C%20Izard&amp;author=S%20Fine&amp;author=D%20Schultz&amp;author=A%20Mostow&amp;author=B%20Ackerman&amp;volume=12&amp;issue=1&amp;publication_year=2001&amp;pages=18-23&amp;pmid=11294223&amp;doi=10.1111/1467-9280.00304&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref005">
<span class="label">5.</span><cite>Durand K, Gallay M, Seigneuric A, Robichon F, Baudouin J-Y. The development of facial emotion recognition: the role of configural information. J Exp Child Psychol. 2007;97(1):14–27. doi: 10.1016/j.jecp.2006.12.001

</cite> [<a href="https://doi.org/10.1016/j.jecp.2006.12.001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17291524/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Exp%20Child%20Psychol&amp;title=The%20development%20of%20facial%20emotion%20recognition:%20the%20role%20of%20configural%20information&amp;author=K%20Durand&amp;author=M%20Gallay&amp;author=A%20Seigneuric&amp;author=F%20Robichon&amp;author=J-Y%20Baudouin&amp;volume=97&amp;issue=1&amp;publication_year=2007&amp;pages=14-27&amp;pmid=17291524&amp;doi=10.1016/j.jecp.2006.12.001&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref006">
<span class="label">6.</span><cite>Covic A, von Steinbüchel N, Kiese-Himmel C. Emotion Recognition in Kindergarten Children. Folia Phoniatr Logop. 2020;72(4):273–81. doi: 10.1159/000500589

</cite> [<a href="https://doi.org/10.1159/000500589" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31256156/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Folia%20Phoniatr%20Logop&amp;title=Emotion%20Recognition%20in%20Kindergarten%20Children&amp;author=A%20Covic&amp;author=N%20von%20Steinb%C3%BCchel&amp;author=C%20Kiese-Himmel&amp;volume=72&amp;issue=4&amp;publication_year=2020&amp;pages=273-81&amp;pmid=31256156&amp;doi=10.1159/000500589&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref007">
<span class="label">7.</span><cite>Richter D, Dietzel C, Kunzmann U. Age differences in emotion recognition: the task matters. J Gerontol B Psychol Sci Soc Sci. 2011;66(1):48–55. doi: 10.1093/geronb/gbq068

</cite> [<a href="https://doi.org/10.1093/geronb/gbq068" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20847040/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Gerontol%20B%20Psychol%20Sci%20Soc%20Sci&amp;title=Age%20differences%20in%20emotion%20recognition:%20the%20task%20matters&amp;author=D%20Richter&amp;author=C%20Dietzel&amp;author=U%20Kunzmann&amp;volume=66&amp;issue=1&amp;publication_year=2011&amp;pages=48-55&amp;pmid=20847040&amp;doi=10.1093/geronb/gbq068&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref008">
<span class="label">8.</span><cite>Tonks J, Williams WH, Frampton I, Yates P, Slater A. Assessing emotion recognition in 9-15-years olds: preliminary analysis of abilities in reading emotion from faces, voices and eyes. Brain Inj. 2007;21(6):623–9. doi: 10.1080/02699050701426865

</cite> [<a href="https://doi.org/10.1080/02699050701426865" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17577713/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Brain%20Inj&amp;title=Assessing%20emotion%20recognition%20in%209-15-years%20olds:%20preliminary%20analysis%20of%20abilities%20in%20reading%20emotion%20from%20faces,%20voices%20and%20eyes&amp;author=J%20Tonks&amp;author=WH%20Williams&amp;author=I%20Frampton&amp;author=P%20Yates&amp;author=A%20Slater&amp;volume=21&amp;issue=6&amp;publication_year=2007&amp;pages=623-9&amp;pmid=17577713&amp;doi=10.1080/02699050701426865&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref009">
<span class="label">9.</span><cite>Lawrence K, Campbell R, Skuse D. Age, gender, and puberty influence the development of facial emotion recognition. Front Psychol. 2015;6:761. doi: 10.3389/fpsyg.2015.00761

</cite> [<a href="https://doi.org/10.3389/fpsyg.2015.00761" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4468868/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26136697/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Psychol&amp;title=Age,%20gender,%20and%20puberty%20influence%20the%20development%20of%20facial%20emotion%20recognition&amp;author=K%20Lawrence&amp;author=R%20Campbell&amp;author=D%20Skuse&amp;volume=6&amp;publication_year=2015&amp;pages=761&amp;pmid=26136697&amp;doi=10.3389/fpsyg.2015.00761&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref010">
<span class="label">10.</span><cite>Ruffman T, Henry JD, Livingstone V, Phillips LH. A meta-analytic review of emotion recognition and aging: implications for neuropsychological models of aging. Neurosci Biobehav Rev. 2008;32(4):863–81. doi: 10.1016/j.neubiorev.2008.01.001

</cite> [<a href="https://doi.org/10.1016/j.neubiorev.2008.01.001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/18276008/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neurosci%20Biobehav%20Rev&amp;title=A%20meta-analytic%20review%20of%20emotion%20recognition%20and%20aging:%20implications%20for%20neuropsychological%20models%20of%20aging&amp;author=T%20Ruffman&amp;author=JD%20Henry&amp;author=V%20Livingstone&amp;author=LH%20Phillips&amp;volume=32&amp;issue=4&amp;publication_year=2008&amp;pages=863-81&amp;pmid=18276008&amp;doi=10.1016/j.neubiorev.2008.01.001&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref011">
<span class="label">11.</span><cite>Russell JA. A circumplex model of affect. J Personal Soc Psychol. 1980;39:1161–78.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Personal%20Soc%20Psychol&amp;title=A%20circumplex%20model%20of%20affect&amp;author=JA%20Russell&amp;volume=39&amp;publication_year=1980&amp;pages=1161-78&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref012">
<span class="label">12.</span><cite>McManis MH, Bradley MM, Berg WK, Cuthbert BN, Lang PJ. Emotional reactions in children: verbal, physiological, and behavioral responses to affective pictures. Psychophysiology. 2001;38(2):222–31. doi: 10.1111/1469-8986.3820222

</cite> [<a href="https://doi.org/10.1111/1469-8986.3820222" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11347868/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychophysiology&amp;title=Emotional%20reactions%20in%20children:%20verbal,%20physiological,%20and%20behavioral%20responses%20to%20affective%20pictures&amp;author=MH%20McManis&amp;author=MM%20Bradley&amp;author=WK%20Berg&amp;author=BN%20Cuthbert&amp;author=PJ%20Lang&amp;volume=38&amp;issue=2&amp;publication_year=2001&amp;pages=222-31&amp;pmid=11347868&amp;doi=10.1111/1469-8986.3820222&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref013">
<span class="label">13.</span><cite>Russell JA, Bullock M. Multidimensional scaling of emotional facial expressions: Similarity from preschoolers to adults. J Personal Soc Psychol. 1985;48(5):1290–8. doi: 10.1037/0022-3514.48.5.1290</cite> [<a href="https://doi.org/10.1037/0022-3514.48.5.1290" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Personal%20Soc%20Psychol&amp;title=Multidimensional%20scaling%20of%20emotional%20facial%20expressions:%20Similarity%20from%20preschoolers%20to%20adults&amp;author=JA%20Russell&amp;author=M%20Bullock&amp;volume=48&amp;issue=5&amp;publication_year=1985&amp;pages=1290-8&amp;doi=10.1037/0022-3514.48.5.1290&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref014">
<span class="label">14.</span><cite>Vesker M, Bahn D, Degé F, Kauschke C, Schwarzer G. Perceiving arousal and valence in facial expressions: Differences between children and adults. Europ J Dev Psychol. 2018;15(4):411–25.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Europ%20J%20Dev%20Psychol&amp;title=Perceiving%20arousal%20and%20valence%20in%20facial%20expressions:%20Differences%20between%20children%20and%20adults&amp;author=M%20Vesker&amp;author=D%20Bahn&amp;author=F%20Deg%C3%A9&amp;author=C%20Kauschke&amp;author=G%20Schwarzer&amp;volume=15&amp;issue=4&amp;publication_year=2018&amp;pages=411-25&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref015">
<span class="label">15.</span><cite>Bradley MM, Lang PJ. Measuring emotion: the Self-Assessment Manikin and the Semantic Differential. J Behav Ther Exp Psychiatry. 1994;25(1):49–59. doi: 10.1016/0005-7916(94)90063-9

</cite> [<a href="https://doi.org/10.1016/0005-7916(94)90063-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/7962581/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Behav%20Ther%20Exp%20Psychiatry&amp;title=Measuring%20emotion:%20the%20Self-Assessment%20Manikin%20and%20the%20Semantic%20Differential&amp;author=MM%20Bradley&amp;author=PJ%20Lang&amp;volume=25&amp;issue=1&amp;publication_year=1994&amp;pages=49-59&amp;pmid=7962581&amp;doi=10.1016/0005-7916(94)90063-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref016">
<span class="label">16.</span><cite>Vesker M, Bahn D, Degé F, Kauschke C, Schwarzer G. Developmental changes in the categorical processing of positive and negative facial expressions. PLoS One. 2018;13(8):e0201521. doi: 10.1371/journal.pone.0201521

</cite> [<a href="https://doi.org/10.1371/journal.pone.0201521" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6075754/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30075000/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Developmental%20changes%20in%20the%20categorical%20processing%20of%20positive%20and%20negative%20facial%20expressions&amp;author=M%20Vesker&amp;author=D%20Bahn&amp;author=F%20Deg%C3%A9&amp;author=C%20Kauschke&amp;author=G%20Schwarzer&amp;volume=13&amp;issue=8&amp;publication_year=2018&amp;pmid=30075000&amp;doi=10.1371/journal.pone.0201521&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref017">
<span class="label">17.</span><cite>Bahn D, Kauschke C, Vesker M, Schwarzer G. Perception of valence and arousal in German emotion terms: A comparison between 9-year-old children and adults. Appl Psycholinguist. 2017;39(3):463–81. doi: 10.1017/s0142716417000443</cite> [<a href="https://doi.org/10.1017/s0142716417000443" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Appl%20Psycholinguist&amp;title=Perception%20of%20valence%20and%20arousal%20in%20German%20emotion%20terms:%20A%20comparison%20between%209-year-old%20children%20and%20adults&amp;author=D%20Bahn&amp;author=C%20Kauschke&amp;author=M%20Vesker&amp;author=G%20Schwarzer&amp;volume=39&amp;issue=3&amp;publication_year=2017&amp;pages=463-81&amp;doi=10.1017/s0142716417000443&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref018">
<span class="label">18.</span><cite>Preißler L, Keck J, Krüger B, Munzert J, Schwarzer G. Recognition of emotional body language from dyadic and monadic point-light displays in 5-year-old children and adults. J Exp Child Psychol. 2023;235:105713.
</cite> [<a href="https://doi.org/10.1016/j.jecp.2023.105713" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37331307/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Exp%20Child%20Psychol&amp;title=Recognition%20of%20emotional%20body%20language%20from%20dyadic%20and%20monadic%20point-light%20displays%20in%205-year-old%20children%20and%20adults&amp;author=L%20Prei%C3%9Fler&amp;author=J%20Keck&amp;author=B%20Kr%C3%BCger&amp;author=J%20Munzert&amp;author=G%20Schwarzer&amp;volume=235&amp;publication_year=2023&amp;pages=105713&amp;pmid=37331307&amp;doi=10.1016/j.jecp.2023.105713&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref019">
<span class="label">19.</span><cite>Gao X, Maurer D. A happy story: Developmental changes in children’s sensitivity to facial expressions of varying intensities. J Exp Child Psychol. 2010;107(2):67–86. doi: 10.1016/j.jecp.2010.05.003

</cite> [<a href="https://doi.org/10.1016/j.jecp.2010.05.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20542282/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Exp%20Child%20Psychol&amp;title=A%20happy%20story:%20Developmental%20changes%20in%20children%E2%80%99s%20sensitivity%20to%20facial%20expressions%20of%20varying%20intensities&amp;author=X%20Gao&amp;author=D%20Maurer&amp;volume=107&amp;issue=2&amp;publication_year=2010&amp;pages=67-86&amp;pmid=20542282&amp;doi=10.1016/j.jecp.2010.05.003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref020">
<span class="label">20.</span><cite>Leppänen JM, Hietanen JK. Positive facial expressions are recognized faster than negative facial expressions, but why?
Psychol Res. 2004;69(1–2):22–9. doi: 10.1007/s00426-003-0157-2

</cite> [<a href="https://doi.org/10.1007/s00426-003-0157-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/14648224/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol%20Res&amp;title=Positive%20facial%20expressions%20are%20recognized%20faster%20than%20negative%20facial%20expressions,%20but%20why?&amp;author=JM%20Lepp%C3%A4nen&amp;author=JK%20Hietanen&amp;volume=69&amp;publication_year=2004&amp;pages=22-9&amp;pmid=14648224&amp;doi=10.1007/s00426-003-0157-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref021">
<span class="label">21.</span><cite>Picardo R, Baron AS, Anderson AK, Todd RM. Tuning to the Positive: Age-Related Differences in Subjective Perception of Facial Emotion. PLoS One. 2016;11(1):e0145643. doi: 10.1371/journal.pone.0145643

</cite> [<a href="https://doi.org/10.1371/journal.pone.0145643" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4703339/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26734940/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Tuning%20to%20the%20Positive:%20Age-Related%20Differences%20in%20Subjective%20Perception%20of%20Facial%20Emotion&amp;author=R%20Picardo&amp;author=AS%20Baron&amp;author=AK%20Anderson&amp;author=RM%20Todd&amp;volume=11&amp;issue=1&amp;publication_year=2016&amp;pmid=26734940&amp;doi=10.1371/journal.pone.0145643&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref022">
<span class="label">22.</span><cite>LoBue V. More than just another face in the crowd: superior detection of threatening facial expressions in children and adults. Dev Sci. 2009;12(2):305–13. doi: 10.1111/j.1467-7687.2008.00767.x

</cite> [<a href="https://doi.org/10.1111/j.1467-7687.2008.00767.x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19143803/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Dev%20Sci&amp;title=More%20than%20just%20another%20face%20in%20the%20crowd:%20superior%20detection%20of%20threatening%20facial%20expressions%20in%20children%20and%20adults&amp;author=V%20LoBue&amp;volume=12&amp;issue=2&amp;publication_year=2009&amp;pages=305-13&amp;pmid=19143803&amp;doi=10.1111/j.1467-7687.2008.00767.x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref023">
<span class="label">23.</span><cite>Sullivan S, Ruffman T, Hutton SB. Age differences in emotion recognition skills and the visual scanning of emotion faces. J Gerontol B Psychol Sci Soc Sci. 2007;62(1):P53-60. doi: 10.1093/geronb/62.1.p53

</cite> [<a href="https://doi.org/10.1093/geronb/62.1.p53" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17284558/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Gerontol%20B%20Psychol%20Sci%20Soc%20Sci&amp;title=Age%20differences%20in%20emotion%20recognition%20skills%20and%20the%20visual%20scanning%20of%20emotion%20faces&amp;author=S%20Sullivan&amp;author=T%20Ruffman&amp;author=SB%20Hutton&amp;volume=62&amp;issue=1&amp;publication_year=2007&amp;pmid=17284558&amp;doi=10.1093/geronb/62.1.p53&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref024">
<span class="label">24.</span><cite>Vaish A, Grossmann T, Woodward A. Not all emotions are created equal: the negativity bias in social-emotional development. Psychol Bull. 2008;134(3):383–403. doi: 10.1037/0033-2909.134.3.383

</cite> [<a href="https://doi.org/10.1037/0033-2909.134.3.383" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3652533/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/18444702/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol%20Bull&amp;title=Not%20all%20emotions%20are%20created%20equal:%20the%20negativity%20bias%20in%20social-emotional%20development&amp;author=A%20Vaish&amp;author=T%20Grossmann&amp;author=A%20Woodward&amp;volume=134&amp;issue=3&amp;publication_year=2008&amp;pages=383-403&amp;pmid=18444702&amp;doi=10.1037/0033-2909.134.3.383&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref025">
<span class="label">25.</span><cite>Kauschke C, Bahn D, Vesker M, Schwarzer G. The role of emotional valence for the processing of facial and verbal stimuli—positivity or negativity bias?
Front Psychol. 2019;10:1654. doi: 10.3389/fpsyg.2019.01654
</cite> [<a href="https://doi.org/10.3389/fpsyg.2019.01654" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6676801/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31402884/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Psychol&amp;title=The%20role%20of%20emotional%20valence%20for%20the%20processing%20of%20facial%20and%20verbal%20stimuli%E2%80%94positivity%20or%20negativity%20bias?&amp;author=C%20Kauschke&amp;author=D%20Bahn&amp;author=M%20Vesker&amp;author=G%20Schwarzer&amp;volume=10&amp;publication_year=2019&amp;pages=1654&amp;pmid=31402884&amp;doi=10.3389/fpsyg.2019.01654&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref026">
<span class="label">26.</span><cite>Hayes GS, McLennan SN, Henry JD, Phillips LH, Terrett G, Rendell PG, et al. Task characteristics influence facial emotion recognition age-effects: A meta-analytic review. Psychol Aging. 2020;35(2):295–315. doi: 10.1037/pag0000441

</cite> [<a href="https://doi.org/10.1037/pag0000441" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31999152/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol%20Aging&amp;title=Task%20characteristics%20influence%20facial%20emotion%20recognition%20age-effects:%20A%20meta-analytic%20review&amp;author=GS%20Hayes&amp;author=SN%20McLennan&amp;author=JD%20Henry&amp;author=LH%20Phillips&amp;author=G%20Terrett&amp;volume=35&amp;issue=2&amp;publication_year=2020&amp;pages=295-315&amp;pmid=31999152&amp;doi=10.1037/pag0000441&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref027">
<span class="label">27.</span><cite>Riddell C, Nikolić M, Dusseldorp E, Kret ME. Age-related changes in emotion recognition across childhood: A meta-analytic review. Psychol Bull. 2024;150(9):1094–117. doi: 10.1037/bul0000442

</cite> [<a href="https://doi.org/10.1037/bul0000442" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39298231/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol%20Bull&amp;title=Age-related%20changes%20in%20emotion%20recognition%20across%20childhood:%20A%20meta-analytic%20review&amp;author=C%20Riddell&amp;author=M%20Nikoli%C4%87&amp;author=E%20Dusseldorp&amp;author=ME%20Kret&amp;volume=150&amp;issue=9&amp;publication_year=2024&amp;pages=1094-117&amp;pmid=39298231&amp;doi=10.1037/bul0000442&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref028">
<span class="label">28.</span><cite>Stienen BMC, Tanaka A, de Gelder B. Emotional voice and emotional body postures influence each other independently of visual awareness. PLoS One. 2011;6(10):e25517. doi: 10.1371/journal.pone.0025517

</cite> [<a href="https://doi.org/10.1371/journal.pone.0025517" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3189200/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22003396/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Emotional%20voice%20and%20emotional%20body%20postures%20influence%20each%20other%20independently%20of%20visual%20awareness&amp;author=BMC%20Stienen&amp;author=A%20Tanaka&amp;author=B%20de%20Gelder&amp;volume=6&amp;issue=10&amp;publication_year=2011&amp;pmid=22003396&amp;doi=10.1371/journal.pone.0025517&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref029">
<span class="label">29.</span><cite>Klasen M, Chen Y-H, Mathiak K. Multisensory emotions: perception, combination and underlying neural processes. Rev Neurosci. 2012;23(4):381–92. doi: 10.1515/revneuro-2012-0040

</cite> [<a href="https://doi.org/10.1515/revneuro-2012-0040" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23089604/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Rev%20Neurosci&amp;title=Multisensory%20emotions:%20perception,%20combination%20and%20underlying%20neural%20processes&amp;author=M%20Klasen&amp;author=Y-H%20Chen&amp;author=K%20Mathiak&amp;volume=23&amp;issue=4&amp;publication_year=2012&amp;pages=381-92&amp;pmid=23089604&amp;doi=10.1515/revneuro-2012-0040&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref030">
<span class="label">30.</span><cite>Ebner NC, Riediger M, Lindenberger U. FACES--a database of facial expressions in young, middle-aged, and older women and men: development and validation. Behav Res Methods. 2010;42(1):351–62. doi: 10.3758/BRM.42.1.351

</cite> [<a href="https://doi.org/10.3758/BRM.42.1.351" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20160315/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Behav%20Res%20Methods&amp;title=FACES--a%20database%20of%20facial%20expressions%20in%20young,%20middle-aged,%20and%20older%20women%20and%20men:%20development%20and%20validation&amp;author=NC%20Ebner&amp;author=M%20Riediger&amp;author=U%20Lindenberger&amp;volume=42&amp;issue=1&amp;publication_year=2010&amp;pages=351-62&amp;pmid=20160315&amp;doi=10.3758/BRM.42.1.351&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref031">
<span class="label">31.</span><cite>Pantic M, Valstar M, Rademaker R, Maat L. Web-based database for facial expression analysis. In: 2005 IEEE international conference on multimedia and Expo 2005 Jul 6 ( 5 p). IEEE.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=2005%20IEEE%20international%20conference%20on%20multimedia%20and%20Expo%202005%20Jul%206%20(%205%20p)&amp;author=M%20Pantic&amp;author=M%20Valstar&amp;author=R%20Rademaker&amp;author=L%20Maat&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref032">
<span class="label">32.</span><cite>Pell MD. Nonverbal Emotion Priming: Evidence from the ?Facial Affect Decision Task?
J Nonverbal Behav. 2005;29(1):45–73. doi: 10.1007/s10919-004-0889-8</cite> [<a href="https://doi.org/10.1007/s10919-004-0889-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Nonverbal%20Behav&amp;title=Nonverbal%20Emotion%20Priming:%20Evidence%20from%20the%C2%A0?Facial%20Affect%20Decision%20Task?&amp;author=MD%20Pell&amp;volume=29&amp;issue=1&amp;publication_year=2005&amp;pages=45-73&amp;doi=10.1007/s10919-004-0889-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref033">
<span class="label">33.</span><cite>Bänziger T, Mortillaro M, Scherer KR. Introducing the Geneva Multimodal expression corpus for experimental research on emotion perception. Emotion. 2012;12(5):1161–79. doi: 10.1037/a0025827

</cite> [<a href="https://doi.org/10.1037/a0025827" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22081890/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Emotion&amp;title=Introducing%20the%20Geneva%20Multimodal%20expression%20corpus%20for%20experimental%20research%20on%20emotion%20perception&amp;author=T%20B%C3%A4nziger&amp;author=M%20Mortillaro&amp;author=KR%20Scherer&amp;volume=12&amp;issue=5&amp;publication_year=2012&amp;pages=1161-79&amp;pmid=22081890&amp;doi=10.1037/a0025827&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref034">
<span class="label">34.</span><cite>Cao H, Cooper DG, Keutmann MK, Gur RC, Nenkova A, Verma R. CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset. IEEE Trans Affect Comput. 2014;5(4):377–90. doi: 10.1109/TAFFC.2014.2336244

</cite> [<a href="https://doi.org/10.1109/TAFFC.2014.2336244" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4313618/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25653738/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Affect%20Comput&amp;title=CREMA-D:%20Crowd-sourced%20Emotional%20Multimodal%20Actors%20Dataset&amp;author=H%20Cao&amp;author=DG%20Cooper&amp;author=MK%20Keutmann&amp;author=RC%20Gur&amp;author=A%20Nenkova&amp;volume=5&amp;issue=4&amp;publication_year=2014&amp;pages=377-90&amp;pmid=25653738&amp;doi=10.1109/TAFFC.2014.2336244&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref035">
<span class="label">35.</span><cite>Livingstone SR, Russo FA. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS One. 2018;13(5):e0196391. doi: 10.1371/journal.pone.0196391

</cite> [<a href="https://doi.org/10.1371/journal.pone.0196391" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5955500/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29768426/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=The%20Ryerson%20Audio-Visual%20Database%20of%20Emotional%20Speech%20and%20Song%20(RAVDESS):%20A%20dynamic,%20multimodal%20set%20of%20facial%20and%20vocal%20expressions%20in%20North%20American%20English&amp;author=SR%20Livingstone&amp;author=FA%20Russo&amp;volume=13&amp;issue=5&amp;publication_year=2018&amp;pmid=29768426&amp;doi=10.1371/journal.pone.0196391&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref036">
<span class="label">36.</span><cite>Busso C, Deng Z, Yildirim S, Bulut M, Lee CM, Kazemzadeh A, Lee S, Neumann U, Narayanan S. Analysis of emotion recognition using facial expressions, speech and multimodal information. In: Proceedings of the 6th international conference on Multimodal interfaces 2004 Oct 13 (p. 205–211).</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%206th%20international%20conference%20on%20Multimodal%20interfaces%202004%20Oct%2013%20(p.%20205%E2%80%93211)&amp;author=C%20Busso&amp;author=Z%20Deng&amp;author=S%20Yildirim&amp;author=M%20Bulut&amp;author=CM%20Lee&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref037">
<span class="label">37.</span><cite>Richoz A-R, Stacchi L, Schaller P, Lao J, Papinutto M, Ticcinelli V, et al. Recognizing facial expressions of emotion amid noise: A dynamic advantage. J Vis. 2024;24(1):7. doi: 10.1167/jov.24.1.7

</cite> [<a href="https://doi.org/10.1167/jov.24.1.7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10790674/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38197738/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Vis&amp;title=Recognizing%20facial%20expressions%20of%20emotion%20amid%20noise:%20A%20dynamic%20advantage&amp;author=A-R%20Richoz&amp;author=L%20Stacchi&amp;author=P%20Schaller&amp;author=J%20Lao&amp;author=M%20Papinutto&amp;volume=24&amp;issue=1&amp;publication_year=2024&amp;pages=7&amp;pmid=38197738&amp;doi=10.1167/jov.24.1.7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref038">
<span class="label">38.</span><cite>Langner O, Dotsch R, Bijlstra G, Wigboldus DH, Hawk ST, Van Knippenberg AD. Presentation and validation of the Radboud Faces Database. Cogn Emotion. 2010;24(8):1377–88.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Cogn%20Emotion&amp;title=Presentation%20and%20validation%20of%20the%20Radboud%20Faces%20Database&amp;author=O%20Langner&amp;author=R%20Dotsch&amp;author=G%20Bijlstra&amp;author=DH%20Wigboldus&amp;author=ST%20Hawk&amp;volume=24&amp;issue=8&amp;publication_year=2010&amp;pages=1377-88&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref039">
<span class="label">39.</span><cite>Zsido AN, Arato N, Ihasz V, Basler J, Matuz-Budai T, Inhof O, et al. Finding an emotional face revisited: Differences in own-age bias and the happiness superiority effect in children and young adults. Front Psychol. 2021;12:580565.
</cite> [<a href="https://doi.org/10.3389/fpsyg.2021.580565" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8039508/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33854456/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Psychol&amp;title=Finding%20an%20emotional%20face%20revisited:%20Differences%20in%20own-age%20bias%20and%20the%20happiness%20superiority%20effect%20in%20children%20and%20young%20adults&amp;author=AN%20Zsido&amp;author=N%20Arato&amp;author=V%20Ihasz&amp;author=J%20Basler&amp;author=T%20Matuz-Budai&amp;volume=12&amp;publication_year=2021&amp;pages=580565&amp;pmid=33854456&amp;doi=10.3389/fpsyg.2021.580565&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref040">
<span class="label">40.</span><cite>Khan RA, Crenn A, Meyer A, Bouakaz S. A novel database of children’s spontaneous facial expressions (LIRIS-CSE). Image Vis Comput. 2019;83.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Image%20Vis%20Comput&amp;title=A%20novel%20database%20of%20children%E2%80%99s%20spontaneous%20facial%20expressions%20(LIRIS-CSE)&amp;author=RA%20Khan&amp;author=A%20Crenn&amp;author=A%20Meyer&amp;author=S%20Bouakaz&amp;volume=83&amp;publication_year=2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref041">
<span class="label">41.</span><cite>Negrão JG, Osorio AA, Siciliano RF, Lederman VR, Kozasa EH, D’Antino ME, et al. The child emotion facial expression set: a database for emotion recognition in children. Front Psychol. 2021;12:666245. doi: 10.3389/fpsyg.2021.666245
</cite> [<a href="https://doi.org/10.3389/fpsyg.2021.666245" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8116652/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33995223/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Psychol&amp;title=The%20child%20emotion%20facial%20expression%20set:%20a%20database%20for%20emotion%20recognition%20in%20children&amp;author=JG%20Negr%C3%A3o&amp;author=AA%20Osorio&amp;author=RF%20Siciliano&amp;author=VR%20Lederman&amp;author=EH%20Kozasa&amp;volume=12&amp;publication_year=2021&amp;pages=666245&amp;pmid=33995223&amp;doi=10.3389/fpsyg.2021.666245&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref042">
<span class="label">42.</span><cite>Nelson NL, Russell JA. Preschoolers’ use of dynamic facial, bodily, and vocal cues to emotion. J Exp Child Psychol. 2011;110(1):52–61.
</cite> [<a href="https://doi.org/10.1016/j.jecp.2011.03.014" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21524423/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Exp%20Child%20Psychol&amp;title=Preschoolers%E2%80%99%20use%20of%20dynamic%20facial,%20bodily,%20and%20vocal%20cues%20to%20emotion&amp;author=NL%20Nelson&amp;author=JA%20Russell&amp;volume=110&amp;issue=1&amp;publication_year=2011&amp;pages=52-61&amp;pmid=21524423&amp;doi=10.1016/j.jecp.2011.03.014&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref043">
<span class="label">43.</span><cite>De Silva LC, Miyasato T, Nakatsu R. Use of multimodal information in facial emotion recognition. IEICE TRANSACTIONS Info Syst. 1998;81(1):105–14.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEICE%20TRANSACTIONS%20Info%20Syst&amp;title=Use%20of%20multimodal%20information%20in%20facial%20emotion%20recognition&amp;author=LC%20De%20Silva&amp;author=T%20Miyasato&amp;author=R%20Nakatsu&amp;volume=81&amp;issue=1&amp;publication_year=1998&amp;pages=105-14&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref044">
<span class="label">44.</span><cite>Van den Stock J, Righart R, de Gelder B. Body expressions influence recognition of emotions in the face and voice. Emotion. 2007;7(3):487–94. doi: 10.1037/1528-3542.7.3.487

</cite> [<a href="https://doi.org/10.1037/1528-3542.7.3.487" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17683205/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Emotion&amp;title=Body%20expressions%20influence%20recognition%20of%20emotions%20in%20the%20face%20and%20voice&amp;author=J%20Van%20den%20Stock&amp;author=R%20Righart&amp;author=B%20de%20Gelder&amp;volume=7&amp;issue=3&amp;publication_year=2007&amp;pages=487-94&amp;pmid=17683205&amp;doi=10.1037/1528-3542.7.3.487&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref045">
<span class="label">45.</span><cite>Epic Games. Unreal Engine [software]. 2019. Available from: <a href="https://www.unrealengine.com" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.unrealengine.com</a></cite>
</li>
<li id="pone.0329554.ref046">
<span class="label">46.</span><cite>Mori M. The uncanny valley: the original essay by Masahiro Mori. IEEE Spectrum. 1970;6.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Spectrum&amp;title=The%20uncanny%20valley:%20the%20original%20essay%20by%20Masahiro%20Mori&amp;author=M%20Mori&amp;volume=6&amp;publication_year=1970&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref047">
<span class="label">47.</span><cite>Tinwell A, Grimshaw M, Nabi DA, Williams A. Facial expression of emotion and perception of the uncanny valley in virtual characters. Comput Human Behav. 2011;27(2):741–9.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Human%20Behav&amp;title=Facial%20expression%20of%20emotion%20and%20perception%20of%20the%20uncanny%20valley%20in%20virtual%20characters&amp;author=A%20Tinwell&amp;author=M%20Grimshaw&amp;author=DA%20Nabi&amp;author=A%20Williams&amp;volume=27&amp;issue=2&amp;publication_year=2011&amp;pages=741-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref048">
<span class="label">48.</span><cite>Amadou N, Haque KI, Yumak Z. Effect of appearance and animation realism on the perception of emotionally expressive virtual humans. In: Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents 2023 Sep 19 (p. 1–8).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%2023rd%20ACM%20International%20Conference%20on%20Intelligent%20Virtual%20Agents%202023%20Sep%2019%20(p.%201%E2%80%938)&amp;title=Effect%20of%20appearance%20and%20animation%20realism%20on%20the%20perception%20of%20emotionally%20expressive%20virtual%20humans.&amp;author=N%20Amadou&amp;author=KI%20Haque&amp;author=Z%20Yumak&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref049">
<span class="label">49.</span><cite>Higgins D, Egan D, Fribourg R, Cowan B, McDonnell R. Ascending from the valley: Can state-of-the-art photorealism avoid the uncanny?. In: ACM Symposium on Applied Perception 2021 2021 Sep 16 (p. 1–5).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Symposium%20on%20Applied%20Perception%202021%202021%20Sep%2016%20(p.%201%E2%80%935)&amp;title=Ascending%20from%20the%20valley:%20Can%20state-of-the-art%20photorealism%20avoid%20the%20uncanny?.&amp;author=D%20Higgins&amp;author=D%20Egan&amp;author=R%20Fribourg&amp;author=B%20Cowan&amp;author=R%20McDonnell&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref050">
<span class="label">50.</span><cite>Saquinaula A, Juarez A, Geigel J, Bailey R, Alm CO. Emotional Empathy and Facial Mimicry of Avatar Faces. In: 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW) 2022 Mar 12 (p. 770–771). IEEE.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=2022%20IEEE%20Conference%20on%20Virtual%20Reality%20and%203D%20User%20Interfaces%20Abstracts%20and%20Workshops%20(VRW)%202022%20Mar%2012%20(p.%20770%E2%80%93771)&amp;author=A%20Saquinaula&amp;author=A%20Juarez&amp;author=J%20Geigel&amp;author=R%20Bailey&amp;author=CO%20Alm&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref051">
<span class="label">51.</span><cite>Kätsyri J, Klucharev V, Frydrych M, Sams M. Identification of synthetic and natural emotional facial expressions. In: AVSP 2003-International Conference on Audio-Visual Speech Processing. 2003.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=AVSP%202003-International%20Conference%20on%20Audio-Visual%20Speech%20Processing&amp;author=J%20K%C3%A4tsyri&amp;author=V%20Klucharev&amp;author=M%20Frydrych&amp;author=M%20Sams&amp;publication_year=2003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref052">
<span class="label">52.</span><cite>Koschate M, Potter R, Bremner P, Levine M. Overcoming the uncanny valley: Displays of emotions reduce the uncanniness of humanlike robots. In: 2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2016 Mar 7 (p. 359–366). IEEE.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=2016%2011th%20ACM/IEEE%20International%20Conference%20on%20Human-Robot%20Interaction%20(HRI)%202016%20Mar%207%20(p.%20359%E2%80%93366)&amp;author=M%20Koschate&amp;author=R%20Potter&amp;author=P%20Bremner&amp;author=M%20Levine&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref053">
<span class="label">53.</span><cite>Krumhuber E, Manstead ASR, Cosker D, Marshall D, Rosin PL. Effects of Dynamic Attributes of Smiles in Human and Synthetic Faces: A Simulated Job Interview Setting. J Nonverbal Behav. 2008;33(1):1–15. doi: 10.1007/s10919-008-0056-8</cite> [<a href="https://doi.org/10.1007/s10919-008-0056-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Nonverbal%20Behav&amp;title=Effects%20of%20Dynamic%20Attributes%20of%20Smiles%20in%20Human%20and%20Synthetic%20Faces:%20A%20Simulated%20Job%20Interview%20Setting&amp;author=E%20Krumhuber&amp;author=ASR%20Manstead&amp;author=D%20Cosker&amp;author=D%20Marshall&amp;author=PL%20Rosin&amp;volume=33&amp;issue=1&amp;publication_year=2008&amp;pages=1-15&amp;doi=10.1007/s10919-008-0056-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref054">
<span class="label">54.</span><cite>Erdfelder E, Faul F, Buchner A. GPOWER: A general power analysis program. Behav Res Method Instrument Comput. 1996;28(1):1–11. doi: 10.3758/bf03203630</cite> [<a href="https://doi.org/10.3758/bf03203630" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Behav%20Res%20Method%20Instrument%20Comput&amp;title=GPOWER:%20A%20general%20power%20analysis%20program&amp;author=E%20Erdfelder&amp;author=F%20Faul&amp;author=A%20Buchner&amp;volume=28&amp;issue=1&amp;publication_year=1996&amp;pages=1-11&amp;doi=10.3758/bf03203630&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref055">
<span class="label">55.</span><cite>MakeHuman Community. MakeHuman [Internet]. 2016. Available from: <a href="https://www.makehumancommunity.org/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.makehumancommunity.org/</a>.</cite>
</li>
<li id="pone.0329554.ref056">
<span class="label">56.</span><cite>Ekman P, Friesen WV. Facial action coding system. Environment Psychol Nonverb Behav. 1978.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Environment%20Psychol%20Nonverb%20Behav&amp;title=Facial%20action%20coding%20system&amp;author=P%20Ekman&amp;author=WV%20Friesen&amp;publication_year=1978&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref057">
<span class="label">57.</span><cite>van der Schalk J, Hawk ST, Fischer AH, Doosje B. Moving faces, looking places: validation of the Amsterdam Dynamic Facial Expression Set (ADFES). Emotion. 2011;11(4):907–20. doi: 10.1037/a0023853

</cite> [<a href="https://doi.org/10.1037/a0023853" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21859206/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Emotion&amp;title=Moving%20faces,%20looking%20places:%20validation%20of%20the%20Amsterdam%20Dynamic%20Facial%20Expression%20Set%20(ADFES)&amp;author=J%20van%20der%20Schalk&amp;author=ST%20Hawk&amp;author=AH%20Fischer&amp;author=B%20Doosje&amp;volume=11&amp;issue=4&amp;publication_year=2011&amp;pages=907-20&amp;pmid=21859206&amp;doi=10.1037/a0023853&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref058">
<span class="label">58.</span><cite>Ekman P. Expression and the nature of emotion. Approaches to emotion. 2014, p. 319–43.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Approaches%20to%20emotion&amp;author=P%20Ekman&amp;publication_year=2014&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref059">
<span class="label">59.</span><cite>Boloorizadeh P, Tojari F. Facial expression recognition: Age, gender and exposure duration impact. Proced-Soc Behav Sci. 2013;84:1369–75.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proced-Soc%20Behav%20Sci&amp;title=Facial%20expression%20recognition:%20Age,%20gender%20and%20exposure%20duration%20impact&amp;author=P%20Boloorizadeh&amp;author=F%20Tojari&amp;volume=84&amp;publication_year=2013&amp;pages=1369-75&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref060">
<span class="label">60.</span><cite>Hoffmann H, Kessler H, Eppel T, Rukavina S, Traue HC. Expression intensity, gender and facial emotion recognition: Women recognize only subtle facial emotions better than men. Acta Psychol (Amst). 2010;135(3):278–83. doi: 10.1016/j.actpsy.2010.07.012

</cite> [<a href="https://doi.org/10.1016/j.actpsy.2010.07.012" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20728864/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Acta%20Psychol%20(Amst)&amp;title=Expression%20intensity,%20gender%20and%20facial%20emotion%20recognition:%20Women%20recognize%20only%20subtle%20facial%20emotions%20better%20than%20men&amp;author=H%20Hoffmann&amp;author=H%20Kessler&amp;author=T%20Eppel&amp;author=S%20Rukavina&amp;author=HC%20Traue&amp;volume=135&amp;issue=3&amp;publication_year=2010&amp;pages=278-83&amp;pmid=20728864&amp;doi=10.1016/j.actpsy.2010.07.012&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref061">
<span class="label">61.</span><cite>Calvo MG, Avero P, Fernández-Martín A, Recio G. Recognition thresholds for static and dynamic emotional faces. Emotion. 2016;16(8):1186–200. doi: 10.1037/emo0000192

</cite> [<a href="https://doi.org/10.1037/emo0000192" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27359222/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Emotion&amp;title=Recognition%20thresholds%20for%20static%20and%20dynamic%20emotional%20faces&amp;author=MG%20Calvo&amp;author=P%20Avero&amp;author=A%20Fern%C3%A1ndez-Mart%C3%ADn&amp;author=G%20Recio&amp;volume=16&amp;issue=8&amp;publication_year=2016&amp;pages=1186-200&amp;pmid=27359222&amp;doi=10.1037/emo0000192&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref062">
<span class="label">62.</span><cite>Matsumoto D, Hwang HC. Judgments of subtle facial expressions of emotion. Emotion. 2014;14(2):349–57. doi: 10.1037/a0035237

</cite> [<a href="https://doi.org/10.1037/a0035237" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24708508/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Emotion&amp;title=Judgments%20of%20subtle%20facial%20expressions%20of%20emotion&amp;author=D%20Matsumoto&amp;author=HC%20Hwang&amp;volume=14&amp;issue=2&amp;publication_year=2014&amp;pages=349-57&amp;pmid=24708508&amp;doi=10.1037/a0035237&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref063">
<span class="label">63.</span><cite>Leiner DJ. SoSci Survey (Version 3.4.10) [Computer software]. 2023. Available at: <a href="https://www.soscisurvey.de" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.soscisurvey.de</a></cite>
</li>
<li id="pone.0329554.ref064">
<span class="label">64.</span><cite>Bartneck C, Kulić D, Croft E, Zoghbi S. Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots. Int J Soc Robot. 2009;1:71–81.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Soc%20Robot&amp;title=Measurement%20instruments%20for%20the%20anthropomorphism,%20animacy,%20likeability,%20perceived%20intelligence,%20and%20perceived%20safety%20of%20robots&amp;author=C%20Bartneck&amp;author=D%20Kuli%C4%87&amp;author=E%20Croft&amp;author=S%20Zoghbi&amp;volume=1&amp;publication_year=2009&amp;pages=71-81&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref065">
<span class="label">65.</span><cite>Weiss A, Bartneck C. Meta analysis of the usage of the godspeed questionnaire series. In: 2015 24th IEEE international symposium on robot and human interactive communication (RO-MAN) 2015 Aug 31 (p. 381–388). IEEE.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=2015%2024th%20IEEE%20international%20symposium%20on%20robot%20and%20human%20interactive%20communication%20(RO-MAN)%202015%20Aug%2031%20(p.%20381%E2%80%93388)&amp;author=A%20Weiss&amp;author=C%20Bartneck&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref066">
<span class="label">66.</span><cite>Bartneck C. Godspeed questionnaire series: Translations and usage. In: International handbook of behavioral health assessment 2023 Feb 2 (p. 1–35). Cham: Springer International Publishing.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=International%20handbook%20of%20behavioral%20health%20assessment%202023%20Feb%202%20(p.%201%E2%80%9335)&amp;author=C%20Bartneck&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref067">
<span class="label">67.</span><cite>Szczepanowski R, Niemiec T, Cichoń E, Arent K, Florkowski M, Sobecki J. Factor analysis of the Polish version of Godspeed questionnaire (GQS). Journal of Automation Mobile Robotics and Intelligent Systems. 2022;16(2).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Automation%20Mobile%20Robotics%20and%20Intelligent%20Systems&amp;title=Factor%20analysis%20of%20the%20Polish%20version%20of%20Godspeed%20questionnaire%20(GQS)&amp;author=R%20Szczepanowski&amp;author=T%20Niemiec&amp;author=E%20Cicho%C5%84&amp;author=K%20Arent&amp;author=M%20Florkowski&amp;volume=16&amp;issue=2&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref068">
<span class="label">68.</span><cite>Onnasch L, Hildebrandt CL. Impact of Anthropomorphic Robot Design on Trust and Attention in Industrial Human-Robot Interaction. J Hum-Robot Interact. 2021;11(1):1–24. doi: 10.1145/3472224</cite> [<a href="https://doi.org/10.1145/3472224" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Hum-Robot%20Interact&amp;title=Impact%20of%20Anthropomorphic%20Robot%20Design%20on%20Trust%20and%20Attention%20in%20Industrial%20Human-Robot%20Interaction&amp;author=L%20Onnasch&amp;author=CL%20Hildebrandt&amp;volume=11&amp;issue=1&amp;publication_year=2021&amp;pages=1-24&amp;doi=10.1145/3472224&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref069">
<span class="label">69.</span><cite>IBM Corp. IBM SPSS Statistics for Windows, Version 28.0. Armonk, NY: IBM Corp; 2021.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=IBM%20SPSS%20Statistics%20for%20Windows,%20Version%2028.0&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref070">
<span class="label">70.</span><cite>Biele C, Grabowska A. Sex differences in perception of emotion intensity in dynamic and static facial expressions. Exp Brain Res. 2006;171(1):1–6. doi: 10.1007/s00221-005-0254-0

</cite> [<a href="https://doi.org/10.1007/s00221-005-0254-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16628369/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Exp%20Brain%20Res&amp;title=Sex%20differences%20in%20perception%20of%20emotion%20intensity%20in%20dynamic%20and%20static%20facial%20expressions&amp;author=C%20Biele&amp;author=A%20Grabowska&amp;volume=171&amp;issue=1&amp;publication_year=2006&amp;pages=1-6&amp;pmid=16628369&amp;doi=10.1007/s00221-005-0254-0&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref071">
<span class="label">71.</span><cite>Green OH. The emotions: A philosophical theory. Springer Science &amp; Business Media; 2012.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20emotions:%20A%20philosophical%20theory&amp;author=OH%20Green&amp;publication_year=2012&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref072">
<span class="label">72.</span><cite>Facts E. Essential facts about the computer and video game industry [Internet]. 2018</cite>
</li>
<li id="pone.0329554.ref073">
<span class="label">73.</span><cite>Schwind V, Henze N. Gender-and age-related differences in designing the characteristics of stereotypical virtual faces. In: Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play 2018 Oct 23 (p. 463–475).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%202018%20Annual%20Symposium%20on%20Computer-Human%20Interaction%20in%20Play%202018%20Oct%2023%20(p.%20463%E2%80%93475)&amp;title=Gender-and%20age-related%20differences%20in%20designing%20the%20characteristics%20of%20stereotypical%20virtual%20faces.&amp;author=V%20Schwind&amp;author=N%20Henze&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref074">
<span class="label">74.</span><cite>Snyder MN, Mares ML. Preschoolers’ choices of television characters as sources of information: Effects of character type, format, and topic domain. J Experimen Child Psychol. 2021;:105034.</cite> [<a href="https://doi.org/10.1016/j.jecp.2020.105034" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33227589/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Experimen%20Child%20Psychol&amp;title=Preschoolers%E2%80%99%20choices%20of%20television%20characters%20as%20sources%20of%20information:%20Effects%20of%20character%20type,%20format,%20and%20topic%20domain&amp;author=MN%20Snyder&amp;author=ML%20Mares&amp;publication_year=2021&amp;pages=105034&amp;pmid=33227589&amp;doi=10.1016/j.jecp.2020.105034&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref075">
<span class="label">75.</span><cite>Krumhuber EG, Skora LI, Hill HCH, Lander K. The role of facial movements in emotion recognition. Nat Rev Psychol. 2023;2(5):283–96. doi: 10.1038/s44159-023-00172-1</cite> [<a href="https://doi.org/10.1038/s44159-023-00172-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat%20Rev%20Psychol&amp;title=The%20role%20of%20facial%20movements%20in%20emotion%20recognition&amp;author=EG%20Krumhuber&amp;author=LI%20Skora&amp;author=HCH%20Hill&amp;author=K%20Lander&amp;volume=2&amp;issue=5&amp;publication_year=2023&amp;pages=283-96&amp;doi=10.1038/s44159-023-00172-1&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref076">
<span class="label">76.</span><cite>Trautmann SA, Fehr T, Herrmann M. Emotions in motion: dynamic compared to static facial expressions of disgust and happiness reveal more widespread emotion-specific activations. Brain Res. 2009;1284:100–15.
</cite> [<a href="https://doi.org/10.1016/j.brainres.2009.05.075" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19501062/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Brain%20Res&amp;title=Emotions%20in%20motion:%20dynamic%20compared%20to%20static%20facial%20expressions%20of%20disgust%20and%20happiness%20reveal%20more%20widespread%20emotion-specific%20activations&amp;author=SA%20Trautmann&amp;author=T%20Fehr&amp;author=M%20Herrmann&amp;volume=1284&amp;publication_year=2009&amp;pages=100-15&amp;pmid=19501062&amp;doi=10.1016/j.brainres.2009.05.075&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref077">
<span class="label">77.</span><cite>Richoz A-R, Lao J, Pascalis O, Caldara R. Tracking the recognition of static and dynamic facial expressions of emotion across the life span. J Vis. 2018;18(9):5. doi: 10.1167/18.9.5

</cite> [<a href="https://doi.org/10.1167/18.9.5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30208425/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Vis&amp;title=Tracking%20the%20recognition%20of%20static%20and%20dynamic%20facial%20expressions%20of%20emotion%20across%20the%20life%20span&amp;author=A-R%20Richoz&amp;author=J%20Lao&amp;author=O%20Pascalis&amp;author=R%20Caldara&amp;volume=18&amp;issue=9&amp;publication_year=2018&amp;pages=5&amp;pmid=30208425&amp;doi=10.1167/18.9.5&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref078">
<span class="label">78.</span><cite>Haddick S, Brown DJ, Connor B, Lewis J, Bates M, Schofield S. Metahumans: Using facial action coding in games to develop social and communication skills for people with autism. In: International Conference on Human-Computer Interaction 2022 Jun 16 (p. 343–355). Cham: Springer International Publishing.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=International%20Conference%20on%20Human-Computer%20Interaction%202022%20Jun%2016%20(p.%20343%E2%80%93355).%20Cham:%20Springer%20International%20Publishing&amp;title=Metahumans:%20Using%20facial%20action%20coding%20in%20games%20to%20develop%20social%20and%20communication%20skills%20for%20people%20with%20autism&amp;author=S%20Haddick&amp;author=DJ%20Brown&amp;author=B%20Connor&amp;author=J%20Lewis&amp;author=M%20Bates&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref079">
<span class="label">79.</span><cite>Higgins D, Zhan Y, Cowan BR, McDonnell R. Investigating the effect of visual realism on empathic responses to emotionally expressive virtual humans. In: ACM Symposium on applied perception 2023 2023 Aug 5 (p. 1–7).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Symposium%20on%20applied%20perception%202023%202023%20Aug%205%20(p.%201%E2%80%937)&amp;title=Investigating%20the%20effect%20of%20visual%20realism%20on%20empathic%20responses%20to%20emotionally%20expressive%20virtual%20humans.&amp;author=D%20Higgins&amp;author=Y%20Zhan&amp;author=BR%20Cowan&amp;author=R%20McDonnell&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref080">
<span class="label">80.</span><cite>Saquinaula A, Juarez A, Geigel J, Bailey R, Alm CO. Emotional empathy and facial mimicry of avatar faces. In: 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW) 2022 Mar 12 (p. 770–771). IEEE.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=2022%20IEEE%20Conference%20on%20Virtual%20Reality%20and%203D%20User%20Interfaces%20Abstracts%20and%20Workshops%20(VRW)%202022%20Mar%2012%20(p.%20770%E2%80%93771).%20IEEE&amp;title=Emotional%20empathy%20and%20facial%20mimicry%20of%20avatar%20faces&amp;author=A%20Saquinaula&amp;author=A%20Juarez&amp;author=J%20Geigel&amp;author=R%20Bailey&amp;author=CO%20Alm&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref081">
<span class="label">81.</span><cite>Amadou N, Haque KI, Yumak Z. Effect of appearance and animation realism on the perception of emotionally expressive virtual humans. In: Proceedings of the 23rd ACM international conference on intelligent virtual agents. 2023. 
Sep
19, p. 1–8.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%2023rd%20ACM%20international%20conference%20on%20intelligent%20virtual%20agents&amp;title=Effect%20of%20appearance%20and%20animation%20realism%20on%20the%20perception%20of%20emotionally%20expressive%20virtual%20humans&amp;author=N%20Amadou&amp;author=KI%20Haque&amp;author=Z%20Yumak&amp;publication_year=2023&amp;pages=1-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref082">
<span class="label">82.</span><cite>Halan S, Sia I, Crary M, Lok B. Exploring the effects of healthcare students creating virtual patients for empathy training. InIntelligent Virtual Agents: 15th International Conference, IVA 2015, Delft, The Netherlands, August 26-28, 2015, Proceedings 15 2015 (pp. 239–249). Springer International Publishing.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=InIntelligent%20Virtual%20Agents:%2015th%20International%20Conference,%20IVA%202015,%20Delft,%20The%20Netherlands,%20August%2026-28,%202015,%20Proceedings%2015%202015%20(pp.%20239%E2%80%93249)&amp;author=S%20Halan&amp;author=I%20Sia&amp;author=M%20Crary&amp;author=B%20Lok&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref083">
<span class="label">83.</span><cite>Kleinsmith A, Rivera-Gutierrez D, Finney G, Cendan J, Lok B. Understanding empathy training with virtual patients. Comput Human Behav. 2015;52:151–8.
</cite> [<a href="https://doi.org/10.1016/j.chb.2015.05.033" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4493762/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26166942/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Human%20Behav&amp;title=Understanding%20empathy%20training%20with%20virtual%20patients&amp;author=A%20Kleinsmith&amp;author=D%20Rivera-Gutierrez&amp;author=G%20Finney&amp;author=J%20Cendan&amp;author=B%20Lok&amp;volume=52&amp;publication_year=2015&amp;pages=151-8&amp;pmid=26166942&amp;doi=10.1016/j.chb.2015.05.033&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref084">
<span class="label">84.</span><cite>Falloon G. Using avatars and virtual environments in learning: What do they have to offer?
Brit J Educ Technol. 2010;41(1):108–22.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Brit%20J%20Educ%20Technol&amp;title=Using%20avatars%20and%20virtual%20environments%20in%20learning:%20What%20do%20they%20have%20to%20offer?&amp;author=G%20Falloon&amp;volume=41&amp;issue=1&amp;publication_year=2010&amp;pages=108-22&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref085">
<span class="label">85.</span><cite>Zhang R, Wu Q. Impact of using virtual avatars in educational videos on user experience. Sci Rep. 2024;14(1):6592. doi: 10.1038/s41598-024-56716-9

</cite> [<a href="https://doi.org/10.1038/s41598-024-56716-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10951246/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38503826/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Rep&amp;title=Impact%20of%20using%20virtual%20avatars%20in%20educational%20videos%20on%20user%20experience&amp;author=R%20Zhang&amp;author=Q%20Wu&amp;volume=14&amp;issue=1&amp;publication_year=2024&amp;pages=6592&amp;pmid=38503826&amp;doi=10.1038/s41598-024-56716-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref086">
<span class="label">86.</span><cite>Doulou A, Pergantis P, Drigas A, Skianis C. Managing ADHD Symptoms in Children Through the Use of Various Technology-Driven Serious Games: A Systematic Review. MTI. 2025;9(1):8. doi: 10.3390/mti9010008</cite> [<a href="https://doi.org/10.3390/mti9010008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=MTI&amp;title=Managing%20ADHD%20Symptoms%20in%20Children%20Through%20the%20Use%20of%20Various%20Technology-Driven%20Serious%20Games:%20A%20Systematic%20Review&amp;author=A%20Doulou&amp;author=P%20Pergantis&amp;author=A%20Drigas&amp;author=C%20Skianis&amp;volume=9&amp;issue=1&amp;publication_year=2025&amp;pages=8&amp;doi=10.3390/mti9010008&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref087">
<span class="label">87.</span><cite>Fabio RA, Caprì T, Iannizzotto G, Nucita A, Mohammadhasani N. Interactive avatar boosts the performances of children with attention deficit hyperactivity disorder in dynamic measures of intelligence. Cyberpsychol Behav Soc Network. 2019;22(9):588–96.</cite> [<a href="https://doi.org/10.1089/cyber.2018.0711" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31441667/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cyberpsychol%20Behav%20Soc%20Network&amp;title=Interactive%20avatar%20boosts%20the%20performances%20of%20children%20with%20attention%20deficit%20hyperactivity%20disorder%20in%20dynamic%20measures%20of%20intelligence&amp;author=RA%20Fabio&amp;author=T%20Capr%C3%AC&amp;author=G%20Iannizzotto&amp;author=A%20Nucita&amp;author=N%20Mohammadhasani&amp;volume=22&amp;issue=9&amp;publication_year=2019&amp;pages=588-96&amp;pmid=31441667&amp;doi=10.1089/cyber.2018.0711&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref088">
<span class="label">88.</span><cite>Rhodes JD. Autonomic Responses During Animated Avatar Video Modeling Instruction of Social Emotional Learning to Students With ADHD: A Mixed Methods Study (Master’s thesis, Brigham Young University). </cite> [<a href="https://scholar.google.com/scholar_lookup?Rhodes%20JD.%20Autonomic%20Responses%20During%20Animated%20Avatar%20Video%20Modeling%20Instruction%20of%20Social%20Emotional%20Learning%20to%20Students%20With%20ADHD:%20A%20Mixed%20Methods%20Study%20(Master%E2%80%99s%20thesis,%20Brigham%20Young%20University)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref089">
<span class="label">89.</span><cite>Alam ZK, Alay A. Effect of positive reinforcement on student academic performance. North Am Acad Res. 2018;1(1).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=North%20Am%20Acad%20Res&amp;title=Effect%20of%20positive%20reinforcement%20on%20student%20academic%20performance&amp;author=ZK%20Alam&amp;author=A%20Alay&amp;volume=1&amp;issue=1&amp;publication_year=2018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref090">
<span class="label">90.</span><cite>Schieltz KM, Wacker DP, Suess AN, Graber JE, Lustig NH, Detrick J. Evaluating the effects of positive reinforcement, instructional strategies, and negative reinforcement on problem behavior and academic performance: an experimental analysis. J Dev Phys Disabil. 2020;32:339–63.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Dev%20Phys%20Disabil&amp;title=Evaluating%20the%20effects%20of%20positive%20reinforcement,%20instructional%20strategies,%20and%20negative%20reinforcement%20on%20problem%20behavior%20and%20academic%20performance:%20an%20experimental%20analysis&amp;author=KM%20Schieltz&amp;author=DP%20Wacker&amp;author=AN%20Suess&amp;author=JE%20Graber&amp;author=NH%20Lustig&amp;volume=32&amp;publication_year=2020&amp;pages=339-63&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref091">
<span class="label">91.</span><cite>Newman TJ, Santos F, Black S, Bostick K. Learning life skills through challenging and negative experiences. Child and Adolescent Social Work Journal. 2022;39(4):455–69.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Child%20and%20Adolescent%20Social%20Work%20Journal&amp;title=Learning%20life%20skills%20through%20challenging%20and%20negative%20experiences&amp;author=TJ%20Newman&amp;author=F%20Santos&amp;author=S%20Black&amp;author=K%20Bostick&amp;volume=39&amp;issue=4&amp;publication_year=2022&amp;pages=455-69&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref092">
<span class="label">92.</span><cite>Neppl TK, Jeon S, Diggs O, Donnellan MB. Positive parenting, effortful control, and developmental outcomes across early childhood. Dev Psychol. 2020;56(3):444–57. doi: 10.1037/dev0000874

</cite> [<a href="https://doi.org/10.1037/dev0000874" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7041851/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32077716/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Dev%20Psychol&amp;title=Positive%20parenting,%20effortful%20control,%20and%20developmental%20outcomes%20across%20early%20childhood&amp;author=TK%20Neppl&amp;author=S%20Jeon&amp;author=O%20Diggs&amp;author=MB%20Donnellan&amp;volume=56&amp;issue=3&amp;publication_year=2020&amp;pages=444-57&amp;pmid=32077716&amp;doi=10.1037/dev0000874&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329554.ref093">
<span class="label">93.</span><cite>Pihlainen K, Reunamo J, Sajaniemi N, Kärnä E. Children’s negative experiences as a part of quality evaluation in early childhood education and care. Early Child Development and Care. 2022;192(5):795–806.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Early%20Child%20Development%20and%20Care&amp;title=Children%E2%80%99s%20negative%20experiences%20as%20a%20part%20of%20quality%20evaluation%20in%20early%20childhood%20education%20and%20care&amp;author=K%20Pihlainen&amp;author=J%20Reunamo&amp;author=N%20Sajaniemi&amp;author=E%20K%C3%A4rn%C3%A4&amp;volume=192&amp;issue=5&amp;publication_year=2022&amp;pages=795-806&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section></section><article class="sub-article" id="pone.0329554.r001"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0329554.r001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329554.r001</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 0</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cruz-Albarran%20IA%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Irving A Cruz-Albarran</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Irving A Cruz-Albarran</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cruz-Albarran%20IA%22%5BAuthor%5D" class="usa-link"><span class="name western">Irving A Cruz-Albarran</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.c" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.c" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.c" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Irving A Cruz-Albarran</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.c" class="d-panel p" style="display: none">
<div>© 2025 Irving A. Cruz-Albarran</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>26 Feb 2025</em>
</p>
<p>Dear Dr. Tsenkova,</p>
<ul class="list" style="list-style-type:disc">
<li><p>Please be clear and detailed in addressing each of the reviewer's comments, which will undoubtedly help to improve the quality of the work..</p></li>
<li><p>Be sure you are in compliance with the journal's guidelines and requirements.</p></li>
</ul>
<p><span>plosone@plos.org</span> . When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a> .</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Irving A. Cruz-Albarran</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Journal Requirements:</p>
<p>When submitting your revision, we need you to address these additional requirements.</p>
<p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at <a href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</a> and <a href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</a></p>
<p>2. Please provide additional details regarding participant consent. In the ethics statement in the Methods and online submission information, please ensure that you have specified (1) whether consent was informed and (2) what type you obtained (for instance, written or verbal, and if verbal, how it was documented and witnessed). If your study included minors, state whether you obtained consent from parents or guardians. If the need for consent was waived by the ethics committee, please include this information.</p>
<p>If you are reporting a retrospective study of medical records or archived samples, please ensure that you have discussed whether all data were fully anonymized before you accessed them and/or whether the IRB or ethics committee waived the requirement for informed consent. If patients provided informed written consent to have data from their medical records used in research, please include this information.</p>
<p>3. Thank you for stating the following financial disclosure:</p>
<p>This work was supported by the DFG (Deutsche Forschungsgemeinschaft) [grant number SFB/TRR 540 135/3 2014].</p>
<p>Please state what role the funders took in the study. If the funders had no role, please state: "The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."</p>
<p>If this statement is not correct you must amend it as needed.</p>
<p>Please include this amended Role of Funder statement in your cover letter; we will change the online submission form on your behalf.</p>
<p>4. Please include your full ethics statement in the ‘Methods’ section of your manuscript file. In your statement, please include the full name of the IRB or ethics committee who approved or waived your study, as well as whether or not you obtained informed written or verbal consent. If consent was waived for your study, please include this information in your statement as well.</p>
<p>5. Please remove your figures from within your manuscript file, leaving only the individual TIFF/EPS image files, uploaded separately. These will be automatically included in the reviewers’ PDF<strong>.</strong></p>
<p><strong>6.</strong> We note that Figure 1, Supporting figure includes an image of a [patient / participant / in the study].</p>
<p>As per the PLOS ONE policy (<a href="http://journals.plos.org/plosone/s/submission-guidelines#loc-human-subjects-research" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://journals.plos.org/plosone/s/submission-guidelines#loc-human-subjects-research</a>) on papers that include identifying, or potentially identifying, information, the individual(s) or parent(s)/guardian(s) must be informed of the terms of the PLOS open-access (CC-BY) license and provide specific permission for publication of these details under the terms of this license. Please download the Consent Form for Publication in a PLOS Journal (<a href="http://journals.plos.org/plosone/s/file?id=8ce6/plos-consent-form-english.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://journals.plos.org/plosone/s/file?id=8ce6/plos-consent-form-english.pdf</a>). The signed consent form should not be submitted with the manuscript, but should be securely filed in the individual's case notes. Please amend the methods section and ethics statement of the manuscript to explicitly state that the patient/participant has provided consent for publication: “The individual in this manuscript has given written informed consent (as outlined in PLOS consent form) to publish these case details”.</p>
<p>If you are unable to obtain consent from the subject of the photograph, you will need to remove the figure and any other textual identifying information or case descriptions for this individual.</p>
<p>7. Please remove all personal information, ensure that the data shared are in accordance with participant consent, and re-upload a fully anonymized data set.</p>
<p>Note: spreadsheet columns with personal information must be removed and not hidden as all hidden columns will appear in the published file.</p>
<p>Additional guidance on preparing raw data for publication can be found in our Data Policy (<a href="https://journals.plos.org/plosone/s/data-availability#loc-human-research-participant-data-and-other-sensitive-data" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/data-availability#loc-human-research-participant-data-and-other-sensitive-data</a>) and in the following article: <a href="http://www.bmj.com/content/340/bmj.c181.long" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.bmj.com/content/340/bmj.c181.long</a>.</p>
<p>Additional Editor Comments :</p>
<p>Reviewers' comments must be addressed in detail, in accordance with the journal's requirements and guidelines.</p>
<p>The graphs should be made using professional software. This will improve their quality.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>
<strong>Comments to the Author</strong>
</p>
<p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: No</p>
<p>Reviewer #3: Yes</p>
<p>**********</p>
<p>2. Has the statistical analysis been performed appropriately and rigorously? --&gt;?&gt;</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>Reviewer #3: Yes</p>
<p>**********</p>
<p>3. Have the authors made all data underlying the findings in their manuscript fully available??&gt;</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a></p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>Reviewer #3: Yes</p>
<p>**********</p>
<p>4. Is the manuscript presented in an intelligible fashion and written in standard English??&gt;</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: No</p>
<p>Reviewer #3: Yes</p>
<p>**********</p>
<p>Reviewer #1: SUMMARY</p>
<p>The authors present an insightful study on the life-course development of emotion processing through the unique combination of low intensity, dynamic, multi-modal, and age-variant stimuli. This approach offers greater ecological validity than is typically achieved in experiments on emotion recognition. The authors provide a strong rationale for why this unique combination – chosen to improve the ‘naturalness’ of the stimuli – contributes to an improved understanding of human emotion recognition across age-groups. The stimuli and method are rigorously described, aiding future replicability. Validation of the Meta-MED data set is a valuable contribution to the field and provides researchers with a vast new array of emotional stimuli with high ecological validity. However, I believe that the manuscript would benefit from further elaboration on the implications of these findings outside the validation of new stimuli. There are additionally some areas where the manuscript would benefit from some minor clarifications. I am grateful to the authors for their hard work on their original study – I wish them luck in their future endeavours.</p>
<p>STRENGTHS</p>
<p>1. It is positive to see that data has been made available through the OSF and I commend the authors’ transparency.</p>
<p>2. In a similar vein, I note that the authors provide access to their newly validated stimuli. As noted above, this is a valuable contribution to the field.</p>
<p>3. The authors have rigorously described their stimuli and methodology. This lends the study towards being highly replicable.</p>
<p>4. The authors have striven to ensure control over extraneous variables, such as the possible ‘uncanniness’ of the artificial stimuli, including suitable statistical investigations of these possible extraneous effects.</p>
<p>5. Results are well-reported and clear, aiding the reader’s understanding of a large set of statistical results.</p>
<p>MAJOR</p>
<p>- No major revisions identified</p>
<p>MINOR</p>
<p>1. An in-text citation for the ‘numerous studies’ referenced in line 55 would help direct readers to the most relevant studies and/or reviews.</p>
<p>2. Across lines 132-135, the authors explain that the valence of the bias induced during emotion recognition tasks (positive vs. negative) is task dependent. Given that the previous paragraphs detail age-dependent differences in positivity/negativity biases, it would be helpful if the authors could clarify whether of not these task-specific biases show any age-dependence or otherwise have the same effects upon children as adults.</p>
<p>3. Lines 169-170 describe how the dependent variables were split across groups for children but not adults. Including a line on the rationale for this split would aid clarity.</p>
<p>4. Captions for figures 4 and 5 would benefit from a note to define what asterisks represent (e.g., * = p &lt; .05).</p>
<p>5. Missing close bracket on reference 32, line 571.</p>
<p>6. Regarding the evolutionary explanation across lines 578-581, where it is stated that children’s negativity bias is driven by heightened threat detections due to their relative vulnerability: Whilst I find this a plausible suggestion, in the absence of specifically testing this hypothesis – or indeed referencing work to support this notion – the authors may wish to consider the addition of an alternative, perhaps non-evolutionary, explanation for this finding. In its current form, I am concerned that the explanation is vulnerable to the ‘just so’ critique of evolutionary explanations in psychology.</p>
<p>Moreover, would the logic presented here not also dictate that angry expressions in physically imposing individuals (e.g., a male adult) should be perceived as more arousing to children than the angry expressions of less physically imposing individuals (e.g., a female child)? I note that the three-way interaction between Age of Faces, Valence Category, and Stimulus Type (described across lines 396-400) seemingly revealed no significant difference in young adults compared to other age groups for negative valence.</p>
<p>7. Further discussion of implications of this study would strengthen the manuscript. The authors do discuss the implications of their findings in relation to the validation of a new stimulus database – and indeed, this is a great strength of the study – however, currently lacking is further discussion of the findings’ real-world implications. For example, through lines 612 and 613, the authors report that ‘The results from both age groups suggest inherent differences in the processing of specific emotions in terms of valence.’ What are the broader implications of this knowledge on the ontogeny of emotion processing? The authors may wish to ground their findings in potential real-world applications (be that in educational settings, creating media, or parenting etc.)</p>
<p>Reviewer #2: The specific comments are as follows:</p>
<p>Abstract</p>
<p>Line 40-41, the authors said “our findings partially align with previous research…”, the statement “previous research” is not entirely appropriate in Abstract because it is too broad for readers. Specifically, it is difficult for readers to have a clear understanding of the term “previous research” when it first appeared in Abstract section.</p>
<p>Keywords</p>
<p>The word “development” is not suitable as a keyword, and it is recommended to select a suitable one again.</p>
<p>Introduction</p>
<p>The structure and presentation of this section are not clear. It is recommended to reorganize the new structure based on the research topic and content. Overall, the introduction section needs a clear main line to gradually induce the purpose, significance, and innovation of the current research.</p>
<p>Specifically,</p>
<p>Line 51-61, this section is too short and there is too little preparation in this section. It is recommended to provide necessary research background and add necessary connecting paragraphs to continue the context.</p>
<p>It is necessary to elaborate on the research progress related to emotion perception in detail, in order to reasonably introduce the innovation of current study.</p>
<p>It is necessary to clearly indicate the new contributions and values of current research in this field or related research areas.</p>
<p>Line 63 Naturalness of emotional stimuli in prior studies</p>
<p>Line 107 Development of emotion perception</p>
<p>Line 137 Usage of Metahumans</p>
<p>Do the above three parts belong to the literature review? If so, please provide a clear structural explanation, for example adding a new section literature review. If not, the necessary explanations should also be provided to facilitate readers' better understanding of the authors' work. In addition, for each individual part, the authors should provide necessary summaries rather than just comments.</p>
<p>Methods</p>
<p>Line 168 Participants, in this study, the number of participants in each group seems to be very small. Can this small sample size meet the requirements of data statistics? How to ensure the reliability and applicability of data results?</p>
<p>Line 266-267, the authors said “three additional scales”, Have the authors conducted a reliability and validity test of these scales?</p>
<p>Results</p>
<p>Line 306-307, although the authors stated that gender differences are not significant, it is best to present data results to support it. In fact, gender is a good factor worth exploring the difference between them.</p>
<p>Line 314, The writing of “t” in the expression of “t-tests” should be italicized, namely, “t-tests”</p>
<p>Line 357, the authors were advised to try changing the presentation format of Figure 2, 3. 5, and 6, such as replacing the current wireframe with a correlation heatmap (including correlation coefficients).</p>
<p>Discussion</p>
<p>Line 572-576, the authors' discussion are suggested to cite necessary references to support the analysis, thereby enhancing the reliability of the discussion.</p>
<p>Line 601, “aligns with previous findings [32]”, the cited reference is the same as those in line 571. It is recommended that the authors search for new relevant literature to support discussion and avoid repeatedly citing the same article.</p>
<p>Line 602-604, the authors' discussion are suggested to cite necessary references to support the analysis, thereby enhancing the reliability of the discussion.</p>
<p>Line 634-637, the authors' discussion are suggested to cite necessary references to support the analysis, thereby enhancing the reliability of the discussion.</p>
<p>Conclusions</p>
<p>Line 660, “likely due to”, it is not appropriate to use the term 'due to' in the conclusion section, because it is widely used in discussion section. The conclusion section should be clear research findings obtained, rather than uncertain or ambiguous statements.</p>
<p>References</p>
<p>Some of the cited references are too outdated. It is recommended to update or supplement with new references.</p>
<p>Figures</p>
<p>The presentation of graphics should be improved to enhance their aesthetic appeal and clarity.</p>
<p>Reviewer #3: The study addresses an extremely relevant and current topic, contributing to its potential impact in the research field. The selected bibliography is well-chosen and up-to-date, providing a solid theoretical foundation and reinforcing the study's scientific grounding.</p>
<p>Despite the significant interest and value of the article, the introduction presents opportunities for improvement, particularly in terms of its organization. Better structuring of this section could lead to a more fluid and coherent reading experience, allowing readers to quickly grasp the research problem, study objectives, and relevance within the field. The introduction could include a clearer exposition of the research gap this study aims to fill, along with a more structured argumentation regarding its importance and innovation, always grounded in previous similar studies.</p>
<p>The methodological description could be more detailed, particularly regarding the participant recruitment process and the type of sampling used. These details are crucial to ensuring research transparency, allowing other researchers to understand and potentially replicate the study. A more precise and rigorous description of the employed method would strengthen the credibility of the results and help justify the methodological choices made throughout the research.</p>
<p>Regarding data presentation, the graphs used are well-constructed and visually clear, facilitating the understanding of the results. However, it would be beneficial if these representations were accompanied by a more in-depth analysis, highlighting their relevance within the research context and providing detailed interpretations of what the data reveal. The inclusion of comparisons with other studies or more extensive explanations could further enrich this section.</p>
<p>Another point that deserves discussion is the designation of so-called "negative emotions." While this term is widely used in the literature, it would be worth questioning its appropriateness. Is the categorization of emotions as "negative" the most suitable? Is there a solid theoretical foundation that justifies this terminology? If not, adopting an alternative nomenclature (such as "unpleasant emotions") or at least discussing the implications of using this concept in the study’s field could be more productive.</p>
<p>Finally, an essential aspect to address is the comparability of this study with previous research. To what extent do the innovations brought by this work allow direct comparisons with other published studies? The study could benefit from a deeper analysis of how it positions itself within the existing scientific landscape, highlighting similarities and differences concerning prior research. This type of discussion could help contextualize the findings more effectively and reinforce the originality of the research.</p>
<p>Overall, the study presents clear and significant strengths. However, with some improvements, it could achieve an even higher level of clarity, scientific rigor, and contribution to the field.</p>
<p>**********</p>
<p><a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a> ). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a></p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: No</p>
<p>Reviewer #3: No</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span> . Please note that Supporting Information files do not need this step.</p></section></article><article class="sub-article" id="pone.0329554.r002"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 8;20(8):e0329554. doi: <a href="https://doi.org/10.1371/journal.pone.0329554.r002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329554.r002</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 1</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.d" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.d" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.d" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub2" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.d" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>12 Apr 2025</em>
</p>
<p>Dear Mr. Cruz-Albarran,</p>
<p>Thank you very much for evaluating our manuscript and giving us the opportunity to revise it. Attached please find the revised version of the manuscript and our detailed replies to each of the concerns raised by the reviewers.</p>
<p>We have carefully responded to all of the suggestions and, for example, restructured the Introduction for better flow and coherence as suggested by the reviewers, and added a Future Directions subheading in the Discussion to address the applications of our findings.</p>
<p>We would like to thank you and all three reviewers for the helpful comments and the overall positive feedback.</p>
<p>We hope that our revision satisfactorily addresses all concerns and look forward to your decision.</p>
<p>Sincerely,</p>
<p>Nikol Tsenkova</p>
<p>On behalf of all authors</p>
<p>Replies to Reviewer 1</p>
<p>1. An in-text citation for the ‘numerous studies’ referenced in line 55 would help direct readers to the most relevant studies and/or reviews.</p>
<p>Thank you for this observation. In the revised manuscript, we have added additional references to support the statement in line 55 (original), which now appear in lines 54-56.</p>
<p>“Consequently, numerous studies have focused on exploring the developmental trajectory of emotion perception across various age groups – mostly in children [5-6] and adults [7], but also in adolescents [8-9], and older adults [10].”</p>
<p>2. Across lines 132-135, the authors explain that the valence of the bias induced during emotion recognition tasks (positive vs. negative) is task dependent. Given that the previous paragraphs detail age-dependent differences in positivity/negativity biases, it would be helpful if the authors could clarify whether of not these task-specific biases show any age-dependence or otherwise have the same effects upon children as adults.</p>
<p>Thank you for this insightful comment. We have clarified whether these findings discussed in lines 132-135 (original) are in children, adults or both age groups (now in lines 81-85).</p>
<p>While some tasks are more likely to produce a positivity bias (e.g., identification tasks: children [19], adults [20]; intensity and arousal ratings: children [21]), others have shown to induce a negativity bias (e.g., visual search tasks: both children and adults [22]; recognition tasks: younger and older adults [23]; for a comprehensive review on infants and children, see [24, 25]).</p>
<p>3. Lines 169-170 describe how the dependent variables were split across groups for children but not adults. Including a line on the rationale for this split would aid clarity.</p>
<p>We fully agree with the reviewer's suggestion and have now added an additional explanation for our decision to split the children groups (lines 154-158 in revised manuscript).</p>
<p>“We tested two separate groups of German children, one for each of the two dependent measures (arousal and valence), and one group of adults who performed both measures simultaneously. In order to avoid fatigue and confusion regarding the difference between the scales, we separated the children sample into two groups. We did not expect such issues with the adult sample, hence there was only one adult group.”</p>
<p>4. Captions for figures 4 and 5 would benefit from a note to define what asterisks represent (e.g., * = p &lt; .05).</p>
<p>Thank you, we have now added that.</p>
<p>5. Missing close bracket on reference 32, line 571.</p>
<p>Thank you for bringing this to our attention. It is now fixed!</p>
<p>6. Regarding the evolutionary explanation across lines 578-581, where it is stated that children’s negativity bias is driven by heightened threat detections due to their relative vulnerability: Whilst I find this a plausible suggestion, in the absence of specifically testing this hypothesis – or indeed referencing work to support this notion – the authors may wish to consider the addition of an alternative, perhaps non-evolutionary, explanation for this finding. In its current form, I am concerned that the explanation is vulnerable to the ‘just so’ critique of evolutionary explanations in psychology.</p>
<p>Thank you for highlighting this crucial point. We have addressed this issue by citing several works that support our observations and have improved the paragraph between lines 573-588.</p>
<p>“Additionally, we observed that anger, not sadness, elicited higher ratings in arousal, which is in line with previous research [11]. Based on this, we hypothesize one possible explanation for our findings. As discussed by Vaish [24] in their literature review, infants may attend to negative faces (such as anger or fear) more frequently than happy ones, given that these emotions signal potential danger, which is more relevant to the infant. From an evolutionarily perspective, we suspect that children, being less capable of defending themselves, could be more reactive to expressions of anger due to the perceived higher threat value. Interestingly, a similar framework was proposed by Vesker and team [14], where they found a positivity bias for arousal from static images. They suggest that children might seek positive information from their surrounding as a means of protection from threats. While their findings revealed a positivity bias, both studies align with the evolutionary mechanisms driving these biases, although the direction may vary based on the stimulus material. Considering that our stimuli, we suspect that the difference in the observed biases in our study and Vesker’s study [14] could be attributed to the nature of our stimuli – dynamically unfolding and containing verbal information that might have enhanced the perception of emotions for both age groups due to the additional cues from the voice (e.g. an angry/sad tone).”</p>
<p>Moreover, would the logic presented here not also dictate that angry expressions in physically imposing individuals (e.g., a male adult) should be perceived as more arousing to children than the angry expressions of less physically imposing individuals (e.g., a female child)? I note that the three-way interaction between Age of Faces, Valence Category, and Stimulus Type (described across lines 396-400) seemingly revealed no significant difference in young adults compared to other age groups for negative valence.</p>
<p>Regarding the comment on suspected higher arousal for angry adult faces compared to angry children faces, we decided to run an additional analysis with the few trials featuring angry faces. We performed a paired-samples t-test to determine whether children participants found adult angry faces as more arousing compared to children’s angry faces. However, we found no significant difference, t(25) = -0.62, p = .53. It is important to note that we only had four trials for each group (adult angry faces and child angry faces), which may have limited the statistical power of this analysis. With a larger number of trials, it could be possible to better explore this potential effect.</p>
<p>7. Further discussion of implications of this study would strengthen the manuscript. The authors do discuss the implications of their findings in relation to the validation of a new stimulus database – and indeed, this is a great strength of the study – however, currently lacking is further discussion of the findings’ real-world implications. For example, through lines 612 and 613, the authors report that ‘The results from both age groups suggest inherent differences in the processing of specific emotions in terms of valence.’ What are the broader implications of this knowledge on the ontogeny of emotion processing? The authors may wish to ground their findings in potential real-world applications (be that in educational settings, creating media, or parenting etc.)</p>
<p>We are extremely grateful for this suggestion and have made the necessary changes by introducing a new subheading within Discussion, entitled “Future Directions” (lines 651-690). We first discuss the importance of employing ecologically valid stimuli for exploring developmental differences in emotion perception (lines 651-657). Then we address the effectiveness of using digital avatars for such endeavors (lines 658-665). Furthermore, we highlight several studies that have successfully implemented digital avatars in areas like teaching people with autism social skills, aiding people with Attention Deficit Hyperactivity Disorder with their overall performance, as well as serving well in educational settings (lines 666-672). Lastly, and most importantly, we underscore the application of our findings in terms of teaching and parenting styles implementing positive reinforcement, as well as the importance of reframing negative experiences to crucial for developing important life skills in youth programs and in early childhood education and care practices (lines 673-690).</p>
<p>Replies to Reviewer 2</p>
<p>Abstract</p>
<p>Line 40-41, the authors said “our findings partially align with previous research…”, the statement “previous research” is not entirely appropriate in Abstract because it is too broad for readers. Specifically, it is difficult for readers to have a clear understanding of the term “previous research” when it first appeared in Abstract section.</p>
<p>Thank you for your suggestion, we have now removed that!</p>
<p>Keywords</p>
<p>The word “development” is not suitable as a keyword, and it is recommended to select a suitable one again.</p>
<p>We acknowledge that the keyword “development” is too general and thus have changed it to “emotional development” to be more specific and in line with the manuscript’s topic.</p>
<p>Introduction</p>
<p>The structure and presentation of this section are not clear. It is recommended to reorganize the new structure based on the research topic and content. Overall, the introduction section needs a clear main line to gradually induce the purpose, significance, and innovation of the current research.</p>
<p>Specifically, Line 51-61, this section is too short and there is too little preparation in this section. It is recommended to provide necessary research background and add necessary connecting paragraphs to continue the context.</p>
<p>It is necessary to elaborate on the research progress related to emotion perception in detail, in order to reasonably introduce the innovation of current study.</p>
<p>It is necessary to clearly indicate the new contributions and values of current research in this field or related research areas.</p>
<p>Line 63 Naturalness of emotional stimuli in prior studies</p>
<p>Line 107 Development of emotion perception</p>
<p>Line 137 Usage of Metahumans</p>
<p>Do the above three parts belong to the literature review? If so, please provide a clear structural explanation, for example adding a new section literature review. If not, the necessary explanations should also be provided to facilitate readers' better understanding of the authors' work. In addition, for each individual part, the authors should provide necessary summaries rather than just comments.</p>
<p>We completely agree with the reviewer’s observation and are grateful for the suggestions. To address this, we have restructured the introduction in several ways.</p>
<p>For clarity, we have removed the subheadings and instead have focused on bridging the separate parts of the introduction with summaries and connecting sentences. Additionally, we have rearranged certain parts. We first start with an overview of developmental changes in emotional development (line 51 to 70), report observed biases in the literature on developmental emotion perception (line 70 to 85), underscore reasons for the issue of inconsistent bias findings and report studies that have addressed these issues (line 86 to 122), and finally, propose a new solution for exploring age-related differences in emotion perception, i.e. by employing digital avatars (line 123 to 141). We conclude with the aims of this study (line 142 to 148).</p>
<p>Methods</p>
<p>Line 168 Participants, in this study, the number of participants in each group seems to be very small. Can this small sample size meet the requirements of data statistics? How to ensure the reliability and applicability of data results?</p>
<p>Thank you for suggesting this clarification. We have mentioned our a priori power analysis in lines 175 and 180, where we calculated that a total of 36 participants would be enough to test our hypothesis. We have collected data from a total of 82 participants (28 adults for both measures, 26 children for arousal, and 28 children for valence). Additionally, we have done post hoc power analyses found in lines 418-420 (for arousal):</p>
<p>“We performed a post-hoc G-Power analysis for arousal (8 measurements, 54 participants) revealed observed power of 71.9%, slightly below the conventional threshold of .80.”</p>
<p>And in lines 507-509 (for valence):</p>
<p>“We performed a post-hoc G-Power analysis (4 measurements, 56 participants) which revealed observed power of 93.1%, indicating ample power for this measure.”</p>
<p>Line 266-267, the authors said “three additional scales”, Have the authors conducted a reliability and validity test of these scales?</p>
<p>Thank you for bringing this to our attention. We have now included a thorough explanation of the scales and several citations showing the validity and reliability of the mentioned scales (line 264-271):</p>
<p>“The Godspeed Questionnaire (GQ) is the most frequently used tool in the field of Human-Robot Interaction [65], with over 3,000 citations as of April 2025, and has been translated into 19 languages. It consists of five scales, which can be used independently: Anthropomorphism (α = 0.87), Animacy (α = 0.92), Likeability (α = 0.70), Perceived Intelligence (α = 0.75), and Perceived Safety (α = 0.91) [66]. The German version of the GQ [67] has been reported to have good internal reliability (α = 0.70). In terms of validity, only the Polish version [68] has undergone factor analysis, which yielded a total variance of 74.24% for the four-factor solution.”</p>
<p>Results</p>
<p>Line 306-307, although the authors stated that gender differences are not significant, it is best to present data results to support it. In fact, gender is a good factor worth exploring the difference between them.</p>
<p>Thank you for this comment. We have now added this information in lines 311-314.</p>
<p>“Furthermore, we found no significant gender effects for either of the two measures – for arousal, F(1, 50) = .11, p = .73, η2p = .002, for valence, F(1, 52) = .23, p = .63, η2p = .004. Thus, we did not include gender in all further analyses.”</p>
<p>Line 314, The writing of “t” in the expression of “t-tests” should be italicized, namely, “t-tests”</p>
<p>Thank you, we have now changed this.</p>
<p>Line 357, the authors were advised to try changing the presentation format of Figure 2, 3. 5, and 6, such as replacing the current wireframe with a correlation heatmap (including correlation coefficients).</p>
<p>Thank you for your insightful suggestion! We have added four correlational heatmaps as Supplementary Material, showing the specific correlational coefficient of the responses between children and adults for each stimulus. We could not include them in the manuscript due to the large size of the correlational heatmap and considered it would be best to have this information in Supplementary Material instead.</p>
<p>Discussion</p>
<p>Line 572-576, the authors' discussion are suggested to cite necessary references to support the analysis, thereby enhancing the reliability of the discussion.</p>
<p>Thank you for your suggestion; we have now added a reference to support our observations (lines 569-572).</p>
<p>Line 601, “aligns with previous findings [32]”, the cited reference is the same as those in line 571. It is recommended that the authors search for new relevant literature to support discussion and avoid repeatedly citing the same article.</p>
<p>Thank you for this suggestion. We acknowledge the repeated citation and have now included an additional study by the same author, which - although based on a different task - shares important similarities with the design and goals of our own experiment. It was challenging to find further relevant literature, considering the limited number of studies that specifically examine arousal and valence ratings across both children and adults, particularly in a non-correlational context. Considering our study’s design and research questions which are closely aligned with the original paper cited, and given that it is, to our knowledge, the only one addressing these specific questions, we focused our comparis</p>
<section class="sm xbox font-sm" id="pone.0329554.s004"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12333977/bin/pone.0329554.s004.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0329554.s004.docx</a><sup> (26.9KB, docx) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0329554.r003"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0329554.r003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329554.r003</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 1</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cruz-Albarran%20IA%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Irving A Cruz-Albarran</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Irving A Cruz-Albarran</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cruz-Albarran%20IA%22%5BAuthor%5D" class="usa-link"><span class="name western">Irving A Cruz-Albarran</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.e" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.e" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.e" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Irving A Cruz-Albarran</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.e" class="d-panel p" style="display: none">
<div>© 2025 Irving A. Cruz-Albarran</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>3 Jul 2025</em>
</p>
<p>Dear Dr. Tsenkova,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
<p>Please submit your revised manuscript by Aug 17 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span> . When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a> .</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Irving A. Cruz-Albarran</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Journal Requirements:</p>
<p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
<p>Additional Editor Comments :</p>
<p>Thank you very much for addressing the reviewers' comments correctly. We kindly ask you to address the minor revisions requested by the last reviewer. Thank you again.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>
<strong>Comments to the Author</strong>
</p>
<p>Reviewer #1: All comments have been addressed</p>
<p>Reviewer #4: All comments have been addressed</p>
<p>**********</p>
<p>2. Is the manuscript technically sound, and do the data support the conclusions??&gt;</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #4: Yes</p>
<p>**********</p>
<p>3. Has the statistical analysis been performed appropriately and rigorously? --&gt;?&gt;</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #4: Yes</p>
<p>**********</p>
<p>4. Have the authors made all data underlying the findings in their manuscript fully available??&gt;</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a></p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #4: Yes</p>
<p>**********</p>
<p>5. Is the manuscript presented in an intelligible fashion and written in standard English??&gt;</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #4: Yes</p>
<p>**********</p>
<p>Reviewer #1: The authors have adequately addressed all my previous comments. I extend my thanks to them for their efforts and wish them the best of luck in their future research.</p>
<p>Reviewer #4: I was not one of the original reviewers, and I can see that their comments have been thoroughly addressed, so my comments refer to other small pieces within the text.</p>
<p>My first comment refers to consent; you state that parents gave consent for their children, but were children asked if they wanted to take part? At that age they would have been able to understand and give assent. Similarly, were children and young people able to give consent for their voices to be used for the vocalisations, or did parents give this consent too?</p>
<p>My second comments is around payment, why did child participants get paid less than the adults?</p>
<p>Thirdly, I was wondering if the race of the avatars made any difference to the perceptions by participants. I know there is some evidence people are better at recognising emotions from those with a similar ethnic background, was this analysis carried out/possible?</p>
<p>Finally, autistic communities prefer 'autistic' rather than 'with autism', using this language would bring the terminology up to date and in line with the autistic people's preferences.</p>
<p>Overall this is a great article, and will add to the field. I found the uncanny valley effect analysis particularly interesting and look forward to seeing it published.</p>
<p>**********</p>
<p><a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a> ). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a></p>
<p>Reviewer #1: No</p>
<p>Reviewer #4: No</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span></p></section></article><article class="sub-article" id="pone.0329554.r004"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 8;20(8):e0329554. doi: <a href="https://doi.org/10.1371/journal.pone.0329554.r004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329554.r004</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 2</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.f" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.f" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.f" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub4" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.f" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>9 Jul 2025</em>
</p>
<p>Response to Reviewer #4:</p>
<p>1. My first comment refers to consent; you state that parents gave consent for their children, but were children asked if they wanted to take part? At that age they would have been able to understand and give assent. Similarly, were children and young people able to give consent for their voices to be used for the vocalisations, or did parents give this consent too?</p>
<p>- Thank you for this important comment. Since the study was conducted online, parents were instructed to involve their children only if they were willing and comfortable taking part. In two cases, children declined participation, which we fully respected.</p>
<p>Regarding the voice recordings, parents again obtained verbal assent from their children, who were informed about the purpose and nature of the recordings. All recordings were made with the children's full knowledge and agreement.</p>
<p>2. My second comments is around payment, why did child participants get paid less than the adults?</p>
<p>- Thank you for the question. The payment difference reflects the shorter duration of the task for children. They rated only half the videos using one scale (about 30 minutes total), whereas adults completed both scales across the full set of 128 videos, which took significantly longer.</p>
<p>3. Thirdly, I was wondering if the race of the avatars made any difference to the perceptions by participants. I know there is some evidence people are better at recognising emotions from those with a similar ethnic background, was this analysis carried out/possible?</p>
<p>- Thank you for this insightful comment. We considered this aspect during the creation of our database, but as the current study did not aim to examine race-related effects, we chose to use only Caucasian avatars to maintain consistency and reduce variability. As such, no analysis on racial bias was possible. However, we plan to expand the database to include more diverse facial representations in future work.</p>
<p>4. Finally, autistic communities prefer 'autistic' rather than 'with autism', using this language would bring the terminology up to date and in line with the autistic people's preferences.</p>
<p>- This is a very helpful comment, it’s much appreciated! We have now changed it as you have suggested.</p>
<p>We would like to thank the reviewer for all the valuable comments and suggestions that improved our manuscript.</p>
<section class="sm xbox font-sm" id="pone.0329554.s005"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers #2.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12333977/bin/pone.0329554.s005.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0329554.s005.docx</a><sup> (15KB, docx) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0329554.r005"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0329554.r005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329554.r005</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 2</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cruz-Albarran%20IA%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Irving A Cruz-Albarran</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Irving A Cruz-Albarran</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cruz-Albarran%20IA%22%5BAuthor%5D" class="usa-link"><span class="name western">Irving A Cruz-Albarran</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.g" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.g" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.g" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Irving A Cruz-Albarran</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.g" class="d-panel p" style="display: none">
<div>© 2025 Irving A. Cruz-Albarran</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>18 Jul 2025</em>
</p>
<p>Developmental differences in perceiving arousal and valence from dynamically unfolding emotional expressions</p>
<p>PONE-D-24-37652R2</p>
<p>Dear Dr. Tsenkova,</p>
<p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
<p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
<p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Editorial Manager®</a>  and clicking the ‘Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at authorbilling@plos.org.</p>
<p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>Kind regards,</p>
<p>Irving A. Cruz-Albarran</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Additional Editor Comments (optional):</p>
<p>Thank you very much for your attention to the comments.</p>
<p>Reviewers' comments:</p></section></article><article class="sub-article" id="pone.0329554.r006"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0329554.r006" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329554.r006</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Acceptance letter</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cruz-Albarran%20IA%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Irving A Cruz-Albarran</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Irving A Cruz-Albarran</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cruz-Albarran%20IA%22%5BAuthor%5D" class="usa-link"><span class="name western">Irving A Cruz-Albarran</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.h" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.h" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.h" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Irving A Cruz-Albarran</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.h" class="d-panel p" style="display: none">
<div>© 2025 Irving A. Cruz-Albarran</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>PONE-D-24-37652R2</p>
<p>PLOS ONE</p>
<p>Dear Dr. Tsenkova,</p>
<p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p>
<p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p>
<p>* All references, tables, and figures are properly cited</p>
<p>* All relevant supporting information is included in the manuscript submission,</p>
<p>* There are no issues that prevent the paper from being properly typeset</p>
<p>You will receive further instructions from the production team, including instructions on how to review your proof when it is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p>
<p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>You will receive an invoice from PLOS for your publication fee after your manuscript has reached the completed accept phase. If you receive an email requesting payment before acceptance or for any other service, this may be a phishing scheme. Learn how to identify phishing emails and protect your accounts at <a href="https://explore.plos.org/phishing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://explore.plos.org/phishing</a>.</p>
<p>If we can help with anything else, please email us at customercare@plos.org.</p>
<p>Thank you for submitting your work to PLOS ONE and supporting open access.</p>
<p>Kind regards,</p>
<p>PLOS ONE Editorial Office Staff</p>
<p>on behalf of</p>
<p>Dr. Irving A. Cruz-Albarran</p>
<p>Academic Editor</p>
<p>PLOS ONE</p></section></article><article class="sub-article" id="_ad93_"><section class="pmc-layout__citation font-secondary font-xs"><div></div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Associated Data</h1></hgroup><ul class="d-buttons inline-list"></ul>
<div class="d-panels font-secondary-light"></div>
<div></div>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
</div></section><section class="body sub-article-body"><section id="_adsm93_" lang="en" class="supplementary-materials"><h2 class="pmc_sec_title">Supplementary Materials</h2>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="caption p">
<span>S1_Trial example. The video with the specific stimulus is presented with either one of the two scales (for the children sample) or with both scales (for the adult sample).</span><p>(TIF)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12333977/bin/pone.0329554.s001.tif" data-ga-action="click_feat_suppl" class="usa-link">pone.0329554.s001.tif</a><sup> (348KB, tif) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="caption p">
<span>S2_Correlational heatmaps. The figures show the correlations separately for the visual and the visual verbal condition of children’ and adults’ responses for arousal and valence. The legend explains how to read the trials properly.</span><p>(PDF)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12333977/bin/pone.0329554.s002.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0329554.s002.pdf</a><sup> (1.7MB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material3_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12333977/bin/pone.0329554.s004.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0329554.s004.docx</a><sup> (26.9KB, docx) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material4_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers #2.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12333977/bin/pone.0329554.s005.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0329554.s005.docx</a><sup> (15KB, docx) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h2 class="pmc_sec_title">Data Availability Statement</h2>
<p>Data are available from the OSF repository [URL: <a href="https://osf.io/3rd7m/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://osf.io/3rd7m/</a>]</p></section></section></article><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0329554"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0329554.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (869.0 KB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12333977/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12333977/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12333977%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12333977/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12333977/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12333977/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40779505/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12333977/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40779505/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12333977/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12333977/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="z39OzQVTsSobYCOcTRDNqUJGrMXhbdBVpvaviRnZWWR0gf2tNkXUyhOfkFtGo9rp">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Web Policies

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    FOIA

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS Vulnerability Disclosure

    
</a>

                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Help

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Accessibility

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Careers

    
</a>

                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NLM

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NIH

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    USA.gov

    
</a>

                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-370d5dd6.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-917ba005.js"></script>
    
    

    </body>
</html>
