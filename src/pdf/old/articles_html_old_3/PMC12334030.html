
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-a68b4900.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-0a3f24ce.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            LKDA-Net: Hierarchical transformer with large Kernel depthwise convolution attention for 3D medical image segmentation - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="67D7142889B743F31C1428002CD136C0.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334030/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="LKDA-Net: Hierarchical transformer with large Kernel depthwise convolution attention for 3D medical image segmentation">
<meta name="citation_author" content="Ming Li">
<meta name="citation_author_institution" content="Graduate School, Shandong University of Traditional Chinese Medicine, Jinan, China">
<meta name="citation_author" content="Jingang Ma">
<meta name="citation_author_institution" content="School of Medical Information Engineering, Shandong University of Traditional Chinese Medicine, Jinan, China">
<meta name="citation_author" content="Jing Zhao">
<meta name="citation_author_institution" content="Qilu University of Technology (Shandong Academy of Sciences), Jinan, China">
<meta name="citation_publication_date" content="2025 Aug 8">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0329806">
<meta name="citation_doi" content="10.1371/journal.pone.0329806">
<meta name="citation_pmid" content="40779579">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334030/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334030/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334030/pdf/pone.0329806.pdf">
<meta name="description" content="Since Transformers have demonstrated excellent performance in the segmentation of two-dimensional medical images, recent works have also introduced them into 3D medical segmentation tasks. For example, hierarchical transformers like Swin UNETR have ...">
<meta name="og:title" content="LKDA-Net: Hierarchical transformer with large Kernel depthwise convolution attention for 3D medical image segmentation">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Since Transformers have demonstrated excellent performance in the segmentation of two-dimensional medical images, recent works have also introduced them into 3D medical segmentation tasks. For example, hierarchical transformers like Swin UNETR have ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334030/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-testid="header" data-header >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            
                <a class="ncbi-header__logo-container" href="https://www.ncbi.nlm.nih.gov/">
                    <img alt="
                                  NCBI home page
                              "
                         class="ncbi-header__logo-image"
                         src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg" />
                </a>
            

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-testid="searchPanel"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      aria-describedby="search-field-desktop-navigation-help-text"
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only"
                           data-testid="label"
                           for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           data-testid="textInput"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    data-testid="button"
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  role="search">
                <label class="usa-sr-only" for="search-field">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="search" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        <a class="usa-link" href="https://www.ncbi.nlm.nih.gov/pmc/advanced/" data-ga-action="featured_link" data-ga-label="advanced_search">
                            Advanced Search
                        </a>
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12334030">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0329806"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0329806.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12334030%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12334030/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12334030/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334030/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0329806" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 8;20(8):e0329806. doi: <a href="https://doi.org/10.1371/journal.pone.0329806" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0329806</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>LKDA-Net: Hierarchical transformer with large Kernel depthwise convolution attention for 3D medical image segmentation</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Ming Li</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Ming Li</span></h3>
<div class="p">
<sup>1</sup>Graduate School, Shandong University of Traditional Chinese Medicine, Jinan, China</div>
<div>Writing – original draft, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Ming Li</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ma%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Jingang Ma</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Jingang Ma</span></h3>
<div class="p">
<sup>2</sup>School of Medical Information Engineering, Shandong University of Traditional Chinese Medicine, Jinan, China</div>
<div>Supervision, Validation</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ma%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Jingang Ma</span></a>
</div>
</div>
<sup>2,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhao%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Jing Zhao</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Jing Zhao</span></h3>
<div class="p">
<sup>3</sup>Qilu University of Technology (Shandong Academy of Sciences), Jinan, China</div>
<div>Supervision</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhao%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Jing Zhao</span></a>
</div>
</div>
<sup>3</sup>
</div>
<div class="cg p">Editor: <span class="name western">Fatih Uysal</span><sup>4</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>Graduate School, Shandong University of Traditional Chinese Medicine, Jinan, China</div>
<div id="aff002">
<sup>2</sup>School of Medical Information Engineering, Shandong University of Traditional Chinese Medicine, Jinan, China</div>
<div id="aff003">
<sup>3</sup>Qilu University of Technology (Shandong Academy of Sciences), Jinan, China</div>
<div id="edit1">
<sup>4</sup>Kafkas University: Kafkas Universitesi, TÜRKIYE</div>
<div class="author-notes p">
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>2661444606@qq.com</span></p>
</div>
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>The authors have declared that no competing interests exist.</p></div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Ming Li</span></strong>: <span class="role">Writing – original draft, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Jingang Ma</span></strong>: <span class="role">Supervision, Validation</span>
</div>
<div>
<strong class="contrib"><span class="name western">Jing Zhao</span></strong>: <span class="role">Supervision</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Fatih Uysal</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Mar 19; Accepted 2025 Jul 22; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Li et al</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12334030  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40779579/" class="usa-link">40779579</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>Since Transformers have demonstrated excellent performance in the segmentation of two-dimensional medical images, recent works have also introduced them into 3D medical segmentation tasks. For example, hierarchical transformers like Swin UNETR have reintroduced several prior knowledge of convolutional networks, further enhancing the model’s volumetric segmentation ability on three-dimensional medical datasets. The effectiveness of these hybrid architecture methods is largely attributed to the large number of parameters and the large receptive fields of non-local self-attention. We believe that large-kernel volumetric depthwise convolutions can obtain large receptive fields with fewer parameters. In this paper, we propose a lightweight three-dimensional convolutional network, LKDA-Net, for efficient and accurate three-dimensional volumetric segmentation. This network adopts a large-kernel depthwise convolution attention mechanism to simulate the self-attention mechanism of Transformers. Firstly, inspired by the Swin Transformer module, we investigate different-sized large-kernel convolution attention mechanisms to obtain larger global receptive fields, and replace the MLP in the Swin Transformer with the Inverted Bottleneck with Depthwise Convolutional Augmentation to reduce channel redundancy and enhance feature expression and segmentation performance. Secondly, we propose a skip connection fusion module to achieve smooth feature fusion, enabling the decoder to effectively utilize the features of the encoder. Finally, through experimental evaluations on three public datasets, namely Synapse, BTCV and ACDC, LKDA-Net outperforms existing models of various architectures in segmentation performance and has fewer parameters. Code: <a href="https://github.com/zouyunkai/LKDA-Net" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/zouyunkai/LKDA-Net</a>.</p></section><section id="sec001"><h2 class="pmc_sec_title">Introduction</h2>
<p>Since 3D medical images possess richer and more detailed spatial information than 2D images, 3D voxel segmentation is a critical technique that enables the visualization of medical images, aids in diagnosis, and facilitates the planning of treatments [<a href="#pone.0329806.ref001" class="usa-link" aria-describedby="pone.0329806.ref001">1</a>, <a href="#pone.0329806.ref002" class="usa-link" aria-describedby="pone.0329806.ref002">2</a>]. For example, hierarchical transformer models like the Swin Transformer have been introduced into the segmentation of 3D medical images and have achieved excellent performance on multiple volumetric segmentation benchmarks [<a href="#pone.0329806.ref003" class="usa-link" aria-describedby="pone.0329806.ref003">3</a>–<a href="#pone.0329806.ref006" class="usa-link" aria-describedby="pone.0329806.ref006">6</a>]. These models have reincorporated some prior knowledge of convolutional neural networks, such as local connectivity and translation invariance, enhancing the applicability of transformers in the field of 3D medical images. Their strategy of splitting the input into patches and using the window-level self-attention mechanism to model global dependencies has further enlarged the receptive field and strengthened the feature representation [<a href="#pone.0329806.ref007" class="usa-link" aria-describedby="pone.0329806.ref007">7</a>]. Therefore, these hybrid transformer-convolution frameworks have achieved a significant improvement in performance [<a href="#pone.0329806.ref008" class="usa-link" aria-describedby="pone.0329806.ref008">8</a>].</p>
<p>However, directly applying 3D transformer models as general backbone networks has issues such as high computational overhead and high memory requirements [<a href="#pone.0329806.ref003" class="usa-link" aria-describedby="pone.0329806.ref003">3</a>]. The computational complexity of the global self-attention mechanism grows quadratically with the increase in input resolution [<a href="#pone.0329806.ref009" class="usa-link" aria-describedby="pone.0329806.ref009">9</a>]. High-resolution 3D medical images often contain a large amount of fine-grained information, imposing higher requirements on the feature extraction capabilities of models [<a href="#pone.0329806.ref010" class="usa-link" aria-describedby="pone.0329806.ref010">10</a>]. Therefore, the design of efficient 3D medical image analysis models still remains to be explored [<a href="#pone.0329806.ref011" class="usa-link" aria-describedby="pone.0329806.ref011">11</a>, <a href="#pone.0329806.ref012" class="usa-link" aria-describedby="pone.0329806.ref012">12</a>]. We believe that compared with transformers, 3D convolutional networks can simulate the behavior of large receptive fields with fewer parameters through depthwise convolutions. The depthwise convolution within local regions can mimic the window-level self-attention computation of transformers [<a href="#pone.0329806.ref003" class="usa-link" aria-describedby="pone.0329806.ref003">3</a>]. Large-kernel depthwise separable convolutions can provide global-level receptive fields for feature extraction, replacing the expensive global self-attention operations of transformers [<a href="#pone.0329806.ref013" class="usa-link" aria-describedby="pone.0329806.ref013">13</a>].</p>
<p>Based on the above considerations, this paper proposes a lightweight three-dimensional convolutional network named LKDA-Net, which uses a large-kernel depthwise convolution attention mechanism to simulate the Transformer self-attention mechanism, aiming to achieve efficient and accurate three-dimensional volumetric segmentation. Specifically, inspired by the design of the Swin Transformer module, we have investigated large-kernel convolution attention mechanisms of different sizes to obtain a larger global receptive field. Furthermore, we use the Inverted Bottleneck with Depthwise Convolutional Augmentation to replace the MLP in the Swin Transformer module, which can enhance the feature expression ability and improve the segmentation performance while reducing the redundancy among channels. We evaluated the LKDA-Net on three public three-dimensional medical image segmentation datasets, and the results show that it outperforms current models with various architectures and has fewer parameters. Our main contributions are as follows:</p>
<ul class="list" style="list-style-type:disc">
<li><p>We propose the LKDA-Net, which is a lightweight convolutional network for three-dimensional medical image segmentation based on the LKDA-Net Block. The LKDA-Net Block explores large-kernel convolution attention mechanisms of different sizes to obtain a larger global receptive field. In addition, we design and use the Inverted Bottleneck with Depthwise Convolution Augmentation to replace the multi-layer perceptron (MLP) to enhance the expression of channel features, so as to reduce the number of parameters and improve the segmentation performance.</p></li>
<li><p>We propose a skip connection fusion module to achieve smooth feature fusion, enabling the decoder to effectively utilize the feature information obtained by the encoder and optimize the feature processing effect.</p></li>
<li><p>We evaluated the segmentation performance of our LKDA-Net on three public datasets and provided visual analysis and parameter quantity comparison. The experimental results demonstrate that our model outperforms current models with various architectures in terms of performance and has significantly fewer parameters.</p></li>
</ul></section><section id="sec002"><h2 class="pmc_sec_title">Related work</h2>
<section id="sec003"><h3 class="pmc_sec_title">CNN-based segmentation approaches</h3>
<p>In the field of deep learning-based image segmentation, the primary network architectures encompass three categories: Convolutional Neural Network (CNN)-based methods, Transformer-based methods, and CNN-Transformer hybrid architectures. CNN-based methods leverage the robust spatial feature extraction capabilities of CNNs to effectively identify and segment anatomical structures in medical images. Convolutional neural networks (CNNs) [<a href="#pone.0329806.ref014" class="usa-link" aria-describedby="pone.0329806.ref014">14</a>–<a href="#pone.0329806.ref016" class="usa-link" aria-describedby="pone.0329806.ref016">16</a>] have emerged as potent means for handling medical image segmentation tasks, owing to their remarkable capacity in seizing multiscale representations, local semantic details, as well as texture particulars. Çiçek and his collaborators [<a href="#pone.0329806.ref017" class="usa-link" aria-describedby="pone.0329806.ref017">17</a>] augmented the U-Net framework by substituting 2D convolutions with 3D operations for the purpose of segmentation in volumetric images with sparse labels. Isensee <em>et al</em>.[<a href="#pone.0329806.ref016" class="usa-link" aria-describedby="pone.0329806.ref016">16</a>] put forward a nnU-Net model founded on the U-Net structure, which incorporates an automated setup to distill features from images at multiple hierarchical levels.</p>
<p>Furthermore, investigators have delved into the acquisition of local-global information via pure CNN architectures, such as deformable convolutions [<a href="#pone.0329806.ref018" class="usa-link" aria-describedby="pone.0329806.ref018">18</a>–<a href="#pone.0329806.ref020" class="usa-link" aria-describedby="pone.0329806.ref020">20</a>], depthwise convolutions [<a href="#pone.0329806.ref021" class="usa-link" aria-describedby="pone.0329806.ref021">21</a>–<a href="#pone.0329806.ref023" class="usa-link" aria-describedby="pone.0329806.ref023">23</a>] and large kernel convolutions [<a href="#pone.0329806.ref013" class="usa-link" aria-describedby="pone.0329806.ref013">13</a>, <a href="#pone.0329806.ref024" class="usa-link" aria-describedby="pone.0329806.ref024">24</a>]. For example,In the detection of bipolar disorder using OCT images, Attention TurkerNeXt employs interpretable attention mechanisms and feature visualization techniques to identify critical anatomical regions that drive the model’s decision-making process[<a href="#pone.0329806.ref025" class="usa-link" aria-describedby="pone.0329806.ref025">25</a>]. Ho <em>et al</em>. [<a href="#pone.0329806.ref026" class="usa-link" aria-describedby="pone.0329806.ref026">26</a>] used relatively large <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e001"><math id="M1" display="inline" overflow="linebreak"><mrow><mn>7</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>7</mn></mrow></math></span> kernels within skip connections to deal with the segmentation of splenic regions. Similarly, Li <em>et al</em>. [<a href="#pone.0329806.ref021" class="usa-link" aria-describedby="pone.0329806.ref021">21</a>] proposed the Lkau-net architecture, integrating extensive depthwise convolutions and large-kernel convolutions within the decoder for medical image volume segmentation. Unfortunately, as the kernel size increases, both model parameters and FLOPs increase significantly, thereby affecting training and inference efficiency. To improve the effectiveness of large kernels, ConvNeXt [<a href="#pone.0329806.ref027" class="usa-link" aria-describedby="pone.0329806.ref027">27</a>] utilizes the potential of large kernel depthwise convolutions, a technique originating from the field of natural image processing. However, in the field of volume segmentation, the application of large-kernel depthwise convolutions is still relatively under-explored. Given the large receptive field provided by these depthwise convolutions, we believe that they have the potential to emulate Transformer behaviors, making them suitable for effective use in volume segmentation tasks.</p></section><section id="sec004"><h3 class="pmc_sec_title">Transformers-based segmentation approaches</h3>
<p>Subsequently, Transformers have achieved remarkable success in natural language processing (NLP), and Transformer-based approaches for medical image segmentation have emerged as a key research focus. By leveraging self-attention mechanisms, these methods capture global dependencies to improve the understanding and segmentation accuracy of complex medical images, particularly those with challenging backgrounds and irregular anatomical structures[<a href="#pone.0329806.ref028" class="usa-link" aria-describedby="pone.0329806.ref028">28</a>]. Recent progress in Vision Transformers [<a href="#pone.0329806.ref029" class="usa-link" aria-describedby="pone.0329806.ref029">29</a>] has overcome Long-Range dependency issues, especially in Medical Image Segmentation [<a href="#pone.0329806.ref006" class="usa-link" aria-describedby="pone.0329806.ref006">6</a>, <a href="#pone.0329806.ref030" class="usa-link" aria-describedby="pone.0329806.ref030">30</a>]. A significant innovation in this area is the Swin-Unet [<a href="#pone.0329806.ref028" class="usa-link" aria-describedby="pone.0329806.ref028">28</a>], which has a U-shaped encoder-decoder structure enhanced with Swin Transformer blocks. Likewise, Jiang <em>et al</em>. introduced SwinBTS [<a href="#pone.0329806.ref031" class="usa-link" aria-describedby="pone.0329806.ref031">31</a>], which uses improved Transformer modules to extract detailed features. Zhou <em>et al</em>.’s nnFormer [<a href="#pone.0329806.ref006" class="usa-link" aria-describedby="pone.0329806.ref006">6</a>] retains the use of convolutional layers for local image details and a hierarchical structure for multi-scale features. However, Transformer-based volumetric segmentation models have a large number of parameters and long training times, and the high computational complexity due to multi-scale feature extraction makes the situation worse [<a href="#pone.0329806.ref032" class="usa-link" aria-describedby="pone.0329806.ref032">32</a>]. This limitation leads to a reconsideration of whether convolutional neural networks can effectively mimic Transformer advantages for efficient feature extraction.</p></section><section id="sec005"><h3 class="pmc_sec_title">CNN-transformer hybrid segmentation approaches</h3>
<p>Recently, researchers have begun exploring methods based on CNN-Transformer hybrid architectures. These hybrid approaches aim to integrate the efficiency of CNNs in processing local image features with the capability of Transformers to capture global dependencies, thereby enhancing both the efficiency and precision of medical image segmentation[<a href="#pone.0329806.ref033" class="usa-link" aria-describedby="pone.0329806.ref033">33</a>]. Some research efforts have aimed to develop hybrid architectures that combine the U-Net model with transformers [<a href="#pone.0329806.ref002" class="usa-link" aria-describedby="pone.0329806.ref002">2</a>, <a href="#pone.0329806.ref005" class="usa-link" aria-describedby="pone.0329806.ref005">5</a>]. This combination intends to use convolutions for local feature extraction and global self-attention to capture comprehensive global-local contextual information. One such innovation is TransUNet [<a href="#pone.0329806.ref033" class="usa-link" aria-describedby="pone.0329806.ref033">33</a>], which has introduced an encoder with a hybrid CNN-Transformer architecture. This enhancement improves segmentation performance by smoothly integrating convolutional neural networks into the Transformer framework. This integration helps to effectively regain local spatial information. TransFuse [<a href="#pone.0329806.ref034" class="usa-link" aria-describedby="pone.0329806.ref034">34</a>] adopted a parallel integration approach that combines Transformers and CNNs to boost the effectiveness of seizing global information. Furthermore, UNETR [<a href="#pone.0329806.ref005" class="usa-link" aria-describedby="pone.0329806.ref005">5</a>] has introduced a new approach to semantic segmentation of medical images by using Transformers. This novel method redefines the task as a 1D sequence-to-sequence prediction problem. Another remarkable contribution is the 3D UX-NET [<a href="#pone.0329806.ref002" class="usa-link" aria-describedby="pone.0329806.ref002">2</a>], which proposes a lightweight volumetric ConvNet module with large kernel depth-wise convolutions. This module fine-tunes stratified features, ultimately leading to improved volumetric segmentation results.</p></section><section id="sec006"><h3 class="pmc_sec_title">Attention mechanism</h3>
<p>Attention mechanism enables the model to focus flexibly and specifically on the critical parts in the image. In the field of medical image segmentation, the attention mechanism encompasses two main categories: spatial attention and channel attention. Among them, the channel attention primarily concentrates on those objects of significance [<a href="#pone.0329806.ref035" class="usa-link" aria-describedby="pone.0329806.ref035">35</a>–<a href="#pone.0329806.ref037" class="usa-link" aria-describedby="pone.0329806.ref037">37</a>], while the spatial attention lays its emphasis on salient regions [<a href="#pone.0329806.ref038" class="usa-link" aria-describedby="pone.0329806.ref038">38</a>, <a href="#pone.0329806.ref039" class="usa-link" aria-describedby="pone.0329806.ref039">39</a>]. Currently, most of the Transformer-based methods directly utilize the self-attention mechanism of Transformer to capture global feature information. For example, MedT [<a href="#pone.0329806.ref040" class="usa-link" aria-describedby="pone.0329806.ref040">40</a>] has proposed a model based on gated axial attention, which extends the current architecture by incorporating a control mechanism into the self-attention. The guided self-attention mechanism proposed by Sinha captures richer context dependencies more accurately [<a href="#pone.0329806.ref041" class="usa-link" aria-describedby="pone.0329806.ref041">41</a>]. The global spatial attention module constructed by TransAttUnet [<a href="#pone.0329806.ref042" class="usa-link" aria-describedby="pone.0329806.ref042">42</a>] successfully combines the global spatial attention with the self-attention, allowing the model to obtain long-range context interaction information. To effectively alleviate the high computational burden caused by self-attention, SegNeXt [<a href="#pone.0329806.ref043" class="usa-link" aria-describedby="pone.0329806.ref043">43</a>] proposes an efficient multi-scale convolutional attention mechanism. PraNet [<a href="#pone.0329806.ref044" class="usa-link" aria-describedby="pone.0329806.ref044">44</a>] introduces reverse attention to improve the accuracy of segmentation boundaries. However, these techniques only focus on spatially salient regions and neglect, to some extent, the attention to important objects in the channel dimension. For instance, although CBAM integrates channel and spatial information, its spatial attention is obtained through channel compression, resulting in the uniform distribution of spatial attention weights among channels [<a href="#pone.0329806.ref045" class="usa-link" aria-describedby="pone.0329806.ref045">45</a>]. MA-Unet [<a href="#pone.0329806.ref046" class="usa-link" aria-describedby="pone.0329806.ref046">46</a>] uses attention gates to properly solve the semantic ambiguity introduced by skip connections and acquires global information at different scales by means of multi-scale prediction fusion.</p></section></section><section id="sec007"><h2 class="pmc_sec_title">Method</h2>
<p><a href="#pone.0329806.g001" class="usa-link">Fig 1</a> illustrates the network architecture of our LKDA-Net constructed based on the LKDA-Net Block. Firstly, we utilize the large kernel projection to extract patch-wise features and input them into the encoder composed of the LKDA-Net Block and the downsampling block. Subsequently, the decoder consisting of the Skip Connection Fusion Module and the upsampling module further extracts features and performs upsampling. Meanwhile, the feature information at different scales of the encoder is fused through the Skip Connection Fusion Module.</p>
<figure class="fig xbox font-sm" id="pone.0329806.g001"><h3 class="obj_head">Fig 1. Overview of the LKDA-Net Architecture.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334030_pone.0329806.g001.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5e7d/12334030/2124c402a426/pone.0329806.g001.jpg" loading="lazy" height="447" width="658" alt="Fig 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329806.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>We adopt a multi-scale hierarchical encoder-decoder structure. Firstly, the feature map is projected into multiple patches that are embedded by means of large-kernel 3D convolution. Each stage within the encoder consists of LKDA-Net Blocks and downsampling modules, which are responsible for handling feature maps at different scales. The decoder is made up of the Skip Connection Fusion Module and upsampling modules. The specific structure of the LKDA-Net Block is illustrated in <a href="#pone.0329806.g002" class="usa-link">Figs 2</a> and <a href="#pone.0329806.g003" class="usa-link">3</a> The specific structure of the Skip Connection Fusion Module is shown in <a href="#pone.0329806.g004" class="usa-link">Fig 4</a>.</p></figcaption></figure><p>For a more detailed comparison, <a href="#pone.0329806.t001" class="usa-link">Table 1</a> comprehensively illustrates the architectural differences between the proposed LKDA-Net and existing models from multiple perspectives. Transformer-based models (e.g., Swin-Unet,nnFormer ) rely on self-attention mechanisms with quadratic computational complexity <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e002"><math id="M2" display="inline" overflow="linebreak"><mrow><mtext mathvariant="italic">O</mtext><mo stretchy="false">(</mo><msup><mtext mathvariant="italic">N</mtext><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></math></span>, which hinders efficient and continuous processing of high-resolution 3D volumetric data. CNN-Transformer hybrid models (e.g., TransUNet, UNETR, Swin-UNETR) utilize Transformers for global context modeling and CNNs for local feature extraction. However, they underutilize the inherent inductive bias of convolutions and introduce redundant cross-channel parameters through MLP modules. Meanwhile, large-kernel convolutional models (e.g., MedNext) capture global information via expansive kernels but fail to explicitly model channel-wise and spatial dependencies, resulting in suboptimal segmentation performance for complex small targets. In contrast, the proposed LKDA-Net addresses these limitations through three key innovations: (1) Large Kernel Depthwise Convolution Attention (LKD Attention) enables global context modeling with linear complexity <em>O</em>(<em>N</em>), preserving 3D spatial continuity while expanding the effective receptive field; (2) Inverted Bottleneck with Depthwise Convolution Enhancement (DWCA) refines local feature representation by decoupling channel interactions and eliminating redundancy; (3) A Group Convolution-based Skip Connection Fusion Module aligns multi-scale encoder-decoder features, effectively mitigating semantic gap issues caused by resolution mismatches.</p>
<section class="tw xbox font-sm" id="pone.0329806.t001"><h3 class="obj_head">Table 1. Architectural Comparison of LKDA-Net with State-of-the-Art Models.</h3>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Model</th>
<th align="left" rowspan="1" colspan="1">Global Modeling Strategy</th>
<th align="left" rowspan="1" colspan="1">Local Feature Extraction</th>
<th align="left" rowspan="1" colspan="1">Feature Fusion Strategy</th>
<th align="left" rowspan="1" colspan="1">Parameter Efficiency Optimization</th>
<th align="left" rowspan="1" colspan="1">Core Differences vs LKDA-Net</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Swin-Unet</td>
<td align="left" rowspan="1" colspan="1">Windowed Self-Attention (W-MSA)</td>
<td align="left" rowspan="1" colspan="1">Multilayer Perceptron (MLP)</td>
<td align="left" rowspan="1" colspan="1">Standard Skip Connections</td>
<td align="left" rowspan="1" colspan="1">Window Partitioning for Reduced Computation</td>
<td align="left" rowspan="1" colspan="1">High computational complexity, relies on self-attention</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">nnFormer</td>
<td align="left" rowspan="1" colspan="1">Hierarchical Self-Attention (LV-MSA)</td>
<td align="left" rowspan="1" colspan="1">MLP</td>
<td align="left" rowspan="1" colspan="1">Multi-scale Feature Concatenation</td>
<td align="left" rowspan="1" colspan="1">Optimized Attention for Efficiency</td>
<td align="left" rowspan="1" colspan="1">Large parameter count, long training time</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">TransUNet</td>
<td align="left" rowspan="1" colspan="1">Transformer Decoder</td>
<td align="left" rowspan="1" colspan="1">CNN Encoder</td>
<td align="left" rowspan="1" colspan="1">Hybrid CNN-Transformer Feature Fusion</td>
<td align="left" rowspan="1" colspan="1">Feature Map Downsampling</td>
<td align="left" rowspan="1" colspan="1">Underutilized convolutional inductive bias</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">UNETR</td>
<td align="left" rowspan="1" colspan="1">Pure Transformer Encoder</td>
<td align="left" rowspan="1" colspan="1">3D Convolutional Decoder</td>
<td align="left" rowspan="1" colspan="1">Cross-scale Feature Concatenation</td>
<td align="left" rowspan="1" colspan="1">Reduced Input Resolution for Efficiency</td>
<td align="left" rowspan="1" colspan="1">High memory consumption, poor scalability for high-resolution data</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Swin-UNETR</td>
<td align="left" rowspan="1" colspan="1">3D Swin Transformer (Shifted Window)</td>
<td align="left" rowspan="1" colspan="1">Depthwise Conv + MLP</td>
<td align="left" rowspan="1" colspan="1">Standard Skip Connections</td>
<td align="left" rowspan="1" colspan="1">Hierarchical Downsampling + Window Attention</td>
<td align="left" rowspan="1" colspan="1">Still relies on high-complexity Transformer operations</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MedNext</td>
<td align="left" rowspan="1" colspan="1">Large-kernel Convolution for Receptive Field</td>
<td align="left" rowspan="1" colspan="1">MLP</td>
<td align="left" rowspan="1" colspan="1">Multi-path Feature Fusion</td>
<td align="left" rowspan="1" colspan="1">Dynamic Kernel Size Adjustment</td>
<td align="left" rowspan="1" colspan="1">No explicit channel/spatial dependency modeling</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>LKDA-Net (Ours)</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>Large Kernel Depthwise Conv Attention (LKD Attention)</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>Inverted Bottleneck with Depthwise Conv Augmentation (DWCA)</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>Skip Fusion Module (Grouped Convolution)</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>Depthwise Separable Conv + Channel-wise Compression</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>Balanced lightweight design, effective global-local fusion</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329806.t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section id="sec008"><h3 class="pmc_sec_title">LKDA-Net block</h3>
<p>Inspired by ConvNeXt and Swin Transformers, we propose the LKDA-Net Block for 3D medical image segmentation. As directly applying transformers as a universal backbone has the problem of high computational complexity, we propose to simulate the self-attention mechanism of transformers for global relationship modeling based on the large kernel depthwise convolution attention mechanism, so as to efficiently extract global features. Furthermore, we have designed an efficient 3D medical image segmentation network. <a href="#pone.0329806.g002" class="usa-link">Fig 2</a> compares the differences between the Swin Transformer Block and our proposed LKDA-Net Block. The distinct designs of the LKDA-Net Block mainly include Large Kernel Depthwise Convolution Attention (LKD Attention) and Inverted Bottleneck with Depthwise Convolution Augmentation (DWCA).</p>
<figure class="fig xbox font-sm" id="pone.0329806.g002"><h4 class="obj_head">Fig 2. (a) Architecture of the Swin Transformer Block.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334030_pone.0329806.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5e7d/12334030/664f288464e3/pone.0329806.g002.jpg" loading="lazy" height="456" width="658" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329806.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>(b) Architecture of the LKDA-Net Block proposed by us. Compared with the Swin Transformer block, the LKDA-Net Block obtains a larger receptive field through Large Kernel Depthwise Convolution Attention (LKD Attention) and Inverted Bottleneck with Depthwise Convolutional Augmentation (DWCA), while also improving the quality of feature representation.</p></figcaption></figure><section id="sec009"><h4 class="pmc_sec_title">Large Kernel Depthwise Convolution Attention (LKD Attention).</h4>
<p>Swin Transformer employs Window-based Multi-head Self-Attention (W-MSA) to capture local dependencies and further utilizes Shifted Window MSA to explore the dependencies among different windows, thereby obtaining a global receptive field. We have found that there is a significant similarity between the per-channel computation of Depthwise Convolution and the weighted summation of self-attention. We believe that using Large Kernel Depthwise Convolution Attention (LKD Attention) can acquire a large receptive field just like MSA does, thus capturing global dependencies.</p>
<p>The LKD Attention proposed by us adaptively utilizes channel-level and spatial-level contextual information through a large receptive field. Specifically, multiple large kernel depthwise convolutions are employed to extract multi-scale features. Moreover, we cascade these large kernel depthwise convolutions, endowing them with increasing dilation rates and growing kernel sizes. On the one hand, this design can recursively aggregate contextual information within the receptive field. On the other hand, the features extracted within deeper and larger receptive fields contribute more to the output, enabling the LKD Attention to capture more effective features.</p>
<p>The LKD Attention is shown in <a href="#pone.0329806.g003" class="usa-link">Fig 3</a>. In the specific operations of the LKD Attention, we first utilize a <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e003"><math id="M3" display="inline" overflow="linebreak"><mrow><mn>1</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>1</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>1</mn></mrow></math></span> convolutional layer to conduct a projection operation that halves the number of channels, aiming to reduce the complexity caused by multiple convolutions. The input feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e004"><math id="M4" display="inline" overflow="linebreak"><mrow><msubsup><mi>X</mi><mrow><mi>i</mi><mi>n</mi></mrow><mrow><mi>l</mi></mrow></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>H</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>W</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>D</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>C</mi></mrow></msup></mrow></math></span> of <em>l</em> layer is projected to the layer <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e005"><math id="M5" display="inline" overflow="linebreak"><mrow><msup><mi>X</mi><mrow><mi>l</mi></mrow></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>H</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>W</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>D</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>C</mi></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></msup></mrow></math></span> (where <em>C</em> represents the number of channels, and <em>H</em>, <em>W</em>, <em>D</em> are the dimensions of the 3D image). Secondly, we perform depthwise convolution operations (DW Conv) with two large kernels respectively on the projected feature maps. The first one is a depthwise convolution with a dilation rate of 1 and a kernel size of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e006"><math id="M6" display="inline" overflow="linebreak"><mrow><mn>5</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>5</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>5</mn></mrow></math></span>, and the second one is a depthwise convolution with a dilation rate of 3 and a kernel size of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e007"><math id="M7" display="inline" overflow="linebreak"><mrow><mn>7</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>7</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>7</mn></mrow></math></span>. The large convolutional kernels can capture the global feature dependencies within a local region, simulating the window-level self-attention calculation in transformers. Meanwhile, the channel independence of depthwise convolutions is also similar to the operations of self-attention on each patch.</p>
<figure class="fig xbox font-sm" id="pone.0329806.g003"><h5 class="obj_head">Fig 3. Details of the LKD Attention within the proposed LKDA-Net Block.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334030_pone.0329806.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5e7d/12334030/26e39c241eb1/pone.0329806.g003.jpg" loading="lazy" height="342" width="739" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329806.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><table class="disp-formula p" id="pone.0329806.e008"><tr>
<td class="formula"><math id="M8" display="block" overflow="linebreak"><mrow><mrow><msub><mi>X</mi><mrow><mi>l</mi></mrow></msub><mo>=</mo><mtext>Project</mtext><mo stretchy="false">(</mo><msubsup><mi>X</mi><mrow><mi>i</mi><mi>n</mi></mrow><mrow><mi>l</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(1)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e009"><tr>
<td class="formula"><math id="M9" display="block" overflow="linebreak"><mrow><mrow><msubsup><mi>X</mi><mrow><mn>1</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo>=</mo><mtext>DW Conv</mtext><mo stretchy="false">(</mo><mn>5</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mi>l</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(2)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e010"><tr>
<td class="formula"><math id="M10" display="block" overflow="linebreak"><mrow><mrow><msubsup><mi>X</mi><mrow><mn>2</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo>=</mo><mtext>DW Conv</mtext><mo stretchy="false">(</mo><mn>7</mn><mo>,</mo><mn>3</mn><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msubsup><mi>X</mi><mrow><mn>1</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(3)</td>
</tr></table>
<p>By cascading the depthwise convolutions with two large kernels, the LKD Attention can obtain an effective receptive field with a size of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e011"><math id="M11" display="inline" overflow="linebreak"><mrow><mn>23</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>23</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>23</mn></mrow></math></span>. The two resulting feature maps <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e012"><math id="M12" display="inline" overflow="linebreak"><mrow><msubsup><mi>X</mi><mrow><mn>1</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>H</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>W</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>D</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>C</mi></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></msup></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e013"><math id="M13" display="inline" overflow="linebreak"><mrow><msubsup><mi>X</mi><mrow><mn>2</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>H</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>W</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>D</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>C</mi></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></msup></mrow></math></span> are concatenated to restore the original number of channels, obtaining the feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e014"><math id="M14" display="inline" overflow="linebreak"><mrow><msubsup><mi>X</mi><mrow><mn>3</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>H</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>W</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>D</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>C</mi></mrow></msup></mrow></math></span>. Then, the global spatial relationships of these features are effectively modeled by applying average pooling and maximum pooling along the channels of the feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e015"><math id="M15" display="inline" overflow="linebreak"><mrow><msubsup><mi>X</mi><mrow><mn>3</mn></mrow><mrow><mi>l</mi></mrow></msubsup></mrow></math></span>.</p>
<table class="disp-formula p" id="pone.0329806.e016"><tr>
<td class="formula"><math id="M16" display="block" overflow="linebreak"><mrow><mrow><msubsup><mi>X</mi><mrow><mn>3</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo>=</mo><mtext>Concat</mtext><mo stretchy="false">(</mo><mo stretchy="false">[</mo><msubsup><mi>X</mi><mrow><mi>l</mi></mrow><mrow><mn>1</mn></mrow></msubsup><mi>;</mi><msubsup><mi>X</mi><mrow><mi>l</mi></mrow><mrow><mn>2</mn></mrow></msubsup><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(4)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e017"><tr>
<td class="formula"><math id="M17" display="block" overflow="linebreak"><mrow><mrow><msub><mi>w</mi><mrow><mi>a</mi><mi>v</mi><mi>p</mi></mrow></msub><mo>=</mo><mtext>Average Pooling</mtext><mo stretchy="false">(</mo><msubsup><mi>X</mi><mrow><mn>3</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(5)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e018"><tr>
<td class="formula"><math id="M18" display="block" overflow="linebreak"><mrow><mrow><msub><mi>w</mi><mrow><mi>m</mi><mi>a</mi><mi>p</mi></mrow></msub><mo>=</mo><mtext>Maximum pooling</mtext><mo stretchy="false">(</mo><msubsup><mi>X</mi><mrow><mn>3</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(6)</td>
</tr></table>
<p>Then, a depthwise convolution with a size of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e019"><math id="M19" display="inline" overflow="linebreak"><mrow><mn>5</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>5</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>5</mn></mrow></math></span> is used to mix and interact the information obtained in the previous step among different spatial feature representations. Finally, the Sigmoid activation function is employed to obtain the weight values <em>w</em><sub>1</sub> and <em>w</em><sub>2</sub>.</p>
<table class="disp-formula p" id="pone.0329806.e020"><tr>
<td class="formula"><math id="M20" display="block" overflow="linebreak"><mo>[</mo><msub><mi>w</mi><mn>1</mn></msub><mo>;</mo><mi> </mi><msub><mi>w</mi><mn>2</mn></msub><mo>]</mo><mo>=</mo><mtext>Sigmoid</mtext><mo>(</mo><mtext>Conv</mtext><mn>5</mn><mo>(</mo><mo>[</mo><msub><mi>w</mi><mrow><mi>a</mi><mi>v</mi><mi>p</mi></mrow></msub><mo>;</mo><mi> </mi><msub><mi>w</mi><mrow><mi>m</mi><mi>a</mi><mi>p</mi></mrow></msub><mo>]</mo><mo>)</mo><mo>)</mo></math></td>
<td class="label">(7)</td>
</tr></table>
<p>These weight values are utilized to adaptively select the features from different large kernels and calibrate them to obtain <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e021"><math id="M21" display="inline" overflow="linebreak"><mrow><msubsup><mi>X</mi><mrow><mn>4</mn></mrow><mrow><mi>l</mi></mrow></msubsup></mrow></math></span>.</p>
<table class="disp-formula p" id="pone.0329806.e022"><tr>
<td class="formula"><math id="M22" display="block" overflow="linebreak"><mrow><mrow><msubsup><mi>X</mi><mrow><mn>4</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo>=</mo><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mn>1</mn></mrow></msub><mo>⊗</mo><msubsup><mi>X</mi><mrow><mn>1</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo stretchy="false">)</mo><mo>⊕</mo><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mn>2</mn></mrow></msub><mo>⊗</mo><msubsup><mi>X</mi><mrow><mn>2</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(8)</td>
</tr></table>
<p>Subsequently, after performing a <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e023"><math id="M23" display="inline" overflow="linebreak"><mrow><mn>1</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>1</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>1</mn></mrow></math></span> convolution operation on the obtained <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e024"><math id="M24" display="inline" overflow="linebreak"><mrow><msubsup><mi>X</mi><mrow><mn>4</mn></mrow><mrow><mi>l</mi></mrow></msubsup></mrow></math></span>, a <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e025"><math id="M25" display="inline" overflow="linebreak"><mrow><mn>5</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>5</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>5</mn></mrow></math></span> SE module is used to weight the feature map, thereby explicitly modeling the interdependencies among the channels of its convolutional features to improve the quality of the feature map representation. Finally, the residual connection is utilized to generate the output feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e026"><math id="M26" display="inline" overflow="linebreak"><mrow><msubsup><mi>X</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow><mrow><mi>l</mi></mrow></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>H</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>W</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>D</mi><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mi>C</mi></mrow></msup></mrow></math></span>.</p>
<table class="disp-formula p" id="pone.0329806.e027"><tr>
<td class="formula"><math id="M27" display="block" overflow="linebreak"><mrow><mrow><msub><mi>w</mi><mrow><mn>3</mn></mrow></msub><mo>=</mo><mtext>Conv</mtext><mn>1</mn><mo stretchy="false">(</mo><mtext>SE</mtext><mo stretchy="false">(</mo><msubsup><mi>X</mi><mrow><mn>4</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(9)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e028"><tr>
<td class="formula"><math id="M28" display="block" overflow="linebreak"><mrow><mrow><msubsup><mi>X</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow><mrow><mi>l</mi></mrow></msubsup><mo>=</mo><msub><mi>w</mi><mrow><mn>3</mn></mrow></msub><mo>⊗</mo><msubsup><mi>X</mi><mrow><mn>4</mn></mrow><mrow><mi>l</mi></mrow></msubsup><mo>+</mo><msubsup><mi>X</mi><mrow><mi>i</mi><mi>n</mi></mrow><mrow><mi>l</mi></mrow></msubsup></mrow></mrow></math></td>
<td class="label">(10)</td>
</tr></table></section><section id="sec010"><h4 class="pmc_sec_title">Inverted Bottleneck with Depthwise Convolution Augmentation (DWCA).</h4>
<p>In the Transformer structure, an inverted bottleneck is designed, that is, the dimension of the hidden layer in the MLP module is four times wider than the input dimension. MobileNetV2 [<a href="#pone.0329806.ref047" class="usa-link" aria-describedby="pone.0329806.ref047">47</a>] first applied the inverted bottleneck structure to Convnet, and then several advanced Convnet models also adopted a similar design. Therefore, we adopt a similar inverted bottleneck structure and use the Depthwise Convolution with a size of 1 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e029"><math id="M29" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 1 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e030"><math id="M30" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 1 to process the features. Inspired by vision transformers, instead of using the batch normalization (BN) commonly used in Convnet, we use Layer Normalization (LN) for the normalization operation. In addition, we replace the RELU activation function with the smoother GELU activation function.</p>
<p>In the Inverted Bottleneck structure we proposed, the features passing through the Layer Normalization layer are first expanded to four times the number of input channels through Depthwise Convolution. After the result is processed by the GELU activation function, the Depthwise Convolution with a size of 1 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e031"><math id="M31" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 1 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e032"><math id="M32" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 1 is then used to independently scale each channel feature back to the original number of input channels. Subsequently, a residual connection is made with the original feature map to obtain the output of the Inverted Bottleneck. We expand and compress the dimensions of each channel in an independent manner, which reduces the redundancy among channels while enhancing the feature expression ability. Therefore, we define the outputs of <em>l</em> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e033"><math id="M33" display="inline" overflow="linebreak"><mrow><mi>l</mi><mspace width="0.167em"></mspace><mrow><mo>+</mo></mrow><mspace width="0.167em"></mspace><mn>1</mn></mrow></math></span> layer of the LKDA-Net Block as follows:</p>
<table class="disp-formula p" id="pone.0329806.e034"><tr>
<td class="formula"><math id="M34" display="block" overflow="linebreak"><mrow><mrow><msup><mover><mrow><mi>z</mi></mrow><mo stretchy="false">^</mo></mover><mi>l</mi></msup><mo>=</mo><msup><mi>z</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>+</mo><mtext>DW Conv</mtext><mo stretchy="false">(</mo><mtext>LN</mtext><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mtext>SE</mtext><mo stretchy="false">(</mo><mtext>LN</mtext><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(11)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e035"><tr>
<td class="formula"><math id="M35" display="block" overflow="linebreak"><mrow><mrow><msup><mi>z</mi><mi>l</mi></msup><mo>=</mo><msup><mover><mrow><mi>z</mi></mrow><mo stretchy="false">^</mo></mover><mi>l</mi></msup><mo>+</mo><mtext>DWCA</mtext><mo stretchy="false">(</mo><mtext>LN</mtext><mo stretchy="false">(</mo><msup><mover><mrow><mi>z</mi></mrow><mo stretchy="false">^</mo></mover><mi>l</mi></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(12)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e036"><tr>
<td class="formula"><math id="M36" display="block" overflow="linebreak"><mrow><mrow><msup><mover><mrow><mi>z</mi></mrow><mo stretchy="false">^</mo></mover><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><msup><mi>z</mi><mrow><mi>l</mi></mrow></msup><mo>+</mo><mtext>DW Conv</mtext><mo stretchy="false">(</mo><mtext>LN</mtext><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mi>l</mi></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mtext>SE</mtext><mo stretchy="false">(</mo><mtext>LN</mtext><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(13)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e037"><tr>
<td class="formula"><math id="M37" display="block" overflow="linebreak"><mrow><mrow><msup><mi>z</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><msup><mover><mrow><mi>z</mi></mrow><mo stretchy="false">^</mo></mover><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>+</mo><mtext>DWCA</mtext><mo stretchy="false">(</mo><mtext>LN</mtext><mo stretchy="false">(</mo><msup><mover><mrow><mi>z</mi></mrow><mo stretchy="false">^</mo></mover><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(14)</td>
</tr></table>
<p>where prediction <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e038"><math id="M38" display="inline" overflow="linebreak"><mrow><msup><mtext mathvariant="italic">z</mtext><mrow><mtext mathvariant="italic">l</mtext></mrow></msup></mrow></math></span> and prediction <em>z</em><sup><em>l</em> + 1</sup> are the outputs of different depth layers of DWConv. SE represents Squeeze-and-Excitation block. LN denotes layer normalization, and DWCA represents Inverted Bottleneck with Depthwise Convolution Augmentation.</p></section></section><section id="sec011"><h3 class="pmc_sec_title">LKDA-Net encoder</h3>
<p>Take a 3D voxel data from the training set as the input of the encoder. Our encoder is divided into five stages. In the first stage, instead of adopting the method of linear projection embedding, we use a Depthwise Convolution layer with a size of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e039"><math id="M39" display="inline" overflow="linebreak"><mrow><mn>7</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>7</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>7</mn></mrow></math></span> to perform patch embedding, obtaining a feature map with a resolution of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e040"><math id="M40" display="inline" overflow="linebreak"><mrow><mfrac><mrow><mi>H</mi></mrow><mrow><mn>2</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>W</mi></mrow><mrow><mn>2</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>D</mi></mrow><mrow><mn>2</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>48</mn></mrow></math></span>. This patch-based feature extraction aims to simulate the operation of first performing patch segmentation and then learning embeddings in visual transformers. Compared with the global self-attention in transformers which has high computational cost, our design provides an efficient alternative to capture global dependencies within local regions. The remaining four stages are each composed of the LKDA-Net Block and downsampling. In the LKDA-Net Block, the Large Kernel Depthwise Convolution Attention is used to model global dependencies, followed by the Inverted Bottleneck with Depthwise Convolutional Augmentation structure which expands and compresses the dimensions of each channel in an independent manner, enhancing the feature expression ability while reducing the redundancy among channels. Instead of using MLP, we use a 3D convolution with a stride of 2 and a convolution kernel size of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e041"><math id="M41" display="inline" overflow="linebreak"><mrow><mn>2</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>2</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>2</mn></mrow></math></span> to halve the resolution of the feature map so that the information of each channel can be better fused. The third, fourth, and fifth stages follow the same operations, resulting in feature map resolutions of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e042"><math id="M42" display="inline" overflow="linebreak"><mrow><mfrac><mrow><mi>H</mi></mrow><mrow><mn>8</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>W</mi></mrow><mrow><mn>8</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>D</mi></mrow><mrow><mn>8</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>192</mn></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e043"><math id="M43" display="inline" overflow="linebreak"><mrow><mfrac><mrow><mi>H</mi></mrow><mrow><mn>16</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>W</mi></mrow><mrow><mn>16</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>D</mi></mrow><mrow><mn>16</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>384</mn></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e044"><math id="M44" display="inline" overflow="linebreak"><mrow><mfrac><mrow><mi>H</mi></mrow><mrow><mn>32</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>W</mi></mrow><mrow><mn>32</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><mi>D</mi></mrow><mrow><mn>32</mn></mrow></mfrac><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>768</mn></mrow></math></span>, respectively. Multiple-scale hierarchical feature representations are extracted at each stage and are further utilized in the segmentation of 3D voxel data.</p></section><section id="sec012"><h3 class="pmc_sec_title">LKDA-Net decoder</h3>
<p>Our decoder utilizes the Skip Connection Fusion Module to further extract hierarchical visual feature representations and optimize the segmentation results. Specifically, firstly, the feature outputs of each encoder stage are concatenated and fused with the upsampled decoding features. Secondly, the results of multi-step fusion are input into a convolutional layer with a softmax activation function to predict the final segmentation probabilities. The application of the Skip Connection Fusion Module in the decoder enables our model to align the semantic information of features at different resolutions, so as to optimize the segmentation of fine-grained volumetric data.</p>
<section id="sec013"><h4 class="pmc_sec_title">Skip connection fusion module.</h4>
<p>Traditional skip connections usually utilize ordinary convolution operations for feature fusion. Such operations can be straightforward and lack flexibility and pertinence during the processing, causing the encoder and decoder to bear more computational loads and data processing pressures. However, the skip connection fusion module we propose adopts group convolution as a key component to effectively address these problems. As shown in <a href="#pone.0329806.g004" class="usa-link">Fig 4</a>, we divide the convolution operation into two groups. One group is specifically dedicated to the refined extraction of “feature-to-feature” for the features from the encoder in the skip connection, while the other group is responsible for carrying out the same operation on the decoder features after upsampling. Here, the kernel size of the group convolution is set to 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e045"><math id="M45" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e046"><math id="M46" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3, the stride is 1, and the padding is 1. In order to achieve a more sufficient and comprehensive feature fusion effect, we add two inverted bottleneck pointwise convolutions after the group convolution operation is completed. The skip connection fusion module can reasonably and adaptively allocate the features before fusion to the group convolution for processing according to the characteristics of the features themselves. Subsequently, the highly efficient and dense pointwise convolutions play a crucial role in the entire feature fusion process and undertake the main task of feature fusion.</p>
<figure class="fig xbox font-sm" id="pone.0329806.g004"><h5 class="obj_head">Fig 4. The structure of the Skip Connection Fusion Module.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5e7d/12334030/df380a992091/pone.0329806.g004.jpg" loading="lazy" height="1303" width="629" alt="Fig 4"></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329806.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The module employs group convolution to separately process encoder and decoder features, followed by inverted bottleneck pointwise convolutions for adaptive fusion.</p></figcaption></figure><p>Moreover, inside the skip connection fusion module, each convolution operation is followed by a GELU activation function layer and a BatchNorm normalization layer, so as to further optimize the feature processing effect and enhance the stability and adaptability of the module. The definition of the Skip Connection Fusion Module Block is as follows:</p>
<table class="disp-formula p" id="pone.0329806.e047"><tr>
<td class="formula"><math id="M47" display="block" overflow="linebreak"><mrow><mrow><msub><mi>f</mi><mrow><mi>c</mi></mrow></msub><mo>=</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi><mo stretchy="false">(</mo><mi>B</mi><mi>N</mi><mo stretchy="false">(</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><msub><mi>f</mi><mi>E</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>,</mo><mi>B</mi><mi>N</mi><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><msub><mi>f</mi><mi>D</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(15)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e048"><tr>
<td class="formula"><math id="M48" display="block" overflow="linebreak"><mrow><mrow><msubsup><mi>f</mi><mrow><mi>f</mi><mi>u</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><mi>′</mi></msubsup><mo>=</mo><mi>B</mi><mi>N</mi><mo stretchy="false">(</mo><msub><mi>σ</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>P</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><msub><mi>f</mi><mrow><mi>c</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(16)</td>
</tr></table>
<table class="disp-formula p" id="pone.0329806.e049"><tr>
<td class="formula"><math id="M49" display="block" overflow="linebreak"><mrow><mrow><msub><mi>f</mi><mrow><mi>f</mi><mi>u</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msub><mo>=</mo><mi>B</mi><mi>N</mi><mo stretchy="false">(</mo><msub><mi>σ</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>P</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><msubsup><mrow><mi>f</mi></mrow><mrow><mi>f</mi><mi>u</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><mi>′</mi></msubsup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(17)</td>
</tr></table>
<p>where <em>f</em><sub><em>E</em></sub> and <em>f</em><sub><em>D</em></sub> are the features of the encoder and the decoder respectively. <em>f</em><sub><em>c</em></sub> is the concatenated feature, and <em>f</em><sub><em>fusion</em></sub> is the output fused feature map of the Skip Connection Fusion Module.</p></section><section id="sec014"><h4 class="pmc_sec_title">Upsampling block.</h4>
<p>The Upsampling Block is mainly constituted by an upsampling layer, a convolution layer, a batch normalization layer, and a ReLU activation function. Specifically, for the upsampling process, we employ the bilinear interpolation method to upscale the feature map by a factor of 2. This technique is beneficial as it can enhance the resolution of the feature map in a relatively smooth and efficient manner. The convolution layer within the block has a kernel size of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e050"><math id="M50" display="inline" overflow="linebreak"><mrow><mn>3</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>3</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>3</mn></mrow></math></span>, a stride of 1, and a padding of 1. Such convolution settings enable the extraction and refinement of local features. Through the combined utilization of these techniques, namely the bilinear interpolation, the convolution operation, the batch normalization, and the ReLU activation, we are capable of not only effectively increasing the resolution of the feature map but also retaining the essential features.</p></section></section><section id="sec015"><h3 class="pmc_sec_title">Loss function</h3>
<p>To calculate the loss between the predicted 3D voxels and the ground truth, we utilize a combination of cross-entropy loss and soft dice loss, leveraging the advantages of both loss functions. The loss function for MedX-Net is defined as follows:</p>
<table class="disp-formula p" id="pone.0329806.e051"><tr>
<td class="formula"><math id="M51" display="block" overflow="linebreak"><mrow><mrow><mrow><mi>ℒ</mi><mo stretchy="false">(</mo><mi>Z</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></msubsup><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mfrac><mrow><mn>2</mn><mo>*</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></msubsup><msub><mi>Z</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>·</mo><msub><mi>P</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></msubsup><msubsup><mi>Z</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mn>2</mn></msubsup><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></msubsup><msubsup><mi>P</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mn>2</mn></msubsup></mrow></mfrac><mo fence="true" form="postfix" stretchy="true"></mo></mrow><mrow><mo fence="true" form="prefix" stretchy="true"></mo><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></msubsup><msub><mi>Z</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mi>log</mi><msub><mi>P</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="label">(18)</td>
</tr></table>
<p>where <em>i</em> represents the total number of 3D voxels, and <em>N</em> is the number of predicted classes. <em>Z</em><sub><em>i</em>,<em>j</em></sub> denotes the ground truth value of class j at voxel i, while <em>P</em><sub><em>i</em>,<em>j</em></sub> represents the predicted probability output of class j at voxel i by the model.</p></section></section><section id="sec016"><h2 class="pmc_sec_title">Experiments and results</h2>
<section id="sec017"><h3 class="pmc_sec_title">Datasets</h3>
<p>Our method was evaluated on three datasets: Synapse, BTCV, and ACDC, with both quantitative metrics and visual analysis results presented. These datasets cover diverse anatomical regions including abdominal multi-organ and cardiac regions, and incorporate imaging modalities such as CT and MRI, enabling comprehensive validation of the model’s segmentation performance and generalization capabilities. As widely recognized and extensively utilized benchmarks in the medical image segmentation community, these datasets provide a reliable reference standard to ensure the credibility and comparability of our experimental findings.</p>
<p><strong>Synapse Dataset.</strong> The Synapse dataset holds a significant position in the field of medical image analysis. It mainly focuses on CT scan data of abdominal organs. The data is sourced from a specific group of 30 patients. The data partitioning approach is inspired by the concept of TransUnet. Specifically, 18 groups of data are allocated to the training set to facilitate the model’s learning and training procedures, which allows the model to acquire the normal and abnormal feature patterns of abdominal organs. The remaining 12 groups of data function as the test set, aiming to assess the performance and accuracy of the trained model and examine the model’s generalization capability on unseen data. This dataset encompasses the delineation of eight distinct organs, specifically the spleen, left kidney, pancreas, stomach, aorta, liver, gallbladder, and right kidney.</p>
<p><strong>BTCV Dataset.</strong> The BTCV Dataset (Beyond-the-Cranial-Vault Abdominal CT Organ Segmentation) comprises 30 training instances and 20 testing instances. Notably, this dataset not only encompasses the eight organs present in the Synapse dataset but also encompasses structures of the esophagus, inferior vena cava, portal and splenic veins, as well as the right and left adrenal glands.</p>
<p><strong>ACDC Dataset.</strong> The ACDC Dataset (Automated Cardiac Diagnosis Challenge) comprises MRI scan images from numerous patients, featuring a composition of 70 training samples, 10 validation samples, and 20 test samples. Within each MRI image, delineations are made for the myocardium (MYO), right ventricle (RV), and left ventricle (LV) regions.</p></section><section id="sec018"><h3 class="pmc_sec_title">Implementation details</h3>
<p>Our developmental environment consists of Ubuntu 18 and PyTorch 1.12. We engage in training utilizing a singular GeForce RTX 3090 endowed with 24GB. The orchestration employs the AdamW optimizer, coupled with a stipulation of a maximum iteration count amounting to 40000, and an initial learning rate instantiation set at 0.0001. As a preliminary step, all images undergo resampling to conform to a uniform voxel spacing and are subsequently cropped to dimensions of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e052"><math id="M52" display="inline" overflow="linebreak"><mrow><mn>96</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>96</mn><mspace width="0.167em"></mspace><mrow><mo>×</mo></mrow><mspace width="0.167em"></mspace><mn>96</mn></mrow></math></span>. As the training regimen unfolds, a symphony of data augmentation methodologies is invoked. Within this repertoire, the repertoire includes scaling, rotation, luminance and contrast modulations, Gaussian noise infusions, as well as Gaussian blurring. For the purpose of experimental evaluation, the Average DSC is harnessed as the yardstick of assessment. Additionally, we employ the Hausdorff Distance 95% (HD95) metric to measure the maximum boundary error of segmentation results, which further reveals the model’s sensitivity to anatomical boundaries. A lower HD95 value indicates higher agreement between the segmentation boundaries and the ground truth annotations. The ground truth and predicted values are denoted by <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e053"><math id="M53" display="inline" overflow="linebreak"><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>∈</mo><mi>Z</mi></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e054"><math id="M54" display="inline" overflow="linebreak"><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>∈</mo><mi>P</mi></mrow></math></span>, respectively, for a given semantic class <em>i</em>. The ground truth and predicted surface point sets are denoted by <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e055"><math id="M55" display="inline" overflow="linebreak"><mrow><msup><mi>z</mi><mi>′</mi></msup><mo>∈</mo><msup><mi>Z</mi><mi>′</mi></msup></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e056"><math id="M56" display="inline" overflow="linebreak"><mrow><msup><mi>p</mi><mi>′</mi></msup><mo>∈</mo><msup><mi>P</mi><mi>′</mi></msup></mrow></math></span>, respectively. The DSC metric is defined as:</p>
<table class="disp-formula p" id="pone.0329806.e057"><tr>
<td class="formula"><math id="M57" display="block" overflow="linebreak"><mrow><mrow><mrow><mi>D</mi><mi>S</mi><mi>C</mi><mo stretchy="false">(</mo><mi>Z</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mn>2</mn><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>I</mi></mrow></msubsup><msub><mi>z</mi><mrow><mi>i</mi></mrow></msub><msub><mi>p</mi><mrow><mi>i</mi></mrow></msub></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>I</mi></mrow></msubsup><msub><mi>z</mi><mrow><mi>i</mi></mrow></msub><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>I</mi></mrow></msubsup><msub><mi>p</mi><mrow><mi>i</mi></mrow></msub></mrow></mfrac></mrow></mrow></mrow></math></td>
<td class="label">(19)</td>
</tr></table>
<p>During the course of the training endeavor, we incorporated a mechanism of profound supervision. To be specific, we employed the cross-entropy loss function and the Dice loss function as the ultimate measures of loss. The computation of loss was executed by contrasting the upsampled images from distinct resolutions of the decoder with their corresponding ground truth values. Consequently, the ultimate objective function for training materialized as the summation of losses across five distinct resolutions:</p>
<table class="disp-formula p" id="pone.0329806.e058"><tr>
<td class="formula"><math id="M58" display="block" overflow="linebreak"><mrow><mrow><mrow><msub><mi>L</mi><mrow><mtext>all</mtext></mrow></msub><mspace width="0.167em"></mspace><mo>=</mo><mspace width="0.167em"></mspace><msub><mi>α</mi><mn>1</mn></msub><msub><mi>L</mi><mrow><mo stretchy="false">{</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo>,</mo><mi>D</mi><mo stretchy="false">}</mo></mrow></msub><mo>+</mo><msub><mi>α</mi><mn>2</mn></msub><msub><mi>L</mi><mrow><mo stretchy="false">{</mo><mfrac><mrow><mi>H</mi></mrow><mrow><mn>2</mn></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>W</mi></mrow><mrow><mn>2</mn></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>D</mi></mrow><mrow><mn>2</mn></mrow></mfrac><mo stretchy="false">}</mo></mrow></msub><mo>+</mo><msub><mi>α</mi><mn>3</mn></msub><msub><mi>L</mi><mrow><mo stretchy="false">{</mo><mfrac><mrow><mi>H</mi></mrow><mrow><mn>4</mn></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>W</mi></mrow><mrow><mn>4</mn></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>D</mi></mrow><mrow><mn>4</mn></mrow></mfrac><mo stretchy="false">}</mo></mrow></msub><mspace linebreak="newline"></mspace><mo>+</mo><msub><mi>α</mi><mn>4</mn></msub><msub><mi>L</mi><mrow><mo stretchy="false">{</mo><mfrac><mrow><mi>H</mi></mrow><mrow><mn>8</mn></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>W</mi></mrow><mrow><mn>8</mn></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>D</mi></mrow><mrow><mn>8</mn></mrow></mfrac><mo stretchy="false">}</mo></mrow></msub><mo>+</mo><msub><mi>α</mi><mn>5</mn></msub><msub><mi>L</mi><mrow><mo stretchy="false">{</mo><mfrac><mrow><mi>H</mi></mrow><mrow><mn>16</mn></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>W</mi></mrow><mrow><mn>16</mn></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>D</mi></mrow><mrow><mn>16</mn></mrow></mfrac><mo stretchy="false">}</mo></mrow></msub></mrow></mrow></mrow></math></td>
<td class="label">(20)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e059"><math id="M59" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><msub><mi>α</mi><mn>1</mn></msub><mo>,</mo><msub><mi>α</mi><mn>2</mn></msub><mo>,</mo><msub><mi>α</mi><mn>3</mn></msub><mo>,</mo><msub><mi>α</mi><mn>4</mn></msub><mo>,</mo><msub><mi>α</mi><mn>5</mn></msub><mo stretchy="false">)</mo></mrow></math></span> respectively denote the magnitudes of loss weights for distinct resolution features. In this context, a greater weight is assigned to the loss of feature maps with higher resolutions, thus facilitating accelerated convergence and superior segmentation outcomes. We established that <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e060"><math id="M60" display="inline" overflow="linebreak"><mrow><msub><mi>α</mi><mn>2</mn></msub><mo>=</mo><mfrac><mrow><msub><mi>α</mi><mn>1</mn></msub></mrow><mrow><mn>2</mn></mrow></mfrac><mo>,</mo><msub><mi>α</mi><mn>3</mn></msub><mo>=</mo><mfrac><mrow><msub><mi>α</mi><mn>2</mn></msub></mrow><mrow><mn>2</mn></mrow></mfrac><mo>,</mo><msub><mi>α</mi><mn>4</mn></msub><mspace width="0.167em"></mspace><mrow><mo>=</mo></mrow><mspace width="0.167em"></mspace><mfrac><mrow><msub><mi>α</mi><mn>3</mn></msub></mrow><mrow><mn>2</mn></mrow></mfrac><mtext> </mtext><mi>a</mi><mi>n</mi><mi>d</mi><mtext> </mtext><msub><mi>α</mi><mn>5</mn></msub><mo>=</mo><mfrac><mrow><msub><mi>α</mi><mn>4</mn></msub></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></math></span>, with the cumulative summation of all loss weight factors equating to 1.</p></section><section id="sec019"><h3 class="pmc_sec_title">Comparison with state-of-the-arts</h3>
<p>We evaluated the performance of LKDA-Net on three segmentation datasets. These segmentation datasets vary in complexity, the number of structures to be segmented, image modalities (CT, MRI), and spatial and phenotypic heterogeneity. This experimental design emphasizes the experimental effect and generalization ability of LKDA-Net in different segmentation tasks. For a detailed evaluation and comparison, we compared the performance with various recent state-of-the-art (SOTA) segmentation models. These models were trained according to the optimal parameters provided in their papers and evaluated through the same 5-fold cross-validation.</p>
<ul class="list" style="list-style-type:disc">
<li><p>CNN-based methods: U-Net [<a href="#pone.0329806.ref048" class="usa-link" aria-describedby="pone.0329806.ref048">48</a>], nnUNet [<a href="#pone.0329806.ref016" class="usa-link" aria-describedby="pone.0329806.ref016">16</a>].</p></li>
<li><p>Transformer-based methods: Swin-Unet [<a href="#pone.0329806.ref028" class="usa-link" aria-describedby="pone.0329806.ref028">28</a>], MISSFormer [<a href="#pone.0329806.ref030" class="usa-link" aria-describedby="pone.0329806.ref030">30</a>], nnFormer [<a href="#pone.0329806.ref006" class="usa-link" aria-describedby="pone.0329806.ref006">6</a>].</p></li>
<li><p>Hybrid CNN-Transformer-based methods: TransUNet [<a href="#pone.0329806.ref033" class="usa-link" aria-describedby="pone.0329806.ref033">33</a>], UNETR [<a href="#pone.0329806.ref005" class="usa-link" aria-describedby="pone.0329806.ref005">5</a>], swin-UNETR [<a href="#pone.0329806.ref009" class="usa-link" aria-describedby="pone.0329806.ref009">9</a>].</p></li>
<li><p>Large convolutional kernel-based methods: 3D UX-NET [<a href="#pone.0329806.ref002" class="usa-link" aria-describedby="pone.0329806.ref002">2</a>], MedNext [<a href="#pone.0329806.ref011" class="usa-link" aria-describedby="pone.0329806.ref011">11</a>].</p></li>
</ul>
<p><a href="#pone.0329806.t002" class="usa-link">Table 2</a>, <a href="#pone.0329806.t003" class="usa-link">Table 3</a> and <a href="#pone.0329806.t004" class="usa-link">Table 4</a> respectively present the performance comparisons on the Synapse multi-organ segmentation dataset, the Automated Cardiac Diagnosis dataset and the BTCV abdominal multi-organ segmentation task. When evaluating the model’s efficiency metrics, we conducted experiments under the same experimental environment and preprocessing pipeline as during the training phase. <a href="#pone.0329806.t005" class="usa-link">Table 5</a> presents the comparative results of parameter counts and computational costs across different models, where the inference time corresponds to the total time required by the model to segment 12 test samples from the Synapse dataset.</p>
<section class="tw xbox font-sm" id="pone.0329806.t002"><h4 class="obj_head">Table 2. The performance comparison between LKDA-Net and other existing methods on the Synapse dataset. The bold data are the optimal ones in single-organ segmentation. Note: Spl: spleen, RKid: right kidney, LKid: left kidney, Gal: gallbladder, Eso: esophagus, Liv: liver, Sto: stomach, Aor: aorta IVC: inferior vena cava, PSV: portal and splenic veins, Pan: pancreas.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Methods</th>
<th align="left" colspan="8" rowspan="1">Synapse</th>
<th align="left" colspan="2" rowspan="1">Average</th>
</tr>
<tr>
<th align="left" rowspan="1" colspan="1"></th>
<th align="left" rowspan="1" colspan="1">Spl</th>
<th align="left" rowspan="1" colspan="1">RKid</th>
<th align="left" rowspan="1" colspan="1">LKid</th>
<th align="left" rowspan="1" colspan="1">Gal</th>
<th align="left" rowspan="1" colspan="1">Liv</th>
<th align="left" rowspan="1" colspan="1">Sto</th>
<th align="left" rowspan="1" colspan="1">Aor</th>
<th align="left" rowspan="1" colspan="1">Pan</th>
<th align="left" rowspan="1" colspan="1">DSC(%)</th>
<th align="left" rowspan="1" colspan="1">HD95 (mm)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">U-Net</td>
<td align="left" rowspan="1" colspan="1">86.67</td>
<td align="left" rowspan="1" colspan="1">68.60</td>
<td align="left" rowspan="1" colspan="1">77.77</td>
<td align="left" rowspan="1" colspan="1">69.72</td>
<td align="left" rowspan="1" colspan="1">93.43</td>
<td align="left" rowspan="1" colspan="1">75.58</td>
<td align="left" rowspan="1" colspan="1">89.07</td>
<td align="left" rowspan="1" colspan="1">53.98</td>
<td align="left" rowspan="1" colspan="1">76.85</td>
<td align="left" rowspan="1" colspan="1">-</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">TransUNet</td>
<td align="left" rowspan="1" colspan="1">85.08</td>
<td align="left" rowspan="1" colspan="1">77.02</td>
<td align="left" rowspan="1" colspan="1">81.87</td>
<td align="left" rowspan="1" colspan="1">63.16</td>
<td align="left" rowspan="1" colspan="1">94.08</td>
<td align="left" rowspan="1" colspan="1">75.62</td>
<td align="left" rowspan="1" colspan="1">87.23</td>
<td align="left" rowspan="1" colspan="1">55.86</td>
<td align="left" rowspan="1" colspan="1">77.49</td>
<td align="left" rowspan="1" colspan="1">31.69</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Swin-Unet</td>
<td align="left" rowspan="1" colspan="1">90.66</td>
<td align="left" rowspan="1" colspan="1">79.61</td>
<td align="left" rowspan="1" colspan="1">83.28</td>
<td align="left" rowspan="1" colspan="1">66.53</td>
<td align="left" rowspan="1" colspan="1">94.29</td>
<td align="left" rowspan="1" colspan="1">76.60</td>
<td align="left" rowspan="1" colspan="1">85.47</td>
<td align="left" rowspan="1" colspan="1">56.58</td>
<td align="left" rowspan="1" colspan="1">79.13</td>
<td align="left" rowspan="1" colspan="1">21.55</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">UNETR</td>
<td align="left" rowspan="1" colspan="1">85.00</td>
<td align="left" rowspan="1" colspan="1">84.52</td>
<td align="left" rowspan="1" colspan="1">85.60</td>
<td align="left" rowspan="1" colspan="1">56.30</td>
<td align="left" rowspan="1" colspan="1">94.57</td>
<td align="left" rowspan="1" colspan="1">70.46</td>
<td align="left" rowspan="1" colspan="1">89.80</td>
<td align="left" rowspan="1" colspan="1">60.47</td>
<td align="left" rowspan="1" colspan="1">78.35</td>
<td align="left" rowspan="1" colspan="1">18.59</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MISSFormer</td>
<td align="left" rowspan="1" colspan="1">91.92</td>
<td align="left" rowspan="1" colspan="1">82.00</td>
<td align="left" rowspan="1" colspan="1">85.21</td>
<td align="left" rowspan="1" colspan="1">68.65</td>
<td align="left" rowspan="1" colspan="1">94.41</td>
<td align="left" rowspan="1" colspan="1">80.81</td>
<td align="left" rowspan="1" colspan="1">86.99</td>
<td align="left" rowspan="1" colspan="1">65.67</td>
<td align="left" rowspan="1" colspan="1">81.96</td>
<td align="left" rowspan="1" colspan="1">18.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Swin-UNETR</td>
<td align="left" rowspan="1" colspan="1">95.37</td>
<td align="left" rowspan="1" colspan="1">86.26</td>
<td align="left" rowspan="1" colspan="1">86.99</td>
<td align="left" rowspan="1" colspan="1">66.54</td>
<td align="left" rowspan="1" colspan="1">95.72</td>
<td align="left" rowspan="1" colspan="1">77.01</td>
<td align="left" rowspan="1" colspan="1">91.12</td>
<td align="left" rowspan="1" colspan="1">68.80</td>
<td align="left" rowspan="1" colspan="1">83.48</td>
<td align="left" rowspan="1" colspan="1">10.55</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3D UX-NET</td>
<td align="left" rowspan="1" colspan="1">95.01</td>
<td align="left" rowspan="1" colspan="1">85.76</td>
<td align="left" rowspan="1" colspan="1">87.59</td>
<td align="left" rowspan="1" colspan="1">67.34</td>
<td align="left" rowspan="1" colspan="1">94.82</td>
<td align="left" rowspan="1" colspan="1">80.01</td>
<td align="left" rowspan="1" colspan="1">90.72</td>
<td align="left" rowspan="1" colspan="1">80.71</td>
<td align="left" rowspan="1" colspan="1">85.26</td>
<td align="left" rowspan="1" colspan="1">11.23</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">nnFormer</td>
<td align="left" rowspan="1" colspan="1">90.51</td>
<td align="left" rowspan="1" colspan="1">86.25</td>
<td align="left" rowspan="1" colspan="1">86.57</td>
<td align="left" rowspan="1" colspan="1">70.17</td>
<td align="left" rowspan="1" colspan="1">
<strong>96.84</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>86.83</strong>
</td>
<td align="left" rowspan="1" colspan="1">92.04</td>
<td align="left" rowspan="1" colspan="1">
<strong>83.35</strong>
</td>
<td align="left" rowspan="1" colspan="1">86.57</td>
<td align="left" rowspan="1" colspan="1">10.63</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LKDA-Net(Ours)</td>
<td align="left" rowspan="1" colspan="1">
<strong>95.63</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>87.08</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>87.39</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>71.08</strong>
</td>
<td align="left" rowspan="1" colspan="1">96.64</td>
<td align="left" rowspan="1" colspan="1">85.30</td>
<td align="left" rowspan="1" colspan="1">
<strong>92.67</strong>
</td>
<td align="left" rowspan="1" colspan="1">81.86</td>
<td align="left" rowspan="1" colspan="1">
<strong>87.21</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>7.33</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329806.t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section class="tw xbox font-sm" id="pone.0329806.t003"><h4 class="obj_head">Table 3. Comparative performance analysis of our LKDA-Net with other existing methods on ACDC dataset. Best results are in bold. Note: RV: Right ventricle, Myo: Myocardium, LV: Left ventricle.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Methods</th>
<th align="left" colspan="4" rowspan="1">ACDC</th>
</tr>
<tr>
<th align="left" rowspan="1" colspan="1"></th>
<th align="left" rowspan="1" colspan="1">RV</th>
<th align="left" rowspan="1" colspan="1">Myo</th>
<th align="left" rowspan="1" colspan="1">LV</th>
<th align="left" rowspan="1" colspan="1">Average DSC(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">U-Net</td>
<td align="left" rowspan="1" colspan="1">87.10</td>
<td align="left" rowspan="1" colspan="1">80.63</td>
<td align="left" rowspan="1" colspan="1">94.92</td>
<td align="left" rowspan="1" colspan="1">87.55</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">TransUNet</td>
<td align="left" rowspan="1" colspan="1">88.86</td>
<td align="left" rowspan="1" colspan="1">84.54</td>
<td align="left" rowspan="1" colspan="1">95.73</td>
<td align="left" rowspan="1" colspan="1">89.71</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Swin-Unet</td>
<td align="left" rowspan="1" colspan="1">88.55</td>
<td align="left" rowspan="1" colspan="1">85.62</td>
<td align="left" rowspan="1" colspan="1">95.83</td>
<td align="left" rowspan="1" colspan="1">90.00</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">UNETR</td>
<td align="left" rowspan="1" colspan="1">85.29</td>
<td align="left" rowspan="1" colspan="1">86.52</td>
<td align="left" rowspan="1" colspan="1">94.02</td>
<td align="left" rowspan="1" colspan="1">88.61</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MISSFormer</td>
<td align="left" rowspan="1" colspan="1">86.36</td>
<td align="left" rowspan="1" colspan="1">85.75</td>
<td align="left" rowspan="1" colspan="1">91.59</td>
<td align="left" rowspan="1" colspan="1">87.90</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">nnFormer</td>
<td align="left" rowspan="1" colspan="1">90.94</td>
<td align="left" rowspan="1" colspan="1">89.58</td>
<td align="left" rowspan="1" colspan="1">95.65</td>
<td align="left" rowspan="1" colspan="1">92.06</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LKDA-Net(Ours)</td>
<td align="left" rowspan="1" colspan="1">
<strong>91.75</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>90.60</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>96.02</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>92.79</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329806.t003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section class="tw xbox font-sm" id="pone.0329806.t004"><h4 class="obj_head">Table 4. Comparison results on the BTCV dataset. Note: Spl: spleen, RKid: right kidney, LKid: left kidney, Gal: gallbladder, Eso: esophagus, Liv: liver, Sto: stomach, Aor: aorta IVC: inferior vena cava, PSV: portal and splenic veins, Pan: pancreas, RAG: adrenal gland, LAG: Left adrenal gland.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Methods</th>
<th align="left" rowspan="1" colspan="1">Spl</th>
<th align="left" rowspan="1" colspan="1">RKid</th>
<th align="left" rowspan="1" colspan="1">LKid</th>
<th align="left" rowspan="1" colspan="1">Gal</th>
<th align="left" rowspan="1" colspan="1">Eso</th>
<th align="left" rowspan="1" colspan="1">Liv</th>
<th align="left" rowspan="1" colspan="1">Sto</th>
<th align="left" rowspan="1" colspan="1">Aor</th>
<th align="left" rowspan="1" colspan="1">IVC</th>
<th align="left" rowspan="1" colspan="1">PSV</th>
<th align="left" rowspan="1" colspan="1">Pan</th>
<th align="left" rowspan="1" colspan="1">RAG</th>
<th align="left" rowspan="1" colspan="1">LAG</th>
<th align="left" rowspan="1" colspan="1">DSC</th>
<th align="left" rowspan="1" colspan="1">HD95</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">U-Net</td>
<td align="left" rowspan="1" colspan="1">90.68</td>
<td align="left" rowspan="1" colspan="1">82.62</td>
<td align="left" rowspan="1" colspan="1">85.05</td>
<td align="left" rowspan="1" colspan="1">57.33</td>
<td align="left" rowspan="1" colspan="1">70.11</td>
<td align="left" rowspan="1" colspan="1">93.44</td>
<td align="left" rowspan="1" colspan="1">73.14</td>
<td align="left" rowspan="1" colspan="1">84.54</td>
<td align="left" rowspan="1" colspan="1">77.33</td>
<td align="left" rowspan="1" colspan="1">70.17</td>
<td align="left" rowspan="1" colspan="1">65.06</td>
<td align="left" rowspan="1" colspan="1">65.95</td>
<td align="left" rowspan="1" colspan="1">62.25</td>
<td align="left" rowspan="1" colspan="1">75.21</td>
<td align="left" rowspan="1" colspan="1">-</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">nnUNet</td>
<td align="left" rowspan="1" colspan="1">95.95</td>
<td align="left" rowspan="1" colspan="1">88.35</td>
<td align="left" rowspan="1" colspan="1">93.02</td>
<td align="left" rowspan="1" colspan="1">70.13</td>
<td align="left" rowspan="1" colspan="1">76.72</td>
<td align="left" rowspan="1" colspan="1">96.51</td>
<td align="left" rowspan="1" colspan="1">
<strong>86.79</strong>
</td>
<td align="left" rowspan="1" colspan="1">88.93</td>
<td align="left" rowspan="1" colspan="1">82.89</td>
<td align="left" rowspan="1" colspan="1">
<strong>78.51</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>79.60</strong>
</td>
<td align="left" rowspan="1" colspan="1">73.26</td>
<td align="left" rowspan="1" colspan="1">68.35</td>
<td align="left" rowspan="1" colspan="1">83.16</td>
<td align="left" rowspan="1" colspan="1">-</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">TransUNet</td>
<td align="left" rowspan="1" colspan="1">94.55</td>
<td align="left" rowspan="1" colspan="1">89.20</td>
<td align="left" rowspan="1" colspan="1">90.97</td>
<td align="left" rowspan="1" colspan="1">68.38</td>
<td align="left" rowspan="1" colspan="1">75.61</td>
<td align="left" rowspan="1" colspan="1">96.44</td>
<td align="left" rowspan="1" colspan="1">83.52</td>
<td align="left" rowspan="1" colspan="1">88.55</td>
<td align="left" rowspan="1" colspan="1">82.48</td>
<td align="left" rowspan="1" colspan="1">74.21</td>
<td align="left" rowspan="1" colspan="1">76.02</td>
<td align="left" rowspan="1" colspan="1">67.23</td>
<td align="left" rowspan="1" colspan="1">67.03</td>
<td align="left" rowspan="1" colspan="1">81.31</td>
<td align="left" rowspan="1" colspan="1">12.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">UNETR</td>
<td align="left" rowspan="1" colspan="1">90.48</td>
<td align="left" rowspan="1" colspan="1">82.51</td>
<td align="left" rowspan="1" colspan="1">86.05</td>
<td align="left" rowspan="1" colspan="1">58.23</td>
<td align="left" rowspan="1" colspan="1">71.21</td>
<td align="left" rowspan="1" colspan="1">94.64</td>
<td align="left" rowspan="1" colspan="1">72.06</td>
<td align="left" rowspan="1" colspan="1">86.57</td>
<td align="left" rowspan="1" colspan="1">76.51</td>
<td align="left" rowspan="1" colspan="1">70.37</td>
<td align="left" rowspan="1" colspan="1">66.06</td>
<td align="left" rowspan="1" colspan="1">66.25</td>
<td align="left" rowspan="1" colspan="1">63.04</td>
<td align="left" rowspan="1" colspan="1">76.00</td>
<td align="left" rowspan="1" colspan="1">8.82</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Swin-UNETR</td>
<td align="left" rowspan="1" colspan="1">94.59</td>
<td align="left" rowspan="1" colspan="1">88.97</td>
<td align="left" rowspan="1" colspan="1">92.39</td>
<td align="left" rowspan="1" colspan="1">65.37</td>
<td align="left" rowspan="1" colspan="1">75.43</td>
<td align="left" rowspan="1" colspan="1">95.61</td>
<td align="left" rowspan="1" colspan="1">75.57</td>
<td align="left" rowspan="1" colspan="1">88.28</td>
<td align="left" rowspan="1" colspan="1">81.61</td>
<td align="left" rowspan="1" colspan="1">76.30</td>
<td align="left" rowspan="1" colspan="1">74.52</td>
<td align="left" rowspan="1" colspan="1">68.23</td>
<td align="left" rowspan="1" colspan="1">66.02</td>
<td align="left" rowspan="1" colspan="1">80.44</td>
<td align="left" rowspan="1" colspan="1">13.49</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MedNext</td>
<td align="left" rowspan="1" colspan="1">95.59</td>
<td align="left" rowspan="1" colspan="1">88.76</td>
<td align="left" rowspan="1" colspan="1">92.87</td>
<td align="left" rowspan="1" colspan="1">66.42</td>
<td align="left" rowspan="1" colspan="1">75.52</td>
<td align="left" rowspan="1" colspan="1">95.81</td>
<td align="left" rowspan="1" colspan="1">76.87</td>
<td align="left" rowspan="1" colspan="1">88.52</td>
<td align="left" rowspan="1" colspan="1">82.01</td>
<td align="left" rowspan="1" colspan="1">76.21</td>
<td align="left" rowspan="1" colspan="1">76.82</td>
<td align="left" rowspan="1" colspan="1">70.31</td>
<td align="left" rowspan="1" colspan="1">66.53</td>
<td align="left" rowspan="1" colspan="1">80.94</td>
<td align="left" rowspan="1" colspan="1">9.34</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">nnFormer</td>
<td align="left" rowspan="1" colspan="1">94.58</td>
<td align="left" rowspan="1" colspan="1">88.62</td>
<td align="left" rowspan="1" colspan="1">93.68</td>
<td align="left" rowspan="1" colspan="1">65.29</td>
<td align="left" rowspan="1" colspan="1">76.22</td>
<td align="left" rowspan="1" colspan="1">96.17</td>
<td align="left" rowspan="1" colspan="1">83.59</td>
<td align="left" rowspan="1" colspan="1">
<strong>89.09</strong>
</td>
<td align="left" rowspan="1" colspan="1">80.80</td>
<td align="left" rowspan="1" colspan="1">75.97</td>
<td align="left" rowspan="1" colspan="1">77.87</td>
<td align="left" rowspan="1" colspan="1">70.20</td>
<td align="left" rowspan="1" colspan="1">66.05</td>
<td align="left" rowspan="1" colspan="1">81.62</td>
<td align="left" rowspan="1" colspan="1">5.15</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LKDA-Net(Ours)</td>
<td align="left" rowspan="1" colspan="1">
<strong>95.97</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>91.21</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>93.71</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>70.54</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>77.07</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>96.84</strong>
</td>
<td align="left" rowspan="1" colspan="1">86.14</td>
<td align="left" rowspan="1" colspan="1">89.02</td>
<td align="left" rowspan="1" colspan="1">
<strong>83.07</strong>
</td>
<td align="left" rowspan="1" colspan="1">77.92</td>
<td align="left" rowspan="1" colspan="1">78.41</td>
<td align="left" rowspan="1" colspan="1">
<strong>73.45</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>68.64</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>83.23</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>4.85</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329806.t004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section class="tw xbox font-sm" id="pone.0329806.t005"><h4 class="obj_head">Table 5. The comparison of the LKDA-Net model with other models in terms of the number of model parameters, computational complexity (FLOPs) and Inference Time (s).</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Methods</th>
<th align="left" rowspan="1" colspan="1">Params(M)</th>
<th align="left" rowspan="1" colspan="1">FLOPs(G)</th>
<th align="left" rowspan="1" colspan="1">Inference Time (s)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">TransUNet</td>
<td align="left" rowspan="1" colspan="1">96.07</td>
<td align="left" rowspan="1" colspan="1">88.91</td>
<td align="left" rowspan="1" colspan="1">2.15</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">UNETR</td>
<td align="left" rowspan="1" colspan="1">92.49</td>
<td align="left" rowspan="1" colspan="1">75.76</td>
<td align="left" rowspan="1" colspan="1">1.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Swin-UNETR</td>
<td align="left" rowspan="1" colspan="1">62.83</td>
<td align="left" rowspan="1" colspan="1">384.2</td>
<td align="left" rowspan="1" colspan="1">3.72</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">nnFormer</td>
<td align="left" rowspan="1" colspan="1">150.5</td>
<td align="left" rowspan="1" colspan="1">213.4</td>
<td align="left" rowspan="1" colspan="1">4.05</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">nnUNet</td>
<td align="left" rowspan="1" colspan="1">68.38</td>
<td align="left" rowspan="1" colspan="1">357.13</td>
<td align="left" rowspan="1" colspan="1">3.41</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3D UX-NET</td>
<td align="left" rowspan="1" colspan="1">53.01</td>
<td align="left" rowspan="1" colspan="1">632.33</td>
<td align="left" rowspan="1" colspan="1">3.88</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MedNext</td>
<td align="left" rowspan="1" colspan="1">11.65</td>
<td align="left" rowspan="1" colspan="1">178.05</td>
<td align="left" rowspan="1" colspan="1">1.28</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LKDA-Net(Ours)</td>
<td align="left" rowspan="1" colspan="1">
<strong>48.52</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>418.52</strong>
</td>
<td align="left" rowspan="1" colspan="1">1.65</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329806.t005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p><strong>Synapse Dataset:</strong> LKDA-Net achieved an overall average Dice Similarity Coefficient (DSC) of 87.21% on the Synapse Dataset, demonstrating the best and most robust overall performance compared with other state-of-the-art (SOTA) methods. Specifically, compared with the baseline model U-Net based on Convolutional Neural Network (CNN), which achieved an average DSC of 76.85%, LKDA-Net achieved a superior overall segmentation performance. Compared to MedNext, which employs large-kernel pure convolutions to expand the receptive field, its lack of explicit modeling of channel-wise and spatial dependencies limits segmentation accuracy for small targets (e.g., adrenal glands). In contrast, LKDA-Net leverages an Inverted Bottleneck with Depthwise Convolution Enhancement (DWCA ) to enable the model to concentrate on critical anatomical regions. In addition, the average DSC obtained by LKDA-Net was also significantly higher than that of methods based purely on Vision Transformer, such as Swin-Unet, MISSFormer, and nnFormer. Although nnFormer had a lower computational complexity, LKDA-Net achieved better segmentation results while reducing the number of model parameters by two-thirds.</p>
<p>When LKDA-Net was compared with hybrid CNN-Transformer methods that had already reached the state-of-the-art level in various segmentation tasks, it achieved a superior overall performance. Although the UNETR had lower floating-point operations, LKDA-Net achieved a higher average DSC in terms of overall segmentation performance. While Swin-UNETR captures long-range dependencies through windowed self-attention (W-MSA), its computational complexity grows quadratically with input resolution, and its MLP modules introduce redundant cross-channel parameters. In contrast, LKDA-Net’s LKD Attention significantly mitigates boundary ambiguity (<a href="#pone.0329806.g006" class="usa-link">Fig 6</a>) and demonstrates superior segmentation performance without relying on the high computational costs associated with Transformer architectures. 3D UX-NET proposed using large-kernel convolutional modules to replace Transformer modules. However, LKDA-Net achieved superior performance with a much lower computational complexity than it, which proved the effectiveness and efficiency of the LKDA-Net Block and the overall architecture we proposed. While MedNext’s large-kernel pure convolutional design yields lower computational complexity and shorter inference time compared to LKDA-Net, LKDA-Net achieves significantly higher segmentation accuracy. LKDA-Net achieves the lowest HD95 of 7.33 mm, significantly outperforming Swin-UNETR (10.55 mm) and 3D UX-NET (11.23 mm). This indicates our model not only improves volumetric overlap but also reduces extreme segmentation errors at organ boundaries. The average DSC of LKDA-Net in all organ-specific segmentation tasks had been significantly improved, indicating that LKDA-Net demonstrated excellent capabilities in feature extraction and representation from different organs.</p>
<figure class="fig xbox font-sm" id="pone.0329806.g006"><h4 class="obj_head">Fig 6. Comparison results of visualization on the BTCV dataset.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334030_pone.0329806.g006.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5e7d/12334030/5974230279f9/pone.0329806.g006.jpg" loading="lazy" height="394" width="760" alt="Fig 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329806.g006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Zoomed-in boxed regions highlight significant differences in segmentation quality. LKDA-Net exhibits superior contour alignment with the ground truth for small anatomical structures, such as the esophagus (ESO), gallbladder (GAL), and adrenal glands (RAG, LAG), while substantially reducing mis-segmentation artifacts.</p></figcaption></figure><p>It can be seen from <a href="#pone.0329806.t005" class="usa-link">Table 5</a> that the number of parameters of the LKDA-Net model is only 44.52 million. Compared with other pure Transformer models, the number of parameters of our model has been significantly reduced. The implementation of LKD Attention within the LKDA-Net Block substantially reduces the model’s parameter count. However, the computationally intensive nature of LKD Attention results in relatively higher FLOPs for our model. This design ensures efficient segmentation while mitigating the risks of overfitting from excessive model parameters and the overconsumption of computational resources. In terms of inference time, LKDA-Net achieves an end-to-end inference time (encompassing data preprocessing, GPU computation, and data transfer) of 1.65 seconds for segmenting 12 test samples, which is notably faster than Swin-UNETR (3.72 seconds) and nnUNet (3.41 seconds), despite their similar FLOPs. Furthermore, compared to 3D UX-NET and MedNext, which rely on large-kernel pure convolutional operations, these models exhibit comparable parameter counts to LKDA-Net but fall short in overall accuracy and inference efficiency.</p>
<p><strong>ACDC Dataset</strong>: LKDA-Net achieved an overall average DSC of 92.79% on the ACDC dataset, demonstrating outstanding and extremely robust overall performance compared with other state-of-the-art (SOTA) methods. Specifically, compared with the U-Net baseline model based on the Convolutional Neural Network, which achieved an average DSC of 87.55%, LKDA-Net achieved a more excellent overall segmentation performance. In addition, the average DSC of LKDA-Net was significantly higher than that of methods based on pure Vision Transformer, such as Swin-Unet and MISSFormer. Even though nnFormer is currently the best-performing method on the ACDC dataset, LKDA-Net still surpassed it in terms of the overall number of parameters and overall segmentation performance. When LKDA-Net was compared with various hybrid CNN-Transformer methods, it also achieved a superior overall performance. Although models such as TransUNet, Swin-Unet, and UNETR have their own advantages in certain aspects. For example, TransUNet has a relatively low number of parameters and computational complexity, Swin-Unet performs fairly well in some indicators, and UNETR has a lower number of floating-point operations, LKDA-Net achieved a higher average DSC in terms of overall segmentation performance. Moreover, compared with the current mainstream models, the average DSC of LKDA-Net in the segmentation tasks of specific regions such as the right ventricle (RV), myocardium (Myo), and left ventricle (LV) has been significantly improved. This indicates that LKDA-Net demonstrates extremely excellent capabilities in feature extraction and representation from different cardiac structures, and it can divide different regions in cardiac magnetic resonance images more accurately, thereby providing more reliable and precise data support for cardiac-related medical research or clinical diagnosis.</p>
<p><strong>BTCV Dataset</strong>: LKDA-Net also demonstrates excellent segmentation performance on the BTCV dataset. The current excellent pure Convolutional Neural Network model, nnUNet, has an average DSC of 83.16%, which is the best-performing model among all current mainstream models. LKDA-Net has an average DSC of 83.23% on the BTCV dataset and outperforms the nnUNet model with a lower number of model parameters and a higher average DSC. Compared with the method based on pure Vision Transformer, nnFormer, LKDA-Net has a significant advantage in average DSC and can capture the feature information in images more effectively, thus transforming it into more accurate segmentation results. When compared with various hybrid CNN-Transformer methods, LKDA-Net also stands out. While TransUNet integrates CNNs and Transformers, its encoder-decoder framework employs a simplistic fusion strategy (direct concatenation of features), leading to insufficient alignment between shallow local features and deep semantic information. In contrast, LKDA-Net achieves more precise segmentation through a skip-connection fusion module (combining grouped convolutions with inverted bottleneck pointwise convolutions). Although TransUNet and UNETR has relatively lower floating-point operations, LKDA-Net achieves a higher value in overall segmentation performance. Notably, LKDA-Net attains an HD95 of 4.85 mm, surpassing nnFormer (5.15 mm) and MedNext (9.34 mm). The reduced HD95 demonstrates the effectiveness of our skip connection fusion module in preserving anatomical details. When handling various tissue and organ segmentation tasks in the BTCV dataset, LKDA-Net can outline the target regions more accurately, and its overall segmentation effect surpasses that of the current mainstream baseline models.</p>
<p><a href="#pone.0329806.g005" class="usa-link">Fig 5</a> presents the visual comparison between LKDA-Net and other models in the Synapse multi-organ segmentation task. UNETR exhibits discontinuities in segmentation boundaries for the pancreas, Swin-UNETR demonstrates erroneous segmentation in the pancreatic head region, and nnFormer suffers from incomplete segmentation of the kidneys. In contrast, LKDA-Net’s predictions align more closely with the ground truth, particularly in capturing subtle boundary structures, where its contour continuity and spatial consistency significantly outperform competing models. These observations validate the enhanced global context modeling capability of LKDA-Net’s large-kernel depthwise separable convolution attention (LKD Attention).</p>
<figure class="fig xbox font-sm" id="pone.0329806.g005"><h4 class="obj_head">Fig 5. The visual segmentation effect of LKDA-Net on the Synapse dataset.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334030_pone.0329806.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5e7d/12334030/523df0a7dacf/pone.0329806.g005.jpg" loading="lazy" height="375" width="720" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0329806.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The parts where our model outperforms other models are marked in yellow. Compared to other models, LKDA-Net demonstrates higher structural continuity in segmenting complex organ boundaries, such as the pancreas and kidneys, achieving results closer to the ground truth.</p></figcaption></figure><p><a href="#pone.0329806.g006" class="usa-link">Fig 6</a> shows the visual comparison between LKDA-Net and other models in the BTCV abdominal multi-organ segmentation task. Here, nnFormer and nnUNet display incomplete segmentation of the portal splenic vein (PSV) with boundary blurring, while Swin-UNETR fails to accurately reconstruct the morphology of the right adrenal gland (RAG). In contrast, LKDA-Net effectively integrates multi-scale features through its Skip Connection Fusion Module, preserving fine-grained anatomical details to enhance segmentation precision while significantly mitigating edge ambiguity.</p></section><section id="sec020"><h3 class="pmc_sec_title">Ablation experiment</h3>
<p>We conducted ablation studies on the LKDA-Net Block and the Skip Connection Fusion Module respectively. In the ablation studies, we evaluated the parameters and segmentation performance of different architectural configurations on the Synapse dataset. We adopted Swin-UNETR as the baseline model and conducted the following experimental configurations: (1) Replacing the Windowed Multi-Head Self-Attention (W-MSA) in Swin Transformer Blocks with our proposed LKD Attention (3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e061"><math id="M61" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e062"><math id="M62" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3 DWConv + 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e063"><math id="M63" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e064"><math id="M64" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 DWConv). (2) Using LKD Attention with larger kernels (5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e065"><math id="M65" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e066"><math id="M66" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 DWConv + 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e067"><math id="M67" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e068"><math id="M68" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 DWConv). (3) Further scaling kernel sizes in LKD Attention (7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e069"><math id="M69" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e070"><math id="M70" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 DWConv + 9 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e071"><math id="M71" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 9 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e072"><math id="M72" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 9 DWConv). (4) Substituting the MLP in Swin Transformer Blocks with our Inverted Bottleneck with Depthwise Convolutional Augmentation (DWCA). (5) Replacing Swin Transformer Blocks with LKDA-Net Blocks. (6) Combining LKDA-Net Blocks with the Skip Connection Fusion Module in the full architecture.</p>
<p><a href="#pone.0329806.t006" class="usa-link">Table 6</a> shows the performance of adding the two modules proposed in this paper to the Swin-UNETR baseline model on the Synapse dataset. The experimental data indicate that the number of parameters of the Swin-UNETR model on the Synapse dataset is 62.83M, and the average DSC is 83.48%. When the Window Multi-Head Self-Attention (W-MSA) in the Swin Transformer Block of the Swin-UNETR model is replaced with the LKD Attention proposed in this paper, the number of parameters of the model is significantly reduced, and the average DSC is also improved. In addition, we conducted experiments with different configurations for the kernel sizes of the two Depthwise Convolutions (DWConv) in the LKD Attention . In the ablation study, we observed that the LKD Attention module achieved a mean Dice Similarity Coefficient (DSC) of 85.21% when employing a combination of 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e073"><math id="M73" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e074"><math id="M74" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 and 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e075"><math id="M75" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e076"><math id="M76" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 convolutional kernels, outperforming other configurations. This superiority can be attributed to two key factors: From the perspective of multiscale context capture, the 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e077"><math id="M77" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e078"><math id="M78" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 kernel effectively captures midrange contextual information within local regions, while the 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e079"><math id="M79" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e080"><math id="M80" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 kernel extends the receptive field to model global anatomical dependencies. Their cascaded design recursively aggregates multiscale features, enhancing the model’s capacity to represent complex organ boundaries and heterogeneous regions. From the standpoint of parameter efficiency and feature representation balance, larger 9 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e081"><math id="M81" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 9 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e082"><math id="M82" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 9 kernels further expand the receptive field, they introduce a significant parameter increase (50.13M) and risk incorporating redundant noise. Conversely, smaller 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e083"><math id="M83" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e084"><math id="M84" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3 kernels fail to adequately cover global dependencies. The 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e085"><math id="M85" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e086"><math id="M86" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 + 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e087"><math id="M87" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e088"><math id="M88" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 configuration achieves an optimal balance between receptive field coverage and feature precision while maintaining a lower parameter count (43.44M).</p>
<section class="tw xbox font-sm" id="pone.0329806.t006"><h4 class="obj_head">Table 6. The ablation experiment data of the LKDA-Net model on the Synapse dataset. We exhibit the parameter number and model performance when our model is incorporated into the baseline, and present the performance of LKD Attention with different convolution kernel sizes.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Methods</th>
<th align="left" rowspan="1" colspan="1">Params (M)</th>
<th align="left" rowspan="1" colspan="1">Average DSC(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Swin-UNETR</td>
<td align="left" rowspan="1" colspan="1">62.83</td>
<td align="left" rowspan="1" colspan="1">83.48</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Use LKD Attention (3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e089"><math id="M89" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e090"><math id="M90" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3 DWConv + 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e091"><math id="M91" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e092"><math id="M92" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 DWConv)</td>
<td align="left" rowspan="1" colspan="1">40.52</td>
<td align="left" rowspan="1" colspan="1">82.81</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Use LKD Attention (5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e093"><math id="M93" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e094"><math id="M94" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 5 DWConv + 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e095"><math id="M95" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e096"><math id="M96" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 DWConv)</td>
<td align="left" rowspan="1" colspan="1">43.44</td>
<td align="left" rowspan="1" colspan="1">85.21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Use LKD Attention (7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e097"><math id="M97" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e098"><math id="M98" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 7 DWConv + 9 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e099"><math id="M99" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 9 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e100"><math id="M100" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 9 DWConv)</td>
<td align="left" rowspan="1" colspan="1">50.13</td>
<td align="left" rowspan="1" colspan="1">84.63</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Use DWCA</td>
<td align="left" rowspan="1" colspan="1">64.23</td>
<td align="left" rowspan="1" colspan="1">84.81</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Use LKDA-Net Block</td>
<td align="left" rowspan="1" colspan="1">44.73</td>
<td align="left" rowspan="1" colspan="1">86.23</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Use LKDA-Net Block + Skip Connection Fusion Module</td>
<td align="left" rowspan="1" colspan="1">48.52</td>
<td align="left" rowspan="1" colspan="1">87.21</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329806.t006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>Furthermore, when the Multi-Layer Perceptron (MLP) in the Swin Transformer Block is replaced with the Inverted Bottleneck with Depthwise Convolutional Augmentation (DWCA) module in the LKDA-Net Block, both the number of parameters and the segmentation performance of the model slightly increase. This module not only improves performance metrics but also addresses the semantic gap by processing the encoder’s shallow local features and the decoder’s deep semantic features separately through grouped convolutions, combined with inverted bottleneck pointwise convolutions to adaptively calibrate feature weights. Furthermore, it employs 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e101"><math id="M101" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0329806.e102"><math id="M102" display="inline" overflow="linebreak"><mrow><mrow><mo>×</mo></mrow></mrow></math></span> 3 grouped convolutions and GELU activation functions to enhance feature extraction, preserving edge details and subtle structures. By leveraging grouped convolutions to reduce redundant computations, the module achieves efficient fusion with minimal parameter overhead.</p>
<p>Moreover, on the basis of adding the LKDA-Net Block to the Swin-UNETR, the Skip Connection Fusion Module is added to the decoder. Although the number of parameters of the model increases to 48.5M, the segmentation performance is improved to 87.21%. These results demonstrate that the LKDA-Net Block can not only reduce the number of parameters of the model by using LKD Attention but also extract multi-scale features with a larger receptive field to improve the segmentation performance. And the Skip Connection Fusion Module can effectively fuse the shallow local information at each stage in the encoder and the deep semantic information in the decoder, further improving the accuracy of segmentation.</p></section></section><section id="sec021"><h2 class="pmc_sec_title">Discussion</h2>
<p>To understand the limitations of LKDA-Net, we analyzed the segmentation results of it and other SOTA methods on the Synapse Dataset and BTCV dataset, with a focus on the cases where the Dice scores were the lowest among these methods. <a href="#pone.0329806.t007" class="usa-link">Table 7</a> presents the Dice scores of these failure cases. The failure cases of these methods exhibited significant consistency, as almost all methods showed relatively low segmentation accuracy in the same instances within the two datasets. Specifically, in the BTCV multi-organ segmentation task, U-Net and TransUNet demonstrated the lowest segmentation accuracy in the first case and had similar low performance in the second case. In contrast, Swin-UNETR and nnFormer had the lowest Dice scores in the second case while performing poorly in the first case. In this study, we compared a variety of model architectures, including the pure convolutional U-Net, the pure Transformer architecture nnFormer, and the hybrid CNN-Transformer frameworks TransUNet and Swin UNETR. Therefore, these limitations might not stem from the architectural design or training. We speculate that certain anatomical features or pathological characteristics may inherently pose difficulties for segmentation algorithms.</p>
<section class="tw xbox font-sm" id="pone.0329806.t007"><h3 class="obj_head">Table 7. Analysis of failure segmentation results from LKDA-Net and other methods on Synapse and BTCV datasets. These segmentation results are appraised based on the average Dice score for each case.</h3>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Methods</th>
<th align="left" colspan="2" rowspan="1">Synapse</th>
<th align="left" colspan="2" rowspan="1">BTCV</th>
</tr>
<tr>
<th align="left" rowspan="1" colspan="1"></th>
<th align="left" rowspan="1" colspan="1">Case #1</th>
<th align="left" rowspan="1" colspan="1">Case #2</th>
<th align="left" rowspan="1" colspan="1">Case #1</th>
<th align="left" rowspan="1" colspan="1">Case #2</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">U-Net</td>
<td align="left" rowspan="1" colspan="1">66.32</td>
<td align="left" rowspan="1" colspan="1">71.22</td>
<td align="left" rowspan="1" colspan="1">61.27</td>
<td align="left" rowspan="1" colspan="1">65.12</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">TransUNet</td>
<td align="left" rowspan="1" colspan="1">63.31</td>
<td align="left" rowspan="1" colspan="1">67.72</td>
<td align="left" rowspan="1" colspan="1">58.27</td>
<td align="left" rowspan="1" colspan="1">61.11</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Swin-UNETR</td>
<td align="left" rowspan="1" colspan="1">58.31</td>
<td align="left" rowspan="1" colspan="1">55.32</td>
<td align="left" rowspan="1" colspan="1">55.40</td>
<td align="left" rowspan="1" colspan="1">52.23</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">nnFormer</td>
<td align="left" rowspan="1" colspan="1">65.24</td>
<td align="left" rowspan="1" colspan="1">63.11</td>
<td align="left" rowspan="1" colspan="1">60.24</td>
<td align="left" rowspan="1" colspan="1">59.21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LKDA-Net</td>
<td align="left" rowspan="1" colspan="1">70.12</td>
<td align="left" rowspan="1" colspan="1">73.57</td>
<td align="left" rowspan="1" colspan="1">67.12</td>
<td align="left" rowspan="1" colspan="1">68.87</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0329806.t007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec022"><h2 class="pmc_sec_title">Conclusions</h2>
<p>We propose the LKDA-Net, which is an effective network architecture for 3D medical image segmentation. The design of LKDA-Net aims to model global relationships by simulating the self-attention mechanism of Transformers based on the LKDA-Net Block, so as to efficiently extract global features and then achieve accurate and efficient 3D volume segmentation. The encoder of LKDA-Net consists of the LKDA-Net Block and downsampling. Specifically, the Large Kernel Depthwise Convolution Attention (LKD Attention) in the LKDA-Net Block extracts multi-scale features by employing multiple large kernel depthwise convolutions. It recursively aggregates the contextual information within the receptive field and captures more effective features in deeper and larger receptive fields at the same time. The Inverted Bottleneck with Depthwise Convolution Augmentation (DWCA) in the LKDA-Net Block reduces the redundancy among channels while enhancing the feature expression ability by independently expanding and compressing the dimensions of each channel. The decoder utilizes the Skip Connection Fusion Module to further extract hierarchical visual feature representations and optimize the segmentation results. Inside it, effective operations such as group convolution are adopted to reasonably allocate features for fusion. The upsampling module improves the resolution of the feature map and retains key features through techniques like bilinear interpolation. We have conducted a comprehensive evaluation of LKDA-Net on multiple publicly available 3D medical image segmentation datasets. The results show that compared with the current Transformer models, LKDA-Net reduces the computational complexity while improving the segmentation performance.</p></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>Our paper utilizes three publicly available datasets for medical image segmentation, namely: Synapse, ACDC, and BTCV. Specific descriptions of these datasets are provided in the “Experiments and Results” section of this paper. Additionally, the datasets used in our model have been uploaded to the figshare website. The Synapse dataset can be accessed via the following DOI: <a href="https://10.6084/m9.figshare.29073904" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.6084/m9.figshare.29073904</a>. The ACDC dataset can be accessed via the following DOI: <a href="https://10.6084/m9.figshare.29071418" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.6084/m9.figshare.29071418</a>. The BTCV dataset can be accessed via the following DOI: <a href="https://10.6084/m9.figshare.29077214" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.6084/m9.figshare.29077214</a>.</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>The author(s) received no specific funding for this work.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0329806.ref001">
<span class="label">1.</span><cite>Azad R, Aghdam EK, Rauland A, Jia Y, Avval AH, Bozorgpour A. Medical image segmentation review: the success of u-net. arXiv preprint
2022. <a href="https://arxiv.org/abs/2211.4830" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2211.4830</a></cite> [<a href="https://doi.org/10.1109/TPAMI.2024.3435571" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39167505/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=Medical%20image%20segmentation%20review:%20the%20success%20of%20u-net&amp;author=R%20Azad&amp;author=EK%20Aghdam&amp;author=A%20Rauland&amp;author=Y%20Jia&amp;author=AH%20Avval&amp;publication_year=2022&amp;pmid=39167505&amp;doi=10.1109/TPAMI.2024.3435571&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref002">
<span class="label">2.</span><cite>Lee HH, Bao S, Huo Y, Landman BA. 3D UX-Net: a large kernel volumetric ConvNet modernizing hierarchical transformer for medical image segmentation. arXiv preprint 2022. <a href="https://arxiv.org/abs/2209.15076" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2209.15076</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=3D%20UX-Net:%20a%20large%20kernel%20volumetric%20ConvNet%20modernizing%20hierarchical%20transformer%20for%20medical%20image%20segmentation&amp;author=HH%20Lee&amp;author=S%20Bao&amp;author=Y%20Huo&amp;author=BA%20Landman&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref003">
<span class="label">3.</span><cite>Tang Y, Yang D, Li W, Roth HR, Landman B, Xu D, et al. Self-supervised pre-training of swin transformers for 3D medical image analysis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. p. 20730–40.</cite>
</li>
<li id="pone.0329806.ref004">
<span class="label">4.</span><cite>Wang W, Chen C, Ding M, Yu H, Zha S, Li J. Transbts: multimodal brain tumor segmentation using transformer. In: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021 : 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I. 2021. p. 109–19.</cite>
</li>
<li id="pone.0329806.ref005">
<span class="label">5.</span><cite>Hatamizadeh A, Tang Y, Nath V, Yang D, Myronenko A, Landman B. Unetr: transformers for 3d medical image segmentation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022. p. 574–84.</cite>
</li>
<li id="pone.0329806.ref006">
<span class="label">6.</span><cite>Zhou HY, Guo J, Zhang Y, Yu L, Wang L, Yu Y. Nnformer: interleaved transformer for volumetric segmentation. arXiv preprint
2021. doi: 10.48550/arXiv.2109.03201</cite> [<a href="https://doi.org/10.48550/arXiv.2109.03201" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=Nnformer:%20interleaved%20transformer%20for%20volumetric%20segmentation&amp;author=HY%20Zhou&amp;author=J%20Guo&amp;author=Y%20Zhang&amp;author=L%20Yu&amp;author=L%20Wang&amp;publication_year=2021&amp;doi=10.48550/arXiv.2109.03201&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref007">
<span class="label">7.</span><cite>Zhang Z, Zhang H, Zhao L, Chen T, Arik SÖ, Pfister T. Nested hierarchical transformer: towards accurate, data-efficient and interpretable visual understanding. AAAI.
2022;36(3):3417–25. doi: 10.1609/aaai.v36i3.20252</cite> [<a href="https://doi.org/10.1609/aaai.v36i3.20252" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=AAAI.&amp;title=Nested%20hierarchical%20transformer:%20towards%20accurate,%20data-efficient%20and%20interpretable%20visual%20understanding&amp;author=Z%20Zhang&amp;author=H%20Zhang&amp;author=L%20Zhao&amp;author=T%20Chen&amp;author=S%C3%96%20Arik&amp;volume=36&amp;issue=3&amp;publication_year=2022&amp;pages=3417-25&amp;doi=10.1609/aaai.v36i3.20252&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref008">
<span class="label">8.</span><cite>Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z. Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. p. 10012–22.</cite>
</li>
<li id="pone.0329806.ref009">
<span class="label">9.</span><cite>Hatamizadeh A, Nath V, Tang Y, Yang D, Roth HR, Xu D. Swin unetr: swin transformers for semantic segmentation of brain tumors in mri images. In: International MICCAI Brainlesion Workshop. 2021. p. 272–84.</cite>
</li>
<li id="pone.0329806.ref010">
<span class="label">10.</span><cite>Peiris H, Hayat M, Chen Z, Egan G, Harandi M. A robust volumetric transformer for accurate 3D tumor segmentation. In: International conference on medical image computing and computer-assisted intervention. Springer; 2022. p. 162–72.</cite>
</li>
<li id="pone.0329806.ref011">
<span class="label">11.</span><cite>Roy S, Koehler G, Ulrich C, Baumgartner M, Petersen J, Isensee F. Mednext: transformer-driven scaling of convnets for medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. 2023. p. 405–15.</cite>
</li>
<li id="pone.0329806.ref012">
<span class="label">12.</span><cite>Vasu PKA, Gabriel J, Zhu J, Tuzel O, Ranjan A. FastViT: a fast hybrid vision transformer using structural reparameterization. arXiv preprint
2023. doi: arXiv:230314189</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=FastViT:%20a%20fast%20hybrid%20vision%20transformer%20using%20structural%20reparameterization&amp;author=PKA%20Vasu&amp;author=J%20Gabriel&amp;author=J%20Zhu&amp;author=O%20Tuzel&amp;author=A%20Ranjan&amp;publication_year=2023&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref013">
<span class="label">13.</span><cite>Li H, Nan Y, Del Ser J, Yang G. Large-Kernel attention for 3D medical image segmentation. Cognit Comput.
2024;16(4):2063–77. doi: 10.1007/s12559-023-10126-7

</cite> [<a href="https://doi.org/10.1007/s12559-023-10126-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11226511/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38974012/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cognit%20Comput.&amp;title=Large-Kernel%20attention%20for%203D%20medical%20image%20segmentation&amp;author=H%20Li&amp;author=Y%20Nan&amp;author=J%20Del%20Ser&amp;author=G%20Yang&amp;volume=16&amp;issue=4&amp;publication_year=2024&amp;pages=2063-77&amp;pmid=38974012&amp;doi=10.1007/s12559-023-10126-7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref014">
<span class="label">14.</span><cite>Zhou Z, Siddiquee MMR, Tajbakhsh N, Liang J. UNet++: redesigning skip connections to exploit multiscale features in image segmentation. IEEE Trans Med Imaging.
2020;39(6):1856–67. doi: 10.1109/TMI.2019.2959609

</cite> [<a href="https://doi.org/10.1109/TMI.2019.2959609" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7357299/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31841402/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Med%20Imaging.&amp;title=UNet++:%20redesigning%20skip%20connections%20to%20exploit%20multiscale%20features%20in%20image%20segmentation&amp;author=Z%20Zhou&amp;author=MMR%20Siddiquee&amp;author=N%20Tajbakhsh&amp;author=J%20Liang&amp;volume=39&amp;issue=6&amp;publication_year=2020&amp;pages=1856-67&amp;pmid=31841402&amp;doi=10.1109/TMI.2019.2959609&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref015">
<span class="label">15.</span><cite>Huang H, Lin L, Tong R, Hu H, Zhang Q, Iwamoto Y. Unet 3: a full-scale connected unet for medical image segmentation. In: ICASSP. IEEE; 2020. p. 1055–9.</cite>
</li>
<li id="pone.0329806.ref016">
<span class="label">16.</span><cite>Isensee F, Jaeger PF, Kohl SAA, Petersen J, Maier-Hein KH. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat Methods.
2021;18(2):203–11. doi: 10.1038/s41592-020-01008-z

</cite> [<a href="https://doi.org/10.1038/s41592-020-01008-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33288961/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat%20Methods.&amp;title=nnU-Net:%20a%20self-configuring%20method%20for%20deep%20learning-based%20biomedical%20image%20segmentation&amp;author=F%20Isensee&amp;author=PF%20Jaeger&amp;author=SAA%20Kohl&amp;author=J%20Petersen&amp;author=KH%20Maier-Hein&amp;volume=18&amp;issue=2&amp;publication_year=2021&amp;pages=203-11&amp;pmid=33288961&amp;doi=10.1038/s41592-020-01008-z&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref017">
<span class="label">17.</span><cite>Çiçek Ö, Abdulkadir A, Lienkamp SS, Brox T, Ronneberger O. 3D U-Net: learning dense volumetric segmentation from sparse annotation. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016 : 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II. 2016. p. 424–32.</cite>
</li>
<li id="pone.0329806.ref018">
<span class="label">18.</span><cite>Yang X, Li Z, Guo Y, Zhou D. DCU-net: a deformable convolutional neural network based on cascade U-net for retinal vessel segmentation. Multim Tools Appl.
2022;81(11):15593–607.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Multim%20Tools%20Appl.&amp;title=DCU-net:%20a%20deformable%20convolutional%20neural%20network%20based%20on%20cascade%20U-net%20for%20retinal%20vessel%20segmentation&amp;author=X%20Yang&amp;author=Z%20Li&amp;author=Y%20Guo&amp;author=D%20Zhou&amp;volume=81&amp;issue=11&amp;publication_year=2022&amp;pages=15593-607&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref019">
<span class="label">19.</span><cite>Yang C, Zhang Z. PFD-Net: pyramid fourier deformable network for medical image segmentation. Comput Biol Med.
2024;172:108302. doi: 10.1016/j.compbiomed.2024.108302

</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2024.108302" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38503092/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Biol%20Med.&amp;title=PFD-Net:%20pyramid%20fourier%20deformable%20network%20for%20medical%20image%20segmentation&amp;author=C%20Yang&amp;author=Z%20Zhang&amp;volume=172&amp;publication_year=2024&amp;pages=108302&amp;pmid=38503092&amp;doi=10.1016/j.compbiomed.2024.108302&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref020">
<span class="label">20.</span><cite>Azad R, Niggemeier L, Hüttemann M, Kazerouni A, Aghdam EK, Velichko Y, et al. Beyond self-attention: deformable large kernel attention for medical image segmentation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision; 2024. p. 1287–97.</cite>
</li>
<li id="pone.0329806.ref021">
<span class="label">21.</span><cite>Li H, Nan Y, Yang G. LKAU-Net: 3D large-kernel attention-based u-net for automatic MRI brain tumor segmentation. In: Annual Conference on Medical Image Understanding and Analysis. Springer; 2022. p. 313–27.</cite>
</li>
<li id="pone.0329806.ref022">
<span class="label">22.</span><cite>Liu Y, Zhang Z, Yue J, Guo W. SCANeXt: Enhancing 3D medical image segmentation with dual attention network and depth-wise convolution. Heliyon.
2024;10(5):e26775. doi: 10.1016/j.heliyon.2024.e26775

</cite> [<a href="https://doi.org/10.1016/j.heliyon.2024.e26775" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10909707/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38439873/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Heliyon.&amp;title=SCANeXt:%20Enhancing%203D%20medical%20image%20segmentation%20with%20dual%20attention%20network%20and%20depth-wise%20convolution&amp;author=Y%20Liu&amp;author=Z%20Zhang&amp;author=J%20Yue&amp;author=W%20Guo&amp;volume=10&amp;issue=5&amp;publication_year=2024&amp;pmid=38439873&amp;doi=10.1016/j.heliyon.2024.e26775&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref023">
<span class="label">23.</span><cite>Zeng N, Fang J, Wang X, Lu X, Huang J, Miao H, et al. Factoring 3D convolutions for medical images by depth-wise dependencies-induced adaptive attention. In: 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2022. p. 883–6. 10.1109/bibm55620.2022.9995195</cite> [<a href="https://doi.org/10.1109/bibm55620.2022.9995195" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0329806.ref024">
<span class="label">24.</span><cite>Shang J, Zhou S. LK-UNet: large Kernel design for 3D medical image segmentation. In: ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2024. p. 1576–80.</cite>
</li>
<li id="pone.0329806.ref025">
<span class="label">25.</span><cite>Arslan S, Kaya MK, Tasci B, Kaya S, Tasci G, Ozsoy F, et al. Attention TurkerNeXt: investigations into bipolar disorder detection using OCT images. Diagnostics (Basel).
2023;13(22):3422. doi: 10.3390/diagnostics13223422

</cite> [<a href="https://doi.org/10.3390/diagnostics13223422" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10669998/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37998558/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diagnostics%20(Basel).&amp;title=Attention%20TurkerNeXt:%20investigations%20into%20bipolar%20disorder%20detection%20using%20OCT%20images&amp;author=S%20Arslan&amp;author=MK%20Kaya&amp;author=B%20Tasci&amp;author=S%20Kaya&amp;author=G%20Tasci&amp;volume=13&amp;issue=22&amp;publication_year=2023&amp;pages=3422&amp;pmid=37998558&amp;doi=10.3390/diagnostics13223422&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref026">
<span class="label">26.</span><cite>Huo Y, Xu Z, Bao S, Bermudez C, Moon H, Parvathaneni P, et al. Splenomegaly segmentation on multi-modal MRI using deep convolutional networks. IEEE Trans Med Imaging.
2019;38(5):1185–96. doi: 10.1109/TMI.2018.2881110

</cite> [<a href="https://doi.org/10.1109/TMI.2018.2881110" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7194446/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30442602/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Med%20Imaging.&amp;title=Splenomegaly%20segmentation%20on%20multi-modal%20MRI%20using%20deep%20convolutional%20networks&amp;author=Y%20Huo&amp;author=Z%20Xu&amp;author=S%20Bao&amp;author=C%20Bermudez&amp;author=H%20Moon&amp;volume=38&amp;issue=5&amp;publication_year=2019&amp;pages=1185-96&amp;pmid=30442602&amp;doi=10.1109/TMI.2018.2881110&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref027">
<span class="label">27.</span><cite>Liu Z, Mao H, Wu CY, Feichtenhofer C, Darrell T, Xie S. A convnet for the 2020 s. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition; 2022. p. 11976–86.</cite>
</li>
<li id="pone.0329806.ref028">
<span class="label">28.</span><cite>Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q. Swin-unet: unet-like pure transformer for medical image segmentation. In: Computer Vision–ECCV 2022 Workshops: Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part III. Springer; 2023. p. 205–18.</cite>
</li>
<li id="pone.0329806.ref029">
<span class="label">29.</span><cite>Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T. An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint
2020. <a href="https://arxiv.org/abs/2010.11929" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2010.11929</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=An%20image%20is%20worth%2016x16%20words:%20transformers%20for%20image%20recognition%20at%20scale&amp;author=A%20Dosovitskiy&amp;author=L%20Beyer&amp;author=A%20Kolesnikov&amp;author=D%20Weissenborn&amp;author=X%20Zhai&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref030">
<span class="label">30.</span><cite>Huang X, Deng Z, Li D, Yuan X. MISSFormer: an effective medical image segmentation transformer. CoRR.
2021. doi: abs/2109.07162</cite> [<a href="https://doi.org/10.1109/TMI.2022.3230943" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37015444/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=CoRR.&amp;title=MISSFormer:%20an%20effective%20medical%20image%20segmentation%20transformer&amp;author=X%20Huang&amp;author=Z%20Deng&amp;author=D%20Li&amp;author=X%20Yuan&amp;publication_year=2021&amp;pmid=37015444&amp;doi=10.1109/TMI.2022.3230943&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref031">
<span class="label">31.</span><cite>Jiang Y, Zhang Y, Lin X, Dong J, Cheng T, Liang J. SwinBTS: a method for 3D multimodal brain tumor segmentation using swin transformer. Brain Sci.
2022;12(6):797. doi: 10.3390/brainsci12060797

</cite> [<a href="https://doi.org/10.3390/brainsci12060797" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9221215/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35741682/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Brain%20Sci.&amp;title=SwinBTS:%20a%20method%20for%203D%20multimodal%20brain%20tumor%20segmentation%20using%20swin%20transformer&amp;author=Y%20Jiang&amp;author=Y%20Zhang&amp;author=X%20Lin&amp;author=J%20Dong&amp;author=T%20Cheng&amp;volume=12&amp;issue=6&amp;publication_year=2022&amp;pages=797&amp;pmid=35741682&amp;doi=10.3390/brainsci12060797&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref032">
<span class="label">32.</span><cite>Shamshad F, Khan S, Zamir SW, Khan MH, Hayat M, Khan FS, et al. Transformers in medical imaging: a survey. Med Image Anal.
2023;88:102802. doi: 10.1016/j.media.2023.102802

</cite> [<a href="https://doi.org/10.1016/j.media.2023.102802" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37315483/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Image%20Anal.&amp;title=Transformers%20in%20medical%20imaging:%20a%20survey&amp;author=F%20Shamshad&amp;author=S%20Khan&amp;author=SW%20Zamir&amp;author=MH%20Khan&amp;author=M%20Hayat&amp;volume=88&amp;publication_year=2023&amp;pages=102802&amp;pmid=37315483&amp;doi=10.1016/j.media.2023.102802&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref033">
<span class="label">33.</span><cite>Chen J, Lu Y, Yu Q, Luo X, Adeli E, Wang Y. Transunet: transformers make strong encoders for medical image segmentation. arXiv preprint
2021. <a href="https://arxiv.org/abs/2102.04306" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2102.04306</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=Transunet:%20transformers%20make%20strong%20encoders%20for%20medical%20image%20segmentation&amp;author=J%20Chen&amp;author=Y%20Lu&amp;author=Q%20Yu&amp;author=X%20Luo&amp;author=E%20Adeli&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref034">
<span class="label">34.</span><cite>Zhang Y, Liu H, Hu Q. Transfuse:fusing transformers, cnns for medical image segmentation. In: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021 : 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I, Strasbourg, France. 2021. p. 14–24.</cite>
</li>
<li id="pone.0329806.ref035">
<span class="label">35.</span><cite>Chen Y, Wang K, Liao X, Qian Y, Wang Q, Yuan Z, et al. Channel-Unet: a spatial channel-wise convolutional neural network for liver and tumors segmentation. Front Genet.
2019;10:1110. doi: 10.3389/fgene.2019.01110

</cite> [<a href="https://doi.org/10.3389/fgene.2019.01110" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6892404/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31827487/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Genet.&amp;title=Channel-Unet:%20a%20spatial%20channel-wise%20convolutional%20neural%20network%20for%20liver%20and%20tumors%20segmentation&amp;author=Y%20Chen&amp;author=K%20Wang&amp;author=X%20Liao&amp;author=Y%20Qian&amp;author=Q%20Wang&amp;volume=10&amp;publication_year=2019&amp;pages=1110&amp;pmid=31827487&amp;doi=10.3389/fgene.2019.01110&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref036">
<span class="label">36.</span><cite>Hu J, Shen L, Sun G. Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. p. 7132–41.</cite>
</li>
<li id="pone.0329806.ref037">
<span class="label">37.</span><cite>Wang Q, Wu B, Zhu P, Li P, Zuo W, Hu Q. ECA-Net: efficient channel attention for deep convolutional neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. p. 11534–42.</cite>
</li>
<li id="pone.0329806.ref038">
<span class="label">38.</span><cite>Jaderberg M, Simonyan K, Zisserman A. Spatial transformer networks. Adv Neural Inf Process Syst.
2015;28.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inf%20Process%20Syst.&amp;title=Spatial%20transformer%20networks&amp;author=M%20Jaderberg&amp;author=K%20Simonyan&amp;author=A%20Zisserman&amp;volume=28&amp;publication_year=2015&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref039">
<span class="label">39.</span><cite>Guo C, Szemenyei M, Yi Y, Wang W, Chen B, Fan C. SA-UNet: spatial attention U-Net for retinal vessel segmentation. In: 2020 25th International Conference on Pattern Recognition (ICPR). 2021. p. 1236–42. 10.1109/icpr48806.2021.9413346</cite> [<a href="https://doi.org/10.1109/icpr48806.2021.9413346" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0329806.ref040">
<span class="label">40.</span><cite>Valanarasu JMJ, Oza P, Hacihaliloglu I, Patel VM. Medical transformer: gated axial-attention for medical image segmentation. In: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021 : 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I. 2021. p. 36–46.</cite>
</li>
<li id="pone.0329806.ref041">
<span class="label">41.</span><cite>Sinha A, Dolz J. Multi-scale self-guided attention for medical image segmentation. IEEE J Biomed Health Inform.
2021;25(1):121–30. doi: 10.1109/JBHI.2020.2986926

</cite> [<a href="https://doi.org/10.1109/JBHI.2020.2986926" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32305947/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20J%20Biomed%20Health%20Inform.&amp;title=Multi-scale%20self-guided%20attention%20for%20medical%20image%20segmentation&amp;author=A%20Sinha&amp;author=J%20Dolz&amp;volume=25&amp;issue=1&amp;publication_year=2021&amp;pages=121-30&amp;pmid=32305947&amp;doi=10.1109/JBHI.2020.2986926&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref042">
<span class="label">42.</span><cite>Chen B, Liu Y, Zhang Z, Lu G, Kong AWK. Transattunet: multi-level attention-guided u-net with transformer for medical image segmentation. IEEE Trans Emerg Topics Comput Intell.
2023.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Emerg%20Topics%20Comput%20Intell.&amp;title=Transattunet:%20multi-level%20attention-guided%20u-net%20with%20transformer%20for%20medical%20image%20segmentation&amp;author=B%20Chen&amp;author=Y%20Liu&amp;author=Z%20Zhang&amp;author=G%20Lu&amp;author=AWK%20Kong&amp;publication_year=2023&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref043">
<span class="label">43.</span><cite>Guo MH, Lu CZ, Hou Q, Liu Z, Cheng MM, Hu SM. Segnext: rethinking convolutional attention design for semantic segmentation. Adv Neural Inf Process Syst.
2022;35:1140–56.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inf%20Process%20Syst.&amp;title=Segnext:%20rethinking%20convolutional%20attention%20design%20for%20semantic%20segmentation&amp;author=MH%20Guo&amp;author=CZ%20Lu&amp;author=Q%20Hou&amp;author=Z%20Liu&amp;author=MM%20Cheng&amp;volume=35&amp;publication_year=2022&amp;pages=1140-56&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0329806.ref044">
<span class="label">44.</span><cite>Fan DP, Ji GP, Zhou T, Chen G, Fu H, Shen J, et al. Pranet: parallel reverse attention network for polyp segmentation. In: International conference on medical image computing and computer-assisted intervention. Springer; 2020. p. 263–73.</cite>
</li>
<li id="pone.0329806.ref045">
<span class="label">45.</span><cite>Woo S, Park J, Lee JY, Kweon IS. Cbam: convolutional block attention module. In: Proceedings of the European Conference on Computer Vision (ECCV). 2018. p. 3–19.</cite>
</li>
<li id="pone.0329806.ref046">
<span class="label">46.</span><cite>Cai Y, Wang Y. Ma-unet: an improved version of unet based on multi-scale and attention mechanism for medical image segmentation. In: Third International Conference on Electronics and Communication; Network and Computer Technology (ECNCT 2021). 2022. p. 205–11.</cite>
</li>
<li id="pone.0329806.ref047">
<span class="label">47.</span><cite>Sandler M, Howard A, Zhu M, Zhmoginov A, Chen L-C. MobileNetV2: inverted residuals and linear bottlenecks. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2018. p. 4510–20. 10.1109/cvpr.2018.00474</cite> [<a href="https://doi.org/10.1109/cvpr.2018.00474" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0329806.ref048">
<span class="label">48.</span><cite>Ronneberger O, Fischer P, Brox T. U-net: convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015 : 18th International Conference, Munich, Germany, October 5–9, 2015, Proceedings, Part III 18. Springer; 2015. p. 234–41.</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>Our paper utilizes three publicly available datasets for medical image segmentation, namely: Synapse, ACDC, and BTCV. Specific descriptions of these datasets are provided in the “Experiments and Results” section of this paper. Additionally, the datasets used in our model have been uploaded to the figshare website. The Synapse dataset can be accessed via the following DOI: <a href="https://10.6084/m9.figshare.29073904" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.6084/m9.figshare.29073904</a>. The ACDC dataset can be accessed via the following DOI: <a href="https://10.6084/m9.figshare.29071418" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.6084/m9.figshare.29071418</a>. The BTCV dataset can be accessed via the following DOI: <a href="https://10.6084/m9.figshare.29077214" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.6084/m9.figshare.29077214</a>.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0329806"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0329806.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (14.1 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12334030/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12334030/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12334030%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334030/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12334030/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12334030/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40779579/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12334030/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40779579/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12334030/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12334030/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="eKam2y5bmJqOLne6lw277JUTU7SvyxPzLedbHK2X5C23w1ZfoZhZwWDEMKVJ7sgt">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Web Policies

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    FOIA

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS Vulnerability Disclosure

    
</a>

                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Help

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Accessibility

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Careers

    
</a>

                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NLM

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NIH

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    USA.gov

    
</a>

                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-370d5dd6.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-917ba005.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-d9849939.js"></script>
    
    

    </body>
</html>
