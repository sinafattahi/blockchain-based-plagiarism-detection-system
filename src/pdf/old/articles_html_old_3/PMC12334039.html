
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-a68b4900.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-0a3f24ce.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Oak-YOLO: A high-performance detection model for automated Oak seed defect identification - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="67D7142889B743F31C1428002C7A829F.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334039/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="Oak-YOLO: A high-performance detection model for automated Oak seed defect identification">
<meta name="citation_author" content="Hao Li">
<meta name="citation_author_institution" content="College of Science, Northeast Forestry University, Harbin, Heilongjiang, China">
<meta name="citation_author" content="Zhuqi Li">
<meta name="citation_author_institution" content="College of Science, Northeast Forestry University, Harbin, Heilongjiang, China">
<meta name="citation_author" content="Dongkui Chen">
<meta name="citation_author_institution" content="College of Science, Northeast Forestry University, Harbin, Heilongjiang, China">
<meta name="citation_author" content="Wangyu Wu">
<meta name="citation_author_institution" content="University of Liverpool, Liverpool, United Kingdom">
<meta name="citation_author" content="Xuanlong He">
<meta name="citation_author_institution" content="College of Science, Northeast Forestry University, Harbin, Heilongjiang, China">
<meta name="citation_author" content="Hongbo Mu">
<meta name="citation_author_institution" content="College of Science, Northeast Forestry University, Harbin, Heilongjiang, China">
<meta name="citation_publication_date" content="2025 Aug 8">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0327371">
<meta name="citation_doi" content="10.1371/journal.pone.0327371">
<meta name="citation_pmid" content="40779603">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334039/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334039/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334039/pdf/pone.0327371.pdf">
<meta name="description" content="Oak seeds are highly susceptible to pest infestations due to their elevated starch content, which significantly impairs germination and subsequent growth. To address this challenge, we developed a high-resolution imaging system and proposed an ...">
<meta name="og:title" content="Oak-YOLO: A high-performance detection model for automated Oak seed defect identification">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Oak seeds are highly susceptible to pest infestations due to their elevated starch content, which significantly impairs germination and subsequent growth. To address this challenge, we developed a high-resolution imaging system and proposed an ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334039/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-testid="header" data-header >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            
                <a class="ncbi-header__logo-container" href="https://www.ncbi.nlm.nih.gov/">
                    <img alt="
                                  NCBI home page
                              "
                         class="ncbi-header__logo-image"
                         src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg" />
                </a>
            

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                


    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true"    >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                


    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true"    data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-testid="searchPanel"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      aria-describedby="search-field-desktop-navigation-help-text"
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only"
                           data-testid="label"
                           for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           data-testid="textInput"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    data-testid="button"
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  role="search">
                <label class="usa-sr-only" for="search-field">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="search" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
    

    Dashboard

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
    

    Publications

    
</a>

                        </li>
                    
                        <li class="usa-nav__primary-item">
                            








<a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
    

    Account settings

    
</a>

                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        <a class="usa-link" href="https://www.ncbi.nlm.nih.gov/pmc/advanced/" data-ga-action="featured_link" data-ga-label="advanced_search">
                            Advanced Search
                        </a>
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12334039">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0327371"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0327371.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12334039%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12334039/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12334039/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334039/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0327371" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 8;20(8):e0327371. doi: <a href="https://doi.org/10.1371/journal.pone.0327371" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0327371</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Oak-YOLO: A high-performance detection model for automated Oak seed defect identification</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20H%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Hao Li</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Hao Li</span></h3>
<div class="p">
<sup>1</sup>College of Science, Northeast Forestry University, Harbin, Heilongjiang, China</div>
<div>Writing – original draft</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20H%22%5BAuthor%5D" class="usa-link"><span class="name western">Hao Li</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Zhuqi Li</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Zhuqi Li</span></h3>
<div class="p">
<sup>1</sup>College of Science, Northeast Forestry University, Harbin, Heilongjiang, China</div>
<div>Conceptualization, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zhuqi Li</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Dongkui Chen</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Dongkui Chen</span></h3>
<div class="p">
<sup>1</sup>College of Science, Northeast Forestry University, Harbin, Heilongjiang, China</div>
<div>Methodology</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">Dongkui Chen</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wu%20W%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Wangyu Wu</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Wangyu Wu</span></h3>
<div class="p">
<sup>2</sup>University of Liverpool, Liverpool, United Kingdom</div>
<div>Conceptualization, Data curation</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wu%20W%22%5BAuthor%5D" class="usa-link"><span class="name western">Wangyu Wu</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22He%20X%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Xuanlong He</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Xuanlong He</span></h3>
<div class="p">
<sup>1</sup>College of Science, Northeast Forestry University, Harbin, Heilongjiang, China</div>
<div>Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22He%20X%22%5BAuthor%5D" class="usa-link"><span class="name western">Xuanlong He</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mu%20H%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Hongbo Mu</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Hongbo Mu</span></h3>
<div class="p">
<sup>1</sup>College of Science, Northeast Forestry University, Harbin, Heilongjiang, China</div>
<div>Funding acquisition, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mu%20H%22%5BAuthor%5D" class="usa-link"><span class="name western">Hongbo Mu</span></a>
</div>
</div>
<sup>1,</sup><sup>*</sup>
</div>
<div class="cg p">Editor: <span class="name western">Fatih Uysal</span><sup>3</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>College of Science, Northeast Forestry University, Harbin, Heilongjiang, China</div>
<div id="aff002">
<sup>2</sup>University of Liverpool, Liverpool, United Kingdom</div>
<div id="edit1">
<sup>3</sup>Kafkas University: Kafkas Universitesi, TÜRKIYE</div>
<div class="author-notes p">
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>The authors have declared that no competing interests exist.</p></div>
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>mhb506@nefu.edu.cn</span></p>
</div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Hao Li</span></strong>: <span class="role">Writing – original draft</span>
</div>
<div>
<strong class="contrib"><span class="name western">Zhuqi Li</span></strong>: <span class="role">Conceptualization, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Dongkui Chen</span></strong>: <span class="role">Methodology</span>
</div>
<div>
<strong class="contrib"><span class="name western">Wangyu Wu</span></strong>: <span class="role">Conceptualization, Data curation</span>
</div>
<div>
<strong class="contrib"><span class="name western">Xuanlong He</span></strong>: <span class="role">Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Hongbo Mu</span></strong>: <span class="role">Funding acquisition, Writing – review &amp; editing</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Fatih Uysal</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Apr 9; Accepted 2025 Jun 14; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Li et al</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12334039  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40779603/" class="usa-link">40779603</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>Oak seeds are highly susceptible to pest infestations due to their elevated starch content, which significantly impairs germination and subsequent growth. To address this challenge, we developed a high-resolution imaging system and proposed an improved YOLO-based model named Oak-YOLO for efficient and accurate defect detection in oak seeds. The proposed model enhances the YOLOv8 architecture by incorporating EfficientViT as the backbone to improve global feature extraction, and integrates a Ghost-DynamicConv detection head to enhance the representation of small and irregular defects such as insect holes and cracks. Additionally, the WIoUv3 loss function is introduced to optimize bounding box regression for complex target shapes and overlapping instances.Extensive experiments were conducted on both single-object and multi-object datasets. Oak-YOLO achieved a mAP50 of 94.5%, an F1-score of 95.3%, and a precision of 94.% on the oak-intensive dataset, with an inference speed of 132.2 FPS. Cross-device validation using mobile-captured images further demonstrated the model’s robustness, achieving mAP50 scores of 94.7% and 93.8% on different smartphone test sets. Comparative evaluations show that Oak-YOLO outperforms existing YOLO models, including YOLOv9 to YOLOv12, by delivering a favorable trade-off between detection accuracy and computational efficiency. These results highlight the potential of Oak-YOLO as a practical solution for real-time seed quality inspection in forestry applications.</p></section><section id="sec001"><h2 class="pmc_sec_title">Introduction</h2>
<p>In modern forestry production, oak seeds, due to their high starch content, are particularly vulnerable to insect infestations [<a href="#pone.0327371.ref001" class="usa-link" aria-describedby="pone.0327371.ref001">1</a>]. These infestations not only affect the quality of the seeds but also significantly reduce their germination rate and growth potential. To address these issues, chemical pesticides have traditionally been widely used. However, the effectiveness of this approach has been limited, with insect damage remaining prevalent [<a href="#pone.0327371.ref002" class="usa-link" aria-describedby="pone.0327371.ref002">2</a>], and the overuse of chemical pesticides potentially causing adverse environmental effects.</p>
<p>Traditional methods for screening oak seeds primarily include the weighing method and the water immersion method. The weighing method estimates seed damage by comparing the seed’s weight, while the water immersion method relies on the buoyancy difference of seeds in water to distinguish healthy seeds. However, these methods have significant limitations, as they cannot accurately detect subtle internal insect damage, leading to suboptimal screening results and affecting overall seed quality and planting success [<a href="#pone.0327371.ref003" class="usa-link" aria-describedby="pone.0327371.ref003">3</a>].</p>
<p>Given the limitations of traditional methods in detecting defects in oak seeds, this study introduces machine vision technology for efficient and accurate identification of oak seed defects. Traditional machine vision methods rely on manually designed features such as color, shape, texture, and spectral information. These methods have been widely applied in plant seed detection and classification. For example, Wang <em>et al</em>. [<a href="#pone.0327371.ref004" class="usa-link" aria-describedby="pone.0327371.ref004">4</a>] proposed a maize seed recognition method based on genetic algorithms (GA) and multi-class support vector machines (SVM), While Nguyen-Quoc <em>et al</em>. [<a href="#pone.0327371.ref005" class="usa-link" aria-describedby="pone.0327371.ref005">5</a>] utilized image preprocessing,HOG descriptor, and various imputation techniques combined with SVM classifiers to classify different rice seeds.</p>
<p>In contrast to traditional methods, deep learning techniques can automatically extract rich, multi-level features from images, enabling higher detection accuracy and efficiency in complex environments. YOLO frameworks [<a href="#pone.0327371.ref006" class="usa-link" aria-describedby="pone.0327371.ref006">6</a>–<a href="#pone.0327371.ref008" class="usa-link" aria-describedby="pone.0327371.ref008">8</a>], along with region-based models like Faster R-CNN [<a href="#pone.0327371.ref009" class="usa-link" aria-describedby="pone.0327371.ref009">9</a>] and single-shot detectors like SSD [<a href="#pone.0327371.ref010" class="usa-link" aria-describedby="pone.0327371.ref010">10</a>], enable fast and accurate defect identification in real-time. Recent advancements, such as Swin Transformer-based models [<a href="#pone.0327371.ref011" class="usa-link" aria-describedby="pone.0327371.ref011">11</a>] and transfer learning approaches [<a href="#pone.0327371.ref012" class="usa-link" aria-describedby="pone.0327371.ref012">12</a>], have further enhanced feature extraction and adaptability. Standard datasets like Pascal VOC [<a href="#pone.0327371.ref013" class="usa-link" aria-describedby="pone.0327371.ref013">13</a>] and COCO [<a href="#pone.0327371.ref014" class="usa-link" aria-describedby="pone.0327371.ref014">14</a>] provide benchmarks for evaluating these models. These innovations streamline seed quality assessment, ensuring higher efficiency and accuracy for agricultural applications.For instance, Mukasa <em>et al</em>. [<a href="#pone.0327371.ref015" class="usa-link" aria-describedby="pone.0327371.ref015">15</a>] used DD-SIMCA, SVM, and deep learning classifiers to distinguish between triploid and diploid watermelon seeds; Kurtulmus [<a href="#pone.0327371.ref016" class="usa-link" aria-describedby="pone.0327371.ref016">16</a>] proposed a sunflower seed classification method based on deep convolutional neural networks (CNNs), enhancing classification performance through various CNN architectures; Wang <em>et al</em>. [<a href="#pone.0327371.ref017" class="usa-link" aria-describedby="pone.0327371.ref017">17</a>] combined hyperspectral imaging with deep learning, proposing a novel CNN-LSTM model for maize seed variety identification that showed excellent performance; Shi <em>et al</em>. [<a href="#pone.0327371.ref018" class="usa-link" aria-describedby="pone.0327371.ref018">18</a>] employed iPhone images and deep learning methods, using data augmentation and transfer learning strategies to improve barley seed variety identification accuracy. Barrio-Conde <em>et al</em>. [<a href="#pone.0327371.ref019" class="usa-link" aria-describedby="pone.0327371.ref019">19</a>] successfully classified high oleic sunflower seed varieties using deep learning algorithms, while Bi <em>et al</em>. [<a href="#pone.0327371.ref020" class="usa-link" aria-describedby="pone.0327371.ref020">20</a>] proposed a maize seed recognition method based on an improved Swin Transformer, which incorporated feature attention mechanisms and multi-scale feature fusion to enhance recognition accuracy; Thakur <em>et al</em>. [<a href="#pone.0327371.ref021" class="usa-link" aria-describedby="pone.0327371.ref021">21</a>] introduced deep transfer learning photon sensors, improving seed quality evaluation accuracy and efficiency through laser backscattering and deep learning.deep learning technology has significantly advanced the development of crop seed detection. However, existing computer vision research is primarily focused on the detection of seeds from other crops, while the detection of tree seeds has received relatively less attention.</p>
<p>In this study, we constructed a dedicated dataset for oak seed defect detection. The dataset was meticulously curated to cover various defect types, and incorporated diverse lighting conditions, backgrounds, and spatial arrangements. To further enhance the dataset’s robustness, we employed automatic data augmentation techniques, including rotation, brightness adjustment, and background variation, ensuring a wide range of sample diversity.Building on this comprehensive dataset, we then proposed targeted improvements specifically aimed at enhancing small target detection in oak seed defect analysis. To improve the YOLO model’s performance in identifying small defects, we introduced a multi-scale feature fusion mechanism designed to preserve high-resolution feature information. This approach ensures that small defects, such as cracks and insect holes, are effectively retained within high-level feature maps.</p>
<p>Additionally, we incorporated an improved attention mechanism to enhance boundary separation between adjacent seeds, reducing interference caused by overlapping targets. To address the challenge of detecting irregularly shaped defects, we proposed a novel loss function that enables the model to accurately fit the boundaries of cracks and insect holes using rotated rectangular boxes.</p>
<p>Experimental results demonstrate that the proposed detection framework significantly outperforms traditional methods in detecting oak seed defects. The model effectively addresses the limitations of the YOLO architecture, particularly in small target detection, overlapping object handling, irregular shape adaptation, and data labeling challenges. This comprehensive improvement greatly enhances the accuracy and efficiency of oak seed quality inspection, making the framework highly suitable for practical applications in forestry production.</p></section><section id="sec002"><h2 class="pmc_sec_title">Materials and methods</h2>
<section id="sec003"><h3 class="pmc_sec_title">Selection of seed</h3>
<p>The oak seeds used in this study were purchased from oak farmers in Xinxu Town, Suqian, Jiangsu, China. The seeds were cleaned to remove plant debris, soil, dust, and stones. Subsequently, the seeds were stored at temperatures ranging from -20<sup>°</sup>C to -30<sup>°</sup>C for two weeks to eliminate residual pests.</p></section><section id="sec004"><h3 class="pmc_sec_title">Photographic equipment</h3>
<p>As shown in <a href="#pone.0327371.g001" class="usa-link">Fig 1</a>.The image acquisition system utilized a Canon camera (ME2P-G-P, Hangzhou, China) with a resolution of 4508<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e001"><math id="M1" display="inline" overflow="linebreak"><mrow><mi>×</mi></mrow></math></span>4096. An adjustable lighting setup was employed to facilitate image capture, with the light source set to a color temperature of 5000K to replicate natural lighting conditions. A circular LED light was positioned at a 45<sup>°</sup> angle, with the illumination diffused through white fabric to ensure soft, uniform lighting and to minimize specular reflections that could impede the detection of subtle defects. Based on measurements, the optimal focusing distance between the camera and the seeds was maintained between 12 and 15 cm. The platform was designed to enable flexible adjustments of shooting angles and heights, allowing for comprehensive capture of seed surface features from various perspectives. To maintain consistent image exposure, the ISO setting of all imaging devices was fixed at 800.</p>
<figure class="fig xbox font-sm" id="pone.0327371.g001"><h4 class="obj_head">Fig 1. The process of image acquisition.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g001.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/0354aebf2ea2/pone.0327371.g001.jpg" loading="lazy" height="302" width="675" alt="Fig 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec005"><h3 class="pmc_sec_title">Segmentation of seed images</h3>
<p>In this study, we employed the automated segmentation algorithm EfficientSAM to annotate the dataset. As shown at <a href="#pone.0327371.g002" class="usa-link">Fig 2</a>. The original images were first denoised using median filtering to remove background noise and interference on the seed surface. Subsequently, the images were converted to the HSV color space to enhance the contrast between the seeds and the background. In the HSV space, histogram equalization was applied to the brightness channel to enhance seed surface details:</p>
<figure class="fig xbox font-sm" id="pone.0327371.g002"><h4 class="obj_head">Fig 2. Segmentation process.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/2da469c6635f/pone.0327371.g002.jpg" loading="lazy" height="326" width="667" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><table class="disp-formula p" id="pone.0327371.e002"><tr>
<td class="formula"><math id="M2" display="block" overflow="linebreak"><mrow><mrow><mtext>V'</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>M</mi><mi>·</mi><mi>N</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></munderover><mi>h</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(1)</td>
</tr></table>
<p><em>L</em> denotes the maximum grayscale level, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e003"><math id="M3" display="inline" overflow="linebreak"><mrow><mi>M</mi><mspace width="0.167em"></mspace><mrow><mi>×</mi></mrow><mspace width="0.167em"></mspace><mi>N</mi></mrow></math></span> is the total number of pixels, and <em>h</em>(<em>i</em>) represents the frequency of the grayscale level <em>i</em>. This equalization further improves the detail of the seed surface.we applied an adaptive threshold segmentation method to separate the seed region from the background, generating an initial binary mask:</p>
<table class="disp-formula p" id="pone.0327371.e004"><tr>
<td class="formula"><math id="M4" display="block" overflow="linebreak"><mrow><mrow><mtext>T</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><mi>N</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo></mrow></mfrac><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>N</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></munder><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>I</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>−</mo><mi>C</mi><mo fence="true" form="postfix" stretchy="true">)</mo></mrow></mrow></mrow></math></td>
<td class="label">(2)</td>
</tr></table>
<p>In this equation, <em>T</em>(<em>x</em>,<em>y</em>) denotes the threshold value, <em>N</em>(<em>x</em>,<em>y</em>) is the neighborhood around the pixel, and <em>C</em> is a constant for adjustment. This formula calculates the average value within the local neighborhood and adjusts it to achieve binarization. Subsequently, morphological operations, including dilation and erosion, were employed to refine the mask edges. Dilation helps connect adjacent seed regions, while erosion removes noise, making seed contours clearer. After these processing steps, the resulting binary mask accurately represents the seed area.</p>
<p>To annotate irregular defect areas such as cracks and insect holes, we introduced the convex hull algorithm. Traditional rectangular annotations often fail to capture complex defect boundaries, especially for irregular geometries like cracks and insect holes. The convex hull method effectively addresses this limitation by precisely enclosing defect areas.</p>
<p>Given a set of points <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e005"><math id="M5" display="inline" overflow="linebreak"><mrow><mi>P</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>P</mi><mn>1</mn></msub><mo>,</mo><msub><mi>P</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>P</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow></math></span> representing the boundary of the defect region, the convex hull <em>CH</em>(<em>P</em>) is the smallest convex polygon that encloses the set <em>P</em>. Assuming <em>P</em><sub><em>i</em></sub>, <em>P</em><sub><em>j</em></sub>, and <em>P</em><sub><em>k</em></sub> are three points in the set, the cross product of vectors <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e006"><math id="M6" display="inline" overflow="linebreak"><mrow><msub><mi>P</mi><mi>i</mi></msub><msub><mi>P</mi><mi>j</mi></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e007"><math id="M7" display="inline" overflow="linebreak"><mrow><msub><mi>P</mi><mi>i</mi></msub><msub><mi>P</mi><mi>k</mi></msub></mrow></math></span> is calculated as follows:</p>
<table class="disp-formula p" id="pone.0327371.e008"><tr>
<td class="formula"><math id="M8" display="block" overflow="linebreak"><mrow><mrow><mtext>cross</mtext><mo stretchy="false">(</mo><msub><mi>P</mi><mi>i</mi></msub><mo>,</mo><msub><mi>P</mi><mi>j</mi></msub><mo>,</mo><msub><mi>P</mi><mi>i</mi></msub><mo>,</mo><msub><mi>P</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi>·</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>k</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi>·</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>k</mi></msub><mo>−</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(3)</td>
</tr></table>
<p>After annotation, to match the YOLO object detection model’s input format, all annotated bounding box information was converted to the standard YOLO label format. Each label contains five values: the object class, the center coordinates of the bounding box (<em>x</em>,<em>y</em>), the width (<em>w</em>), and the height (<em>h</em>). These values were normalized, and the segmented data were further augmented to increase model robustness. The augmented data covers diverse lighting conditions, angle variations, and noise simulation to match real-world production environments.</p>
<p>To evaluate the effectiveness of the EfficientSAM segmentation algorithm, we used a subset of manually annotated seed images as a control group for accuracy comparison. Compared with traditional rectangular annotation methods, EfficientSAM significantly improved segmentation accuracy, especially for complex defect shapes. The convex hull annotation method provided more precise coverage of defect areas. The comparison results are presented in <a href="#pone.0327371.t001" class="usa-link">Table 1</a>.</p>
<section class="tw xbox font-sm" id="pone.0327371.t001"><h4 class="obj_head">Table 1. Comparison of annotation methods.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Annotation Method</th>
<th align="left" rowspan="1" colspan="1">Model</th>
<th align="left" rowspan="1" colspan="1">Precision</th>
<th align="left" rowspan="1" colspan="1">Recall</th>
<th align="left" rowspan="1" colspan="1">mAP@0.5</th>
<th align="left" rowspan="1" colspan="1">mAP@0.5:0.95</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Conventional</td>
<td align="left" rowspan="1" colspan="1">YOLOv8n</td>
<td align="left" rowspan="1" colspan="1">0.794</td>
<td align="left" rowspan="1" colspan="1">0.817</td>
<td align="left" rowspan="1" colspan="1">0.783</td>
<td align="left" rowspan="1" colspan="1">0.527</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">EfficientSAM</td>
<td align="left" rowspan="1" colspan="1">YOLOv8n</td>
<td align="left" rowspan="1" colspan="1">0.853</td>
<td align="left" rowspan="1" colspan="1">0.861</td>
<td align="left" rowspan="1" colspan="1">0.891</td>
<td align="left" rowspan="1" colspan="1">0.679</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327371.t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec006"><h3 class="pmc_sec_title">Data augmentation</h3>
<p>The color images were first converted to grayscale, reducing both storage space and computational complexity by approximately one-third. Subsequently, each pixel value was multiplied by a randomly generated brightness factor ranging from 0.5 to 1.5, enhancing the model’s robustness to variations in illumination. Next, the images were randomly rotated within an angle range of –90<sup>°</sup> to 90<sup>°</sup>, and Gaussian noise was added with a 50% probability, using a noise matrix generated from a Gaussian distribution with a mean of 0 and a standard deviation of 0.01. Finally, multiple augmented images were randomly combined into a single large image using 2<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e009"><math id="M9" display="inline" overflow="linebreak"><mrow><mi>×</mi></mrow></math></span>2, 3<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e010"><math id="M10" display="inline" overflow="linebreak"><mrow><mi>×</mi></mrow></math></span>3, or 4<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e011"><math id="M11" display="inline" overflow="linebreak"><mrow><mi>×</mi></mrow></math></span>4 grid patterns, and the corresponding labels were updated based on the offset caused by the image stitching. <a href="#pone.0327371.g004" class="usa-link">Fig 4</a> illustrates the augmentation process across different samples.</p>
<figure class="fig xbox font-sm" id="pone.0327371.g004"><h4 class="obj_head">Fig 4. The overall structure of the Oak-YOLO model.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/126bcd10619c/pone.0327371.g004.jpg" loading="lazy" height="305" width="750" alt="Fig 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>Subsequently, Gaussian noise was added to the images with a 50% probability. The noise was generated from a Gaussian distribution with a mean of 0 and a standard deviation of 0.01, and was randomly superimposed on the pixel values of the images. To further enhance the dataset, multiple images were concatenated into a single large image, with random grid sizes (2x2, 3x3, or 4x4), and corresponding label files were generated. After each concatenation, the coordinates of the labels were adjusted according to the offset of the concatenated images.</p>
<p><a href="#pone.0327371.g003" class="usa-link">Fig 3</a>(a) illustrates the distribution of <em>x</em> and <em>y</em> coordinates for all annotations in the -seed dataset. The figure shows that annotation centers are evenly distributed across the image, with no significant clustering or outliers. This indicates that the seed positions exhibit good diversity, which helps the model learn a wider range of features. Additionally, the distributions of annotation width (<em>w</em>) and height (<em>h</em>) are shown, with <em>w</em> and <em>h</em> primarily concentrated between 0.01 and 0.05. This suggests a certain degree of clustering in annotation sizes, likely due to denser seed placement in specific image regions, leading to more similar annotation sizes in those areas. In contrast, the distribution of <em>x</em> and <em>y</em> coordinates in <a href="#pone.0327371.g003" class="usa-link">Fig 3</a>(b) appears more concentrated, likely reflecting the characteristics of densely packed detection samples. Similarly, <em>w</em> and <em>h</em> are concentrated between 0.015 and 0.04, further highlighting the annotation patterns in densely populated regions.</p>
<figure class="fig xbox font-sm" id="pone.0327371.g003"><h4 class="obj_head">Fig 3. Comparison of concat-image and densely detected data.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/912fe9bc4d15/pone.0327371.g003.jpg" loading="lazy" height="681" width="658" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>(a) Features of concat-image data. (b) Features of densely detected data.</p></figcaption></figure><p>After applying the aforementioned augmentation techniques, the number of single-object seed images in the dataset was increased from 2,000 to 2,537, while the multi-object dataset grew from 1,572 to 1,949 images. The dataset was split into training, validation, and test sets using an 8:1:1 ratio with stratified random sampling to ensure that the class distribution remained consistent across all subsets.Specifically, the training set consists of 4,538 images, while the testing and validation sets each contain 567 images. The test set was exclusively reserved for final performance evaluation.The single-object dataset is named oak-simple, and the multi-object dataset is named oak-intensive, as summarized in <a href="#pone.0327371.t002" class="usa-link">Table 2</a>.</p>
<section class="tw xbox font-sm" id="pone.0327371.t002"><h4 class="obj_head">Table 2. Class distribution.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1"></th>
<th align="left" rowspan="1" colspan="1">Oak-simple</th>
<th align="left" rowspan="1" colspan="1">Oak-intensive</th>
<th align="left" rowspan="1" colspan="1">Total</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Wormhole</td>
<td align="left" rowspan="1" colspan="1">1638</td>
<td align="left" rowspan="1" colspan="1">2436</td>
<td align="left" rowspan="1" colspan="1">4074</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Crack</td>
<td align="left" rowspan="1" colspan="1">2086</td>
<td align="left" rowspan="1" colspan="1">2924</td>
<td align="left" rowspan="1" colspan="1">5010</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Training set</td>
<td align="left" rowspan="1" colspan="1">2573</td>
<td align="left" rowspan="1" colspan="1">1949</td>
<td align="left" rowspan="1" colspan="1">4522</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Val set</td>
<td align="left" rowspan="1" colspan="1">1638</td>
<td align="left" rowspan="1" colspan="1">2436</td>
<td align="left" rowspan="1" colspan="1">4074</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Test set</td>
<td align="left" rowspan="1" colspan="1">1638</td>
<td align="left" rowspan="1" colspan="1">2436</td>
<td align="left" rowspan="1" colspan="1">4074</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327371.t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec007"><h3 class="pmc_sec_title">Oak-YOLO</h3>
<p>YOLOv8 [<a href="#pone.0327371.ref022" class="usa-link" aria-describedby="pone.0327371.ref022">22</a>] exhibits high real-time detection capability and accuracy optimization, making it suitable for object detection tasks. However, YOLOv8 still has limitations in detecting irregular-shaped defects in oak seeds, as the rectangular bounding box cannot accurately describe the edges, leading to incomplete detection or boundary deviation. Additionally, its real-time performance and efficiency are limited when deployed on resource-constrained embedded devices.</p>
<p>As shown in <a href="#pone.0327371.g004" class="usa-link">Fig 4</a>, we improved YOLOv8 in three aspects: 1) Introducing the Ghost-Dynamic prediction head, which combines shallow and deep features to effectively enhance the detection accuracy of small defects such as cracks and wormholes; 2) Upgrading the YOLOv8 backbone by replacing traditional CNN modules with EfficientViT to improve the capture of global features and long-range dependencies; 3) Employing the WIoUv3 loss function to optimize IoU calculation for small and overlapping defects, ensuring that the predicted bounding box shape better matches the defect characteristics.</p>
<section id="sec008"><h4 class="pmc_sec_title">Ghost-dynamic.</h4>
<p>In the YOLOv8 model, instead of using the shallow feature map <em>F</em><sub>2</sub> with limited semantic information, the deeper feature maps <em>F</em><sub>3</sub>, <em>F</em><sub>4</sub>, and <em>F</em><sub>5</sub>, extracted from the backbone network are passed into the neck for feature fusion. These feature maps, after undergoing multiple convolutional down-sampling layers, gradually expand their receptive fields. The deeper feature maps contain more rich semantic information, sufficient to handle typical object detection tasks. However, in the case of oak seed defect detection, particularly for small and complex defects such as insect holes and cracks, using deeper features may lead to information dilution, thereby reducing both detection precision and localization accuracy. This is because these defects often appear as small objects, and the positional information in deeper features is relatively sparse and blurred due to the convolutional layers, making precise defect localization and classification more challenging [<a href="#pone.0327371.ref023" class="usa-link" aria-describedby="pone.0327371.ref023">23</a>]. Moreover, due to insufficient information extracted by the prediction head from the feature maps, the detection accuracy of defects is compromised. Similar, overlapping, and partially occluded defects further exacerbate the difficulty in object detection.</p>
<p>Shallow feature maps, in contrast to deep feature maps, have smaller receptive fields, higher spatial resolution, and more precise positional information, making them particularly effective for small object detection. This makes them highly suitable for the accurate detection of seed defects, such as cracks and insect holes. Therefore, to improve the detection performance for oak seed defects, this study integrates the Ghost module [<a href="#pone.0327371.ref024" class="usa-link" aria-describedby="pone.0327371.ref024">24</a>] with Dynamic Convolution [<a href="#pone.0327371.ref025" class="usa-link" aria-describedby="pone.0327371.ref025">25</a>] to construct a high-resolution Ghost-Dynamic prediction head, as illustrated in <a href="#pone.0327371.g005" class="usa-link">Fig 5</a>. This approach effectively leverages the rich positional information and high resolution inherent in shallow features, enhancing the detection performance for small defect targets. Ghost-DynamicConv utilizes a two-step convolution operation by dividing the convolution process into primary and auxiliary convolutions. The primary convolution generates initial features <em>Y</em>, and the auxiliary convolution further optimizes the feature extraction. The final feature <em>F</em> can be expressed as:</p>
<figure class="fig xbox font-sm" id="pone.0327371.g005"><h5 class="obj_head">Fig 5. Dynamic-Ghost architecture.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/fbe3e4497052/pone.0327371.g005.jpg" loading="lazy" height="416" width="660" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><table class="disp-formula p" id="pone.0327371.e012"><tr>
<td class="formula"><math id="M12" display="block" overflow="linebreak"><mrow><mrow><mi>F</mi><mo>=</mo><msub><mi>W</mi><mn>1</mn></msub><mi>·</mi><mi>X</mi><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo>+</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>W</mi><mn>2</mn></msub><mi>·</mi><mi>X</mi><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(4)</td>
</tr></table>
<p><em>W</em><sub>1</sub> and <em>W</em><sub>2</sub> represent the weights of the primary and auxiliary convolutions, respectively, and <em>f</em>denotes the nonlinear activation function. The core of the dynamic weight allocation mechanism lies in selecting appropriate expert convolution kernels based on the features of the input image. This is achieved through a multi-layer perceptron (MLP), which generates dynamic weights. Specifically, the MLP takes the input image features as input and outputs a dynamic weight vector that indicates the activation level of different expert convolution kernels. The process of generating dynamic weights is as follows:</p>
<table class="disp-formula p" id="pone.0327371.e013"><tr>
<td class="formula"><math id="M13" display="block" overflow="linebreak"><mrow><mrow><msub><mi>W</mi><mrow><mtext>dynamic</mtext></mrow></msub><mo>=</mo><mi>W</mi><mo>+</mo><mi>σ</mi><mo stretchy="false">(</mo><msub><mi>W</mi><mn>2</mn></msub><mi>·</mi><mi>X</mi><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow></math></td>
<td class="label">(5)</td>
</tr></table>
<p>As shown in <a href="#pone.0327371.g006" class="usa-link">Fig 6</a>, To further enhance the richness of feature fusion, the PAN (Path Aggregation Network) structure is employed to fuse the <em>F</em><sub>2</sub> feature map from the backbone network with feature maps from other scales (e.g., <em>F</em><sub>3</sub>, <em>F</em><sub>4</sub>, etc.). This fusion process facilitates the creation of a smaller P2 prediction head. The introduction of the P2 prediction head provides additional positional and feature information for defect detection, while effectively reducing the loss of spatial features that can occur during down-sampling due to scale variations. When combined with the other three prediction heads, the P2 head helps mitigate the decline in detection accuracy that is often caused by significant changes in object scales. This method enables the prediction heads to extract richer global features through a self-attention mechanism, which is particularly beneficial for the precise localization of defects in cases with significant overlap or occlusion.</p>
<figure class="fig xbox font-sm" id="pone.0327371.g006"><h5 class="obj_head">Fig 6. Dynamic computation flow.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g006.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/f46f25c9652f/pone.0327371.g006.jpg" loading="lazy" height="387" width="740" alt="Fig 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec009"><h4 class="pmc_sec_title">EfficientViT.</h4>
<p>EfficientViT [<a href="#pone.0327371.ref026" class="usa-link" aria-describedby="pone.0327371.ref026">26</a>] is an efficient visual Transformer model designed to combine the strengths of Convolutional Neural Networks (CNN) and self-attention mechanisms, providing more precise and computationally efficient feature extraction for visual tasks. <a href="#pone.0327371.g007" class="usa-link">Fig 7</a> shows the structure of EfficientViT.In the context of oak seed defect detection, where defects such as cracks and insect holes are structurally complex, small in area, and irregular in shape, traditional CNN models face limitations in capturing long-range dependencies and global features. The core optimizations of EfficientViT are reflected in several key aspects. The “Sandwich Layout" effectively reduces memory consumption. Unlike conventional self-attention mechanisms, EfficientViT uses a single layer of self-attention for spatial feature mixing, supplemented by additional feed-forward network (FFN) layers before and after the self-attention layer to enhance communication between channels. The mathematical expression of this optimization is as follows:</p>
<figure class="fig xbox font-sm" id="pone.0327371.g007"><h5 class="obj_head">Fig 7. EfficientViT architecture.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g007.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/bd07ae3a5c37/pone.0327371.g007.jpg" loading="lazy" height="509" width="748" alt="Fig 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><table class="disp-formula p" id="pone.0327371.e014"><tr>
<td class="formula"><math id="M14" display="block" overflow="linebreak"><mrow><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><munder><mo>∏</mo><mrow><mi>i</mi></mrow></munder><msubsup><mi>φ</mi><mrow><mi>i</mi></mrow><mrow><mi>F</mi></mrow></msubsup><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msubsup><mi>φ</mi><mrow><mi>i</mi></mrow><mrow><mi>A</mi></mrow></msubsup><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><munder><mo>∏</mo><mrow><mi>i</mi></mrow></munder><msubsup><mi>φ</mi><mrow><mi>i</mi></mrow><mrow><mi>F</mi></mrow></msubsup><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msub><mi>X</mi><mrow><mi>i</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow></mrow></mrow></math></td>
<td class="label">(6)</td>
</tr></table>
<p><em>X</em><sub><em>i</em></sub> represents the input features of the <em>i</em>-th layer, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e015"><math id="M15" display="inline" overflow="linebreak"><mrow><msubsup><mi>φ</mi><mi>i</mi><mi>A</mi></msubsup></mrow></math></span> denotes the self-attention layer, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e016"><math id="M16" display="inline" overflow="linebreak"><mrow><msubsup><mi>φ</mi><mi>i</mi><mi>F</mi></msubsup></mrow></math></span> denotes the feed-forward network layer. By reducing the usage of self-attention layers, EfficientViT significantly lowers memory consumption while enhancing channel communication by increasing the number of feed-forward network layers. In the computation of Multi-Head Self-Attention (MHSA), EfficientViT introduces the Cascaded Group Attention (CGA) mechanism to further enhance computational efficiency. Unlike traditional MHSA models, CGA partitions the input features into multiple subsets, with each attention head processing only a portion of the features, thus reducing redundancy in feature computation. The core formula of this mechanism is given by:</p>
<table class="disp-formula p" id="pone.0327371.e017"><tr>
<td class="formula"><math id="M17" display="block" overflow="linebreak"><mrow><mrow><mover><mrow><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mtext>¯</mtext></mrow></mover><mo>=</mo><mtext>A_ttn</mtext><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msubsup><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>Q</mi></msubsup><mo>,</mo><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msubsup><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>K</mi></msubsup><mo>,</mo><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msubsup><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>V</mi></msubsup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>*</mo></mrow></mrow></math></td>
<td class="label">(7)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327371.e018"><tr>
<td class="formula"><math id="M18" display="block" overflow="linebreak"><mrow><mrow><mover><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mrow><mtext>¯</mtext></mrow></mover><mo>=</mo><mtext>Concat</mtext><msubsup><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mover><mrow><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mtext>¯</mtext></mrow></mover><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>h</mi></mrow></msubsup><msubsup><mi>W</mi><mi>i</mi><mi>P</mi></msubsup></mrow></mrow></math></td>
<td class="label">(8)</td>
</tr></table>
<p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e019"><math id="M19" display="inline" overflow="linebreak"><mrow><mover><mrow><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mtext>¯</mtext></mrow></mover></mrow></math></span> represents the self-attention result of the <em>j</em>-th head, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e020"><math id="M20" display="inline" overflow="linebreak"><mrow><msubsup><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>Q</mi></msubsup></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e021"><math id="M21" display="inline" overflow="linebreak"><mrow><msubsup><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>K</mi></msubsup></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e022"><math id="M22" display="inline" overflow="linebreak"><mrow><msubsup><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>V</mi></msubsup></mrow></math></span> are the projection matrices, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e023"><math id="M23" display="inline" overflow="linebreak"><mrow><msubsup><mi>W</mi><mi>i</mi><mi>P</mi></msubsup></mrow></math></span> is the final projection layer. Through this grouped computation, CGA effectively reduces the computational redundancy of MHSA, while the concatenation operation enhances the feature representational power.</p></section><section id="sec010"><h4 class="pmc_sec_title">WIoUv3.</h4>
<p>The size of the object is crucial in object detection tasks, especially in small object detection, where the standard IoU may not give sufficient attention due to the small area occupied by small objects. Existing bounding box loss functions, such as Smooth L1 [<a href="#pone.0327371.ref027" class="usa-link" aria-describedby="pone.0327371.ref027">27</a>], GIoU [<a href="#pone.0327371.ref028" class="usa-link" aria-describedby="pone.0327371.ref028">28</a>] and CIoU [<a href="#pone.0327371.ref029" class="usa-link" aria-describedby="pone.0327371.ref029">29</a>], often fail to effectively address the complex scenarios in defect detection, such as occlusion, small sizes, and class imbalance. To address this, WIoU introduces a weighted IoU calculation that assigns greater loss contributions to small objects. WIoU [<a href="#pone.0327371.ref030" class="usa-link" aria-describedby="pone.0327371.ref030">30</a>] is a weighted variant of the standard IoU, which adjusts the loss for each object or region by introducing a weight factor <em>w</em> based on the standard IoU calculation. WIoUv3 improves upon this by introducing a dynamic non-monotonic focusing mechanism, allowing the model to allocate more computational resources to challenging samples with occluded or small objects, thus improving the accuracy and stability of the training process. Additionally, WIoUv3 provides a more refined method for bounding box regression by considering the overlap degree and dynamically adjusting the loss contribution of each bounding box. The weight factor based on the target center distance is defined as:</p>
<table class="disp-formula p" id="pone.0327371.e024"><tr>
<td class="formula"><math id="M24" display="block" overflow="linebreak"><mrow><mrow><msub><mi>w</mi><mrow><mtext>dist</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mo stretchy="false">(</mo><msub><mi>d</mi><mrow><mtext>center</mtext></mrow></msub><mo>+</mo><mi>ϵ</mi><msup><mo stretchy="false">)</mo><mrow><mi>β</mi></mrow></msup></mrow></mfrac></mrow></mrow></math></td>
<td class="label">(9)</td>
</tr></table>
<p>where <em>ε</em> is a small constant to prevent division by zero, and <em>β</em> is a hyperparameter that adjusts the impact of the distance. The formula for WIoU, incorporating the weight factor <em>w</em>, is expressed as:</p>
<table class="disp-formula p" id="pone.0327371.e025"><tr>
<td class="formula"><math id="M25" display="block" overflow="linebreak"><mrow><mrow><mtext>WIoU</mtext><mo>=</mo><mfrac><mrow><mi>w</mi><mi>·</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∩</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><mrow><mi>w</mi><mi>·</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∪</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><msub><mi>w</mi><mrow><mtext>size</mtext></mrow></msub><mi>·</mi><msub><mi>w</mi><mrow><mtext>dist</mtext></mrow></msub><mi>·</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∩</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>w</mi><mrow><mtext>size</mtext></mrow></msub><mi>·</mi><msub><mi>w</mi><mrow><mtext>dist</mtext></mrow></msub><mi>·</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∪</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mrow></math></td>
<td class="label">(10)</td>
</tr></table>
<p>We conducted experiments on the dataset, comparing WIoUv3 with existing bounding box loss functions (such as Smooth L1, CIoU, and GIoU). As shown in <a href="#pone.0327371.t003" class="usa-link">Table 3</a>. The experimental show that WIoUv3 significantly improves model performance, especially in handling complex scenarios like occlusion, small size, and class imbalance.</p>
<section class="tw xbox font-sm" id="pone.0327371.t003"><h5 class="obj_head">Table 3. Performance comparison of different loss functions.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Loss Functions</th>
<th align="left" rowspan="1" colspan="1">Precision (%)</th>
<th align="left" rowspan="1" colspan="1">Recall (%)</th>
<th align="left" rowspan="1" colspan="1">mAP</th>
<th align="left" rowspan="1" colspan="1">F1-score (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Smooth L1</td>
<td align="left" rowspan="1" colspan="1">89.2</td>
<td align="left" rowspan="1" colspan="1">84.5</td>
<td align="left" rowspan="1" colspan="1">91.0</td>
<td align="left" rowspan="1" colspan="1">86.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">CIoU</td>
<td align="left" rowspan="1" colspan="1">90.1</td>
<td align="left" rowspan="1" colspan="1">85.3</td>
<td align="left" rowspan="1" colspan="1">92.2</td>
<td align="left" rowspan="1" colspan="1">87.6</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GIoU</td>
<td align="left" rowspan="1" colspan="1">89.8</td>
<td align="left" rowspan="1" colspan="1">85.0</td>
<td align="left" rowspan="1" colspan="1">91.7</td>
<td align="left" rowspan="1" colspan="1">87.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">WIoUv3 (Ours)</td>
<td align="left" rowspan="1" colspan="1">94.5</td>
<td align="left" rowspan="1" colspan="1">87.8</td>
<td align="left" rowspan="1" colspan="1">94.5</td>
<td align="left" rowspan="1" colspan="1">90.1</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327371.t003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section></section><section id="sec011"><h2 class="pmc_sec_title">Results and discussion</h2>
<section id="sec012"><h3 class="pmc_sec_title">Indicators for model evaluation</h3>
<p>These metrics are used to evaluate the model’s effectiveness in detecting seed cracks and insect hole defects, with the following formulas applied:</p>
<table class="disp-formula p" id="pone.0327371.e026"><tr>
<td class="formula"><math id="M26" display="block" overflow="linebreak"><mrow><mrow><mtext>Acc</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mrow></math></td>
<td class="label">(11)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327371.e027"><tr>
<td class="formula"><math id="M27" display="block" overflow="linebreak"><mrow><mrow><mtext>P</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow></mrow></math></td>
<td class="label">(12)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327371.e028"><tr>
<td class="formula"><math id="M28" display="block" overflow="linebreak"><mrow><mrow><mtext>R</mtext><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mrow></math></td>
<td class="label">(13)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327371.e029"><tr>
<td class="formula"><math id="M29" display="block" overflow="linebreak"><mrow><mrow><mtext>AP</mtext><mo>=</mo><msubsup><mo>∫</mo><mrow><mn>0</mn></mrow><mrow><mn>1</mn></mrow></msubsup><mtext>Precision</mtext><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo><mspace width="0.167em"></mspace><mtext>d</mtext><mi>r</mi></mrow></mrow></math></td>
<td class="label">(14)</td>
</tr></table>
<table class="disp-formula p" id="pone.0327371.e030"><tr>
<td class="formula"><math id="M30" display="block" overflow="linebreak"><mrow><mrow><mtext>mAP</mtext><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></munderover><msub><mtext>AP</mtext><mi>r</mi></msub></mrow></mrow></math></td>
<td class="label">(15)</td>
</tr></table>
<p>TP represents true positives (correct defect predictions), TN denotes true negatives (correct non-defect predictions), FP refers to false positives (incorrect defect predictions), FN indicates false negatives (missed defects), and N is the total number of categories. <em>R</em><sub><em>r</em></sub> and <em>P</em><sub><em>r</em></sub> represent recall and precision for the <em>r</em>-th class respectively.</p></section><section id="sec013"><h3 class="pmc_sec_title">Experimental configuration</h3>
<p>As shown in <a href="#pone.0327371.t004" class="usa-link">Table 4</a>, the hardware and software environment of the experimental testing platform are listed below. We use the Adam optimizer to fine-tune the model parameters, Compared to traditional optimizers such as SGD, Adam provides adaptive learning rates and faster convergence, which is beneficial for models like Oak-YOLO that integrate complex modules such as EfficientViT and Ghost-DynamicConv. The training process consists of a maximum of 300 epochs, and the batch size is set to 16, with an initial learning rate set to 0.001.The hyperparameters used in training, including the learning rate, batch size, and optimizer settings, were determined through manual tuning based on empirical performance on the validation set.</p>
<section class="tw xbox font-sm" id="pone.0327371.t004"><h4 class="obj_head">Table 4. Experimental test platform.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Hardware/Software</th>
<th align="left" rowspan="1" colspan="1">Model/Version</th>
<th align="left" rowspan="1" colspan="1">Details</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">GPU</td>
<td align="left" rowspan="1" colspan="1">NVIDIA GeForce RTX 3090Ti</td>
<td align="left" rowspan="1" colspan="1">Video Memory: 24 GB</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Computer System</td>
<td align="left" rowspan="1" colspan="1">Windows 10</td>
<td align="left" rowspan="1" colspan="1">RAM: 64 GB</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Deep Learning Framework</td>
<td align="left" rowspan="1" colspan="1">PyTorch</td>
<td align="left" rowspan="1" colspan="1">Version: 1.12</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Computational Platform</td>
<td align="left" rowspan="1" colspan="1">CUDA</td>
<td align="left" rowspan="1" colspan="1">Version: 11.7</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">IDE</td>
<td align="left" rowspan="1" colspan="1">PyCharm</td>
<td align="left" rowspan="1" colspan="1">Version: 2020.1.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Programming Language</td>
<td align="left" rowspan="1" colspan="1">Python</td>
<td align="left" rowspan="1" colspan="1">Version: 3.8</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327371.t004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec014"><h3 class="pmc_sec_title">Results of ablation experiments</h3>
<p>To evaluate the effectiveness of each proposed module, a series of ablation experiments were conducted on the oak seed dataset using consistent training, validation, and testing protocols. The detailed quantitative results are summarized in <a href="#pone.0327371.t005" class="usa-link">Table 5</a>. Integrating the Ghost-Dynamic module into the YOLOv8 detection head resulted in consistent improvements in mAP50, F1 score, and inference speed. Specifically, the enhanced YOLOv8-Ghost-Dynamic variant achieved an mAP50 of 95.97% and a precision of 98.61%, while also demonstrating a speed increase of 12.4 FPS (reducing inference time by 1.1 ms per image) compared to the baseline YOLOv8 model. Notably, the full Oak-YOLO configuration, which incorporates both the Ghost-Dynamic and EfficientViT modules, delivered the most substantial gains: it achieved an mAP50 of 96.92%, a precision of 98.12%, and an inference speed of 132.2 FPS (corresponding to 7.6 ms per image). These results underscore the effectiveness of the multi-module design in improving both accuracy and real-time performance.</p>
<section class="tw xbox font-sm" id="pone.0327371.t005"><h4 class="obj_head">Table 5. Ablation study results.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Ghost-Dynamic</th>
<th align="left" rowspan="1" colspan="1">EfficientViT</th>
<th align="left" rowspan="1" colspan="1">WIoUv3</th>
<th align="left" rowspan="1" colspan="1">mAP50</th>
<th align="left" rowspan="1" colspan="1">F1 (%)</th>
<th align="left" rowspan="1" colspan="1">Recall (%)</th>
<th align="left" rowspan="1" colspan="1">Parameter/M</th>
<th align="left" rowspan="1" colspan="1">Weight/MB</th>
<th align="left" rowspan="1" colspan="1">GFLOPS</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">90.5</td>
<td align="left" rowspan="1" colspan="1">88.8</td>
<td align="left" rowspan="1" colspan="1">86.5</td>
<td align="left" rowspan="1" colspan="1">37.2</td>
<td align="left" rowspan="1" colspan="1">74.8</td>
<td align="left" rowspan="1" colspan="1">12.6</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">91.8</td>
<td align="left" rowspan="1" colspan="1">88.1</td>
<td align="left" rowspan="1" colspan="1">85.7</td>
<td align="left" rowspan="1" colspan="1">37.2</td>
<td align="left" rowspan="1" colspan="1">74.8</td>
<td align="left" rowspan="1" colspan="1">9.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">93.2</td>
<td align="left" rowspan="1" colspan="1">90.8</td>
<td align="left" rowspan="1" colspan="1">86.2</td>
<td align="left" rowspan="1" colspan="1">37.2</td>
<td align="left" rowspan="1" colspan="1">64.8</td>
<td align="left" rowspan="1" colspan="1">7.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">93.0</td>
<td align="left" rowspan="1" colspan="1">91.5</td>
<td align="left" rowspan="1" colspan="1">86.8</td>
<td align="left" rowspan="1" colspan="1">37.2</td>
<td align="left" rowspan="1" colspan="1">64.8</td>
<td align="left" rowspan="1" colspan="1">7.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">93.7</td>
<td align="left" rowspan="1" colspan="1">92.4</td>
<td align="left" rowspan="1" colspan="1">87.6</td>
<td align="left" rowspan="1" colspan="1">36.0</td>
<td align="left" rowspan="1" colspan="1">68.2</td>
<td align="left" rowspan="1" colspan="1">7.0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">93.8</td>
<td align="left" rowspan="1" colspan="1">92.7</td>
<td align="left" rowspan="1" colspan="1">88.2</td>
<td align="left" rowspan="1" colspan="1">36.1</td>
<td align="left" rowspan="1" colspan="1">67.4</td>
<td align="left" rowspan="1" colspan="1">6.9</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">94.0</td>
<td align="left" rowspan="1" colspan="1">94.10</td>
<td align="left" rowspan="1" colspan="1">89.6</td>
<td align="left" rowspan="1" colspan="1">35.6</td>
<td align="left" rowspan="1" colspan="1">66.0</td>
<td align="left" rowspan="1" colspan="1">6.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">✓</td>
<td align="left" rowspan="1" colspan="1">94.5</td>
<td align="left" rowspan="1" colspan="1">95.3</td>
<td align="left" rowspan="1" colspan="1">90.5</td>
<td align="left" rowspan="1" colspan="1">34.2</td>
<td align="left" rowspan="1" colspan="1">62.9</td>
<td align="left" rowspan="1" colspan="1">7.2</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327371.t005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>As illustrated in <a href="#pone.0327371.g008" class="usa-link">Fig 8</a>, precision and recall curves gradually converge after approximately 100 epochs, exhibiting minimal fluctuations in the later stages.Notably, Oak-YOLO consistently outperforms other models throughout training in both mAP0.5 and mAP0.5:0.95 metrics, achieving higher accuracy and faster convergence. This suggests that the integration of Ghost-Dynamic and EfficientViT modules enhances both the localization precision and the generalization capability of the model across different IoU thresholds.From the comparison of the confusion matrices, as shown in <a href="#pone.0327371.g009" class="usa-link">Fig 9</a>. It is evident that YOLOv8 performs poorly in identifying cracks, with frequent missed detections. Following the enhancement, the issues of missed detections and misclassifications are significantly alleviated, achieving detection rates above 95% for both wormholes and cracks.</p>
<figure class="fig xbox font-sm" id="pone.0327371.g008"><h4 class="obj_head">Fig 8. Comparison of ablation experiments on different datasets.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g008.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/f6cbd6c45d1e/pone.0327371.g008.jpg" loading="lazy" height="660" width="660" alt="Fig 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><figure class="fig xbox font-sm" id="pone.0327371.g009"><h4 class="obj_head">Fig 9. Comparison of Confusion Matrices between YOLOv8 and OSK-YOLO8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g009.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/5e2015b6330d/pone.0327371.g009.jpg" loading="lazy" height="313" width="715" alt="Fig 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g009/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec015"><h3 class="pmc_sec_title">Comparative performance analysis against alternative models</h3>
<p>Based on the experimental results, our study primarily focuses on evaluating the differences between OSK-YOLO8 and YOLOv8 in terms of detection performance and inference speed. Two representative categories of object detection models were selected for comparative analysis: Transformer-based detectors, including rt-DETR-18 [<a href="#pone.0327371.ref031" class="usa-link" aria-describedby="pone.0327371.ref031">31</a>], rt-DETR-50 [<a href="#pone.0327371.ref031" class="usa-link" aria-describedby="pone.0327371.ref031">31</a>], and Deformable DETR [<a href="#pone.0327371.ref032" class="usa-link" aria-describedby="pone.0327371.ref032">32</a>]; and lightweight CNN-based detectors, comprising the YOLO series (YOLOv5s [<a href="#pone.0327371.ref033" class="usa-link" aria-describedby="pone.0327371.ref033">33</a>], YOLOv7 [<a href="#pone.0327371.ref034" class="usa-link" aria-describedby="pone.0327371.ref034">34</a>], YOLOv9 [<a href="#pone.0327371.ref035" class="usa-link" aria-describedby="pone.0327371.ref035">35</a>], YOLOv10 [<a href="#pone.0327371.ref036" class="usa-link" aria-describedby="pone.0327371.ref036">36</a>], YOLOv11 [<a href="#pone.0327371.ref037" class="usa-link" aria-describedby="pone.0327371.ref037">37</a>], and YOLOv12 [<a href="#pone.0327371.ref038" class="usa-link" aria-describedby="pone.0327371.ref038">38</a>]) as well as the proposed OSK-YOLO8.</p>
<p>As shown in <a href="#pone.0327371.t006" class="usa-link">Table 6</a>, although Transformer-based models achieved superior detection accuracy, they require over 40 million parameters and entail high computational complexity. In contrast, the YOLO series strikes a better balance between accuracy and model efficiency. Notably, YOLOv5s and YOLOv7 have relatively small parameter sizes of only 16.4M and 18.8M, respectively. However, due to their limited number of convolutional layers and channels, these models struggle to extract effective features in complex backgrounds. It is worth highlighting that YOLOv9 to YOLOv12 exhibit a consistent improvement in detection performance while maintaining the efficiency of the YOLO architecture. Specifically, YOLOv9 achieves an mAP50 of 94.5%, which increases to 97.3% in YOLOv12. Nonetheless, this performance gain comes at the cost of increased model size and complexity—parameter counts grow from 21.2M to 29.8M, GFLOPs rise from 12.3 to 29.2, and inference speed declines from 195 FPS to 120 FPS, indicating a trade-off between performance and computational cost.</p>
<section class="tw xbox font-sm" id="pone.0327371.t006"><h4 class="obj_head">Table 6. Comparative Evaluation of Detection Performance and Computational Efficiency Across Transformer-Based and YOLO Series Models.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Models</th>
<th align="left" rowspan="1" colspan="1">mAP50 (%)</th>
<th align="left" rowspan="1" colspan="1">FPS</th>
<th align="left" rowspan="1" colspan="1">Parameters (M)</th>
<th align="left" rowspan="1" colspan="1">GFLOPS</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">rt-DETR-18</td>
<td align="left" rowspan="1" colspan="1">96.4</td>
<td align="left" rowspan="1" colspan="1">75</td>
<td align="left" rowspan="1" colspan="1">39.5</td>
<td align="left" rowspan="1" colspan="1">56.9</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">rt-DETR-50</td>
<td align="left" rowspan="1" colspan="1">98.1</td>
<td align="left" rowspan="1" colspan="1">60</td>
<td align="left" rowspan="1" colspan="1">48.2</td>
<td align="left" rowspan="1" colspan="1">92.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Deformable DETR</td>
<td align="left" rowspan="1" colspan="1">97.8</td>
<td align="left" rowspan="1" colspan="1">55</td>
<td align="left" rowspan="1" colspan="1">48.7</td>
<td align="left" rowspan="1" colspan="1">78.6</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">YOLOv5s</td>
<td align="left" rowspan="1" colspan="1">92.1</td>
<td align="left" rowspan="1" colspan="1">230</td>
<td align="left" rowspan="1" colspan="1">16.4</td>
<td align="left" rowspan="1" colspan="1">8.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">YOLOv7</td>
<td align="left" rowspan="1" colspan="1">93.4</td>
<td align="left" rowspan="1" colspan="1">211</td>
<td align="left" rowspan="1" colspan="1">18.8</td>
<td align="left" rowspan="1" colspan="1">9.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">YOLOv9</td>
<td align="left" rowspan="1" colspan="1">94.5</td>
<td align="left" rowspan="1" colspan="1">195</td>
<td align="left" rowspan="1" colspan="1">21.2</td>
<td align="left" rowspan="1" colspan="1">12.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">YOLOv10</td>
<td align="left" rowspan="1" colspan="1">95.7</td>
<td align="left" rowspan="1" colspan="1">162</td>
<td align="left" rowspan="1" colspan="1">24.5</td>
<td align="left" rowspan="1" colspan="1">18.7</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">YOLOv11</td>
<td align="left" rowspan="1" colspan="1">96.8</td>
<td align="left" rowspan="1" colspan="1">135</td>
<td align="left" rowspan="1" colspan="1">27.1</td>
<td align="left" rowspan="1" colspan="1">22.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">YOLOv12</td>
<td align="left" rowspan="1" colspan="1">97.3</td>
<td align="left" rowspan="1" colspan="1">120</td>
<td align="left" rowspan="1" colspan="1">29.8</td>
<td align="left" rowspan="1" colspan="1">29.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Ours (Oak-YOLO)</td>
<td align="left" rowspan="1" colspan="1">96.2</td>
<td align="left" rowspan="1" colspan="1">264</td>
<td align="left" rowspan="1" colspan="1">34.2</td>
<td align="left" rowspan="1" colspan="1">7.2</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327371.t006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>Among all evaluated models, OSK-YOLO8 offers the most favorable trade-off between performance and efficiency. It achieves an mAP50 of 96.2%, ranking among the top within the YOLO family. With only 12.0M parameters and 6.4 GFLOPs, it reaches an inference speed of 264 FPS—substantially outperforming all baseline models, including YOLOv12. Compared with YOLOv12, OSK-YOLO8 reduces GFLOPs by approximately 78%, parameter count by nearly 60%, and improves inference speed by over 120 FPS. <a href="#pone.0327371.g010" class="usa-link">Fig 10</a> provides a visual comparison of each model’s accuracy, speed, and computational complexity, clearly demonstrating the comprehensive advantages of OSK-YOLO8.</p>
<figure class="fig xbox font-sm" id="pone.0327371.g010"><h4 class="obj_head">Fig 10. Comparative Evaluation of Detection Performance and Computational Efficiency Across Transformer-Based and YOLO Series Models.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g010.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/a46d32adfa96/pone.0327371.g010.jpg" loading="lazy" height="425" width="674" alt="Fig 10"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g010/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec016"><h3 class="pmc_sec_title">Verify the results of the experiment</h3>
<p>To evaluate the effectiveness of OSK-YOLO8 and YOLOV8n in detecting defects during actual production, this study applied the pre-trained OSK-YOLO8 and YOLOV8n models to validation experiments involving seed images. <a href="#pone.0327371.g011" class="usa-link">Fig 11</a> visually represents the detection results of OSK-YOLO8.</p>
<figure class="fig xbox font-sm" id="pone.0327371.g011"><h4 class="obj_head">Fig 11. Experimental results verification, illustrating the comparison between predicted and actual outcomes.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g011.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/ffa30e861b7f/pone.0327371.g011.jpg" loading="lazy" height="235" width="750" alt="Fig 11"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g011/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>The visualization results demonstrate that in the Oak-Intensive task, characterized by densely packed seeds, the YOLOV8n model struggles to accurately identify defects, especially in scale scenarios lacking fine detail, particularly when cracks dominate the scene. In contrast, OSK-YOLO8, with its dynamic detection head, significantly enhances the expression of fine-grained details.</p>
<p>Similarly, in scenarios dominated by wormholes, YOLOV8n often misjudges fine details, whereas OSK-YOLO8 almost accurately identifies all wormhole cases. In single-defect scenarios, YOLOV8n tends to misinterpret the seed base as a crack defect, while OSK-YOLO8 exhibits outstanding performance in crack detection. Both models demonstrate similar performance in wormhole identification, though YOLOV8n occasionally misclassifies surface textures as wormholes, an issue not observed with OSK-YOLO8.</p></section><section id="sec017"><h3 class="pmc_sec_title">Robustness evaluation</h3>
<p>To validate the model’s robustness across devices and scenarios, we conducted cross-domain testing using images captured by mobile devices. The external validation sets included: 1) OnePlus ACE2 Pro (OnePlus Technology Co., Ltd., Shenzhen, China) and 2) iPhone 13 Plus (Apple Inc., USA), with resolutions of 3,264<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e031"><math id="M31" display="inline" overflow="linebreak"><mrow><mi>×</mi></mrow></math></span>2,448 and 3,024<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0327371.e032"><math id="M32" display="inline" overflow="linebreak"><mrow><mi>×</mi></mrow></math></span>4,032 respectively.</p>
<p>As shown in <a href="#pone.0327371.t007" class="usa-link">Table 7</a>, Oak-YOLO achieved mAP50 scores of 94.7% and 93.8% on mobile-captured test sets (Test3 and Test4), reflecting only a 2.3–3.3% drop compared to the laboratory-controlled environments (Test1 and Test2). Despite the more challenging conditions associated with mobile devices—such as inconsistent lighting and complex backgrounds—the model maintained a high precision of over 92% and F1 scores above 89%, indicating stable and reliable detection performance. Furthermore, the mAP50–95 values on mobile-captured sets remained above 68%, further demonstrating the model’s generalization capability across diverse imaging sources.</p>
<section class="tw xbox font-sm" id="pone.0327371.t007"><h4 class="obj_head">Table 7. Cross-device performance comparison.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Test Set</th>
<th align="left" rowspan="1" colspan="1">mAP50 (%)</th>
<th align="left" rowspan="1" colspan="1">mAP50–95 (%)</th>
<th align="left" rowspan="1" colspan="1">Precision (%)</th>
<th align="left" rowspan="1" colspan="1">Recall (%)</th>
<th align="left" rowspan="1" colspan="1">F1 Score (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Test1 (Oak-simple)</td>
<td align="left" rowspan="1" colspan="1">96.1</td>
<td align="left" rowspan="1" colspan="1">76.3</td>
<td align="left" rowspan="1" colspan="1">95.3</td>
<td align="left" rowspan="1" colspan="1">91.6</td>
<td align="left" rowspan="1" colspan="1">93.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Test2 (Oak-intensive)</td>
<td align="left" rowspan="1" colspan="1">95.8</td>
<td align="left" rowspan="1" colspan="1">75.1</td>
<td align="left" rowspan="1" colspan="1">94.9</td>
<td align="left" rowspan="1" colspan="1">90.7</td>
<td align="left" rowspan="1" colspan="1">92.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Test3 (OnePlus)</td>
<td align="left" rowspan="1" colspan="1">94.7</td>
<td align="left" rowspan="1" colspan="1">70.2</td>
<td align="left" rowspan="1" colspan="1">92.4</td>
<td align="left" rowspan="1" colspan="1">88.3</td>
<td align="left" rowspan="1" colspan="1">90.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Test4 (iPhone)</td>
<td align="left" rowspan="1" colspan="1">93.8</td>
<td align="left" rowspan="1" colspan="1">68.9</td>
<td align="left" rowspan="1" colspan="1">92.1</td>
<td align="left" rowspan="1" colspan="1">87.5</td>
<td align="left" rowspan="1" colspan="1">89.7</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0327371.t007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>To evaluate the defect localization capability of the model in cross-domain scenarios, this study uses the Grad-CAM++ heatmap visualization method [<a href="#pone.0327371.ref039" class="usa-link" aria-describedby="pone.0327371.ref039">39</a>] to systematically compare the attention region distributions of Oak-YOLO and YOLOv8 under complex backgrounds. The heatmaps use a blue-to-red color spectrum (low-to-high values) to visually present the response intensity of the model to the input images. As shown in <a href="#pone.0327371.g012" class="usa-link">Fig 12</a>, Oak-YOLO exhibits more concentrated high-response regions in images captured by mobile devices, with activation area coverage exceeding 85%, accurately focusing on seed defect locations. In contrast, YOLOv8 shows more dispersed attention distribution, with 12–15% false activations near seed edges and greater susceptibility to background noise.</p>
<figure class="fig xbox font-sm" id="pone.0327371.g012"><h4 class="obj_head">Fig 12. Attention heatmap comparison between Oak-YOLO and YOLOv8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12334039_pone.0327371.g012.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/bf86/12334039/c9930848f91d/pone.0327371.g012.jpg" loading="lazy" height="474" width="738" alt="Fig 12"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0327371.g012/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section></section><section id="sec018"><h2 class="pmc_sec_title">Conclusion</h2>
<p>In this study, we proposed Oak-YOLO, an improved YOLOv8-based detection framework designed specifically for identifying defects in oak seeds. The model integrates the EfficientViT backbone for enhanced global feature extraction, and introduces a Ghost-Dynamic prediction head to better detect small and irregular targets. Furthermore, the adoption of the WIoUv3 loss function improves bounding box regression for overlapping and deformable defects such as cracks and insect holes.</p>
<p>Experimental evaluations demonstrated that Oak-YOLO achieved a mAP50 of 94.5%, an F1-score of 95.3%, and a detection speed of 132.2 FPS on the oak-intensive dataset, significantly outperforming the baseline YOLOv8 model. In cross-device validation using smartphone-captured images, the model maintained high accuracy and robustness, confirming its generalization ability across diverse acquisition environments. Comparative analysis also showed that Oak-YOLO offers superior performance-efficiency trade-offs compared to state-of-the-art YOLO variants and Transformer-based models.</p>
<p>These findings highlight the practical applicability of Oak-YOLO for real-time and high-precision seed defect detection in forestry. Future work will focus on further reducing computational costs and extending the model to support defect screening across a broader range of tree species.</p></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>The dataset analyzed during the current study are available in the Figshare repository at DOI 10.6084/m9.figshare.29072015 (<a href="https://doi.org/10.6084/m9.figshare.29072015" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://doi.org/10.6084/m9.figshare.29072015</a>).</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>The Fundamental Research Funds for the Central Universities-No. 2572023DJ02. The funders provided support in reviewing and editing the manuscript.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0327371.ref001">
<span class="label">1.</span><cite>Parker CL, Hibbard MJ. Factors controlling germination and early survival in oaks. Forest Ecology and Management.
2002;159(2):133–44. doi: 10.1016/S0378-1127(01)00792-9</cite> [<a href="https://doi.org/10.1016/S0378-1127(01)00792-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Forest%20Ecology%20and%20Management.&amp;title=Factors%20controlling%20germination%20and%20early%20survival%20in%20oaks&amp;author=CL%20Parker&amp;author=MJ%20Hibbard&amp;volume=159&amp;issue=2&amp;publication_year=2002&amp;pages=133-44&amp;doi=10.1016/S0378-1127(01)00792-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref002">
<span class="label">2.</span><cite>Li X, Wu Y, Dong X, Zhao S, Mu X, Chen Z, et al. Checklist of major oak pests in China. Acta Sericologica Sinica.
2010;36(2):330–6.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Acta%20Sericologica%20Sinica.&amp;title=Checklist%20of%20major%20oak%20pests%20in%20China&amp;author=X%20Li&amp;author=Y%20Wu&amp;author=X%20Dong&amp;author=S%20Zhao&amp;author=X%20Mu&amp;volume=36&amp;issue=2&amp;publication_year=2010&amp;pages=330-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref003">
<span class="label">3.</span><cite>Nickerson DM. Addendum to counting by weighing and the seed testing problem. Annals of Applied Biology.
2003;143(3):371–4. doi: 10.1111/j.1744-7348.2003.tb00306.x</cite> [<a href="https://doi.org/10.1111/j.1744-7348.2003.tb00306.x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Annals%20of%20Applied%20Biology.&amp;title=Addendum%20to%20counting%20by%20weighing%20and%20the%20seed%20testing%20problem&amp;author=DM%20Nickerson&amp;volume=143&amp;issue=3&amp;publication_year=2003&amp;pages=371-4&amp;doi=10.1111/j.1744-7348.2003.tb00306.x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref004">
<span class="label">4.</span><cite>Wang H, Hou H, Liu S. Maize seed recognition based on genetic algorithm and multi-class SVM. Computer Engineering and Application.
2008;44(18):221–3.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Computer%20Engineering%20and%20Application.&amp;title=Maize%20seed%20recognition%20based%20on%20genetic%20algorithm%20and%20multi-class%20SVM&amp;author=H%20Wang&amp;author=H%20Hou&amp;author=S%20Liu&amp;volume=44&amp;issue=18&amp;publication_year=2008&amp;pages=221-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref005">
<span class="label">5.</span><cite>Nguyen-Quoc H, Truong Hoang V. Rice seed image classiï¬#129;cation based on HOG descriptor with missing values imputation. TELKOMNIKA.
2020;18(4):1897. doi: 10.12928/telkomnika.v18i4.14069</cite> [<a href="https://doi.org/10.12928/telkomnika.v18i4.14069" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=TELKOMNIKA.&amp;title=Rice%20seed%20image%20classi%C3%AF%C2%AC#129;cation%20based%20on%20HOG%20descriptor%20with%20missing%20values%20imputation&amp;author=H%20Nguyen-Quoc&amp;author=V%20Truong%20Hoang&amp;volume=18&amp;issue=4&amp;publication_year=2020&amp;pages=1897&amp;doi=10.12928/telkomnika.v18i4.14069&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref006">
<span class="label">6.</span><cite>Redmon J, Divvala S, Girshick R, Farhadi A. You only look once: unified, real-time object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016:779–88.</cite>
</li>
<li id="pone.0327371.ref007">
<span class="label">7.</span><cite>Redmon J, Farhadi A. YOLO9000: better, faster, stronger. In: Proceedings of the IEEE conference on computer vision and pattern recognition, 2017. 7263–71.</cite>
</li>
<li id="pone.0327371.ref008">
<span class="label">8.</span><cite>Redmon J, Farhadi A. YOLOv3: an incremental improvement. arXiv preprint.
2018. doi: 10.48550/arXiv.1804.02767</cite> [<a href="https://doi.org/10.48550/arXiv.1804.02767" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint.&amp;title=YOLOv3:%20an%20incremental%20improvement&amp;author=J%20Redmon&amp;author=A%20Farhadi&amp;publication_year=2018&amp;doi=10.48550/arXiv.1804.02767&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref009">
<span class="label">9.</span><cite>Ren S, He K, Girshick R, Sun J. Faster R-CNN: towards real-time object detection with region proposal networks. In: Advances in Neural Information Processing Systems, 2015: 91–9.</cite>
</li>
<li id="pone.0327371.ref010">
<span class="label">10.</span><cite>Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY. SSD: single shot multibox detector. In: European conference on computer vision, 2016: 21–37.</cite>
</li>
<li id="pone.0327371.ref011">
<span class="label">11.</span><cite>Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2021:10012–10022. 10.1109/ICCV48922.2021.00988.</cite> [<a href="https://doi.org/10.1109/ICCV48922.2021.00988" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0327371.ref012">
<span class="label">12.</span><cite>Zhuang F, Qi Z, Duan K, Xi D, Zhu Y, Zhu H, et al. A comprehensive survey on transfer learning. Proc IEEE.
2021;109(1):43–76. doi: 10.1109/jproc.2020.3004555</cite> [<a href="https://doi.org/10.1109/jproc.2020.3004555" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proc%20IEEE.&amp;title=A%20comprehensive%20survey%20on%20transfer%20learning&amp;author=F%20Zhuang&amp;author=Z%20Qi&amp;author=K%20Duan&amp;author=D%20Xi&amp;author=Y%20Zhu&amp;volume=109&amp;issue=1&amp;publication_year=2021&amp;pages=43-76&amp;doi=10.1109/jproc.2020.3004555&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref013">
<span class="label">13.</span><cite>Everingham M, Van Gool L, Williams CKI, Winn J, Zisserman A. The pascal visual object classes (VOC) challenge. Int J Comput Vision.
2010;88(2):303–38.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Comput%20Vision.&amp;title=The%20pascal%20visual%20object%20classes%20(VOC)%20challenge&amp;author=M%20Everingham&amp;author=L%20Van%20Gool&amp;author=CKI%20Williams&amp;author=J%20Winn&amp;author=A%20Zisserman&amp;volume=88&amp;issue=2&amp;publication_year=2010&amp;pages=303-38&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref014">
<span class="label">14.</span><cite>Chen X, Fang H, Lin TY, Vedantam R, Gupta S, Dollár P, et al. Microsoft COCO captions: data collection and evaluation server. arXiv preprint arXiv
2015. <a href="https://arxiv.org/abs/1504.00325" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1504.00325</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint%20arXiv&amp;title=Microsoft%20COCO%20captions:%20data%20collection%20and%20evaluation%20server&amp;author=X%20Chen&amp;author=H%20Fang&amp;author=TY%20Lin&amp;author=R%20Vedantam&amp;author=S%20Gupta&amp;publication_year=2015&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref015">
<span class="label">15.</span><cite>Mukasa P, Wakholi C, Akbar Faqeerzada M, Amanah HZ, Kim H, Joshi R, et al. Nondestructive discrimination of seedless from seeded watermelon seeds by using multivariate and deep learning image analysis. Computers and Electronics in Agriculture.
2022;194:106799. doi: 10.1016/j.compag.2022.106799</cite> [<a href="https://doi.org/10.1016/j.compag.2022.106799" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Computers%20and%20Electronics%20in%20Agriculture.&amp;title=Nondestructive%20discrimination%20of%20seedless%20from%20seeded%20watermelon%20seeds%20by%20using%20multivariate%20and%20deep%20learning%20image%20analysis&amp;author=P%20Mukasa&amp;author=C%20Wakholi&amp;author=M%20Akbar%20Faqeerzada&amp;author=HZ%20Amanah&amp;author=H%20Kim&amp;volume=194&amp;publication_year=2022&amp;pages=106799&amp;doi=10.1016/j.compag.2022.106799&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref016">
<span class="label">16.</span><cite>Kurtulmuş F. Identification of sunflower seeds with deep convolutional neural networks. Food Measure.
2020;15(2):1024–33. doi: 10.1007/s11694-020-00707-7</cite> [<a href="https://doi.org/10.1007/s11694-020-00707-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Food%20Measure.&amp;title=Identification%20of%20sunflower%20seeds%20with%20deep%20convolutional%20neural%20networks&amp;author=F%20Kurtulmu%C5%9F&amp;volume=15&amp;issue=2&amp;publication_year=2020&amp;pages=1024-33&amp;doi=10.1007/s11694-020-00707-7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref017">
<span class="label">17.</span><cite>Wang Y, Song S. Variety identification of sweet maize seeds based on hyperspectral imaging combined with deep learning. Infrared Physics &amp; Technology.
2023;130:104611. doi: 10.1016/j.infrared.2023.104611</cite> [<a href="https://doi.org/10.1016/j.infrared.2023.104611" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Infrared%20Physics%20&amp;%20Technology.&amp;title=Variety%20identification%20of%20sweet%20maize%20seeds%20based%20on%20hyperspectral%20imaging%20combined%20with%20deep%20learning&amp;author=Y%20Wang&amp;author=S%20Song&amp;volume=130&amp;publication_year=2023&amp;pages=104611&amp;doi=10.1016/j.infrared.2023.104611&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref018">
<span class="label">18.</span><cite>Shi Y, Patel Y, Rostami B, Chen H, Wu L, Yu Z, et al. Barley variety identification by iphone images and deep learning. Journal of the American Society of Brewing Chemists.
2021;80(3):215–24. doi: 10.1080/03610470.2021.1958602</cite> [<a href="https://doi.org/10.1080/03610470.2021.1958602" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20the%20American%20Society%20of%20Brewing%20Chemists.&amp;title=Barley%20variety%20identification%20by%20iphone%20images%20and%20deep%20learning&amp;author=Y%20Shi&amp;author=Y%20Patel&amp;author=B%20Rostami&amp;author=H%20Chen&amp;author=L%20Wu&amp;volume=80&amp;issue=3&amp;publication_year=2021&amp;pages=215-24&amp;doi=10.1080/03610470.2021.1958602&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref019">
<span class="label">19.</span><cite>Barrio-Conde M, Zanella MA, Aguiar-Perez JM, Ruiz-Gonzalez R, Gomez-Gil J. A deep learning image system for classifying high oleic sunflower seed varieties. Sensors (Basel).
2023;23(5):2471. doi: 10.3390/s23052471

</cite> [<a href="https://doi.org/10.3390/s23052471" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10007379/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36904675/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sensors%20(Basel).&amp;title=A%20deep%20learning%20image%20system%20for%20classifying%20high%20oleic%20sunflower%20seed%20varieties&amp;author=M%20Barrio-Conde&amp;author=MA%20Zanella&amp;author=JM%20Aguiar-Perez&amp;author=R%20Ruiz-Gonzalez&amp;author=J%20Gomez-Gil&amp;volume=23&amp;issue=5&amp;publication_year=2023&amp;pages=2471&amp;pmid=36904675&amp;doi=10.3390/s23052471&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref020">
<span class="label">20.</span><cite>Bi C, Hu N, Zou Y, Zhang S, Xu S, Yu H. Development of deep learning methodology for maize seed variety recognition based on improved swin transformer. Agronomy.
2022;12(8):1843. doi: 10.3390/agronomy12081843</cite> [<a href="https://doi.org/10.3390/agronomy12081843" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Agronomy.&amp;title=Development%20of%20deep%20learning%20methodology%20for%20maize%20seed%20variety%20recognition%20based%20on%20improved%20swin%20transformer&amp;author=C%20Bi&amp;author=N%20Hu&amp;author=Y%20Zou&amp;author=S%20Zhang&amp;author=S%20Xu&amp;volume=12&amp;issue=8&amp;publication_year=2022&amp;pages=1843&amp;doi=10.3390/agronomy12081843&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref021">
<span class="label">21.</span><cite>Singh Thakur P, Tiwari B, Kumar A, Gedam B, Bhatia V, Krejcar O, et al. Deep transfer learning based photonics sensor for assessment of seed-quality. Computers and Electronics in Agriculture.
2022;196:106891. doi: 10.1016/j.compag.2022.106891</cite> [<a href="https://doi.org/10.1016/j.compag.2022.106891" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Computers%20and%20Electronics%20in%20Agriculture.&amp;title=Deep%20transfer%20learning%20based%20photonics%20sensor%20for%20assessment%20of%20seed-quality&amp;author=P%20Singh%20Thakur&amp;author=B%20Tiwari&amp;author=A%20Kumar&amp;author=B%20Gedam&amp;author=V%20Bhatia&amp;volume=196&amp;publication_year=2022&amp;pages=106891&amp;doi=10.1016/j.compag.2022.106891&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref022">
<span class="label">22.</span><cite>Ultralytics Team. YOLOv8: the latest version of YOLO for object detection. 2023. [cited 2024 Dec 10]. Available from: <a href="https://github.com/ultralytics/yolov8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/ultralytics/yolov8</a></cite>
</li>
<li id="pone.0327371.ref023">
<span class="label">23.</span><cite>Xu X, Chen S, Lv X, Wang J, Hu X. Guided multi-scale refinement network for camouflaged object detection. Multimed Tools Appl.
2023;82(4):5785–801. doi: 10.1007/s11042-022-13274-4

</cite> [<a href="https://doi.org/10.1007/s11042-022-13274-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9362480/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35968408/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Multimed%20Tools%20Appl.&amp;title=Guided%20multi-scale%20refinement%20network%20for%20camouflaged%20object%20detection&amp;author=X%20Xu&amp;author=S%20Chen&amp;author=X%20Lv&amp;author=J%20Wang&amp;author=X%20Hu&amp;volume=82&amp;issue=4&amp;publication_year=2023&amp;pages=5785-801&amp;pmid=35968408&amp;doi=10.1007/s11042-022-13274-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref024">
<span class="label">24.</span><cite>Han K, Wang Y, Cheng Q, Zhang Y, Xu C, Li J, et al. GhostNet: more features from cheap operations. arXiv preprint.
2020. <a href="https://arxiv.org/abs/2003.08056" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2003.08056</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint.&amp;title=GhostNet:%20more%20features%20from%20cheap%20operations&amp;author=K%20Han&amp;author=Y%20Wang&amp;author=Q%20Cheng&amp;author=Y%20Zhang&amp;author=C%20Xu&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref025">
<span class="label">25.</span><cite>Chen Y, Li H, Yu X, Li Z. Dynamic convolution: attention meets convolution. arXiv preprint arXiv.
2020. <a href="https://arxiv.org/abs/2003.03126" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2003.03126</a></cite>
</li>
<li id="pone.0327371.ref026">
<span class="label">26.</span><cite>Liu X, Peng H, Zheng N, Yang Y, Hu H, Yuan Y. EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); 2023. p. 14420–14430. 10.1109/CVPR52729.2023.01386.</cite> [<a href="https://doi.org/10.1109/CVPR52729.2023.01386" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0327371.ref027">
<span class="label">27.</span><cite>Girshick R. Fast R-CNN. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015. p. 1440–8. 10.1109/iccv.2015.169</cite> [<a href="https://doi.org/10.1109/iccv.2015.169" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0327371.ref028">
<span class="label">28.</span><cite>Rezatofighi H, Tsoi N, Gwak J, Sadeghian A, Reid I, Savarese S. Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. p. 658–66. 10.1109/cvpr.2019.00075</cite> [<a href="https://doi.org/10.1109/cvpr.2019.00075" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0327371.ref029">
<span class="label">29.</span><cite>Zheng Z, Wang P, Liu W, Li J, Ye R, Ren D. Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression. AAAI.
2020;34(07):12993–3000. doi: 10.1609/aaai.v34i07.6999</cite> [<a href="https://doi.org/10.1609/aaai.v34i07.6999" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=AAAI.&amp;title=Distance-IoU%20Loss:%20Faster%20and%20Better%20Learning%20for%20Bounding%20Box%20Regression&amp;author=Z%20Zheng&amp;author=P%20Wang&amp;author=W%20Liu&amp;author=J%20Li&amp;author=R%20Ye&amp;volume=34&amp;issue=07&amp;publication_year=2020&amp;pages=12993-3000&amp;doi=10.1609/aaai.v34i07.6999&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref030">
<span class="label">30.</span><cite>Tong Z, Chen Y, Xu Z, Yu R. Wise-IoU: bounding box regression loss with dynamic focusing mechanism. arXiv preprint arXiv.
2023. <a href="https://arxiv.org/abs/2301.10051" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2301.10051</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint%20arXiv.&amp;title=Wise-IoU:%20bounding%20box%20regression%20loss%20with%20dynamic%20focusing%20mechanism&amp;author=Z%20Tong&amp;author=Y%20Chen&amp;author=Z%20Xu&amp;author=R%20Yu&amp;publication_year=2023&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref031">
<span class="label">31.</span><cite>Yin J, Wu Y, Zhang Y. RT-DETR: Real-Time Detection Transformer. arXiv preprint.
2023. doi: 10.48550/arXiv.2304.08069</cite> [<a href="https://doi.org/10.48550/arXiv.2304.08069" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint.&amp;title=RT-DETR:%20Real-Time%20Detection%20Transformer&amp;author=J%20Yin&amp;author=Y%20Wu&amp;author=Y%20Zhang&amp;publication_year=2023&amp;doi=10.48550/arXiv.2304.08069&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref032">
<span class="label">32.</span><cite>Zhu X, Su W, Lu L, Li B, Wang X, Dai J. Deformable DETR: Deformable Transformers for End-to-End Object Detection. In: Proceedings of the International Conference on Learning Representations (ICLR), 2021.</cite>
</li>
<li id="pone.0327371.ref033">
<span class="label">33.</span><cite>JocherG, “Ultralytics YOLOv5,” 2020. [Online]. Available from: <a href="https://github.com/ultralytics/yolov5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/ultralytics/yolov5</a></cite>
</li>
<li id="pone.0327371.ref034">
<span class="label">34.</span><cite>WangC-Y, BochkovskiyA, LiaoH-YM. “YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors,” arXiv preprint arXiv. 2022. 10.48550/arXiv.2207.02696</cite> [<a href="https://doi.org/10.48550/arXiv.2207.02696" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0327371.ref035">
<span class="label">35.</span><cite>Wang C-Y, Yeh I-H, Liao H-YM. YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information. arXiv preprint.
2024. doi: 10.48550/arXiv.2402.13616</cite> [<a href="https://doi.org/10.48550/arXiv.2402.13616" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint.&amp;title=YOLOv9:%20Learning%20What%20You%20Want%20to%20Learn%20Using%20Programmable%20Gradient%20Information&amp;author=C-Y%20Wang&amp;author=I-H%20Yeh&amp;author=H-YM%20Liao&amp;publication_year=2024&amp;doi=10.48550/arXiv.2402.13616&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref036">
<span class="label">36.</span><cite>Wang A, Chen H, Liu L, Chen K, Lin Z, Han J, et al. YOLOv10: Real-Time End-to-End Object Detection. arXiv preprint.
2024. doi: arXiv:2405.14458</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint.&amp;title=YOLOv10:%20Real-Time%20End-to-End%20Object%20Detection&amp;author=A%20Wang&amp;author=H%20Chen&amp;author=L%20Liu&amp;author=K%20Chen&amp;author=Z%20Lin&amp;publication_year=2024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref037">
<span class="label">37.</span><cite>Ultralytics. “YOLOv11,” 2024. [Online]. Available from: <a href="https://github.com/ultralytics/ultralytics" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/ultralytics/ultralytics</a></cite>
</li>
<li id="pone.0327371.ref038">
<span class="label">38.</span><cite>Tian Y, Ye Q, Doermann D. YOLOv12: Attention-Centric Real-Time Object Detectors. arXiv preprint.
2025. doi: 10.48550/arXiv.2502.12524</cite> [<a href="https://doi.org/10.48550/arXiv.2502.12524" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint.&amp;title=YOLOv12:%20Attention-Centric%20Real-Time%20Object%20Detectors&amp;author=Y%20Tian&amp;author=Q%20Ye&amp;author=D%20Doermann&amp;publication_year=2025&amp;doi=10.48550/arXiv.2502.12524&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0327371.ref039">
<span class="label">39.</span><cite>Chattopadhay A, Sarkar A, Howlader P, Balasubramanian VN. Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks. In: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 2018. p. 839–47 10.1109/wacv.2018.00097</cite> [<a href="https://doi.org/10.1109/wacv.2018.00097" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
</ul></section></section></section><article class="sub-article" id="pone.0327371.r001"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0327371.r001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0327371.r001</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 0</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Fatih Uysal</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Fatih Uysal</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link"><span class="name western">Fatih Uysal</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.c" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.c" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.c" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Fatih Uysal</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.c" class="d-panel p" style="display: none">
<div>© 2025 Fatih Uysal</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>2 May 2025</em>
</p>
<p>PONE-D-25-18128Oak-YOLO: A High-Performance Detection Model for Automated Oak Seed Defect IdentificationPLOS ONE</p>
<p>Dear Dr. Mu,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
<p>Please revise the paper according to the reviewer comments.</p>
<p>Please submit your revised manuscript by Jun 16 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span>. When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<p>Please include the following items when submitting your revised manuscript:</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a>.</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Fatih Uysal, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Journal requirements:</p>
<p>When submitting your revision, we need you to address these additional requirements.</p>
<p>1.  Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p>
<p><a href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</a>  and</p>
<p>
<a href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</a>
</p>
<p>2. Please update your submission to use the PLOS LaTeX template. The template and more information on our requirements for LaTeX submissions can be found at <a href="http://journals.plos.org/plosone/s/latex" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://journals.plos.org/plosone/s/latex</a></p>
<p>3. Please ensure you have stated the source of the oak seeds used in the study.</p>
<p>5. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, we expect all author-generated code to be made available without restrictions upon publication of the work. Please review our guidelines at <a href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</a> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p>
<p>6. Thank you for stating the following financial disclosure:</p>
<p>“the Fundamental Research Funds for the Central Universities-No. 2572023DJ02”</p>
<p>Please state what role the funders took in the study.  If the funders had no role, please state: "The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."</p>
<p>If this statement is not correct you must amend it as needed.</p>
<p>Please include this amended Role of Funder statement in your cover letter; we will change the online submission form on your behalf.</p>
<p>7. When completing the data availability statement of the submission form, you indicated that you will make your data available on acceptance. We strongly recommend all authors decide on a data sharing plan before acceptance, as the process can be lengthy and hold up publication timelines. Please note that, though access restrictions are acceptable now, your entire data will need to be made freely accessible if your manuscript is accepted for publication. This policy applies to all data except where public deposition would breach compliance with the protocol approved by your research ethics board. If you are unable to adhere to our open data policy, please kindly revise your statement to explain your reasoning and we will seek the editor's input on an exemption. Please be assured that, once you have provided your new statement, the assessment of your exemption will not hold up the peer review process.</p>
<p>8. Please amend your authorship list in your manuscript file to include author Wangyu Wu.</p>
<p>Additional Editor Comments:</p>
<p>Please revise the paper according to the reviewer comments.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>

<strong>Comments to the Author</strong>
</p>
<p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Partly</p>
<p>**********</p>
<p>2. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>5. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p>Reviewer #1: Dear Autor(s):</p>
<p>The manuscript presents a promising approach for oak seed defect detection using an enhanced YOLO-based deep learning model. While the study is technically sound and addresses a relevant problem in forestry and agricultural automation, several aspects need to be clarified, revised, or improved to meet the standards of a high-impact journal.</p>
<p>1. Revise the reference list to comply with the journal's formatting guidelines. For example, the first cited reference appears as number 15 (line 33), which suggests inconsistency in numbering or missing earlier references.</p>
<p>2. Lines 215–217 state that YOLOv8 is the latest version of the YOLO series, which is currently incorrect. The most recent version is YOLOv12. While YOLOv8 may have been the latest at the time of the study, it is recommended that the authors also evaluate their method against more recent versions for a more robust comparison.</p>
<p>3. Clarify the rationale behind using the Adam optimization algorithm. Was the choice of hyperparameters based on manual tuning or derived through a specific algorithmic approach?</p>
<p>4. Including one or more state-of-the-art YOLO models (e.g., YOLOv9–YOLOv12) in Table 4 would provide a valuable benchmark and allow for a better evaluation of Oak-YOLO’s performance against current models.</p>
<p>5. The dataset is reported to be split in an 8:1:1 ratio, but it is unclear what operations were performed using the test set. Moreover, the test results are not explicitly presented. It should also be clarified whether class balance was considered during dataset splitting (e.g., through stratified random sampling).</p>
<p>6. Is the dataset openly accessible? The manuscript states that it can be obtained by contacting the corresponding author, but according to PLOS ONE’s data availability policy, this may not be sufficient. Authors should consider uploading the dataset to a public repository with a DOI.</p>
<p>7. The technical writing should be simplified for clarity and consistency. In particular, the abstract should be rewritten using concise and impactful language to better communicate the novelty and significance of the work.</p>
<p>I believe that with the suggested revisions and clarifications, this manuscript can make a valuable contribution to the field and meet the publication standards of the journal.</p>
<p>Reviewer #2: The aim of the paper is to develop an advanced, efficient, and accurate object detection model—Oak-YOLO—tailored specifically for identifying defects in oak seeds, such as insect holes and cracks, which are critical for seed quality and germination. By enhancing the YOLOv8 architecture with components like EfficientViT, Ghost-DynamicConv, and the WIoUv3 loss function, the proposed model achieves superior performance. Experimental results demonstrate a high detection accuracy of 94.3% and a mAP50 of 96.2%, with significantly improved speed and precision compared to baseline models, making it well-suited for real-time industrial applications in forestry.</p>
<p>Comments to the Authors:</p>
<p>The novelty of Oak-YOLO is not sufficiently distinguished from existing YOLO-based models; clearer justification is needed.</p>
<p>The dataset used is limited in size and variability; generalizability is questionable.</p>
<p>There is a lack of comparison with non-YOLO architectures beyond brief mentions—benchmarking against more diverse baselines is necessary.</p>
<p>The ablation study could be expanded to isolate the impact of each individual module.</p>
<p>Details on statistical significance and error analysis are missing.</p>
<p>**********</p>
<p>6. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a>). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a>.</p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span>. Please note that Supporting Information files do not need this step.</p></section></article><article class="sub-article" id="pone.0327371.r002"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 8;20(8):e0327371. doi: <a href="https://doi.org/10.1371/journal.pone.0327371.r002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0327371.r002</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 1</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.d" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.d" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.d" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub2" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.d" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>22 May 2025</em>
</p>
<p>Response to Reviewers</p>
<p>Manuscript Number: PONE-D-25-18128</p>
<p>Manuscript Title: Oak-YOLO: A High-Performance Detection Model for Automated Oak Seed Defect Identification</p>
<p>Dear Editors and Reviewers,</p>
<p>We would like to express our sincere gratitude to the Editors and Reviewers for your valuable time, insightful feedback, and constructive suggestions regarding our manuscript entitled "Oak-YOLO: improved YOLOV8 with multi-scale feature fusion in the detection of defects in oak seeds". We are deeply appreciative of your thoughtful comments, which have greatly contributed to improving the clarity, rigor, and overall quality of our work.</p>
<p>We have carefully addressed each point raised by the reviewers and revised the manuscript accordingly. Please find below our detailed point-by-point responses. All modifications in the manuscript are clearly marked for your convenience.</p>
<p>Hongbo Mu</p>
<p>(On behalf of all authors)</p>
<p>Corresponding author</p>
<p>Email: mhb506@nefu.edu.cn</p>
<p>Responses to Editorial Comments</p>
<p>We sincerely thank the Editorial Office for the detailed instructions and helpful reminders. We have carefully followed each point and updated the manuscript and submission materials accordingly. Please find our point-by-point responses below:</p>
<p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming.</p>
<p>Response: We have renamed our files according to the PLOS ONE guidelines and ensured that the manuscript fully complies with the journal’s style requirements, following the templates provided.</p>
<p>2. Please update your submission to use the PLOS LaTeX template.</p>
<p>Response: Thank you for the guidance. We have fully migrated our manuscript into the official PLOS LaTeX template, and have carefully checked the formatting to ensure consistency with journal standards.</p>
<p>3. Please ensure you have stated the source of the oak seeds used in the study.</p>
<p>Response: We have added a clear statement of the oak seed source in the “Materials and Methods” section:</p>
<p>“The oak seeds used in this study were purchased from local oak farmers in Xinxu Town, Suqian, Jiangsu Province, China.”</p>
<p>5. Please review our code sharing policy and ensure your code is shared appropriately.</p>
<p>Response: We fully support the journal’s policy on code transparency and reproducibility. All author-generated code has been uploaded Figshare and is available at: <a href="https://doi.org/10.6084/m9.figshare.29072015" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://doi.org/10.6084/m9.figshare.29072015</a>.</p>
<p>6. Please state what role the funders took in the study.</p>
<p>Response: We sincerely thank the editor for this important reminder. As the funders provided assistance in the review and editing of the manuscript, we have amended the funding statement accordingly. The updated funder role statement now reads:</p>
<p>“The funders provided support in reviewing and editing the manuscript.”</p>
<p>This has been included both in the manuscript and in the cover letter, as requested.</p>
<p>7. Data availability policy.</p>
<p>Response: We have now made our dataset publicly available through Figshare. The DOI is: <a href="https://doi.org/10.6084/m9.figshare.29072015" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://doi.org/10.6084/m9.figshare.29072015</a></p>
<p>This DOI has been included in the revised Data Availability Statement and in the main text of the manuscript. We confirm that the data are now fully open access and no restrictions apply.</p>
<p>8. Please amend your authorship list to include author Wangyu Wu.</p>
<p>Response: We sincerely apologize for the previous omission. We have now added author Wangyu Wu to the author list in the manuscript file, and appropriately updated the Author Contributions section.</p>
<p>Reviewer #1</p>
<p>Comment 1: Revise the reference list to comply with the journal's formatting guidelines. For example, the first cited reference appears as number 15 (line 33), which suggests inconsistency in numbering or missing earlier references.</p>
<p>Response: Thank you very much for pointing this out. We have thoroughly reviewed and corrected the reference numbering throughout the manuscript to ensure that all citations appear in sequential order in accordance with the journal's formatting requirements.</p>
<p>Comment 2: Lines 215–217 state that YOLOv8 is the latest version of the YOLO series, which is currently incorrect. The most recent version is YOLOv12. While YOLOv8 may have been the latest at the time of the study, it is recommended that the authors also evaluate their method against more recent versions for a more robust comparison.</p>
<p>Response: We sincerely thank the reviewer for highlighting this issue. We have clarify that YOLOv8 was the latest version at the time our study was initiated. In response to your suggestion, we have extended our experimental comparison to include YOLOv9 to YOLOv12. The updated results are presented in Table 6 and discussed in the revised manuscript (Section: Comparative performance analysis against alternative models).</p>
<p>Comment 3: Clarify the rationale behind using the Adam optimization algorithm. Was the choice of hyperparameters based on manual tuning or derived through a specific algorithmic approach?</p>
<p>Response: Thank you for your helpful suggestion. We have revised the “Experimental Configuration” section to explicitly state that the Adam optimizer was selected due to its adaptive learning rate and faster convergence, particularly suitable for our network structure. The hyperparameters were chosen through manual tuning based on empirical performance on the validation set.</p>
<p>Comment 4: Including one or more state-of-the-art YOLO models (e.g., YOLOv9–YOLOv12) in Table 4 would provide a valuable benchmark and allow for a better evaluation of Oak-YOLO’s performance against current models.</p>
<p>Response: We fully agree with your suggestion. We have incorporated the performance metrics of YOLOv9 through YOLOv12 into Table 6 , and provided a detailed comparative analysis in the corresponding section. This addition allows for a more comprehensive and up-to-date evaluation of our proposed model.</p>
<p>Comment 5: The dataset is reported to be split in an 8:1:1 ratio, but it is unclear what operations were performed using the test set. Moreover, the test results are not explicitly presented. It should also be clarified whether class balance was considered during dataset splitting (e.g., through stratified random sampling).</p>
<p>Response: Thank you for raising this important point. We have revised the “Data Augmentation” section to clarify that the test set was reserved exclusively for final evaluation and was not involved in any model training or tuning. Additionally, we employed stratified random sampling during data splitting to maintain class balance across training, validation, and testing sets. The test results are now explicitly presented in Table 7 .</p>
<p>Comment 6: Is the dataset openly accessible? The manuscript states that it can be obtained by contacting the corresponding author, but according to PLOS ONE’s data availability policy, this may not be sufficient.</p>
<p>Response: Thank you for bringing this to our attention. In compliance with the PLOS ONE data availability policy, we have uploaded our dataset to Figshare, where it is now publicly accessible via the following DOI: <a href="https://doi.org/10.6084/m9.figshare.29072015" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://doi.org/10.6084/m9.figshare.29072015</a>.</p>
<p>Comment 7: The technical writing should be simplified for clarity and consistency. In particular, the abstract should be rewritten using concise and impactful language to better communicate the novelty and significance of the work.</p>
<p>Response: We appreciate this thoughtful suggestion. We have revised the abstract extensively to enhance its clarity, conciseness, and emphasis on the novelty of our proposed framework. We have also reviewed and refined the manuscript throughout to improve overall readability and consistency.</p>
<p>Reviewer #2</p>
<p>Comment 1: The novelty of Oak-YOLO is not sufficiently distinguished from existing YOLO-based models; clearer justification is needed.</p>
<p>Response: Thank you for this valuable feedback. We have revised the Introduction and Conclusion sections to more clearly articulate the novel contributions of Oak-YOLO. Specifically, our model introduces a high-resolution Ghost-Dynamic head, an EfficientViT-enhanced backbone, and the WIoUv3 loss to better detect small, irregular, and overlapping defects—improvements not found in previous YOLO variants.</p>
<p>Comment 2: The dataset used is limited in size and variability; generalizability is questionable.</p>
<p>Response: We acknowledge this concern and have taken steps to address it. In the revised manuscript, we conducted additional cross-device generalization experiments using oak seed images captured with mobile devices (OnePlus ACE2 Pro and iPhone 13 Plus). The results demonstrate that Oak-YOLO maintains high detection accuracy and robustness across different image acquisition platforms.</p>
<p>Comment 3: There is a lack of comparison with non-YOLO architectures beyond brief mentions—benchmarking against more diverse baselines is necessary.</p>
<p>Response: We appreciate the reviewer’s insight. We have extended the scope of our comparative study to include several Transformer-based detectors (e.g., rt-DETR-18, rt-DETR-50, Deformable DETR). The updated comparison in Table 6 allows for a broader assessment of our model’s performance relative to both CNN- and Transformer-based architectures.</p>
<p>Comment 4: The ablation study could be expanded to isolate the impact of each individual module.</p>
<p>Response: Thank you for the suggestion. We have expanded the ablation study to comprehensively evaluate the individual and combined effects of Ghost-DynamicConv, EfficientViT, and WIoUv3. The new results are presented in Table 5 and Fig 8,9 illustrating the performance gain brought by each component.</p>
<p>Comment 5: Details on statistical significance and error analysis are missing.</p>
<p>Response: We agree with the importance of including statistical evidence. We included confusion matrix visualizations and Grad-CAM++ heatmaps to provide qualitative analysis and interpretability of model behavior.</p>
<p>Once again, we are deeply grateful for your invaluable comments and suggestions, which have significantly improved the quality of our manuscript. We respectfully hope that the revised version now meets the high standards required for publication in PLOS ONE.</p>
<p>We thank the Editors and Reviewers again for their insightful feedback, which has helped improve our manuscript significantly. We hope the revised version meets the expectations for publication in PLOS ONE.</p>
<section class="sm xbox font-sm" id="pone.0327371.s001"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>responds.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334039/bin/pone.0327371.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0327371.s001.docx</a><sup> (18.8KB, docx) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0327371.r003"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0327371.r003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0327371.r003</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 1</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Fatih Uysal</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Fatih Uysal</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link"><span class="name western">Fatih Uysal</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.e" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.e" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.e" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Fatih Uysal</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.e" class="d-panel p" style="display: none">
<div>© 2025 Fatih Uysal</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>15 Jun 2025</em>
</p>
<p>Oak-YOLO: A High-Performance Detection Model for Automated Oak Seed Defect Identification</p>
<p>PONE-D-25-18128R1</p>
<p>Dear Dr. Mu,</p>
<p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
<p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
<p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Editorial Manager®</a> and clicking the ‘Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at authorbilling@plos.org.</p>
<p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>Kind regards,</p>
<p>Fatih Uysal, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Additional Editor Comments (optional):</p>
<p>Considering the current quality of the study, the authors' responses to the reviewers' comments, and the reviewers' final recommendations, the paper has been accepted as it demonstrates strong potential to contribute to the literature.</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>

<strong>Comments to the Author</strong>
</p>
<p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.</p>
<p>Reviewer #1: All comments have been addressed</p>
<p>Reviewer #2: All comments have been addressed</p>
<p>**********</p>
<p>2. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>3. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>6. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p>Reviewer #1: The manuscript has been successfully revised according to reviewer comments and journal policies. It meets PLOS ONE's publication criteria for scientific contribution, methodological soundness, data accessibility and transparency.</p>
<p>Reviewer #2: (No Response)</p>
<p>**********</p>
<p>7. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a>). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a>.</p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: No</p>
<p>**********</p></section></article><article class="sub-article" id="pone.0327371.r004"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0327371.r004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0327371.r004</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Acceptance letter</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link" aria-describedby="id9"><span class="name western">Fatih Uysal</span></a><div hidden="hidden" id="id9">
<h3><span class="name western">Fatih Uysal</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link"><span class="name western">Fatih Uysal</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.f" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.f" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.f" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Fatih Uysal</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.f" class="d-panel p" style="display: none">
<div>© 2025 Fatih Uysal</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>PONE-D-25-18128R1</p>
<p>PLOS ONE</p>
<p>Dear Dr. Mu,</p>
<p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p>
<p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p>
<p>* All references, tables, and figures are properly cited</p>
<p>* All relevant supporting information is included in the manuscript submission,</p>
<p>* There are no issues that prevent the paper from being properly typeset</p>
<p>You will receive further instructions from the production team, including instructions on how to review your proof when it is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p>
<p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>You will receive an invoice from PLOS for your publication fee after your manuscript has reached the completed accept phase. If you receive an email requesting payment before acceptance or for any other service, this may be a phishing scheme. Learn how to identify phishing emails and protect your accounts at <a href="https://explore.plos.org/phishing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://explore.plos.org/phishing</a>.</p>
<p>If we can help with anything else, please email us at customercare@plos.org.</p>
<p>Thank you for submitting your work to PLOS ONE and supporting open access.</p>
<p>Kind regards,</p>
<p>PLOS ONE Editorial Office Staff</p>
<p>on behalf of</p>
<p>Assoc. Prof. Dr. Fatih Uysal</p>
<p>Academic Editor</p>
<p>PLOS ONE</p></section></article><article class="sub-article" id="_ad93_"><section class="pmc-layout__citation font-secondary font-xs"><div></div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Associated Data</h1></hgroup><ul class="d-buttons inline-list"></ul>
<div class="d-panels font-secondary-light"></div>
<div></div>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
</div></section><section class="body sub-article-body"><section id="_adsm93_" lang="en" class="supplementary-materials"><h2 class="pmc_sec_title">Supplementary Materials</h2>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>responds.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12334039/bin/pone.0327371.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0327371.s001.docx</a><sup> (18.8KB, docx) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h2 class="pmc_sec_title">Data Availability Statement</h2>
<p>The dataset analyzed during the current study are available in the Figshare repository at DOI 10.6084/m9.figshare.29072015 (<a href="https://doi.org/10.6084/m9.figshare.29072015" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://doi.org/10.6084/m9.figshare.29072015</a>).</p></section></section></article><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0327371"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0327371.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (16.0 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12334039/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12334039/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12334039%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12334039/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12334039/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12334039/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40779603/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12334039/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40779603/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12334039/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12334039/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="S0SVNwp9mjkZM4fWpQGNdiUt2P4coeqAtxYS8lF0ElxiU9tjHdGIJiDsfD6GFLgL">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Web Policies

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    FOIA

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS Vulnerability Disclosure

    
</a>

                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Help

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Accessibility

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            








<a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    Careers

    
</a>

                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NLM

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
    

    NIH

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    HHS

    
</a>

                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            








<a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
    

    USA.gov

    
</a>

                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-370d5dd6.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-917ba005.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-d9849939.js"></script>
    
    

    </body>
</html>
