Effective assessment of patient‐centred care (PCC) is integral to clinical communication training. This study investigates the relationship between Objective Structured Clinical Examination (OSCE) scores and simulated patient (SP) questionnaire responses of 58 Optometry students. The analysis aims to identify specific aspects of PCC that correlate with examiner‐assessed high performance, as well as highlight discrepancies between SP and examiner evaluations.

Analysis of the SP questionnaires and OSCE performance scores from communication‐focused OSCE stations was conducted. The data were analysed using Spearman's rank correlations to assess associations between SP‐rated elements such as ability to trust student practitioner, opportunities to ask questions, feeling concerns were addressed, willingness to return, willingness to refer to friend and/or family and OSCE scores.

Trust, willingness to return and referral likelihood demonstrated strong positive correlations with OSCE performance, with willingness to return and referral likelihood demonstrating the strongest correlations. However, specific subdomains, such as the ability to ask questions, showed variability in importance to SPs across station types, being more relevant in counselling than in case history.

The findings suggest that global assessments may better capture patient perceptions of students' communication competence than microskills, such as the student's ability to invite questions, which vary in relevance across contexts. Aligning clinical training and evaluation frameworks with SP priorities, including addressing patient concerns and fostering trust, can enhance both patient satisfaction and the validity of PCC assessments.

Patient‐centred care (PCC) is the cornerstone of effective health care practice, emphasising the importance of communication, empathy and shared decision‐making. [1] Across the health professions, curriculum design has broadened and become more inclusive to capture perspectives from service users. [2] This raises critical questions regarding the alignment of formal assessments with the perceptions of those at the heart of the encounter: the patients.

In preregistration training programmes, PCC assessment is typically guided by validated teaching and assessment frameworks that outline specific communication competencies. Established models such as the Calgary–Cambridge Observation Guide (CCOG), [8], SEGUE [9] and the Maastricht History‐Taking and Advice Scoring System (MAAS) [10] provide structured approaches to teaching and evaluating communication skills. These frameworks define explicit learning outcomes, ensuring that students are systematically trained and assessed on key aspects of patient‐centred communication, including information gathering, shared decision‐making and rapport‐building.

Currently the Doctor of Optometry (OD) programme uses the CCOG framework in the teaching and assessment of communication skills. Communication is explicitly taught via workshops, tutorials and employing active learning strategies such as role play and observation. All students receive an equal level of exposure to this framework in their communication training. Patient feedback in the OD programme is incorporated into formative assessments but is not systematically included in summative evaluations.

There is limited understanding of which domains within PCC assessments align with SPs' subjective experiences and examiner perspectives. Exploring these alignments could provide deeper insights into the interpersonal aspects of care that traditional rubric‐based evaluations may overlook.

Objective Structured Clinical Examinations (OSCEs) were selected for this study due to their widespread use as a standardised method for assessing clinical skills, particularly through structured, scenario‐based interactions with SPs. This paper explores the correlation between OSCE communication assessments and SP perceptions, to identify the PCC metrics that most strongly align with high student performance. By addressing these key elements, the study aims to enhance the understanding and evaluation of PCC in a preregistration OD programme, thus giving further insight and applicability to other health and medical professions.

Ethics approval was granted by the Human Research Ethics Committee of the University of Western Australia (REF/2023 ET0000224 obtained June 2023) and the study protocol adhered to the tenets of the National Statement on Ethical Conduct in Human Research (National Statement) and to the Declaration of Helsinki. [11] SPs were recruited from an actor pool specifically for the OSCE, with all actors and examiners receiving standardised briefings and training to ensure consistency. A total of 58 students participated in the OSCE, evaluated by six examiners and eight SPs across eight consultations over 2 days (Table1). Due to resource constraints, stations were run concurrently to ensure all 58 students were assessed in one day. All student data were included in the analysis, with the cohort consisting of 43 females and 15 males, aged 23–29. Data collection occurred midway through the students' postgraduate course. Examiners were a mix of internal educators and external practitioners.

OSCE communication stations consisted of 10 min case history and counselling assessments designed to simulate clinical scenarios students would commonly encounter. Within the Case History stations, students were required to take a patient history and present a case summary to the examiner. Counselling stations involved discussing management options with the patient, incorporating shared decision‐making.

Due to time and sampling constraints, an abridged 5‐item and point Likert questionnaire was developed, sampling elements from DISQ, CCOG, MAAS and SEGUE for both OSCE station types. The questionnaire (AppendixAand Table2) includes items focused on fundamental components of effective patient‐centred communication. The SP ratings reflected whether the interaction fostered trust in the student practitioner, provided opportunities to ask questions, addressed their concerns and encouraged a willingness to return. The questionnaire scores were then compared to the overall OSCE's scores evaluated by the examiners through a separate assessment rubric. The examiner rubric was subskill based, assessing specific communication behaviours such as attentive listening, minimal use of jargon, appropriate nonverbal cues (e.g. eye contact and open body language) and demonstration of empathy for the patient encounter with exploration of the patient's experience. The SP ratings were outcome‐based, capturing the overall impact of these behaviours on the patient's experience of PCC and representing the integrated effect of the subskills assessed by the examiner. SPs completed the questionnaire immediately after the student OSCE interaction to assess the quality of the PCC they received.

Examiner rubric was subskill based assessing specific communication behaviours … SP ratings were outcome‐based, capturing the overall impact.

The descriptive analysis highlights the perception of PCC from the SP perspective across the student cohort as shown in Tables3and4. The findings reveal strong SP ratings across both communication OSCE stations, indicating positive perceptions of students' performance. Notably, students demonstrated proficiency in trust‐building and effectively addressing patient concerns across both station types. Overall, SPs expressed willingness to return to the student as a practitioner, with supporting rates of 80% and 78%.

Descriptive analysis of the four case history stations within SP questionnaire.

Descriptive analysis of the four counselling stations within SP questionnaire.

Spearman's rank analysis as detailed in Table5examined associations between SP questionnaire and OSCE performance. This analysis was conducted at the question domain level, correlating specific PCC items with OSCE scores to assess for internal correlation. Spearman's rank analysis was selected due to the assessment of monotonic relationships between ordinal and nonnormally distributed data, making it well‐suited for evaluating associations between SP questionnaire responses and OSCE performance scores, which were ranked and not assumed to have a linear relationship.

Spearman's rank of communication OSCE performance correlation and individual SP questionnaire domains.

As shown in Table5, Questions 4 and 5; assessing patient comfort in returning to or recommending the student practitioner, demonstrated strong inter‐item correlation as well as strong associations with OSCE scores. These items consistently showed high positive correlations in case history stations (ρ= 0.82–0.95,p< 0.001). While the correlations were weaker in Counselling 1 (ρ= 0.43–0.58), they were notably stronger in Counselling 2 (ρ= 0.72,p< 0.001). This correlation suggests that global impressions of trust and rapport, rather than specific communication skills like eliciting questions or demonstrating empathy, may strongly influence patient decisions about future interactions and recommendations.

Global impressions of trust and rapport, rather than specific communication skills … may strongly influence patient decisions about future interactions.

The ability to ask questions showed variable correlations across communication stations (ρ= 0.19–0.69), indicating it was not a consistently strong predictor of OSCE performance. Compared to other metrics such as trust and addressing concerns, questioning appeared to be a less reliable indicator of communication effectiveness in this context.

Additional analysis was conducted to further explore differences between SP questionnaire ratings and examiner‐assessed OSCE scores. While no significant differences were found between station types, variations between the two SPs within each station highlight the individualised nature of patient perception. These discrepancies reflect how each patient may uniquely interpret and evaluate their care experience, underscoring the subjective and personal lens through which communication is assessed. As individual SP scores and total OSCE performance were not directly matched, the data could not be visually represented; however, trends show that examiners consistently rated students more critically than SPs. This discrepancy may reflect a greater emphasis on public safety and integration of clinical knowledge within examiner assessments.

…each patient may uniquely interpret and evaluate their care experience, underscoring the subjective and personal lens through which communication is assessed.

A limitation of this study was the formulation of the questionnaire, which was influenced by the time constraints of the OSCE and the inability to utilise a validated DISQ questionnaire. Separate analysis indicated high internal construct validity between both questions, suggesting potential redundancy (Spearman's Rho 0.81–0.99), highlighting the need for further refinement in question design to capture distinct elements of PCC.

Opportunities for patients to ask questions were more prominent in the counselling stations, highlighting a limitation in the questionnaire design, which aimed to assess both history‐taking and counselling using a single framework. While the assessment rubric was structured to evaluate clinical data exploration and communication equally, the two station types differed in focus: Case history emphasised structured information gathering, whereas counselling centred on explaining clinical information and supporting patient understanding. This mismatch reflects the distinct communication subskills required in each context; history‐taking often involves practitioner‐led questioning, while counselling encourages more patient‐led dialogue. This difference likely contributed to SPs placing greater emphasis on relational aspects, such as trust and addressing concerns, over microskills like eliciting questions.

SPs placing greater emphasis on relational aspects, such as trust and addressing concerns, over micro‐skills like eliciting questions.

To address these limitations, future questionnaire designs should distinguish more clearly between initial rapport‐building and development of trust in considering developing separate tools for case history and counselling contexts. This would allow more accurate weighting of elements like the ability to ask questions, particularly where relevant to the station's communication goals. In line with established frameworks such as CCOG, MAAS and SEGUE; which emphasise the value of inviting patient questions and concerns, students received high ratings in counselling stations for allowing space for patient input. Additionally, replacing the referral item with a measure of patient confidence in decision‐making could provide more meaningful insights into how well students support informed, patient‐centred choices.

The observed variability in SP ratings across similar station types may reflect differences in how SPs interpret and apply scoring criteria, even with standardised scripts and training. This underscores the subjective nature of SP evaluation and the importance of regular calibration and reflective training for SPs to ensure consistency. Specifically, training should support SPs in distinguishing between general impressions and targeted assessment criteria, improving the reliability of SP contributions to summative evaluations. While the script aimed to capture nuances in social and adverse considerations relevant to patient‐centred communication, it may have been insufficient to fully represent the complexities of individual SP perceptions and scoring behaviours. Furthermore, the questionnaire did not capture the qualitative reasoning behind SPs' scoring decisions. While quantitative metrics provide useful insights into performance, they lack the depth to explain why certain behaviours were rated highly or poorly.

While trust and rapport appear to play a dominant role in SP ratings, further research is needed to determine whether these impressions reflect genuine communication competence or if they are influenced by external factors such as student confidence or perceived professionalism. Future studies should explore these relationships in larger cohorts or across different healthcare disciplines. The results highlight the value of integrating both SP and examiner assessment into PCC assessment to ensure a more holistic and reliable evaluation of communication skills.

The findings highlight the critical role of trust and rapport in shaping patient decisions about future interactions and referrals, shifting the focus from discrete communication behaviours to broader relational competencies. This aligns with the study's aims to examine the correlation between OSCE station scores and SP feedback, as well as to identify the aspects of PCC most strongly associated with high examiner‐assessed performance. These findings advance our understanding of PCC assessment by highlighting potential gaps in current educational frameworks, particularly in distinguishing between specific subskills (e.g. eliciting questions and addressing concerns) and overarching relational competencies.

The findings indicate that patient feedback should play a more substantial role in both formative and summative assessments of PCC. Given that trust and referral likelihood were the strongest predictors of performance, healthcare and optometry programmes should consider embedding structured patient feedback into competency‐based evaluations, ensuring these relational skills are explicitly assessed. One approach could be allocating a small percentage of OSCE marks to patient feedback, reinforcing its value within existing assessment frameworks.

Additionally, patient surveys should be conducted at multiple stages of clinical training beyond OSCEs, to provide students with ongoing insights into their communication effectiveness and how it influences patient trust. Expanding patient feedback beyond formative assessments and incorporating it into summative evaluations could enhance the authenticity of PCC assessments, ensuring students are equipped not only to diagnose and treat but also to establish meaningful patient relationships. By providing a direct avenue for patients to assess communication effectiveness, this approach supports an ongoing feedback loop that reinforces desired PCC behaviours and prepares students for patient‐centred clinical practice.

While established frameworks such as CCOG, MAAS and SEGUE offer robust guidance, this study highlights the need to prioritise trust‐building as a distinct and explicit core competency in clinical training and assessment within health and medical programmes. Although commonly viewed as part of rapport‐building, trust development warrants greater emphasis to be recognised as a distinct and essential component of PCC. Enhancing frameworks to integrate relational competencies more explicitly will not only improve patient satisfaction but also strengthen clinical outcomes in future healthcare practice.