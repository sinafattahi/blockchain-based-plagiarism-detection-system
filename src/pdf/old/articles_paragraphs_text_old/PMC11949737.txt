Visuomotor integration (VMI) impairments are common in children with cerebral palsy (CP) and can impact performance of goal‐directed upper‐extremity tasks. VMI impairment is clinically assessed using the gold‐standard Beery‐Buktenica test, whereas research paradigms use computerized assessments incorporating eye and hand movement tracking with touchscreen displays. Immersive virtual reality (VR) may potentially enable more ecologically valid VMI assessments through the inclusion of 3D tasks and visual distractions. However, the potential of immersive VR as a VMI assessment method in children with CP has not been evaluated. The current study aims to investigate how VR can assess VMI impairments in children with CP.

Twelve children with CP completed the Beery‐Buktenica VMI test and performed eye‐only, hand‐only and eye‐hand VMI tasks in touchscreen, visually simple VR and visually complex VR conditions. Eye and hand endpoint accuracy and task completion time quantified VMI performance. We compared performance on each task and in each environment between children with below‐ versus above‐average Beery‐VMI scores.

There were no significant relationships between Beery‐VMI score and eye‐hand task performance in visually simple VR. Compared to the touchscreen task, participants demonstrated significantly reduced eye and hand endpoint accuracy in visually simple VR, with no difference between Beery‐VMI groups. Children with below‐average Beery‐VMI scores decreased eye endpoint accuracy and increased trial completion time in visually complex VR.

Findings from this pilot study do not support immersive VR as a VMI assessment method in children with CP.

This study found no correlation between Beery‐Buktenica scores and VR eye‐hand coordination task performance, suggesting that these assessments differ fundamentally in sensorimotor demands.

Eye and hand endpoint accuracy was worse in VR compared to the touchscreen task for children with CP, potentially due to the lack of haptic feedback in VR. Indeed, the absence of haptic feedback in VR may pose greater challenges for children with VMI impairments, who often rely on tactile feedback for compensation.

Performance in immersive VR was characterized by reduced accuracy and longer task completion times as compared to the touchscreen environment, especially for children with below‐average Beery‐Buktenica scores. Children with stronger baseline VMI abilities showed better adaptation to VR, suggesting that VR taskbeds may offer unique insights into speed‐accuracy trade‐offs in this population.

The increase in eye movements in the visually complex VR condition suggests that children with CP prioritized hand accuracy over gaze stability when faced with visual distractions. This may reflect real‐world challenges where many visual stimuli are present.

Future research should explore how VR‐based assessments relate to real‐world tasks and whether advances in haptic and optical tracking technologies can enhance their clinical utility.

Cerebral palsy (CP) is the most common cause of physical disability in childhood, impacting approximately 3.1/1000 children and adolescents in the United States (Zablotsky et al.2019). In addition to primary motor and/or sensory impairments (Rosenbaum et al.2007; Philip et al.2020; Poitras et al.2021), children with CP may demonstrate visuomotor integration (VMI) impairments that impact motor performance of common tasks such as handwriting, dressing, and engaging in sports (Noureen et al.2022; Steenbergen and Gordon2006; Sarlegna and Sainburg2009). VMI is defined as the ability to perceive and process visual input and coordinate it with a motor response to produce accurate, goal‐directed movements (Schneck 2005). The Beery‐Buktenica Developmental Test of Visual‐Motor Integration (6th edition) (Beery and Beery2010) is a gold‐standard VMI assessment for children with CP and is widely used in clinical practice. It is a paper‐and‐pencil assessment requiring children to replicate progressively complex geometric forms. The Beery‐VMI has strong construct validity (van Hartingsveldt et al.2011), an interrater reliability of 0.75–0.88 and a test–retest reliability of 0.54–0.58 in children with CP (Harvey et al.2018). Other valid and reliable clinical VMI assessment methods include the Developmental Test of Visual Perception (Hammill, Pearson, and Voress2014) and the VMI subtest of the Peabody Developmental Motor Scales (Folio and Fewell2000). Advantages of the Beery‐VMI over other scales include its strong psychometric properties, widespread clinical adoption among pediatric therapists (Ohl and Schelly2022; Dunford, Bannigan, and Wales2013), and low burden of administration and established normative data for categorizing performance.

Reaching movements provide an ideal paradigm for assessing VMI ability because they require coordination of visual target identification, motor planning and execution in a way that directly relates to functional tasks. In typically developing children, accurate reaching typically develops between 5 and 13 months of age and refines further, with most children achieving adult‐like timing and accuracy by 9 years of age (Favilla2006; von Hofsten2007). In contrast, children with CP frequently struggle with accurate reaching movements needed to point at or grasp objects (Chang et al.2005; Rönnqvist and Rösblad2007; Crotti et al.2024), demonstrating difficulties in locating objects, coupled eye, head, and hand movements during reaching, and reaching accuracy (Saavedra et al.2009; Dutton et al.2004).

In research settings, computerized reach‐to‐target tasks using eye and hand movement trackers and touchscreen digitization can quantify the visual and motor contributions of VMI impairments during reaching tasks in children with CP. For example, Ciman et al. (2018) used a flatscreen task to evaluate VMI for children with cerebral visual impairment. Saavedra et al. (2009) used eye‐hand coordination tasks to evaluate the functional coupling of the eye, head, and hand during reaching movements in children with CP using continuous visual feedback to guide reaching movements (Verrel, Bekkering, and Steenbergen2008). Hand movements during reaching are coupled with eye and head movements and are significantly slower in children with CP compared to typically developing children (Saavedra et al.2009). This heavy reliance on vision for movement makes children with CP less able to scan for key visual cues for action planning in response to environmental changes (Steenbergen and Gordon2006). Although touchscreen reach‐to‐touch tasks provide precise measurement of endpoint accuracy, they constrain movement to a 2D plane and fail to capture the complexity of real‐world reaching movements that require coordination of arm movements through 3D space and integration of depth cues. Immersive virtual reality (VR) viewed in a head‐mounted display (HMD) may offer unique capabilities that could enhance VMI assessment, including eye and hand movement (Imaoka, Flury, and de Bruin2020; Spitzley and Karduna2019) and systematic manipulation of task demands during virtual object interaction in a 3D environment. Compared to touchscreen displays, immersive VR provides a more ecologically valid environment by enabling real‐time, body‐responsive interactions in a naturalistic setting (Bortone et al.2018; Harris et al.2019). Kinematic patterns and visuomotor scaling in immersive VR are the same as in the physical environment during reach to grasp movements in healthy adults (Arlati et al.2022; Furmanek et al.2019). Immersive VR also uniquely provides the ability to manipulate the complexity of visual task stimuli (Lubetzky et al.2019; Rossi et al.2019). This is important in VMI assessment in order to tease out the contribution of the visual component of VMI. Children with CP may struggle to locate objects in a crowded visual field or may have difficulty tracking fast‐moving targets (Dutton et al.2004). As a compensation method, children with CP may use online visual monitoring of movements to compensate for underlying sensorimotor deficits (Verrel, Bekkering, and Steenbergen2008; van Roon, Steenbergen, and Meulenbroek2005). Increased visual attention to the affected hand has been reported both at the beginning and during object transport (Verrel, Bekkering, and Steenbergen2008). However, close visual monitoring on hand may compromise the planning process as a whole, impairing the ability to scan the visual scene and identify task‐relevant information in advance.

However, the use of immersive VR as an assessment method is limited by fundamental differences between VR and physical environments that may influence VMI, including a lack of haptic feedback and altered perceptual cues. Indeed, healthy adults demonstrate reduced visuospatial accuracy (Valori et al.2021), inaccurate depth estimation, reduced motor performance, and different spatial–temporal patterns when reaching for objects in immersive VR compared to the physical environment (Furmanek et al.2019). To the authors' knowledge, no studies have compared reaching kinematics in children with CP in immersive VR versus the physical environment, although studies in non‐immersive VR indicate differences in hand and trunk movement kinematics during reaching (Robert and Levin2018).

Immersive VR is an accepted intervention modality for distraction and pain management (e.g., Lambert et al.2020) but is in its infancy as a neuromotor rehabilitation intervention and evaluation method, and feasibility studies have demonstrated the safety of immersive VR in pediatric populations (Bortone et al.2018; Shen et al.2022). Recent technological advances, including lightweight HMDs with embedded eye tracking and controller‐free, markerless gesture detection, have increased the clinical accessibility of immersive VR. This study represents a first step in evaluating the potential of immersive VR for VMI assessment in children with CP through an experimental paradigm involving eye‐only, hand‐only, and eye‐hand VMI tasks (Saavedra et al.2009). By comparing performance across gold‐standard clinical, computerized, and novel VR environments, we aimed to understand both the opportunities and limitations of VR‐based VMI assessment in this population.

Objective 1: Describe VMI task performance in paper‐and‐pencil (Beery‐Buktenica), touchscreen, and visually simple immersive VR (Simple VR) conditions.

Beery‐Buktenica score will correlate with VMI task performance in the eye‐hand task in touchscreen and Simple VR.

VMI task performance will differ between touchscreen and Simple VR.

VMI task performance in Simple VR will be worse in children with below‐average versus above‐average Beery‐Buktenica scores.

Objective 2: Explore the impact of additional visual distractions in immersive VR (Complex VR) on VMI task performance.

All participants will demonstrate poorer VMI task performance in Complex VR compared to Simple VR.

The impact of additional visual complexity will be higher for children with below‐average Beery scores compared to children with above‐average Beery scores.

The study took place in the CP clinic at the Barbara Bush Children's Hospital. Participants and their caregivers provided written‐informed assent and consent. The research protocol was approved by the Institutional Review Board (IRB) at Maine Medical Center and Northeastern University.

Children and youth aged 7–16 years were recruited by a study team member. Eligibility criteria included Gross Motor Function Classification System (GMFCS) Levels I–IV and Manual Ability Classification System (MACS) Levels I–III. Exclusion criteria included dominant arm elbow or shoulder flexion contracture exceeding 10°, the presence of uncorrected visual acuity or visual field deficits such as homonymous hemianopsia or oculomotor disturbance (as reported by the child's caregiver), uncontrolled photosensitive seizures (occurrence of at least one seizure in the last 3 months), hemineglect and cognitive impairments that would prevent comprehension of the two‐step instructions required for task completion (as judged by the caregiver).

The study took place in a 70‐square‐foot clinic room equipped with a nonadjustable desk and a height‐adjustable chair. Figure1illustrates the study procedure. To characterize the study sample, a registered physical therapist administered the following clinical assessments: Box and Blocks Test (Mathiowetz, Federman, and Wiemer1985), ABILHAND‐Kids (Arnould et al.2004), Modified Ashworth Scale (Harb and Kishner2021), range of motion and proprioception testing (Fugl‐Meyer) (Gladstone, Danells, and Black2002).

Experimental procedure. All children completed Box and Blocks Test (Lambert et al.2020), ABILHAND‐Kids (Shen et al.2022), Modified Ashworth Scale (Mathiowetz, Federman, and Wiemer1985), range of motion and proprioception testing (Fugl‐Meyer) (Arnould et al.2004), followed by the reach to touch task in touchscreen, Simple VR and Complex VR conditions.

Participants then completed the Beery‐Buktenica VMI (6th edition; long form format containing all 24 Beery‐VMI items; excluding visual perception and motor coordination subtests) (Beery and Beery2010) using their dominant hand. The touchscreen condition was displayed on a 24‐in. touchscreen monitor. A Tobii Nano eye tracker (60 Hz) was mounted at the bottom edge of the monitor to collect eye data (Figure2A). Task performance, eye tracking, and video streams were time synchronized via a customized MATLAB (2020a) program. The Simple and Complex VR conditions were displayed on an HTC Vive Eye Pro (HTC Inc., Taoyuan, Taiwan) with an embedded Tobii eye tracker (90 Hz). Hand movement was tracked using a Vive tracker attached to the reaching hand. Hand interactions in VR were recorded using Manus Prime II (Manus Inc., Gledrop, Netherlands) gloves to enable tracking of finger movements and equalize perception‐action reach to touch affordances between the touchscreen and the VR conditions (Figure2A). The task was programmed in Unity (2019.4.10f1).

(A) Experimental setup. The top figure is the setup of the touchscreen condition, and the bottom figure is the setup of immersive VR conditions. (B) Experimental tasks. In the eye‐only task, children need to keep their hands on the table and only look at the target. In the hand‐only task, children keep their gaze fixated at the center and only use hands to reach to touch the target. In the eye‐hand task, children were required to look at and touch the target at the same time. (C) Target position calculation. Target positions were determined in the touchscreen condition because of the limit of the screen size. The same target positions were kept in VR conditions. (D) Beery‐VMI task, touchscreen, Simple VR, and Complex VR condition. Target diameters and positions were the same across touchscreen and VR conditions for each child.

Children were seated in front of the desk in a position where they could easily reach the touchscreen with their dominant hand. Arm length was measured to individualize the target location and ensure similar target viewing angles across participants. A height‐adjustable chin rest was used to restrict head movement. A marker was placed on the desk to indicate hand starting position.

Eye tracking calibration was conducted before the task sessions in each environment using the standardized Tobii calibration program. Children then undertook the three study tasks in a nonrandomized order: an eye‐only task isolating eye movement where children were asked to keep their hand still while moving their gaze from the 5‐cm‐diameter central fixation point (a yellow dot) to the gaze target upon its appearance on‐screen; a hand‐only task where children kept their gaze on the central fixation while reaching to touch the target with their index finger; and an eye‐hand task in which children both looked and touched the target (Figure2B). Children were instructed to return their gaze and hand back to the central fixation and starting position after each trial.

The target was a 10‐cm‐diameter bulls‐eye shape appearing in a randomized sequence of two locations in the testing side visual field: 10 cm above the midline and at a 6° viewing angle and 10 cm below the midline and at a 16° viewing angle (Figure2C). There were four trials per target (total = 8 per task). This trial number of trials was chosen to keep total testing duration appropriate for a clinical setting. Each trial had a maximum duration of 60 s.

Following the touchscreen condition, the child was fitted with the HMD, Vive tracker, and haptic glove and introduced to immersive VR task interaction (Figure2D). The above testing procedures were repeated in Simple and then in Complex VR. The Simple VR condition replicated the touchscreen tasks with the same dimensional parameters. The Complex VR condition added a virtual classroom context: Children were seated behind a virtual desk near two animated avatars. Visual distractions (flying paper planes, bouncing basketballs, and flying balloons) were presented. To introduce realistic depth cues, the bulls‐eye target was replaced by a red balloon target of the same diameter (Figure2D).

Participants undertook a total of 72 trials across the 3 tasks and 3 conditions (touchscreen, Simple VR, and Complex VR).

The assessment was scored by a registered physical therapist using the standard scoring manual. For analyses, participants were divided into two groups based on Beery Standard score (which compares an individual's performance to a normative sample of the same age): ‘below‐average’ (participants with below‐average, low, and very low Beery scores) and ‘above‐average’ (participants with average, above‐average and high Beery scores).

Eye and hand endpoint accuracy is the distance (in cm) between gaze or finger position and the center of the target at the last frame of each trial.

The period (in seconds) from the start of a trial (i.e., the appearance of target) to the end of the trial, defined as a successful eye gaze on the target in the eye‐only task, a target touch in the hand‐only task, both eye gaze and hand touch on target in the eye‐hand task, or the expiration of the 60‐second trial period.

The mean trial completion rate was 90.32% (SD = 23.82%) in the touchscreen condition and 88.75% (SD = 27.38%) in the VR conditions. Eye tracking data validity was recorded via the ‘isValid’ function in Tobii API. A primary cause of invalid data in the touchscreen condition was suboptimal head position relative to the eye tracker caused by inability to individualize the experimental setup. We excluded all trials with less than 85% of valid eye data. After cleaning, 112 trials (88.2% of all tested trials) in the touchscreen condition, 106 trials (96.8% of all tested trials) in Simple VR, and 93 trials (98.4% of all tested trials) in Complex VR were retained for analysis.

To identify artifacts, we calculated the angular displacement between consecutive gaze samples using normalized vectors. Gaze velocity was computed as the angular displacement (in degree) divided by the change in the time (in seconds) between gaze samples. Gaze samples where gaze velocity exceeded 800°/s were removed.

RStudio (Version 4.1.3) was used for all statistical analyses. We did not undertake separate analyses for each target location. Nonparametric analyses were selected due to the small sample size and significant deviations from normality confirmed by Shapiro–Wilk tests.

Spearman's correlations evaluated relationships between Beery‐Buktenica scores and each outcome measure in the eye‐hand task in touchscreen and Simple VR (Hypothesis1a). Wilcoxon rank‐sum tests evaluated performance differences on each task in each condition between Beery groups (Hypothesis1b). Within each condition, for each of the three tasks, Wilcoxon rank sum tests compared the magnitude of change (touchscreen − Simple VR) in each outcome measure between Beery groups (Hypothesis1c). Effect sizes were calculated usingr = z/√n, wherezrepresents thezscore derived from the Wilcoxon rank sum statistic andnrepresents the total sample size.

Wilcoxon signed‐rank tests compared the results of each outcome measure between Simple and Complex VR (Hypothesis2a). Wilcoxon rank sum tests compared the magnitude of change (Simple − Complex VR) in each outcome measure between Beery groups (Hypothesis2b). Effect sizes were calculated usingr = w/√n, wherewrepresents the Wilcoxon signed rank statistic andnrepresents the number of paired observations.

Twelve children (mean age = 9.9 ± 2.7 years) with hemiplegic, diplegic, or quadriplegic CP at GMFCS Level I‐IV and MACS Levels I‐IV participated in the study. Table1summarizes participant demographics (TableA1).

Abbreviations: GMFCS, Gross Motor Function Classification System; MACS, Manual Ability Classification System.

Beery and Beery (2010) classify Beery standard scores 120–129 ashigh, 110–119 asabove average, 90–109 asaverage, 80–89 asbelow average, 70–79 aslowand < 70 asvery low.

Box and Blocks scores in typically developing children and adolescents aged 7–16 years range from 54 to 76 (each year as an age group) (Lambert et al.2020). Higher score indicates better performance.

Abilihand Logit percentage 40%–70% indicates moderate impairments. Functional limitations are apparent in daily activities, requiring some assistance or adaptive strategies. Below 40% indicates severe impairment, with significant difficulty in performing manual tasks independently (Shen et al.2022).

Ashworth score of 0 indicates normal muscle tone, 1 indicates mild to moderate spasticity with resistance occurring after movement, 2 indicates moderate spasticity with significant resistance but movement is still possible and 3 indicates severe spasticity, where resistance significantly limits passive movement (Mathiowetz, Federman, and Wiemer1985).

Fugl Meyer upper limb range of motion assessment evaluates a total of 44 points across 11 joints in both arms. Each joint is scored on a scale from 0 to 2. A score of 2 points indicates normal range of motion, 1 point indicates limited range of motion and 0 points indicates no movement (Arnould et al.2004).

No significant correlations were found between Beery‐Buktenica score and either eye or hand endpoint accuracy for the eye‐hand task in touchscreen or Simple VR. In the eye‐only task, all children demonstrated decreases in eye endpoint accuracy in Simple VR compared to touchscreen (Figure3A,v= 3,p= 0.009,r= 0.64). In the hand‐only task, all children reduced hand endpoint accuracy in Simple VR compared to touchscreen (Figure3B,v= 2,p= 0.006,r= 0.63). In the eye‐hand task, all children reduced both eye (Figure3C,v= 3,p= 0.034,r= 0.67) and hand endpoint accuracy (Figure3D,v= 4,p= 0.004,r= 0.89) in Simple VR compared to touchscreen.

Eye and hand endpoint accuracy in touchscreen and Simple VR conditions. Asterisks represent statistical significance (*p< 0.05; **p< 0.01). (A) Eye gaze endpoint distribution and error values in the eye‐only task. (B) Hand endpoint accuracy and errors in the hand‐only task. (C) Eye endpoint distribution and errors in the eye‐hand task. (D) Hand endpoint distribution and errors in the eye‐hand task.

In the eye‐only task in Simple VR, children with below‐average Beery scores had lower eye endpoint accuracy (Figure3A,w= 10,p= 0.047,r= 0.36) and longer trial duration (Figure4A,w= 7,p= 0.015,r= 0.25) compared to children with above‐average Beery scores. In the hand‐only task in Simple VR, children with below‐average Beery scores had longer trial duration (Figure4B,w= 4,p= 0.015,r= 0.25) compared to children with above‐average Beery scores. In the eye‐hand task, no differences were found between Beery groups on any outcome measure.

Task performance in Simple VR and Complex VR conditions. Asterisks represent statistical significance (*p< 0.05; **p< 0.01). (A) Trial duration during each task for children in different Beery groups in touchscreen, Simple VR and Complex VR conditions. Black error bars denoted standard error. (B) Endpoint errors for each Beery group in Simple and Complex VR conditions. Error bars denoted standard errors.

In the eye‐only task, there were no differences in eye endpoint accuracy or trial completion time between Simple and Complex VR. In the hand‐only task, all participants had higher hand accuracy (v= 4,p= 0.048,r= 0.85) in Complex compared to Simple VR. In the eye‐hand task, all participants had lower eye endpoint accuracy (v= 3,p= 0.037,r= 0.67) in Complex compared to Simple VR.

There were no significant differences in performance between Simple to Complex VR conditions between Beery groups in the eye‐only task. In the hand‐only task, children with below‐average Beery scores had significantly higher hand endpoint accuracy in Complex compared to Simple VR (Figure4B,w= 4,p= 0.015,r= 0.11), whereas the performance of children with above‐average Beery scores did not change. In the hand‐only task, children with below‐average Beery scores had significantly reduced trial completion time in Complex compared to Simple VR (Figure4A,w= 2,p= 0.038,r= 0.13), whereas the performance of children with above‐average Beery scores did not change. In the eye‐hand task, children in the below‐average Beery group demonstrated longer trial completion time in Complex compared to Simple VR (Figure4A,w= 4,p= 0.041,r= 0.11). Children in the above‐average Beery group demonstrated shorter trial completion time in Complex compared to Simple VR (Figure4A,w= 3.5,p= 0.032,r= 0.22). Children with below‐average Beery scores had higher hand endpoint accuracy in Complex VR in this task (Figure4B,w= 6,p= 0.047,r= 0.17), whereas hand endpoint accuracy did not change in children with above‐average Beery scores in Complex compared to Simple VR.

Much remains to be understood about how to evaluate the impact of VMI impairments on performance of daily activities in children with CP. Technological characteristics of immersive VR suggest that it may provide a precise and ecologically valid VMI testing platform, enabling assessments of VMI impairments in 3D space. The first step in exploring this potential is to understand how performance of a VMI task in immersive VR relates to performance on gold‐standard clinical VMI assessments, such as the Beery‐Buktenica. This exploratory study did not find a relationship between children's performance on the Beery‐Buktenica VMI measure and their performance on an eye‐hand coordination task in immersive VR. This finding may reflect fundamental differences in how these assessments capture VMI abilities. The Beery‐Buktenica test emphasizes fine motor control and precise hand‐eye coordination in a 2D plane, whereas our VR reaching task engaged gross motor movements in 3D space with different sensorimotor demands. Additionally, the increased sensory feedback and enhanced depth perception provided by immersive VR may have helped children compensate for VMI challenges that are more apparent in paper‐based tasks.

Understanding the potential unique contributions of VMI assessment in a reach‐to‐target task in 3D space (immersive VR) requires comparison to the same reach to target task on a 2D touchscreen. On average, children reduced eye and hand endpoint accuracy in immersive VR but did take longer to complete the task. Dichotomizing our sample by Beery‐Buktenica score revealed that children with below‐average Beery scores had lower speed and poorer accuracy in immersive VR as compared to children with average or above‐average Beery scores. These findings suggest that immersive VR may provide unique insights into speed‐accuracy trade‐offs in children with varying levels of VMI ability. The additional perceptual and motor demands of operating in 3D space may reveal VMI challenges not captured by 2D assessments. Indeed, research with healthy adults demonstrates that motor control in immersive VR requires more sensory resources as compared to the physical environment (Slobounov et al.2015). Although all participants showed some reduction in accuracy in VR compared to the touchscreen, those with stronger baseline VMI abilities appeared better able to adapt to the novel environment. The immersive VR environment may thus offer a more sensitive tool for understanding how children with different VMI capabilities navigate complex speed‐accuracy demands in real‐world‐like conditions, where depth perception and functional reach movements are required. This could have important implications for understanding functional motor performance beyond what can be assessed through traditional paper‐based or 2D digital tasks.

The absence of haptic feedback in the VR environment represents a critical difference that may explain both the overall reduction in accuracy and the particular challenges faced by children with lower VMI scores. Although touchscreen interactions provide immediate tactile confirmation of target contact, reaching in VR requires children to rely solely on visual feedback to determine endpoint position. This lack of haptic confirmation may be especially challenging for children with VMI impairments, who might typically rely more heavily on tactile feedback to compensate for VMI challenges. The combination of increased degrees of freedom in 3D movement and absence of haptic feedback could create a more demanding task environment that reveals subtle differences in how children with varying VMI abilities integrate and adapt to primarily visual feedback. This suggests that VR‐based assessments might be particularly valuable for understanding how children process and utilize visual feedback in isolation, providing insights into VMI challenges that may be masked in tasks where multiple sensory feedback channels are available.

The visually complex VR condition was designed to create stimulus‐driven attentional shifts to further evaluate the impact of added visual distractions on task performance. All participants demonstrated reduced eye endpoint accuracy in Complex compared to Simple VR. This suggests that managing visual distractors places additional demands on attentional resources during visuomotor tasks, regardless of baseline VMI ability. Indeed, attentional deficits are more prevalent among children with CP than in the general population, with overall estimates ranging from 19% to 35% (Craig, Savino, and Trabacca2019) compared with 8.7% among the general US population (Casseus, Cheng, and Reichman2024).

The finding that visual complexity affected eye movements more notably than hand movements may indicate that children prioritize maintaining hand accuracy at the expense of gaze stability when faced with competing visual stimuli. This pattern could have important implications for understanding how children with CP perform VMI tasks in real‐world environments, which typically contain numerous visual distractors. The ability to systematically manipulate visual complexity in VR while maintaining precise measurement of both eye and hand movements provides a unique opportunity to understand how attention and VMI interact—a relationship that is difficult to study in traditional clinical assessments or natural environments where visual complexity cannot be as carefully controlled.

Interestingly, children with below‐average Beery scores increased hand endpoint accuracy in both hand‐only and eye‐hand tasks in Complex VR. These might be due to the change in target shape from a ‘bulls‐eye’ to a balloon, which provided more depth cues. Subsequent research should evaluate whether visual target characteristics influence VMI performance in 3D space. The obvious visual perceptual differences of task interaction in immersive VR as compared to the real world require careful consideration when making conclusions relevant to real world performance. Even with high‐quality graphics in immersive virtual environments, textures, lighting and shadows may be simplified or approximated in VR, which can affect object perception and interaction. Compared to the physical environment, healthy adults demonstrate slower and less smooth movements during reaching in VR, with more uncertainty in estimating the size and distance of virtual objects. (Furmanek et al.2019) Depth perception in VR can be less accurate than in the real world due to the limitations of display technology and rendering methods. VR systems often simulate depth cues through stereopsis but might not perfectly mimic other cues like motion parallax and accommodation. (Robert and Levin2018; Gerig et al.2018) The potential latency between the user's actions and the system's visual response may also increase cognitive load and impact motor performance, as users must not only perform the task but also learn how to interact effectively within the artificial setting.

Study findings can be interpreted through two complementary theoretical frameworks. First, Perception‐Action Coupling theory (Warren1990) suggests that perception and action are inherently linked systems, where changes in perceptual information directly influence motor planning and execution. This framework suggests that introducing novel perceptual conditions, such as those in immersive VR, may challenge established perception–action relationships, particularly in children with CP who already demonstrate altered sensorimotor integration. Through this lens, our findings of reduced accuracy in VR may reflect disruption of the typical perception–action coupling that children with CP have developed in physical environments. The altered depth cues and lack of haptic feedback in VR create a mismatch between expected and actual sensory consequences of movement. This theoretical perspective helps explain why children with below‐average Beery scores showed greater difficulty—their potentially less robust perception–action coupling may make them more vulnerable to perturbations in the usual relationship between visual information and motor output. This aligns with previous work showing that children with CP have particular difficulty updating their motor responses when visual information is manipulated.

Internal models' theory (Francis and Wonham1976) provides a second relevant framework, proposing that the brain maintains and updates representations of body state and movement consequences for motor control. This theory predicts that children with CP may face unique challenges in developing and updating internal models in novel environments like VR, where typical sensorimotor relationships are altered. This theoretical perspective also helps explain why adding visual complexity differentially impacted children with below‐average Beery scores, as managing additional attentional demands may further tax their ability to update and utilize internal models effectively.

More research is required to understand whether VR‐based VMI assessment may capture distinct aspects of VMI that complement, rather than replicate, insights gained from established clinical measures. New lightweight and lower cost HMDs such as the Meta Quest 3 (Meta Platforms Inc., Menlo Park, CA) employ high‐precision optical hand tracking, eliminating the need for VR gloves by enabling natural gesture recognition and finger‐level articulation detection (Meta Horizonn.d.). In addition, advances in haptic technology include multimodal haptic feedback through integrated force sensors, vibrotactile actuators, and variable‐tension mechanisms, enabling users to experience realistic tactile sensations when interacting with virtual objects, from subtle texture variations to precise force feedback during grasping and manipulation (Pacheco‐Barrios, Ortega‐Márquez, and Fregni2024). Future studies should examine how performance in immersive VR environments correlates with performance of the same task in the physical world. Although VR offers precise measurement and standardized testing conditions, understanding how the absence of haptic feedback and other physical constraints in VR influences performance relative to real‐world activities will be crucial for establishing the ecological validity and clinical utility of VR‐based VMI assessments.

Our small sample size limits statistical power and prevented comparisons between children with different GMFCS and MACS levels. Although using individual trials in our analyses provided more data points, this approach risks inflation of statistical power. Future studies with larger samples are needed to enable robust repeated measures analyses and stratification by functional classification levels. Indeed, given our small sample size, individual outlier performances may have substantially influenced group‐level findings, highlighting the importance of replicating these results in larger cohorts to confirm observed patterns. Data collection in a pediatric CP clinic increased recruitment possibilities but prevented the use of a motion capture system to track hand movement in the touchscreen condition. It also limited time for children to familiarize themselves with immersive VR task interaction. The clinic space and furniture layout constrained adjustments for optimal eye tracking in the touchscreen condition, resulting in data loss. More information specific to data collection feasibility can be found in Cheng et al. (2023). Eye tracker sampling rates prevented the calculation of robust saccadic data but provided practical options for collecting valid eye gaze information in a clinic environment.

We lacked information in children's clinical records regarding both their primary visual impairments and the presence of any attentional deficit diagnoses. This is problematic given that approximately two‐thirds of children with CP may have impaired visual acuity and/or visual field defects such as cortical visual impairment. (Philip et al.2020) VMI can be influenced by visual difficulties such as abnormal ocular movements (fixation, saccades, and pursuits) in children with CP (Gerig et al.null). Feasibility restrictions limited our ability to include an attentional evaluation as part of our study procedures.

Methodological limitations include the fact that noncounterbalanced task presentation potentially confounded performance due to practice or fatigue effects. Finally, the lack of a typically developing comparison group is a limitation. Without data from typically developing children, we cannot determine whether the observed differences between VR and touchscreen performance are specific to children with CP or reflect general adaptations to VR environments that would also be seen in children without motor impairments.

Initial exploration of immersive VR for VMI assessment in children with CP revealed variable relationships between traditional and VR‐based measures. Although performance in VR did not directly correlate with Beery‐VMI scores, VR tasks exposed differences between children with above‐ and below‐average VMI abilities, particularly in response to visual complexity. The unique characteristics of VR environments—including 3D movement demands and absence of haptic feedback—appeared to both challenge and support motor performance in unexpected ways. For example, enhanced depth cues from 3D targets improved accuracy for some children, whereas additional visual distractions consistently impacted eye movement control. These findings suggest that VR‐based assessment may capture distinct aspects of VMI that complement traditional clinical measures. As VR technology becomes increasingly accessible, future research should examine how VR task performance relates to functional activities and whether VR‐based assessment could provide clinically relevant insights into VMI abilities not captured by current tools.