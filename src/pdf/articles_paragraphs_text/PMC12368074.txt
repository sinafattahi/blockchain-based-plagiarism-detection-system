This study tackles the challenge of image recognition for datasets with high inter-class similarity, using 18 native Platycerium species as a case study. Due to their substantial visual similarities, initial training with ResNet50 yielded a baseline accuracy of less than 10%. To address this, we conducted a comprehensive analysis using multidimensional confusion matrices to identify seven primary confusion factors, such as image edges, textures, and shapes, and stratified the dataset into processed and unprocessed images optimized for these factors through adjustments in saturation, brightness, and sharpening. A refinement process leveraging confusion matrices and bootstrapping was proposed to address ambiguous classes, significantly improving recognition of highly similar species. Recognition accuracy increased to approximately 60% after applying confusion factor analysis and image optimization, with further gains to over 80% using EfficientNet-b4 and over 90% using EfficientNet-b7. These findings highlight the importance of feature selection and grouped analysis in recognizing highly similar images, offering a robust framework for optimizing recognition accuracy in challenging datasets and providing valuable insights for advancing image recognition technologies.

The online version contains supplementary material available at 10.1038/s41598-025-12502-9.

In the contemporary field of technology, deep learning techniques have made significant strides across various domains, particularly in areas like image recognition, handwriting and text identification, speech recognition, and language translation, demonstrating their robust capabilities. The evolution of these technologies profoundly impacts the development across different academic disciplines, especially in achieving breakthroughs in accuracy and efficiency. Specifically, in the area of image analysis and object recognition, deep learning also shows tremendous potential for application1.

However, one of the main challenges currently faced is that identifying the physical characteristics of plants through observation becomes increasingly difficult. Even experts in the field struggle to accurately identify some rare plant species, making the task even more daunting for the layperson. In this context, the application of artificial intelligence technologies becomes critically important. By using features such as leaf contour, eccentricity, centroid, and compactness as inputs to the system, plants can be accurately classified. Additionally, techniques like Principal Component Analysis (PCA), Hu’s moment invariants, and morphological feature-based analysis are widely used to develop efficient and accurate automated plant identification tools2.

Other studies have shown, such as the research on the Taichung Metropolitan Park plant identification system, that combining multiple features of flowers and leaf images captured on smartphones can significantly improve identification accuracy3. Similarly, the application of deep learning in real-time flower identification systems has shown outstanding performance, with Google’s Inception v3 model achieving an accuracy rate of over 95%4. Furthermore, plant recognition systems based on iOS devices using CNN models have achieved recognition rates of up to 90% for specific plants5. Research on the identification of poisonous plants also underscored its importance in public safety protection, covering 39 types of poisonous plants with an average recognition rate close to 90%6.

According to literature8, a comparison of different connection methods found that cross-layer connections significantly improve classification accuracy. The issue of high confusion in plant identification has been effectively resolved through dataset processing and model optimization7. Plant species identification in images or videos poses challenges due to the diversity of species, changes in orientation, viewing angles, and cluttered backgrounds. The work in8also demonstrates that both traditional and deep learning methods were used for identification. In traditional methods, features were extracted using Hu moments, Haralick textures, Local Binary Patterns, and color channel statistics, and classification was performed using multiple classifiers. In deep learning methods, the VGG 16 and VGG 19 CNN models showed higher accuracy in both standard and real-time datasets, surpassing traditional methods9.

Moreover, the ECOUAN team’s performance in the LifeCLEF 2015 challenge was particularly notable. The team used deep learning methods, with the entire system learning without the need for manually designed components. Through the use of pre-training and fine-tuning strategies with 1.8 million images in convolutional neural networks, the team successfully transferred the recognition capabilities learned from general domains to the plant identification task. This approach exceeded the best results of 2014, ranking fourth among all competing teams and tenth in 18 competitions10.

Finally, according to the most similar study in11for reference, it is known that plants at the seedling stage are extremely similar to weeds, which fiercely compete with crops for nutrients and water, severely affecting crop yield losses, with the impact reaching nearly 100%. This paper adopted the ResNet network model to enhance accuracy in image classification, ultimately changing the impact of weeds on agriculture11.

The aforementioned studies2–11address recognition across a broad range of different animal and plant species, with most species having distinct differences, making it relatively easy to achieve effective recognition results through parameter tuning. However, studies on plants of the same species significantly increase the difficulty of recognition. The main goal of this study is to explore the high similarity of Platycerium, providing valuable contributions to solving future high-similarity recognition problems. The economic value of Platycerium also draws widespread attention in current plant cultivation, and the completion of an automated recognition neural network will aid in the development of this field.

In summary, this paper establishes a framework applicable to various datasets and deep learning models. This framework includes directions such as dataset collection and organization, data preprocessing, model construction, model optimization, refined process design for extracting confused categories, and enhancement of high confusion factors. Through the establishment of this framework, we aim to provide more efficient solutions for subsequent recognition-related research, promote the development of image recognition applications across different fields, and further enhance the efficiency and accuracy of image recognition.

Platycerium, a distinctive plant within the Polypodiaceae family, primarily inhabit tropical and subtropical regions12. They survive as epiphytes, deriving nutrients from decomposing organic matter on trees, and are named for their antler-like fronds. The fronds are primarily divided into two types: the basal trophophores that protect the roots, store water, and facilitate photosynthesis, and the more conspicuous sporophores that, aside from conducting photosynthesis, also use the sporangia on the back of the leaves for reproduction. Due to the high similarity among various Platycerium species, this study explores different datasets and image optimization techniques, aiming to use this similar sample set as a case study to enhance the efficacy of artificial intelligence in image recognition and to provide valuable insights and inspiration for related fields.

For performance experiments, two datasets were utilized. The original image dataset was trained using a neural network without any image optimization, while the complex background dataset employed a special image optimization technique. This technique retained the parts of the image necessary for recognition learning while removing irrelevant background elements to enhance recognition performance.

Before training the network model with the datasets, all images with reading errors and related error messages were reviewed and displayed. These images were subsequently excluded from the training process to ensure the quality of the network model training. At the same time, the most suitable dataset for subsequent experiments was identified.

The research utilized Platycerium as examples, obtaining samples through both botanical garden photography and online searches, labeled by the authors, and augmented using Keras with rotation, flipping, and scaling to enhance diversity, resulting in 18 different species including Alcicorne, Bifurcatum, Veitchii, Willinckii, Andinum, Elephantotis, Coronarium, Ellisii, Hillii, Quadridichotomum, Stemaria, Grande, Superbum, Wandae, Holttumii, Ridleyi, Madagascariense, and Wallichii, with an average of 90 images collected per species as shown in Fig.1. See Appendix A for detailed characteristics of the 18 Platycerium species. The sporophores of Alcicorne, Bifurcatum, Veitchii, and Willinckii exhibit bifurcation, with those of Alcicorne and Veitchii specifically growing upwards; Andinum and Elephantotis feature large, elongated oval leaves; Coronarium sporophores are spoon-shaped; Ellisii, Hillii, Quadridichotomum, and Stemaria have broad leaf shapes with slight differences; Grande, Superbum, Wandae, and Holttumii are large Platycerium, the differences between Grande and Superbum lie in the branching of the sporophores, while Wandae and Holttumii differ in the splitting patterns of their sporophores.; Ridleyi and Madagascariense sporophores feature brain-like patterns with minor differences; Wallichii sporophores resemble butterfly wings.

This paper adopted the ResNet50 residual neural network as the core architecture due to its established role as a standard benchmark in plant identification studies, typically achieving high accuracies in less challenging datasets7,11. This choice allows us to highlight the specific difficulties posed by the high inter-class similarity of Platycerium species, which significantly reduces baseline performance. ResNet50 effectively addresses the performance degradation problem caused by training error accumulation6,13, thereby enhancing training efficiency and speed. To further optimize the model, we adjusted hyperparameters during the training process and introduced batch normalization modules, confirming their effectiveness in addressing the challenges of gradient vanishing and explosion.

Our experimental results indicated that removing complex backgrounds from the dataset slightly improves the model’s accuracy on the validation set, demonstrating that our initially constructed network model still has room for overall performance improvement14. Furthermore, we delved into the application of the deep residual learning framework, which achieves exemplary accuracy at a depth of 152 layers on ImageNet14. We analyzed residual blocks and incorporated optimized units from15, enhancing identity mappings in ResNet50-v2 to improve training efficiency. Due to ResNet50-v2’s suboptimal performance (~ 60% accuracy) on this challenging dataset, we evaluated EfficientNet (b4 and b7) as a planned alternative, leveraging its compound scaling method16for improved efficiency and accuracy.

In this experiment, for the construction and performance evaluation of the baseline network model, we utilized datasets from 18 native species of staghorn ferns, each category averaging 90 images. By comparing the original image dataset with the dataset processed to remove complex backgrounds, we trained using the ResNet50 model. Although previous research showed higher recognition accuracy, our baseline model, facing an unoptimized dataset with high similarity, showed less than 10% accuracy, highlighting the need for improved handling of highly similar datasets. The reason can be primarily attributed to the high inter-class similarity among the 18 Platycerium species, which poses a significant challenge for standard deep learning models without tailored preprocessing or optimization. In the unoptimized dataset, visual features such as leaf shapes, textures, and edges are nearly indistinguishable across species, leading to poor discriminative performance.

Simultaneously, this study also focuses on the problem of hyperparameter search in the machine learning field and its optimization challenges. Hyperparameter selection plays a crucial role in the model construction process, affecting the model’s final performance. The complexity and importance of this selection process further affirm that establishing disciplined and theoretically sound search strategies is critical for achieving efficient models17.

Precise configuration of hyperparameters is vital for preventing overfitting. In our experiments, we adjusted batch size, learning rate, and iteration number to determine the optimal hyperparameter settings in the training of the ResNet50-v2 model, laying the foundation for subsequent experiments18. Additionally, we incorporated callback function modules to monitor the training process and automatically adjust the learning rate, effectively preventing overfitting19.

Finally, the dataset was divided into 70% (1134 images) for training, 20% (324 images) for validation, and 10% (162 images) for testing. We trained using the ResNet50-v2 and EfficientNet-b3 models, and presented the data results through historical trend charts. Tables1and2clearly demonstrate the significant enhancements in recognition performance of the network models due to these optimization modules. We have conducted at least five independent training runs with different random seeds to obtain the provided accuracy.

In this study, we extensively explored Keras-based data augmentation to enhance the training dataset, generating diverse images from the original dataset to address insufficient data volume. We applied transformations including rotation angles (up to 30°), position shifts (up to 20% of image dimensions), shear transformations (up to 0.2 radians), scaling ratios (0.8 to 1.2), and horizontal and vertical flips to ensure dataset diversity20. To mitigate the risk of overfitting to superficial visual cues introduced by OpenCV-based enhancements (e.g., sharpening, brightness adjustments), we expanded the augmentation pipeline to include random noise injection (Gaussian noise with a standard deviation of 0.01) and color jittering (random adjustments to hue by ± 0.1, contrast by ± 0.2, and brightness by ± 0.2). These additions promote the learning of robust, semantically meaningful features, reducing reliance on specific visual artifacts.

The large-scale image ontology database, ImageNet, built on WordNet, provides tens of millions of annotated images, covering 80,000 synsets, offering a rich and diverse resource for computer vision research21. In model training, we utilize the pre-trained weights from ImageNet to enrich the model’s image information, effectively reducing the errors associated with training from scratch and accelerating the convergence process of the model, significantly enhancing the image recognition performance of the target detection22.

Referencing Table3, the deep learning model ResNet50, leveraging the strengths of convolutional operations, demonstrates superior performance in classification tasks, particularly with image-based datasets, compared to traditional machine learning approaches such as support vector machine (SVM), decision tree, and random forest. Moreover, the integration of Keras’s Data Augmentation and ImageNet pre-trained weights has significantly improved model training efficiency. After data augmentation, the accuracy of the model on the validation set has noticeably increased, demonstrating that optimizing image recognition and learning frameworks can effectively enhance recognition accuracy, achieving accuracy to around 60%23.

ML approaches and ResNet50 with/without data augmentation.

In this experimental approach, techniques such as hyperparameter tuning, callback functions, and data shuffling were introduced to enhance the foundational performance of the model. Combined with ImageNet’s pre-trained weights and Keras data augmentation techniques, a comparative study between the original image dataset and the optimized complex background dataset was conducted. Our model achieved higher diversity and cognitive performance during training. Although these optimization methods have increased the recognition accuracy to 60%, the issue of confusion among highly similar species remains.

Comprehensive experimental results indicated that, while the network model structure enhanced through data augmentation and improved image cognitive performance achieves certain success in image recognition as demonstrated in2–11, there is still room for improvement in recognition performance for datasets with high degrees of similarity. In subsequent experiments, we incorporate refined process designs for extracting confused categories and use image data statistical reports and confusion matrices for detailed observation and analysis of the original images. Ultimately, the OpenCV programming library was utilized to customize enhancements for factors causing confusion, further optimizing the network model’s training efficiency.

We trained our deep learning model using the ResNet50-v2 architecture, combined with various optimization strategies such as Keras data augmentation, pre-trained ImageNet weights, and callback functions. The collective effect of these strategies results in the multidimensional confusion matrix displayed in Fig.224, used to evaluate the actual versus predicted data across 18 classification categories.

In preliminary experiments, we observed that the traditional ResNet50 model performed poorly in identifying high confusion rate categories. Therefore, we adopted ResNet50-v2 as the base model25, which optimized training loss through identity mapping and improved the network’s learning capabilities. Based on the evaluation of actual versus predicted data, we identified the need for a more in-depth analysis method. Consequently, we explored bootstrapping as an additional analytical approach.

In our study’s various analytical method discussions, we referenced statistical methods to address the issue of class imbalance26, which enhances data accuracy and focus on solutions for supervised classification problems, encompassing classifier construction, object representation, and evaluation27. We also delved into the application of deep learning in solving partial differential equations, becoming familiar with information on Physics-Informed Neural Networks (PINNs)28. We learned that the research adopted the RAR method to improve the training efficiency of PINNs and the Python library DeepXDE to support applications in complex geometric domains. Additionally, deep learning calibration for option pricing models29highlight the shortcomings of existing methods and propose improvements. Moreover, we have learned about the individual difference measurement methods for closed-demand30, and finally, the concept of rapid mixed-dimensionality reduction methods for feature selection and extraction is provided in31, which effectively eliminate redundant information.

In view of the various analytical methods discussed above, this research designed a concept based on feature selection and grouped analysis methods. The refined design process for extracting confused categories obtained through the bootstrapping process is illustrated in Fig.3.

Bootstrapping resamples data to evaluate stability, combined with grouped analysis to cluster similar features (e.g., edges, textures), enhancing confusion category extraction.

In this study, we employed bootstrapping and grouped analysis to refine the extraction of confusion categories from the Platycerium dataset. Bootstrapping was used to resample the dataset iteratively, generating multiple subsets to assess the stability and robustness of the model’s performance across the 18 highly similar species. For each iteration, we evaluated and filtered feature combinations—such as edges, textures, and shapes—that contributed most to classification errors, as identified by the multidimensional confusion matrix. Subsequently, grouped analysis clustered these features into similarity-based groups, enabling us to pinpoint specific confusion factors (e.g., overlapping leaf textures or edge patterns) that hindered accurate recognition. This iterative process was repeated 10 times, balancing computational efficiency with comprehensive analysis of all categories. By resampling and clustering in this manner, we minimized redundant training efforts typically associated with bootstrapping alone, while enhancing the model’s ability to distinguish subtle differences. Furthermore, we observed that the effectiveness of this approach scaled with dataset size: larger datasets amplified the feedback from grouped analysis, leading to more precise identification of confusion factors.

As shown in Fig.4, through the comparison between feature selection, grouped analysis methods, and bootstrapping, we found that the process using bootstrapping, combined with feature selection and grouped analysis, is more efficient when handling datasets with balanced and sufficient amount of samples. We thus propose a refined process for extracting confused categories. Initially, using the data results produced by the multidimensional confusion matrix and applying feature selection and grouped analysis, we enhanced the precise identification and classification of highly similar plant species. Finally, using image data statistical reports for visualization and analysis, in-depth observation and analysis of various categories of images are conducted during the refined extraction process. During the observation of images, we noticed plant species with apparent similarities showing only about 20% similarity in data results. Therefore, 20% is served as a benchmark for similarity assessment to identify factors causing high confusion rates. This study ultimately categorizes seven types of similar plant species, extracting key factors causing high confusion rates, such as image edges, textures, and shapes. This classification not only relies on physical features but also considers the performance and interrelations of categories within the dataset, providing a crucial methodology to enhance the overall efficiency of the image recognition system.

Algorithm flow for confusion class extraction refinement.

Synthesizing the analyses above, this study not only enhances the efficiency of data processing, but also effectively reveals the key factors impacting the model’s recognition capabilities through precise bootstrapping processes and feature selection grouped analysis. This holds significant practical significance for understanding and improving the performance of classification models.

Building on deep learning insights, we applied OpenCV to preprocess images, enhancing features like edges and textures identified as confusion factors. To address concerns about overfitting to superficial visual cues, we incorporated regularization and validation techniques to ensure the model learns semantically meaningful features. Specifically, we employed dropout (rate of 0.3) and L2 regularization (coefficient of 0.01) in the fully connected layers of EfficientNet-b4 and EfficientNet-b7, reducing reliance on enhanced features. Additionally, Grad-CAM visualization confirms that the model prioritizes biologically relevant features (e.g., leaf shapes and sporophore patterns) over superficial enhancements, validating its robustness32.

Additionally, this research provides a brief introduction to the fundamentals of OpenCV, particularly tailored for programmers at all levels, especially those focusing on image processing and computer vision research. This introduction not only offers a basic understanding of OpenCV’s extensive functionalities but also highlights its flexibility and practicality in various application scenarios33.

Furthermore, this study delves into specific examples and steps of OpenCV in real-time image processing applications, including an in-depth analysis of its application in this field, covering its practicality and efficiency in solving real-time problems. This discussion is crucial for understanding the central role of OpenCV in modern image processing technologies34.

The primary objective of this research is to explore the impact of high-confusion factor enhancement on the recognition efficiency of deep learning network models. We initially trained the ResNet50-v2 model to assess enhancement effects, optimizing hyperparameters (batch size, learning rate, iteration number) and using callback functions to dynamically adjust the learning rate, preventing overfitting as detailed in Section “Construction and optimization of network model architecture”18,19.

wheref(x,y) represents the pixel value of the original image at coordinates (x,y), andk(m,n) indicates the offset of the convolution kernel at coordinates (m,n) relative to the kernel center. This specific sharpening kernel is used to enhance the image’s edges, making it appear clearer and improving the contrast between a pixel and its surrounding neighborhood.

respectively, whereLrepresents the original lightness value andL′ is the adjusted lightness value; “lightness” refers to the percentage adjustment of brightness; Similarly,Sdenotes the original saturation value, andS′ is the adjusted saturation value; “saturation” indicates the percentage adjustment of saturation. If the computed new lightness or saturation values exceed 1, they are capped at 1 to ensure values remain within a reasonable range. This is because the values in the HLS color space’s L and S channels are confined to the [0,1] range, representing brightness and saturation percentages.

Normalization was also employed. The objective is to linearly normalize the image data to the specified range. The computation is provide as following.

where γ is the gamma value, which is set to 0.4 throughout. Gamma correction is a nonlinear operation used to adjust image contrast, particularly enhancing the visibility of dark details. When γ < 1, the dark details of the image are enhanced, and when γ > 1, the bright details are enhanced. In our experiments, by setting γ to 0.4, we aim to enhance the dark details of images.

To evaluate these techniques, we conducted experiments using EfficientNet-b4 and EfficientNet-b7, incorporating regularization and expanded data augmentation (Section “Data augmentation”) to mitigate overfitting. Table4compares model performance, with EfficientNet-b7 achieving the highest accuracy (98.46%), precision, recall, and F1 score, outperforming ResNet50-v2 due to its compound scaling16. Grad-CAM visualizations35as show in Fig.5confirm that EfficientNet-b7 focuses on biologically relevant features, reducing the risk of overfitting to superficial cues like enhanced edges or features. Heatmaps showing that EfficientNet-b7 focuses on biologically relevant features like leaf shapes and sporophore patterns, confirming that the model learns semantically meaningful features rather than superficial enhancements.

Performance comparison between ResNet50-v2 and EfficientNet.

Grad-CAM visualization when adopting EfficientNet-b7.

Table4compares model performance, with EfficientNet-b7 achieving the highest accuracy, precision, recall, and F1 score, outperforming ResNet50-v2 due to its compound scaling16. In this experiment, we divided the dataset into images processed for high confusion factor enhancement and unprocessed original images. OpenCV was used for custom enhancement settings, improving recognition of difficult features. The combination of regularization, hyperparameter optimization, callback functions, and expanded data augmentation significantly improved recognition efficiency, with EfficientNet-b7 achieving over 98% accuracy. These results validate the effectiveness of the refined process for extracting confused categories and high-confusion factor enhancements, with EfficientNet-b7 outperforming other models due to its optimized feature extraction for highly similar datasets.

To further illustrate the effectiveness of the proposed methods, Table5presents six figures that visualize the training dynamics of the network models, ResNet50-v2, EfficientNet-b4, and EfficientNet-b7. These plots clearly demonstrate that, as the number of epochs increases, both the accuracy and loss metrics stabilize, indicating convergence of the training process. The steady rise in accuracy and the consistent decline in loss, with minimal fluctuations in the later epochs, provide robust evidence that the training is complete and has reached a converged state. This convergence validates the reliability of the network architectures and the optimization strategies employed, including data augmentation, hyperparameter tuning, and high confusion factor enhancement, in effectively learning the features of the highly similar Platycerium dataset.

Accuracy and loss for training and validation for ResNet50-v2, EfficientNet-b4, and EfficientNet-b7.

Furthermore, Fig.6illustrates the traditional image recognition model architecture alongside the refined process design for extracting confused categories and the high confusion factor enhancement treatment proposed in this study. By examining the performance differences of the ResNet50-v2 network model, it is evident from the figure that after applying the refined process design for extracting confused categories and high confusion factor enhancement treatment, there is a significant improvement in the model’s recognition rate on the test set, and a substantial reduction in test set loss values. Additionally, the experiments were conducted on an NVIDIA RTX 3080 GPU with 48 GB of memory. Training ResNet50-v2 took approximately 4 h per run, while EfficientNet-b7 required around 6 h due to its larger architecture. Memory consumption peaked at 40 GB for EfficientNet-b7.

Performance for high obfuscation factor enhancement.

In summary, this study makes a significant contribution to enhancing image recognition performance by combining the refined process of extracting confused categories with high confusion factor enhancement techniques, along with various convolutional network models. This provides valuable guidance and reference for future research in related fields.

Although transformer-based models such as DaViT36and Swin-B Transformer37have demonstrated state-of-the-art performance in various large-scale image recognition tasks, recent studies38–40suggest that convolutional neural networks (CNNs) remain highly competitive—and often superior—in scenarios involving small or medium-sized datasets with high intra-class similarity. In our experiments, EfficientNet-b7, a CNN with compound scaling, outperformed both DaViT and Swin-B on the Platycerium dataset, achieving 98.46% accuracy compared to 64.1% and 78.04% respectively, as shown in Table6, where both Transformer-based and CNN-based networks were trained under identical experimental settings, including regularization, hyperparameter optimization, callback mechanisms, and comprehensive data augmentation to ensure a fair and consistent performance comparison.

Performance comparison between transformer-based and CNN-based networks.

This observation is consistent with prior research which demonstrates that CNNs better capture local visual patterns such as edges and textures, which are crucial for distinguishing between species with minor morphological differences9,38. Besides, transformers tend to require large datasets and substantial pretraining to generalize effectively41. Additionally, fine-grained classification tasks, such as plant species or medical image recognition, CNN-based architectures often yield more stable performance with lower risk of overfitting, especially in the absence of extensive pretraining40,42.

Therefore, in high-similarity datasets where local texture, contour, and shape features dominate, CNN-based models—especially those with enhanced feature selection processes like in this study—remain the preferred choice over transformer-based counterparts. These findings reaffirm the importance of task-specific model selection rather than defaulting to the most recent architectural trend.

In this study, we delved into optimizing image recognition within datasets with high similarity, exploring four experimental avenues. These directions not only showcase novel technical applications but also yield three significant contributions toward bolstering the overall efficiency of image recognition technology. Firstly, we devised network model architectures tailored to diverse sample sets. Secondly, we crafted refined processes for extracting obfuscation categories, along with custom settings for enhancing high obfuscation factors.

Our initial experiments training the ResNet-50 base network on such datasets revealed a recognition accuracy of less than 10%, underscoring the imperative for advancements in processing highly similar data. Despite employing optimization strategies such as hyperparameter tuning and data augmentation techniques, the recognition accuracy improved to 60%, underscoring persisting issues of confusion among closely resembling species.

Our main contribution focused on refining the process of obfuscation category extraction, integrating bootstrapping techniques with multidimensional confusion matrices and feature selection concepts. This culminated in a comprehensive methodology utilizing statistical analysis for data visualization, pinpointing factors driving high confusion rates and enhancing overall recognition efficiency.

In our performance enhancement pursuit, we leveraged OpenCV to amplify key factors identified in obfuscation category extraction, resulting in notable improvements in recognition rates. Notably, EfficientNet-b7 emerged as the superior model, outperforming others significantly across multiple assessments.

While transformer-based models offer promising performance in large-scale general-purpose datasets, our experiments and recent comparative studies confirm that convolutional models remain more effective in fine-grained classification tasks involving high inter-class similarity and smaller dataset sizes. In particular, EfficientNet-b7 not only surpassed DaViT and Swin-B in accuracy but also required less training time, reinforcing its suitability for plant species recognition with limited and visually similar samples.

These contributions not only elevate recognition performance but also unveil critical challenges in identifying highly similar datasets. They offer crucial directions and strategies for future image recognition technologies, particularly in tackling highly similar features, devising comprehensive optimization processes, and furnishing effective methodologies for subsequent research endeavors.

Below is the link to the electronic supplementary material.

This work was supported by the National Science and Technology Council, Taiwan, under contract NSTC 114-2221-E-035-004.