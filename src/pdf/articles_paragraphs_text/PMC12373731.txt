Extracting signals from noisy backgrounds is a fundamental problem in signal processing across a variety of domains. In this paper, we introduce Noisereduce, an algorithm for minimizing noise across a variety of domains, including speech, bioacoustics, neurophysiology, and seismology. Noisereduce uses spectral gating to estimate a frequency-domain mask that effectively separates signals from noise. It is fast, lightweight, requires no training data, and handles both stationary and non-stationary noise, making it both a versatile tool and a convenient baseline for comparison with domain-specific applications. We provide a detailed overview of Noisereduce and evaluate its performance on a variety of time-domain signals.

Natural signals such as speech, electrophysiology, and bioacoustics are challenging to record in isolation. Sensors, both biological and artificial, tend to record these signals in the context of noisy environments. To record from a singing songbird in its natural environment, for example, a microphone will pick up not only the bird but the richness of its sensory environment—a babbling brook, chirping crickets, wind passing through leaves, and the croaks of a nearby frog. Such ’noise’ can both provide important context for the signal of interest and important confounds. For example, a classifier trained to predict bird species from song recordings can be biased by environmental context; a babbling brook in the background might cause a Wood Thresh to be classified as a Robin. That error would in turn lead to downstream inaccuracies in estimating the migratory patterns of both birds. These same technical challenges arise in a variety of domains, from detecting action potentials to distinguishing seismic events from human activity.

Determining what constitutes noise versus signal is highly context-dependent. Consider two researchers: one focusing on the croaking of the American Bullfrog and the other analyzing the song of the Wood Thrush. They might approach the same audio recording yet define signal and noise in vastly different ways. Fortunately for these hypothetical researchers, the vocalizations of Bullfrogs and Wood Thrushes can be relatively easily distinguished from each other. Bullfrogs produce sounds in a frequency range of approximately 200-2000 Hz43, whereas Wood Thrushes vocalize in a higher spectrum, roughly 2000-9000 Hz22. Therefore, by applying a simple low-pass or high-pass filter, each researcher can effectively isolate the vocalizations of their respective species with minimal effort.

Signal and noise events that overlap spectro-temporally pose a greater challenge but are not insurmountable. If the signal and noise retain identifiable structures, we can devise algorithms to exploit these structures and eliminate unwanted noise. For instance, the persistent 60-Hz hum from nearby electronics in a poorly grounded electrophysiology implant exhibits temporal structure. This constant hum can be identified and algorithmically removed from the signal. Noise reduction algorithms harness these structural differences to distinguish and separate noise from the signal.

The challenge of separating signal from noise exists across many domains. While much focus in recent years has been on machine-learning based noise reduction algorithms, these algorithms are generally domain-specific; machine learning generally relies on large, often labeled, datasets that do not exist in all domains. These algorithms have been exhaustively reviewed for various signal domains in prior literature44,58,64. For domain general applications, out of the purview of any domain-specific machine learning models conventional approaches to noise reduction remain valuable58.

Here, we survey the utility of our algorithm, Noisereduce, a fast, domain-general, spectral-subtraction-based algorithm available in Python. Noisereduce has already been available open-source for over five years and has found utility in a variety of different domains including bioacoustics17,41,42,45,48,51, brain-machine interfacing11,25,26, livestock welfare monitoring4,24, human emotion analysis29,40, medical and clinical diagnostics27,32,39,55,68, seismic monitoring38, and many other domains. Until now, its performance has not been rigorously validated. Here, we address this by validating Noisereduce on several time-domain signals and comparing it to other conventional algorithms. Our findings show Noisereduce is fast and performs well, making it a strong candidate for domain-general applications where large datasets are unavailable and a solid baseline for comparing machine-learning-based algorithms.

Noisereduce belongs to a class of noise reduction algorithms that perform spectral-subtraction in the time-frequency domain5. Spectral subtraction algorithms subtract an estimate of the noise spectrum from a noisy signal in an attempt to improve the signal-to-noise ratio of the signal. The challenge in developing a spectral subtraction algorithm is in determining what constitutes noise and how that estimate of noise should be subtracted from the signal. For example, in Stephan Boll’s original paper on spectral subtraction5a Fast Fourier Transform (FFT) is taken on a on a noise-only portion of speech recording. a Short-Time Fourier Transformation (STFT) is the computed over the signal and the estimated noise magnitude is subtracted from each frequency component (clamping values above zero). In practice, this approach can leave behind unwanted noise artifacts and several variants exist to overcome these issues60. Spectral gating, the approach Noisereduce takes, is one of several variants of spectral subtraction. Spectral gates merge the concept of noise gating. Spectral gates are noise gates that act in the time-frequency domain, by masking specific time-frequency components to be subtracted away, while leaving other time-frequency components unaltered. This approach is commonly used in auditory scene analysis, where an auditory mixture is decomposed into time-frequency components, and an Ideal Binary Mask28is estimated to determine which components to attenuate (zeros) and which to pass through unaffected (ones). In practice masks are rarely binary- but are used to determine what proportion of the signal to attenuate. The success of spectral gating in noise reduction can be seen in its adoption in professional audio analysis software such as Adobe Audition (EffectsNoise Reduction/RestorationNoise Reduction in version 25.2), Audacity (EffectsNoise Reduction in version 3.7.3), and iZotope RX (Spectral De-noise* in version RX11). Noisereduce represents an open-source, lightweight, Python approach to spectral gating.

Noisereduce accepts two inputs: (1)X, the time-domain recording to be denoised and (2, optionally), a time-domain recording containing only noise, used to calculate noise statistics. Noisereduce operates through the following steps (Fig1).

If the noise recording is not provided to the algorithm, the noise statistics are computed on directly on the recording (). A more detailed description of the algorithm and its parameters are given in5.1.

Basic outline of Noisereduce algorithm. (A) A block diagram of the steps of Noisereduce. The stationary version of the time-frequency mask is depicted. (B) An example waveform (U.S. President George W Bush stating “I know that human beings and fish can coexist peacefully”) passing through the Noisereduce pipeline. The non-stationary algorithm is not shown here.

In natural settings, background noise often varies over extended periods. For example, in bioacoustics, weather can shift within minutes, while in electrophysiology, the activity rates of nearby neurons may increase as animals transition between states, such as sleeping and waking. Consequently, it is advantageous to enable Noisereduce to adapt its noise definition over time48. To address this, we introduced a non-stationary variant of Noisereduce, where mask statistics are calculated using a sliding window across the signal rather than relying solely on an isolated noise clip. This non-stationary approach is particularly beneficial for signals such as those from hydrophones in underwater bioacoustics, where the engine hum of a boat can fluctuate as the hydrophone drifts toward and away from the boat towing it. To decide whether non-stationary noise reduction is appropriate, one can test whether the signal is stationary either using formal testing6or by inspecting the signal manually for periods of fluctuating noise levels. Normalizing audio signals to channel-specific fluctuations in amplitude has proven useful for tasks like bioacoustic species identification35.

The non-stationary algorithm omits the need for a noise recording () since noise statistics are directly derived from the signal recording (). In this revised approach, statistics for the noise threshold are computed over a sliding window for each frequency channel. This approach dynamically sets noise gate thresholds for each frequency channel, as opposed to static settings across the entire recording. For additional details, see Section5.1.

Figure2illustrates the non-stationary algorithm’s utility. We took a one-minute recording of an American Robin (Macaulay Library 321642131; Fig2A) and added the non-stationary noise of an airplane passing overhead (Fig2B). We then applied both stationary and nonstationary Noisereduce (Fig2C-D). During the highest amplitude period of airplane noise, the stationary algorithm leaves additional noise artifacts in the recording, unlike the non-stationary version (Fig2E-G, Blue/Green, 20-30 seconds). Conversely, more of the signal is lost in sections with lower noise amplitude (e.g. 40-60 seconds, red/purple). We quantified this as the absolute error in dB, relative to the noise-free recording (Fig2H) exemplifting that the non-stationary algorithm performs consistently better with non-stationary noise in this case.

Comparison of stationary and non-stationary noise reduction. (A) Spectrogram of clean recording of an American Robin (Macaulay Library 321642131). (B) Airplane noise imposed over Robin Recording. (C-D) Denoising of (B) with (C) stationary noisereduce and (D) nonstationary noisereduce (window size of 2 seconds) (E-F) Magnitude error in stationary noisereduce vs ground truth for (E) stationary noisereduce and (F) nonstationary noisereduce. (G) Magnitude difference between stationary and nonstationary noisereduce. (H) Error (in dB) from ground truth for stationary (green) and nonstationary (purple) noisereduce.

To evaluate the performance of Noisereduce, we tested it on a set of benchmark datasets across four domains: speech, bioacoustics, electrophysiology, and seismology (see5.2). We compared its results against several noise reduction algorithms (see5.3). The evaluation metrics used in the comparison are detailed in5.5.

Speech is the best-established domain for enhancement and noise reduction33. Many speech noise reduction applications are well-suited to machine learning methods, especially deep neural networks like convolutional neural networks (CNNs)36,66, long short-term memory networks (LSTMs)12,19, and Generative Adversarial Network (GANs)18,46, which outperform any conventional algorithm. We therefore submit that Noisereduce in this domain for two purposes. First, as a candidate “conventional algorithm” baseline. Second, Noisereduce may remain useful for speech applications where machine-learning based approaches might not be well suited, such as out-of-domain speech signals, very lightweight applications where computational costs of machine-learning based approaches are too cumbersome, or in creating new datasets with varying manipulations on noise levels.

We evaluated Noisereduce against other noise reduction conventional algorithms on the NOIZEUS dataset21,34,65across various SNR levels (0, 5, 10, and 15 dB). Examples of speech spectrograms obtained with Noisereduce, Wiener30, Iterative Wiener30, Subspace16,20, Spectral Subtraction5and Savitzky-Golay52appear in Fig3. Particularly, Noisereduce preserves the speech signal without distortions, unlike other algorithms that add artifacts, particularly under low SNR. The performance metrics used were Short-Time Objective Intelligibility (STOI)57and Perceptual Evaluation of Speech Quality (PESQ)47, which assess speech intelligibility and quality, respectively. STOI and PESQ results are in Tables1and2, showing that Noisereduce outperforms other conventional algorithms at all tested SNR levels, for the hyperparameters we sampled (Table9).

Noise reduction samples from different algorithms applied to the ’sp04’ sample from the NOIZEUS dataset (SNR: 10 dB, exhibition noise).

STOI performance metric on NOIZEUS dataset (mean ± SEM) for different algorithms across various SNR levels.

PESQ performance metric on NOIZEUS dataset (mean ± SEM) for different algorithms across various SNR levels.

To further evaluate its performance, we compared Noisereduce against a state-of-the-art deep learning-based model, Denoiser12. While Denoiser had higher STOI and PESQ scores (Tables3and4), Noisereduce achieved competitive results with substantially lower computational overhead. Specifically, Denoiser requires over 33 million trainable parameters, whereas Noisereduce uses efficient signal processing techniques that require minimal computational resources and provide faster runtime (see Section2.5).

Comparison of STOI performance metric for Noisereduce and Denoiser across various SNR levels (mean ± SEM).

Comparison of PESQ performance metric for Noisereduce and Denoiser across various SNR levels (mean ± SEM).

Bioacoustic signals are recorded across Earth’s diverse bioregions, with conditions often unique to each dataset. Consequently, state-of-the-art machine-learning methods are rarely available64, making bioacoustics is an ideal domain for applying Noisereduce. To our knowledge, no benchmark dataset exists for bioacoustic noise reduction, unlike NOIZEUS34for speech. To fill this gap, we developed “NOIZEUS Birdsong”49, a benchmark dataset modeled after NOIZEUS’s methodology and structure. We sampled recordings from 14 European starlings, with five 40-second songs from each bird, all recorded in an acoustically isolated chamber2. To simulate realistic conditions, we added noise at four SNRs: 0, 5, 10, and 15 dB. Noise samples were taken from the “Soundscapes from around the world” dataset from Xeno Canto61. We selected eight distinct soundscape categories which we named: “rain”, “town”, “wind”, “waterfall”, “insects”, “swamp”, “frogscape”, and “forest”. Each soundscape contains various sources of noise and were sampled from the European Starling’s natural geographic range. The dataset exhibits diverse spectro-temporal noise characteristics, illustrated in5.2(Fig9).

Spectrograms of a sample from the “Birdsong NOIZEUS” dataset at an SNR of 10 dB, showcasing the clean signal, the noisy signals with different types of environmental noise.

We evaluated Noisereduce’s performance using the NOIZEUS Birdsong dataset and compared it with Savitzky-Golay and Wiener filtering, which are both domain-general noise reduction algorithms that performed well in the speech analysis. We measure improvements in Segmental Signal-to-Noise Ratio (SegSNR), which evaluates the quality of noise reduction across temporal segments, and Source-to-Distortion Ratio (SDR), which quantifies both signal degradation and residual noise. We find that Noisereduce outperforms the other conventional algorithms on both metrics (Tables5,6; Figure4).

SegSNR [dB] performance metric on Birdsong NOIZEUS dataset (mean ± SEM) for different algorithms across various SNR levels.

SDR [dB] performance metric on Birdsong NOIZEUS dataset (mean ± SEM) for different algorithms across various SNR levels.

Noise reduction samples from different algorithms applied to the ’B335’ sample from the NOIZEUS Birdsong dataset (SNR: 10 dB, waterfall noise).

Extracellular electrophysiology is a key tool in recording single-neuron activity as animals interact with their environment. A challenge here is detecting extracellular spikes and assigning them to individual neurons, a process known as spikesorting. Current algorithms tackle this in steps: initially detecting spikes by thresholding amplitude or convolving the signal with spike templates, then iteratively clustering these putative spikes to estimate neuron identities, which provide templates for further detection. We tested whether Noisereduce could enhance initial spike detection by improving the SNR between spikes and background noise.

We created a dataset of biophysically realistic neural recordings using the MEArec library9, simulating extracellular electrophysiology. Ground truth spikes were simulated from 10 neurons (8 excitatory, 2 inhibitory, Fig5A), with noise from 300 background neurons. Simulated data were used as real recordings lack ground truth.

Noisereduce results on a simulated extracellular recording. (A) Sample neuron waveform templates. (B) A sample of 100ms of z-scored sampled neural data, with the original data in red and the denoised signal in black. (C-D) A spectrogram of the same data in B. (E) Amplitude of action potentials (blue) versus background noise (grey) in the original signal versus the denoised signal. (F) Reciever Operator Characteristic (ROC) curve of spike detection using the SpikeInterface detect_peaks algorithm to detect spikes.

We applied a modified Noisereduce approach to this data (Fig5B), omitting the spectral mask smoothing step, which is computationally intensive and unnecessary for preliminary spike detection where spike shape is not used. We compared the output of Noisereduce to the untreated signal (bandpass filtered at 200-6000Hz; Fig5C-D). We found that the spike amplitude (z-scored; Fig5E) increased relative to background noise. To assess detection improvement, we used the SpikeInterface detection algorithm10and computed an ROC curve by varying the detection threshold. Noisereduce was compared against three conditions: baseline bandpass filtering, Wiener filtering, and Savitzky-Golay filtering (Fig5F). An Area Under the Curve (AUC) analysis found highest performance with Noisereduce (Noisereduce=0.97; Savitzky-Golay=0.96; Wiener = 0.94; Baseline=0.91), suggesting its suitability for initial spike detection. Given that spectral masking can alter spike shapes, we advise using Noisereduce solely for initial spike detection, not clustering.

Seismic event detection methods focus on identifying the onset of these events, a critical step for accurately locating and characterizing seismic activity1,15. A widely used approach is the Short-Time Average over Long-Time Average (STA/LTA) algorithm59,63, which calculates the ratio of short-term to long-term signal averages to detect events. However, background noise from the environment and equipment makes detection less reliable, resulting in missed detections and false alarms.

Following Zhu et al. (2019)67, we tested Noisereduce on seismic waveforms from the ObsPy library3(see Fig6, top). To simulate realistic conditions, we added white and pink noise at SNRs ranging from 0 to 15 dB and evaluated detection accuracy by comparing STA/LTA-detected onset times between denoised and clean recordings. As with the spike-detection analysis, we applied a modified Noisereduce, omitting the smoothing step. In detecting the onset time of seismic activity, Noisereduce outperformed three baseline methods — no filtering, Wiener, and Savitzky-Golay - across all SNR levels, with the most significant improvements in low-SNR conditions (0 and 5 dB) for both noise types (see Tables7and8).

(Top) Seismic recording sample “ev0_6.a01.gse2” from ObsPy dataset. The trigger, determined using the STA/LTA algorithm, marks the signal onset (red line). Noise added (pink, SNR = 1dB) and Noisereduce and DeepDenoiser are compared. (Bottom) Performance metrics for the seismology dataset. DeepDenoiser comparisons were generated using the DeepDenoiser API at “https://ai4eps-deepdenoiser.hf.space”.

Onset detection error (mean ± SEM) for different algorithms across various SNR levels for white noise.

Onset detection error (mean ± SEM) for different algorithms across various SNR levels for pink noise.

To further assess Noisereduce’s quality in detecting seismic signals, we compared it to DeepDenoiser67, a deep neural network-based approach for denoising seismic waveforms. We compared the SNR of the signal post-denoising, the correlation coefficient between clean and denoised signals, and the change in maximum amplitude of the signal from the clean recording. While Noisereduce underperforms compared to the deep learning approach on all metrics, its performance is closer to the deep learning model than any of the other conventional algorithms, with the exception of the amplitude of the denoised signal, which is more greatly decreased in Noisereduce (Fig.6, bottom).

Speed is a critical factor in selecting a noise reduction method, especially in applications requiring real-time or near-real-time analysis. Noise reduction algorithms are of limited use if they cannot process signals in a timely manner, as delays can become bottlenecks in analytical workflows. Noisereduce supports GPU parallelization, which significantly improves processing speed. To evaluate performance, we measured the average runtime across various signal lengths using an NVIDIA GeForce RTX 3070 GPU. The results (Fig7) demonstrate that GPU-accelerated Noisereduce outperforms other noise reduction algorithms, highlighting its potential for real-time applications.

Runtime analysis comparing GPU-based Noisereduce, CPU-based Noisereduce, GPU-based Denoiser, CPU-based Denoiser, Wiener filter, and Savitzky-Golay filter on an RTX 3070 GPU with batch size of 32, and sample rate of 16 kHz.

Noisereduce relies on a small number of hyperparameters which impact how noise is detected and attenuated (Table9).

The main two parameters to consider are n_std_thresh_stationary and prop_decrease. n_std_thresh_stationary sets the threshold for what to consider signal in terms of standard deviations of power above (or below) the mean power for each frequency channel. prop_decrease then determines the extent to which we remove the below-threshold noise. We additionally include noise_window_size_nonstationary_ms in the nonstationary version of the algorithm, which is the window over which threshold statistics are computed. freq_mask_smooth_hz and time_mask_smooth_ms, are used to smooth the mask using a Gaussian kernel, with the shape of the kernel defined by those parameters. A further implicit parameter is the duration of noise clip presented to the algorithm. For example, a very short noise clip may not accurately reflect the statistics of the noise profile in the full recording. Finally, n_fft, win_length, and hop_length are all parameters used to compute the spectrogram and should be set at values that would visibly capture spectrotemporal structure in your signal, if you were to plot the spectrogram.

We performed an analysis of the robustness of this parameter selection on the Birdsong NOIZEUS dataset (Fig.8). Although the optimal parameters will be both dataset and downstream application specific, the analysis given here may provide some intuition for Noisereduce users. Broadly, we observe that a good range of choices for n_std_thresh_stationary is an intermediate range, between 1 and 5. prop_decrease generally improves in performance even at 1.0. Smooth mask is more variable on the metrics we analyzed (SDR and SegSNR). Finally, as we increase the duration of the noise clip thus performing a better estimate over noise, noise reduction improves (here maximum noise clip duration was the dataset maximum of 1 second). However, these metrics are an imperfect proxy for both perceptual quality and value in downstream tasks. To aid in intuiting the value of these parameters, we include supplementary audio clips of each of the sames in Fig.8.

Visualization of parameters varied on the Birdsong NOIZEUS dataset (forest noise, SNR=0). (A) n_std_thresh (B) prop_decrease (C) freq_mask_smooth_hz and time_mask_smooth_ms (D) Noise clip length. (E) SDR values over a range of parameters for the Birdsong NOIZEUS dataset.

In this work we provide a validation for Noisereduce as a domain-general noise-reduction algorithm. Our findings demonstrate that Noisereduce can perform similarly to and often outperform traditional noise reduction algorithms, making it suitable for a number of applications such as in bioacoustics and electrophysiology. Additionally, it can be used as a baseline comparison in domains where extensive domain-general machine-learning based approaches already exist. An important advantage of Noisereduce is its support for GPU parallelization, which accelerates processing speed compared to CPU-only algorithms. The algorithm does not rely on training data, and its lightweight design makes it suitable for real-time use or resource-limited settings where deep learning may not be practical. Noisereduce is publicly available as a Python package, actively maintained, and easy to use.

Limitations to the Noisereduce algorithmWe do not recommend Noisereduce for all applications. In many domains, including speech, domain-specific and often supervised approaches exist that will generally outperform Noisereduce. For example, the Denoiser algorithm we presented above. In other applications, we find that Noisereduce is a valuable starting place to improve signal to noise ratio. Even in domains where substantial domain-specific efforts exist, Noisereduce can remain a valuable tool. For example, Noisereduce can be used as an augmentation tool in creating training datasets, and it can be used in applications where a high-throughput low-latency approach is needed. Noisereduce is also subject to the challenges of spectral masking. When time-frequency components contain both signal and noise, a binary masking approach will not optimally separate the signal from noise. Noisereduce also operates by assuming that the highest amplitude components of the recording are signal, which is not always the case. Noisereduce also works best when noise is either stationary or nonstationary over timescales that are longer than the signal; when noise is intermittent over short timescales, particularly when the amplitude and frequency of the noise is similar, Noisereduce will not be able to differentiate signal from noise. In all cases, we recommend users carefully analyze the outputs of Noisereduce before blindly using it in an analysis pipeline.

Limitations in comparisonsThe work presented here attempts to benchmark Noisereduce against a set of comparison algorithms on several noise-reduction quality metrics. However, these comparisons are neither complete not exhaustive. Each of the algorithms we presented here have applications which they are good at, and applications in which they fail. In the analyses we provided here, we tried to produce a good-faith attempt to parameterize each algorithm in such a way that it would perform well. However, the hyperparameters chosen for both Noisereduce and its comparisons (Table9) were neither exhaustively scanned, nor were they systematically optimized. There are also several other domain-general noise reduction approaches which have not been compared here, for example wavelet-based approaches13,14and Empirical Mode Decomposition7,8. We therefore ask that readers interpret these results as we have, i.e. that noisereduce performs very well on the metrics we have provided, and at least consistent with other approaches. It is also true that the ’metrics’ that we chose to make comparisons are themselves only a very rough proxy of what is wanted out of a noise reduction algorithm, which differ depending on application. There are no algorithms which perfectly reflect human perceptual judgement in any domain. Even if there were, noise reduction algorithms that optimized for human perception would not necessarily be optimized for the many possible downstream tasks that one might perform on the denoised signal.

where indicesiandjdenote time frames and frequency bins, respectively.

We can then create the mask for the signal. To do this, we need to compute the STFT () of the signal clip ().

The binary mask () is then computed on the signal spectrogram (), based on the thresholds for each frequency bin.

To reduce artifacts from sharp transitions,can optionally be smoothed using a 2-D filter, characterized byand, which define the half-width of the filter in frequency and time, respectively.

The expressions are defined forand, effectively creating symmetric triangular windows. The normalization constantis determined such that the sum of all elements in the 2-D filter satisfies.

If smoothing is applied, the smoothed maskis obtained by convolving the original maskwith the smoothing filter.

We can then apply the mask to the STFT of the signal () by multiplyingorwithto produce the masked STFT ().

whereis a scaling factor that controls the strength of the masking effect.

The masked STFT () is then inverted back into the time-domainusing an inverse STFT.

Non-stationaryThe non-stationary algorithm differs from the stationary version of Noisereduce in how the noise mask is computed. The central goal of the non-stationary algorithm is to compute a noise mask locally in time rather than globally across the entire recording or dataset, to account for fluctuations in the noise floor. To accomplish this, we simply compute the mean and standard deviation of the frequency components over a sliding window onXwithout a noise clip, and then proceed with the rest of the algorithm normally.

Soft MaskWhile the current implementation uses a binary mask (0 or 1), future work could explore a soft mask with values between 0 and 1 to achieve smoother signal-noise separation.

SpeechThe evaluation included thirty phonetically balanced speech utterances from the “Noisy Speech Corpus” (NOIZEUS) database21,34,65, a database specifically designed for noisy speech research. These utterances were combined with eight distinct real-world noise types, including suburban train, babble, car, exhibition hall, restaurant, street, airport, and train station noises, at SNRs of 0 dB, 5 dB, 10 dB, and 15 dB, following Method B of the ITU-T P.56 standard23.

BirdsongWe created the Birdsong NOIZEUS dataset49in the likeness of the speech NOIZEUS dataset34. We selected 70 song samples from 14 European starlings (5 samples of 40 seconds each). These recordings were selected from a larger collection previously gathered by the authors for prior publications2,50. The original dataset contains several hundred 30 to 60 second recordings per bird, obtained from wild-caught European starlings in Southern California. Recordings were performed in acoustically isolated chambers to ensure high-quality audio capture. We sampled noise from the “Soundscapes from around the world” dataset from Xeno Canto61. We hand selected 8 soundscapes from this dataset which we named “rain”, “town”, “wind”, “waterfall”, “insects”, “swamp”, “frogscape”, and “forest”. Each soundscape contains various sources of noise and were sampled from the European Starling’s natural range. For each song, we selected a different segment of the soundscape (soundscapes were around 5-20 minutes each). An example of the dataset can be seen in Figue9. We set the SNR based on loudness measured using the pyloudnorm Python library56. Additionally, for each song and noise clip we included a 1-second clip of noise sampled randomly (at the same SNR of the audioclip). This dataset is publicly available on Zenodo (DOI: 10.5281/zenodo.13947444).

SeismologyThe seismic data used in this study was obtained from the ObsPy Trigger/Picker Tutorial3, including waveform recordings from three seismic stations: EV, RJOB, and MANZ. The dataset, recorded in January 1970, contains natural seismic events specifically selected for evaluating triggering and picking algorithms. For our analysis, we used all available signals from the trigger dataset, excluding those with low SNR. To simulate realistic seismic and instrumental noise, we added both white noise and pink noise at varying SNR ratios (0, 5, 10, and 15 dB).

ElectrophysiologyElectrophysiology datasets were generated using the MEArec9Python library so that we would have access to ground truth spiking events alongside electrophysiology. Some non-simulated ephys datasets record ground truth events, e.g. by pairing extracellular recordings with intracellular recordings37, but, since not all cells are recorded intracellularly, they are of limited value in differentiating between false positive and true positive detections of other cells. facilitates the generation of customizable extracellular spiking activity datasets by leveraging biophysically detailed simulations. It achieves this by first creating templates of extracellular action potentials using realistic cell models, positioned around electrode probes within a simulation environment. These cell models, drawn from established neuroscience databases, undergo intracellular simulation to compute transmembrane currents using tools like NEURON, while the extracellular potentials are calculated using methods such as the line-source approximation via the LFPy package. This process allows MEArec to accurately simulate various neural dynamics and probe configurations, offering a flexible framework for evaluating and developing spike sorting methods under controlled experimental conditions.

We generated a dataset comprising a monotrode (single channel) recording 10 minutes in length. The recording had 10 neurons (8 excitatory and 2 inhibitory). Spikes ranged in amplitude from 75-150uV. Background noise was generated using 300 simulated neurons that were further away from the probe (each with a maximum amplitude of 75uV). Simulated data were bandpass filtered between 300 and 6000 Hz.

Wiener FilterThe Wiener filter, as implemented in31, is an adaptive noise reduction algorithm that analyze local statistics within a sliding window. The filter adapts to local signal characteristics by weighting the difference between the noisy observation and the local mean based on the local variance. The filter applies minimal smoothing in high-variance regions to preserve significant signal features, while employing more aggressive smoothing in low-variance areas presumed to be noise-dominated.

wherey[n] is the observed noisy signal,is the local mean within a window centered aroundn,is the variance of the signal in that window, andis the estimated noise variance calculated as the average of all local variances across the signal.

We used the SciPy implementation62as a comparison.

whereis the speech power spectral density, andis the noise variance.

To determine if a frame contains speech, a simple energy threshold is used. When speech is detected, the algorithm refines the clean signal estimate by iteratively calculating Linear Predictive Coding (LPC) coefficients of the input frame, which model the vocal tract as an all-pole filter. These LPC coefficients help estimate the speech power spectrum and update the Wiener filter to reduce noise. After denoising, new LPC coefficients are calculated from the denoised signal, further improving the filter.

whereis the smoothing factor, andis the energy of the input frame.

We used the pyroomacoustics library53as a comparison.

whererepresents the window size,are the input data points within the window, andare the convolution coefficients derived by the polynomial fitting.

We used the SciPy implementation62as a comparison.

whereis the Fourier transform of the noisy signal, andis the estimated noise spectrum. Thefunction ensures that the resulting magnitude is non-negative. The noise spectrumis obtained during periods of silence in the signal, under the assumption that only noise is present.

To reconstruct the clean signal, the inverse Fourier transform is applied towhile using the phase information from the noisy signal.

We used the pyroomacoustics library53as a comparison.

SubspaceThe Subspace algorithm16,20performs noise reduction by projecting the noisy signaly[n] onto a lower-dimensional subspace that primarily contains the clean signal, while the noise is assumed to be in the complementary subspace.

whereis the noise covariance matrix,is the covariance matrix of the input noisy signal, andIis the identity matrix.is obtained during periods of silence in the signal, under the assumption that only noise is present.

whereis the projection matrix, derived from the positive eigenvectors of.

We used the pyroomacoustics library53as a comparison.

DeepDenoiserThe DeepDenoiser67is a deep neural network designed to denoise seismic signals by learning to separate the signal from noise. It is trained on datasets containing both noisy and clean waveform data. During the denoising process, the input seismic signal is first converted into the time-frequency domain. The network then uses a series of fully convolutional layers with skip connections to generate two masks: one for the signal and one for the noise. Finally, the denoised signal and the estimated noise are obtained by applying the inverse Short Time Fourier Transform.

DenoiserDenoiser12is a deep learning model designed to denoise speech signals. It processes raw audio waveforms using an encoder-decoder architecture with skip connections. The model is optimized across both time and frequency domains through multiple loss functions and is trained end-to-end on paired datasets of noisy and clean speech.

We used the hyperparameters listed in Table10for comparison.

STOIShort-Time Objective Intelligibility (STOI)57is a widely used objective metric for assessing speech intelligibility, particularly in noisy environments. It functions by comparing short-time segments of clean reference and degraded speech signals, quantifying the level of degradation in terms of intelligibility. STOI calculates the correlation between the two signals over overlapping time windows, producing a score between 0 and 1. Higher scores indicate better intelligibility.

PESQPerceptual Evaluation of Speech Quality (PESQ)47is another objective metric designed to evaluate speech quality as perceived by human listeners. It incorporates a psychoacoustic model to simulate the human auditory system, comparing degraded or processed speech to a clean reference. PESQ captures both time-domain distortions and perceptual differences, generating a score between −0.5 and 4.5. Higher scores signify better perceived speech quality. Unlike STOI, which focuses on intelligibility, PESQ is more concerned with overall speech quality.

wherexis the true clean signal, andis the estimated signal.

whereNis the total number of segments, andSNR(n) is the SNR for the n-th segment.

AUCThe Receiver Operating Characteristic (ROC) curve is used to evaluate the performance of binary classifiers. The ROC curve plots true positive rate against false positive rate for various classification thresholds. The Area Under the Curve (AUC) quantifies the overall performance, ranging from 0.5 (random guessing) to 1.0 (perfect classification).

We thank David Burshtein for their feedback on an earlier version of this manuscript. Published by a grant from the Wetmore Colles fund.