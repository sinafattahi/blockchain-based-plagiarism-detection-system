Background: Patients increasingly turn to large language models (LLMs) and social media platforms for medical advice. The accuracy of these sources, particularly compared to peer-reviewed clinical practice guidelines, remains poorly characterized.

Materials and methods: This cross-sectional study evaluated the perceived accuracy of spine-related medical advice generated by ChatGPT (ChatGPT (OpenAI, powered by GPT-4, San Francisco, CA, USA), TikTok (Los Angeles, CA, USA), and the North American Spine Society (NASS) clinical practice guidelines. Medical advice for four spine pathologies was collected from each source. Sixteen orthopedic surgeons rated the accuracy of excerpted recommendations on a 10-point Likert scale. Descriptive statistics summarized mean ratings and standard deviations.

Results: For lumbar stenosis, mean (±SD) accuracy scores were 7.75 ± 2.11 for ChatGPT, 7.00 ± 1.80 for NASS, and 2.50 ± 1.54 for TikTok. For lumbar spondylolisthesis, scores were 7.56 ± 1.50 for ChatGPT, 5.94 ± 2.63 for NASS, and 5.31 ± 2.49 for TikTok. For lumbar disc herniation with radiculopathy, scores were 7.25 ± 2.13 on ChatGPT, 7.06 ± 1.55 on NASS, and 6.44 ± 2.03 on TikTok. For cervical radiculopathy, scores were 7.13 ± 1.38 for ChatGPT, 4.00 ± 2.44 for NASS, and 6.50 ± 2.12 for TikTok.

Conclusions: ChatGPT-generated outputs received the highest ratings for perceived accuracy. NASS guidelines, while evidence-based and peer-reviewed, remain inaccessible to most patients. Professional societies may consider adapting guideline content for dissemination via widely used digital platforms to improve public education and reduce misinformation.

The public increasingly queries large language models (LLMs) and social media platforms for medical advice, yet the accuracy and clinical reliability of these sources remain poorly characterized [1,2]. Although professional society clinical practice guidelines, such as those from the North American Spine Society (NASS), offer comprehensive, evidence-based recommendations, these documents are often inaccessible or difficult to interpret for the general public. As a result, many patients, particularly younger individuals, seek medical information from non-peer-reviewed but easily accessible sources, including social media and LLMs.

TikTok, a video-sharing platform with over one billion active users, has become a significant source of health information, particularly among individuals aged 18 to 30. However, multiple studies have demonstrated that TikTok content frequently contains incomplete or inaccurate medical information, often provided by individuals without appropriate medical expertise [3-5]. Moreover, recent data suggest a direct, positive correlation between video virality and inaccuracy [6].

In parallel, LLMs like ChatGPT (OpenAI, powered by GPT-4, San Francisco, CA, USA) have become widely adopted for their ability to generate detailed and coherent responses to complex queries. Although ChatGPT has demonstrated utility in various domains, including healthcare, concerns remain regarding its potential to produce inaccurate or overly verbose content that may mislead patients without medical training, who may be unaware of and unable to assess the inaccuracy [7-9].

In light of these trends, the accuracy and clinical utility of digital platforms must be systematically assessed. The quality of spine-related medical information generated by ChatGPT and TikTok (Los Angeles, CA, USA) was evaluated and compared to NASS guidelines, which served as the reference standard. Active orthopaedic surgeons were surveyed to assess the perceived accuracy of information generated by these platforms. The study aimed to evaluate the feasibility of integrating these tools into patient education and to identify opportunities for professional societies to improve dissemination of guideline-based content through widely used digital platforms.

This was a cross-sectional study designed to evaluate the perceived accuracy of spine-related medical advice generated by ChatGPT, TikTok, and NASS guidelines.

Medical advice on four common spine pathologies, such as cervical radiculopathy, lumbar disc herniation, lumbar spondylolisthesis, and lumbar stenosis, was collected for analysis. Prompts were submitted to ChatGPT using the query format “Give me advice on (condition),” with a new chat opened for each prompt to prevent prior responses from influencing new ones. The responses generated by ChatGPT are included in the appendix. Similarly, TikTok was queried using the phrase “Advice on (condition),” and the first three relevant videos for each condition were reviewed and transcribed. The URLs for each TikTok video are also provided in the appendix. Recommendations for each diagnosis were excerpted from the corresponding NASS guidelines [10-13].

Excerpted advice from each source was incorporated into an online survey form. Each participant was blinded to the source of the information. The survey was distributed electronically to 16 orthopedic surgeons. Participants were asked to rate the clinical accuracy of each treatment recommendation using a 10-point Likert scale, where 1 represented entirely inaccurate advice and 10 represented entirely accurate advice.

Descriptive statistics were calculated for each condition and information source. An overall comparison between platforms was performed using one-way analysis of variance (ANOVA). Post-hoc pairwise comparisons were conducted using Tukey’s honestly significant difference (HSD) test to evaluate pairwise differences in perceived accuracy between platforms. A p-value < 0.05 was considered statistically significant for all comparisons. All analyses were performed using Python (SciPy and statsmodels packages, Wilmington, DE, USA).

A total of 16 orthopedic surgeons (aged 26-69 years) responded. Ratings were assigned using a 10-point Likert scale, where higher scores indicated greater perceived accuracy.

For lumbar stenosis, the mean (±SD) accuracy scores were 7.75 ± 2.11 for ChatGPT, 7.00 ± 1.80 for NASS guidelines, and 2.50 ± 1.54 for TikTok. For lumbar spondylolisthesis, mean (±SD) scores were 7.56 ± 1.50 for ChatGPT, 5.94 ± 2.63 for NASS guidelines, and 5.31 ± 2.49 for TikTok. For lumbar disc herniation with radiculopathy, mean (±SD) scores were 7.25 ± 2.13 for ChatGPT, 7.06 ± 1.55 for NASS guidelines, and 6.44 ± 2.03 for TikTok. For cervical radiculopathy, mean (±SD) scores were 7.13 ± 1.38 for ChatGPT, 4.00 ± 2.44 for NASS guidelines, and 6.50 ± 2.12 for TikTok. These scores are presented in Table1.

An overall comparison across all platforms and conditions was performed using ANOVA. The ANOVA demonstrated a statistically significant difference in accuracy ratings between the three platforms (F = 13.21, p < 0.00001). Post-hoc pairwise analysis using the Tukey HSD test revealed that ChatGPT was rated significantly higher than TikTok (mean difference 2.41, p < 0.001), and NASS guidelines was rated significantly higher than TikTok (mean difference 1.28, p = 0.018). There was no statistically significant difference between ChatGPT and NASS guidelines (mean difference 1.13, p = 0.087). This analysis is presented in Table2.

In this study, the perceived accuracy of spine-related medical advice generated by ChatGPT, TikTok, and NASS guidelines was evaluated by orthopedic surgeons. ChatGPT consistently received the highest perceived accuracy ratings, followed by NASS guidelines and TikTok content. Although the NASS guidelines represent evidence-based recommendations developed through peer review and expert consensus, ChatGPT-generated outputs were often rated comparably or more favorably than those from other sources. Ratings for TikTok content demonstrated substantial variability.

Statistical testing confirmed significant differences in perceived accuracy between platforms. ANOVA demonstrated overall differences between sources, and post-hoc testing identified significantly higher accuracy ratings for ChatGPT and NASS guidelines compared to TikTok, with no significant difference between ChatGPT and NASS guidelines. These findings suggest that, for selected clinical topics, ChatGPT may produce advice perceived by physicians as approaching or matching guideline-based accuracy.

The consistency of ratings across conditions also differed between platforms. ChatGPT demonstrated relatively narrow standard deviations, indicating consistent performance across a wide range of queries. In contrast, TikTok exhibited wide variability, likely reflecting its unregulated nature, heterogeneous contributors, and lack of editorial oversight. NASS guidelines demonstrated intermediate variability, potentially reflecting challenges associated with excerpting guideline recommendations for simplified survey evaluation. While NASS guidelines are rigorously developed, they are not optimized for direct patient consumption, which may affect perceived clarity and accuracy when simplified.

These findings are consistent with prior evaluations of medical content quality on digital platforms. Prior studies have demonstrated that popular health information on TikTok often contains substantial inaccuracies, with greater viewership sometimes correlating with misinformation [6,14]. LLMs, such as ChatGPT, have shown potential clinical utility in prior studies; however, their outputs are generated probabilistically and not directly authored or reviewed by clinicians [7,15].

Several limitations warrant consideration. The sample size was limited to 16 orthopedic surgeons, and only four spine conditions were evaluated. TikTok sampling was restricted to the first three videos returned per condition. Ratings reflect physician perceptions of accuracy rather than objective measures of clinical correctness or patient outcomes. Furthermore, public awareness of clinical practice guidelines remains limited, and even when accessible, many patients lack the necessary training to locate and interpret these resources.

These results highlight an opportunity for professional societies to improve the dissemination of high-quality, evidence-based guidance. As patients increasingly seek medical information from LLMs and social media platforms, organizations may improve public education by adapting guideline content into accessible, accurate formats that align with contemporary patterns of digital media consumption.

This study evaluated the perceived accuracy of spine-related medical advice generated by ChatGPT, TikTok, and NASS guidelines. Orthopaedic surgeons rated ChatGPT outputs most highly, followed by NASS guidelines and TikTok content, though variability existed across conditions. While NASS guidelines are developed through expert consensus and peer-reviewed evidence, they are not widely accessible or easily interpretable by the public. As patients increasingly seek medical information through social media and AI platforms, professional societies may enhance public education by adapting evidence-based guidance into accessible formats for widely used digital channels, thereby mitigating misinformation and improving patient understanding.

Cervical radiculopathy occurs when a cervical nerve root is compressed or irritated, resulting in neck pain that radiates into the upper extremity, accompanied by sensory, motor, or reflex changes in a dermatomal/myotomal distribution.

Animal subjects:All authors have confirmed that this study did not involve animal subjects or tissue.