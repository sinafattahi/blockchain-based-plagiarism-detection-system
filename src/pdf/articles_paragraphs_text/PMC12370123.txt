Ship object detection and fine-grained recognition of remote sensing images are hot topics in remote sensing image processing, with applications in fishing vessel operation command, merchant ship navigation route planning, and other fields. In order to improve the detection accuracy for different types of remote sensing ship objects, this paper proposes a ship object perception and feature refinement method based on the improved ReDet, called Mamba-ReDet (M-ReDet). First, this paper designs a ship object fine-grained feature extraction backbone (Mamba-ReResNet, M-ReResNet), which selects and reconstructs the unique features of different types of ship objects through the Mamba’s selective memory to improve the algorithm’s ability to extract fine-grained features. Secondly, the M-ReDet consists of the Ship Object Perception Module (SOPM) and the Ship Feature Refinement Module (SFRM), which can extract the ship’s spatial position information from the feature map, fuse different scales of spatial position information and use this information to refine the fine-grained features to improve the detection accuracy of the algorithm for different categories of ships. Finally, we use the KFIoU and Focal Loss as the regression loss and classification loss of the algorithm to improve the accuracy of the training. The experimental results show that the mAP0.5of the M-ReDet algorithm on the FAIR1M(ship) and DOTAv1.0 visible light (RGB) remote sensing image datasets are 43.29% and 82.09%, respectively, which is 2.78% and 3.34% higher than that of the ReDet.

Remote sensing ship object detection and fine-grained recognition have important research value in fishery monitoring, navigation planning, and other fields. However, problems when using optical remote sensing images for these tasks include inconsistent ship size and rotation angle, as well as complex backgrounds. Therefore, many problems must be solved when designing a remote sensing object detection algorithm that can extract the fine-grained features of ship objects and enhance spatial positioning and feature refinement capabilities.

The commonly used remote sensing object detection algorithms can be categorized into three classes: the Convolutional Neural Network (CNN), the Transformer [1], and the Mamba [2] based remote sensing object detection algorithm. Firstly, the first class of algorithms is mainly improved based on classical networks such as YOLO [3], SSD [4], Faster R-CNN [5], and ResNet [6]. In 2021, XueYang et al. proposed the GWD algorithm [7], which uses the Gaussian Wasserstein Distance to characterize the distances between rotated boxes and solve the problem of a discontinuous range of rotation angles. The same year, XueYang et al. proposed the R3Det algorithm [8], which designed a feature refinement module to obtain the object’s position information and realize feature alignment. Jiaming Han et al. proposed the ReDet algorithm [9], which encodes rotational equivariant and rotational invariant features to improve the detection accuracy of remote-sensing objects. In 2022, Liping Hou et al. proposed the SASM algorithm [10], which uses two strategies, the Shape Adaptive Selection (SAS) and the Shape Adaptive Measurement (SAM), to realize the selection and evaluation of positive samples.

The size differences of remote sensing objects are significant, and the information in the background is rich. It is important to separate the unique information of the object from the background, and the Transformer can effectively solve this problem. In 2022, Li Qingyun et al. proposed the TRD algorithm [11], which reconstructed the Transformer and effectively extracted the spatial position information of the object and the correlation information between instances. In 2023, Wei Liu et al. proposed the AMTNet algorithm [12], which combines CNN and Transformer to reconstruct the backbone network. Through feature exchange, different scales of feature maps are used to improve the network’s feature extraction performance for changing regions. In 2024, Mingji Yang et al. proposed the Hybrid DETR algorithm [13], which can extract alienation features between remote sensing objects and then use the alienation features to distinguish small objects in complex backgrounds through the SODM module, improving the algorithm’s detection ability for small remote sensing objects.

Mamba-based remote sensing object detection algorithm is a new type of algorithm that has become more popular in recent years [14], and its selective structure reduces the computational complexity of the Transformer. It can highlight the remote sensing object’s adequate feature information to improve the algorithm’s robustness. In 2024, Yue Zhan et al. proposed the MambaSOD algorithm [15], which uses a dual Mamba-driven feature extractor for RGB and depth information to model remote dependencies in multimodal inputs with linear complexity. Moreover, designs a cross-modal fusion Mamba model to capture multimodal features. In the same year, Tushar Verma et al. proposed the SOAR algorithm [16], which combines the mamba and YOLOv9 [17], reduces the loss of practical information, expands the receptive field, and can effectively detect remote sensing objects.

Many algorithms are incompatible with different sizes of ship objects; some algorithms are only improved for small objects, and others are only improved for large and obscured objects.

With the deepening of the network layers, the fine-grained features of remote-sensing ship objects will be gradually lost, and some algorithms do not make long-term memory retention for the fine-grained features of remote-sensing ships.

The complementary information fusion between feature maps of different scales is critical, but this process also requires selective learning to reduce the interference of redundant contextual information on remote sensing ship detection and fine-grained recognition of different sizes.

We propose the M-Bottleneck to construct a new backbone network to achieve selective retention of fine-grained features of small ships in shallow feature maps and long-term memory of semantic features of large ships in deep feature maps and, at the same time to expand the receptive field to reduce the algorithm’s false detection of objects such as berths.

This article designs the SFRM module to reconstruct feature maps of different resolutions and selectively supplement the information difference between feature maps of different levels to improve the algorithm’s detection and fine-grained recognition capabilities for ships of different sizes.

This paper compares the effects of different combinations of CrossEntropyLoss, SmoothL1Loss, Focal Loss, and KFIoU used in the algorithm’s training and designs several groups of comparative experiments to find the optimal loss function configuration, improve the algorithm’s regression and classification accuracy, and reduce the loss of accuracy due to the imbalance in the number of different ship categories.

This article proposes a new remote sensing ship object detection algorithm called M-ReDet and conducts multiple comparative and ablation experiments on the FAIR1M(ship) [20] and DOTA datasets [21] to verify the effectiveness of the SOPM, SFRM modules, and optimized loss functions.

ReDet is a classic remote sensing object detection algorithm based on convolutional neural networks. The design of the ReDet [22] mainly aims to solve two problems. First, based on the rotation variation characteristics of convolutional neural networks, ReDet proposes a rotation-invariant backbone network to extract the rotation-invariant features of remote sensing objects. Second, the RRoIAlign only performed a spatial alignment, and there was no alignment in the channel dimension. The RiRoI Align module, which consists of RPN (Region Proposal Network) and RT (ROI Transformer), can extract the features of the rotation recommendation region for classification and regression.

The overall structure of the ReDet mainly consists of a backbone network (ReResNet50) and Neck (ReFPN) [23], which can effectively extract the rotation-invariant features of the remote-sensing object. After the remote sensing image passes through ReResNet50, the algorithm can first obtain the rotation-invariant features of the remote sensing object. In the computation of the rotational features of multiple orientations, these computations share the weights, dramatically reducing the number of computational parameters required for each rotation. The rotation-invariant features of multiple orientations can be obtained by inputting the remote-sensing image of a fixed orientation. After that, we can fuse the feature maps of different layers in the ReFPN, and after the RiRoI Align module, the rotation-invariant features of the same remote sensing object can be extracted from the rotation-isotropic features, which contain features such as the tail and wing of an airplane, the aspect ratio of a ship, can enhance the accuracy of the remote sensing object detection.Fig 1shows the network structure of the ReDet.

yt;iis the output of the attention mechanism;q,k,v, anddare the same asQ,K,V, anddk,gis the number of groups.

ztis the input sequence,W∆,b∆,C, andDare the learnable parameters,A¯tandB¯tare the discretization parameters,htis the hidden state, andytis the output sequence. Mamba’s memorability can capture the contextual information of smaller targets, and the selectivity can dynamically adjust the proportion of the tail-category features to reduce the accuracy impact brought about by long-tailed distribution.

This paper proposes the M-ReDet algorithm based on the ReDet algorithm by improving its Backbone, Neck, and Head to improve its detection and fine-grained recognition ability for ships of all sizes. The overall framework of the M-ReDet algorithm is shown inFig 2, which consists of the M-ReResNet50, the M-ReFPN, and the Head, respectively. The specific structure of each module after improvement will be described in detail in the subsequent subsections.

In this paper, the ship object perception module can better extract the edge, texture, semantic, and other features of remote sensing ship objects and improve the network’s ability to detect ships and fine-grained recognition in complex sea areas such as ports. The SOPM, as the main component of the backbone, can selectively extract and highlight the features of the object’s area. Its memorability can also pass the features extracted by the upper level of the feature map to the next layer so that the next layer can selectively extract the contextual information of different sizes of remote-sensing ship objects. The specific structure of SOPM is shown inFig 3.

Before the feature map enters the SOPM, the remote sensing image passes through a 7 × 7 convolutional layer to initially extract the features such as ship texture and edges. Then the feature map enters into the multilayered ResLayer, which are all composed of M-Bottleneck and Bottleneck. Inside the M-Bottleneck, the feature map is first computed by the convolution, MLP, and Sigmoid to calculate the feature map channel weights, which weight the semantic, edge, and other features of the remote sensing ships to get the fine-grained features needed for ship detection and recognition. Then, the SOPM module serializes the feature map. After the serialized features pass through the mamba’s SSM module, the module can selectively extract the features of the ship object corresponding to the size of the feature map at that level and pass other features to the next level through memory to supplement the contextual information and fine-grained features of the ship corresponding to the size of the next level.

Corresponding to the four-layer ResLayer of ReResNet50, M-ReResNet50 also has four layers of ResLayers. However, the number of M-Bottlenecks in each layer differs from that of ReResNet50, and the ResLayers with different numbers of M-Bottlenecks have different capabilities for feature extraction, object perception, and localization for remote sensing ship objects. In order to find the optimal quantity ratio and ensure the total number of M-Bottleneck and Bottleneck is 3-4-6-3, this paper conducts the following experiments, setting the quantity of M-Bottleneck in each layer as 0-0-0-0, 0-1-1-0, 1-1-1-1, 1-2-2-1 and 2-2-2-2 respectively, and observing the effect of the M-Bottleneck quantity configuration on the algorithm. M-Bottleneck number configuration and the experimental results are shown inTable 1.

According toTable 1, when the number of M-Bottleneck in each layer of ResLayer is set to 1-1-1-1, the mAP0.5of the algorithm is the highest, which is 41.52%, and the accuracy is improved by 1.01% compared to ResLayer in ReResNet50. With the increase of the number of M-Bottlenecks, M-ReDet’s accuracy does not continue to increase but stabilizes around 41.52%; adding too many M-Bottlenecks will introduce additional computation, so in this paper, we chose 1-1-1-1 as the configuration parameter of ResLayer.

Compared with ReResNet stacking a large number of small convolutional kernels, stacking a small number of M-Bottleneck can obtain a substantial receptive field enhancement, and theoretically, M-Bottleneck can obtain the global receptive field, which can effectively improve the algorithm’s extraction ability for the contextual information of ship objects. The feature maps in the backbone network pass through the SOPM module; the algorithm can obtain the rotation-isotropic, rotation-invariant, and fine-grained features of various ship objects. The SFRM module in this section can fully use these features to fuse the upper-layer low-resolution ship fine-grained semantic features with the lower-layer high-resolution ship localization information to improve the algorithm’s ship detection and fine-grained recognition accuracy. The structure diagram of the SFRM module is shown inFig 4.

The ReDet algorithm uses CrossEntropy Loss [29] and SmoothL1 Loss [30] as the classification and regression losses in the Head, respectively. The selection of the loss function has a particular impact on the algorithm’s classification and regression accuracy. In this subsection, the classification and regression losses of the M-ReDet are modified as the Focal Loss and KFIoU Loss.

Focal Loss can deal with the long-tailed distribution problem that exists in ship object detection and fine-grained recognition tasks by adjusting the balance factor and focus factor to change the value size of the classification loss of each ship in the FAIR1M(ship) dataset; there are a total of nine categories of ships, namely Dry Cargo ship, Engineering Ship, Fishing Boat, Liquid Cargo Ship, Motorboat, Passenger Ship, Tugboat, W-ship, and other-ship, the number of ships in each category varies is shown inFig 5.

Whereptis the class label prediction probability,αtis the balance factor,γis the focus factor, andαtandγcan adjust the effect of different categories of samples on the classification loss.VB1,VB2, andVB3are the areas of the prediction box, the actual box, and the overlapping region, respectively. This subsection also investigates the effect of different combinations of loss functions on the accuracy of ship object detection and fine-grained recognition, andTable 2shows the results.

AsTable 2shows, among the four loss function combinations, the mAP0.5of the Focal Loss + KFIoU is 41.39%, which is 0.88%, 1.07% and 0.52% higher than the CrossEntropy Loss + SmoothL1 Loss, Focal Loss + SmoothL1 Loss, and CrossEntropy Loss + KFIoU combinations, respectively. Thus, it is suitable for training the M-ReDet algorithm.

The experimental environment used for the M-Redet remote sensing ship target detection and fine-grained recognition algorithm designed in this paper is CUDA11.1, Intel i7-11700 CPU, NVIDIA GeForce RTX 3080Ti, and the deep learning framework is PyTorch. In this paper, the comparison and ablation experiments are conducted using DOTAv1.0 and FAIR1M(ship) datasets to evaluate the detection accuracy of M-Redet and the effectiveness of each module. The experiment selects SGD as an optimizer, set lr = 0.0025, momentum = 0.9, weight_decay = 0.0001, warmup_iters = 500, warmup_ratio = 1.0/3, and the learning rate strategy is linear. The overall training has 100 epochs, and if the algorithm converges ahead, then end the training round early.Fig 6shows the loss convergence curve of M-ReDet on the DOTAv1.0 and FAIR1M(ship) datasets.Table 3shows the experimental environment, parameter configuration, and model resource consumption.

The DOTAv1.0 dataset has a total of 2806 images, which contains 15 types of remote sensing targets, namely the plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field, swimming pool, and the number of instances is 188282. The number of training sets in the DOTAv1.0 dataset is 1411, the number of validation sets is 458, and the number of test sets is 937. In order to facilitate the training of the algorithm, in this paper, the remote sensing images in the DOTAv1.0 dataset are cropped into 1024 × 1024 size, totaling 21046 remote sensing images.Fig 7shows the example of the DOTAv1.0 dataset.Fig 8illustrates the distribution of quantities for each category in the DOTAv1.0 dataset.

Fig 7is attributed to the DOTA open-source database and is available from the DOTA database (URL (s):https://captain-whu.github.io/DOTA/dataset.html).

The FAIR1M(ship) dataset contains all remote sensing images containing remote sensing ship objects in the FAIR1M2.0 dataset. The dataset contains a total of 13238 remote sensing ship images, the number of instances is 58982, and there are a total of nine types of ship instances, namely the Dry Cargo Ship, Engineering Ship, Fishing Boat, Liquid Cargo Ship, Motorboat, Passenger Ship, Tugboat, W-ship, and other-ship. The number of ships in each category is 17474, 3897, 9031, 3883, 15445, 1924, 1897, 1178 and 4253, respectively. The FAIR1M(ship) dataset is rich in all kinds of ships, which is suitable for the research of remote sensing ship object detection and fine-grained recognition.Fig 9shows part of the FAIR1M(ship) dataset examples.

Fig 9is attributed to the FAIR1M open-source database and is available from the FAIR1M database (URL (s):https://gaofen-challenge.com/benchmark).

N is the number of object categories, andAPiis the average accuracy of each category.

This subsection compares the M-ReDet algorithm with the commonly used rotated object detection algorithms such as the GWD, R3Det, RoI Transformer [33], Rotated Faster RCNN, Rotated RetinaNet, Rotated Reppoints [34], S2ANet [35], SASM, KFIoU, and ReDet to verify its effectiveness on the DOTA dataset.Fig 10shows the detection results of the M-ReDet algorithm on the DOTA dataset.

Fig 10is attributed to the DOTA open-source database and is available from the DOTA database (URL(s):https://captain-whu.github.io/DOTA/dataset.html).

The detection result shows that the M-ReDet can detect small, medium and large remote-sensing objects well. Faced with simple remote-sensing objects such as tennis courts and object-intensive scenarios such as harbors, the M-ReDet’s detection accuracies are mostly above 90%, and some occluded and small objects have false and missed detection.Table 4shows the experimental results of comparing the M-ReDet and the above 10 algorithms.

* means that the results from our previous work [36].

The AP-max of M-ReDet is similar to other algorithms but has the highest AP-min, which verifies its effectiveness in enhancing the classification and detection accuracy of remote-sensing objects with small samples.Table 5shows the detection accuracies of various remote-sensing objects by the M-ReDet algorithm on the DOTA dataset.

The M-ReDet has the highest detection accuracy for the tennis course with an AP value of 90.8%, and for the remote sensing objects, such as planes, ships, basketball courses, harbors, large vehicles, and helicopters, also have high detection accuracies of around 90%. Since the M-ReDet uses M-Bottleneck to expand the receptive field when facing remote sensing objects with large sizes and high aspect ratios, such as bridges, the algorithms in this paper can combine the contextual information, such as selectively utilizing the road information on both sides of the bridge to make the detection judgment. Finally, there is a 4% accuracy improvement in AP-min(bridge).Fig 11shows the training results of the above types of algorithms.

The 11 algorithms all use the pre-trained model to train; as seen fromFig 10, the SASM and Rotated RepPoints algorithms’ convergence is relatively slower compared to the other algorithms. Most of the algorithms converge to the optimal mAP0.5in about 10 epochs. The M-ReDet algorithm achieves convergence after 10 epochs, and the final mAP0.5 is stabilized at about 82.09%. Its training curve is also in the top left corner, indicating optimal convergence speed and accuracy.

The M-ReDet algorithm performs well in dense ship distribution, sparse ship distribution, simple remote sensing background, and complex remote sensing background.Fig 12shows its ship detection and fine-grained recognition results on the FAIR1M(ship) dataset. Since the appearance and aspect ratio of these nine types of ship objects are similar, it is necessary to extract the fine-grained features of each type of ship from the texture and semantic information in order to recognize the type of ship accurately. The M-ReDet algorithm first extracts the fine-grained features of the spatial location of the ship by using the SOPM and then extracts the different features between the different types of ships by using the SFRM.

Fig 12is attributed to the FAIR1M open-source database and is available from the FAIR1M database (URL(s):https://gaofen-challenge.com/benchmark).

In the ship detection and fine-grained recognition comparison experiments, the M-ReDet and RoI Transformer, SASM, ReDet, R3Det, Faster RCNN, Rotated RetinaNet, GWD, S2ANet, KFIoU, LSKNet [37] algorithms use the same experimental hyper-parameters and the remote sensing ship images used for training are cropped to 1024 × 1024 size. The mAP0.5of M-ReDet is 43.29%, which is higher than the above algorithms by 7.85%, 13.71%, 2.78%, 13.46%, 12.11%, 18.61%, 13.57%, 9.59%, 7.7% and 3.17%, respectively. M-ReDet has the highest AP-max and AP-min, which reflects from the side that the algorithm in this paper uses the SOPM and SFRM modules to enhance the size of the receptive field so that the M-ReDet can extract more contextual information, which can enhance the detection accuracy of the ship objects of all size of ships.Table 6shows the detection results of various algorithms on the FAIR1M(ship) dataset.

There are nine categories of ship objects in the FAIR1M(ship) dataset. The detection and fine-grained recognition accuracy of ships is proportional to the number of instances of this category, except that there are many ships in the category of “other-ships” that do not have subdivided categories or the category determination information is ambiguous, which results in their detection and fine-grained recognition accuracy of only 15.9%. The M-ReDet algorithm has the highest detection accuracy for “Dry-Cargo-Ship,” with an AP of 71.20%. The “Motorboat” has 7921 instances, but because of its small size, the overall AP is lower than “Dry-Cargo-Ship” at 64.7%.Table 7shows the training results for each of the nine categories of ships.

Table 8demonstrates the AP values of 9 types of ships detected by the 11 algorithms, respectively, to show more clearly the impact of the M-ReDet algorithm on the detection and fine-grained recognition of various types of ships. As seen from the table, the M-ReDet algorithm achieves the optimal AP in the detection results of 7 classes of ships. It is lower than the LSKNet and ReDet only in Passenger-Ship and W-ship detection results.

After 15 epochs of training, the mAP0.5of the M-ReDet algorithm is finally stabilized near 43.29%; because of the addition of M-Bottleneck, stabilizing the internal parameters of the module requires more training epochs of updating, and its convergence speed is slightly slower compared to the other algorithms. However, it has the highest accuracy of ship detection and fine-grained recognition, which is shown inFig 13for the comparison of the M-ReDet algorithm with the other 10 algorithms.Fig 13shows the training results of all algorithms.

The M-ReDet mainly consists of the M-ReResNet50, M-ReFPN and detection head, where the SFRM adds the M-Bottleneck and initializes it as configured in subsection 3.2. The SFRM and the FPN constitute the M-ReFPN module together.Table 9shows the results of the ablation experiments of M-ReDet, which mainly investigate the effects of the SOPM, SFRM, KFIoU and Focal Loss alone or both or together on the mAP0.5of the M-ReDet algorithm, and the results of the ablation experiments are as follows.

It can be seen fromTable 9that using each module alone can improve the mAP0.5of the M-ReDet algorithm; the mAP0.5are 41.52%, 41.45%, and 41.39%, respectively, which are improved by 1.01%, 0.94%, and 0.88% compared to the baseline model. Improvement modules used two by two can also improve the mAP0.5of the algorithm. Finally, M-ReDet using SOPM, SFRM, KFIoU, and Focal Loss at the same time achieves the optimal ship detection and fine-grained recognition accuracy, with a mAP0.5 of 43.29%, which is 2.78% higher than that of the baseline model ReDet, andFig 14shows the difference between the ReDet and the M-ReDet on the FAIRM (ship) dataset for ship detection and fine-grained results.

(a)Detection and fine-grained results of the ReDet;(b)Detection and fine-grained results of the M-ReDet.Fig 14is attributed to the FAIR1M open-source database and is available from the FAIR1M database (URL (s):https://gaofen-challenge.com/benchmark).

FromFig 14, it can be found that the SOPM module in the M-ReDet algorithm expands the receptive field, which enables the algorithm to extract more contextual information, such as the sea surface and ports. With that information, the M-ReDet algorithm can detect the remote sensing ship objects that are in the edge position or obscured; for example, in the first line of the remote sensing image, M-ReDet successfully detects the incomplete ship objects that are on the top, right and bottom left, respectively. Moreover, the SOPM and SFRM modules can selectively memorize the fine-grained features of remote sensing ship objects of different sizes to minimize the loss of features of small ship objects in the downsampling process and to improve the algorithm’s ability to detect small ships, for example, the first image in the second row, M-ReDet detects a tiny “other-ship” object located in the center of the image, but its size is too small, and the classification confidence is low. In addition, the enlarged receptive field of M-ReDet can also avoid the false detection of large buildings such as bridges and dock berths; for example, in the second row, ReDet recognizes the dock berth in the second image and the bridge in the third image as a ship.

The main innovation of this paper is the design of the M-ReDet algorithm framework. The proposal of two improved modules, SOPM and SFRM, which form a new backbone network (M-ReResNet50) and a new Neck (M-ReFPN), which enable the algorithm to selectively learn and retain fine-grained information about objects of different sizes of ships, and to fuse the contextual information required for memorization, which improves the algorithm’s detection and fine-grained recognition ability for ships of different size. After several sets of comparison and ablation experiments, the detection robustness of the M-ReDet algorithm designed in this paper is optimal.

Small ship feature information is easily lost in the downsampling process, resulting in the loss of detection and fine-grained recognition accuracy. The M-Bottleneck in the SOPM module can extract and memorize the fine-grained features of small ships and send them to the subsequent layers for further feature extraction. In the SFRM module, the feature maps of different layers can also realize the complementary selective information so that the information on ships of different sizes can be better retained. The SOPM module can also expand the receptive field and cooperate with the SFRM module to supplement the contextual information required by different scale feature maps to reduce the probability of misdetection of similar ship objects. The optimization of classification loss and regression loss is also essential for the algorithm training process. However, this paper selects only four commonly used loss functions for permutation and combination to try the optimal loss function scheme. There are no further attempts to compare and analyze the recent excellent loss functions. Due to the limitation of hardware memory, this paper also did not further improve the number of configurations of M-Bottleneck in the backbone network to optimize the module structure and training hyperparameters of M-Bottleneck based on the experimental training results. In the future, we will make improvements to the structure and configuration of M-Bottleneck in subsection 3.2, continue to optimize the training loss function of the algorithm, and try to incorporate the fine-grained feature information into the loss function for the ship detection and fine-grained recognition tasks further to improve the regression and classification accuracy of the algorithm. In addition, this study can be integrated with other remote sensing data in the future, allowing for the design of multimodal remote sensing image fusion modules that combine visible light, SAR, and infrared data to enhance the algorithm’s remote sensing ship detection capability in harsh weather conditions, such as cloudy and foggy scenes; other fields can also utilize this work, such as agricultural object detection [38–39].

Remote sensing ship detection and fine-grained recognition face significant challenges due to high inter-class similarity in aspect ratios, ambiguous appearance features among vessel categories, arbitrary orientation variations, and multi-scale object characteristics. This paper proposes the M-ReDet, a memory-augmented ship perception network with feature refinement mechanisms, to address these issues. The optimization of the loss function of M-ReDet further improves the classification and regression accuracy of the algorithm. The comparative and ablation experiments on the FAIRM(ship) and DOTA datasets prove the effectiveness of this paper’s improved modules. Finally, in the remote sensing ship and fine-grained recognition task, the M-ReDet’s mAP0.5is 43.29%, validating the effectiveness of our algorithm in complex maritime scenarios.

The author would like to express thanks to anonymous reviewers for all careful review of the paper and kind suggestions made to improve overall quality of the manuscript.