Protein-ligand interactions are crucial for understanding various biological processes and drug discovery and design. However, experimental methods are costly; single-ligand-oriented methods are tailored to specific ligands; multi-ligand-oriented methods are constrained by the lack of ligand encoding. In this study, we propose a structure-based method called LABind, designed to predict binding sites for small molecules and ions in a ligand-aware manner. LABind utilizes a graph transformer to capture binding patterns within the local spatial context of proteins, and incorporates a cross-attention mechanism to learn the distinct binding characteristics between proteins and ligands. Experimental results on three benchmark datasets demonstrate both the effectiveness of LABind and its ability to generalize to unseen ligands. Further analysis validates that LABind can effectively integrate ligand information to predict binding sites. Additionally, the application of LABind is extended to binding site center localization, sequence-based methods, and molecular docking tasks.

Here, authors present LABind, a ligand-aware deep learning framework for predicting binding sites of small molecules and ions, with the capacity to generalize to unseen ligands.

Protein-ligand interactions play a fundamental role in many biological processes1, such as enzyme catalysis, signal transduction, and molecular recognition2–4. These interactions are not only critical for understanding cellular functions but are also pivotal in drug discovery and design5. Although techniques such as X-ray crystallography, nuclear magnetic resonance, and cryo-electron microscopy can precisely determine binding sites, their extensive resource consumption limits widespread application. Given the rapid growth in the number of known protein structures and small molecules6,7, developing a computational method to accurately and efficiently identify protein-ligand binding sites is of importance.

In literature8, computational methods for predicting protein-ligand binding sites started from valuable small datasets derived from biological experiments. Some methods train individual models on single datasets targeting specific ligands. Consequently, predicting binding sites necessitates selecting the corresponding model for the specific target ligand. We refer to these methods as single-ligand-oriented methods. Furthermore, single-ligand-oriented methods can be categorized into template-based methods, sequence-based methods, and structure-based methods, depending on the types of protein information used. Template-based methods such as IonCom8, MIB9, and GASS-Metal10employ alignment algorithms to match known ligand binding sites from similar proteins to the query protein. However, without high-quality known protein templates, these methods often fail to achieve the desired outcomes. Sequence-based methods identify binding sites solely based on protein sequence information. TargetS11employs a sliding-window strategy to extract features from protein sequences, and uses a support vector machine to predict binding sites. However, the lack of experimental spatial structure information limits the effectiveness of sequence-based methods, leading to a growing interest in structure-based methods. DELIA12utilizes fixed-size convolution kernels to learn protein-ligand interaction patterns from 2D distance matrices of proteins and uses bidirectional long short-term memory networks to extract features from protein sequences. GraphBind13encodes protein structural context as graph features and then uses hierarchical graph neural networks to predict binding sites from protein structure graphs. LigBind14undergoes pre-training on a broad set of ligands, then fine-tunes for over 1000 specific ligands. GeoBind15combines surface point clouds with graph networks to predict protein-nucleic acid binding sites. While single-ligand-oriented methods often achieve better results for specific ligands, their inherent specialization limits their effectiveness when applied to unseen ligands.

With advances in biological experiments, more binding datasets are available. It is now possible to combine several datasets containing multiple ligands to train a single computational method. We refer to these methods as multi-ligand-oriented methods. LMetalSite16and GPSite17utilize multi-task learning to combine multiple ligand datasets and train a single model for multiple specific ligands. However, LMetalSite and GPSite are still limited to specific ligands. Many multi-ligand-oriented studies (e.g., P2Rank18, DeepSurf19, and DeepPocket20) represent protein structures as corresponding features, such as the solvent accessible surface. They rely directly on protein structures without considering specific ligands, thereby underscoring the importance of the protein structures themselves. However, these methods overlook the differences in binding pattern among different ligands. Ligand-oriented models share the same inability to predict protein binding sites for unseen ligands, as they lack an explicit encoding of ligand properties during the training stage. In summary, it is both necessary and challenging to develop a unified model that incorporates ligand information for accurately addressing a wide range of different ligand scenarios.

In this study, we propose a method capable of predicting protein binding sites for ligands that are not present in the training set. A binding site is defined here as a set of protein residues located within a specific distance from the ligand21. Consequently, the classification task in LABind is per-residue prediction, aiming to determine whether each residue in a protein is part of a binding site. During the training stage, our method explicitly models ions and small molecules as well as proteins. Thus, it can also identify protein binding sites for ligands not seen during the training stage. As our model explicitly learns the representations of ligands, we name our model ligand-aware binding site prediction (LABind for short). It is worth noting that LigBind also considers the characteristics of ligands and can identify binding sites for unseen ligands using the pre-training model. Nevertheless, the effectiveness of its pre-training is limited, and it still requires fine-tuning with specific ligands to achieve more accurate prediction results. Additionally, as partners in protein binding, ligands can be either biomacromolecules (e.g., proteins, RNAs) or other small molecules (including ions). We assume that protein-biomacromolecules binding may exhibit many different properties compared with protein-small molecule binding22. In this study, we focus on predicting protein binding sites for small molecules and ions. LABind utilizes a recently published protein pre-trained language model (Ankh23) to obtain sequence representations of proteins, and uses a molecular pre-trained language model (MolFormer24) to represent molecular properties based on ligand Simplified Molecular Input Line Entry System (SMILES) sequences. It encodes proteins into graphs and uses a graph transformer to capture potential binding patterns in the local spatial context of proteins. By employing a cross-attention mechanism, it learns the distinct binding characteristics between proteins and ligands. Although LABind is a structure-based approach, we also develop a sequence-based program that leverages the structures predicted by ESMFold25to implement a sequence-based prediction strategy. Ultimately, LABind comprehensively considers the impact of ligand information on binding sites and encompasses all small molecules and ions through a unified model. The modeling of ligands enhances the understanding of interactions between proteins and ligands, which contributes to the formation of binding sites. This multi-ligand capability enables the model to learn representations shared between different ligand binding sites, while also learning representations specific to each ligand binding site. Comprehensive experiments demonstrate that LABind shows marked advantages over other multi-ligand-oriented and single-ligand-oriented methods. First, LABind outperforms other advanced methods in various benchmark datasets (DS1, DS2, and DS3, see Benchmark datasets for dataset details). Importantly, as a multi-ligand-oriented method that effectively incorporates ligand information into its model architecture, LABind demonstrably improves the accuracy of predicting binding sites for a variety of ligands, including small molecules, ions, and unseen ligands. The integration of ligand characteristics not only enhances the model’s performance but also underscores the pivotal role of these big data-pretrained features in identifying binding sites. Second, LABind outperforms competing methods in predicting binding site centers through clustering of predicted binding residues. The robustness of our method is validated by applying it to proteins without experimentally determined structures, even when using predicted structures from ESMFold and OmegaFold26. LABind consistently demonstrates resilience and reliability. Furthermore, LABind shows a strong ability to effectively distinguish between different ligands and substantially enhance the accuracy of molecular docking tasks. Additionally, LABind showcases its potential and reliability in practical applications by successfully predicting the binding sites of the SARS-CoV-2 NSP3 macrodomain with unseen ligands. Through ablation studies, we demonstrate the importance of each protein feature source in LABind, with the protein representation proving to be crucial and the positive impact of ligand features also being evident. Additionally, the visualization of residue representations derived from the attention-based learning interaction shows how LABind effectively captures information about protein-ligand interactions, thereby learning the differences between binding and non-binding sites and enhancing the accuracy of predictions.

In our comparative analysis of LABind against other methodologies across three benchmark datasets (DS1, DS2, and DS3), LABind exhibited superior performance. LABind also outperformed competing methods in the prediction of binding site centers. We evaluated the influence of predicted protein structures on the identification of binding sites, focusing on how variations in structural accuracy affect the reliability of binding site prediction. To further evaluate the effectiveness of LABind in distinguishing between different ligands, we visualized LABind’s predictions of binding sites in two proteins associated with distinct ligands. Moreover, the binding sites predicted by LABind were utilized to improve the accuracy of docking poses generated by Smina27. We conducted a case study on the binding sites of the SARS-CoV-2 NSP3 macrodomain. The insights gained from this case further validate LABind’s applicability in real-world scenarios. Additionally, we performed ablation studies to explore how various input features influence model’s predictive outcomes. Lastly, we visualized the residue representations obtained from the attention-based learning interaction to demonstrate their effectiveness in capturing crucial information about protein-ligand interactions.

The overall architecture of LABind is illustrated in Fig.1a. First, the SMILES sequence of the ligand is input into the MolFormer pre-trained model to obtain the ligand representation. The sequence and structure of the protein receptor are input into Ankh and DSSP28, respectively, to obtain the protein embedding and DSSP features, which are then concatenated to form the protein-DSSP embedding. The protein structure is converted into a protein graph through the graph converter module, where the node spatial features are angles, distances, and directions derived from atomic coordinates, and the edge spatial features include directions, rotations, and distances between residues. Then, the protein-DSSP embedding obtained previously is added to the node spatial features of the protein graph, resulting in the final protein representation (including residue representations and residue-residue interactions). Next, the ligand representation and the protein representation are processed through the attention-based learning interaction to learn the interactions between them. Finally, a multi-layer perceptron (MLP) classifier is used to predict the binding sites of the protein with the ligand.

aThe model architecture of LABind. The protein sequence and ligand SMILES are input into the Ankh and MolFormer pre-trained language models, respectively, to obtain protein embeddings and ligand representations. Node spatial features and edge spatial features are extracted from the protein structure by the graph converter module. After being combined with protein-DSSP embeddings to form protein representations, the binding sites are predicted by the attention-based learning interaction and an MLP classifier.bThe architecture of the graph converter module. For node spatial features, R and S represent the centroid of the heavy side chain atoms and the position of the nearest atom on the surface, respectively. For edge spatial features, the Cαis used to calculate the directions and torsion angles. The distances from all neighboring atoms to the target Cαare calculated as part of the edge spatial features.cThe architecture of the attention-based learning interaction. Local contextual information of the protein representations is learned through a neighbor-attention, and a cross-attention is used to facilitate the interactions between the ligand representation and residue representations. Residue-residue interactions are updated using the updated residue representations and original residue-residue interactions, and the ligand representation is updated by performing both max-pooling and averaging on residue representations.dThe architecture of the multi-head cross-attention. Attention scores are computed using residue representations as queries and the ligand representation as both keys and values.

To evaluate the performance of LABind on protein and ligand binding site prediction, we used recall (Rec), precision (Pre), F1 score (F1), Matthews correlation coefficient (MCC), area under the receiver operating characteristic curve (AUC) and area under the precision-recall curve (AUPR) as evaluation metrics. In addition, we used DCC (distance between the predicted binding site center and the true binding site center) and DCA (distance between the predicted binding site center and the closest ligand atom) to evaluate the performance in localizing binding site centers. The calculation methods for all metrics are provided in Supplementary Note1. Due to the highly imbalanced distribution and number of binding sites and non-binding sites, MCC and AUPR are more reflective of the performance of a model in imbalanced two-class classification tasks29,30. In addition to AUC and AUPR, which are not affected by threshold selection, other metrics are calculated using a threshold to convert the predicted binding probabilities to binary predictions. This threshold is determined by maximizing the MCC value. Therefore, we adopted AUPR as the criterion for hyperparameter optimization and ablation studies.

To illustrate the improvements achieved by LABind, we compared it with five multi-ligand-oriented methods (P2Rank, DeepSurf, DeepPocket, LMetalSite, and GPSite) and four single-ligand-oriented methods (IonCom, DELIA, GraphBind, and LigBind). Among the compared methods, P2Rank, DeepSurf, and DeepPocket are structure-based approaches that predict the spatial center of a binding site rather than individual binding residues. The introductions of these methods are provided in Supplementary Note2.

We compared LABind with literature single-ligand-oriented methods on the DS1 dataset. For a fair comparison, LABind, like LigBind, was trained on the DS1 training set and tested on the DS1 test set. The results of these 19 ligands on the DS1 test set are presented in Table1. Compared to the best-performing benchmark method, LigBind, the MCCs of LABind are 0.049-0.221 higher across all ligands except NO2-and HEM. Although LABind underperforms LigBind by 0.029 and 0.015 in MCC on NO2-and HEM, respectively, the average MCC of LABind is 21.8% higher than that of LigBind. It is worth noting that the metrics of LigBind are still targeted at models for individual ligands. Here, we compared LABind with only four methods on the DS1 test set. Since the DS1 test set overlaps with the DS2 training set, we excluded the metrics for GPSite and LMetalSite that were trained on the DS2 dataset. All metrics of all methods are shown in Supplementary Table1. When ligand information is ignored, with ligand representations set to an all-one embedding for all ligands, LABind-NL achieves an average MCC of 0.499 among these 19 ligands. The average MCC of LABind-NL is comparable to that of LigBind, but it is 0.110 lower than that of LABind. This result demonstrates that some protein-ligand binding samples in the DS1 training set are insensitive to the ligand. This is attributed to the fact that protein-ligand binding sites are generally located in pockets (clefts, grooves) on the surface of proteins31, which have the capacity to interact with multiple ligands. Consequently, similar proteins with similar ligands can exhibit analogous binding sites32. LABind-NL can capture the commonality of how protein structure influences binding sites in protein-ligand binding patterns.

Boldface and underline indicate the best performance and the second-best performance among the compared methods for each ligand, respectively. The results of other methods were obtained from the report of LigBind. LABind-NL represents the performance of LABind without ligand representations.

Moreover, we compared LABind with multi-ligand-oriented methods on the DS2 dataset. To compare with multi-ligand-oriented methods, we retrained LABind on the DS2 training set and tested on the DS2 test set. We compared LABind with the nine methods on six ligands, including ATP, HEM, Zn2+, Ca2+, Mg2+, Mn2+. The results on the DS2 dataset are given in Table2. The MCC of LABind is lower than that of GPSite only on the ATP test set, while it is higher than other prediction methods on other ligands. The average MCC of LABind shows a 4.7% improvement over that of GPSite on the DS2 test set. Due to the smaller size of metal ions, the interactions of the metal ions with proteins are often found significantly more versatile and flexible compared with those of larger size ligands33. LABind demonstrates improved performance in identifying metal ion-binding sites. However, the performance advantage of LABind-NL is not as appreciable on the DS2 dataset. We believe this is due to the smaller number of proteins, which prevents the network from effectively learning the common pattern across different ligand-protein bindings. Notably, LABind demonstrates strong performance on both small molecule ligands and ion ligands.

Boldface indicates the best performance among the compared methods for each ligand. The results of LigBind were obtained from the LigBind-G program. The results of other methods were obtained from the report of GPSite. LABind-NL represents the performance of LABind without ligand representations.

Having established the prior performance of LABind on ligand-oriented methods, we trained LABind on a comprehensive dataset as a final predictor that is capable of identifying binding sites between proteins and ligands unseen in the DS3 training set. The hyperparameter settings for the DS3 training set were identical to those used for the DS1 and DS2 training sets. The DS3 picking set (see DS3: recent assembled dataset for ligand-aware model for details) was used solely to tune hyperparameters and determine the binary classification threshold based on the model’s outputs, ensuring that no information from the test set influenced the training process. As shown in Supplementary Note3, the training sets used by competing methods share a higher sequence identity with the DS3 test set compared to the DS3 training set. This could potentially introduce some bias and inadvertently favor the competing methods during evaluation. Figure2illustrates that LABind still outperforms LigBind on the DS3 test set, demonstrating its improved generalization capability on unseen data. Note that IonCom, DELIA, GraphBind, LMetalSite, and GPSite are unable to predict unseen ligands. In addition, P2Rank, DeepSurf, and DeepPocket primarily predict binding site centers rather than individual binding residues. Therefore, only LigBind was retained for this comparison. The comparison of MCC and AUPR between LigBind and LABind across all ligands is shown in Fig.2a. Among the 474 unseen ligands, LABind outperforms LigBind in MCC on 307 unseen ligands, and achieves higher AUPR on 366 unseen ligands. As shown in Fig.2b, for the entire binding sample, LABind exceeds LigBind by 0.125 in F1, 0.131 in MCC, and 0.152 in AUPR. Overall, the results demonstrate that LABind achieves performance improvement on unseen ligands compared to LigBind.

aMCC and AUPR comparison between LABind and LigBind. The number of binding residues is represented by the color of the points. The X-axis and Y-axis represent the corresponding values of LigBind and LABind, respectively.bOverall performance metrics of LABind and LigBind on the DS3 dataset. ‘*’ represents that ESMFold-predicted structures are used as the structural input. The results of LigBind were obtained from the LigBind-G program.

We further evaluated and compared the performance of LABind with three baseline methods (P2Rank, DeepSurf, and DeepPocket) in predicting binding site centers on the benchmark datasets. Since LABind does not directly output predicted binding site centers, we applied the mean-shift clustering algorithm34to the predicted binding residues from LABind to determine the binding site centers. This mean-shift-based identification of binding site centers and the evaluation protocol are detailed in Supplementary Note4. Table3summarizes the performance of different methods in localizing binding site centers for small-molecule ligands. For the DS1 and DS2 test sets, we retained only the data involving small-molecule ligands, as the compared methods were specifically trained for small-molecule binding prediction. LABind outperforms other methods for predicting binding site centers on the DS1 and DS2 test sets; however, these datasets have limited ligand diversity. In contrast, the DS3 test set includes a broader range of ligands. On the DS3 test set, LABind achieves a DCC success rate only 0.8% lower than P2Rank but surpasses all other methods in DCA performance. As the DS1, DS2, and DS3 datasets are not specifically curated for evaluating binding site center localization, we further selected 64 binding sites from the COACH420 benchmark as an additional test set. Following DeepPocket’s stringent data filtering criteria, we excluded protein-ligand complexes from COACH420 that share more than 50% sequence identity with proteins in the DS3 training set or exhibit both greater than 0.9 ligand similarity and more than 30% sequence identity. The resulting dataset, termed COACH64, is described in detail in Supplementary Note5. On the COACH64 test set, LABind achieves the best overall performance, with DCC and DCA success rates 4.7% and 3.1% higher, respectively, than those of the second-best method. Although LABind is not specifically designed to predict binding site centers, these results demonstrate its strong potential in this regard.

Boldface and underline indicate the best performance and the second-best performance among the compared methods, respectively. The success rates of DCC and DCA are both based on the top-npredicted binding site centers, wherendenotes the number of identical cognate ligands.

The preceding results are based on holo structures (ligand-bound protein structures). However, physicochemical interactions between binding sites and ligands can induce conformational changes in proteins, leading to transitions between apo structures (unbound protein structures) and holo structures35. Accurately predicting binding sites on apo or predicted structures poses a greater challenge and holds higher practical significance than predictions on holo structures. Furthermore, due to the limitations of some experimental techniques, many proteins currently lack experimentally determined tertiary structures. For proteins without accurate atomic positions, protein structure prediction methods were combined to achieve sequence-based ligand binding site predictions. Specifically, we tested the effects of combining ESMFold and OmegaFold with LABind separately. The primary reason we did not choose AlphaFold236is its requirement for multiple sequence alignment, which is time-consuming. We assessed the quality of predicted structures by calculating the TM-score between the native and predicted structures using US-align37. Figure3illustrates how the performance of LABind depends on high-quality protein structures and identifies which protein structure predictor is more compatible with LABind. The distributions of TM-scores for ESMFold and OmegaFold on the DS1 test set are presented in Fig.3a. The average TM-score of ESMFold is higher than that of OmegaFold on the DS1 test set (0.861 vs 0.824). As shown in Fig.3b, when LABind was retrained on the DS1 dataset using predicted structures from ESMFold and OmegaFold, respectively, it is evident that LABind based on native structures outperforms those based on predicted structures. This result is expected, as native structures provide more accurate surface and spatial information. Specifically, the average AUPR achieved using ESMFold and OmegaFold is lower than that achieved using the native structure by 0.055 and 0.066, respectively. Although the performance using OmegaFold-predicted structures is suboptimal, the average AUPR remains 0.065 higher than that of LigBind. Additionally, as shown in Fig.3c, we selected predicted structures with a TM-score  <0.5 on the DS1 test set to examine the robustness of LABind for low-quality predictions. On the DS1 test set, 252 OmegaFold-predicted structures and 152 ESMFold-predicted structures have a TM-score  <0.5. As visualized in Fig.3c, due to the larger number of low-quality proteins in OmegaFold, the linear regression line for LABind based on ESMFold-predicted structures consistently lies above the line based on OmegaFold-predicted structures.

aThe distributions of the TM-scores between the native structures and the predicted structures by ESMFold and OmegaFold on the DS1 test set (n= 2626 protein chains). The violin plot displays the minima and maxima (lower and upper ends), median (the white dot in the center), interquartile range (lower and upper ends of the box), and 1.5 times the interquartile range (whiskers).bThe AUPR performance of LABind across various predicted structures on the DS1 test set.cThe relationship between proteins with TM-score  <0.5 for ESMFold and OmegaFold in each ligand test set of DS1 dataset. The lines are fitted to the TM-score and AUPR original data using linear regression.

For proteins without experimentally determined structures from wet-lab, we instead chose the predicted structures from dry-lab approaches like ESMFold. We compared the DS1 training and test sets under four different scenarios, as shown in Supplementary Table2. As observed, when the structure types in the DS1 training and test sets differ, the average AUPR remains nearly the same. Although the average AUPR using predicted structures for both training and testing is 0.003 higher than when using native structures for training and predicted structures for testing, we still opted to use the LABind trained on native structures to make predictions on predicted structures. To further validate the robustness of LABind on ESMFold-predicted structures, we used the predicted structures from ESMFold as structural input to predict binding sites on the DS3 test set. As illustrated in Fig.2b, the performance of LABind using ESMFold-predicted structures still outperforms LigBind evaluated on holo structures by 0.082 in MCC and 0.094 in AUPR. This demonstrates the robustness of LABind when using predicted structural inputs and its potential generalizability to sequence-based binding site prediction. To facilitate better application in scientific experiments, we developed a prediction pipeline based on both sequence and structure information. The detailed processing workflow is illustrated in Supplementary Fig.1. For proteins with experimentally determined structures, these structures are directly used by LABind. For proteins with only amino acid sequences, predictions are made using the structures generated by ESMFold. Thus, LABind can be extended to predict the binding sites of protein-small molecule ligand interactions under various conditions.

To verify whether LABind, as a unified model, can accurately distinguish the binding sites of the same protein with different ligands, we selected two proteins from the DS1 test set that interact with multiple ligands. We selected the alkaline phosphatase from Sphingomonas in complex with inorganic phosphate (PDB ID: 5XWK, chain: A, 5XWK_A) with Zn2+, Ca2+and PO43-, as well as the crystal structure of human MORC2 with spinal muscular atrophy mutation S87L protein (PDB ID: 5OFB, chain: B, 5OFB_B) with Zn2+and ATP. Figure4a shows the binding scores predicted by LABind for two proteins with different ligands, where only residues with prediction scores greater than 0.3 are considered. For 5XWK_A, the predicted scores for Zn2+and PO43–overlap significantly, while the predicted scores for Ca2+are primarily located toward the rear of the protein. There is a small overlap between the high-scoring regions for Ca2+and Zn2+, but the scores for Ca2+are relatively lower compared to those for Zn2+. For 5OFB_B, the high-scoring region for ATP is concentrated at the front of the protein sequence, while that for Zn2+is mainly located toward the rear of the protein sequence. As shown in Fig.4b, it is evident that the positions of Zn2+and PO43-are in close proximity, which leads to a high degree of overlap in the binding sites. This aligns with the binding scores predicted by LABind. On the other hand, the Ca2+ion occupies a distinct region, and LABind also captures the corresponding binding sites. Similarly, as shown in Fig.4c, Zn2+and ATP are spatially distant in 5OFB_B, so their binding sites do not overlap. The binding scores predicted by LABind clearly reflect this difference and identify the corresponding binding sites. This indicates that LABind can also distinguish between the interactions of small molecules and ions. Furthermore, the scores predicted by LABind for Zn2+binding sites are generally high. Zn2+binding sites are relatively easier to identify, whereas Ca2+binding sites are more difficult to recognize, which is consistent with previous studies11. Collectively, these results suggest that LABind demonstrates a clear ability to distinguish between different ligands, which forms the basis for developing a unified model.

aThe prediction scores of LABind for the binding sites of two proteins with different ligands.bVisualization of the complex structure of the 5XWK_A protein with Zn2+, Ca2+, and PO43–ligands.cVisualization of the complex structure of the 5OFB_B protein with Zn2+and ATP ligands. Cyan and yellow spheres represent Zn2+and Ca2+, respectively. PO43-is represented by four red spheres (oxygen) and one orange sphere (phosphorus), while ATP is represented by a stick model. Additional visualization of the LABind prediction results for the binding regions is provided, and the binding regions are rotated by a certain angle to facilitate observation. True positives, false positives, and false negatives represent binding residues correctly predicted, non-binding residues incorrectly predicted as binding, and binding residues incorrectly predicted as non-binding, respectively.

To further investigate the potential application of binding sites, we used LABind-predicted binding sites to assist in molecular docking tasks, to both examine how helpful these predictions are for Smina blind docking and explore the potential for further improvement of LABind. We selected CASF-2016 dataset38to evaluate performance on molecular docking tasks and used Smina as the docking program. Subsequently, LABind failed to identify binding sites on 16 complexes, resulting in 269 protein-ligand complexes for further analysis. Since all complexes in the CASF-2016 dataset consist of a single protein and a single ligand, we utilized the average position and spatial extent of the binding residues predicted by LABind to determine the center and radius of the search box for the specific ligand in each complex (binding site-guided docking). For comparison, we employed both blind docking and cognate ligand-guided docking. Blind docking was performed using the geometric center of the protein and a box that encompasses the whole structure. Cognate ligand-guided docking was performed based on the position of the native ligand using the –-autobox_ligand option in Smina. It is noted that this option denotes using almost known binding sites. Supplementary Note6provides a description of three experimental details and procedures. We assessed the accuracy of ligand poses, defining a successful docking outcome by a strict RMSD threshold of  < 2 Å. The success rate of the top 20 structures was used to compare the effectiveness of docking with and without LABind-predicted binding sites. As shown in Fig.5, docking using LABind-predicted binding sites improves the average top-20 success rate by 19.1% over blind docking. However, it still underperforms by 21.9% compared to docking with the cognate ligand, indicating room for further improvement. These results demonstrate the potential of LABind in enhancing blind docking scenarios and highlight directions for future enhancement.

‘Blind’, ‘Binding sites’, and ‘Cognate ligands’ represent the success rates of blind docking, binding site-guided docking, and cognate ligand-guided docking, respectively.

NSP3 macrodomain is an essential component of the replication and transcription complex. It participates in the replication and infection processes of coronaviruses through various mechanisms and is one of the important factors in the interaction between the virus and the host39. We selected the protein chain from SARS-CoV-2 NSP3 macrodomain (PDB ID: 5SQQ, chain: A)40and the unseen ligand (Chemical ID: QU3), as an example for demonstration. The ground-truth binding sites were derived from the BioLiP2 database. We selected LABind trained on the DS3 dataset to predict the binding site. We utilized LABind and similar methods to predict the ligand binding site of the example protein. Supplementary Fig.2apresents the structure of the protein-ligand complex. As depicted in Supplementary Fig.2b, LABind accurately predicted all binding residues without falsely identifying any additional non-binding residues. The prediction result of LigBind is presented in Supplementary Fig.2c. LigBind predicted 13 binding residues, including 5 true positives and 8 false positives. One binding residue was a false negative, and the incorrectly predicted residues exhibiting a more dispersed distribution on the binding residues. The visualizations of P2Rank, DeepSurf and DeepPocket for this case are also shown in Supplementary Fig.2d–f, respectively. Methods targeting the prediction of binding site centers exhibit a tendency to misidentify non-binding residues. This indicates that LABind not only enables the joint prediction of binding residues and binding site centers, but also offers certain assistance in practical applications.

We wish to justify that the results we presented in the previous sections are not coincidental but result from the intentional design and training of LABind. Therefore, we presented investigations into the protein and ligand features of LABind. We found that the protein representation is the best comprehensive representation of the protein sequence and structure. We also discovered that incorporating ligand attributes significantly enhances the performance of LABind.

Ablation studies were conducted to verify the contribution of each protein feature source of LABind. As illustrated in Fig.6a, we compared the impact of removing four main features—protein embeddings (Ankh features), residue-residue interactions of the protein representations, protein-DSSP embeddings (Ankh features + DSSP features), and protein graphs on the performance of binding site identification on the DS1 test set with six different ligands for demonstration. We selected metal ions (Zn2+and Ca2+), anions (PO43-), and small molecules (ATP, HEM, and GDP) and reported the average AUPR on the DS1 test set. Removing features meant that regardless of the input, we replaced the features with a matrix of all ones. When removing the protein embeddings from LABind, the average AUPR decreases by 0.043 across all 19 ligands (average AUPR of 0.573). This confirms that the absence of protein sequence information leads to a decrease in LABind’s performance. The removal of residue-residue interactions (retention of solely the positional residue-residue indices) results in a 0.132 decrease in average AUPR. Neglecting the relative positions of residues results in inadequate performance of LABind (average AUPR of 0.484). The removal of protein-DSSP features within LABind leads to a decrease of 0.048 in average AUPR. When protein graphs (node spatial features and edge spatial features) are removed, an action that randomizes residue-residue interactions as well as the spatial information of residue atoms, LABind experiences the most significant decrease in AUPR. This ablated model underperforms LABind by 0.132, 0.284, 0.135, 0.200, 0.200, and 0.224 in AUPR on Zn2+, Ca2+, PO43-, ATP, GDP, and HEM test sets. Given the complete lack of protein graph information, the model’s performance declines significantly (average AUPR from 0.616 to 0.433). Hence, it is evident that protein graphs are of paramount importance within the LABind architecture.

aAblation studies on the six ligands in the DS1 test set. ‘w/o embedding’, ‘w/o edge’, ‘w/o dssp’ and ‘w/o graph’ denote removing protein embedding, removing residue-residue interactions, removing protein-DSSP embedding and using only the protein-DSSP embedding as input, respectively. AVG represents the average AUPR across all ligands.bPerformance improvement of LABind over LABind without ligand information on the six ligand test sets in the DS1 dataset. AVG represents average values across all ligands. MCC and AUPR are colored in blue and yellow, respectively. Light colors indicate results obtained without using ligand information, while textured dark colors indicate performance improvements achieved with ligand information. The values indicate the improvement achieved by incorporating ligand information.cPerformance comparison of LABind and LABind trained in a grouped manner. LABind denotes training on all ligands, and Group denotes training with categories based on ions and small molecules.dVisualization of the distributions of residue representations on six ligands before (left) and after (right) the attention-based learning interaction. Non-binding residues refer to amino acids that are not part of the binding sites for these six ligands and are represented in gray. Colored points represent binding residues for their respective ligands (e.g., points labeled Zn2+are Zn2+binding residues).

Ligand features provide pivotal information for identifying binding sites. The MCCs and AUPRs of LABind without ligand information and LABind are illustrated in Fig.6b. The results show that LABind achieved improvements of 9.0–34.4% in MCC and 15.8–57.5% in AUPR on the six test sets over LABind without ligand information. When ligand information is removed, the average MCC and AUPR decreases by 0.110 and 0.152, respectively. The results clearly demonstrate that the model without ligand representations performs significantly worse than LABind. This indicates that cross-attention can effectively integrate features of both ligands and proteins. As presented in Table2, multi-ligand-oriented methods show a distinct difference in recognizing binding sites for ions and small molecules. Therefore, we categorized the DS1 dataset into small molecules and ions for separate training, and compared the performance of these two models on the test set. As shown in Fig.6c and Supplementary Table3, the average MCC of LABind trained on the entire DS1 training set is 3.6% higher than that of LABind trained in a grouped manner. We believe this is because a unified model can learn from a larger and more diverse set of ligands, which contributes to better overall performance, whereas segregated training limits the data available to each model.

In addition to the importance of protein and ligand features for binding site prediction tasks, the integration of features from both ligands and receptors can also enhance prediction accuracy. However, effectively integrating these features remains a challenge. The deep learning architecture of LABind helps to quantitatively solve this integration task. To validate the roles of cross-attention and the graph network, we visualized the distributions of residue representations on the DS1 test set of Zn2+, Ca2+, PO43-, ATP, GDP and HEM using t-SNE41. We performed non-linear dimensionality reduction visualization on the residue representations to demonstrate the positive impact of the attention-based learning interaction on the prediction task. Due to the large number of residues in the test sets, we randomly selected 50 binding proteins for each ligand. As shown in Fig.6d, the representations of binding and non-binding residues before the attention-based learning interaction exhibit highly overlapping distributions, making them difficult to distinguish. In contrast, the residue representations after the attention-based learning interaction considerably improve the discrimination between binding residues and non-binding residues. Additionally, a noticeable gap is observed between the main distribution areas of binding residues and non-binding residues, indicating that LABind effectively captures the differences between them. LABind clusters the binding residues of different ligands into the same regions because the optimization objective during model training is to distinguish whether a residue is part of a binding site.

In this study, we proposed LABind, a structure-based method to predict the binding sites of proteins for all ions and small molecules in a ligand-aware manner. LABind utilizes a cross-attention mechanism to facilitate interactions between ligand representations and residue representations, learning the intrinsic relationships of protein-ligand binding. Additionally, LABind integrates the atomic positions of proteins with embeddings from pre-trained language models, achieving a robust representation of proteins. By utilizing graph networks, it effectively learns the domain information of protein graphs. LABind is not only capable of accurately predicting unseen ligands but also outperforms competing methods in specific ligand recognition. Although LABind is a structure-based method, we also developed a program that enables a sequence-based approach by utilizing structures predicted by ESMFold.

To comprehensively analyze the performance of LABind, we conducted multiple experiments: comparative experiments on three benchmark datasets and one additional dataset; demonstrations of the same protein binding with different ligands; auxiliary molecular docking experiments; predictions of binding sites for the SARS-CoV-2 NSP3 macrodomain protein with an unseen ligand; ablation studies; and visualizations of hidden residue representations. We first benchmarked LABind against other single-ligand-oriented methods using the DS1 dataset, and LABind achieved an improvement of 0.109 in the average MCC compared to LigBind. We further trained and tested LABind on a multi-ligand-oriented dataset DS2, and the predictive performance of LABind also surpassed existing multi-ligand-oriented methods, demonstrating its competitiveness in predicting known ligands. To test the effectiveness of LABind on unseen ligands, we additionally established a dataset DS3 for training and testing unseen ligands. Despite the increased difficulty, compared with LigBind, LABind demonstrated an average improvement of 0.131 in MCC and 0.125 in F1, showing both high precision and robustness. Furthermore, LABind outperformed competing methods in binding site center localization across three benchmark datasets and the COACH64 dataset. Specifically, LABind achieved an average increase of at least 8.0% in DCC success rate and 6.5% in DCA success rate across all evaluated datasets. Through the application of LABind in predicting binding sites for the same protein with various ligands, we demonstrated that LABind can accurately differentiate the binding sites among distinct ligands. Moreover, when combining the binding sites identified by LABind with Smina, we found that these binding sites can effectively assist in molecular docking tasks. Additionally, a case study on SARS-CoV-2 NSP3 macrodomain-related proteins highlighted LABind’s competitiveness in practical applications. To uncover the roles of input features and computational components in LABind, we conducted ablation studies and visualized residue representations derived from the attention-based learning interaction. Our exploration of the computational characteristics of LABind yielded several key insights: (1) The combination of Ankh, DSSP, and spatial information into a graph is the most effective representation for LABind; (2) The involvement of ligand representations is key to LABind; (3) Training all ligands collectively can be beneficial for LABind. By visually analyzing the evolution of protein features before and after the attention-based learning interaction, it is evident that LABind can effectively capture binding site information through protein-ligand interactions.

The features, design, and training of LABind result in consistent performance improvements across datasets. This achievement can be attributed to several key factors: First, the rich feature representation reduces the difficulty of fitting. Second, cross-attention effectively integrates ligand features with protein features. Third, LABind is a unified model capable of handling various types of ligands, which avoids the need to train a separate model for each ligand. This multi-ligand capable model captures both shared representations across different ligand binding sites and distinct representations specific to each binding site. However, LABind currently lacks the capability to identify protein-macromolecule binding sites. In summary, we believe that LABind provides certain assistance in the work related to protein binding sites, and it can offer help in understanding protein functions and in the field of drug design. It can also help narrow down the search range for other tasks related to binding sites, such as molecular docking. In the future, we will further expand our method to predict binding sites for other macromolecules and other functional sites, such as phosphorylation sites, peptide binding sites, and protein-protein interaction sites, to provide more comprehensive and effective assistance in biological research.

The dataset created by LigBind contains ligand-specific sets of 1159 ligands, pre-training sets consisting of 1301 ligands, and unseen sets of 16 ligands. We selected 19 ligands from LigBind’s ligand-specific dataset to construct the DS1 benchmark dataset. The details of the DS1 dataset are described in Supplementary Note7. The proteins in these datasets were derived from the BioLiP42database, released in February 2021. To ensure low sequence similarity between the training and test sets, redundant proteins sharing more than 30% sequence identity with any protein in the test set were removed using CD-HIT43. Proteins were divided into training and test sets based on their release date relative to January 2017, with those released before this date assigned to the training set and those released after assigned to the test set. In this study, we selected a subset of ions and small molecules from the LigBind dataset to test and evaluate the performance of LABind. Notably, the ligands in the test set appear in the corresponding training sets.

The dataset was composed of 10 ligands including biomacromolecules (DNA, RNA, peptide and protein), small molecules (ATP and HEM) and metal ions (Zn2+, Ca2+, Mg2+, and Mn2+). We focus on the identification of binding sites for proteins with small molecules and ions. Therefore, we excluded biomacromolecules to construct the DS2 dataset, which ultimately includes six ligands (ATP, HEM, Zn2+, Ca2+, Mg2+, and Mn2+). The small molecule dataset and the metal ion dataset were constructed from the BioLiP database released on 29 March 2023 and 29 December 2021, respectively. For small molecule datasets, proteins released before 1 January 2021 were assigned to the training set, while the remaining proteins were assigned to the test set. For metal ions datasets, proteins released before 1 January 2020 were assigned to the training set, while the remaining proteins were assigned to the test set. Supplementary Note8shows the details of the DS2 dataset. Consistent with DS1, the ligands in the test set also appear in the corresponding training set.

To ensure the predictive accuracy of the model on unseen ligands, we constructed a dataset (DS3) from the BioLiP2 database21released on April 3, 2024. The DS3 dataset is a comprehensive dataset for predicting protein binding sites with small molecules and ions. Figure7illustrates the construction process of the DS3 dataset. First, we removed protein chains with resolutions of  >3.0 Å and lengths of  >1500 residues. Similar to previous studies14,17, we used MMseqs244to remove redundancy from receptor proteins. Proteins with protein sequence identity below 30% and alignment coverage greater than 30% were retained. Next, we used protein data released before January 1, 2021, for the training set. Data released after this date that contained ligands present in the training set were used as the picking set, whereas those containing ligands not observed in the training set were used as the test set. Finally, we removed protein sequences from the training and picking sets that shared more than 30% sequence identity with the protein sequences in the test set. This approach ensures that the ligands in the DS3 test set were not seen in the training set and that the protein structures are sufficiently distinct, thereby enabling a better assessment of the model’s generalization capability on unseen data. We removed ligands with fewer than two binding protein chains in the training, picking, and test sets to reduce potential bias introduced by individual binding samples. We used the training set to train the model’s weights, the picking set to determine the threshold for binary classification, and the test set to evaluate the model’s generalization performance on unseen ligands. Due to anomalies encountered with certain ligands and proteins when utilizing the LigBind online server, which rendered them unprocessable for prediction, these ligands were consequently excluded from the DS3 test set. A description of anomalies with the LigBind web server is provided in Supplementary Note9. Specifically, the training set contains 21651 protein chains and 3651 ligands; the picking set contains 4024 protein chains and 1229 ligands; and the unseen test set contains 1049 protein chains and 474 ligands. Supplementary Note10and Supplementary Table4present MMseqs2 command operations and the Chemical IDs of the unseen ligands in the test set, respectively. Here, a residue is defined as a binding residue if the minimum atomic pair distance between the residue’s non-hydrogen atoms and the ligand’s non-hydrogen atoms is less than the sum of their Van der Waals radii plus 0.5 Å.

aThe screening process from the BioLiP2 database.bThe process of splitting DS3 dataset.

Consistent with previous studies16,17, DS1 and DS2 datasets were divided into training/test sets according to stringent sequence diversity thresholds (maximum 30% sequence identity between test and training sequences). Each training set was partitioned into five folds. For the same set of hyperparameters, five models were trained on four folds and validated on the remaining fold. Notably, the validation was only performed on the fold against those other four training folds based on the training set, without using the held-out test set. The final trained models with the optimized hyperparameters were used in the prediction task on the test set. The final predictions on the test set were obtained by averaging the outputs of the five converged models. The DS3 dataset was also divided into training/picking/test sets following sequence diversity thresholds. Additionally, none of the ligands in the test set appear in the training set. The picking set of the DS3 dataset is only used to determine the cutoff thresholds for binary classification on DS3 test set when required by certain evaluation metrics (e.g., MCC and F1). We emphasize that the picking set and the training set are both sufficiently distinct from the test set.

LABind utilizes the protein pre-trained language model Ankh to obtain protein sequence embeddings without multiple sequence alignment. Ankh is an encoder-decoder transformer45with ConvBERT46for generating amino acid-level embeddings. Ankh is an optimized protein language model focused on improving performance through protein-specific optimization rather than just scaling up the model size. We use the 1536-dimensional embeddings generated by Ankh as protein embeddings. Compared with ESM-2 (3B), Ankh occupies less GPU memory, enabling it to process proteins with longer sequences. LABind utilizes some structural properties predicted by DSSP, including relative solvent accessibility (RSA) and secondary structure types. We integrate protein embeddings from Ankh with DSSP features to construct protein-DSSP embeddings, which are subsequently combined with spatially-based protein graphs to generate the final protein representations.

MolFormer can learn molecular structures solely from SMILES sequences without precise 3D data. Its framework primarily utilizes rotary positional embeddings and linear attention mechanisms, making it both scalable and computationally efficient. Therefore, for a ligand, LABind leverages MolFormer to obtain a 768-dimensional embedding of the ligand through its SMILES sequence. The ligand representations are directly derived from MolFormer’s outputs.

Here,xis the original feature vector,xnormis the normalized feature that is finally input into the model, andxminandxmaxare the minimum and maximum values of the feature type on the training dataset, respectively.

whereQirepresents the residue coordinate system,biis the negative bisector of the angle formed by the Ni, Cαi, and Ciatoms, andniis a unit vector normal to this plane.uiandviare computed from the backbone atomic coordinates, andxrepresents the position of the corresponding atom. Fig.1b shows the architecture of the graph converter module.

(i) Node spatial features. Using the residue coordinate system, with Cαas the origin, we calculate the distance and direction of each atom within each residue to supplement the pre-trained embeddings. Given the clear correlation between protein-ligand interactions and surface features48, we calculate the protein surface using MSMS49and compute the distance from each residue to the nearest atom S on the surface. Thus, a residueiis composed of an atomic set {Ni, Cαi, Ci, Oi, Ri, Si} (Rirepresents the centroid of all heavy atoms in the side chain). We calculate the internal distance and directional features based on the residue coordinate system. We use a Gaussian radial basis function RBF( ⋅ ) to encode pairwise atomic distancesD, and then concatenate the resulting distance embeddings accordingly. For the directional feature, the directions from the Cαatom to other atoms are calculated, then transformed into the residue coordinate system, and finally flattened into a vector. The directional feature is composed of components from[dNi,dCi,dOi,dRi,dSi], for example,dRi=QiTxRi−xCαi∥xRi−xCαi∥. Additionally, since the protein backbone consists of the atoms N, Cαand C, we calculate the bond angles and dihedral angles for the atoms on the backbone.

(ii) Edge spatial features. Based on the residue coordinate system, the relation between the neighboring coordinate systems is mainly determined by Cα. Similar to node spatial features, edge spatial features include directions, distances, and orientations. For edgeeij, we calculate the distances from all atoms ({Nj, Cαj, Cj, Oj, Rj, Sj}) in residuejto the Cαatom of residuei, as well as the directional information based on the coordinate system of residuei. Namely, the directional feature isQiTxAj−xCαi∥xAj−xCαi∥, and the distance feature isRBF(∣∣xAj−xCαi∣∣), where Ajrepresents all atoms in residuej. The spatial relationship between different residue coordinate systems is represented by orientation featureq(QiTQj), where an orientation encoding function q( ⋅ ) is employed to encode the quaternion representation of the spatial rotation matrixQiTQj50.

The attention-based learning interaction is the most crucial component in LABind, with its specific framework shown in Fig.1c. Multi-head neighbor-attention and multi-head cross-attention are two key modules for learning protein representations and interacting ligand information.

Here,hi,hj, andhi′are the representations corresponding to the residues in the protein at the layerl.eijrepresents the interaction between residueiand residuej.WQ,WK, andWVare the learnable weight matrices.dis the dimension of the hidden layer, and ∥ denotes vector concatenation. Additionally, we apply multi-head attention to transform residue representations and residue-residue interactions multiple times to capture diverse information. The outputs are directly concatenated and linearly projected using a learned matrixWO.

Here,mrepresents the ligand representation,h″represents the updated residue representations,αmrepresents the attention coefficients computed between residue representations and the ligand representation, andsoftmaxrefers to the softmax function. In this work, we employ the multi-head cross-attention mechanism, and the outputs from these heads are concatenated.

Here,xrepresents the input, and the learnable parameters include weights (W1,W2) and biases (b1,b2). SiLU52represents the sigmoid-weighted linear unit activation function. Additional model algorithms are provided in Supplementary Note11.

We used grid search to optimize the hyperparameters of the network. Given that the DS2 training set is the smallest, we optimized the hyperparameters using this dataset. The AUPRs corresponding to different hyperparameters on the DS2 dataset are provided in Supplementary Table5. Specifically, we employed a four-layer graph transformer framework with cross-attention modules, four attention heads, and 256 hidden units. The hyperparameters were as follows:ϵ=0.1, Topk= 30, Nrbf= 8. Here,ϵ, Topk, and Nrbfrepresent the ratio of Gaussian noise added during training, the number of neighboring residues in the protein representation, and the output dimension of the radial basis function, respectively. Dropout rate and attention dropout rate were set to 0.3 and 0.1 to avoid overfitting, respectively. For model optimization, we employed Adam optimizer53with the NoamOpt strategy45. The model was trained using the binary cross-entropy loss, with a peak learning rate of 4 × 10−4and a warmup period of 11 epochs. We trained the model for 70 epochs and employed early stopping if there was no improvement in validation performance for 10 epochs.

We implemented LABind based on Python v3.8 and Pytorch v2.2.154. The versions of other software used are provided in Supplementary Note12. We trained LABind for the DS1 dataset on two Nvidia A100 GPUs, and for the DS2 and DS3 datasets on five Nvidia GeForce RTX 2080Ti GPUs. The choice of hardware depended on the longest protein length in each dataset.

Further information on research design is available in theNature Portfolio Reporting Summarylinked to this article.

This work was supported in part by the National Natural Science Foundation of China Fund under Grants (62272335 to L.Q. and 62002251 to T.W.), in part by Jiangsu Colleges and Universities QingLan project (L.Q.), in part by University-Industry Collaborative Education Program (L.Q.), in part by Project Funded by the Priority Academic Program Development of Jiangsu Higher Education Institutions, and in part by the Collaborative Innovation Center of Novel Software Technology and Industrialization (L.Q., T.W., and Q.L.).