Growing dementia prevalence underscores the need for efficient screening methods, but lengthy digital assessments often cause fatigue among older adults. To address this, we developed and validated the Digital Assessment of Cognitive Impairment (DACI), a brief mobile application designed to accurately identify cognitive impairment (CI). Initially, 304 older adults (272 healthy, 32 cognitively impaired) completed both a pencil-and-paper Cognitive Impairment Screening Test (CIST) and a full-length DACI. The best-performing CatBoost model achieved an area under the curve (AUC) of 0.813, with sensitivity of 0.903, requiring an average completion time of 321 s. Subsequent feature selection identified two essential subtests for a compact DACI. This compact version was then validated with an additional 297 participants (227 healthy, 70 cognitively impaired), achieving improved diagnostic performance (AUC = 0.871) in only 91 s. DACI effectively distinguishes cognitively impaired older adults with minimal test duration, potentially reducing fatigue and improving adherence. By providing rapid, accurate remote assessment without additional equipment, DACI may significantly enhance accessibility of cognitive screening among older adults. Future studies are warranted to validate DACI’s feasibility and performance in unsupervised home settings.

The online version contains supplementary material available at 10.1038/s41598-025-14130-9.

The global prevalence of dementia is projected to reach 152 million by 20501. Early detection and timely intervention for cognitive impairment (CI) can delay significant functional decline in Alzheimer’s disease (AD)2–11. Recent medical advances, including FDA-approved treatments, have shown promising outcomes in slowing cognitive decline12, making early screening crucial for effective intervention13.

Typical cognitive assessments require qualified neuropsychologists, appropriate facilities, and accessible locations14. However, the growing aging population and anticipated healthcare professional shortage might limit access to in-person assessments4,15. Mobility challenges, economic constraints, and infection risks further restrict service availability16–18. Additionally, the expertise required for scoring and interpretation limits daily assessment capacity19.

Researchers have explored digital technologies for cognitive screening to address these challenges20–25. The COVID-19 pandemic accelerated the development of remote cognitive assessments and app-based testing8,26,27. These digital tools enable remote participation through web or mobile applications with minimal staff assistance. Digital biomarkers in addition enhance assessment resolution, increasing sensitivity and accuracy3,23,28–37. In conjunction with this, these tools facilitate rapid result delivery through AI and data science, analyzing both active user inputs and passive digital biomarkers automatically38–45. These advances improve both accessibility and resource utilization efficiency.

However, these digital tools face early-stage challenges, particularly in usability. Poor user experience design and small interfaces impede adoption among older adults46,47. Lower digital literacy in older populations compounds these difficulties, whilst prolonged engagement can increase cognitive and physical fatigue48,49. Consequently, large-scale controlled studies have been reluctant to adopt digital tools, leaving their effectiveness largely unvalidated3. Whilst digital applications are recognised for conserving resources41, concerns persist about performance inconsistencies and variable task duration across age groups50.

Developing optimised digital tools that maximise screening accuracy whilst minimising test duration becomes crucial within this context. The potential for digital assessment tools to distinguish cognitive impairment from healthy controls more accurately in reduced time offers significant promise for consistent performance across age groups. In this regard, this paper aims to develop and validate the Digital Assessment of Cognitive Impairment (DACI)—a cost-effective, mobile-based tool for remote cognitive impairment screening in older adults. Using machine learning, DACI detects cognitive decline without additional sensor equipment. We also introduce a compact DACI version with fewer tasks that maintains accuracy whilst reducing administration time. We anticipate this solution will demonstrate consistent performance and high validity for cognitive impairment assessment.

Building on this model, we conducted an optimisation process to identify the minimal set of tasks within DACI by assessing the relative importance of each test. This involved enumerating the feature importance from the collected data. We then applied constrained optimisation, utilising an exhaustive search algorithm aimed at minimising assessment time. The ultimate objective was to identify the optimal set of features that could achieve predictive performance statistically equivalent to that of the best AUC model.

Finally, the compact version of DACI, compiled from the selected tests, was subsequently employed in a follow-up experiment involving an additional 297 participants. Data collected from this experiment were used to develop a new predictive model using machine learning (ML) algorithms. Afterwards, we evaluated the performance of the most accurate predictive model by comparing its screening accuracy and completion time to those of the original model, which was trained on data from the full-length DACI.

The DACI is, aforementioned, a mobile-based screening tool for cognitive impairment. It primarily consists of four cognitive tasks, each designed to evaluate key cognitive domains, including executive function, associative memory, working memory, and calculation ability. We designed a dedicated task for each domain as described below. Participants performed the multiple sessions each task (Table1).

Based on a comprehensive and systematic review, we developed DACI as an expanded digital version of the pencil-and-paper-based CIST test. We digitized the original test and enhanced each task according to findings from our literature review. Through our analysis, we identified executive function, associative memory, working memory, and calculation as the cognitive functions most relevant for early detection of Alzheimer’s disease (AD). We subsequently selected and digitized appropriate cognitive tasks for each domain. These tasks included the Stroop test, symbol association, self-ordered pointing task, and arithmetic tasks (cognitive domains and DACI tasks in Alzheimer’s disease are shown in Supplementary Table1).

The National Institute on Aging-Alzheimer’s Association reported that pathophysiological changes in the medial temporal lobe, including the hippocampus, might be associated with probable AD and dementia51. We therefore focused first on addressing cognitive domains relevant to hippocampal functions, then designing neuropsychological tests based on these functions.

A previous study found that the newly developed CQ score, which incorporates the Stroop test and simple calculation, showed significant correlation with hippocampal volume52. Supporting this finding, researchers demonstrated that Stroop interference effects were significantly greater for the MCI group compared to the cognitively normal (CN) group53,54. Based on these findings, we selected and digitized the numeric Stroop test for inclusion in DACI.

We also considered that AD is highly correlated with the diagnosis of amnestic mild cognitive impairment (aMCI). According to multiple studies, many individuals develop AD within 3–6 years after receiving an aMCI diagnosis55–57. Early cognitive changes in aMCI can be particularly sensitive to associative recall58, which led us to include the symbol association test.

Researchers also found that prodromal decline in working memory preceded decline in other cognitive domains59,60. Based on this evidence, researchers have used the SOPT to examine changes in working memory associated with normal aging61–64. These findings led us to include SOPT in DACI.

Early manifestations of AD include calculation deficits. Compared to patients with mild AD, patients with moderate AD performed worse on written arithmetic tasks (P< 0.002). Patients with mild AD showed preservation of single- and multiple-digit addition and subtraction operations, as well as single-digit division operations, on WRAT-3 problems. In contrast, only single-digit addition was preserved in patients with moderate AD65.

To be effective as a screening test for older adults, the tool was designed to be completed in approximately 5 min, considering the importance of minimizing fatigue48,49. Additionally, we employed an auditory instruction technique that has been proven effective for older populations66.

To assess executive function, we developed a Numeric Stroop task adapted from the past research on Stroop test52,67. Mismatched digits and the number of digits served as the main stimuli in the Numeric Stroop task (e.g., 1111, 111, 444, 3, 44, 2), although matched stimuli (e.g., 333, 22, 4444) were occasionally included.

The task consisted of two phases, each comprising ten questions. In the first phase, participants were asked to select a number that matched the one presented on the screen. For example, if ‘7’ was displayed, participants were required to select the same number, ‘7’, as the correct response. During the first phase, we designed the task to elicit the reverse Stroop effect. This effect occurs when the unattended dimension (i.e., the number of digits) interferes with the processing of the attended dimension (i.e., digit identity)68. To elicit this interference, the task was designed so that participants required more time to read incongruent digit stimuli (e.g., 111, 33, 4, 2), thereby demonstrating the reverse numeric Stroop effect.

In the second phase, participants were asked to count the total number of items displayed and select the matching quantity. For example, if ‘111’ appeared on the screen, participants were expected to choose ‘3’ representing the total count of the displayed digits (Fig.2A). During this phase, we designed the task to elicit the Stroop effect. For this effect to emerge, the unattended dimension (i.e., digit identity) must interfere with the processing of the attended dimension (i.e., numerosity)68. Accordingly, the task was designed to induce longer response times when participants counted incongruent digit stimuli (e.g., 111, 33, 444, 2), thus producing a numeric Stroop effect.

Snapshot of the DACI tasks. (A) Numeric Stroop task; a screen of part A. (B) Remembrance of paired symbols task. (C) Quantitative Comparison task. (D) Find an Unselected Symbol task.

TheRemembrance of paired symbols taskwas designed to assess associative memory function69. The task consisted of two phases: an associative learning phase and a quiz phase. During the associative learning phase, participants were instructed to memorise six associations sequentially displayed on the screen, each consisting of two symbols (Fig.2B-left). Six pairs of symbols were presented in the initial session, with each pair shown for 4 s. In the quiz phase, participants were asked to select the symbol in the right panel that was associated with the symbol displayed in the left panel (Fig.2B-right). This phase involved six matching tasks, corresponding to the six symbol pairs shown in the learning phase. In this regard, participants were required to answer six questions, each assessing their capacity to recall the previously memorised symbol associations.

To assess calculation capacity, we designed aquantitative comparison taskbased upon themulti-digit written arithmetic testin the Wide-Range Achievement Test-3 (WRAT-3)65. Here, participants were asked to select the equation with the larger outcome expected from two presented options. If the outcome of both equations were identical, participants were instructed to select the ‘equal’ button (Fig.2C). The task included three levels of difficulty, with each level consisting of four questions, which resulted in a total 12 questions.

The Find an Unselected Symbol task was designed to assess working memory based upon the Self-Ordered Pointing Task (SOPT)70,71. We note that Petrides et al. and Geva et al. indicate that only high memory load trials are useful in distinguishing between individuals with and without hippocampal damage70,71. In line with this, the DACI does not include memory loads exceeding 6. In this task, a set of six-item stimuli was presented in a 2 × 3 array. Whilst the layout of the stimuli remained constant across screens, their positions were randomised (Fig.2D). The participants were asked to select a stimulus item on each screen that had not been selected in previous. The participants were not allowed to select the same location on three consecutive screens to prevent them from employing a strategy that simplified the task. The task comprised three trials, each containing six questions.

To conduct experiments using the DACI, we recruited 304 older adults (272 with normal cognitive function and 32 with cognitive impairment) from four domestic institutions (see Supplementary Sect.Ifor more details on the institutions). Ethical approval for this study was obtained from the Institutional Review Board (IRB) of Ewha Womans University Mokdong Hospital (EUMC 2021-08-011) and that of Yonsei University (IRB #7001988-202110-HR-1397-03). All study procedures were conducted in accordance with the relevant guidelines and regulations, including the Declaration of Helsinki and applicable local ethical standards. Table2shows the demographic information of the participants.

Demographic characteristics of the study participants.

*For Age, Education, and CIST score, homogeneity of variance was first assessed with Levene’s test (α = 0.05); when variances were equal we applied the standard independent-samples t-test, and when unequal we used Welch’s t-test. Gender differences were evaluated with a χ2test. “n.s.” indicates non-significant findings.

All participants were aged 50 years or older and were able to read and write. The exclusion criteria were defined as the presence of any of the following characteristics: (1) suspected or diagnosed major neurological or psychiatric illnesses, (2) severe visual or hearing impairments that interfere with an appropriate response, or (3) any other significant medical conditions. Table2summarises the demographic characteristics of the participants.

Written informed consent was obtained from all participants. Under the guidance and supervision of medical staff, all participants were first asked to complete a pencil-and-paper-based Cognitive Impairment Screening Test (CIST) for preclinical validation. The CIST was developed by National Institute of Dementia in South Korea, which is recognised for its culturally and ethnically tailored adaptation of the Mini-Mental State Examination (MMSE) for the South Korean population (see Supplementary Sect.IIfor more details).

Based on the results of the CIST, neurologists categorised the participants as either cognitive impairment (CI) or healthy controls (HC). The cutoff score for the categorisation is as follows—participants were categorised as CI if their CIST scores fell 1.5 standard deviations below the age- and education-adjusted norms72.The normative data used for this classification were organized by both age and education level. This ensured that participants were compared against age- and education-appropriate norms, thereby controlling for demographic factors known to influence neuropsychological performance when determining CI vs. HC classification. (see Supplementary Table2for the age-and education-adjusted CIST norms).

All participants subsequently completed the digitised cognitive assessment using DACI on the same day. DACI automatically recorded all behavioural and performance data, including response time, lead time, correct or incorrect status. Behavioural data refer to task interaction measures such as response time for each question and time spent on tasks, while performance data indicate response accuracy, including the total number of correct answers and question-level correctness. These data were later used to construct datasets for developing predictive models with machine learning (ML) algorithms. The behavioural and performance data were labelled according to the classifications provided by neurologists (e.g., CI or HC).

We note that the CIST consisted of 13 questions. It was expected to take approximately 10 min whereas the DACI took approximately 6 min. On average, both assessments were completed within 16 min. The duration of DACI for each participant was computed based on the system logs. It is, on average, 321 s (approximately 6 min).

Using the combined logs and test results labelled with participant classifications by neurologists, we developed predictive models to screen for cognitive impairment through various ML algorithms. The primary objective was to identify the model that achieved the highest classification accuracy based on participants’ data collected from DACI.

Given assessment tests in DACI, it collected a total of 84 features to classify participants as either CI or HC. In the first phase of theNumeric Stroop task, 18 features were extracted, including lead time, the number of correct responses, reaction times for all 10 questions, and descriptive statistics of the reaction times (e.g., sum, mean, standard deviation, minimum, maximum, and median). The same set of features was extracted during the second phase of the task. For theRemembrance of Paired Symbols task, 14 features were extracted, comprising lead time, the number of correct responses, and reaction time for each of the six items. Additionally, we used descriptive statistics on the reaction times for the questions. For thequantitative comparison task, we used 31 features, including the total number of correct responses, correctness status for each of 12 questions, and reaction time for all 12 questions. The same set of descriptive statistics on the reaction time was used, consistent with the previous tasks. Lastly, we extract three features from theFind an Unselected Symbol task. The lead-time for each of the three trials was computed for each feature.

We used scikit-learn (v0.24.2) to train and test the models, experimenting with ten ML algorithms commonly employed in research73,74. The algorithms includedCatBoost,Bagging,Logistic Regression,Light GBM(LGBM),Naïve Bayes,XGBoost,Random Forest,Gradient Boosting,Support Vector Machine (SVM)with two kernels:Radial Basis Function (RBF)and aLinear kernel.

For the descriptive features (inputs), we used all 84 features. The total number of samples was 304, identical to the number of participants. To train and evaluate the predictive models, we randomly split the entire dataset into training and test sets, based on aRepeated Stratified K-foldapproach. Overall, 80% of the data (HC = 218; CI = 26) were allocated for training, and the remaining 20% (HC = 54, CI = 6) were used for the test set. All features were standardised (see Supplementary Sect.IIIfor more details on standardization).

We set the number of folds (K) to five and the number of repetitions to four. This configuration enabled the construction of 20 classifiers for each algorithm, resulting in 200 (20 models per algorithm × 10 algorithms) simulations to determine the best model. To evaluate the classification performance, we used two distinct measures: the (i)Area Under Curve(AUC) score and (ii) accuracy (see Supplementary Sect.IVfor more details on evaluation metrics).

A primary objective was to identify the minimal set of subtests required to reduce the duration of cognitive assessment whilst preserving classification accuracy. To achieve this, additional computational experiments were conducted to explore the feasibility of minimising the number of features needed to develop a compact version of DACI.

In the process, we identified a minimal feature set by examining the degree of feature importance in relation to the objective described above. We estimated the importance of all features, which resulted in the selection of the minimal set and its associated subtests. The final step involved validating the subtests that best aligned with this goal.

To reduce the number of features, we considered two metrics for each feature in order to measure the dominance of each one (i.e., the degree to what extent a feature influences classification performance). These metrics included the feature importance in relation to prediction accuracy, andpvalue indicating the statistical significance of a feature when comparing the mean values between HC and CI groups through a statistical test.

To that end, we first estimated a feature importance of all 84 features, which were used in the predictive model (for the screening of HC and CI) as described in Sect. ''Finding the minimal set of subtests to develop the compact DACI''. We subsequently measured the importance of each feature using the best-performing classifier (CatBoostin this case; see Sect.Performance on predictive models trained on the full-length DACI datafor more details).

The feature importance represents the extent to which a prediction changes, on average, when value of a specific feature is changed. The higher the value of importance is, the greater the change in the prediction is, on average, when the feature is changed. We computed the feature importance using Eq. (5) in Supplementary Sect.V.

For each feature, we conducted Levene’s test (α = 0.05) to assess homogeneity of variances between the HC and CI groups; where Levene’s test was non-significant (p> 0.05) we applied Student’s independent two-sample t-test, and where it was significant (p≤ 0.05) we used Welch’sttest. Given that both HC and CI groups exceedn= 30, we relied on the central limit theorem to justify approximate normality of the sampling distribution of the mean. This procedure allowed us to determine whether any significant differences existed between feature values of the HC and CI groups.

To identify the minimum number of subtests required to maintain an optimal classification performance, we constructed various combinations using the original 84 features. Rather than employing all those features, we selected 41 features that demonstrated statistically significant differences in feature value between the HC and CI groups. We applied a baselinepvalue threshold of 0.01 to each feature. As a result, we successfully identified 41 features that were significantly different between the two groups.

Next, we generated candidate feature sets using a mathematical combination technique applied to the selected features. This approach involved systematically exploring all possible combinations rather than relying on random selection. Subsequently, candidate feature sets were generated using mathematical combination techniques applied to the 41 selected features. This process involved systematically exploring all possible combinations rather than relying on random selection. A cumulative approach was instead employed, with taking into account thepvalue of each feature into the selection process.

Initially, we sorted the features in ascending order based upon theirpvalue. The generation of candidate feature sets began with the inclusion of the feature possessing the lowestpvalue. Therefore, the first candidate feature set contained only a single feature. The second candidate set was compiled by adding the feature possessing the next-lowestpvalue. This iterative process continued, with each subsequent candidate set including the feature directly following in the ranking.

For the illustrative example, let us assume that we have three features—f1,f2andf3withpvalues of 0.001, 0.0001, and 0.01, respectively. We can then obtain a setS= {f2,f1,f3} which reflects the ascendingpvalue. In this setting, the initial candidate feature set is {f2}, followed by {f2,f1}, and finally {f2,f1,f3}. Accordingly, the candidate feature sets were generated as {{f2}, {f2,f1}, {f2,f1,f3}}.

Given the candidate feature sets, we analysed them to identify the minimal feature set that could achieve the highest AUC score. We used the highest AUC score as the upper bound for classification performance. Once the optimal set with the highest AUC score was determined, we conducted further exploration to identify a feature combination that could significantly reduce the duration of cognitive assessment whilst preserving statistically equivalent classification performance (i.e., the performance of the optimal feature set vs., the upper bound).

Let= () represents the candidate feature sets, where the subscript of, ranging from1ton, denotes the index of each set. The functionreturns the duration required to complete the cognitive assessment for a given, whilstrefers to the predictive model that returns the classification performance (e.g., the ROC-AUC score) based upon a set of features. The setrepresents the feature set that leads to achieving the highest classification performance, serving as the upper bound for classification accuracy. Equation (2), representing the constraint, means that statistical tests (e.g.,ttest, Wilcoxon rank-sum test) should indicate no significant difference between the classification performance ofand.

As outlined above, the final outcome of this constrained optimisation is the identification of the optimal feature set that maximises classification performance whilst minimising assessment duration. For example, let us consider= 21; that is the predictive model with a candidate feature set containing 21 features. We denoted it to, which can achieve the highest classification performance, either CI or CH, we assumed in this context.

As an example, let us suppose= 13, which represents one candidate feature set with 13 features. If the predictive modelcan achieve statistically the same highest AUC score as that of, it is obvious that the task involving 13 features will naturally require less time than the task involving 21 features, whilst maintaining statistically equivalent classification accuracy.

Algorithm 1 illustrates the constraint optimisation process, with the final output being the optimal feature set that meets these criteria.

To test and validate the ML model, we applied a Repeated Stratified K-fold method. We set the folds (K) to five and repeated them four times to split the dataset into the training and test sets. To ensure fairness in the data splitting process, we fixed the random seeds across all operations. This allowed us to assess variations in the AUC score as the number of features changed within the same dataset.

Demographic characteristics of the follow-up study participants.

The experiment procedure is absolutely the same as that in Sect.Initial human subject experiment incorporating predictive model using comprehensive version of DACI. Obtaining the written consent form from all participants, we instructed them to complete a pencil-and-paper based CIST. Again, neurologists classified the participants as either CI or HC on the basis of CIST results. Of course the cutoff score is the same as that in Sect.Initial human subject experiment incorporating predictive model using comprehensive version of DACI. The participants at this time performed the compact version of DACI afterwards instead of the full-length, comprehensive version on the same day.

As expected, we used the same methodology to build the predictive model in this experiment. After collecting the user logs and test results, coupled with participant classifications provided by neurologists, we built the predictive models for the screen of CI and HC. The identical set of ML algorithms, the same version of scikit-learn, and the same training and evaluation approach described in Sect.Initial human subject experiment incorporating predictive model using comprehensive version of DACIwere used.

The total number of samples was 297 which was identical to the number of participants. The dataset was split into training and test sets in an 80:20 ratio, the same as that of the initial experiment. The number of folds (K) was again set to five, with four repetitions, resulting in the construction of 20 classifiers per algorithm, enabling 200 simulations (20 models per algorithm × 10 algorithms) to seek the best-performing model. We used two distinct measures to evaluate the performance: the (i)Area Under Curve(AUC) score and (ii) accuracy (see Supplementary Sect.IVfor more details on evaluation metrics).

A key distinction in this experiment was that participants used the DACI compact version. As a result, we anticipated that the number of features collected from the DACI compact version would be lower than that of the DACI full-length version.

Table4shows the AUC, sensitivity, specificity, and accuracy of the best-performing predictive model on average. In this case, the model trained using the CatBoost ML algorithm exhibited the highest performance among the ten candidate models (see Supplementary Fig.S1for the AUC distributions across all models; Supplementary Fig.S2for accuracy distributions; Supplementary Fig.S3for confusion matrices; Supplementary Fig.S4for ROC curves). We set the AUC 0.813 as an upper-bound used for finding a minimal set of subtests to develop the compact version of DACI (The full set of results for the ten candidate models are shown in Supplementary Table3).

Performance of the predictive model trained by catboost on the classification of HC and CI.

Table5presents the results of the degree of feature dominance derived from theCatBoostmodel. The top three features are listed in descending order of dominance, with the complete all results available in Supplementary Table4.

To ensure these results, we conducted a correlation test between feature importance and thepvalue across all features. The test results confirmed a significant correlation (p< 0.05, R2= 0.1144), suggesting that the lower thepvalue is, the higher the feature importance is. We note thatpvalues were used as the primary criterion for selecting the minimal feature set as feature importance often varies depending on the ML algorithm.

Response Time_Part A_Question 5—Refers to the reaction time for Question 5 of theNumeric Stroop Task (Part A).

Response Time_Part A Mean—Represents the average reaction time across all six questions inPart A of the Numeric Stroop Task.

Response Time_Part A Sum—Indicates the total response time for the six questions inPart A of the Numeric Stroop Task.

Lead Time_Total—Denotes the overall time required to complete aRemembrance of Paired Symbols task.

Response Time_Question Sum—Represents the cumulative reaction time to complete all questions in theRemembrance of Paired Symbols task.

Response Time_Question Min—Refers to the shortest reaction time recorded among all questions in theQuantitative Comparison task.

Lead Time_Question 2—Reflects the total time required to complete one set of theFind an Unselected Symbol task.

In addition to the t-test–based feature selection and CatBoost-based feature importance, we further employed SHAP (SHapley Additive exPlanations) analysis to visualize how each feature contributes to the classification outcome. The SHAP summary plots are presented in Supplementary Fig.S5, highlighting the relative impact of key features.

Although the best-performing model in the initial experiment included all 84 features, we actually used only 41 out of 84 as described in the Materials and Methods section. This approach resulted in an AUC of 0.813, raising the optimistic expectation that curating features based on inter-group differences might lead to higher AUC scores.

We thus combined various subsets of 41 features to identify the minimal feature set that could achieve the highest AUC score. The classifier trained with 21 features demonstrated the maximum AUC score of 0.8329. These were derived from three subtests:Numeric Stroop,Remembrance paired symbols, andQuantitative Comparison. It resulted in an average assessment time of 224 s. We set this as the upper bound for the classification performance.

Using Algorithm 1, we identified 13 features that constituted the optimal set which could minimise the cognitive assessment time whilst retaining performance being statistically the same as the upper bound. We subsequently conducted additional tests on assessment duration and AUC scores to ensure that the optimal feature set was selected appropriately. To identify features with no significant effect on AUC scores, we performed a paired samplettest between the upper bound and groups with fewer than 21 features (Fig.3A). The results emphasised that 12 groups were not statistically different from the group utilising 21 features.

Results of the Experiment on Designing a Minimal Subtest Set. (A) The plot displays AUC scores of predictive models trained with varying numbers of features. The AUC score of the best-performing group was significantly higher than that of the others. The X-axis represents the number of features, while the Y-axis indicates the AUC score. Error bars denote the standard error of the mean. (B) The plot illustrates task duration in relation to the number of features. The X-axis represents the number of features, and the Y-axis indicates task duration. Group A includes 16 to 20 features, Group B comprises 2 to 15 features, and Group C consists of a single feature. (C) The plot shows AUC scores in relation to task duration. The number next to each circle dot represents the number of features used. For overlapping values, the feature set with the higher AUC score is displayed to the right. The X-axis represents task duration, while the Y-axis shows the AUC score.

We further conducted a statistical test to identify significant differences in assessment duration across groups (Fig.3B). When using 2 to 15 features, only two subtests—Numeric StroopandRemembrance paired symbols—were selected. In this case, the assessment duration required was 91 s on average. When a single feature was used, theRemembrance of Paired Symbolssubtest was selected exclusively, with an average assessment duration of 47 s.

In summary, seven feature groups—2, 4, 5, 6, 13, 14, and 15—showed no statistically significant difference in AUC score, but exhibited a statistically significant difference in cognitive assessment duration. These seven groups were designated as the best-performing groups (Fig.3C). We then selected the 13-feature group, achieving the highest AUC score of 0.8232, was ultimately selected as the optimal feature set. Through this process, we validated the optimal solution determined by the proposed algorithms. These findings suggest that cognitive impairment (CI) screening can be conducted in significantly less time, as only two tasks are required within the compact DACI.

To further validate the compact version of DACI, comprising only two subtests as derived in Sect. 3.2, an additional experiment was conducted with 297 new participants (227 HC and 70 CI). Following the same protocol as the earlier experiment, participants first completed the CIST, followed by the DACI. Using these newly collected data, we performed the same approach (Repeated Stratified K-fold) to evaluate as in the initial experiment.

Table6presents the performance of the CatBoost-based model, which once again demonstrated the highest prediction capacity. The results confirm that the compact version of DACI sustained statistically the same prediction performance as that of the full-length DACI. These findings suggest that the simplified DACI can serve as a reliable and user-friendly screening tool that retains essential sensitivity for detecting CI whilst significantly reducing assessment duration.

Prediction performance with a compact set of DACI on screening HC and CI (N= 297) using the catboost model.

The development of DACI was based on a comprehensive review of existing digitized cognitive assessment tools for detecting Alzheimer’s disease symptoms. This review guided the selection of task components that were both theoretically sound and practically appropriate for rapid digital assessment. In designing DACI, we placed particular emphasis on incorporating tasks that are sensitive to early cognitive changes. These changes occur specifically in executive functioning51–54,67,68, associative memory58,69, calculation52,65, and working memory59–64,70,71.

After developing the tool, we evaluated its clinical validity by comparing DACI-derived performance outcomes with neurologist classifications using a standardized cognitive screening instrument (CIST). In this study, we found that data collected from the carefully designed DACI effectively distinguished older adults with CI from HC. Among the predictive models trained on all 84 features from 304 participants, the CatBoost algorithm achieved the highest AUC of 0.813, with a sensitivity of 0.903 and a specificity of 0.733 (Table4). This result demonstrated that DACI effectively distinguished between the cognitively impaired and healthy individuals, supporting its diagnostic relevance75–80.

We selected CatBoost as the primary predictive model for DACI based on its superior sensitivity—a critical factor in screening tests81,82. Previous studies have reported that remote digital cognitive assessments achieved an accuracy of 0.84, sensitivity of 0.86, and specificity of 0.83 for classifying CI from HC participants83. In comparison, DACI’s performance using neuropsychological tests alone demonstrated moderate superiority in screening sensitivity for individuals at risk of cognitive impairment compared to methods proposed in previous studies75,76, while achieving competitive accuracy with early screening approaches77that incorporate diverse modalities78–80. Our findings suggest that DACI represents an effective screening tool for cognitive impairment in older adults, with demonstrated high sensitivity. However, DACI’s specificity was lower than that reported in previous studies, indicating that further research should be required to enhance this performance metric.

In addition, we developed a compact version of DACI to reduce completion time without compromising screening accuracy. The compact version exhibited strong potential for achieving superior classification performance (AUC 0.871) compared to previous studies while simultaneously reducing assessment duration. We employed optimization techniques to identify a minimal set of subtests that significantly reduced completion time. Through successive feature refinement, 13 significant features were identified from two subtests. We then conducted an additional round of human subject experiments using DACI-mini with 297 new participants. After collecting user behavioral data, we successfully validated our approach by developing an early screening model that reduced the average task duration to 91 s while achieving optimal AUC performance (0.871).

Given that this cognitive screening application targets older adults, shorter assessment duration enhances sustained attention during cognitive tasks while reducing fatigue, both of which are essential for accurately measuring cognitive function. Minimizing completion time is crucial when developing digital assessment tools for older populations due to the potential for fatigue resulting from prolonged digital device use48,49. In this study, we validated a digital assessment tool that maintains measurement accuracy while significantly reducing completion time. This enhancement improves the tool’s practical utility in clinical and community settings, where time-efficient screening is essential84–86.

However, a potential limitation is that the results from the compact DACI might be slightly different from those obtained with the full-length DACI. Behavioural and performance data during the assessment could be influenced by the order and combination of tasks. For instance, previously completed tasks might be likely to affect performance on subsequent tasks, resulting in a “carryover effect.” Since the task order and composition in the full-length DACI was different from those in the compact DACI, this could bring about some variations in user experience and cognitive processes. Consequently, discrepancies could arise in the behavioural and performance data collected across different versions, raising concerns about feature set inconsistencies.

Mobile-based cognitive assessments offer several advantages over traditional screening methods and other digital devices, including reduced experimenter influence, greater accessibility, minimal need for invasive testing, faster administration, and improved cost- and time-efficiency5. Previous studies have in addition demonstrated that compact, mobile app-based remote cognitive assessments exhibit high validity and reliability in detecting cognitive impairment in older adults29.

Given that usability is a significant limitation of digital cognitive assessments in older adults, particularly those with cognitive impairments, the application of remote cognitive assessments in this population is essential. Many digital screening tools require additional equipment87, which can be challenging to implement in older adults without professional supervision. DACI, however, can be deployed using only participants’ mobile devices that eliminates the need for supplementary equipment or external support. This system has the potential to provide not only cost-effective but also convenient means which can significantly contribute to cognitive assessment accessibility in older adults.

This study had several limitations. First, the relatively small sample size of participants with cognitive impairments is likely to have constrained the results. Larger studies involving more patients with cognitive impairment could enhance the resolution, precision and generalisability of DACI’s screening performance.

In addition, the study was conducted in a semi-supervised setting. In other words, participants had assistance during the installation of the DACI mobile application. At the same time, they were provided with a concise guideline at the point of consent. Supervisors repeated instructions when necessary to address misunderstandings. Although DACI was designed for independent use by older adults in home environments, the study was conducted in an institutional setting to ensure a controlled environment. Despite minimising supervisors’ intervention, it would be evident that assistance was often required. This highlights the need for further preparation in future research to assess DACI’s feasibility in unsupervised environments. It is also necessary to validate the efficacy of the compact DACI in real-world settings.

As an additional usability assessment independent of the main study, we performed System Usability Scale (SUS) evaluations with 75 older adult participants. The average score was 57.53 (SD = 15.06), which falls below the commonly accepted threshold of 68 for acceptable usability88,89. This indicates that DACI’s usability remains somewhat limited, particularly for older users. The relatively low score likely reflects age-related factors such as reduced digital literacy and unfamiliarity with similar technologies. Based on these findings, future research should prioritize improving the user experience to enhance overall usability.

Cognitive impairment was assessed using the CIST. Whilst CIST demonstrated an AUC of 0.83 for distinguishing normal participants from those with MCI72, further research incorporating detailed neuropsychological assessments is warranted to classify cognitive impairment among healthy older adults using DACI.

Despite these limitations, the findings indicate that DACI is a valuable remote cognitive assessment tool, demonstrating high sensitivity in screening for cognitive impairment in older adults.

Below is the link to the electronic supplementary material.

The authors extend their sincere gratitude to the anonymous reviewers who dedicate their time and expertise to evaluate this manuscript. The funding agency was not involved in the study design, collection, analysis, interpretation of data, the writing of this article or the decision to submit it for publication.