The kidney is an important organ that helps clean the blood by removing waste, extra fluids, and harmful substances. It also keeps the balance of minerals in the body and helps control blood pressure. But if the kidney gets sick, like from a tumor, it can cause big health problems. Finding kidney issues early and knowing what kind of problem it has is very important for good treatment and better results for patients. In this study, different machine learning models were used to detect and classify kidney tumors. These models included Decision Tree, XGBoost Classifier, K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM). The dataset splitting is done in two ways 80:20 and 75:25 and the models worked best with the 80:20 split. Among them, the top three models—SVM, KNN, and XGBoost—were tested with different batch sizes, which are 16 and 32. SVM performed best when the batch size was 32. These models were also trained using two types of optimizers, called Adam and SGD. SVM did better when using the Adam method. SVM had the highest accuracy of 98. 5%, then came KNN with 90.4%. This method will help healthcare professionals in the early diagnosis of disease.

The kidney is an important organ that helps keep the body healthy by cleaning the blood, removing waste, extra water, and harmful substances. It also helps control the balance of minerals in the blood and keeps blood pressure normal. But problems with the kidneys, especially kidney tumors, can be a big issue for health around the world. Recent data shows that renal cell carcinoma makes up about 85% of all kidney tumors and is one of the top ten most common types of cancer globally1,2. Finding kidney tumors early and diagnosing them correctly is very important for helping patients live longer and get treatment on time. Traditional ways of diagnosing, like using imaging or looking at tissue samples under a microscope, work well but rely a lot on the skills of doctors and can take a lot of time. In recent years, machine learning has become a strong tool to help with analyzing medical images and classifying diseases3. These machine learning models can pick up on complicated patterns in data, which helps in finding diseases early and may ease the workload for healthcare workers. Several machine learning techniques, such as Decision Trees, Random Forests, Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and ensemble approaches like XGBoost, have been used successfully in different medical classification tasks4–6. Even though these models have shown good results on their own, there is not much research that directly compares them for the specific task of classifying kidney tumors7.

In this study, a comparison of several machine learning models to detect and classify kidney tumors early is done. The models like Decision Tree, Random Forest, KNN, SVM, and XGBoost are used for the evaluation. We test them using different ways of splitting data, varying batch sizes, and different optimization methods. Our findings show that SVM performs best when set up correctly, making it a strong choice for predicting kidney tumors. By carefully testing and improving these models, our work helps create better and faster tools for diagnosing kidney tumors. This can help doctors make better decisions and improve patient care.

Comprehensive Model Comparison: A detailed and thorough comparison of several commonly used machine learning methods—Decision Tree, Random Forest, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and XGBoost—is provided with the goal of early detection and classification of kidney tumors.

Data Splitting Strategies: The different ways of splitting data, like 80:20 and 75:25, affect the performance of various models. This study gives useful information about dividing data influences, the accuracy of classification, and the model’s ability to work well in new situations, which is very important for medical diagnosis.

Analysis of Batch Sizes and Optimizers: The study looks at how different batch sizes, like 16 and 32, and optimization methods, such as Adam and SGD, affect the performance of various models. It provides a thorough analysis that shows how important it is to carefully adjust hyperparameters, something that is often ignored in other research. The results also show how these choices can have a big impact on how well predictions are made.

Optimal Model Recommendation and Clinical Impact: We found that the SVM model, which was fine-tuned using the Adam optimizer and a batch size of 32, performed the best with an accuracy of 98.5%, which is better than other models.

This improved model not only works better technically but also has important real-world medical value. It helps doctors diagnose kidney tumors faster and more accurately, which can lead to better patient care and less work for medical professionals.

The rest of the paper is structured as a literature review in Section “Literature review”, followed by Dataset description and Research methods in Section “Dataset description and research methods”, experiments and results in Section “Experiments and results”, and conclusion in Section “Conclusion”.

Deep learning is a promising new technology for kidney disease detection. It has the potential to recover the primary detection of kidney disease, which can lead to better outcomes for patients. The researchers had performed work on CKD dataset for the diagnosis of kidney disease in human beings. Hannan et al.8had shown CNN based method and had obtained ultrasound images from Philips Mindray -DC 70. Akben et al.9had used preprocessing + KNN with ANN by using the CKD dataset. They had obtained the values of accuracy as 96% and 81%. Polat et al.10had again used the CKD dataset with SVM technique and achieved accuracy as 98.5%. Sinha et al.11had used the CKD dataset with KNN and SVM. They had obtained the values of accuracy as 78.75 and 78.35 respectively on KNN and SVM. Song et al.12had presented CKD diagnosis method. They had used Bagging, RF, and XGBoost methods and had obtained the values of AUC as 0.74, 0.87, 0.87. Qadir et al.13had used DenseNet201 model for the diagnosis of CKD. They had used 12,446 CT images for the diagnosis. They attained the value of accuracy as 99.44%. Swain et al.14shown SVM, RF models for the analysis of CKD. They had achieved the values of accuracy as 99.33% and 98.67% on SVM and RF respectively. Zabihollahy et al.15had presented CNN based method for the diagnosis of kidney disease and obtained accuracy as 83.75%. Ren et al.16had proposed a hybrid neural network for the diagnosis of CKD medical images and obtained the value of accuracy as 89.7%. Table1shows the related work findings of the researchers working on kidney disease.

It might be difficult to interpret imaging tests, like CT or MRI, to distinguish between benign and malignant kidney tumors. Certain benign lesions might resemble dangerous tumors in appearance, possibly misguiding doctors and resulting in needless treatments. Kidney disease detection using deep learning is an exciting new area of research. As a result of its ability to enhance early diagnosis of kidney disease, patient outcomes may improve. Figure1shows the proposed theme of the study.

A comparative analysis of machine learning models has been performed for the classification of kidney disease into normal and tumor, aiding in early diagnosis and improved patient outcomes.

Two data splitting ratios 80:20, 75:25 has been employed on five machine learning models namely KNN, XGBoost, Random Forest, SVM, and Decision Tree. The models performed best with the 80:20 ratio split.

SVM attained the highest accuracy of 98.5% at batch size 32 and performed best when optimized with Adam, making it the most effective model for kidney tumor classification, followed by KNN with 90.4%.

The study starts by gathering images of kidney tumors as shown in Fig.2. A comprehensive dataset comprising 12,446 images, which serves as the foundation for training and evaluating multiple machine learning models. Next, the images go through some preparation steps, like resizing them to 224 × 224 pixels and adjusting their values to a standard range. After that, features are extracted from the images to form feature vectors, which are then used as input for different types of classifiers like K-Nearest Neighbor (KNN), Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF), and XGBoost. Testing is done using different ways to split the data like 80:20 and 75:25, different optimizers to update the model like Adam and SGD, and different batch sizes for processing data like 16 and 32. The results show that the SVM classifier works best when paired with the Adam optimizer and a batch size of 32, showing good performance on various sets of data.

Dataset sample (a) Normal, (b) Cyst, (c) Tumor, (d) Stone.

Table2summarizes the number of images for each disease class in the kidney dataset, along with their corresponding division into training (80%) and testing (20%) sets. This balanced split ensures sufficient data for model learning and reliable performance evaluation.

Distribution of the dataset with training and testing split.

Before starting with feature extraction and classification, all kidney tumor images were first processed to improve their quality and make them more consistent. The images were resized to 224 × 224 pixels and adjusted to a [0, 1] range to make the brightness and contrast uniform, which helps the model learn better and faster. Then, specific features related to the structure and texture of the images were manually created and combined into feature vectors. These vectors were used as input for different classifiers like Decision Tree, XGBoost, K-Nearest Neighbors, Random Forest, and Support Vector Machine. The dataset was divided into training and testing parts using ratios of 80:20 and 75:25. The models were checked using accuracy, precision, recall, and F1-score. Among them, the Support Vector Machine performed the best with an accuracy of 98. 5%, using a batch size of 32 and the Adam optimizer.

This section delivers a detailed depiction of the various machine learning models employed in the study. Each model, is explained in terms of its underlying algorithm, working principles, and suitability for the given dataset.

andare the feature values of two points and n is the total number of features.

is the weight vector,is the bias term,shows the input feature vector.

The Random Forest ensemble learning approach constructs several decision trees and combines their output to improve accuracy and reduce overfitting. It is applied to jobs involving both regression and classification37. The algorithm follows a bagging (Bootstrap Aggregating) approach, where each tree is trained on a random subset of the data, and the final prediction is obtained by majority voting (classification) or averaging (regression). The way a Random Forest (RF) works starts with a dataset D that has M features and N samples. It builds T decision trees. For each tree, it uses a method called bootstrap sampling to create a new group D_t by picking N samples from D randomly, allowing some samples to be chosen more than once. When building each tree, the random subspace method is used, which randomly selects F features from the total M features. At each step in the tree, the best feature to split on is chosen based on a specific rule, like the Gini Index or Entropy. Each decision tree is grown completely by splitting nodes again and again until all the nodes are pure or until a set maximum depth is reached, without cutting any branches off. After all T trees are built, each one makes a prediction. The final result is found by combining all these predictions—using the most common answer for classification or the average for regression. This method of using many trees together makes the model more reliable, less likely to overfit the data, and generally better at making accurate predictions.

whereis the output of the new tree andis the learning rate.

This process repeats many times, gradually improving the model’s accuracy and making it very effective at making predictions.

This section presents an ablation study analyzing the impact of different splitting ratios, comparing model performance across various optimizers and batch sizes.

This subsection examines the effect of varying dataset splitting ratios, specifically 80:20 and 75:25, on model performance.

The Fig.5illustrates a comparative analysis of five machine learning classifiers—KNN, XGBoost, SVM, DT, RF—based on key performance metrics. Among the models, SVM achieves the highest accuracy as 95%,indicating its strong generalization capability. KNN follows closely with 93% accuracy, while XGBoost also performs well at 91%. DT and RF show slightly lower accuracy at 88% and 86%,respectively. Precision, is highest for SVM with value as 94%, suggesting it makes fewer false positive predictions compared to others. Finally, Youden’s Index, which balances sensitivity and specificity, is highest for SVM with value as 76%. In contrast, DT has the lowest value as 70%, suggesting room for improvement.

Overall, SVM demonstrates the best classification performance, while KNN and XGBoost also provide strong results.

The Fig.6provides a comparative analysis of five machine learning classifiers— KNN, XGBoost, SVM, DT, RF. Among the classifiers, SVM achieves the highest accuracy as 92%, indicating strong generalization and predictive capability. KNN as 91% and XGBoost as 89% also perform well, while DT shows value as 85% and RF as 83% have slightly lower accuracy. Precision, which measures how many of the predicted positive cases are actually positive, is highest for SVM as 91%, suggesting fewer false positives, while the other models range between 85 and 88%.

Overall, SVM, KNN, and XGBoost demonstrate strong classification performance. The choice of the best model depends on the importance of precision versus sensitivity in the specific application.

From the last section it is seen that model has performed best on the split ratio of 80:20. Out of the five models SVM, KNN and XGBoost are selected as the best performing models. Now in this section the model training of these three models is performed on different batch sizes and different optimizers.

In this sub-section, the performance of three models is performed on the basis of different batch sizes.

The Table3and Fig.7presents the performance evaluation of three machine learning models—SVM, KNN and XGBoost—across different batch sizes (16 and 32)39,40. For SVM, the performance significantly improves when the batch size increases from 16 to 32. The precision rises from 70.42 to 87.47%, and sensitivity improves from 74.28 to 83.20%, leading to an F1-score increase from 69.2 to 86.35. The accuracy also improves from 87 to 94%, making SVM the best-performing model at batch size 32. KNN also shows an improvement with a larger batch size. At batch size 16, it has a precision of 76.42%,sensitivity of 78.1%, and an F1-score of 76.7, leading to 84% accuracy. This suggests that KNN benefits from a larger batch size, though not as significantly as SVM. XGBoost, however, shows a mixed trend leading to a slight increase in accuracy to 84%. Overall, SVM at batch size 32 performs the best, while KNN shows steady improvement, and XGBoost remains relatively stable across batch sizes.

Performance comparison of different models with 16 and 32 Batch sizes.

In this sub-section, Table4and Fig.8illustrates the performance of three models that is performed on the basis of different optimizers using best performing 32 batch size.

Performance comparison of different models with 32 Batch size and different optimizers.

The performance of three machine learning models—SVM, KNN, and XGBoost—using two distinct optimizers, Adam and SGD, is contrasted in the table. Precision, sensitivity, F1-score, and accuracy—all crucial measures for evaluating classification performance—are used to evaluate the models. For SVM, the Adam optimizer significantly outperforms SGD. With Adam, precision is 89.42%, sensitivity is 84.28%, and the F1-score reaches 89.2, leading to 92% accuracy. In contrast, using SGD results in lower values across all metrics. This suggests that Adam allows SVM to generalize better and make more precise predictions.

KNN also performs better with Adam. At Adam optimization, it achieves 80.14% precision, 84.42% sensitivity. When using SGD, the metrics slightly decrease, with precision at 81.28%. This indicates that KNN is more stable across both optimizers, but Adam provides a slight edge. XGBoost, however, performs better with SGD in precision (75.71%) but worse in other metrics. Overall, Adam optimization enhances performance across all models, particularly for SVM and KNN. This suggests that Adam is generally a better choice for these models in classification tasks.

Table5gives a comparison of two image sets used in recent studies to test diagnostic models for chest diseases. The first set is called the Lungs Disease Dataset (2022), and it has five different categories: Bacterial Pneumonia, Coronavirus Disease, Normal, Tuberculosis, and Viral Pneumonia. It has about 10,095 images in total, with roughly 2,000 images in each category. The images are of moderate quality, around 400 × 300 pixels. The second set is called the Tuberculosis Chest X-rays (Shenzhen) dataset (2020), and it only includes images for tuberculosis. It has 662 images, which are high-resolution, measuring 3000 × 2900 pixels.

The Lungs Disease Dataset had better results in all the main evaluation measures. It had a precision of 87.32%, sensitivity of 93.46%, F1-score of 92.23%, and an accuracy of 96.30%. The Shenzhen dataset had slightly lower scores: precision of 86.45%, sensitivity of 92.36%, F1-score of 91.18%, and accuracy of 94. 20%. The Lungs Disease Dataset shows better ability to work well with different types of lung diseases, making it a good choice for broad studies that look at many different lung conditions.

The comparison with other researchers is shown in Table6and Fig.9. The author43has used SVM with same number of images as proposed has obtained the value of accuracy as 93.0% and author17has used XGBoost and scoring the value of accuracy as 85.66%. Although the proposed SVM model with 32 batch size and Adam optimizer the model is scoring the value of accuracy as 98.5%.

To demonstrate the effectiveness of our proposed approach, we compared it with several existing state-of-the-art methods for kidney tumor detection and classification reported in the literature. As shown in Table [X], earlier studies have utilized various machine learning and deep learning techniques, including XGBoost43,45, convolutional neural networks (CNNs)46–48, multimodal deep learning models44, knowledge distillation49, YOLOv850, ResNet3451, vision transformers (ViT)52, and DenseNet20153. Reported accuracies among these methods range from 71 to 91%, with the highest accuracy achieved by DenseNet201 at 91%. In contrast, our approach employing an SVM model with a batch size of 32 and the Adam optimizer significantly outperforms these methods, achieving an accuracy of 98.5%. This remarkable improvement highlights the robustness and reliability of our proposed method, which integrates model tuning and comprehensive evaluation through an ablation study. The superior performance indicates that our model has strong potential for real-world clinical deployment to assist in the early and precise detection of kidney tumors.

In this study, we showed the effectiveness of machine learning models that can detect and classify kidney tumors early using a set of 12,446 CT scans. We tested five different machine learning models—Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Decision Tree (DT), Random Forest (RF), and XGBoost—to find out which one works best for the classification. An ablation study is also performed to see how different ways of splitting the data i.e. 80:20 and 75:25 affect the results, and we found that the 80:20 split always gave better performance. Further tests looked at how batch sizes and the choice of optimization method affect model accuracy. The SVM model had the best accuracy, reaching 98.5%, when trained with a batch size of 32 and the Adam optimizer, which was better than the other models. KNN was the next best model with an accuracy of 90.4%, also using the Adam optimizer. These results show that machine learning, especially SVM, can be very helpful for healthcare professionals in early diagnosis, which could lead to quicker action and better patient results.

However, this study has some limitations. The experiments were done with just one dataset, which might make it hard to apply the results to different groups of people or various imaging situations. Also, the current work doesn’t use explainable AI (XAI) methods, which are important for building trust and making the results easier to understand in real medical field.

Future work will focus on using explainable AI methods to give clear and easy-to-understand explanations of how the model makes predictions, helping doctors better understand the reasoning behind the decisions. Also, using techniques like ensemble learning could help make the diagnosis more accurate and reliable.