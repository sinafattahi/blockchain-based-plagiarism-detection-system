Diagnosing rare genetic disorders relies on precise phenotypic and genotypic analysis, with the Human Phenotype Ontology (HPO) providing a standardized language for capturing clinical phenotypes. Rule-based HPO extraction tools use concept recognition to automatically identify phenotypes, but they often struggle with incomplete phenotype assignment, requiring significant manual review. While large language models (LLMs) hold promise for more context-driven phenotype extraction, they are prone to errors and “hallucinations,” making them less reliable without further refinement. We present RAG-HPO, a Python-based tool that leverages retrieval-augmented generation (RAG) to elevate accuracy of HPO term assignment by LLM. This approach bypasses the limitations of baseline models and eliminates the need for time- and resource-intensive fine-tuning. RAG-HPO integrates a dynamic vector database, containing > 54,000 phenotypic phrases mapped to HPO IDs, which allows real-time retrieval and contextual matching. The RAG-HPO workflow begins by extracting phenotypic phrases from clinical text via an LLM and then matching them via semantic similarity to entries within the database. The best term matches are returned to the LLM as context for final HPO term assignment of each phrase.

Performance was benchmarked on 112 published case reports with 1792 manually assigned HPO terms and compared to Doc2HPO, ClinPhen, and FastHPOCR. In evaluations, RAG-HPO + LLaMa-3.1 70B achieved a mean precision of 0.81, recall of 0.76, and an F1 score of 0.78—significantly surpassing conventional tools (p< 0.00001). RAG-HPO returned 1648 terms, of which 19.1% (315) were false positives that did not exactly match our manually annotated standard. Among these, < 1% (1/315) represented hallucinations, and 1.3% (4/315) represented terms with no ontological relationship to the desired target; the remaining false positives (95.2%, 300/315) were broader ancestor terms of the target term, which may still be relevant to users in many contexts.

RAG-HPO is a user-friendly, adaptable tool designed for secure evaluation of clinical text and outperforms standard HPO-matching tools in precision, recall, and F1. Its enhanced precision and recall represent a substantial advancement in phenotypic analysis, accelerating the identification of genetic mechanisms underlying rare diseases and driving progress in genetic research and clinical genomics. RAG-HPO is available athttps://github.com/PoseyPod/RAG-HPO.

The online version contains supplementary material available at 10.1186/s13073-025-01521-w.

In genomic medicine and research, phenotypic and genotypic analyses are critical for achieving accurate molecular diagnoses. Deep phenotyping allows for a detailed understanding of a patient’s clinical presentation, which can then be matched to potential genetic causes [1]. Genomic analysis provides the molecular insights necessary to identify pathogenic variants that may be contributing to disease [2]. Together, these approaches facilitate a comprehensive evaluation of patients, particularly those with rare or undiagnosed conditions, and offer the possibility of uncovering genetic etiologies that might otherwise remain elusive [3]. Integrating clinical and molecular data is essential for providing patients with concrete answers, guiding their treatment, and improving outcomes [4–7].

The Human Phenotype Ontology (HPO) is a standardized, hierarchically structured vocabulary that underpins modern deep-phenotyping workflows [8–10]. Its hierarchical structure links broad categories to increasingly specific descriptors, enabling clinicians and researchers to capture subtle phenotypic differences—even when clinical terminology varies—while preserving computational comparability [11]. Each concept is assigned a stable identifier (e.g., HP:0004322 “short stature”), allowing rapid, large-scale matching of phenotypically similar individuals to candidate genetic variants [10,12,13]. Over time, revision and expansion of the HPO term set has led to a modest number of obsolete terms [10].

Figure1illustrates both the breadth and depth of the HPO as a resource for identifying patient phenotypes. Figure1A shows that nearly half of the 19,024 non-obsolete terms belong to the musculoskeletal system, while only 245 (< 2%) HPO terms within the database describe developmental biology-related phenotypes. Figure1B highlights the ontology’s specificity by organ system, with median nodal depths clustering between six and ten across organ systems. However, every branch also contains range of nodal depths from general (< 3-level) to ultra-specific (> 12-level) terms (e.g., HP:0001507 “growth abnormality” vs. HP:0008845 “mesomelic short stature”). This combination of wide anatomic coverage and variable specificity makes the HPO uniquely suited for machine-readable phenotypic data, laying the groundwork for advanced genotype–phenotype matching tools that drive precision genomic medicine today [14–16].

In rare disease research, HPO terms have become central to phenotype-driven variant prioritization and genotype–phenotype correlation [17]. Tools such as Exomiser use patient-specific HPO profiles to rank candidate variants by phenotypic similarity, improving diagnostic yield and reducing the time to diagnosis [18,19]. At the research level, platforms like the Monarch Initiative integrate HPO annotations across human and model organism datasets to support discovery of novel gene–disease associations [10,20]. Beyond genetics, HPO also enables the stratification of patients with overlapping clinical features in studies of complex traits, treatment response, and multi-system diseases, underscoring its broad utility in biomedical research [21,22].

While HPO terms provide a standardized framework for cataloging and investigating patient phenotypes, the process of extracting deep phenotyping information from clinical text remains labor-intensive and reliant on clinical expertise. Many tools, such as Doc2HPO, ClinPhen, and FastHPOCR, seek to automate the extraction of relevant phenotypic phrases from clinical records using dictionary-based, concept recognition methods [12,13,23]. This approach involves comparing segments of text against a predefined lexicon of domain-specific terms, such as HPO terms, to identify phenotypes within clinical text. While straightforward and interpretable, concept recognition lacks the ability to interpret nuanced contexts, such as family history and pertinent negatives, leading to the incorrect assignment of HPO IDs. These tools often miss a substantial portion of patient phenotypes, necessitating thorough manual cross-examination to ensure accurate phenotype annotation.

Large language models (LLMs) are advanced computational programs trained to understand and generate natural language based on patterns present in human-generated text [24,25]. As part of the increased popularity of generative artificial intelligence (AI), these programs offer a promising opportunity to significantly improve automated deep phenotyping [26,27]. Within the biomedical field, remarkable advances in LLM technology have led to rapid growth in the use of technology in various clinical and basic science applications, including interpretation of radiological results or analyzing medical text [28–32]. The ability of LLMs to understand natural language context is a critical component for improving the ability to extract relevant clinical phenotypes from patient data. Consequently, LLMs have entered the phenotypic analysis space through tools like PhenoTagger, PhenoBERT, and PhenoGPT [33–35].

However, the integration of LLMs into phenotypic analysis presents new challenges. As other authors have noted, currently available LLMs are resource-intensive and slow in their reasoning compared to other machine learning methods [23]. Importantly, they are also prone to hallucinations, in which the model generates incorrect information and confidently asserts it as fact [36]. Many LLM-based tools for phenotypic analysis address these concerns through fine-tuning, which involves further training of a model with additional data [35,37,38]. While this process increases the accuracy of LLM responses, it is computationally expensive (requiring substantial GPU and RAM), time-consuming, and necessitates specialized expertise in working with generative AI models [26,36,39]. Additionally, fine-tuning may not always improve LLM performance in the desired way, especially with large-scale models [40,41]. With the rapid advancement of LLM technology and continual updates to the HPO lexicon, fine-tuning LLMs for phenotypic analysis becomes impractical for most clinicians and researchers involved in diagnosing rare genetic diseases.

Alternatively, retrieval-augmented generation (RAG) is a practical solution to the limitations of fine-tuning. RAG uses vector databases to retrieve relevant information from source documents in real-time, making it easier and faster to update the system with new information [42–44]. Users can refresh the underlying vector database with minimal effort, allowing the system to stay current without the need to retrain the LLM. This makes RAG more cost-effective and adaptable, particularly for users without advanced technical expertise and who use a resource that constantly updates, while still leveraging the power of LLMs to enhance the accuracy and precision of results.

We have developed a Python-based program, RAG-HPO [45], to apply RAG to deep phenotyping, dramatically increasing LLMs’ ability to accurately assign HPO terms to patient phenotypes. This enables us to leverage the LLMs’ superior ability to extract phenotypic information from clinical data, improving the process of automated deep phenotyping. RAG-HPO is simple to use, does not require extensive understanding of LLMs, works with any language model, and does not require computationally intense resources like GPUs and large amounts of RAM for use. By utilizing a vector database, RAG-HPO can be easily updated with new information from the HPO database and user input. When paired with LLaMa-3, RAG-HPO demonstrates superior precision and recall compared to popular dictionary-based concept recognition tools.

RAG-HPO is a Python-based tool designed to extract clinical phenotypes from medical free text and assign HPO terms to those phrases using RAG. Users can employ any LLM of their choosing by providing an Application Programming Interface (API) key. Below, we describe implementation of this tool with the LLaMa-3.1 70B for the benchmarking of RAG-HPO.

The vector database used by RAG-HPO utilizes a Python dictionary with key-value pairs, where keys are clinical words and phrases that reliably match to HPO ID numerical values (e.g., furrowed tongue: HP:0000221). The initial dictionary was extracted from the HPO database and includes term titles, names, definitions, and synonyms paired with their respective HPO IDs. To expand its coverage, we generated additional synonyms and short phrases for each HPO term using LLaMa-3.1 70B, with instructions focused on medical practice and current literature. After removing duplicates, we submitted each candidate phrase for validation using Doc2HPO and ClinPhen, retaining only those that accurately mapped back to the original HPO ID. This process added several thousand custom entries, each reliably associated with the correct HPO term, resulting in a final custom dictionary of over 54,000 unique phrases that correspond to individual HPO IDs stored in JavaScript Object Notation (JSON) format (Additional file 1: Vector database dictionary). The refined dataset serves as the basis for the vector database used in RAG-HPO. Fastembed was employed to convert each term within the JSON file to a high-dimensional vector to capture semantic relationships between phenotypic terms and factor together lineage information to bring like terms closer to one another, enabling enhanced similarity searches during phenotypic matching.

The embedded database is stored as a NumPy array for use by RAG-HPO, serving as a resource to support the LLM in making accurate assignments. When the analysis program is initiated, the NumPy array is indexed in a vector database optimized for dense vector retrieval, allowing for efficient approximate nearest-neighbor searches. Relevant metadata is also indexed within the database to assist the LLM in reasoning through the assignment process.

The program processes free text provided as strings, converting them into HPO terms. Input text can either be supplied directly by the user or batch-processed from a CSV file containing clinical notes. The text is first passed to the user’s chosen LLM via an API call, where a custom system prompt instructs the LLM to extract phenotypic phrases that describe the patient’s health status, while disregarding non-relevant information such as administrative details or general observations that do not pertain to the patient’s health (Fig.2A, Additional file 2: Fig. S1).

Once extracted, these phenotypic phrases are prepared for the HPO term assignment process, which begins with semantic similarity search (Fig.2B). To optimize computational efficiency and reduce the workload on the LLM, each extracted phrase is first compared to the metadata within the embeddings array using fuzzy matching. Exact matches are directly included in the results without further processing, while phrases that do not have an exact match are converted into high-dimensional semantic vectors using Fastembed. The generated embeddings, along with associated metadata (such as HPO terms, lineage information, and organ system), are compared to the developed HPO database in a similarity search via Facebook AI Similarity Search, an efficient package for indexing and searching vector data, to identify related HPO terms (Fig.2B). The program retrieves the top 20 most semantically similar vectors, along with their associated metadata, including the relevant HPO terms, lineage, and other contextual information. Then, the surrounding sentence from the original clinical text is retrieved to provide additional context for understanding the extracted phrase.

The extracted clinical phrases are resubmitted to the LLM, which uses the additional context provided by the metadata to select the most appropriate and detailed HPO term representing the most distal matching node (highest possible information content) in the ontology that fits the level of information provided by the phenotypic phrase (Fig.2C). After each extracted phrase and metadata block are passed through the LLM, a completed list of HPO terms for the whole passage is returned to the user (Fig.2D). When analyzing batches, the results are saved as a JSON object list within a copy of the original CSV file under a new column.

RAG-HPO, ClinPhen, Doc2HPO, and FASTHPOCR were evaluated based on their ability to accurately extract HPO terms from previously published case reports. This case study cohort (CSC) comprised 120 case reports sourced from two peer-reviewed journals:BMJ Case ReportsandOxford Medical Case Reports. These journals were selected for their rigorous editorial standards and comprehensive coverage of diverse medical specialties, including gastroenterology, musculoskeletal, dermatology, hematology, cardiovascular, pulmonary, renal, endocrinology, reproductive health, neuroanatomy, and psychiatry. Both journals are widely recognized in the medical community and are indexed in PubMed Central, ensuring accessibility and credibility. A third-year medical student, independent of the evaluation team, screened the reports to minimize selection bias.

Each case within the study described one to two patients with a distinct set of phenotypes for a particular illness. Manual annotation of HPO terms was initiated by the medical student, who extracted and matched phenotypic features from each case report to specific HPO IDs. Subsequently, additional team members, including a certified genetic counselor and a physician-scientist trainee, reviewed and completed the annotations to ensure accuracy and consistency across the dataset. Discrepancies were resolved through consensus discussions involving the annotators and medical genetics faculty, who selected the most appropriate HPO term based on clinical expertise and the context of each case. Reports describing fewer than four discernible phenotypic features were excluded, leaving 112 cases with 1794 (1208 unique) assignable HPO terms across the cohort with a mean of 15.8 HPO terms per case (Table1). Additional file 2: Fig. S2 demonstrates the organ system distribution and nodal depth of the terms found within the CSC.

We also evaluated RAG-HPO and other HPO analysis software using the gold standard corpora (GSC), which contains 229 entries describing genetic conditions [15]. After applying the same filtering criteria as for our case study cohort (i.e., removing entries with fewer than four distinct phenotypes and manually assigning HPO terms), we curated a final dataset of 114 entries comprising 1013 HPO terms, of which 415 were unique, averaging 8.9 terms per case (Table1). We then compared our manually assigned HPO terms for each GSC entry to those comprising the original annotation. Additional file 2: Fig. S3 illustrates entries for which we excluded overly general terms present in the original annotations. Additional file 2: Fig. S4 displays the organ system distribution and nodal depth of the terms found within the GSC, comparable to Fig.1and Additional file 2: Fig. S2 for the HPO database and CSC.

Figure3demonstrates key differences between the full HPO dataset, the GSC, and our CSC. While the GSC covers a range of organ systems, it is heavily skewed toward a small subset of systems, much like the HPO database (Fig.3A and B, respectively). Over 56.5% of the annotated HPO terms in the GSC fall within just three systems: the nervous system (23.7%), musculoskeletal system (22.2%), and head/neck/special senses (10.6%). Other systems such as the endocrine/metabolic (5.8%), genitourinary (3.5%), cardiovascular (3.1%), and immune (5.4%) systems are markedly underrepresented. This distribution reflects a bias inherent to the OMIM-derived source material, which often emphasizes neurodevelopmental and congenital disorders.

Such imbalances reduce the generalizability of the GSC as a benchmark for phenotype extraction tools aimed at broad clinical application. In contrast, our CSC demonstrates a more even and realistic distribution of phenotypic systems (Fig.3C), enhancing its value as a performance benchmark for general-purpose tools like RAG-HPO.

Briefly, ClinPhen segments clinical notes into sentences and subsentences, normalizing and matching phrases against an HPO synonym dictionary while filtering out irrelevant or negated mentions [13]. Doc2HPO integrates several natural language processing engines—including string-based matching via the Aho–Corasick algorithm and MetaMap-based methods—to provide an interactive interface for real-time curation and negation detection [12]. FASTHPOCR uses a fast dictionary-based approach that first consolidates morphologically equivalent tokens into clusters, creating an index of HPO concept signatures for rapid candidate matching [23]. Detailed descriptions of these methods are available in their respective publications.

For our evaluation, ClinPhen [46] was downloaded fromhttp://bejerano.stanford.edu/clinphen/and Doc2HPO [47] was accessed via its web interface athttps://doc2hpo.wglab.org/. FASTHPOCR [48] was obtained from its GitHub repository (https://github.com/tudorgroza/fast_hpo_cr) in June 2024. It is noteworthy that both ClinPhen and Doc2HPO utilize an older version of the HPO dataset, which may lead to the extraction of obsolete terms.

In contrast, RAG-HPO’s performance was evaluated using an updated HPO version accessed in August 2024 (v2024-08-13). For the evaluation, we tested RAG-HPO using several LLMs available for use through groq.com, including LLaMa-3 70B, LLaMa-3.1 8B, LLaMa-3.1 70B, LLaMa-4 Scout (16B), Misra 24B, and Deepseek-R1. These LLMs represent a wide variety of open-source LLMs that are reasonably available for researchers and clinicians to access for their own evaluations and use in research. Testing across several versions of LLaMa language models allowed us to demonstrate the changes in performance with incremental improvements of LLMs over time.

In our analysis, we considered any HPO term called by an analysis tool to be a true positive if it matched the manually annotated standard. False positives were then any called HPO term that did not match manually annotated terms. Failure to identify an HPO term from the standard was considered a false negative. The number of true positives, false positives, and false negatives were used to calculate three key metrics: precision, recall, and the F1 score. These scores were used in determining the performance of the HPO analysis tools.

Python scripts were used to evaluate performance and develop figures. To keep evaluations objective and consistent, we developed a Python script to accurately compared the output of each program against the manually annotated standards. The script automatically split multi-term entries, removed invalid rows (entries without HPO term IDs), and counted the true positives, false positives, and false negatives for each case. These counts were used to calculate precision, recall, and F1 scores per case and we reported the average of these scores for the evaluated cohorts. Additionally, we built Python scripts to generate the figures within this paper using a variety of packages, includingnumpy(2.2.5),pandas(2.2.3),scipy(1.15.2),statsmodels(0.14.4), andmatplotlib(3.10.1). Original verbatim inputs for the CSC and GSC, the manual annotations, and all outputs for this study are available for review in Additional file 3: RAG-HPO tests and data analysis.

RAG greatly increases LLM’s ability to identify correct HPO terms. The goal of RAG is to improve a language model’s capacity to retrieve and generate contextually relevant information, thereby reducing “hallucinations.” In HPO analysis, hallucinations typically appear as non-existent or inappropriate HPO identifiers that do not match the underlying clinical phrases. Without RAG (or similar informational scaffolding), current LLMs struggle to assign HPO terms accurately.

A 44-year- oldsuper-morbidly- obeseman body mass index (BMI 63) underwent sleeve gastrectomy for weight loss and was found to havemultiple adenomatous fundic gland polypson final pathology. Subsequent workup included esophagogastroduodenoscopy which revealedinnumerable polyps of the remaining gastric fundus and bodyconsistent with fundic gland polyps, normal duodenum without polyps, andBarrett’s oesophagus. Colonoscopy was significant forinnumerable polyps of varying sizes up to 1.5 cm throughout the colon,with relative rectal sparing. Biopsies were consistent withtubular adenoma and hyperplastic polyps.Thyroid ultrasound was within normal limits and abdominal CT was significant forleft-sided 3.4 cm mesenteric massrepresenting scarring versus possible desmoid. Family history was significant for colon cancer diagnosed in his maternal grandfather at age 72, paternal grandmother who died of metastatic cancer at age 50 with unknown primary and mother diagnosed with thyroid cancer at age 40. Physical examination was notable formorbid obesity(BMI 45), cardiopulmonary examination within normal limits and abdomen with well-healing surgical scars from prior sleeve gastrectomy.

As shown in this example, most LLMs capture the majority of clinical phenotypes available in a particular case (see Additional file 2: Fig. S5 for full results). However, they frequently failed to assign proper HPO codes, yielding uniformly low precision, recall, and F1 scores (Table2).

By contrast, RAG-HPO greatly improved the ability of LLMs to accurately assign HPO terms. To evaluate this improvement, we compared the performance of LLaMa-3 70B both individually and as part of RAG-HPO (Table3) in the assignment of HPO terms for the case study cohort. When using the LLaMa-3 70B model alone (“baseline”), precision, recall, and F1 scores were 0.12, 0.09, and 0.10, respectively. However, when combined with RAG-HPO, these metrics improved dramatically, yielding a precision of 0.71, recall of 0.61, and an F1 score of 0.64. These results confirm that RAG significantly enhances LLM performance in HPO analysis, with the RAG component directly responsible for the improvements in accuracy.

The performance of RAG-HPO was benchmarked against ClinPhen, Doc2HPO, and FastHPOCR using our case study cohort (Table4). To assess the influence of LLM choice on performance, we paired RAG-HPO with six different back-end models. With the sole exception of DeepSeek R1, every LLM configuration of RAG-HPO performed on par with, or significantly better than, the conventional dictionary-based concept recognition tools (pair wise statistics, Additional file 2: Fig. S6). RAG-HPO performance improved with increased LLM parameter scaling and improvements to underlying architecture, context windows, and other back-end improvements made to LLMs over time.

RAG-HPO + LLaMa-3.1 70B achieved the strongest overall results, posting the highest precision (0.81), recall (0.76), and F1 score (0.78) in the evaluation (Fig.4, Table4). This configuration identified 1333 correct HPO terms, which was more than twice the true-positive count of ClinPhen (635) and Doc2HPO (634), and an almost 1.6-fold increase compared to FastHPOCR (816). Although FastHPOCR retrieved a larger number of terms per case than the other traditional tools, its larger false-positive burden (FP = 726) reduced both precision (0.53) and F1 (0.49).

Figure5compares each tool’s output with the manually annotated case study reference set. RAG-HPO + LLaMA-3.1 70B recovered the largest share of ground-truth terms while adding the fewest false positives among RAG configurations, yielding the overall best error balance. Among stand-alone concept recognition tools, FastHPOCR retrieved the most correct terms but incurred a high false positive rate while ClinPhen and Doc2HPO only captured about one-third of the gold standard, but with a much lower rate of false positives. Together, these results underscore that even the best traditional tools lag behind RAG-HPO, both by missing true phenotypes and by misattributing HPO assignments.

We modeled the probability that a program assigns the correct HPO term using a binomial generalized estimating equation with patient-level clustering. Predictors were program, organ system, and nodal depth. Model coefficients were converted to odds ratios and summarized in three panels: a forest plot of organ-system effects (Fig.6A), a fitted-probability curve across nodal depths (Fig.6B), and box plots of predicted hit probabilities for each program (Fig.6C).

Organ system had a limited influence on accuracy (Fig.6A). Cardiovascular, genitourinary, and head/neck terms showed moderately higher odds of correct assignment (odds ratio ≈ 8–11 versus reference), while most other systems clustered around unity. All tools exhibited a monotonic decline in accuracy as terms became more specific (Fig.6B); predicted hit probability fell steadily from shallow (depth ≤ 3) to deep nodes (depth ≥ 10). However, tool choice remained the strongest determinant of success (Fig.6C). Almost every RAG-HPO configuration out-performed the concept recognition tools FastHPOCR, ClinPhen, and Doc2HPO, with RAG-HPO + LLaMa-3.1 70B achieving the highest predicted hit probability across the depth range.

In our analysis, we accepted only the most specific term (highest information content) to describe each phenotype. However, there is value in identifying directly related ancestor terms, which are often broader descriptors of the phenotypes observed (Fig.7). To better understand the meaning of these incorrectly attributed terms, we conducted a deeper analysis of the relationship of false positives to the manually annotated HPO terms, specifically focusing on the ontological connections between the HPO terms selected by the programs and those identified through manual annotation. We categorized the false positive HPO terms into three distinct groups: (1) terms that were direct ancestors of the target HPO terms, (2) terms that were indirectly related to the target terms through a shared ancestor, and (3) terms that were completely unrelated to the correct terms. Additionally, for RAG-HPO, we classified any false positive HPO terms that did not exist in the HPO database as “hallucinations” generated by the system (Fig.7A).

RAG-HPO paired with LLaMa-3.1 70B generated 315 false positives: 70 direct ancestors (22%), 230 distant relatives (73%), 14 unrelated terms including deprecated codes (4%), and only a single hallucination (< 1%) (Fig.7B). The other RAG variants showed a similar pattern, although the less-aligned models (e.g., LLaMA-3 70B and LLaMA-4 Scout) contained a larger unrelated fraction. Among concept recognition tools, FASTHPOCR recorded 729 false positives, dominated by distant relatives (79%) with 13 unrelated terms (2%); ClinPhen and Doc2HPO produced 376 and 264 FPs, respectively, with distant relatives comprising 62–66%. True hallucinations were absent in all three rule-based tools and remained rare for RAG-HPO.

Unrelated HPO terms assigned by RAG-HPO rarely included errors attributable to poor semantic similarity search. When RAG-HPO identifies a phenotypic phrase for term assignment, the phrase is vectorized and compared to the custom vector database we created. In rare cases, the search returned database entries that were completely unrelated to the extracted clinical phrases. As currently written, the LLM must choose an HPO term from the list generated by similarity search, a constraint designed to prevent hallucinations, but which can lead to nonsensical HPO choices.

A deeper understanding of the false positive population helps us to better understand the reasoning for their development and determine ways to reduce their occurrence and improve our precision. This analysis also highlights the importance of understanding the level of detail needed for specific downstream applications. In some instances, less specific but related terms may be acceptable.

As previously mentioned, we curated the GSC down to 114 notes each containing at least four distinct phenotypes for a total of 1013 HPO terms (415 unique). With this dataset, we evaluated the performance of RAG-HPO, using both LLaMa-3 70B and LLaMa-4 Scout back-ends, against FastHPOCR, Doc2HPO, and ClinPhen.

RAG-HPO + LLaMA-3 70B returned 766 true positives, 358 false positives, and 245 false negatives, corresponding to a mean precision of 0.69, recall 0.77, and F1 0.71 (Table5). Substituting the larger-context LLaMA-4 Scout backbone increased true positives to 783 but also raised false positives to 490, lowering precision to 0.62 and F1 to 0.68 while maintaining the highest recall (0.79).

Among the concept recognition tools, FastHPOCR achieved the next-best balance (precision 0.74, recall 0.70, F1 0.70), whereas Doc2HPO and ClinPhen prioritized precision (0.80 and 0.61, respectively) at the cost of markedly reduced recall (0.44 and 0.42) and lower F1 scores (0.56 and 0.46).

Compared with the CSC, RAG-HPO’s precision decreased by almost 12%, likely because GSC entries are research summaries rather than clinical notes, introducing broader language and citation artifacts that inflate distant-relative matches. However, RAG-HPO retained the highest overall F1 and the strongest recall across this markedly different text genre, confirming the generalizability of the retrieval-augmented framework.

We developed RAG-HPO, a RAG framework designed to enable precise and comprehensive extraction of HPO terms from clinical case narratives using LLMs. RAG-HPO consistently outperforms existing tools in both precision and recall while maintaining flexibility and ease of use (Fig.3). While LLMs are capable of identifying phenotypic descriptions in text, they frequently misassign or incompletely map terms to HPO identifiers when operating in isolation (Table3). RAG-HPO mitigates these errors by grounding generation in a curated knowledge base during inference. This integration of retrieval reduces the incidence of hallucinated or invalid terms and promotes accurate alignment between clinical language and ontology-based concepts. When evaluated on two independent datasets, the combination of RAG-HPO + LLaMA-3.1 70B achieved the highest F1 scores among all tested tools. It more than doubled the true positive rate of traditional dictionary-based concept recognition systems, with hallucinations accounting for less than 1% of the output (Table4).

One of RAG-HPO’s key strengths is its adaptability and ease of use, even for users with limited experience in machine learning or language model deployment. Unlike traditional fine-tuning approaches, which demand significant time, computational resources, and specialized expertise, our program leverages RAG to enable high-performance HPO term extraction with minimal technical burden. Users can easily update the vector database with just a few clicks, ensuring that the tool remains current and effective without the need for retraining or model modification.

The program is also simple to run, making it accessible to researchers and clinicians with limited technical expertise. At the same time, its modular Python structure allows for easy integration, modification, and ongoing improvement. RAG-HPO enables users to submit plain text without needing to preprocess data or integrate into electronic health records (EHR). This adaptability allows it to handle free-form medical text, making it especially useful in rare disease research, where patient descriptions often vary in format (Fig.2A).

This flexibility extends to model deployment. RAG-HPO is compatible with any LLM supporting the OpenAI API framework, including those hosted locally for HIPAA-compliant workflows. This model-agnostic design allows researchers and clinicians to select backends that best meet their infrastructure and privacy needs. As demonstrated by our cross-platform analysis of LLM performance, model selection is a key factor in optimizing precision and recall. For example, replacing the baseline LLaMA-3 70B with the alignment-tuned LLaMA-3.1 70B led to notable gains in both precision (0.71 to 0.81) and recall (0.61 to 0.76), resulting in an F1 score of 0.78 (Table3). Interestingly, the lightweight LLaMA-4 Scout (16B parameter size) approached a similar level of performance (F1 = 0.66) to LLaMA-3 70B, despite being smaller. These findings highlight the value of backbone selection and suggest that newer model families may continue to improve with future releases.

Performance profiling revealed additional insights into program behavior. Organ system had a minimal overall effect on performance, with odds ratios clustering around 1 (Fig.6A). Slight boosts were observed for cardiovascular, genitourinary, and head and neck terms, with some reaching odds ratios between 8 and 11. However, the organ system type was not the dominant driver of performance. Nodal depth, which can be used as a measure of HPO term specificity, had a more consistent and monotonic effect: deeper terms reduced hit probability across all tools, reflecting the increased difficulty of matching complex or distal ontology nodes (Fig.6B) with described phenotypes. Although all variants of RAG-HPO displayed a similar performance drop rate as other tools, most had a consistently higher F1 compared to the more traditional dictionary-based concept recognition tools.

RAG-HPO + LLaMA-3.1 70B tripled the odds of correct HPO assignment compared to Doc2HPO (Fig.6C). Even the weakest RAG variant performed as well as or better than the strongest rule-based tool. A trade-off was observed between backbone strength and hit probability behavior: stronger models generally yielded higher recall and captured more distant matches, while weaker models offered slightly higher precision but lower sensitivity overall. Larger context windows, such as those used in LLaMa-4 Scout, appeared to lift recall but also slightly inflated false positives, reflecting a precision-recall trade-off.

Finally, RAG-HPO’s structural modularity enables incremental improvement over time. Its components, including system prompts, embedding generation, similarity search, and LLM integration, can be independently modified, swapped, or upgraded. The retrieval database can be updated without retraining the underlying model, ensuring compatibility with evolving versions of the HPO as long as the embedding process for the database is the same as the embedding process used during the deep phenotype extraction process. This makes RAG-HPO not only performant and flexible, but also capable of keeping up with changes in the LLM field.

Recently, others have used a predetermined dataset, the GSC, to evaluate their analysis programs [15,23,35]. The GSC was derived from entries in the Online Mendelian Inheritance in Man (OMIM) database and contains a mix of content types, including structured disease descriptions and a subset of passages formatted to resemble clinical notes. While this dataset has served as a comparative benchmark for several tools, it presents significant limitations for evaluating systems designed for clinical NLP and deep phenotype extraction.

Many of the GSC entries resemble scientific abstracts or informational summaries describing well-known genetic conditions such as neurofibromatosis, Gorlin syndrome, and branchio-oto-renal syndrome. However, this format often results in annotated truth sets populated with general or lower information-content HPO terms, even when more specific alternatives are appropriate. For instance, in case 1,347,096, the phrase “basal cell carcinoma” appears in the source text, yet the annotation includes both HP:0002671 (“basal cell carcinoma”) and the more general HP:0030731 (“carcinoma”). In contrast, our approach to deep phenotyping emphasizes selecting the single most descriptive and specific term, with the understanding that downstream analyses can leverage the HPO’s hierarchical structure to trace lineages and identify ancestral terms as needed. As a result, reliance on the GSC as a primary evaluation benchmark may underestimate the capabilities of tools like RAG-HPO, which are designed to prioritize specificity, precision, and contextual depth in phenotype assignment.

Although we had access to the GSC, we deliberately developed a separate cohort composed of published clinical case reports—the CSC. Unlike the condensed, citation-style summaries found in the GSC, these reports more closely resemble real-world clinical documentation, capturing greater variability, language, and contextual detail more reflective of medical records. To ensure data quality, we applied strict inclusion criteria and employed multilayered manual annotation, with review by medical genetics professionals to promote consistency and accuracy.

Beyond differences in size and term count (the GSC contained 114 entries with 415 unique HPO terms (8.9 per case on average), while our CSC included 112 reports with 1208 unique terms and a higher annotation density (15.8 terms per case)), the structural composition of the two datasets further underscores their divergence. The GSC is heavily skewed toward a narrow set of organ systems—primarily nervous, musculoskeletal, and head/neck domains—while significantly underrepresenting others such as endocrine, cardiovascular, and genitourinary systems (Additional file 2: Fig. S4B). This bias reflects the GSC’s origin in genetic reference databases and its focus on well-characterized monogenic conditions. However, the HPO database has matured to support a much broader array of clinical concepts, extending beyond neurological and congenital disorders. In contrast, our case study cohort offers a more balanced and comprehensive distribution across organ systems (Fig.3), providing a more robust foundation for evaluating phenotype extraction tools intended for general clinical application.

Despite these limitations, RAG-HPO demonstrated strong performance on the GSC dataset, achieving the highest recall (0.77–0.79) and F1 score (0.71) among all tools tested (Table4). This underscores the robustness of the RAG-HPO architecture and its ability to generalize across dataset types. However, the relative success of FASTHPOCR on this benchmark may reflect alignment with the dataset’s structure, as both were developed by the same research group—a factor that may inflate apparent performance. These findings highlight the broader challenge of overfitting to familiar benchmarks and reinforce the need for diverse, heterogeneous evaluation sets.

One limitation of RAG-HPO is its processing speed. While individual clinical notes may take up to 45 s to analyze, other tools, such as FastHPOCR, can process the entire 112 case cohort in that time. However, this longer runtime for RAG-HPO is offset by its superior precision and comprehensive output, which reduces the need for downstream correction and ultimately supports more accurate deep phenotype extraction.

Another challenge is that the precision of RAG-HPO depends heavily on the quality of the vector database used for semantic similarity searches. While our current database is carefully curated and regularly updated, it remains largely oriented toward genetic disease phenotypes, reflecting the historical focus of HPO itself. However, it is also limited by the available HPO terms, which are tailored toward features recognized in medical genetics. This introduces the potential for rare mismatches—particularly when semantic similarity scores rank related but suboptimal terms highly. While retrieval constraints minimize outright hallucinations, they may still return less-than-ideal terms if the embedding space misfires. However, most false positives returned by RAG-HPO are closely related terms rather than completely irrelevant matches. These near misses may still hold clinical value, particularly in differential diagnosis workflows or patient comparative studies where broader phenotypic context is useful.

As the HPO database matures, we expect precision to improve. To support this ongoing process, we have made the full program for vector database creation, as well as the supplementary metadata used to enrich each term, publicly available on GitHub. A spreadsheet-format supplement document enables users to view and update curated entries, which are merged with the current hp.obo to regenerate the full index. This FAIR-compliant structure allows end users to integrate their own validated entries and optionally submit them back to the authors for inclusion in future versions after review. Our long-term goal is to promote this as a community-supported effort, enabling sustainable, open-source improvement of the database for as long as the tool remains clinically or scientifically valuable. However, the subjective nature of phenotype interpretation can still influence results, as different users may prioritize varying levels of specificity. While RAG-HPO is designed to identify the most precise HPO term for a given phenotypic phrase, some users may seek a broader characterization of a patient’s phenotype.

The inherent subjectivity of clinical evaluations, through which different physicians may arrive at different interpretations of the same patient, can also impact the phenotyping process. Although HPO terms aim to standardize patient presentations, variability in clinical assessment may still affect the final set of phenotypes attributed to an individual patient. While minimized with the use of LLMs in the automated deep phenotyping process, this subjectivity will persist when comparing cases of individuals seen by different physicians. Further downstream analyses focused on comparing HPO term lineages along with the specific HPO terms can further mitigate the impact of clinician subjectivity in patient evaluation.

Finally, RAG-HPO’s recall is influenced by the choice of LLM backend. More powerful models with broader contextual understanding generally yield higher recall and better interpretation of complex or obliquely phrased phenotypes. Fortunately, the system is designed to be model-agnostic, allowing users to substitute any OpenAI-compatible LLM. We selected LLaMa-3 70B as a balance between performance and accessibility, but ongoing improvements in open-source language models are likely to yield continued gains in speed, accuracy, and flexibility.

Our primary objective in developing RAG-HPO is to create an efficient and user-friendly tool for automated deep phenotyping. Looking ahead, we plan to further improve the vector database by incorporating user contributions of verified phrases that consistently map to established HPO terms. This community-driven approach will support ongoing refinement and ensure the system remains responsive to evolving clinical language.

Although RAG-HPO was initially designed for deep phenotype extraction in the context of rare genetic variant discovery, its potential applications extend to other areas of medicine that rely on patient comparison or characterization based on clinical presentation. Future efforts will focus on expanding these use cases to include phenotype clustering, cohort matching, and differential diagnosis. To support these broader goals, we are also exploring hybrid pipelines that combine high-precision dictionary filters with retrieval-based augmentation to balance specificity and recall.

As RAG-HPO continues to evolve, we aim to establish it as a generalizable, open-source clinical phenotyping tool. We plan to promote ontology-aware evaluation frameworks that classify errors by term relationships (e.g., ancestor, relative, unrelated), enabling more nuanced assessments of tool performance. These strategies will support more scalable, adaptable applications of RAG-HPO across research and clinical settings.

RAG-HPO represents a significant advancement in the field of automated deep phenotyping, demonstrating a marked improvement over traditional HPO tools in both precision and recall. By leveraging retrieval-augmented generation (RAG) and a dynamic vector database of over 54,000 phenotypic terms, RAG-HPO offers clinicians and researchers an accessible, computationally efficient solution for phenotype extraction that is both adaptable and resource-light. Unlike conventional LLM-based tools, RAG-HPO minimizes the need for fine-tuning and reduces the risks of “hallucinations,” making it a reliable and scalable tool for phenotypic analysis, particularly in rare disease diagnosis and clinical genomics.

Additional file 1. Vector database dictionary. This spreadsheet file contains a table of all key-value pairs, mapping each clinical word or phrase (“key”) to an HPO ID (“value”).

Additional file 2. Supplemental text and figures. This text file includes supplemental text demonstrated the system messages generated by RAG-HPO and provided to the user, as well as supplemental figures.

Additional file 3. RAG-HPO tests and data analysis. This spreadsheet file contains the HPO IDs for the case sets against which benchmarking was performed, as well as the benchmarking data itself, across all tested tools.

We would like to thank the following for their help and support in creating RAG-HPO: Thanks to Steve Ludke, PhD, Maxim Seferovic, PhD, and Shinya Yamamoto DVM, PhD for your guidance in coding in Python; thanks to Gwendolyn Hummel for biostatistics guidance; and thanks to Cole Deisseroth for being a resource for information on HPO analysis and the application of NLP to deep phenotyping.

BTG designed the study, created the software, collected, analyzed and interpreted the data, and drafted the manuscript. PY selected the test cohort independently of other authors. BTG and LW created the manually annotated standard. LW, PY, NG, EARM, HD, MD, and AJ assisted with data collection, analysis, and interpretation. JRL assisted with data interpretation. JEP supervised the study design, data collection and analysis. All authors read and approved the final manuscript.

This study was supported by the US NIH National Human Genome Research Institute (NHGRI) U01HG011758to the Baylor College of Medicine Genomic Research to Elucidate the Genetics of Rare disease center (BCM-GREGoR).