Official websites use .govA.govwebsite belongs to an official
                            government organization in the United States.

Secure .gov websites use HTTPSAlock(LockLocked padlock icon) orhttps://means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.

This paper includes independent research funded by the UK National Institute for Health and Care Research (NIHR) under its Programme Grants for Applied Research scheme (Grant Reference Number RP-PG-1209-10040). The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health and Social Care.

Received 2025 Mar 26; Accepted 2025 Jul 23; Collection date 2025.

This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See:https://creativecommons.org/licenses/by/4.0/.

We draw on the experience of our collaborative evolution towards a primary care learning health system and consider the conditions necessary for such a system. We call for greater integration of research and quality improvement and a sharper definition of learning health systems.

Clinical research can only benefit patient and population health if findings are incorporated into routine care. There are delays and inappropriate variations in the uptake of evidence-based care and withdrawal of low-value or even harmful treatments.5This translation gap limits the health, social and economic impacts of clinical research. Persistent inappropriate variations in care undermine efforts to achieve equity of outcomes; their magnitude cannot be explained away by population and casemix factors.

However, there are pitfalls in applying this evidence base to improvement efforts. First, it is hard to predict with confidence whether a given implementation strategy will work for a given targeted evidence-based practice. Some degree of judgement and acceptance of risk is inevitably required. Second, the effectiveness of most implementation strategies is typically modest, although still potentially important at a population level. This is partly because new randomised trials often test implementation interventions against control (no intervention) conditions rather than actively exploring how to enhance effectiveness through head-to-head trials of different interventions; this contributes to research waste.9Third, there is limited evidence on the cost-effectiveness of implementation strategies and uncertainty about which targeted priorities would yield the greatest returns on investment, thereby handicapping decision-making in the face of competing priorities.

Our academic-commissioner collaboration has started to address these barriers to implementation. We began with an agreement that the academic team would lead bids for competitive grant income focused on implementation research while the primary care commissioner (responsible for planning and purchasing services to improve population healthcare and outcomes) would act as the host National Health Service (NHS) organisation for research and actively support governance and delivery. The commissioner also provided pump-priming funding for posts to augment bidding capacity. Despite successive commissioner reconfigurations, there has been continuity in the personnel supporting research functions, now based within one NHS Integrated Care Board for West Yorkshire.

Learning health systems offer a vehicle for optimising implementation strategies and building a cumulative science. This includes the design and delivery of head-to-head trials of interventions. As well as adding to knowledge on how to enhance effectiveness, such trials can appeal to healthcare commissioners and providers as no participating practices are denied interventions. For example, a primary care organisation with an established audit and feedback programme may wish to establish whether adding practice facilitation to feedback makes a worthwhile difference. Sometimes there is a strong underpinning evidence base for such changes. However, there is often uncertainty around whether any changes have resulted in desired impacts. Examining trends over time is useful, but attributing any improvements or declines in performance to programme changes can be challenging, given the complex contexts of and multiple other influences on healthcare delivery (pandemics being an extreme example of a system disruption). This uncertainty about effectiveness is not only an ‘academic’ concern. Time and money spent on additional interventions such as practice facilitation will be wasted if they do not improve care. Randomised trials provide robust evidence of effects, but setting up and delivering individual trials can be costly and time-consuming in the absence of an established infrastructure.

Learning health systems can combine repeated cycles of data-driven improvement with robust evaluation. When they incorporate randomised or rigorous quasi-experimental evaluations, learning health systems produce reliable evidence of effectiveness. By virtue of being embedded within existing large-scale programmes using routinely collected data, they can be relatively efficient while delivering real-world evidence. Learning health systems represent a shift from researchers coming into healthcare systems to ‘do improvement and evaluation’ towards greater collaboration that directly addresses organisational needs and delivers research sustainably embedded within systems rather than becoming an ornamental burden.

Mutual stability to promote continuity in the partnership between healthcare system and researchers.

A shared understanding of equipoise to ensure that ‘negative’ evaluation results are not misrepresented as research failures or a lack of impact of improvement activities.

General practice trust in the data and improvement methods.

Systematic selection of clinical priorities for change so that they are underpinned by a strong evidence base, offer scope for improvement and can provide sufficient returns on investment.

Availability of data to assess performance and processes or resources for improvement.

Sufficient methodological skill mix in the core team with access to wider experience and skills as needed.

An ‘engine house’ to design candidate interventions which draws on practical knowledge, empirical evidence and theoretical perspectives.

Aligned resources and timelines for intervention delivery (eg, compiling and disseminating performance feedback) and evaluation (eg, design and analysis).

Stable programme funding to allow long-term planning and system evolution.

Regular contact to monitor and troubleshoot improvement and evaluation activities.

Proportionate approaches to ethical oversight and governance arrangements that balance protections and research burdens for individuals and organisations.

Demonstration of benefits to healthcare system and patient populations.

Mutual organisational stability is required to promote continuity in partnerships between healthcare systems and researchers and foster a shared vision. Our partnership has matured from being transactional (ie, dependent on mutual favours) towards becoming transformational (with genuinely shared goals). Our long-term aim is to develop a self-sustaining system which combines continuous improvement with rigorous evaluation. However, it takes time to build improvement activities and evaluations of increasing ambition. Although we are approaching two decades of collaboration, there are continuing threats to the sustainability of our learning health system, such as the uncertainties involved in securing competitive grant funding, changes in personnel and further health service reconfigurations. Our current work combines external funding from a series of short to medium-term research grants with NHS staff resourcing. Both of these are vulnerable to changing funding climates or changes in organisational leadership and priorities.

While learning health systems can drive systematic improvement, a shared understanding of the nature of experimentation is necessary to ensure that any ‘negative’ evaluation results are not misrepresented as failures. It is as useful to know what does not work as well as what does. Furthermore, a trial demonstrating that a new implementation strategy is not more effective than an existing strategy may take place against an underlying trend of improvement.

Clinical priorities for change should be systematically selected so that they are underpinned by a strong evidence base, offer scope for improvement and can provide sufficient returns on investment. Our work to date has been largely responsive; a more strategic approach could involve integrating health economics throughout recurrent evaluation cycles, that is, beyond a standard cost-effectiveness evaluation plugged into the end of a trial. For example, there are competing priorities for implementation and evaluation in primary care; are the greatest gains likely to be achieved through targeting diabetes, asthma or depression care and is this most likely to be cost-effective through audit and feedback, educational webinars or computerised decision support? Modelling of different scenarios may indicate which targets and interventions are (1) highly likely to be cost-effective, and hence worth implementation rather than evaluating, (2) promising but uncertain, and hence worth evaluating and (3) highly unlikely to be cost-effective, and hence not worth pursuing. This approach may help further reduce research waste while guiding ongoing implementation strategies.

The core team should possess a sufficient skill mix, including quality improvement, data management and analysis and implementation science, with scope to bring in other experience and skills as needed, such as topic-specific clinical leadership and qualitative methods. This means that any programme of work can take a holistic approach. While randomised trials and rigorous quasi-experiments (such as interrupted time series analyses) can answer questions about effectiveness, other parallel studies can inform delivery and generate knowledge. Process evaluations, often using qualitative methods of enquiry, can offer important insights into why interventions work or do not. Economic evaluations can inform decisions about whether any benefits outweigh intervention costs.

It can be challenging to align both resources and timelines for improvement activities and evaluation. For example, research and commissioning partners inevitably share risks in seeking competitive grant funding for evaluations and improvement activities that may need to be postponed to align with grant funding cycles and set-up processes for research. Therefore, stable programme funding can substantially facilitate long-term planning and system evolution.

Regular formal and informal contact between collaborators is essential to maintain relationships and to anticipate and solve problems. We have worked closely together to promote proportionate approaches to ethical oversight and governance that balance protections and research burdens for individuals and organisations. For example, our aforementioned randomised trials evaluating the effects of a multi-faceted intervention on the implementation of evidence-based indicators used an ‘opt-out’ approach to recruiting general practices.12Practices were included in the trials unless they actively declined participation. As well as minimising administrative burden for practices and facilitating the attainment of recruitment targets, this approach also meant that our findings were more likely to be applicable to most general practices than those which had expressed an interest in research.

We have established a partnership that integrates research and improvement in a drive for more effective, efficient and equitable care. Evidence of our evolution includes shifts from separate to shared goals, from being either researcher or service-driven to collaboratively owned, from being project-focused to programmatic and progression with varying levels of scale, methodological rigour and novelty.

There is a myth that there are necessary trade-offs between real-world relevance and robust research. There need not be any compromise. Rigorous effectiveness evaluations can be integrated within healthcare systems and large-scale improvement programmes, an approach that was advocated within the original conceptualisation of learning health systems.20In recognising future learning health systems, we propose that such features are definitional rather than optional.