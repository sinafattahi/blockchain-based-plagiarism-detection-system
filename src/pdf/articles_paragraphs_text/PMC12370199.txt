Auditory stream segregation plays a crucial role in understanding the auditory scene. This study investigates the role of tactile stimulation in auditory stream segregation through psychophysics experiments and a computational model of audio-tactile interactions. We examine how tactile pulses, synchronized with one group of tones (high- or low-frequency tones) in a sequence of interleaved high- and low-frequency tones (ABA- triplets), influence the likelihood of perceiving integrated or segregated auditory streams. Our findings reveal that tactile pulses synchronized with a single tone sequence (either the A-tone or B-tone sequence) enhance perceptual segregation, while pulses synchronized with both tone sequences promote integration. Based on these findings, we developed a dynamical model that captures interactions between auditory and tactile neural circuits, including recurrent excitation, mutual inhibition, adaptation, and noise. The proposed model shows excellent agreement with the experiment. Model predictions are validated through psychophysics experiments. In the model, we assume that selective tactile stimulation dynamically modulates the tonotopic organization within the auditory cortex. This modulation facilitates segregation by reinforcing specific tonotopic responses through single-tone synchronization while smoothing neural activity patterns with dual-tone alignment to promote integration. The model offers a robust computational framework for exploring cross-modal effects on stream segregation and predicts neural behavior under varying tactile conditions. Our findings imply that cross-modal synchronization, with carefully timed tactile cues, could improve auditory perception with potential applications in auditory assistive technologies aimed at enhancing speech recognition in noisy settings.

The difficulty in following a single voice or conversation in noisy situations is one of the main frustrations of people with hearing impairment, even when fitted with modern hearing aids [1]. This difficulty arises from the challenge of separating a single sound stream from many competing sounds, a process known as auditory stream segregation [2,3]. Auditory stream segregation has been widely studied with the auditory streaming paradigm, an idealized stimulus for which two sequences of pure tones can be segregated into different streams [4,5]. Stream segregation has been shown to be influenced by cross-modal interaction [6]. For example, synchronizing visual flashes with tones in an ABA- sequence has been shown to promote segregation, suggesting that temporal alignment across sensory modalities can bias perceptual organization [7,8]. Similarly, visual distractors in multitone masking paradigms have been found to enhance auditory selective attention and improve target detection within complex soundscapes [9]. Tactile cues have also been shown to improve the ability to segregate interleaved melodies, even under challenging listening conditions [10]. Tactile stimuli have the capacity to improve speech recognition in noise [11–13]. However, our understanding of the underlying neural dynamics of this process is still limited.

Sensory substitution theory provides a compelling framework for understanding how one sensory modality can compensate for a deficit in one or augment another during complex perceptual tasks [14,15]. Auditory perception, for example, can be influenced by stimuli from other sensory modalities, such as vision [16]. Although the impact of visual input on auditory perception has been extensively studied, the influence of somatosensory (tactile) stimuli remains less well understood [17–19]. While research suggests that tactile inputs can modulate auditory perception [20,21], it is unclear which attributes of tactile and auditory signals (e.g., frequency, intensity, rhythm, timing, and duration) play the most critical role in enhancing or suppressing the combined sensory experience.

In [22], the influence of tactile distractors on the ability to discriminate the frequency and intensity of auditory tones was studied through a series of psychophysical experiments. The authors demonstrated that auditory frequency perception was systematically biased by tactile distractors: distractors at frequencies lower than that of the auditory tones induced larger bias effects than distractors at higher frequencies. They also observed that tactile distractors biased the intensity of auditory perception. The magnitude of this effect scaled with distractor intensity, but did not vary with distractor frequency [22]. These findings highlight the complex interplay between tactile and auditory modalities, raising questions about the nature of cross-modal interactions between touch and sound, as well as the specific neural mechanisms underlying audio-tactile interactions.

Multisensory integration is traditionally thought to occur in higher association cortices, from which multisensory signals are relayed to (subcortical) areas involved in planning and executing actions [23,24]. According to this view, multisensory integration occurs only after unisensory information has been thoroughly processed along its specific sensory hierarchy. However, recent results challenge this notion and suggest that multisensory interactions can occur in early sensory areas. In particular, fMRI [25–29] and electrophysiological studies in monkeys [30–32] found multisensory activations in brain areas considered unisensory, supporting the idea that multisensory integration can occur early in sensory processing through feedforward mechanisms, independent of attention (preattentive bottom-up mechanisms [25]).

The audiotactile cross-modal interaction could occur due to both direct and indirect pathways connecting auditory and somatosensory processing areas [33]. Some neurons within the primary auditory cortex (A1) respond to tactile stimuli, directly reflecting tactile processing in auditory regions [30,34]. Tactile pulses can influence tonotopic responses in the auditory cortex [35]. Although A1 is traditionally associated with processing sound frequencies, studies show that it can also respond to multisensory inputs [21,36], such as tactile, particularly when those stimuli are temporally or spatially relevant to auditory processing. Tactile stimulation, especially rhythmic pulses, can influence the timing and phase of neural oscillations in A1 [37,38], potentially enhancing the synchronization of responses. This is more likely if the tactile input is closely synchronized with auditory input, as it can alter the salience of specific frequencies [39,40].

Mathematical modeling has advanced our understanding of bistable phenomena including auditory streaming [41,42] and tactile rivalry [43]. A variety of models have been proposed to account for the segregation of two streams of sounds [41,44]. Rankinet al. [45–47] proposed a neuromechanistic model that captures the complex dynamics of perceptual alternations, including the dependence of perceptual durations on parameters such as frequency differences and presentation rate. The model reproduces characteristic features, such as the log-normal distribution of perceptual durations and the build-up effect. The model replicates the periodic, pulsatile responses of A1 and its dependence on stimulus features, which are pooled as inputs to a downstream competition stage. The competition between units arises from a combination of mutual inhibition, adaptation, and additive noise mechanisms, which are thought to contribute to perceptual bistability at cortical stages [48–50].

Although multisensory integration has received significant attention within the neuroscience community, the number of mechanistic mathematical models of multisensory integration remains limited. To bridge this gap, we propose a novel computational model for audio-tactile integration that can be generalized to other cross-sensory interactions. By integrating neuromechanistic modeling with psychophysics experiments, we elucidate how tactile sensations influence the perception of multiple sound sources and the underlying neural computations driving audio-tactile interactions. The proposed model extends the mathematical framework of auditory streaming. Specifically, we use the hypothesis that tactile inputs enhance excitation in the tonotopic response within the primary auditory cortex to extend the model of interactions between primary (A1) and non-primary auditory cortices presented in [45]. We experimentally validate the model. Our model aligns with preattentive bottom-up mechanisms proposed in [25]. Our work not only provides a robust platform for exploring audio-tactile interactions but also sets the stage for investigating other cross-sensory integrations, enhancing our fundamental understanding of multisensory processing.

Approval was obtained from the ethics committee of the University of Exeter (eEMPS000058). The procedures used in this study adhere to the tenets of the Declaration of Helsinki. Written formal consent was obtained from all individual participants included in the study.

Six volunteers (3 male, mean age33.83±6.74SD) were recruited for Experiment 1 and twelve volunteers (7 male, mean age35.5±10.72SD) were recruited for Experiment 2 from the University of Exeter. Each gave written informed consent and received minor monetary compensation for participating in a 1-hour session. Participants were naive to the purpose of the study and did not self-declare any neurological or sensory disorders. Procedures were in compliance with guidelines for research with human participants and approved by the University of Exeter Research Ethics Committee.

Here we used the auditory streaming paradigm [51] in which participants listen to a sequence of interleaved high- and low-frequency tones repeated in ABA- triplets (“A” and “B” tones, “-” silent gap) (Fig 1A). They were instructed to report the “integrated” percept when they perceived these sequences either as a single integrated stream (A B A - A B A - A B A -), and the “segregated” percept when they heard two distinct streams: one consisting of only A tones and the other of only B tones (concurrent: A - A - A - A - A - A - and - B - - -B  - - - B - -) (Fig 1B). Participants sat in a sound-isolated booth and attended to auditory stimuli while vibrotactile stimulators were attached to their left index finger. To ensure that participants fully understood these interpretations, auditory and visual demonstrations were provided. Participants were instructed to report their perceptions passively, without trying to favor one organization over the other. They used keyboard presses to indicate their perceptual responses.

A:Repeating ABA- triplet sequences are composed of higher-frequency pure tones (A) interleaved with lower-frequency pure tones (B), each with a durationTD, and separated by a frequency differenceΔf. The time interval between successive tone onsets (indicated by dashed vertical lines) corresponds to the inverse of the presentation rate (1/PR). The “-” in ABA- represents a silent interval of duration 1/PR. In this study, tone duration is set toTD= 1/PR, ensuring that the offset of an A tone aligns precisely with the onset of the subsequent B tone.B:The stimulus is perceived in one of two ways: either as an integrated single stream (ABA-ABA-) or as two segregated streams (A-A-A-A- and -B—B–). Each experimental trial consisted of five consecutive ABA- triplets, and participants were asked to report their perception as integrated or segregated for the last triplet of the 5-triplet sequence.C:In the Tact AB condition, tactile stimulations atfToccur during both A- and B-tone intervals. In contrast, in the Tact B and Tact A conditions, tactile stimulations occur only during B-tone or A-tone intervals, respectively.

The auditory stimuli consisted of five consecutive ABA- triplets, where each triplet included125mspure tones (A and B) followed by a125mssilence (“-”), resulting in a total triplet duration of500ms. Cosine-squared ramps of10mswere applied to the onset and offset of each tone to ensure smooth transitions and avoid acoustic artifacts. The higher frequency A tones were a variableΔfsemitones (st) above the lower frequency B tones. A minimum2sinterval between trials was used after which participants could run the next trial when ready.

Vibrotactile stimuli consisted of125mssinusoidal vibratory pulses at200Hz. We used tactile stimulation synchronized with a subset of the tones in an ABA- triplet. The tactile pulse amplitude was calibrated prior to the experiment to be clearly perceptible but non-intrusive, based on pilot testing with naive participants. Three different tactile pulse timings were considered (Fig 1C): one trial with tactile pulses synchronized with A tones (Tact A), one with tactile pulses synchronized with B tones (Tact B), and one with tactile pulses synchronized with both A and B tones (Tact AB). Additionally, a trial without tactile stimulation (audio only, Tact Off) was included. The tactile pulse amplitude was fixed across all conditions to isolate the effect of timing and alignment. Participants were instructed to focus on auditory stimuli and report their perceptions based on the final auditory triplet presented in each trial.

The sequence of tones was played binaurally through Sennheiser HD 400 PRO headphones. We used miniature vibrotactile electromagnetic solenoid-type stimulators (18mmdiameter, Dancer Design tactors [52]) driven by a tactile amplifier (Dancer Design Tactamp [52]) to deliver tactile stimuli. The voltages applied to the tactors for200Hzsinusoidal vibration was3.38V. In Experiment 1, each of the 6 participants completed 240 trials, consisting of 20 repetitions for each of three tactile conditions (Tact B, Tact AB, and Tact Off), combined with four levels of frequency difference (Δf={3,4,5,6}st). We used a 6 × 6 Latin square design with 40 randomized and unique grids so that the order of conditions for each participant in a block of trials was counterbalanced within/across participants and block repetitions (one block of 6 participants and 40 repetitions). In Experiment 2, each of the 12 participants completed 160 trials, consisting of 40 repetitions for each of the four tactile conditions (Tact B, Tact AB, Tact Off, and Tact A) at a fixed frequency difference (Δf=4st). Here we used a 4 × 4 Latin square design with 120 randomized and unique grids (three blocks of 4 participants and 40 repetitions).

To investigate the effects of frequency difference (Δf) and tactile condition on the proportion of trials reported as segregated, we employed a generalized linear mixed model (GLMM) with a logit link function [53], accounting for repeated measures within participants. This statistical analysis method is useful for modeling non-normally distributed responses, such as binary outcomes (segregated, not segregated), and incorporates both fixed effects and a random intercept for participants to account for individual variability [54]. The fixed effects in the model estimate population-level coefficients, representing the relationship between predictor variables and the response, while random effects control for individual variations.

Different GLMMs were fitted to explore the main effects ofΔf, tactile conditions, and their interaction. The GLMM provides coefficient estimates that indicate the strength and direction of the effect of predictor variables on the outcome; a positive coefficient indicates that an increase in the predictor variable raises the likelihood of the outcome occurring, while a negative coefficient suggests the opposite. The absolute value of the coefficient represents the strength of the relationship, with larger values indicating a stronger effect. The significance level of 0.05 is used throughout this paper. All statistical analyses were conducted in the statistical packageR. To estimate Cohen’s effect size [55], we first compute the odds ratio (OR) based on the observed frequencies of the binary outcome variable in Experiment 1. This odds ratio compares the odds of the event occurring in the test group to those in the control group. For the sample size estimation, we used G*Power [56] with a z-test for the difference between two independent proportions, employing an allocation ratio of 1 to achieve 80% power.

To investigate the effect of tactile pulses on the segregation of auditory streams, we developed a mechanistic mathematical model that captures interactions within auditory and tactile neural circuits. The neuronal circuits for competition and perceptual encoding included in the model are assumed to be downstream and receive inputs from the primary auditory cortex (A1). Neuronal activity is represented by mean firing rates, with competitive interactions arising through excitatory and inhibitory connections, slow adaptation, synaptic depression, and intrinsic noise.

Here,rkare the mean firing rates of each population, indexed byk={A,AB,B}. Dynamics of each population also depends on, spike frequency adaptation variableak, the recurrent NMDA-excitation variableek, and the synaptic depression of excitatory connectionsdk.τr,τa,τeandτdare synaptic time constants of respective variables.βeis the strength of recurrent NMDA-excitationek, which is modulated by the slow synaptic depressiondk.βiis the strength of instantaneous inhibition by the other populationsrk. Inhibition from therABunit to therAandrBunits is twice as strong.gis the strength of spike-frequency adaptation,ak.κis the strength of the slow synaptic depression.

Input termsTAandTBrepresent tactile pulses that are synchronized with tone A and tone B, respectively. These inputs are defined by the doubleα-function given inEq 4. The spread of these tactile pulses across tonotopic locations is defined by the tactile weighting functionν(Δf)=Mtkexp(−Δfσtk), with parametersσt1andMt1when tactile pulses are synchronized with one tone (Tact A or Tact B), and parametersσt2andMt2when tactile pulses are synchronized with both A and B tones (Tact AB).

whereτXis the timescale,γthe strength andζ(t)a white noise process with zero mean.

Values of all model parameters defined throughout this section are given inTable 1. Simulations were run in MATLAB using a standard Euler-Murayama time stepping scheme with a stepsize of1ms. Reducing this stepsize by a factor of 10 did not change the results.

In Experiment 1, we investigate the effect of tactile pulses on the ability to segregate sound streams. In the experiment, participants listen to a sequence of five auditory triplets, consisting of interleaved high- and low-frequency tones arranged in an ABA- pattern (Fig 1A). The frequency difference was varied across a range, including lower frequency differences where the proportion of perceptual segregation tends to be low. They report whether they perceive the tones as a single, integrated stream or as segregated into two streams, based on their perception of the final triplet. We used tactile stimulation synchronized with a subset of the tones in an ABA- triplet (Fig 1C). Data were collected from 6 participants, repeating 20 times each of 12 conditions (1440 observations in total), with each participant reporting their perception across various conditions and frequency differences. When tactile stimulation timing matches only the B tone sequence, the proportion of trials in which participants report perceiving the sounds as segregated increases. When the tactile pulse timing matches the A and B tones a bias towards integration emerges. This effect was observed over a range of frequency difference values (Fig 2A).

A:Experiment 1 (Combined effect of frequency difference and tactile conditions): Data were collected from six participants, each completing 20 repetitions at four frequency difference levels (Δf={3,4,5,6}) crossed with three tactile conditions, Tact B, Tact Off, Tact AB. Participants reported their perception across various tactile conditions and frequency differences. The proportion of trials in which the final (5th) triplet was reported as segregated increases withΔfand increases (decreases) in Tact B (Tact AB) relative to Tact Off. Error bars show the standard error of the proportion segregated.B:Experiment 2 (replicability, model validation): This experiment examines a fixed frequency difference (Δf=4) and includes an additional tactile condition in which tactile pulses are aligned with tone A in ABA triplets. Compared to the reference condition Tact Off, the condition Tact B had a marginally significant positive effect, while Tact A and Tact AB exhibited significant positive and negative effects, respectively. Participants’ perception of segregation was significantly lower in Tact B compared to Tact A.C:The change in proportion segregated compared to reference condition Tact Off.

To investigate the relationship between frequency difference (Δf) and tactile condition and their effect on the perception of segregation in participants, we used a generalized linear mixed model (GLMM) analysis. The fixed effects ofΔfand tactile condition and their interaction were examined to assess how they influenced the likelihood of segregation. A random intercept for participants was included to account for individual variability. The analysis revealed significant fixed effects for bothΔfand tactile conditions. Specifically, the coefficient forΔf(Coeff. estimate =1.01,p< 0.001) indicates a strong positive relationship between frequency difference and the likelihood of perceiving segregation, demonstrated in other studies without tactile stimulation [3,4,45]. Additionally, Tact B showed a positive effect (Coeff. estimate=1.28,p= 0.047), while Tact AB exhibited a non-significant negative effect (Coeff. estimate = −0.64,p= 0.35). Notably, the interaction terms betweenΔfand the tactile conditions did not reach statistical significance, suggesting that tactile conditions have a consistent effect and do not vary significantly across different frequency differences (Table 2).

We also conducted statistical analyses on two reduced models that separately examined theΔf:Tact AB andΔf:Tact B interactions. In both cases, the interaction terms remained non-significant. Consequently, we adopted a simplified model excluding the interaction terms. This model revealed significant main effects forΔf(Coeff. estimate = 0.96,p< 0.001) and both tactile conditions Tact B (Coeff. estimate = 0.42,p= 0.005) and Tact AB (Coeff. estimate = −0.32,p= 0.033); seeTable 3.

Analyzing the data at a specific frequency difference (Δf=4) showed that Tact B had a positive effect on segregation perception compared to Tact Off (Coeff. estimate = 0.36,p= 0.179), whereas Tact AB exhibited a negative effect (Coeff. estimate = −0.45,p= 0.103). However, neither effect reached statistical significance (Table 4). The odds ratios of Tact AB and Tact B compared to the reference level of Tact Off are equal to 0.64 and 1.44, corresponding to Cohen’s d (log odds ratio) 0.25 and 0.20, respectively. We used the smaller effect size of 0.2 of the tactile stimulation in Experiment 1 to estimate the sample size of a follow-up experiment (Experiment 2). To detect an effect size of 0.2 with80%power using a z-test for the difference between two independent proportions, G*Power [56] estimates a required sample size of at least 444 (equivalent to 12 participants completing 40 repetitions for each condition).

Results from Experiment 1 reveal that tactile pulses have an effect on the ability to segregate auditory streams. Tactile pulses synchronized with tone B promote segregation, while those synchronized with both A and B tones promote integration. This suggests a complex interaction between tactile and auditory modalities, where the timing and context of tactile input are crucial. In Experiment 2, we investigate tactile stimulation at a fixed frequency difference (Δf=4, close to equidominance point) to better understand the effects of temporal alignment of tactile and auditory stimuli. To this end, Experiment 2 also includes an additional tactile condition in which tactile pulses are aligned with A tones in ABA- triplets (in this way participants are exposed to twice as many tactile stimuli as in Tact B condition). The results of the experiment are shown inFig 2B. The change in the proportion segregated for each tactile condition compared to the reference condition Tact Off is depicted inFig 2C. Values greater than zero indicate a bias towards segregation, while values less than zero indicate a bias towards integration.

Compared to the reference condition Tact Off, Tact AB had a significant negative effect (Coeff. estimate = −0.54,p< 0.001), while Tact A (Coeff. estimate = 0.53,p< 0.001) and Tact B (Coeff. estimate = 0.26,p= 0.048) exhibited significant positive effect; see alsoTable 5. Compared to Tact B, Tact A demonstrates a significantly higher proportion of segregation (Coeff. estimate = 0.27,p= 0.046).

The model includes three neuronal populations. Two of them pool inputs from A1 regions centered on the frequencies A and B. The third population receives input from an intermediate tonotopic location, approximately(A+B)/2; seeFig 3A). The specific frequency tones exhibit full amplitude at their respective tonotopic locations, gradually decaying as they spread across other tonotopic locations (decay functionω(Δf)). Similarly, the effect of tactile pulses is assumed to diminish across tonotopic locations, as illustrated by the changes in inputs corresponding to Tact B (represented by dashed curves inFig 3B). When tactile pulses are synchronized with B-tones, they enhance the tonotopic response at location B and also induce an excitatory effect on other locations (AB and A), though to a lesser extent. We assumed a similar pattern of excitation for tactile pulses synchronized with A tones (decay functionν(Δf)). However, when tactile pulses were synchronized with both tones, we considered a decaying effect, modeled with a different profile, to account for any residual impact from one tactile pulse to the next, as they occur at a shorter temporal distance. The top panel inFig 3Cshows a model simulation of the recurrent excitation variables for each population without the tactile effect (Tact Off). When the central AB unit is active (integrated), peripheral units are suppressed through mutual inhibition. Increasing adaptation for AB raises the probability of noise-induced switching, leading to the activation and dominance of units A or B (segregated), which in turn suppresses the integrated (AB) unit. The panels below depict predicted percepts under various tactile conditions, showing that Tact B increases the segregation proportion, while Tact AB decreases it compared to the Tact Off condition.

A:Neuromechanistic model with competition between units (rA,rAB,rB) driven by inputs from three locations in tonotopic map in A1 (at A, B and a location in between). The competition stage is located downstream of (and takes input from) A1, with mutual inhibition between units (βi), recurrent excitation (βe), slow adaptation (γ) and noise-driven competition.B:Inputs to the respective populationsrA,rABandrBfor an ABA- triplet which shows the spread of inputs across the model’s tonotopy. The A-tone and B-tone inputs (IAandIB) have full amplitude at their respective tonotopic locations and gradually decay as they spread across other tonotopic locations (solid curves). The effect of tactile pulses is also considered to decay across tonotopic locations (changes in inputs corresponding to Tact B represented by dashed curves).C:The top panel displays a model simulation of the recurrent excitation variables for each population without tactile effect (Tact Off). When the central AB unit is active (purple/ integrated), peripheral units are suppressed via mutual inhibition. Increasing adaptation in the AB unit raises the likelihood of noise-induced switching. The panels below show predicted percepts for different tactile conditions with tact B (Tact AB) increasing (decreasing) the segregation proportion compared to Tact Off condition.

To investigate the mechanism of the effect of tactile pulses on the segregation of auditory streams we checked if the model presented in the section “Mathematical model for audio-tactile interaction” can be used to reproduce experimental observations. To this end, we used a genetic algorithm optimization implemented in Matlab functiongato find parametersMa,Mt1,Mt2andσa,σt1,σt2of theω(Δf)andν(Δf)functions; used in Eqs (3and4), respectively. The other model parameters were fixed throughout the optimization procedure. We defined the cost function as the mean squared error between the proportions of segregation observed in experiments and the proportions of segregation obtained in simulations. Details of the optimization procedure can be found in the computer code associated with the paper.

Results of the simulations for the optimal parameter values are shown inFig 4. Experimental data are presented as bar plots, while results of the model simulations are shown as dashed curves with data points at different frequency differences (Δf={3,4,5,6}st) and varying tactile conditions (Fig 4A). TheΔf-dependent profiles for the auditory input spreadω(Δf)(Fig 4C) and tactile input spreadν(Δf)(Fig 4D) were determined through the optimization algorithm. The decaying input function was estimated to have an amplitude ofMa=0.95±0.04and a slope ofσa=3.10±0.12. For the case where tactile pulses are aligned with one tone, the parameters areMt1=0.45±0.12andσt1=1.2±0.22. When tactile pulses are aligned with both tones, the amplitude isMt2=0.10±0.04and the slope isσt2=18±7. Confidence intervals are based on 20 repetitions of the fitting procedure. The effect of tactile input amplitude is smaller compared to auditory input, and the decay across the tonotopic representation is steeper in single-tone alignment than in auditory decay, while remaining almost uniform in dual-tone alignment.

A-B:Experimental data are presented as bar plots, while simulated data are shown as dashed curves with data points at different frequency differences (Δf=3,4,5,6st) and under different tactile conditions. Error bars show the standard error of the proportion segregated.C-D:TheΔf-dependent profiles for the auditory input spreadω(Δf)and tactile input spreadν(Δf)(modeled as exponential decays) were derived using an optimization algorithm minimizing the mean squared error between experimental and computational data from Experiment 1. These weighting functions were then used to compare computational model predictions with experimental data in Experiment 2.

We used model parameters estimated using data from Experiment 1 to run a new set of simulations that included tactile stimulus synchronized with tone A. We compared the results of the new simulation with data from Experiment 2. Without any additional fitting, the model made an accurate quantitative prediction that the level of segregation under the new tactile condition, Tact A, is higher than Tact B.Fig 4Billustrates excellent alignment between the model and the experimental data.

By synchronizing tactile pulses with one group of tones in the ABA- triplet sequence, we explored the perceptual shifts between integration (one stream) and segregation (two streams) based on the timing and pattern of tactile stimulation. Our findings imply that cross-modal synchronization can dynamically shift perceptual boundaries in auditory streaming. When tactile pulses coincided only with tone B, experiments demonstrated an increased likelihood of segregating the tones. When both A and B tones were paired with tactile pulses, participants perceived the sounds as a single stream more often. This suggests that tactile stimulation synchronized with a single group of tones in the sequence enhances perceptual distinction between tones, reinforcing the auditory separation needed for segregation, while alignment of both tone sequences and tactile pulses provides a unified cross-modal cue, leading the auditory system to interpret the sequence as more cohesive. Tactile conditions have a consistent effect across various frequency differences suggesting that tactile timing relative to sound structure is a strong factor in stream segregation.

In our model, tactile pulses modulate auditory perception by altering tonotopic responses in the auditory cortex. In the primary auditory cortex, neurons that respond to similar frequencies are organized in clusters along the tonotopic map [58]. Excitatory neurons activate neighboring frequency-tuned neurons, while inhibitory interneurons create lateral inhibition, sharpening frequency specificity and limiting spread across the map [59]. When tactile pulses align with a single group of tones, they appear to selectively boost responses in regions associated with that tone, with the effect decaying across the tonotopic map. This supports segregation by enhancing contrast in tonotopic representation. When both tones are paired with tactile stimuli, broader, more uniform activation across tonotopic areas reduces this contrast, promoting integration.

The increased segregation observed when tactile pulses align with tones A in the ABA- triplets, compared to B tones, is particularly notable due to the presence of two tactile pulses instead of one. With two tactile stimuli per ABA triplet, the tactile impact curve decays as sharply as before. However, because the tactile stimuli are now repeated twice within a triplet, they reinforce the auditory processing of tone A, leading to a more pronounced segregation effect.

We hypothesize that the observed effect could be explained through neural mechanisms in the auditory cortex. When tactile pulses synchronize with auditory tones, they interact with tonotopic circuits, activating frequency-tuned neurons and spreading in a balanced manner controlled by excitation and inhibition. When tactile pulses activate neurons associated with both tones, the individual effects integrated in time lead to a combined activation in the neural responses that do not decay as sharply across the tonotopic map. This could be due to recurrent excitation, where the remaining activity from one pulse keeps the neural population primed, leading to a more sustained response. The combination of effects at both tonotopic locations could create a stable perceptual outcome that enhances integration, as the stimulation is spread out more evenly [60].

Alternatively, the experimental observations could be explained by temporal averaging of excitatory and inhibitory effects. Such averaging would lead to the tactile impact curve that is smaller and smoother when tactile pulses align with both tones versus just one. When tactile pulses are synchronized with both tones, the excitatory and inhibitory responses may partially overlap and combine across time, leading to a more balanced, averaged response across the tonotopic map. This smoothing effect would reduce the peak strength of activation but maintain it across a wider spatial area, resulting in a smaller yet more stable effect. In contrast, with tactile pulses aligned with one tone, the activation is enhanced for that specific frequency region. Such localized amplification could explain the stronger but spatially limited effect in the one-tone condition.

Using the statistical GLM model we observed no significant interaction between frequency difference (Δf) and tactile pulses. However, an interaction effect with a small effect size may still exist (particularly in the case of Tact B) but could remain undetected given the statistical power of our study. This shows that within the limits of our study’s power, the model does not require the inclusion of multiplicative terms. Additionally, theΔf-dependant additive term results in a small interaction effect. This approach provides a simplified framework to capture the subtle tactile effects without introducing complex interaction terms. For the current dataset, an additive model appears sufficient. However, we cannot entirely rule out the presence of a subtle interaction that might be detected with a larger sample. This could be investigated further in future studies.

Our model parameter fitting approach allows us to estimate the strength ofΔf-dependent additive effects and assess the degree ofΔfdependency across different tactile conditions. The results from parameter fitting through optimization indicate that these effects are stronger and moreΔf-dependent in the case of Tact B, compared to the Tact AB effect, which appears weaker and less dependent onΔf(seeFig 4). These findings are consistent with the visual interaction effect of Tact B observed inFig 2.

Earlier studies using MEG [61,62] and intracranial EEG [63] have highlighted the potential role of mutual inhibition and selective attention in auditory streaming, suggesting that attention may modulate streaming through the sharpening of tonotopic representations. Our findings align with this framework, indicating that tactile pulses can serve as attention-enhancing cues that selectively reinforce tonotopic responses (top-down attentional mechanism). While our model emphasizes a bottom-up, feedforward modulation of tonotopic activity by tactile input, we acknowledge that attention-based enhancement of the attended tone stream via tactile pairing may also contribute to the observed perceptual biases. Future studies employing explicit manipulations of attention or incorporating distractor tasks could help disentangle the relative contributions of bottom-up and top-down mechanisms.

While our model is not biophysical and does not distinguish between cortical layers or neuronal types, it is still useful to consider potential biological substrates. Research shows that both intra-modal and cross-modal interactions often target superficial cortical layers, including the apical dendrites of pyramidal neurons in sensory cortex [64,65]. Within this framework, tactile input may influence auditory processing by increasing excitatory input to apical dendrites in frequency-specific columns of auditory cortex. This could modulate neural gain or bias perception toward tones that are synchronized with tactile input, aligning with our psychophysical findings and model behavior. Although our approach simplifies cortical architecture, the proposed model aligns with evidence that apical dendritic excitation integrates contextual and multisensory inputs. Future work could incorporate anatomically based, layered circuits to explore how dendritic processing shapes cross-modal auditory perception.

Rankinet al. (2015) [45] proposed a neuromechanistic model capable of capturing the build-up effect through a gradual decrease in A1 response amplitudes applied to the first three triplets, thereby replicating the favor of integration during the build-up phase. Here we used a simplified version of this model [46] without amplitude modulation. This decision was driven by the current uncertainty surrounding how tactile inputs are processed during the build-up phase, and whether their influence strengthens or diminishes over time. Further experiments specifically designed to capture the influence of tactile pulses during the early triplets, and to assess whether this effect gradually changes across subsequent triplets, would help quantify the interaction of these effects.

One notable point of comparison between our findings and previous work arises when examining the proportion of segregation reported at intermediate frequency differences. InFig 2of our study, participants reported segregation approximately 65% of the time atΔf=5. This contrasts with the model presented in Rankinet al. (2015) [45], where equidominance between integration and segregation was observed nearΔf=5. This difference likely reflects both variations in experimental design (Rankinet al. [45] modeled long trials with full time-based dynamics, whereas we used short sequences with perceptual reports based on the final triplet) and differences in participant characteristics (e.g., age range, musical training, attentional strategies).

Based on the current stage of our model, which is built upon the present dataset, we predict that tactile pulses aligned with either the first or last A tones would bias perception toward segregation. Our ongoing experiments investigate the effects of tactile pulses that are asynchronous with the auditory tones (specifically, when tactile pulses lead or lag the tones), and we are expanding the model to incorporate neural mechanisms that may underlie these effects. Tactile pulses delivered in the inter-tone gaps could either reduce interference or enhance temporal binding, depending on their proximity to the A and B tones and the number of repetitions in each triplet.

Fig 2suggests that tactile stimulation may be more effective at promoting segregation at lowerΔfvalues, and more effective at promoting integration at higherΔfvalues. This pattern might become even more pronounced at more extreme frequency differences beyond the range tested in our study. Future studies could investigate whether tactile cues exert their strongest influence near perceptual thresholds. Such investigations will help constrain future model development and clarify how temporal alignment and frequency separation jointly shape cross-modal perceptual organization.

Previous studies have demonstrated that tactile distractors can bias auditory frequency perception, shifting perceived frequencies toward those of the tactile pulses [22]. This bias is more pronounced with lower-frequency tactile distractors compared to higher frequencies. In this study, we specifically used tactile pulses with frequencies lower than the auditory tones. Future research could investigate the impact of higher-frequency tactile pulses on auditory perception to determine if they produce a different bias or interaction pattern, deepening our understanding of multisensory integration dynamics across varying frequency contexts.

Our experimental and mathematical modeling results reveal a spatio-temporal interaction where tactile input dynamically influences auditory organization through timing and tonotopic alignment. Specifically, our findings show that dual-tone alignment creates a broad, stable neural response, while single-tone alignment produces a sharper, more localized effect. This interaction suggests that tactile cues, precisely timed and placed, can be employed to fine-tune auditory processing. Such intricate control offers valuable insights into multisensory integration, especially for the design of devices for individuals with hearing impairments or difficulties with speech recognition in noisy environments. By strategically incorporating tactile feedback, our approach can improve and optimize the design of these devices, enhancing auditory clarity and speech recognition.

This study highlights the significant role of tactile pulses in modulating auditory perception, demonstrating that synchronized tactile input can influence the integration and segregation of auditory streams through spatiotemporal interactions. These effects arise from interactions within tonotopic circuits, where excitatory and inhibitory dynamics shape the perceptual outcome. Single-tone alignment creates localized enhancement and dual-tone alignment generates broader, averaged responses. The consistency of tactile effects across varying frequency differences highlights the robustness of tactile-auditory interactions. These insights open pathways for future research to explore how tactile inputs of different frequencies influence auditory perception, further advancing our understanding of multisensory integration. Importantly, the ability to fine-tune auditory processing using precisely timed tactile cues has promising applications, particularly in assistive technologies. This work lays the foundation for innovative solutions that capitalize on multisensory integration to optimize auditory experiences.

We would like to thank Marc Goodfellow and Mark Fletcher for their valuable discussions and insightful suggestions during the early stages of this project.