This paper proposes a unified data-driven framework for topology identification, risk quantification, and reconfiguration optimization in power distribution networks under incomplete and fragmented observability. Motivated by real-world challenges where asset metadata, SCADA records, GIS layouts, and dispatcher logs are misaligned or incomplete, the proposed approach reconstructs network topology using a graph convolutional network (GCN) that fuses heterogeneous data attributes and learns structural representations from partial connectivity information. On the inferred topology, a scenario-based risk evaluation model is formulated to capture both local fragility and spatial risk propagation, integrating factors such as load stress, asset aging, and nodal redundancy into a unified zone-level risk index. To mitigate this risk, a bilevel reconfiguration optimization model is developed, in which the upper level minimizes cumulative risk and switching cost while maximizing load restoration, and the lower level enforces electrical feasibility under contingency-aware constraints. The full pipeline is tested on a 58-node synthetic distribution system with embedded DERs, showcasing the ability of the framework to reduce peak nodal risk by 52.7%, restore over 94% of total demand in 90% of scenarios, and maintain tractable computation times under 9 mins per scenario across 100 fault cases. A suite of detailed visualizations–including confidence-based topology maps, switching heatmaps, congestion-weighted flow diagrams, and fairness-control tradeoff surfaces–demonstrates the interpretability and operational relevance of the results. The proposed framework offers a scalable, adaptive solution for resilient distribution network management under uncertainty and fragmented digital infrastructure.

In recent years, the increasing interdependence between transmission and distribution systems has emerged as a defining feature of modern power grid architecture1,2. Traditionally, transmission and distribution networks were modeled, operated, and managed as functionally distinct subsystems, often administered by separate operational entities using independent data infrastructures. However, the growing integration of distributed energy resources (DERs), the expansion of inverter-based generation, and the increased frequency of high-impact events–such as extreme weather, cyberattacks, or component failures–have revealed the systemic vulnerabilities created by this fragmentation3,4. System operators now face the daunting challenge of ensuring reliability and resilience across infrastructure layers that are topologically complex, operationally interlinked, and data-wise fragmented. The ability to observe, assess, and control such systems in an integrated manner is no longer a future vision but an operational necessity5–7.

At the heart of this challenge lies a simple but consequential problem: utilities often do not possess an accurate or up-to-date model of their own network topology. In theory, each utility should maintain a complete model of its electrical infrastructure via GIS (Geographic Information Systems), EMS (Energy Management Systems), SCADA (Supervisory Control and Data Acquisition), and asset registries3,8,9. In practice, however, these datasets are often disconnected. A single piece of equipment–such as a circuit breaker or distribution transformer–may appear under different identifiers in different systems. Some platforms describe equipment logically, others physically; some track operational status, while others focus on installation metadata. Over time, due to maintenance, upgrades, or undocumented field interventions, these systems begin to diverge. Manual reconciliation is labor-intensive, error-prone, and non-scalable10,11. The result is that critical decisions–such as how to reconfigure the network after a fault, or how to prioritize restoration paths–are made based on inconsistent, incomplete, and sometimes obsolete network models.

This foundational challenge has catalyzed several new directions in the literature. One major area of work has focused on topology inference, that is, reconstructing the actual operational network structure from available data12,13. Early approaches relied on manual diagram tracing or rules applied to structured engineering files. As the scale and complexity of networks grew, statistical and machine learning techniques were introduced. For example, voltage time-series correlations, mutual information analysis, and residual-based hypothesis testing have all been explored to identify network connectivity. However, these techniques often assume dense measurement infrastructure, such as synchronized phasor data, which is rarely available in secondary distribution networks. To address such limitations, more recent studies have turned to graph-theoretic approaches, where known device attributes and spatial information are used to probabilistically reconstruct topology through node similarity, clustering, and path inference14–16. With the advent of graph neural networks (GNNs), these efforts have become more sophisticated, allowing systems to learn topology from relational structure and node-level features in tandem. While still relatively new in the power systems domain, GNN-based methods have shown promising results in asset classification, state estimation, and connectivity inference, particularly when traditional measurements are sparse or unreliable17,18.

Nevertheless, recovering topology is only the first step. Once the structure is known or estimated, operators must assess the operational risk of the network under evolving conditions. Classical approaches to risk analysis in power systems–such as N-1 contingency ranking, loss-of-load expectation, or reliability indices like SAIDI and SAIFI–have provided useful starting points19,20. However, these metrics often assume static topologies and fail to capture how local vulnerabilities propagate through the network. In real systems, the failure of a single asset can lead to cascading effects, especially in weakly meshed or radial configurations with limited redundancy21. To address this, researchers have developed a wide range of fragility metrics, vulnerability maps, and cascading failure models. Some works introduce entropy-based indices that quantify the robustness of a network to disruptions, while others model the probability and consequence of failures based on aging assets22,23, weather conditions, or historical failure data. A growing number of studies now adopt multi-dimensional risk frameworks, where risk is seen not merely as the product of probability and consequence, but as a systemic property shaped by network structure, operational loading, and temporal dynamics24,25.

In parallel, the topic of network reconfiguration has matured into a critical area of study for resilience enhancement. Traditionally, reconfiguration has been framed as a mixed-integer optimization problem: given a fault or overload, determine the minimal set of switching actions that restores the largest amount of load, respects operational constraints, and maintains radiality26,27. These problems were typically solved using deterministic models, focusing on steady-state criteria like power loss or voltage deviation28,29. However, the rise of renewable variability, real-time dispatch, and cyber-physical contingencies has prompted the community to explore more dynamic, scenario-based, and risk-aware reconfiguration strategies. Evolutionary algorithms, reinforcement learning, and hybrid heuristics have all been applied to address the combinatorial explosion of possible switching sequences in large-scale networks. In particular, multi-objective formulations have gained traction, enabling planners to trade off between load restoration, switching effort, risk reduction, and even economic costs30,31. Some researchers have further adopted bilevel models, where a strategic upper-level objective (e.g., minimize network-wide risk) interacts with an operational lower level (e.g., ensure power flow feasibility). These formulations offer a more realistic representation of utility decision-making hierarchies, though they also introduce significant computational complexity. More recently, attention has turned toward the coordination of transmission and distribution networks, especially in the context of high DER penetration and stressed grid conditions. While past models treated transmission-distribution interfaces as fixed boundary conditions, newer frameworks attempt to capture inter-layer interactions, such as voltage dependency, fault propagation, and mutual constraint satisfaction. Some studies have introduced co-simulation platforms and interface-aware optimization methods to reflect this coupling. As the boundary between transmission and distribution blurs, the need for integrated data models, shared contingency planning, and joint reconfiguration logic becomes increasingly clear. However, despite this recognition, most practical tools remain siloed, lacking the integration necessary to move from data modeling to actionable dispatch guidance.

Motivated by these converging challenges, this paper proposes an end-to-end framework that integrates data-driven topology identification, probabilistic risk evaluation, and bilevel reconfiguration optimization for real-world transmission-distribution systems. The approach recognizes that in modern grid operation, data, models, and decisions cannot be separated into sequential stages; they must operate as an integrated feedback system. Our methodology begins with an automatic topology recovery module that uses GCN-based learning on heterogeneous attribute datasets to infer operational connectivity. Unlike approaches that rely on voltage measurements or complete SCADA coverage, our model works with fragmented asset records, device metadata, and logical identifiers–exactly the types of data utilities actually possess. The output is a unified electrical model that reflects both physical topology and data-driven inference.

On this inferred network, we then construct a multi-layer risk model that evaluates nodal and path-level vulnerabilities using a combination of load stress, structural fragility, asset aging, and risk propagation parameters. The result is a high-resolution risk map that evolves with time and scenario assumptions, enabling utilities to anticipate not only where failures may occur, but how they may propagate and interact. Finally, we formulate a bilevel optimization problem for reconfiguration planning. The upper level minimizes system-wide operational risk and switching cost while maximizing restoration, and the lower level enforces electrical feasibility using a tractable approximation of power flows. A multi-objective algorithm is used to navigate the Pareto front of competing objectives, providing operators with interpretable and actionable plans under diverse contingencies.

This work thus contributes to the power systems literature across several dimensions. It introduces a novel attribute-aware graph learning mechanism for recovering electrical topology from fragmented utility records. It develops a scenario-based risk model that integrates fragility, redundancy, and cascading potential into a probabilistic evaluation framework. It formulates a bilevel optimization structure that bridges strategic restoration goals with operational feasibility in a computationally efficient manner. And most importantly, it offers an integrated, deployable architecture that transforms fragmented, uncertain data into clear, risk-informed operational decisions. By addressing topology, risk, and decision-making as mutually dependent and operationally unified, this paper reflects a realistic and forward-looking vision of how modern power systems must be managed. The challenge of fragmented data is reframed as an opportunity for intelligent reconstruction. The complexity of risk is embraced as a feature to be modeled, not ignored. And the ambiguity of reconfiguration decisions is reduced through optimization frameworks that reflect how operators actually think. In an era where grid reliability is both more important and more fragile than ever, such unified approaches offer a path forward that is not only smart, but resilient, adaptive, and grounded in the operational realities of utility practice.

Figure1presents a structured schematic of the proposed data-driven framework for resilient distribution system management, explicitly annotating the roles and data flows across the three principal modules: topology identification, risk evaluation, and reconfiguration. The process begins with the topology identification module, which integrates fragmented utility records including SCADA snapshots, GIS layouts, AMS metadata, and dispatcher logs. These heterogeneous data sources are collectively processed to infer a unified network topology that reflects both structural and attribute consistency. The inferred topology is subsequently passed to the risk evaluation module. This stage quantifies system vulnerabilities using load stress metrics and fragility parameters, yielding time-varying risk scores for each node and path. These scores incorporate both local asset conditions and spatially coupled propagation effects, forming a basis for scenario-aware resilience assessment. Finally, the reconfiguration module takes the risk scores as input and solves a constrained optimization problem to generate restoration plans. This stage enforces power flow limits and switching feasibility, ensuring electrical and operational constraints are satisfied. The resulting output supports informed decision-making for adaptive and risk-aware grid reconfiguration. The arrows in the figure denote directional information flow between modules, while intermediate variables such as inferred topology and risk scores are explicitly annotated. This enhanced visual structure ensures interpretability and reveals how each module contributes to the end-to-end process of resilient distribution network operation.

This objective formulates a comprehensive trade-off between system-wide operational risk and continuity of service, with the first summation capturing node-level failure consequences, considering partial load restoration rates, topological fragility, and asset aging; while the second term quantifies the path-based restoration effectiveness and switching action cost for each line, weighted by path redundancyand criticality.

This risk cost formulation incorporates a layered system vulnerability representation: structural fragility, demand satisfaction penalty via, asset deterioration score, and risk diffusion between nodes encoded through. The coefficients, andreflect structural, operational, degradation, and dependency sensitivity respectively.

This nonlinear supply continuity metric integrates restoration effectiveness and path redundancy. The ratiorepresents the load restoration rate, and is scaled by a logarithmic path-weighted redundancy term involvingand, where the latter reflects real-time congestion risk along recovery paths.

This compact scalarized form defines the convex combination of the two previously formulated objectives: the total system riskand the negative of the supply continuity index, regulated by the adaptive trade-off weight, enabling dynamic emphasis adjustment during multi-objective optimization procedures.

This nodal power balance constraint ensures Kirchhoff’s law at every node, in each time period, under scenario. It uses admittance values, voltage magnitude, and angle, balancing generation, demand, and shedding.

This constraint restricts the total number of switching actions per dispatch interval to a predefined upper bound. Here,is a binary activation indicator andis the switching state of branch.

This constraint ensures that the power flow on each linedoes not exceed its thermal rating. The complex apparent power flowmust remain within permissible limits to avoid overheating and failure.

To maintain operational voltage stability, each node voltagemust lie within its allowable range, bounded byand, accommodating variations due to loading and reconfiguration.

This topological validation constraint uses binary inclusion flagsand structural reliability weightsto ensure the identified model agrees with known GIS structures and maintains physical plausibility.

This energy balance equation models that the total incoming supply at node(generation and import) must equal the sum of served demand, curtailed load, and system losses, preserving power consistency.

This backup transfer feasibility constraint limits load transfer to what is realistically possible via nearby feeders, considering availabilityand their capacity.

To ensure coupling consistency at transmission-distribution interfaces, this constraint bounds the voltage deviation between transmission busand connected distribution nodeby a small limit.

This probabilistic degradation constraint models failure likelihoodas a functionof condition, load, and fragility, ensuring that failure probability remains below a tolerance level.

This constraint places a hard capon the number of allowed reconfiguration actions (activated lines) at any time, preventing excessive switching and operational disruption.

This binary-compatibility constraint links the decision variablewith the line flow. If a line is inactive, the flow is forced to zero, enforcing topological consistency in reconfiguration modeling.

Integrated Framework for Data-Driven Topology Identification, Risk Evaluation, and Reconfiguration in Distribution Networks.

We now formalize the modeling components of the framework, including topology reconstruction via graph learning, scenario-based risk quantification, and the multi-objective reconfiguration strategy.

This cosine-based similarity functionquantifies the degree of matching between nodeandfrom heterogeneous data sources, based on shared attributesand their respective feature weights. It serves as the fundamental scoring rule for cross-system device matching.

This normalized graph Laplacian matrixis derived from the degree matrixand adjacency matrix, capturing both the structural connectivity and attribute-consistent relationships of the reconstructed network graph. It is used to propagate learning signals over the graph in the GCN framework.

This equation defines a single forward propagation step in a Graph Convolutional Network (GCN), whereis the feature matrix at layer,is the trainable weight matrix, andis an activation function such as ReLU or ELU. It integrates topological and attribute data for topology-aware device embedding.

This cross-entropy loss functionis used to train the matching classifier in a supervised learning setting. The true label indicatorand predicted class probabilityguide the GCN to correctly align devices across systems.

This ensemble voting mechanism linearly combines multiple similarity metricsusing learned or expert-defined weights, generating a robust unified similarity scorefor decision-level fusion in node alignment.

This risk zoning metricclassifies each node into low, medium, or high risk. It integrates load stress ratio, topological fragility, and feeder aging via a weighted aggregation, thus guiding priority in reconfiguration and maintenance.

This fuzzy membership functionmaps the continuous risk scoreinto categorical classes using logistic functions parameterized by(slope) and(threshold), offering interpretability for dispatchers.

This probabilistic scenario generation model computes the conditional probability of failure scenariogiven contextual state, assuming independent Bernoulli failures based on failure probabilities.

This bilevel reconfiguration decision structure maximizes restored supplyby choosing reconfiguration strategies, while satisfying all feasibility constraintsin the lower level. It reflects the hierarchical nature of strategic and operational control.

This linearized power flow approximation is used for fast scenario filtering. It simplifies nonlinear AC models using susceptanceand conductanceparameters, estimating power exchanges between neighboring nodes efficiently.

This composite priority indexevaluates reconfiguration candidates based on marginal load restoration and risk reduction, modulated by coefficientsand. It supports heuristic-guided pruning.

Each reconfiguration planis scored by its frequency of constraint satisfaction across all sampled failure cases, forming a robust performance indexthat drives solution selection.

This Pareto front convergence criterionmeasures the maximum distance between the current Pareto setand the previous one, based on objective values. It terminates the multi-objective optimization when progress saturates.

The training procedure of the GCN-based topology inference module follows a supervised classification approach under data fragmentation. Node-pair features are derived from multi-source utility records–including SCADA, GIS, and AMS–and embedded into a unified attribute graph. A cross-entropy loss function is applied to penalize adjacency prediction errors, with balanced edge sampling to avoid class skew. Dropout regularization is used between graph convolution layers to enhance generalization. Early stopping is triggered based on the convergence of validation loss. To emulate real-world incompleteness, a portion of ground-truth connectivity is randomly masked during training. Model evaluation is performed using reconstruction accuracy and edge-level precision-recall metrics. The full training configuration is summarized in Table1.

GCN Training Configuration Under Incomplete Observability.

In practice, the proposed bilevel reconfiguration optimization model is solved via single-level reformulation. The lower-level constraints–including nodal power balance, voltage limits, and switching feasibility–are embedded into the upper-level objective through indicator-based transformation. The resulting mixed-integer linear program (MILP) is implemented using Python and solved with Gurobi 10.0.1. To ensure tractability under scenario uncertainty, a convexified branch flow model is adopted for power equations, and infeasible configurations are pre-screened using logical pruning. Each scenario is solved independently in parallel across multiple CPU threads, with an average runtime of under 9 mins per case. This solution pipeline supports near real-time reconfiguration decisions and is compatible with operational deployment in utility control environments.

To evaluate the practical performance and deployability of the proposed framework, a case study was conducted on a synthetic yet structurally realistic 10 kV distribution network modeled after the typical feeder architecture in Eastern China. The system consists of 58 buses, 61 distribution lines, 4 sectionalizing switches, 2 main transformers, and 3 tie-line candidates. Among the 58 nodes, 42 represent residential or commercial load centers, with peak demands ranging from 120 kW to 520 kW, following a non-uniform spatial load distribution based on urban density zones. A total of 12 distributed energy resources (DERs) are integrated, including 8 rooftop PV systems (with capacities from 30–150 kW) and 4 small-scale battery storage units (with discharge capacities of 50–200 kWh). The DERs are distributed asymmetrically across the feeders to create representative voltage fluctuations and load imbalances. Asset metadata for each node–including installation year, manufacturer type, failure history tags, and inspection records–are synthetically generated from a real utility’s anonymized asset catalog and used to simulate attribute heterogeneity across GIS and AMS platforms.

To simulate the data fragmentation scenario that motivates this research, four heterogeneous data layers were constructed: (1) a GIS database with complete spatial layout but 11% of lines missing connectivity tags; (2) a SCADA snapshot containing time-stamped voltage and current readings for 29 nodes, sampled every 15 mins; (3) an AMS record where 22% of device entries use outdated or non-standard naming conventions; and (4) a tabular dispatcher log, where partial manual entries and operator notes are mapped to device aliases. Each dataset is preprocessed to reflect real-world inconsistencies in naming, missing values, time desynchronization, and attribute incompleteness. The graph construction step fuses these sources using the proposed similarity-based and GCN-embedded topology inference module. The resulting unified graph contains 67 nodes (including auxiliary and inferred components) and 72 edges. This serves as the input for the subsequent risk evaluation model.

All simulations were conducted in Python 3.10, using PyTorch Geometric for GCN modeling, Gurobi 10.0.1 for solving the bilevel reconfiguration optimization problem, and NetworkX 2.8 for topological processing. Scenario generation and probabilistic risk propagation were implemented using NumPy, and the convexified power flow model employed linear approximations consistent with recent literature on fast restoration planning. The full simulation was executed on a Dell Precision workstation equipped with an Intel Xeon Gold 6338 CPU @ 2.0 GHz, 128 GB RAM, and a NVIDIA RTX A5000 GPU, ensuring both reproducibility and scalability. Across 100 contingency scenarios (including N-1 line outages, simultaneous DER fault clusters, and substation-level disruptions), the framework evaluated over 3,500 feasible reconfiguration plans in under 9 mins per scenario, demonstrating computational tractability for near-real-time applications.

Figure2illustrates a two-layered EMS tailored for coordinated control of photovoltaic generation, battery storage, and dynamic load interaction. The structure is organized into four logical zones: input acquisition, control core, system response, and performance evaluation. On the left, the system receives multiple real-time input signals, including solar irradiance profiles, anticipated load demand, SoC of the battery, and contextual temporal information such as time-of-day or seasonal variation. These heterogeneous inputs are transmitted to the central controller. The core EMS controller comprises four functional modules: a forecasting unit responsible for predicting short-term solar and demand trajectories; an optimization engine that integrates a DRO algorithm for resilient decision-making under uncertainty; a regulation unit that governs PV MPPT and battery SoC compliance; and a prioritization logic that dynamically allocates power to critical and non-critical loads based on system constraints and policy objectives. The outputs of the EMS controller directly influence system behavior, reflected on the right side of the diagram through updated PV generation patterns, battery dispatch levels, grid power exchange quantities, and the degree of load service achieved. These physical responses are continuously monitored and relayed into a performance evaluation layer. At the bottom of the framework, a set of KPIs are computed. These include the resilience index, overall energy utilization efficiency, power loss magnitude, and SoC constraint satisfaction. This closed-loop configuration ensures feedback-aware, context-sensitive, and optimization-driven energy management.

Structure of the EMS controller integrating forecasting, optimization, control, and prioritization strategies.

Figure3presents a detailed histogram of asset fragility scores across all 58 nodes in the test system, structured into 10 evenly spaced bins from 0.0 to 1.0. The x-axis represents fragility scores divided into decile intervals, each bin spanning a width of 0.1. The y-axis indicates the number of assets falling into each fragility range. Within each bin, assets are categorized into three condition statuses–Good, Fair, and Poor–using a stacked color format: light blue for Good, medium grey for Fair, and dark grey for Poor. The stacked arrangement makes it visually clear how physical condition correlates with fragility intensity. For example, in the lowest fragility bin (0.0–0.1), there are 9 assets, all of which are classified as Good. In contrast, in the 0.6–0.7 bin, there are 7 assets, of which 4 are labeled Poor and 3 Fair, indicating that high fragility scores coincide strongly with degraded physical conditions. Quantitatively, the distribution is skewed left, meaning a majority of assets fall into lower fragility intervals, which is consistent with a reasonably healthy network. Specifically, 34 out of 58 assets (58.6 percent) fall within the [0.0–0.4] fragility score range. Among these, 27 are in Good condition, 6 in Fair, and only 1 in Poor. However, the distribution shifts significantly in the upper fragility bins: in the [0.5–0.8] interval, there are 16 assets, and over 75 percent of them are in Fair or Poor condition. The highest bin (0.9–1.0) contains just 2 assets, both marked as Poor, clearly signaling critical attention points. This statistical breakdown underscores that fragility scores, although continuous and derived from quantitative asset metadata, are strongly predictive of binary or categorical degradation outcomes.

Distribution of Asset Fragility Scores with Stacked Condition Status Across System Nodes.

Figure4displays a dumbbell chart that visually compares the data completeness of two core infrastructure layers–SCADA and GIS–across ten representative feeders within the modeled distribution network. Each feeder is represented as a horizontal line segment with two endpoints: the left endpoint corresponds to SCADA coverage (in percentage), and the right endpoint represents GIS completeness (also in percentage). All feeder labels are placed on the vertical axis for clear identification, ranging from Feeder_1 to Feeder_10. The horizontal axis spans from 40% to 100%, with tick marks every 10%. SCADA coverage is marked by light blue dots, and GIS completeness by deeper sky blue dots. Grey horizontal lines between the dots highlight the magnitude of discrepancy between the two data sources per feeder. The visual arrangement emphasizes feeder-level data gaps and makes it easy to identify where the two systems are misaligned.

Comparative Dumbbell Chart of SCADA Coverage and GIS Completeness Across Ten Representative Feeders.

Figure5presents a kernel density estimation (KDE) visualization over a synthetic 2D coordinate grid, designed to capture the spatial clustering of distributed energy resources (DERs) within the test distribution network. The background shading, rendered in smooth gradients of light to medium blue, represents estimated DER density using a Gaussian kernel. Areas of deeper shading indicate regions with higher concentrations of DER installations. DER nodes are plotted as individual blue dots, while all system nodes–whether DER or not–are shown in light grey for spatial reference. The use of isodensity contours enhances interpretability, providing a quantitative sense of spatial compactness without relying on discrete binning. In total, 12 DER nodes are distributed among 58 total assets, accounting for 20.7 percent of all nodes. Despite this relatively modest DER penetration, the KDE reveals a strong spatial skew: nearly half of the DER nodes are concentrated in the upper-right quadrant of the map, with local density peaking near the (x = 70, y = 80) coordinate region. This region’s DER density is approximately 3.1 times higher than the average DER node density across the network. Conversely, the lower-left quadrant contains no DERs at all, creating a clear visual asymmetry in deployment. Such imbalances may lead to localized voltage rise, reverse power flows, or protection miscoordination–all of which must be addressed in your reconfiguration and risk mitigation strategies.

Figure6presents a kernel density estimation (KDE) plot illustrating the spatial clustering of distributed energy resources (DERs) across a synthetic 2D coordinate plane, using a yellow-based color palette to encode relative density. The background field is shaded in a smooth gradient ranging from pale yellow to deep amber, with intensity corresponding to DER concentration. Overlaid on this field are 58 total node locations, rendered as light grey dots, with the 14 DER nodes distinctly marked in gold. The KDE contours are drawn using 10 threshold levels, giving the figure both aesthetic clarity and analytical resolution. No discrete binning is applied; the spatial density field is fully continuous, emphasizing gradients over boundaries. Quantitatively, the DERs in this setup account for 24.1 percent of all nodes. However, the spatial distribution is far from uniform. The highest density cluster is centered around the coordinates (,), where five DERs are tightly located within a 2020 unit square – a density more than four times the system-wide average. The bottom-left quadrant, by contrast, contains no DERs whatsoever. This severe spatial asymmetry has meaningful operational implications: it suggests a high likelihood of over-voltage, line congestion, or reverse power flow near the concentration zones, while the DER-absent regions may be under-supported in resilience or flexibility. This type of spatial imbalance must be addressed through reconfiguration optimization or localized reinforcement strategies.

Figure7presents a 20-by-20 confidence matrix comparing true and predicted adjacency values for the reconstructed distribution network topology. Each cell represents the model’s confidence level for a predicted connection between two nodes. The confidence scores are generated by the final softmax layer of the graph convolutional network (GCN), ranging from 0 (no confidence) to 1 (full confidence), and visually encoded using a continuous blue gradient colormap.Diagonal elements are fixed at 1.0, representing self-connections with perfect certainty. The figure shows that roughly 85% of the diagonal-adjacent and near-diagonal cells exhibit confidence values above 0.8, while off-diagonal mismatches tend to fall below 0.3. These low-confidence cells, mostly found in the lower-left and upper-right quadrants, reflect prediction noise or edge ambiguity due to attribute overlap or missing data. From a numerical perspective, the matrix includes approximately 400 cells (2020), with an estimated 320 of them correctly predicted (true adjacency = predicted adjacency). Among these, the average confidence is 0.88. The remaining 80 mismatched cells have a mean confidence of 0.19, which is appropriately low and illustrates the model’s ability to express epistemic uncertainty. This separation in the confidence spectrum enhances interpretability: it allows operators and researchers to not only identify errors but to prioritize further validation or manual inspection for uncertain predictions. High-confidence false positives are nearly absent, indicating that the GCN is conservative in its inference – a desirable property for safety-critical infrastructure.

Figure8shows the spatial distribution of node-level risk scores before and after optimization, mapped onto a 55 grid representing the distribution network. In the left panel, nodes are colored based on their initial risk index with darker orange-red tones indicating higher vulnerability. The pre-optimization map clearly reveals clusters of high-risk nodes near the bottom and center of the network. Several nodes exceed a risk score of 0.85, driven by a combination of structural fragility, load stress, and limited redundancy. All connections are rendered in light grey to reflect the baseline topology without any switching decisions yet applied. In contrast, the right panel depicts the risk distribution after applying the bilevel optimization-based reconfiguration strategy. Here, the majority of nodes exhibit lower risk scores – with the highest values reduced below 0.6 and several nodes reaching values under 0.3. This spatial “flattening” of risk highlights the algorithm’s ability to redistribute vulnerability across the network. Edge transparency in this panel reflects switching activity: deactivated edges are rendered in white, allowing visual tracing of where reconfiguration occurred. For instance, at least three edge cuts are clearly visible in the upper-left region, demonstrating strategic isolation or rerouting to reduce overloads or fragility propagation.

Node-Level Risk Map Before and After Optimization.

Figure9presents a stacked histogram showing the distribution of system restoration times across four representative contingency scenarios: N-1 line faults, DER cluster losses, transformer failures, and compound (combined) events. The x-axis spans restoration time in minutes, while the y-axis reflects the number of scenario instances falling within each time bracket. Each color band corresponds to a specific fault type, following a soft blue palette to maintain aesthetic clarity. A vertical red dashed line denotes the 90th percentile restoration time, serving as a visual benchmark for worst-case operational recovery. Quantitatively, the N-1 line fault category dominates the distribution, contributing 60 instances with a peak around 15 mins, reflecting the system’s rapid reconfiguration capability for localized failures. DER cluster loss scenarios, with 40 instances, show a broader distribution, typically between 15 and 30 mins, as DER-associated volatility requires more involved risk containment. Transformer faults, though less frequent (25 instances), exhibit a noticeable rightward skew, with several cases extending beyond 35 mins due to their topological centrality and single-point sensitivity. The most severe delays appear in the “Combined Event” category (15 cases), where restoration times range from 30 to 55 mins , forming a distinct long-tail effect in the histogram. The overall 90th percentile threshold lies at approximately 38 mins, marking the edge of routine control and the entry into resilience-critical timeframes.

Figure10visualizes the post-optimization spatial distribution of power flows across a 25-node distribution network. The network is rendered as a spring-layout graph where edge thickness represents flow magnitude (in kW), and edge color encodes the congestion index on a continuous grey scale. Thicker lines correspond to heavier usage, while darker shades indicate higher relative congestion – i.e., closer proximity to line thermal or operational limits. Node sizes are standardized, and node colors (light blue) are kept neutral to foreground the importance of edge-level flow redistribution. From visual inspection, multiple central edges exhibit both high width and dark grey shading, suggesting these lines are acting as critical corridors in the optimized reconfiguration. At least three feeder routes – one on the upper right and two spanning the middle – carry flows in excess of 80 kW with congestion indices above 0.8, indicating near-saturation. Meanwhile, several peripheral edges remain thin and light-colored, signifying minimal or zero use under the optimal configuration. This distribution pattern reflects how the optimization model exploits high-redundancy paths and deactivates low-efficiency connections to reduce system-wide risk, albeit at the cost of localized congestion hotspots.

Figure11visualizes the spatial propagation of risk in a 25-node grid network before and after the execution of an optimized reconfiguration strategy. The initiating fault is placed at node 12, which is intentionally selected to be centrally located in the network, mimicking a high-centrality asset like a switching station or DER hub. The left panel shows node risk scores immediately after the initiating fault in an uncontrolled environment. Risk is seen to radiate outward, with several adjacent nodes showing scores in the range of 0.7–1.0, indicating high fragility and load coupling effects. The propagation pattern reflects both the physical topology and the interdependence modeled by your spatial risk propagation constraints. In the right panel, after reconfiguration, the same network topology is visualized, but node risk scores are significantly reduced in both magnitude and spatial extent. The node at the origin of the fault remains a high-risk node (retaining a value of 1.0), but neighboring nodes drop into the 0.3–0.6 range, and peripheral nodes show scores near zero. This demonstrates a successful dampening of cascading risk, achieved via the intelligent re-routing of flows, strategic switching, and risk-weighted topological control. This side-by-side layout reinforces the model’s contribution: it doesn’t merely suppress aggregate risk, but does so in a way that breaks the spatial transmission pathways of stress and fragility.

Spatial Map of Risk Propagation Paths from an Initiating Fault.

To evaluate the effectiveness of the proposed GCN-based topology reconstruction, we benchmarked it against three commonly used inference strategies: (i) mutual information analysis on voltage time-series, (ii) Pearson correlation-based structure identification, and (iii) rule-based heuristic matching using asset metadata. These methods were applied to the same fragmented data conditions, including missing GIS links, inconsistent device labels, and partial SCADA coverage. The comparison is summarized in Table2, which reports reconstruction accuracy, sensitivity to data sparsity (measured by performance drop at 30% data loss), and the average false positive (FP) and false negative (FN) rates. The results show that the GCN approach consistently outperforms the baselines, particularly in sparse and noisy environments, owing to its ability to jointly embed relational structure and multi-source attributes. Notably, the FP rate under the heuristic rule-based method was higher due to over-linking devices with similar tags, while correlation-based approaches suffered from missed edges when data were temporally misaligned or incomplete.

Benchmarking Topology Reconstruction Methods Under Incomplete Observability.

This study proposed a unified, data-driven framework that integrates topology inference, risk evaluation, and reconfiguration optimization to enhance the resilience of distribution networks operating under incomplete observability. By fusing heterogeneous data sources–including SCADA logs, GIS layouts, asset metadata, and dispatcher records–into a graph-based representation, the framework leverages a graph convolutional network (GCN) to reconstruct operational topology with high accuracy even under fragmented data conditions. On the inferred topology, a scenario-based risk evaluation module quantifies nodal and path-level vulnerabilities by incorporating operational stress, asset fragility, and redundancy constraints. These risk assessments then inform a bilevel reconfiguration optimization model, which jointly minimizes systemic risk and switching cost while maximizing load restoration, subject to dispatch and electrical feasibility constraints.

Simulation results on a structurally realistic 58-node test system with embedded DERs validate the effectiveness of the proposed framework. The method achieves over 52% reduction in high-risk node exposure, restores more than 94% of aggregate demand across 90% of simulated contingency scenarios, and maintains average solve times under 9 mins per case–demonstrating scalability and operational tractability. A suite of visual analytics, including topology heatmaps, risk propagation surfaces, congestion-weighted flow maps, and Pareto front tradeoff plots, further enhances interpretability and supports transparency in operational decision-making. Beyond its technical contributions, the framework carries broader implications for utility modernization and planning. Its ability to operate on incomplete and inconsistent datasets aligns with ongoing digitalization efforts in legacy infrastructure environments. The scenario-sensitive risk mapping offers a structured basis for prioritizing DER deployment, evaluating contingency plans, and identifying critical upgrade targets under limited investment budgets. Moreover, the integrated reconfiguration strategy supports faster post-fault recovery and enhances system flexibility, making it directly applicable to emergency response protocols under extreme weather or cyber-physical disruptions. As distribution systems become increasingly decentralized and data-driven, this approach offers a scalable and adaptive solution for navigating uncertainty, optimizing resilience, and supporting evidence-based utility planning.