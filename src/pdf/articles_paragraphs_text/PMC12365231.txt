Coverage optimization stands as a foundational challenge in Wireless Sensor Networks (WSNs), exerting a critical influence on monitoring fidelity and holistic network efficacy. Constrained by the limited energy budgets of sensor nodes, the imperative to maximize network longevity while sustaining sufficient coverage has ascended to the forefront of research priorities. Traditional deployment methodologies frequently falter in complex topographies and dynamic operational environments, encountering difficulties in striking an optimal equilibrium between coverage quality and energy efficiency. To mitigate these inherent limitations, this paper introduces ACDRL (Adaptive Coverage-Aware Deployment based on Deep Reinforcement Learning)—a novel strategy that enables intelligent, self-optimizing node placement in WSNs through deep reinforcement learning paradigms. Our proposed framework establishes a sophisticated deep reinforcement learning architecture integrating a multi-objective reward mechanism and hierarchical state representation, which innovatively resolves the dual predicaments of coverage optimization and energy balancing in intricate scenarios. Extensive simulation results validate that ACDRL consistently outperforms state-of-the-art approaches by maintaining superior coverage ratios, significantly extending network operational lifespan, and demonstrating enhanced adaptability in high-density deployment scenarios.

Wireless Sensor Networks (WSNs), as the core component of intelligent sensing systems, have become a critical link connecting the physical and virtual worlds amid the digital transformation wave. With their distributed sensing, autonomous networking, and low-power characteristics, WSNs are reshaping the paradigm of data collection and environmental monitoring1–4. Composed of numerous miniature intelligent nodes, WSNs capture environmental data through built-in sensors and work collaboratively via wireless communication to form an integrated system. From precision irrigation in agriculture to predictive maintenance of industrial equipment, from wildlife habitat monitoring to smart building energy consumption management, the applications of WSNs have permeated all aspects of the socio-economic landscape5–7. However, beneath this extensive applicability lies a persistent challenge: the coverage issue in practical deployments remains a fundamental bottleneck constraining WSN technology’s full potential. Traditional static deployment methods often fail to balance the trade-off between coverage quality and system resource consumption, especially in scenarios with complex terrain or dynamically changing demands8,9. For example, in earthquake early warning systems, improper sensor placement may result in a lack of monitoring in key areas, while in border surveillance applications, overly dense node distribution could lead to rapid energy depletion10–12.

Against this backdrop, traditional wireless sensor network deployment paradigms primarily include deterministic layout schemes and random dispersion strategies13,14. These methods have achieved certain successes in controlled environments and large-scale rapid deployment scenarios, respectively. Geometric programming constructs the theoretically optimal coverage using triangular or hexagonal grids, while probabilistic distribution models predict the coverage effects of a large number of nodes based on statistical principles15,16. However, these classical methods face numerous challenges in practical applications. The geometric layout assumes flat terrain with no obstacles; once the environment becomes complex or contains barriers, its theoretical optimality is difficult to maintain17. While random distribution is easy to implement, it cannot ensure the sensing quality of critical areas and often leads to uneven node distribution and energy waste18–20. More critically, as the demands for sensing tasks dynamically change and the spatial-temporal heterogeneity of environmental conditions increases, static deployment schemes fail to adapt to such variations, causing coverage efficiency to degrade rapidly over time21. Furthermore, existing deployment strategies generally lack a comprehensive consideration of real-world factors, such as node energy heterogeneity, communication interference, and sensing capability degradation, resulting in a significant gap between theoretical models and practical outcomes22. In multi-objective optimization, traditional methods typically employ simplified treatments, such as weighted summation, which makes it difficult to effectively balance multi-dimensional objectives, such as coverage, energy consumption, and cost.

To address these limitations, artificial intelligence technologies23,24have sparked a wave of transformation in the wireless network field in recent years, injecting unprecedented intelligent power into traditional communication systems. Deep learning, with its outstanding feature extraction capabilities, has shown significant advantages in applications such as IoT traffic prediction and intelligent traffic control25. Meanwhile, reinforcement learning (RL), as a learning paradigm based on the “interaction-reward” mechanism, is particularly suitable for sequential decision-making problems in wireless networks. Traditional Q-learning has been successfully applied in channel allocation and heterogeneous network interference management in mobile communication systems. Notably, reinforcement learning demonstrates unique potential in the optimization of WSN deployment26. Unlike static deployment methods, RL-based WSN deployment strategies continuously optimize node positions through environmental interactions, achieving a dynamic balance between coverage and energy efficiency27,28. Especially in scenarios with dynamically changing environmental conditions, RL agents can adaptively adjust sensor density distribution according to spatiotemporal variations in coverage demands, effectively avoiding monitoring blind spots29. The multi-agent reinforcement learning (MARL) framework further provides a theoretical foundation for distributed collaborative deployment in WSNs, enabling the network to achieve global coverage optimization while minimizing communication overhead30. Deep reinforcement learning (DRL), by combining the representational power of deep neural networks with the decision-making mechanism of reinforcement learning, further enhances the strategy learning efficiency in complex WSN deployment scenarios, offering an effective approach to solving coverage optimization problems in high-dimensional state spaces31.

Building on these advancements, this study tackles the coverage sensing deployment problem in WSNs using a Reinforcement Learning (RL) framework. Traditional WSN deployments often face complex terrains and dynamic environmental changes, which pose significant challenges to conventional heuristic algorithms. These traditional methods are particularly ill-equipped to handle heterogeneous sensing requirements and energy constraints, making it difficult to achieve multi-objective optimization. In response to these challenges, we propose an ACDRL deployment strategy framework. By incorporating deep neural networks, ACDRL effectively approximates high-dimensional state-action value functions, thereby addressing the complex environmental state representations inherent in WSN deployments. The key innovation of ACDRL lies in its dual reward mechanism and adaptive learning rate adjustment strategy, which dynamically balance the maximization of coverage with the efficient management of energy consumption.

A deep reinforcement learning framework specifically designed for WSN deployment problems is proposed. This framework effectively handles high-dimensional state spaces using deep neural networks, enabling adaptive modeling for complex environments and heterogeneous sensing requirements, overcoming the limitations of traditional deployment methods in dynamic environments.

An innovative reward function system is designed, considering both coverage maximization and energy consumption balancing as two objectives. Through an adaptive learning rate adjustment strategy, the algorithm convergence is accelerated, achieving an approximately optimal deployment strategy under multi-objective optimization.

A hierarchical state representation method is developed to reduce learning complexity, effectively addressing the dimensionality disaster problem in large-scale sensor network deployments. This improves the scalability and practicality of the algorithm, allowing ACDRL to be applied to larger-scale and more complex real-world WSN deployment scenarios while maintaining performance advantages.

The remainder of the paper is organized as follows: Section2briefly reviews related work in the field of target coverage and the fundamentals of reinforcement learning; Section3introduces the basic concepts related to WSN coverage; Section4outlines the basic principles of ACDRL; Section5presents the experimental results; and finally, Section6summarizes the research and discusses future research directions.

Coverage optimization, as one of the core issues in WSNs, has a decisive impact on network quality of service and system performance32. Essentially, the coverage problem involves how to effectively deploy and schedule limited sensor resources to meet specific sensing requirements. Depending on the monitoring targets, researchers classify the coverage problem into three typical paradigms: point coverage, barrier coverage, and area coverage. Point coverage focuses on continuous monitoring of discrete target points; barrier coverage aims to minimize the probability of unmonitored penetrations; while area coverage emphasizes comprehensive sensing within a specific spatial range33. To address these problems, various algorithmic strategies have been proposed in the literature, such as location-based coverage methods, techniques that estimate distances using signal strength, and dynamic coverage control mechanisms that combine mobile nodes. In recent years, some innovative studies have explored coverage algorithms that do not require location, distance, or angle information, providing feasible solutions for resource-constrained environments34.

From an architectural perspective, coverage control algorithms can be divided into centralized and distributed categories. In large-scale sensor networks, distributed scheduling protocols have gained widespread attention due to their good scalability. Typical distributed coverage control usually adopts a time-slot division mechanism, where sensor nodes wake up at the start of each round and make activity decisions through local communication35. However, this message-exchange-based decision-making mode may lead to significant energy consumption as the network scale increases, which contradicts the energy-saving goals of WSNs. To overcome this challenge, various optimization strategies have been proposed, including adaptive wake-up mechanisms, learning-based activity prediction, and hierarchical coverage scheduling frameworks36. Particularly, recent studies show that reinforcement learning techniques can reduce redundant message exchanges and optimize node activity decisions, significantly reducing protocol overhead while ensuring coverage quality. This provides a new approach for next-generation intelligent WSN coverage control37.

Reinforcement learning (RL), as a key technology in the field of artificial intelligence, establishes a real-time interaction mechanism between the agent and the environment, thereby enabling a new paradigm for sequential decision optimization45. Within the framework of Markov decision processes (MDP), RL demonstrates significant potential in solving complex decision-making problems.46Traditional Q-learning, due to its simplicity and efficiency, has been widely applied in the field of network optimization. La et al.38optimized the balance between the number of monitoring targets and charging time using Q-learning, while Wei et al.39and Soni et al.40applied it to charging path planning, effectively improving system efficiency and network lifespan. As problem complexity increases, deep reinforcement learning (DRL) overcomes the limitations of traditional tabular Q-learning in handling high-dimensional continuous state spaces by integrating the representational power of deep neural networks with the decision-making mechanism of RL47. Deep Q-Networks (DQN), as a representative algorithm of DRL, approximates the state-action value function using neural networks, showing outstanding performance in fields such as video games, robotics, and WSNs48. The charging optimization algorithm based on DRL proposed by Cao et al.41successfully maximized system revenue under MC capacity constraints, while the actor-critic framework developed by Jiang et al.42and Yang et al.43effectively reduced the number of non-working nodes while extending the network lifetime. Mohajer et al.44propose a novel dynamic offloading framework that addresses the challenges of traffic fluctuations and resource diversity in mobile edge networks. Their approach integrates a sparse multi-head graph attention mechanism for accurate traffic prediction and an adaptive offloading strategy based on the twin delayed deep deterministic policy gradient algorithm, achieving enhanced performance and adaptability in various operational scenarios. Despite the clear advantages of DRL in handling complex environmental decisions, challenges such as data dependency, computational overhead, and algorithm instability still need to be addressed through innovative framework designs and optimization techniques49,50. The comparison and limitations of existing methods are detailed in Table1.

Comparative analysis of WSN coverage optimization methodologies.

The deployment of wireless sensor networks faces three fundamental challenges: (1) the dimensionality curse in large-scale networks where traditional methods fail to scale, (2) the dynamic adaptation gap where static strategies cannot respond to spatiotemporal variations, and (3) the multi-objective conflict between coverage quality, energy efficiency, and network longevity. While deep reinforcement learning (DRL) offers promising solutions, standard architectures lack hierarchical state representations for network structures, conventional reward designs oversimplify coverage-energy trade-offs, and fixed learning-rate mechanisms cannot adapt to dynamic WSN environments. Our work addresses these limitations through three key innovations: a hierarchical state abstraction capturing global-local patterns, a time-variant reward architecture dynamically reweighting objectives, and gradient-aware learning rate adaptation for non-stationary optimization landscapes.

This section first defines the research boundaries and core assumptions, laying the theoretical foundation for the subsequent discussion. It then delves into the coverage sensing model in WSNs and its mathematical representation. By systematically reviewing the evolutionary trajectory and applicable scenarios of existing coverage models in the WSN field, key parameters and technical indicators are extracted, and a formalized description framework for the coverage sensing deployment problem is established. This formalization not only facilitates the precise formulation of the problem but also provides the necessary theoretical support for the subsequent reinforcement learning-based solutions. Coverage models in wireless sensor network are shown in Fig.1.

Schematic diagram of wireless sensor network coverage sensing deployment.

The sensor node distribution function, whererepresents the target coverage area, and the node positions follow a uniform distribution over the area, reflecting the randomness of the deployment. However, in real-world scenarios, non-homogeneous terrains are common. To address this, we introduce a non-uniform distribution functionthat accounts for terrain variations and obstacles. This distribution can be adapted based on prior knowledge of the environment, allowing the model to better reflect real-world conditions.

The location sensing capabilityholds for each node, and the communication connectivity graph ( G(V, E) ) satisfies full connectivity, i.e.,. In practical deployments, maintaining full connectivity may not always be feasible, especially in dynamic environments. We relax this assumption by allowing partial connectivity and incorporating a connectivity maintenance mechanism in the deployment strategy to ensure robust communication.

The global node countis known to all nodes in the network, constituting the deployment density parameter, which influences the formulation of the coverage strategy. In dynamic environments, the node count may vary due to node failures or additions. We address this by incorporating a dynamic node count estimation mechanism, allowing nodes to adapt their strategies based on real-time network information.

In real-world deployment environments, the sensing capabilities of sensor nodes are influenced by various environmental factors, which make the traditional binary disk model inadequate for accurately describing coverage characteristics. Environmental noise, signal attenuation, terrain obstruction, and atmospheric conditions, among other factors, contribute to the probabilistic nature of the sensing capabilities. Therefore, this study introduces a probabilistic sensing model to more precisely characterize the coverage characteristics of nodes.

Here,represents the Euclidean distance between the nodeand the target point;denotes the reliable sensing radius, within which the probability of detecting the target is 1;represents the maximum sensing radius, beyond which the node cannot detect the target. The parametersandcontrol the decay rate and the shape of the sensing probability curve, respectively, and are closely related to the sensor type and environmental characteristics.

where,represents the operational state of sensor(1 for active, 0 for sleep).

where,represents the remaining energy of nodeat time, andis the energy threshold required for normal sensing.

This probabilistic sensing model not only more accurately reflects the sensing characteristics in real-world environments but also naturally integrates energy constraints into the coverage calculation process. This provides a more realistic environmental model for the adaptive deployment strategy based on reinforcement learning, helping the algorithm learn more robust deployment strategies during the training process.

where:represents the global network coverage state, with a dimension of,represents the local sensor state, including energy levels, current sensing radius,represents the aggregated features of neighboring node states.

whereis a dimensionality reduction function, which can be based on region partitioning feature aggregation or principal component analysis. This representation method enables efficient fusion of multi-scale state information, allowing the sensor network to achieve adaptive region priority adjustment and dynamic balance between energy consumption and coverage quality under limited computational resources. Additionally, it facilitates real-time optimization of the network topology.

To ensure robustness under partial observability and node failures, the hierarchical state representation incorporates mechanisms for synchronization and fault tolerance. Specifically, the local and neighbor states are periodically updated to reflect the current operational status of each node and its surroundings. In the event of node failures, the global state representation is designed to accommodate dynamic changes in network topology, allowing the system to adaptively adjust its strategy based on the remaining active nodes.

where:represents the active/sleep decision, enabling dynamic control of the network’s activity level.represents the adjustment of the sensing radius, which can be discretized intoKlevels, supporting fine-grained control of sensing capability.

The reward function is designed to balance the trade-offs between activating more nodes and expanding individual node coverage. Specifically, the energy efficiency reward componentpenalizes excessive use of sensing radius, encouraging nodes to optimize their energy consumption while maintaining adequate coverage. This ensures that the network can adaptively adjust its strategy based on the current energy levels and coverage requirements, achieving a dynamic balance between coverage quality and energy conservation.

whererepresents the standard deviation, which is used to measure the imbalance in energy consumption. By minimizing the standard deviation of energy consumption, the network load balancing is promoted, thereby extending the overall network lifetime.

It should be noted that while the standard deviation is a common metric for measuring energy consumption imbalance, it may not be robust under skewed distributions. Alternative metrics such as the Gini coefficient could also be considered for more accurately capturing the energy imbalance in certain scenarios. The Gini coefficient, which is often used in economics to measure inequality, can provide a different perspective on the distribution of energy consumption across nodes. However, in our current model, the standard deviation is chosen due to its simplicity and computational efficiency. Future work could explore the integration of other metrics to enhance the robustness of the load balancing reward.

The coefficients,, andcan be dynamically adjusted over time to balance the optimization objectives at different stages.

The reward function is designed with a time-varying weighting system, where energy efficiency is prioritized at the beginning of the network deployment (with a larger), and as time progresses, the focus gradually shifts towards coverage quality and load balancing (with increasingand), enabling dynamic strategy adjustment throughout the WSN lifecycle.

Actor Network:, outputs the probability distribution of taking actions in a given state. By introducing a hierarchical decision process, it first determines the node’s working state (), and then optimizes the sensing radius (r). Additionally, this network integrates an attention mechanism, which dynamically focuses on key areas and energy-constrained nodes.

Critic Network:, estimates the state value function. By using a dual time-scale evaluation, it simultaneously considers immediate coverage rewards and long-term network lifetime, while also incorporating graph convolution layers to effectively capture the spatial correlations among nodes.

Both networks share the underlying feature extraction layers to accelerate knowledge sharing and improve learning efficiency. A graph feature extraction module, specifically designed to adapt to the dynamic changes in the wireless sensor network topology, is also integrated.

whereis the initial learning rate, andandcontrol the rate of learning decay, whileis the gradient-based adaptive factor. This learning rate adjustment scheme combines time decay and gradient-adaptive mechanisms, providing a higher learning rate in regions with large gradients to quickly escape local optima, while ensuring stable convergence over time.

The choice of the gradient-aware learning rate adjustment mechanism is motivated by the need to balance exploration and exploitation in the dynamic WSN environment. The parameterplays a crucial role in determining the sensitivity of the learning rate to the gradient magnitude. A careful selection ofis necessary to ensure that the learning rate adaptation does not lead to instability, especially in the presence of noise which is common in WSN deployments. Experimental analysis and parameter tuning are required to determine the optimal values for,, and, taking into account the specific characteristics of the WSN application and the level of environmental noise. Future work could investigate more sophisticated adaptive learning rate strategies that incorporate additional information about the environment’s dynamics and the agent’s performance over time.

where. Exponential term temporarily boosts learning rate during large gradients, while denominator ensures ultimate convergence.

Extensive simulation experiments are conducted on the ns-2 platform to comprehensively evaluate the performance of the ACDRL model51. In the experimental design, a 300300 meter simulation environment is constructed, where 2,500 sensor nodes are randomly and uniformly distributed, each with a 30-meter effective sensing radius. The specific parameters of the experiment are shown in Table2. To provide a complete picture of the experimental setup, we have included the DRL-specific training parameters in Table3, such as the initial learning rate, batch size, and discount factor.

To enhance the clarity and reproducibility of our experimental setup, we have introduced Table3, which details key DRL training parameters such as the initial learning rate, batch size, and discount factor. These parameters are essential for understanding the training process and replicating our results. Utilizing the parameter settings from equation2, withand, the coverage performance of the wireless sensor network after deployment is presented in Table4.

To provide a more comprehensive evaluation of the robustness of our ACDRL model, we have incorporated variance values for each data point in Table4. These variance values, denoted as ± in the table, offer insights into the stability and reliability of the coverage performance across multiple simulation runs. These statistical measures are crucial for assessing the robustness of the ACDRL model in various scenarios.

In practical deployment scenarios, wireless sensor networks are often challenged by factors such as hardware defects, adverse weather conditions, electromagnetic interference, energy depletion, and signal attenuation. These factors can lead to node performance degradation or complete failure, which in turn affects network topology and overall service quality. To evaluate the robustness of the ACDRL algorithm under such challenging conditions, we have designed a series of targeted experiments to simulate node failures and assess the model’s performance.

Random node failures with a proportion of 0.05 are introduced into the simulation environment, and the network coverage performance changes at different time points are tracked and recorded52,53. Figure2presents the key results of this analysis. From Fig.2(a), it can be observed that when the total number of nodes is 1,500, even with 5% of the nodes failing, the ACDRL algorithm is able to maintain approximately 87% coverage, demonstrating significant fault tolerance. More notably, as shown in Fig.2b, when the node density is increased to 2,500, the impact of the same proportion of node failures on system performance is further reduced, with the coverage rate decreasing by only about 4.3%.

This excellent fault tolerance is attributed to the adaptive learning mechanism of ACDRL. The algorithm not only optimizes node distribution during the initial deployment phase but also dynamically adjusts the coverage strategy during operation, effectively responding to changes in network topology. When certain areas experience coverage gaps due to node failures, neighboring nodes can autonomously adjust their coverage range and operational cycle through the reinforcement learning framework, thereby filling these gaps. This distributed self-healing feature enables ACDRL to perform exceptionally well in high-density deployment scenarios, providing reliable support for large-scale practical applications.

As shown in Fig.2, although increasing the sensor node density enhances network robustness, its effect on the initial coverage improvement exhibits diminishing returns. When 1,500 nodes are deployed in the network, even with a certain proportion of nodes failing, ACDRL is still able to maintain a high coverage rate through its intelligent learning mechanism. More notably, when the number of nodes increases to 2,500, the impact of the same proportion of node failures on the system coverage rate is significantly reduced, which demonstrates that ACDRL has stronger fault tolerance in high-density deployment scenarios. Figure3presents the relationship between node density and network lifetime, and the data indicates that the ACDRL algorithm effectively balances the relationship between coverage rate and energy consumption. This advantage stems from the core design concept of the algorithm–maintaining the minimum necessary active node set while ensuring that coverage requirements are met. Notably, ACDRL can dynamically adjust node wake-up patterns based on environmental changes and energy consumption, achieving load balancing and preventing energy depletion in specific areas, which would otherwise cause coverage holes. This adaptive energy management mechanism enables ACDRL to demonstrate significant advantages in large-scale deployment scenarios with resource constraints.

To comprehensively evaluate the comparative performance of ACDRL against existing mainstream coverage optimization algorithms, a standardized testing environment is constructed where up to 1,000 sensor nodes are randomly distributed within a 6060 meter area, and each node has a sensing radius of 12 meters, the specific parameters of the experiment are shown in Table5. Table6provides a comparison of key features among different methods in the literature. Each column represents a specific feature: Online scheduling indicates whether the method supports dynamic online scheduling; Sensing coverage indicates whether the method optimizes sensor coverage; and Survival of nodes indicates whether the method considers node survival, such as through energy-saving strategies to extend node lifetime.

Comparison of key features among different methods in the literature.

Figure4presents the coverage rate comparison results based on the probabilistic attenuation sensing model under different node density conditions. The data clearly shows that, while the performance of various algorithms is similar in the low-density deployment stage (node count < 300), ACDRL gradually exhibits significant advantages as the number of nodes increases. Especially when the node density reaches a moderate level, the coverage rate of ACDRL is, on average, about 2.5% higher than that of competing algorithms such as OPFS54, DDPG55, and DQN56. This performance advantage mainly arises from ACDRL’s adaptive learning ability–by continuously interacting with the environment, the algorithm can identify coverage holes and optimize node wake-up strategies to form a more efficient coverage pattern. More importantly, ACDRL is able to maintain a continuously growing coverage rate in high-density areas (node count > 700), while other algorithms tend to reach saturation, which fully demonstrates the adaptability and scalability of the deep reinforcement learning-based coverage optimization method in complex network environments.

This study presents ACDRL, a novel deep reinforcement learning-based deployment strategy for wireless sensor networks that simultaneously optimizes coverage and energy efficiency. The key innovation of ACDRL is its adaptive learning mechanism that combines spatial feature extraction with dual-stream value estimation, enabling sensor nodes to dynamically adjust their operational parameters through environmental interactions while considering network topology, energy states, and coverage requirements. The framework incorporates a novel reward shaping technique that balances immediate coverage gains with long-term energy conservation, along with a dynamic exploration-exploitation strategy that automatically adapts to network conditions. Extensive simulations demonstrate ACDRL’s superior performance compared to conventional methods (OPFS, DDPG, DQN), particularly in challenging scenarios involving high-density deployments and node failures. These results highlight the framework’s robustness and practical applicability. Future work will investigate ACDRL’s extension to heterogeneous networks, mobile sensor scenarios, and multi-objective optimization problems to further improve network performance in complex environments.

Future work will investigate ACDRL’s extension to heterogeneous networks, mobile sensor scenarios, and multi-objective optimization problems to further improve network performance in complex environments. Specifically, we aim to integrate heterogeneous node capabilities (e.g., varying sensing ranges and energy capacities) into the state representation and develop a multi-agent communication protocol to handle cooperative deployment of mobile sensors, while expanding the reward function to incorporate latency and communication overhead metrics for more comprehensive optimization.