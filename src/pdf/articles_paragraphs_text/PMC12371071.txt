In modern industrial construction sites, safety inspection tasks face challenges such as large-scale variations and complex multi-object detection scenarios, particularly in detecting critical safety indicators like flames, smoke, personnel attire, and operational behaviors. To address these limitations, this paper proposes YOLO-GL, an enhanced detection network featuring three key innovations: (1) a redesigned Parallelized Local-Global Multi-Level Fusion Module (C2f_gl) with a local-global attention mechanism for improved multi-scale feature representation; (2) a hierarchical feature fusion architecture; and (3) an adaptive feature fusion shuffling module (Multi-Scale Fusion Module and Adaptive Channel Shuffling Module, MSF&ACS) for dynamic optimization of cross-scale semantic relationships. Extensive experiments on composite datasets combining public benchmarks and industrial site collections demonstrate that YOLO-GL achieves state-of-the-art performance, improving mAP@0.5 by 3.5% (from 70.8% to 74.3%) and mAP@0.5:0.95 by 2.9% (from 38.7% to 41.6%) for flame detection, while maintaining real-time processing at 80.59 FPS. The proposed architecture exhibits superior robustness in complex environments, offering an effective solution for industrial safety monitoring applications.

Industrial safety monitoring systems require precise detection of potential hazards, such as pre-ignition smoke signatures and personnel collapse incidents during high-risk operations. Recent frequent occurrences of fires and workplace accidents in factories have led to significant economic losses and severe threats to worker safety and corporate social responsibility. Current safety protocols rely heavily on manual supervision, which suffers from high latency, excessive labor costs, and limited monitoring coverage. These limitations highlight the urgent need for AI-powered detection frameworks with enhanced robustness and contextual awareness, particularly for complex industrial environments with dynamic illumination and occluded visual fields1.

Existing safety monitoring technologies can be categorized into three types: sensor-based, video-based, and deep learning-based detection. Sensor-based methods monitor environmental parameters like temperature and gas, while video-based approaches use cameras and computer vision algorithms to detect anomalies. Deep learning techniques leverage neural networks to extract high-level features, improving detection accuracy and robustness. However, each method has limitations: sensor-based systems are prone to environmental interference, video-based approaches struggle with false positives and negatives in complex scenes, and deep learning models demand substantial computational resources, hindering deployment in resource-constrained environments.

Object detection, a crucial subfield of computer vision, predicts target categories and locations in images with minimal latency, enabling applications such as autonomous driving and drone surveillance. Recent advancements in YOLO-series detectors2–10have achieved remarkable efficiency-accuracy trade-offs through innovations including efficient feature extraction modules such as Cross Stage Partial Network (CSPNet)11and EfficientRep12, as well as multi-scale fusion strategies such as Path Aggregation Network (PAN)13and BiFPN14. Nevertheless, these methods face challenges in handling complex scenes, fine-grained targets, or multi-task scenarios.

Existing multi-scale feature fusion approaches, such as PAN and BiFPN, primarily focus on bidirectional feature aggregation. PAN employs a bottom-up and top-down pathway to enhance feature representation, while BiFPN introduces weighted connections to balance the contributions of different feature levels. However, these methods exhibit limitations in handling complex scenes, fine-grained targets, or multi-task scenarios due to their rigid fusion mechanisms and insufficient exploitation of hierarchical feature interactions.

In contrast, our proposed hierarchical feature fusion architecture incorporates triple-level feature interactions through the MSF&ACS module, which dynamically adjusts feature contributions based on channel-wise similarity and attention mechanisms. Specifically, MSF&ACS leverages attention-based channel shuffling to achieve more flexible and adaptive feature integration across different scales. This approach not only enhances the discriminative power of feature representations but also improves the model’s robustness in diverse detection scenarios, particularly excelling in challenging environments with fine-grained targets or complex backgrounds.

The C2f module in YOLOv8, while efficient, has limited capacity for deep modeling of local-global features, hindering accuracy-speed balance in cluttered backgrounds or dense target scenarios. Additionally, existing fusion methods rely on fixed concatenation operations, failing to adaptively adjust fusion strategies based on semantic correlations across multi-scale features. Redundant computational designs further constrain performance improvements.

Enhanced C2f_gl Module15: A redesigned feature extraction module incorporating a local-global attention mechanism to strengthen feature capture capabilities in scenarios with large target size variations and high target density.

Multi-Level Feature Fusion Network16: A hierarchical fusion architecture integrating triple-level feature fusion (shallow, middle, and deep) to enhance multi-scale detection capabilities in complex environments.

Multi-Scale Fusion Module and Adaptive Channel Shuffling Module (MSF&ACS)17: A dynamic fusion module enhancing semantic relevance and complementarity across multi-level features, improving detection accuracy and robustness.

Real-time object detection has garnered significant attention due to its critical role in applications requiring low-latency performance, such as autonomous driving, surveillance systems, and robotics. The YOLO-series models have emerged as dominant solutions in this domain, owing to their efficient architectures and superior performance. Early versions (YOLOv1–v3) established the foundational framework, comprising a backbone network, feature fusion neck, and detection head, laying the groundwork for subsequent improvements. YOLOv4 introduced CSPNet and an enhanced Path Aggregation Network (PAN), combined with advanced data augmentation techniques (Mosaic and CutMix) and multi-scale optimization strategies, significantly boosting detection accuracy and speed. However, it still faces limitations in handling complex backgrounds and fine-grained targets, particularly in detecting small and occluded objects. YOLOv6 further optimized the backbone and neck architecture through BiC and SimCSPSPPF modules, adopting self-distillation and anchor-assisted training strategies to enhance model performance. YOLOv7 proposed the E-ELAN module to improve gradient flow and explored cost-free enhancement techniques, achieving a better balance between efficiency and accuracy. Despite these advancements, these methods still encounter challenges in multi-task scenarios, necessitating more flexible feature fusion mechanisms. YOLOv8 integrated the C2f module for efficient feature extraction and fusion, setting a new benchmark in real-time detection. YOLO11 further refined the backbone and neck architecture by introducing the C3k2 and C2PSA components, optimizing the overall design and training process to improve both accuracy and processing speed. Nevertheless, existing methods still struggle with complex backgrounds and fine-grained targets, highlighting the need for more adaptive and robust feature fusion mechanisms. YOLOv1218marks a pivotal advancement in the YOLO series by introducing a novel attention mechanism, A2C2f, which dynamically weights channels and spatial locations to enhance feature representation. This mechanism enables the model to better capture both local and global contextual information, addressing challenges in complex scenes and fine-grained target detection. By integrating attention mechanisms into the YOLO framework, YOLOv12 sets a new benchmark for real-time object detection, particularly in scenarios requiring robust feature fusion and multi-task adaptability.

End-to-end object detection has emerged as a novel paradigm by eliminating handcrafted components and post-processing steps, thereby simplifying the detection pipeline. DETR pioneered this approach by leveraging a Transformer-based architecture and employing Hungarian matching for one-to-one prediction19. However, DETR suffers from slow convergence and poor performance in detecting small objects, limiting its practicality in real-time applications. Subsequent works have focused on addressing these limitations: Deformable-DETR accelerated convergence through deformable multi-scale attention, improving detection accuracy for small objects20; DINO integrated contrastive denoising and hybrid query selection to further enhance detection accuracy21; and RT-DETR balanced accuracy and latency by introducing a hybrid encoder and uncertainty-minimized query selection22. Inspired by these advancements, YOLOv10 adopted dynamic label assignment and redundancy-free prediction mechanisms, surpassing RT-DETR in both speed and accuracy. Despite these improvements, the computational overhead of Transformer-based architectures remains a challenge, particularly for high-resolution image processing, necessitating further innovations in efficiency optimization.

Feature fusion strategies play a pivotal role in determining the performance of object detection models. Traditional methods like PAN and BiFPN employ fixed multi-scale fusion rules, often failing to adapt to dynamic scenes or diverse target scales. PAN enhances feature representation through bottom-up and top-down pathways, but its fusion mechanism lacks flexibility, making it difficult to handle complex backgrounds. BiFPN introduced weighted connections to balance contributions from different feature levels, but its high computational complexity limits its practicality in real-time applications. YOLOv8 adopted a concatenation-based fusion approach, which is computationally efficient but restricts the selective flow of information. Gold-YOLO addressed this limitation by introducing a Gather-and-Distribute (GD) mechanism, which globally aggregates multi-level features and injects contextual information into higher layers, thereby reducing information loss and improving detection accuracy across various scales. However, existing methods still struggle with fine-grained target detection and complex backgrounds, highlighting the need for more adaptive and context-aware fusion mechanisms. Our proposed approach aims to address these limitations by leveraging hierarchical feature interactions and dynamic attention mechanisms.

Localized attention mechanisms have emerged as a key solution to address the high computational cost of global self-attention in vision transformers. Specifically, the Swin Transformer23introduced local window attention with adaptive partitioning, significantly reducing computational complexity while maintaining high performance. Axial attention24and criss-cross attention25further optimized computational efficiency by operating along single dimensions such as rows or columns. The CSWin Transformer26extended this approach by employing parallel horizontal and vertical stripe attention, achieving a better balance between efficiency and accuracy through simultaneous feature processing in both horizontal and vertical directions.

Although recent work27has optimized attention mechanisms through local-global relationships, Mamba-based approaches28remain suboptimal for real-time applications, primarily due to their high computational complexity and memory consumption. To address these limitations, we propose a streamlined local attention mechanism and seamlessly integrate it with the C2f module. This mechanism enhances computational efficiency and detection accuracy without complex architectural modifications by dynamically adjusting attention window sizes and selectively fusing features. Experimental results demonstrate that our approach excels in resource-constrained environments such as embedded devices, providing an efficient and practical solution for real-time object detection.

These modules complement each other while ensuring efficient use of computational resources.

Traditional C2f modules are well-known for their lightweight design and efficiency in feature extraction for object detection models. However, different feature locations have varying effects on the model’s perception, and these unique pieces of information are worth preserving and enhancing. Despite their strengths, traditional C2f modules exhibit limitations in capturing global contextual information when processing complex backgrounds or dense object scenarios. To address this, we propose an enhanced C2f_gl module that integrates a Local Perception mechanism through parallel pathways to improve comprehensive feature representation, as illustrated in Fig.2.

Parallelized local-global multi-level fusion module.

Given an input feature, where,,, anddenote batch size, input channels, height, and width, respectively, the module first applies aconvolutional layer to reduce the number of channels, thereby lowering the computational complexity. This operation yields intermediate features, whererepresents the compressed channel dimension (: output channels;: compression ratio). The intermediate features are split into two branches:, where. Thebranch is processed through stacked residual units to extract deeper features while preserving original semantic information, resulting in.

To overcome the limitations of fixed concatenation in multi-scale feature fusion, we propose a dynamic fusion strategy that adaptively adjusts cross-scale feature weights. This module operates in two stages, as shown in Fig.3. In the first stage, the Multi-Scale Fusion (MSF) module resizes and concatenates features from three different scales, effectively capturing both fine-grained and high-level semantic information, which is particularly beneficial for detecting objects of varying sizes. In the second stage, the Adaptive Channel Shuffling (ACS) module performs channel shuffling on the concatenated features guided by attention weights, enabling the model to dynamically focus on the most relevant features, thereby enhancing feature representation. Overall, this approach significantly improves the model’s detection performance in complex environments, particularly for targets that are blurred, partially occluded, or otherwise challenging to detect.

Multi-scale fusion module and adaptive channel shuffling module (MSF&ACS).

The Adaptive Channel Shuffling Module (ACS) dynamically rearranges channel groups of the input tensor through a learned attention mechanism, emphasizing critical features while suppressing irrelevant information. This design enhances feature representation by adaptively reweighting and reorganizing channels based on their semantic importance, which is particularly effective in complex scenarios where feature relevance varies significantly.

The processed similarities then undergo non-linear normalization through,where the transformation inincorporates the Sigmoid activation function defined in.

This step enhances the discriminative power of the attention mechanism by capturing higher-order interactions among channel groups. Channel reordering is performed based on the refined attention weights, generating a shuffled tensor. This process ensures that channels with higher importance scores are prioritized, while less relevant channels are suppressed. Residual connectionpreserves original features while enhancing gradient flow, producing final output.

This concatenated output is processed through the ACS module before channel compression via 11 convolutionto achieve target dimensionality.

The MSF&ACS module dynamically adjusts the fusion strategy based on the semantic relevance among shallow, middle, and deep features. Specifically, this module employs an adaptive channel selection mechanism to prioritize the most informative features at each hierarchical level. This mechanism ensures that the fusion process is optimized for the unique characteristics of each feature level, thereby enhancing the overall representational capacity of the network.

Shallow features are processed through the MSF module to preserve fine-grained details and enhance local feature representation. Middle features are aligned and fused by integrating shallow features, deep features, and the SPPF module, achieving a balance between local and global information. This integration improves the detection capability for medium-sized objects. Deep features are processed through the MSF module to leverage high-level semantic information, thereby enhancing global contextual understanding and the detection of large objects.

In contrast, FPN propagates high-level semantic features to lower levels via a top-down pathway, but it lacks the ability to dynamically adjust feature weights based on semantic relevance. Similarly, PANet enhances feature fusion by incorporating a bottom-up pathway, but it still relies on static fusion rules.

By clearly defining the mapping logic for shallow, middle, and deep features, YOLO-GL ensures a comprehensive and adaptive feature fusion process. This approach maximizes detection performance across diverse scenarios, addressing the challenges posed by varying object scales and complex backgrounds. The structured interaction among the three feature levels enables the model to achieve superior robustness and accuracy in object detection tasks.

The YOLOv8 architecture was selected as the baseline model due to its empirically validated balance between detection accuracy and computational efficiency in real-time applications. To enhance feature representation capabilities, we implemented our proposed local-global attention branch and multi-scale fusion mechanism, with comprehensive ablation studies conducted under identical training-from-scratch protocols. Experiments were executed in a CUDA-accelerated environment comprising Python 3.8.19, PyTorch 2.11, and CUDA 12.0, supported by an Intel®Corei7-14700KF CPU, NVIDIA GeForce RTX 4090 GPU (24GB VRAM), and 64GB DDR4 RAM. All experiments strictly maintained consistent hyperparameter settings as detailed in Table1.

To address the lack of suitable public datasets for industrial safety monitoring, we developed a dedicated dataset by integrating public sources (flame, helmet datasets) with self-collected images captured under varying lighting conditions (natural/low-light) and perspectives (horizontal/elevation/depression) in real industrial environments. The initial 13,068-image collection (5,802 flames, 5,546 hot work, 1,720 workwear) was further enriched by incorporating additional multi-scenario flame images to enhance the effectiveness of flame detection. The dataset underwent rigorous augmentation including random rotation, HSV saturation adjustment, and noise injection, followed by manual curation to ensure quality. The dataset is divided into training, testing, and validation sets using stratified sampling based on scene characteristics such as lighting conditions, viewpoints, and target density, ensuring a balanced representation of heterogeneous scenarios. The dataset was split in an 8:1:1 ratio, annotated with bounding boxes and class labels using LabelImg, offering comprehensive coverage of industrial safety scenarios (Fig.4).

To ensure comprehensive coverage of targets across varying scales and distances, the dataset was meticulously collected and filtered, encompassing a wide range of target sizes and diverse scene contexts. The specific proportional distribution of target sizes and their respective scale ranges within the dataset is illustrated in Fig.5. This approach ensures a robust representation of industrial safety scenarios, enhancing the model’s adaptability to real-world applications.

In industrial datasets, True Positive (TP) represents the number of targets correctly identified; False Positive (FP) indicates the number of targets incorrectly identified; and False Negative (FN) denotes the number of targets that were not detected.

where O denotes the constant order, K is the convolution kernel size, C represents the number of channels, M indicates the input image size, and i denotes the number of iterations.

Additionally, FPS (Frames Per Second) measures the number of image frames processed in one second, providing a direct indicator of the operational speed of the image detection model.

As demonstrated in Table2, our YOLO-GL model outperforms contemporary YOLO variants and classical detection networks, including Faster R-CNN29, Gold-YOLO, RT-DETR, YOLOv3, YOLOv5, YOLOv6, YOLOv8, YOLOv98, YOLOv10, YOLO11, YOLOv12 and FBRT-YOLO30. With a parameter count of 4.29 million, the YOLO-GL achieves state-of-the-art mean Average Precision (mAP) scores of 74.3% (mAP@0.5) and 41.6% (mAP@0.5:0.95), surpassing all compared models while maintaining computational efficiency. Notably, the model attains a detection speed of 80.59 FPS, meeting real-time requirements, and a recall rate of 70.4%, which exceeds most of the compared detection networks. YOLO-GL, with its smaller model size, surpasses the size of the S models in the YOLO series across generations. These results confirm the optimal balance of YOLO-GL between model compactness, efficiency, and detection accuracy.

Comparison with state-of-the-art models when selecting flame as the target, the model was compared with other YOLO variants and their derivatives in terms of mAP@0.5, mAP@0.5:0.95, parameter count, frame rate and recall rate.

All results were obtained without employing advanced training techniques such as knowledge distillation or PGI to ensure a fair comparison.

The use of diverse datasets for training and validation plays a crucial role in enhancing the performance and robustness of models. It significantly improves the generalization capability, performance, and robustness of object detection models, ensuring their effectiveness in various real-world applications. In this study, we conducted a comparative experiment between YOLOV8n and YOLO-GL on the VisDrone 2019 dataset, a publicly available large-scale drone vision dataset31. The VisDrone 2019 dataset, released by Tianjin University, consists of 10,209 static images captured by drones from various perspectives. It includes 6,471 images for training, 548 for validation, and 1,610 for testing, encompassing approximately 2.6 million object instances. The dataset covers 10 object categories, such as pedestrians, cars, and trucks, across 14 urban and rural environments in China.

As demonstrated in Table3, the YOLO-GL model exhibits enhanced detection performance, with improvements of 0.4% and 0.3% in mAP@0.5 and mAP@0.5:0.95, respectively, compared to YOLOV8n. Analyzing the precision performance across different categories, the YOLO-GL model demonstrates superior detection capabilities for targets such as pedestrians, people, bicycles, and cars. These results highlight the advantages of YOLO-GL in detecting multiple objects in large-scale, complex, and diverse environments. Furthermore, we conducted a comparative analysis of the detection results between YOLOv8n and YOLO-GL, as illustrated in Fig.6. In the first row of image comparisons, our model successfully detected the person on the motorcycle, which was missed by YOLOv8n. In the second image, YOLO-GL accurately identified the seated individual, demonstrating its enhanced capability in detecting occluded or complex postures. In the third image, YOLO-GL exhibited superior performance in detecting the target object, further underscoring its robustness in challenging scenarios. These observations reinforce the improved detection accuracy and reliability of YOLO-GL in diverse and intricate environments.

Comparison of YOLOV8n and YOLO-GL on the VisDrone dataset.

The comparative detection results on the VisDrone 2019 Dataset are presented, where (a) represents the detection results of YOLOv8n and (b) represents the detection results of YOLO-GL.

As depicted in Table4, we conducted a performance evaluation of the proposed C2f_gl module within the main network, comparing it with other versions such as the C3 module in YOLOv5, the C2f module in YOLOv8, and the C3k2 module in YOLO11, and the A2C2f module in YOLOv12. These five feature extraction modules were integrated into the YOLOv8 backbone for training. The experimental results indicate that our C2f_gl module achieved the best performance in terms of mAP@0.5, mAP@0.5-0.95, and Recall.

Precision comparison of the C2f module and its variants in the YOLOv8 baseline model.

The mAP@0.5 and mAP@0.5:0.95 for other target objects were compared with the original network,as illustrated in Fig.7. Specifically, YOLO-GL showed enhancements in mAP@0.5 from 60.8% to 61.1% for smoke, 89.2% to 89.4% for polish, 92.7% to 94.5% for weld, 66.0% to 67.1% for fall, and 79.1% to 80.0% for clothes, while mAP@0.5:0.95 improved from 50.5% to 51.2% for smoke, 95.8% to 96.4% for polish, 53.8% to 58.3% for fall, and 67.4% to 68.9% for clothes. These results indicate that YOLO-GL consistently outperforms YOLOv8n across multiple categories, particularly in challenging scenarios such as fall and weld, where significant improvements were observed.

Presents the comparison of mAP50 and recall rates for six target categories–smoke, normal, polish, weld, fall and clothes–with the YOLOv8n model.

For multi-object detection in complex scenarios, Fig.8demonstrates the superior capability of YOLO-GL in detecting distant, small, and blurry targets compared to YOLOv6n, Gold-YOLO, and YOLOv8n. The enhanced performance is attributed to the integration of the Parallelized Local-Global Multi-Level Fusion module (C2f_gl) and the Multi-Scale Fusion Module and Adaptive Channel Shuffling Module (MSF&ACS), which significantly improve detection robustness across diverse scenarios involving multi-class, multi-scale objects against cluttered backgrounds. These modules enable YOLO-GL to effectively capture fine-grained details and global contextual information, ensuring accurate and reliable detection even in challenging environments.

Comparative analysis of multi-object detection performance across different models: (a) original image, (b) YOLOv6n, (c) Gold-YOLO, (d) YOLOv8n, and (e) YOLO-GL. Rows 1 and 2 focus on small or distant targets, while rows 3 and 4 demonstrate clothing and multi-person scenario detections.

Table5presents the ablation study results, which systematically evaluate the individual contributions of each proposed component. When only the C2f_gl module is introduced, both the mAP and recall rates show notable improvements compared to the baseline YOLOv8 model. Similarly, when the MSF&ACS fusion network is implemented independently, significant enhancements in mAP and recall are observed relative to YOLOv8. Notably, the highest performance is achieved when both the C2f_gl module and the MSF&ACS fusion network are integrated, resulting in the maximum values for mAP and recall. These findings demonstrate that each component contributes meaningfully to the model’s overall performance, with the combined architecture delivering the most substantial gains in detection accuracy and robustness. This systematic analysis underscores the effectiveness of the proposed modules and their synergistic impact on the model’s capabilities.

In the fusion network section, we conducted ablation studies on YOLOv6, YOLOv10, YOLO11, and YOLOv12. As shown in Table6, the results demonstrate that our fusion network significantly outperforms the original networks in terms of mAP@0.5, mAP@0.5-0.95, and recall rate. Specifically, for YOLOv6, the mAP@0.5 increased from 71.0% to 71.7%, and the recall rate improved from 62.1% to 66.2%. For YOLOv10, the mAP@0.5 rose from 69.9% to 70.9%, and the recall rate increased from 62.5% to 65.5%. In the case of YOLO11, the mAP@0.5 improved from 73.1% to 73.4%, and the recall rate surged from 67.6% to 69.2%. Lastly, for YOLOv12, the mAP@0.5 remained stable at 72.8%, but the recall rate increased from 67.2% to 67.5%. These results highlight the effectiveness of our fusion network in enhancing the performance of YOLO variants across multiple metrics.

Ablation study on MSF&ACS modules in YOLO variants.

Systematic experiments (Fig.9) demonstrate that the C2f_gl module achieves maximum accuracy gain (73.1%) at optimal parameters (,, and), highlighting the importance of parameter tuning for robust multi-object detection. Comparative feature visualization with YOLOv8 (Fig.10) further reveals the proposed model’s superior perceptual capabilities in challenging tasks, where it effectively suppresses irrelevant regions while enhancing target features, outperforming the baseline in both robustness and precision.

Parameter sensitivity analysis: From left to right: Threshold(T): Impact of threshold parameter t under fixedand, Alpha(): Impact of balance coefficientwith constrained t = 0.4 and, Beta() Impact of scaling factorunder t = 0.4 and.

Feature visualization comparison: (a) Original image (b) Activation maps from YOLOv8’s C2f module (c) Enhanced activations from YOLO-GL’s C2f_gl module.

As demonstrated in Fig.11, the YOLO-GL framework was successfully validated through field deployment using an integrated hardware system consisting of a Hikvision industrial spherical camera with environmental sensors and a portable computing unit (Intel®Corei7-14650HX CPU, NVIDIA RTX 4070 GPU with 8GB GDDR6 VRAM, 16GB DDR5 RAM) running Python 3.8.19 with PyTorch 2.11 and CUDA 12.0 acceleration. Despite hardware limitations, the system achieved stable real-time detection performance exceeding 30 FPS, meeting operational requirements for industrial applications while demonstrating the framework’s practical deployability in resource-constrained scenarios.

Equipment and field testing: The left panel (a) depicts the on-site testing of the equipment, while the right panel (b) presents a frontal view of the equipment.

This study improves industrial safety inspection accuracy and reduces false positives/negatives by optimizing the backbone and neck components of the YOLO framework. For feature extraction, we propose a Parallelized Local-aware Multi-Level Fusion module (C2f_gl), which significantly enhances detection precision for localized small targets. In feature fusion, a novel strategy incorporating a Multi-Scale Feature Fusion Shuffling module strengthens cross-layer semantic integration and optimizes channel arrangements. Experimental results demonstrate that YOLO-GL achieves competitive accuracy and recall rates. However, the current model exhibits suboptimal inference efficiency on low-power devices, necessitating further architectural refinement and quantization compression to balance computational efficiency and detection accuracy.

This work was supported in part by the Basic Research Project (Key Research Project) of the Education Department of Liaoning Province (No. JYTZD2023009), and in part by the Fundamental Research Funds for the Universities of Liaoning province (320224081).