The assessment of short-answer questions (SAQs) in medical education is resource-intensive, requiring significant expert time. Large Language Models (LLMs) offer potential for automating this process, but their efficacy in specialized medical education assessment remains understudied. To evaluate the capability of five LLMs to grade medical SAQs compared to expert human graders across four distinct medical disciplines. This study analyzed 804 student responses across anatomy, histology, embryology, and physiology. Three faculty members graded all responses. Five LLMs (GPT-4.1, Gemini, Claude, Copilot, DeepSeek) evaluated responses twice: first using their learned representations to generate their own grading criteria (A1), then using expert-provided rubrics (A2). Agreement was measured using Cohen’s Kappa and Intraclass Correlation Coefficient (ICC). Expert-expert agreement was substantial across all questions (average Kappa: 0.69, ICC: 0.86), ranging from moderate (SAQ2: 0.57) to almost perfect (SAQ4: 0.87). LLM performance varied dramatically by question type and model. The highest expert-LLM agreement was observed for Claude on SAQ3 (Kappa: 0.61) and DeepSeek on SAQ2 (Kappa: 0.53). Providing expert criteria had inconsistent effects, significantly improving some model-question combinations while decreasing others. No single LLM consistently outperformed others across all domains. LLM strictness in grading unsatisfactory responses varied substantially from experts. LLMs demonstrated domain-specific variations in grading capabilities. The provision of expert criteria did not consistently improve performance. While LLMs show promise for supporting medical education assessment, their implementation requires domain-specific considerations and continued human oversight.

Short Answer Questions (SAQs) are a vital component of assessment in medical education, offering distinct advantages over multiple-choice questions (MCQs) by requiring students to actively recall and articulate knowledge rather than merely recognizing correct answers [1]. SAQs play a crucial role in evaluating students’ understanding of complex medical concepts, their ability to synthesize information, and their capacity to express clinical reasoning in a concise format. Unlike MCQs that rely primarily on recognition, SAQs assess higher-order cognitive processes and better simulate the recall-based reasoning required in actual clinical practice [2]. SAQs assess students’ ability to articulate medical concepts clearly and concisely in writing – a critical skill for medical documentation, patient notes, and professional communication that cannot be evaluated through MCQs [3].

Despite their pedagogical value, the adoption of SAQs in medical education faces significant practical challenges. The grading of SAQs is notably time-intensive, requiring substantial faculty resources and expertise [4]. Grading open questions is a time-intensive task for teachers, which has a negative impact on scalability and can push educators toward favoring MCQs despite their limitations in assessing certain competencies [5]. The labor-intensive nature of SAQ grading can lead to delayed feedback, grader fatigue, and potential inconsistencies in evaluation, all of which compromise the educational value of these assessment tools [6]. SAQs, while pedagogically superior for certain types of assessment, are often underutilized due to the practical limitations of manual grading [7].

Recent years have witnessed remarkable advancements in artificial intelligence (AI), particularly in the domain of Large Language Models (LLMs). These sophisticated AI systems, including models such as GPT-4, Claude, Gemini, Copilot, and DeepSeek, have demonstrated unprecedented capabilities in understanding, generating, and analyzing human language [8]. ChatGPT could achieve scores on the United States Medical Licensing Examination (USMLE) that would be considered passing for medical students, specifically achieving accuracies of 64.4% on NBME-Free-Step1 and 57.8% on NBME-Free-Step2 questions, outperforming previous language models by a significant margin [9].

The capabilities of LLMs in medical education extend to performance on basic science subjects that form the foundation of medical curricula. In the field of anatomy, GPT-4 demonstrated the highest performance among six different LLMs on USMLE-style MCQs, correctly answering 60.5% of questions, followed by Copilot (42.0%) and ChatGPT-3.5 (41.0%) [10]. For embryology, LLMs correctly answered 78.7% of the questions, with GPT-4o and Claude performing best (89.7% and 87.5% correct answers, respectively) [11]. In neuroscience education, selected AI-driven chatbots could accurately answer 67.2% of MCQs from a medical neuroscience course, with Claude and GPT-4 outperforming other models with 83% and 81.7% correct answers, respectively, exceeding average student results [12]. For biochemistry, LLMs demonstrated impressive results, correctly answering 81.1% of questions on average – notably outperforming medical students by 8.3%, with Claude exhibiting the strongest performance (92.5% correct), followed by GPT-4 (85%), Gemini (78.5%), and Copilot (64%) [8]. Recent research in cardiovascular physiology assessment has demonstrated significant performance differences between the same LLM versions, with ChatGPT-4 achieving 83.33% accuracy compared to ChatGPT-3.5’s 60% accuracy on concept-based questions (p= 0.045) [13]. These findings collectively demonstrate that current LLMs possess substantial knowledge of basic medical sciences, though with important variations across models and topics. LLMs can achieve impressive accuracy rates, but their performance is not uniform across all medical domains [14].

The convergence of the need for more efficient SAQ grading methods and the advanced capabilities of LLMs presents a promising opportunity for innovations in medical education assessment. The ability of LLMs to understand, analyze, and evaluate complex textual responses makes them potentially valuable tools for automating or assisting with the grading of SAQs [15]. Automatic Short Answer Grading (ASAG) has been an area of research interest for several decades, with various approaches developed over time, but the advent of LLMs represents a significant leap forward in the capabilities of ASAG systems [5]. Unlike earlier rule-based systems or traditional statistical approaches such as keyword matching, n-gram analysis, and classical machine learning methods (e.g., support vector machines, naive Bayes), modern LLMs represent a paradigm shift in statistical modeling for natural language processing. Built on transformer architectures with attention mechanisms, LLMs employ deep neural networks trained on vast datasets, making reliable, scalable assessment approaches ever more pressing to learn probabilistic representations of language patterns. These transformer-based models can process and understand natural language in context through sophisticated statistical inference, potentially enabling them to evaluate student responses with a degree of nuance and comprehension previously unattainable by earlier automated systems [5]. LLMs offer several potential advantages for SAQ grading in medical education. They can provide immediate feedback, scale to accommodate large numbers of student responses, maintain consistency in evaluation criteria, and potentially reduce the subjective biases that can affect human graders [6]. Furthermore, LLMs could potentially offer detailed, individualized feedback on student responses, enhancing the formative aspect of assessment [14]. LLMs could transform traditional grading systems, characterized by their uniform and often manual approaches, into more nuanced, scalable, and efficient solutions [5,6].

Despite these promising potential applications, the use of LLMs for grading medical SAQs remains a relatively unexplored territory. While studies have examined the general capabilities of LLMs in understanding and generating medical content, there is limited research specifically investigating their effectiveness in evaluating student responses to medical SAQs. This gap in the literature is particularly notable given the unique challenges and requirements of medical education assessment, where accuracy, reliability, and validity are paramount [16].

Initial explorations into LLM-based grading in other domains suggest both promise and challenges. LLMs could achieve moderate agreement with human graders on undergraduate medical education short answer questions, with GPT-4 showing low rates of false positives and high precision for fully correct answers [5,15]. However, there are also limitations, including variability in performance across different questions and the need for high-quality sample solutions or rubrics to guide the LLM evaluation.

The integration of LLMs into medical education assessment also raises important ethical and practical considerations. Issues of bias, transparency, accountability, and the appropriate balance between automation and human judgment must be carefully addressed [6]. The deployment of LLMs in educational contexts must be approached with attention to both practical and ethical challenges to ensure that these technologies enhance rather than compromise the quality and fairness of assessment [17].

While preliminary research suggests the potential of LLMs in supporting SAQ grading, there remains a significant gap in our understanding of how these models perform specifically in medical education [5,15]. The unique characteristics of medical knowledge and reasoning, the high stakes of medical assessment, and the specific requirements of medical education create distinct challenges and opportunities for LLM application that warrant dedicated investigation.

This research gap is particularly significant given the importance of developing more efficient and effective assessment methods in medical education. As medical knowledge continues to expand and evolve, and educational programs face increasing demands for accountability and quality assurance, the need for reliable, scalable assessment approaches becomes ever more pressing. If LLMs can effectively support the grading of SAQs, they could enable more widespread use of this valuable assessment format, potentially enhancing the quality of medical education and assessment.

The potential of LLMs extends beyond medical assessment to transforming educational experiences across learning levels. These models can enhance reading and writing skills in elementary education, generate practice problems in secondary education, and assist university students with research tasks and complex material organization. However, successful educational integration requires careful consideration of its limitations, including interpretability challenges and potential biases, necessitating continued human oversight to ensure educational quality [18].

The emerging paradigm of ‘precision medical education’ integrates longitudinal data and analytics to drive precise educational interventions that address each individual learner’s needs and goals [19]. LLMs could potentially play a role in this paradigm, contributing to more personalized, data-driven approaches to medical education.

In light of these considerations, this study aims to explore the performance of LLMs in grading medical SAQs, focusing on understanding their agreement with expert human graders, the factors influencing their performance, and the potential implications for medical education assessment.

This research evaluated the capability of Large Language Models to grade Short Answer Questions in medical examinations compared to expert human graders. This study involved a retrospective analysis of an existing examination database without directly interacting with students or collecting personal information. The files received from the assessment office were extracted from previous exams and contained 4 SAQs (one from anatomy, histology, embryology, and physiology) with 201 anonymous randomized answers each. So, there were 804 responses for evaluation in total. All prompts, parameters, and methods used with LLMs were documented to maintain transparency.Figure 1summarizes SAQs used in this study, their respective grading scales, and steps in data collection.

The workflow diagram that illustrates the data collection process.

Three independent faculty members (Expert X, Expert Y, and Expert Z) in the rank of full professor in the relevant fields constituted the expert panel. These experts graded responses independently without knowledge of student identity or other experts’ grades. Standardized, detailed rubrics were developed for each question to ensure consistent evaluation criteria.

Five different LLMs publicly available at the moment were included in the study: GPT-4.1 (OpenAI), Gemini 2.0 Flash (Google), Claude 3.7 Sonnet (Anthropic), Copilot (Microsoft), and DeepSeek R1 (Hangzhou Basic Technology Research Co.). The LLM evaluation process occurred in two phases. In the first attempt (A1), each LLM graded all 804 student responses based on internally generated criteria and correct answers based on learned representations. In the second attempt (A2), each LLM graded the same 804 student responses again, but this time using the expert-provided correct answers and grading criteria.Table 1shows the different prompts used for A1 and A2 attempts. The prompts were developed using 10 tips for prompt development suggestions [20].

This two-phase approach allowed for consistency testing, as each LLM evaluated all responses twice to test for consistency in grading and the impact of expert rubrics. The study also accounted for variations in prompt sensitivity by testing the impact of providing expert rubrics versus allowing LLMs to use criteria based on learned representations, and examined domain specificity by analyzing differences in LLM performance across the four medical disciplines.

The ability of GPT-4.1, Gemini, Claude, Copilot, and DeepSeek to grade student answers for 4 SAQs was assessed in May-June 2025. Data was collected in A1 and A2 independent testing sessions for each chatbot, separated by 24-hour intervals, ensuring each attempt was conducted as a fresh conversation session without access to previous responses. All LLM evaluations were conducted using the default settings provided by each model’s standard web interface without manual adjustment of hyperparameters such as temperature or top_p. This approach was chosen to reflect realistic usage scenarios where educators would interact with these LLMs through standard interfaces rather than API calls with customized parameters.

Results were compiled into separate CSV files for each SAQ (SAQ1-SAQ4). Each CSV file contained 13 columns representing grades from the 3 expert graders (Expert X, Expert Y, Expert Z), 5 LLMs’ first attempt (Claude A1, Copilot A1, DeepSeek A1, Gemini A1, GPT-4.1 A1), and 5 LLMs’ second attempt (Claude A2, Copilot A2, DeepSeek A2, Gemini A2, GPT-4.1 A2). A total of 10,452 responses (grades) were analyzed.

Cases where LLM grading students’ responses as ‘0’ (unsatisfactory) rationales were compiled and analyzed, with documentation provided in separate files for each SAQ. The quality and relevance of explanations provided by LLMs for their scoring decisions were evaluated, as was the LLMs’ handling of ambiguous or partially correct answers.

A consensus grade was created from the three experts for each response to serve as a gold standard. This consensus facilitated the calculation of performance metrics for LLMs against the expert standard and allowed for assessment of improvement from A1 to A2 for each LLM to evaluate the impact of expert rubrics.

Agreement metrics were calculated between Expert-Expert pairs, Expert-LLM pairs (for both A1 and A2), LLM-LLM pairs, and A1-A2 for each LLM (to measure the impact of expert rubrics). Agreement among experts was quantified using Cohen’s Kappa and Intraclass Correlation Coefficient.

The statistical analysis compared mean scores and distribution of scores across experts and LLMs, analyzed differences in grading patterns across the four medical disciplines, identified systematic biases or strengths in LLM grading, conducted error analysis on cases with high disagreement, and evaluated the impact of expert rubrics on LLM performance (A1 vs. A2).

Performance metrics were calculated against the expert consensus for LLMs. The degree of improvement from A1 to A2 for each LLM was measured to evaluate the impact of expert rubrics on LLM performance. Statistical analyses were performed using R version 4.2.2 (R Foundation for Statistical Computing, Vienna, Austria).

A large amount of data was received and analyzed, including 2412 grades provided by three experts and 8040 responses from five LLMs. The examples of LLM responses to A1 and A2 prompts during SAQ grading are shown inFigure 2.

Copilot responses to A1 prompt - model-derived answers and rubrics (left side) and A2 prompt - expert-guided answers and rubrics (right side) in SAQ3 grading.

The analysis of inter-rater reliability among the three expert graders (X, Y, and Z) revealed substantial agreement across all four short answer questions, as shown inTable 2. The average Cohen’s Kappa values ranged from moderate to almost perfect agreement, providing a strong baseline against which to evaluate LLM performance. Expert agreement was consistently substantial across all question types, with SAQ4 (embryology) showing substantial to almost perfect agreement (average Kappa: 0.87), while SAQ2 (physiology) demonstrated the lowest, though still moderate, agreement (average Kappa: 0.57).

Legend: This table presents agreement metrics between expert graders (X, Y, Z) across four different short-answer questions. Cohen’s Kappa values measure agreement adjusted for chance: 0.21–0.40 (fair), 0.41–0.60 (moderate), 0.61–0.80 (substantial), > 0.81 (almost perfect). ICC (Intraclass Correlation Coefficient) measures consistency and absolute agreement among multiple raters: < 0.50 (poor), 0.50–0.75 (moderate), 0.75–0.90 (good), > 0.90 (excellent). The two metrics provide complementary perspectives on inter-rater reliability.

The Intraclass Correlation Coefficient (ICC) results further confirmed the high level of reliability among expert graders. The ICC values ranged from good (0.72 for SAQ1) to excellent (0.91 for SAQ2, 0.87 for SAQ3, and 0.92 for SAQ4), with an overall average of 0.86, indicating good to excellent consistency in absolute agreement among the expert raters. Notably, while SAQ2 showed the lowest Kappa values, it demonstrated excellent ICC (0.91), suggesting that despite some categorical disagreements captured by Kappa, the overall scoring pattern and magnitude of scores assigned by experts were highly consistent for this question.

When using internally generated criteria based on learned representations (A1), LLMs showed highly variable performance across question types and models, as detailed inTable 3. Notable findings included best performances from Claude on SAQ3 (anatomy, Kappa: 0.61) and DeepSeek on SAQ2 (physiology, Kappa: 0.53), while the worst performances were observed for Copilot on SAQ1 (histology, Kappa: 0.04), DeepSeek on SAQ3 (anatomy, Kappa: 0.06), and Gemini on SAQ2 (physiology, Kappa: 0.06). GPT-4.1 emerged as the most consistent performer with Kappa values ranging from 0.13 to 0.39. This revealed significant variability in LLM performance when using their own grading criteria, with domain-specific strengths and weaknesses apparent across the different models.

Expert-LLM agreement (Cohen’s Kappa) – A1 attempt.

Legend: This table shows the Cohen’s Kappa values measuring agreement between each LLM (using internally generated criteria based on learned representations in the A1 attempt) and the average expert consensus. Values between 0.21–0.40 represent fair agreement, while values between 0.41–0.60 indicate moderate agreement and 0.61–0.80 substantial agreement.

When provided with expert grading criteria (A2), the agreement patterns shifted dramatically, as presented inTable 4. Key observations included best performances from Gemini on SAQ3 (anatomy, Kappa: 0.61) and GPT-4.1 on SAQ3 (anatomy, Kappa: 0.58), while the worst performances were noted for Claude on SAQ4 (embryology, Kappa: 0.03) and Claude on SAQ2 (physiology, Kappa: 0.05). GPT-4.1 remained the most consistent performer with Kappa values ranging from 0.10 to 0.58. These results demonstrated that providing expert criteria did not universally improve agreement and, in some cases, substantially decreased it.

Expert-LLM agreement (Cohen’s Kappa) – A2 attempt.

Legend: This table presents Cohen’s Kappa values measuring agreement between each LLM (using expert-provided criteria in the A2 attempt) and the average expert consensus. Values between 0.21–0.40 represent fair agreement, while values between 0.41–0.60 indicate moderate agreement and 0.61–0.80 substantial agreement.

The impact of providing expert criteria varied dramatically by model and question type, as shown inTable 5. Significant improvements were observed for DeepSeek on SAQ3 (anatomy, +0.50), Copilot on SAQ1 (histology, +0.22), and GPT-4.1 on SAQ3 (anatomy, +0.23). Conversely, significant declines were noted for Claude on SAQ3 (anatomy, −0.48), Copilot on SAQ2 (physiology, −0.34), and DeepSeek on SAQ2 (physiology, −0.31). Gemini emerged as the most consistent model, showing minimal changes across all SAQs when provided with expert criteria.

Legend: This table displays the absolute change in Cohen’s Kappa values when moving from LLMs using their own criteria (A1) to using expert-provided criteria (A2). Positive values represent improvement in agreement with experts, while negative values indicate decreased agreement. Cohen’s Kappa is a unitless measure ranging from −1 to + 1, where + 1 indicates perfect agreement, 0 indicates agreement equivalent to chance, and negative values indicate agreement worse than chance. Asterisks (*) denote statistically significant differences (p< 0.05) between A1 and A2 attempts.

The optimal model varied by question type, and providing expert criteria did not consistently improve performance, as detailed inTable 6. For SAQ1 (histology), GPT-4.1 performed best with its own criteria (Kappa: 0.39), while Claude achieved the highest agreement when using expert criteria (Kappa: 0.36). DeepSeek excelled in SAQ2 (physiology) in both conditions, though with better performance using its own criteria. SAQ3 (anatomy) saw the highest overall agreement levels, with Claude (A1) and Gemini (A2) both achieving substantial agreement with experts (Kappa: ~0.61). Despite having the highest expert agreement, SAQ4 (embryology) consistently showed the lowest LLM-expert agreement levels.

Legend: This table identifies the best-performing LLM for each question type under both conditions (A1: using their own criteria, A2: using expert-provided criteria) and shows their respective Cohen’s Kappa values.

Expert graders demonstrated remarkable consistency in identifying unsatisfactory responses, as shown inTable 7. The expert consensus identified 13 unsatisfactory responses (6.5%) in SAQ1, 8 (4.0%) in SAQ2, 4 (2.0%) in SAQ3, and 3 (1.5%) in SAQ4. LLMs, however, showed substantial variation in their strictness. In SAQ1 (histology), DeepSeek A1 was notably stricter (17.4%) than experts, while Copilot A1 gave no ‘0’ scores. For SAQ2 (physiology), Gemini was much stricter (18.9% in A1, 19.4% in A2) than experts. Most dramatically, in SAQ4 (embryology), DeepSeek A1 identified far more unsatisfactory responses (36.3%) than experts (1.5%).

Legend: This table presents the number and percentage of responses assigned a score of ‘0’ (unsatisfactory) by each expert grader and LLM across all four SAQs. Expert Consensus represents cases where at least two of the three experts assigned a ‘0’ score.

In identifying unsatisfactory responses, the agreement with expert consensus varied significantly across models and question types, as detailed inTable 8. DeepSeek showed the highest overall agreement (77.9%) using its own criteria (A1), while Copilot demonstrated the most improvement (+22.6%) when provided with expert criteria (A2). SAQ2 (physiology) showed near-perfect agreement (97.5–100%) between LLMs and experts, while SAQ1 (histology) and SAQ3 (anatomy) had the lowest agreement (29.2–30.8%).

Agreement with expert consensus on unsatisfactory responses.

Legend: This table shows the number and percentage of expert-identified unsatisfactory responses that were also marked as unsatisfactory by each LLM. The format ’3/13 (23.1%)’ indicates that the LLM correctly identified 3 out of 13 responses that experts marked as unsatisfactory, representing 23.1% agreement. Asterisks (*) denote statistically significant differences (p< 0.05) between A1 and A2 attempts.

Claude: Most balanced overall, but showed inconsistent alignment with experts across different subjects. Dramatic decline in performance on SAQ3 (−0.48) and SAQ4 (−0.20) when provided with expert criteria.

Copilot: Most improved with expert criteria (+22.6% for unsatisfactory responses), demonstrating strong adaptability to expert standards. Perfect detection of unsatisfactory responses in SAQ3 with expert criteria.

DeepSeek: Highest initial alignment with experts using its own criteria (77.9% for unsatisfactory responses) but less adaptable to expert rubrics. Exceptionally strict in SAQ4, marking 36.3% of responses as unsatisfactory compared to experts’ 1.5%.

Gemini: Most consistent performer with minimal changes between A1 and A2 attempts. Notably stricter in SAQ2, marking approximately 19% of responses as unsatisfactory compared to experts’ 4%.

GPT-4.1: Moderate overall performance with balanced results across different medical domains. Most consistent performer in the A1 attempt with the smallest range of Kappa values (0.13–0.39).

Each LLM demonstrated distinct grading patterns across the medical disciplines. Claude showed balanced overall performance but experienced a dramatic decline in agreement on SAQ3 (−0.48) and SAQ4 (−0.20) when provided with expert criteria. Copilot demonstrated the most improvement with expert criteria (+22.6% for unsatisfactory responses), achieving perfect detection of unsatisfactory responses in SAQ3 when using expert criteria. DeepSeek achieved the highest initial alignment with experts using its own criteria (77.9% for unsatisfactory responses) but was less adaptable to expert rubrics and was exceptionally strict in SAQ4, marking 36.3% of responses as unsatisfactory compared to experts’ 1.5%. Gemini maintained the most consistent performance with minimal changes between A1 and A2 attempts, though it was notably stricter in SAQ2, marking approximately 19% of responses as unsatisfactory compared to experts’ 4%. GPT-4.1 demonstrated moderate overall performance with balanced results across different medical domains and was the most consistent performer in the A1 attempt, with the smallest range of Kappa values (0.13–0.39).

Our analysis of 804 student responses across four medical SAQs revealed significant variations in how Large Language Models (LLMs) perform compared to expert human graders. The key finding was substantial inconsistency in LLM grading performance across different question types, medical domains, and grading criteria. While no LLM consistently matched expert agreement (average Kappa: 0.69, average ICC: 0.86) across all questions, certain models achieved substantial agreement in specific contexts, notably Claude on SAQ3 (Kappa: 0.61) and DeepSeek on SAQ2 (Kappa: 0.53). Surprisingly, providing expert-developed grading criteria did not consistently improve LLM performance, with some models showing significant decreases in accuracy when using expert rubrics.

Contrasting with our findings of variable LLM performance in SAQ grading, recent research in histology assessment has demonstrated high LLM accuracy, with all five major models (GPT-4.1, Claude 3.7 Sonnet, Gemini 2.0 Flash, Copilot, and DeepSeek R1) achieving a mean accuracy of 91.1% (SD 7.2) on USMLE-style multiple choice questions [21]. This study revealed minimal inter-system variability, with performance ranging from DeepSeek’s 90.3% to Gemini’s 92.0%, and no statistically significant differences between models (p> 0.05), suggesting that the assessment format (MCQ vs. SAQ grading) may be a critical determinant of LLM reliability in medical education contexts.

Most LLMs during SAQ grading demonstrated high precision for fully correct answers and low rates of false positives, but struggled with partially correct responses and showed an overall tendency toward conservative grading. These findings suggest that while LLMs have promising capabilities for supporting medical education assessment, their application requires domain-specific considerations and continued human oversight.

Expert agreement (Kappa 0.69, ICC 0.86) establishes a strong baseline against which to evaluate LLM performance. The substantial to almost perfect agreement among experts (Kappa ranging from 0.57 for SAQ2 to 0.87 for SAQ4) and good to excellent ICC values (ranging from 0.72 for SAQ1 to 0.92 for SAQ4) demonstrate the reliability of human expert grading in medical education. The dual measurement approach using both Cohen’s Kappa and ICC provides complementary insights into rater consistency, with Kappa focusing on categorical agreement and ICC capturing the overall pattern and magnitude of scoring. This robust measurement approach aligns with previous research on the importance of expert consensus in evaluating medical knowledge [5].

Notably, the divergence between Kappa and ICC values for certain questions (particularly SAQ2, with a moderate Kappa of 0.57 but an excellent ICC of 0.91) suggests that while experts may occasionally assign different categorical scores, their overall evaluation patterns remain highly consistent. This pattern indicates that even when specific scoring decisions differ, experts maintain reliable judgments about the relative quality of student responses, reinforcing the validity of the expert consensus as a benchmark for LLM evaluation.

A key finding is the considerable variability in LLM performance across different SAQs and between attempts. For instance, Claude performed exceptionally well on SAQ3 in the first attempt (Kappa 0.61) but showed a dramatic decrease in the second attempt (−0.48). Conversely, DeepSeek improved significantly on SAQ3 in the second attempt (+0.50) while declining on other questions. This variability suggests that LLM performance is highly context-dependent and can be influenced by factors such as question type, medical domain, and the specificity of grading criteria provided.

Our findings align with research showing that GPT-4 achieved high precision for fully correct answers but had challenges with partially correct ones [6]. In radiology assessment contexts, human experts have consistently demonstrated superior performance compared to LLMs, with residents achieving 63.33% and 57.5% accuracy compared to AI models (Bard: 44.17%, Bing: 53.33%, ChatGPT: 45%) on Fellowship of the Royal College of Radiologists examination-style questions [22]. Notably, this study revealed significant agreement among AI models (ICC = 0.628) but paradoxically poor agreement between human residents (Kappa = −0.376), suggesting that while AI models may converge on similar grading patterns, human expertise introduces valuable variability in assessment approaches. Similarly, Decision Trees in grading anatomical OSPEs have shown an average accuracy of 94.49%, suggesting that different AI approaches may be suitable for different assessment contexts [23].

A striking observation is that providing expert grading criteria (A2) did not consistently improve LLM performance compared to their own criteria (A1). This contradicts the intuitive expectation that explicit expert guidance would enhance grading accuracy. For instance, Claude’s agreement with experts on SAQ3 decreased dramatically from 0.61 to 0.13 when provided with expert criteria. This suggests that LLMs may develop their own internal models of medical knowledge assessment that sometimes diverge from expert-defined criteria.

This phenomenon may reflect challenges in how LLMs interpret and apply complex rubrics, particularly in specialized medical domains. Research has noted that LLMs might struggle with nuanced interpretation of domain-specific evaluation criteria [5,24]. The variability in how LLMs respond to expert criteria highlights the need for careful prompt engineering and potentially model fine-tuning for specific assessment contexts.

Our results reveal substantial differences in LLM performance across medical disciplines. For example, all models performed better on SAQ3 than on SAQ4, despite SAQ4 having the highest expert agreement (Kappa: 0.87, ICC: 0.92). This suggests that the difficulty of a question for experts does not necessarily correlate with its difficulty for LLMs, pointing to fundamental differences in how human experts and AI models process and evaluate medical knowledge.

The high ICC values across all questions indicate that experts maintained consistent scoring patterns even for technically challenging questions like SAQ2. In contrast, LLMs showed wide performance variability on the same questions, suggesting they may lack the nuanced domain understanding that allows human experts to maintain scoring consistency across different types of responses.

These domain-specific variations align with findings reporting varying LLM performance across different topics in medical embryology, biochemistry, and anatomy [8,11,25], and similar variations in neuroscience and physiology topics [13,26]. The inconsistent performance across domains indicates that medical educators should consider domain-specific validation before implementing LLM-based grading in any particular subject area [2].

The analysis of responses graded as unsatisfactory (score of ‘0’) reveals important patterns in how LLMs evaluate incorrect answers. The significant variation in strictness across LLMs is notable, with some models (e.g., DeepSeek in SAQ4: 36.3%) being considerably more strict than experts (1.5%), while others (e.g., Copilot in SAQ1: 0%) were more lenient than experts (6.5%).

The high agreement on unsatisfactory responses for SAQ2 (lung compliance) across all models (97.5–100%) contrasts sharply with the lower agreement for SAQ1 (tissue types) and SAQ3 (ureter constrictions). This suggests that some medical concepts may have more clearly defined boundaries of correctness that are consistently recognizable by both humans and AI, while others have more nuanced or subjective assessment criteria.

LLMs demonstrated distinct grading patterns, with models like DeepSeek consistently being stricter while others like Claude more closely aligned with expert consensus. Research has found that ChatGPT could pass short answer assessments in undergraduate medical education but outperformed only underperforming students [15]. This suggests potential utility for LLMs in identifying clearly incorrect responses, potentially reducing the grading burden for educators.

Our analysis of false positives (incorrectly assigning a high score) and false negatives (incorrectly assigning a low score) provides critical insights for potential implementation. The relatively low rate of false positives, particularly for GPT-4.1 and Claude, is encouraging for using LLM in medical education, where avoiding the passing of incorrect knowledge is paramount. However, the higher rate of false negatives across all LLMs indicates a tendency toward conservative grading that might unfairly penalize some students.

This pattern aligns with observations that GPT-4 was significantly more conservative in grading than human evaluators in undergraduate medical education [5]. In high-stakes medical assessment, the consequences of false positives (incorrectly passing students) may be considered more serious than false negatives (incorrectly failing students), suggesting that a conservative bias might be acceptable in certain contexts, particularly for preliminary screening.

LLMs provided various rationales for their grading decisions that offer insights into their evaluation processes. Common explanations for more lenient grading included recognizing partial understanding and valuing foundational knowledge over precise terminology. More strict grading was justified by identifying significant misunderstandings, particularly in treatment-related questions where precision is crucial [15,27].

These rationales suggest that LLMs apply a form of educational philosophy in their grading, sometimes prioritizing encouragement of student effort over strict adherence to technical precision. Research has noted that fine-tuned LLMs could adapt to different educational assessment philosophies [28]. The ability of LLMs to articulate grading rationales represents a potential advantage over traditional automated grading systems, offering transparency that could enhance trust and acceptance among educators and students.

- Model Selection by Domain: Different LLMs excelled in different domains, suggesting that optimal results might be achieved by selecting specific models for specific subject areas.

- Hybrid Grading Approaches: The high ICC values among experts (average 0.86) demonstrate the strong reliability standard that automated systems should aspire to match. Given the strong performance on identifying fully correct answers but inconsistencies with partially correct ones, a hybrid approach where LLMs provide initial screening followed by human review of borderline cases could maximize efficiency while maintaining assessment quality.

- Formative vs. Summative Assessment: The observed limitations suggest that LLMs may currently be more appropriate for formative assessment, where immediate feedback is valuable and stakes are lower, rather than high-stakes summative assessment where the high expert ICC values (0.72–0.92) set a reliability standard that current LLMs cannot consistently achieve across all domains.

- Prompt Engineering: The inconsistent impact of providing expert criteria suggests that careful design of prompts and grading instructions is crucial for effective LLM implementation.

These practical considerations align with recommendations emphasizing the need for domain-specific expertise in AI implementation for medical education [29,30], and guidelines for effective prompt engineering with ChatGPT in medical education [31,32].

This study has several limitations that should be acknowledged. First, our analysis focused on a limited set of four SAQs across specific medical disciplines, potentially limiting generalizability to other content areas. Second, we compared LLM performance to a consensus of three expert graders, which, while robust (as evidenced by high ICC values ranging from 0.72 to 0.92), may not capture the full range of acceptable grading approaches in medical education. Third, the study did not explore the potential impact of prompt engineering or model fine-tuning, which might significantly improve LLM performance. Fourth, while we followed established best practices for prompt development and conducted pilot testing to refine our prompts, we did not explore model-specific prompt optimization. Our deliberate choice to use standardized prompts across all LLMs ensured fair comparison of their capabilities, but model-specific optimization might yield improved individual performance. Future research should investigate how customized prompting strategies for each model could enhance LLM performance in medical assessment tasks.

Future research should extend this analysis to a broader range of medical disciplines and question types, investigate the impact of different prompting strategies and fine-tuning approaches, and explore student perceptions of LLM-generated feedback. Additionally, longitudinal studies examining how LLM-assisted grading affects learning outcomes would provide valuable insights into the educational impact of these technologies. Future studies could also examine alternative statistical approaches to measure LLM-expert agreement beyond Kappa and ICC, particularly methods that might better capture the nuanced differences in how humans and AI models evaluate partial understanding.

As LLM technology continues to evolve rapidly, the capabilities demonstrated in this study are likely to improve further. Models like GPT-4.1 and Claude, while showing promising performance in certain domains, still fall short of human expert consistency across all medical topics. However, the pace of advancement in AI suggests that the gap between human and AI performance in specialized domains like medical education may continue to narrow.

This study demonstrates that LLMs show promising but variable performance in grading medical SAQs, wOB designed the research. IMF collected the questions and anonymous responses. PG, OB, and VM graded the questions. IMF collected and anonymized expert grading data. VM did the statistical analysis. All authors were involved in interpreting data, drafting the article, and revising it critically. All have approved the submitted and final versions.ith significant differences across models, medical domains, and question types. While no single LLM consistently matched expert-level performance across all questions, several models demonstrated substantial agreement with experts in specific contexts, particularly in identifying fully correct or incorrect responses.

Our dual analysis approach using both Cohen’s Kappa and Intraclass Correlation Coefficient (ICC) provided complementary insights into grading reliability. Expert graders maintained substantial agreement (average Kappa: 0.69) and excellent consistency (average ICC: 0.86) across all question types, establishing a robust benchmark for evaluating LLM performance. The high ICC values (ranging from 0.72 to 0.92) demonstrate the consistent scoring patterns that human experts achieve even for technically complex questions, a level of reliability that current LLMs cannot yet match across all medical domains.

The finding that providing expert-developed grading criteria did not consistently improve LLM performance (and in some cases significantly decreased it) reveals a complex relationship between explicit rubrics and LLM grading capabilities. This suggests that the successful implementation of LLM-based grading systems will require careful consideration of prompt engineering, domain-specific calibration, and understanding each model’s inherent grading tendencies.

The findings suggest that LLMs may have a valuable role to play in medical education assessment, particularly in reducing grading burden through preliminary screening or in providing immediate formative feedback. However, their current limitations, including inconsistent performance across domains, variability in response to expert rubrics, and domain-specific grading biases, indicate that human oversight remains essential, especially in high-stakes assessment contexts where the reliability standards established by expert ICC values must be maintained.

As we navigate the integration of AI into medical education, this study underscores the importance of a thoughtful, domain-specific approach that leverages the strengths of both human expertise and artificial intelligence. The future of medical assessment likely lies not in replacing human judgment with AI, but in developing synergistic approaches that enhance efficiency, consistency, and educational value through the thoughtful integration of these powerful tools, with careful attention to the reliability standards that expert graders consistently achieve.

The author(s) reported there is no funding associated with the work featured in this article.

No potential conflict of interest was reported by the author(s).

The clinical trial number is not pertinent to this study as it does not involve medicinal products or therapeutic interventions.