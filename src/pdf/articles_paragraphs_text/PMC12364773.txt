Positron emission tomography (PET) is a valuable tool for cancer diagnosis but generally has a lower spatial resolution compared to computed tomography (CT) or magnetic resonance imaging (MRI). High-resolution PET scanners that use silicon photomultipliers and time-of-flight measurements are expensive. Therefore, cost-effective software-based super-resolution methods are required. This study proposes a novel approach for enhancing whole-body PET image resolution applying a 2.5-dimensional Super-Resolution Convolutional Neural Network (2.5D-SRCNN) combined with logarithmic transformation preprocessing. This method aims to improve image quality and maintain quantitative accuracy, particularly for standardized uptake value measurements, while addressing the challenges of providing a memory-efficient alternative to full three-dimensional processing and managing the wide dynamic range of tracer uptake in PET images. We analyzed data from 90 patients who underwent whole-body FDG-PET/CT examinations and reconstructed low-resolution slices with a voxel size of 4 × 4 × 4 mm and corresponding high-resolution (HR) slices with a voxel size of 2 × 2 × 2 mm. The proposed 2.5D-SRCNN model, based on the conventional 2D-SRCNN structure, incorporates information from adjacent slices to generate a high-resolution output. Logarithmic transformation of the voxel values was applied to manage the large dynamic range caused by physiological tracer accumulation in the bladder. Performance was assessed using the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). The quantitative accuracy of standardized uptake values (SUV) was validated using a phantom study.

The results demonstrated that the 2.5D-SRCNN with logarithmic transformation significantly outperformed the conventional 2D-SRCNN in terms of PSNR and SSIM (p< 0.0001). The proposed method also showed an improved depiction of small spheres in the phantom while maintaining the accuracy of the SUV.

Our proposed method for whole-body PET images using a super-resolution model with the 2.5D approach and logarithmic transformation may be effective in generating super-resolution images with a lower spatial error and better quantitative accuracy.

The online version contains supplementary material available at 10.1186/s40658-025-00791-y.

Positron emission tomography (PET) is a functional imaging modality used to visualize the distribution of radioactive tracers in the body. PET with [18F] fluorodeoxyglucose (FDG) is widely used for detecting, diagnosing, staging, and restaging malignant diseases. FDG-PET can image glucose metabolism in vivo and provide different information compared to computed tomography (CT) and magnetic resonance imaging (MRI) (i.e., anatomical information); however, PET has a relatively low spatial resolution. The spatial resolution of a conventional PET image is about 4 mm full width at half maximum (FWHM). Therefore, detecting tumors < 5 mm is difficult due to partial volume effects. Silicon photomultiplier (SiPM)-based PET scanners provide high-resolution PET images, which are expected to detect small tumors [1–5]. These clinical PET systems have also achieved precise time-of-flight measurements (< 400 ps FWHM in coincidence time resolutions). These devices, which improve image resolution compared to conventional PET scanners without increasing the radioactivity dosage, are expected to become more widely used in the future. However, these hardware-dependent devices are expensive and are currently available in only a limited number of facilities, with many old scanners still in use worldwide. Therefore, software-based approaches that improve image quality are expected to play a significant role.

Super-resolution (SR) has been used to improve the resolution of low-resolution (LR) images. The SR convolutional neural network (SRCNN) was first reported by Dong et al. at the European Conference on Computer Vision 2014 [6] and has since been actively studied. These technologies are expected to be applied in nuclear medicine imaging [7]. We previously reported the application of SR technology in PET imaging to reduce radiation exposure [8]. Several studies on super-resolution for PET have been conducted, such as combining low-resolution PET with MRI [9], the very deep SR (VDSR) model [10], and GAN-based self-supervised SR (SSSR) models [11–13].

However, several issues remain unresolved in previous studies. First, the quantifiability, particularly the accuracy of standardized uptake values (SUVs), has not yet been validated. Since SUVs are important for the assessment of treatment responses, developing artificial intelligence (AI) models that preserve SUVs is necessary. Second, using PET images as three-dimensional (3D) data in processing deep neural networks is desirable, as information from adjacent slices is used; however, 3D models are highly memory-consuming compared to 2D slice-by-slice learning. Third, a large dynamic range due to the physiological hyperaccumulation of FDG in the bladder may disturb effective machine learning. The bladder often exhibits extremely high SUVs, such as 30 or > 100. As a preprocessing step for an AI model, the SUV is usually normalized to a specific value. If normalized to SUV 0 to the maximum, the range of SUV 0 to 10, which is important for clinical situations, is degraded. Conversely, normalizing to SUV 0 to 20, for example, results in the loss of information for SUVs higher than 20.

Therefore, we propose a 2.5D-SRCNN, which is based on the SRCNN [6] and can learn 3D data with less memory consumption. For instance, in training a super-resolution model with an input slice thickness of 4 mm and an output slice thickness of 2 mm, the input data were nine LR images (the focus image and four slices before and after it), and the correct data consisted two HR images corresponding to one LR focus image. Additionally, we applied a preprocessing step involving the logarithmic transformation of the voxel values of the training images. We expected that logarithmic transformation would improve learning by decreasing the relative voxel value of the bladder while maintaining the quantitative nature of the voxel values of tissues important for diagnosis. The purpose of this study is to propose a super-resolution method for whole-body PET images using a model that reflects 3D information and preprocessing logarithmic transformation and to evaluate the error from the correct image and the accuracy of the SUV.

This retrospective observational study was conducted in accordance with the guidelines of the Declaration of Helsinki and approved by the Institutional Review Board (approval No. 020–0070). A PET/CT scanner (Vereos; Philips, Amsterdam, Netherlands) equipped with high-resolution SiPM-based detectors was used for image acquisition. The PET component of the Vereos PET/CT system consists of 18 detector modules; each detector module consists of an array of 40 × 32 lutetium-yttrium oxyorthosilicate (LYSO) crystals (4 × 4 × 19 mm each) that are individually coupled to SiPM detectors (1:1 coupling) [5]. Data from 90 patients who underwent whole-body FDG PET/CT examinations as part of routine clinical practice were analyzed. The inclusion criteria ensured that all cases included imaging from at least the neck to the thighs, with some cases also covering the brain and legs. Patients showing strong accumulation of external urinary catheters were excluded from the study. Details of patient characteristics and image acquisition protocols are provided in Table1.

The National Electrical Manufacturers Association (NEMA) phantom was used to evaluate the quantitative accuracy of the predicted images by measuring the SUVmax. Six spherical phantoms with internal diameters of 10 mm, 13 mm, 17 mm, 22 mm, 28 mm, and 37 mm were filled with FDG solution (Hot: Background = 8:1 and 4:1) and imaged with the same PET/CT scanner used for the human study. The phantom images (4:1) and analysis results are described in thesupplementary materials.

In this study, images from the same case at different resolutions were reconstructed separately from the list-mode data in order to perform supervised learning, which requires corresponding data. For patient images, a total of 21,940 low-resolution (LR) slices with voxel sizes of 4 × 4 × 4 mm and a 144 × 144 matrix, and 43,880 high-resolution (HR) slices with voxel sizes of 2 × 2 × 2 mm and 288 × 288 matrices were used in this study. Slices corresponding to a 4-mm thickness from the top and bottom of each case were excluded after reconstruction due to significant noise. The 3D iterative list-mode ordered-subset expectation maximization algorithm with time-of-flight information and point-spread function (PSF) resolution recovery was used under the following conditions: iterations, 3; subsets, 12 for LR slices and 17 for HR slices. All PET images were corrected for attenuation and scatter using CT images. The reconstruction parameters for LR and HR images were optimized to produce the best image quality for the clinical conditions at our hospital.

For the phantom images, the emission scan time was 600 s, with 3 iterations and 12 subsets for LR images and HR images, and no PSF resolution recovery was applied.

In this study, we used 2D-SRCNN as a control, which is a CNN-based model, as the control [6]. SRCNN takes a low-resolution image as input and converts it into a high-resolution image. The network parameters were optimized using the training data. The greatest advantage of SRCNN is that the model is simple; therefore, the computational cost is low. It is composed of only three convolutional layers, including an output layer. The first, second, and third layers support patch extraction, nonlinear mapping, and reconstruction, respectively. The detailed architecture is shown in Fig.1(A); 8,129 parameters comprised the constructed 2D-SRCNN.

PET data are tomographic images composed of multiple slices. PET has limited spatial resolution; therefore, the adjacent slices have information about the slice of interest. This makes it ideal to use the information from adjacent slices for SR. In addition, for 3D images, there is no one-to-one correspondence between the LR and HR images due to different slice thicknesses. Specifically, two 2-mm slices need to be generated from a single 4-mm slice. Therefore, we aimed to build an SR model using 3D data. Ideally, the input and output of the network should be 3D volume data. However, general 3D learning is unrealistic because it inputs and outputs 3D data from the entire body and requires hundreds of times more memory than 2D learning. Therefore, we extended the conventional 2D-SRCNN model to create a memory-friendly SRCNN model (2.5D-SRCNN) that uses multiple input slices, with the output image corresponding to one input slice. By using the minimum number of slices as input, this model can be trained using less memory than the general 3D learning model.

Figure1(B) illustrates our 2.5D-SRCNN, where the input tensor consisted of 288 × 288 × 9 pixels, and the number of output slices was set to two because one LR image corresponds to two HR images. This allows the model itself to play the role of inter-slice interpolation. Considering the increase in input data, we doubled the number of channels in the intermediate layers compared to the conventional SRCNN; 104,898 parameters comprised the constructed 2.5D-SRCNN. We used a Rectified Linear Unit (ReLU) activation layer in the 2D-SRCNN and 2.5D-SRCNN.

We constructed a 2D-Deep Denoising SRCNN (DDSRCNN) [14,15] as a control model in the SUV quantification evaluation of the proposed method under the conditions of the previous paper. DDSRCNN has a structure similar to U-Net and is used for super-resolution and denoising; 5,091,841 parameters comprised the constructed 2D- DDSRCNN.

In deep learning, a common preprocessing step is to divide the voxel value by the maximum value of all images and normalize it to a range between zero and one. However, the physiological accumulation of FDG in urine poses a problem for deep learning models that maintain quantification in whole-body PET images. The main excretion pathway of FDG is the urinary tract. In whole-body PET images, the voxel value of physiological accumulation in urine is very high, with SUVmax exceeding 100, whereas the SUVmax of tumors is often less than 10. This leads to a concern that the normalization of whole-body PET images by the maximum voxel value could result in an excessively large dynamic range, making it less efficient for SR in the 0–10 range. To minimize the effect of high accumulation of FDG in the bladder, we applied a logarithmic transformation as a preprocessing step.

Where x is the raw SUV image value (SUV), y is the logarithmically transformed SUV image value, and (x + 1) is applied to avoid a div/0 error in the log (0).

2D-SRCNN models (conventional models): Because the image matrix sizes of the LR and HR image data were different, the number of voxels of the LR image data was doubled in the 3D direction using the nearest neighbor interpolation method to match the HR image data sizes. Learning was performed using a single LR image as the input and a single HR image as the output.

2.5D-SRCNN models (proposed models): For the same reason, the number of voxels in each LR image was doubled in the two-dimensional direction using the nearest neighbor interpolation method to match the HR image sizes. Nine LR images consisting of the central slice and four slices above and below it were used as input, and two HR images corresponding to the central slice were used as the output for learning.

2D-DDSRCNN models (proposed models): Same as 2D-SRCNN, the number of voxels of the LR image data was doubled in the 3D direction using the nearest neighbor interpolation method to match the HR image data sizes. Learning was performed using a single LR image as the input and a single HR image as the output.

In addition, five types of SR models (2D-SRCNN, 2D-LOG-SRCNN, 2.5D-SRCNN, 2.5D-LOG-SRCNN, and 2D-DDSRCNN) were trained using the training data of 50 cases, the validation data of 20 cases and the test data of 20 cases (randomly assigned).

The 2D-SRCNN, 2.5D-SRCNN, and 2D-DDSRCNN were trained using PET images of the raw SUV. The voxel values were divided by the maximum voxel value of all the images and normalized to [0,1].

The 2D-LOG-SRCNN and 2.5D-LOG-SRCNN were trained using PET images after a logarithmic transformation. The voxel values were divided into six (determined experimentally) after the logarithmic transformation and normalized to [0,1].

Machine learning was performed for 300 epochs with SRCNN and 100 epochs with DDSRCNN using Adam as the optimizer. The mean squared error (MSE) was used as the loss function. All experiments were performed under the following conditions: OS, Windows 10 pro 64 bit; CPU, Intel Core i9-10900 K; Memory, 128GB; GPU, 1xNVIDIA GeForce RTX 2080 Ti 11GB; Framework, Keras 2.2.4, and TensorFlow 2.0.0; Language, Python 3.7.7.

Where N0is the number of voxels, MAX is the peak value of the signal, which was set to the maximum voxel value of each HR image in this study; MSE(x, y) is the simplest and most widely used full-reference quality metric, calculated by averaging the squared intensity differences of distorted and reference image voxels; µxand µyare the means of x and y; σxand σyare the standard deviation of x and y; σxyis the covariance of x and y; and L is the dynamic range of the voxel values. The value of PSNR(x, y) approaches infinity as MSE(x, y) approaches zero, and a small value of PSNR(x, y) implies large numerical differences between x and y. The SSIM ranges from 0 to 1, where 1 denotes a perfect match between two images. These indices were calculated for the LR image, and the SR images of each model were predicted using the LR image as the input and the HR images as the correct images. In the objective evaluation, 9,710 slices from the test data (20 cases) were used.

For the NEMA phantom images, SR images were predicted using LR images as input images using a CNN model trained with clinical data. The phantom images were not used for training. An in-house Python-based graphical user interface system was used for qualitative and quantitative assessments. As shown in Fig.3, the six spheres of the NEMA phantom were surrounded by a 3D bounding box to measure SUVmax for model comparison.

Out of the 20 clinical image test cases, the radiologist identified 13 as showing pathological uptake. For these 13 cases, a 3–5 cm spherical VOI was placed at the most prominent abnormal uptake site, and the SUVmax was measured. The relative absolute error (RAE) of the SUVmax was calculated using the HR image as the correct image.

For the 20 clinical image test cases, ROIs were placed on major organs and tissues, and SUVmean values were measured for HR, LR, and SR images. The following ROI settings were applied: a 15-mm circular ROI in the descending aorta for the blood pool; a 30-mm circular ROI in the right hepatic lobe of the liver; a 30-mm circular ROI in the bladder; a 30-mm circular ROI in the lumbar vertebral body for the bone marrow; a 30-mm circular ROI in the right gluteal muscles.

Where µVOIis the means of voxel value in VOI; σVOIis the standard deviation of voxel value in VOI. For the phantom images, five 30-mm circular VOIs were placed in the background of the slice that did not contain the tumor sphere. For the clinical images, one 30-mm circular VOI was placed in the right lobe of the liver for each of the 20 cases in the test data. These indices were calculated for the HR image, the LR image, and the SR images predicted by each model using the LR image as input.

A paired t-test was used for statistical analysis to test the differences between the models. Differences were considered statistically significant whenp< 0.05. When conducting multiple comparisons, the Bonferroni correction was applied. All statistical analyses were performed using JMP Pro version 17 (SAS Institute, Cary, NC, USA).

In the objective evaluation, Fig.4; Table2show the error of the SR images compared to the HR images as the ground truth. The PSNR was 36.73 ± 3.22 dB, and the SSIM was 0.967 ± 0.017 for LR images. All four SRCNN models we proposed outperformed the LR images (p< 0.0001).

The 2.5D-SRCNN (PSNR, 40.28 ± 4.20 dB; SSIM, 0.982 ± 0.017) was superior to 2D-SRCNN (PSNR, 38.87 ± 3.71 dB; SSIM, 0.978 ± 0.023) (p< 0.0001 for PSNR and SSIM, Fig.5a, b, c), and 2.5D-LOG-SRCNN (PSNR, 40.54 ± 4.10 dB; SSIM, 0.984 ± 0.011) was superior to 2D-LOG-SRCNN (PSNR, 39.07 ± 3.75 dB; SSIM, 0.981 ± 0.012) (p< 0.0001 for PSNR and SSIM, Fig.5d, e, f), indicating that the quality of the SR images generated using the 2.5D-SRCNN structure was significantly better compared to those generated using the conventional 2D-SRCNN.

2D-LOG-SRCNN was superior to 2D-SRCNN (p< 0.0001 for PSNR and SSIM, Fig.5g, h, i), and 2.5D-LOG-SRCNN was superior to 2.5D-SRCNN (p< 0.0001 for PSNR and SSIM, Fig.5j, k, l), indicating that the quality of the SR images generated using the models with logarithmic transformation was significantly better compared to those generated using the raw SUV models.

Among the four models, 2.5D-LOG-SRCNN achieved the best performance (p< 0.0001), indicating that the effects of the 2.5D and logarithmic transformations may be additive.

Figure6shows maximum intensity projection (MIP) images of the NEMA phantom. In the qualitative assessment, the positions and sizes of the spheres were reproduced in all AI-predicted images, with no false spheres observed in the background.

Figure7shows the results of the quantitative evaluation by plotting SUVmax against the diameter of the spheres. Visibility remained high even for small tumor spheres, and quantitative accuracy was maintained even for large spheres in all predicted images. A similar trend was observed for 2D-SRCNN and 2.5D-SRCNN: the 2.5D-SRCNN was closer to the 2-mm voxel image (correct image). Log transformation in 2D-LOG-SRCNN and 2.5D-LOG-SRCNN improved results for the 10 mm sphere, with 2.5D-LOG-SRCNN producing values for the 10 mm and 13 mm spheres closest to the 2-mm voxel image.

Figure8shows enlarged images of the tumor spheres. Although some differences existed in the internal textures, the true 2-mm voxel image was well reproduced. However, in the 4-mm voxel image (B), smaller spheres showed a degraded representation. AI-predicted images (C to G) compared to the input image (B), showed that all sphere sizes were better restored to resemble image A. In particular, image F reproduces the small spheres very well, closely matching image A. The profile curve across the center of the tumor sphere is shown in FigureS4.

Table3shows the results of evaluating the RAE of SUVmax at the VOI placed on the abnormal accumulation for each of the 13 models. Compared to HR images, the mean error for LR images was 20.0%, while the mean error for the model with the largest error was 10.7%.

A paired t-test using the Bonferroni-correctedp-value (0.05/15) as the threshold showed that all models significantly improved RAE compared to LR images, with no significant differences of RAE between models.

Table4shows the average SUVmean values measured across 20 cases, with ROIs placed on five locations. Using the HR images as the ground truth, the maximum deviation in the averaged SUVmean among the LR images and all five super-resolution models was within 2.4% for the blood pool, 2.6% for the liver, 2.0% for the bladder, 1.3% for the bone marrow, and 1.7% for the muscle.

Figure9shows the results of evaluating the CV of the NEMA phantom in the background for each model; the noise level assessed by CV was equal to or lower than the noise in the HR images for all models.

Figure10shows the results of evaluating the CV of the liver in the clinical images of each model; the noise level assessed by CV was equal to or lower than the noise in the HR images for all models.

In addition to the global analysis, a local analysis was conducted to identify specific regions or organs that showed improvements in image quality (Fig.11). As shown in the image obtained by subtracting the HR image from the predicted image; the three slices of the brain, lung, and liver showed the greatest improvement with 2.5D-LOG-SRCNN in high- and low-intensity regions. The subtracted image indicates that, in the tumor located near the apex of the heart, the 2.5D-LOG-SRCNN illustrated voxel values closest to the 2-mm voxel image, leading to improved quantification of the SUV.

In this study, we proposed a 2.5D-SRCNN with a logarithmic transformation for whole-body PET imaging. In the error evaluation, PSNR and SSIM were calculated for the correct and super-resolution images to evaluate the effect of the proposed model and logarithmic transformation on accuracy. As shown in Table3, PSNR and SSIM were significantly higher in 2.5D-SRCNN compared to 2D-SRCNN, indicating that the model structure of 2.5D-SRCNN effectively improved PSNR and SSIM. Table3also shows that PSNR and SSIM were significantly higher in 2D-LOG-SRCNN than in 2D-SRCNN, and in 2.5D-LOG-SRCNN than in 2.5D-SRCNN, indicating that pre-processing with logarithmic transformation effectively improved PSNR and SSIM.

For the quantitative evaluation, NEMA phantom images were acquired and used to verify the quantitative accuracy of each tumor sphere. As shown in Fig.6, the positions and sizes of the spheres were reproduced without any false spheres appearing in the background. As shown in Fig.8and FigureS3, in the two experiments with a different SUVmax and position, the SR images showed improved depictions of the 13 and 10 mm spheres compared to the LR images in both SR models. As shown in Fig.7, by comparing the SUV curves of 2D-SRCNN vs. 2D-LOG-SRCNN and 2.5D-SRCNN vs. 2.5D-LOG-SRCNN, the logarithmic transformation models recovered the SUVmax of the smallest (10 mm) sphere more accurately. One concern is that in the 17 mm sphere, models other than the 2.5D-LOG-SRCNN exhibited up to an 11% increase in SUVmax. This tendency was particularly evident in non-logarithmic transformation models (i.e.,2.5D-SRCNN, 2D-SRCNN and 2D-DDSRCNN). A likely explanation is that the learning process prioritized enhancing SUVs in smaller structures to compensate for partial volume effects, with a size threshold for SUV enhancement set at 17 mm. Additionally, this phenomenon may be attributed to edge artifacts introduced by the OSEM method. As the tumor sphere decreases in size, the diameter of the edge artifact also shrinks. However, at certain sizes, these artifacts may overlap, leading to a distinct delineation that could have influenced the AI’s predictions. In addition to the phantom-based evaluation described above, the analysis using clinical images revealed that SUVmean was preserved before and after the super-resolution processing, as shown in Table4. This supports the consistency of SUV values in clinically relevant structures. Overall, in the quantitative evaluation using the NEMA phantom, the proposed method enhanced the depiction of small tumors without producing a false representation in the output image, while maintaining the SUV of the phantom sphere.

As a comparison with other state-of-the-art methods, we constructed a 2D-DDSRCNN and compared its performance with the proposed method in terms of SUV quantification and noise evaluation by CV. As shown in Figs.7,8,9and10, in both analyses, 2.5D-LOG-SRCNN showed equal or better results than 2D-DDSRCNN, even though the number of parameters of 2.5D-LOG-SRCNN is 1/50 of 2D-DDSRCNN.

The proposed 2.5D-SRCNN was effective in reducing the error while maintaining quantitative accuracy, even for lightweight SR models. The ability of the logarithmic transformation to improve the learning of images containing extremely high voxel values is an important finding for future deep learning research on PET images.

In a previous study, Song et al. proposed an SR method for PET images by combining MRI structural information with PET images and spatial information and performed supervised learning with a model based on a VDSR CNN [10,18]. Although their study dealt with PET data in a slice-by-slice manner, they separated LR slices into multiple HR images using high-resolution MRI images. Georgescu et al. achieved SR in the 3D direction of CT images by performing single-image SR (SISR) twice in different axial directions [19]. Therefore, while SR in the axial direction using SISR has been performed in medical imaging, multi-image SR (MISR), where multiple slices are simultaneously input, has not been well studied.

The unique features of this study include the data input style of the MISR model, which inputs images before and after the focused LR image and outputs one HR image for each LR image. One of the key features of this study is the application of 2.5D to primitive models, which significantly improved performance while maintaining a small number of parameters. The 2.5D learning approach enhanced spatial accuracy and achieved better quantification in the super-resolution of phantom images with fewer parameters than the 2D-DDSRCNN. Another notable feature of this study is the use of logarithmic transformation for pixel values. This transformation effectively compressed the wide dynamic range of PET images without any loss of pixel information. Beyond improving spatial resolution, the logarithmic transformation also mitigated the recovery of the 10 mm sphere and reduced the overemphasis on the 17 mm sphere, particularly in the phantom evaluation. We verified the quantitative accuracy of SUV, which has not been performed in previous SR PET studies, and showed that our method improved quantification in SR images compared to LR images.

where Z is the number of images, Y is the image height, X is the image width, C is the number of channels, and P is the number of bytes per voxel. The actual memory consumption during training is defined as the number of input images multiplied by the number of channels in the training model. Using this formula, the data size when using the whole-body image of one case at a time, assuming that the number of images is 482, (the median of the 2-mm images of the case data in this study), is D3D= 482 × 288 × 288 × 1 × 4 = 159,916,032 (bytes). By contrast, the 2.5D-SRCNN model proposed in this study inputs nine slices at a time, such that D2.5D= 1 × 288 × 288 × 9 × 4 = 2,985,984 (bytes). Hence, D3D: D2.5D≈ 53.6: 1. This result shows only the size of the input data of the LR image and does not consider the amount of data in the input of the HR image or the computation in the intermediate layers of the model. This method reduces memory consumption without cutting or shrinking the training images. Reducing the memory consumption required to predict SR images is expected to reduce costs and facilitate widespread use by lowering the required specifications of the computational equipment. This method is useful because it can lead to the development of higher-resolution SR models with higher memory consumption.

High-resolution SiPM PET/CT systems have been reported to improve the detection of small lesions [1–4]. Therefore, a technology that generates high-resolution PET images using inexpensive software is useful because it can provide a wide range of clinical examinations with high diagnostic performance. Furthermore, once a system that can generate SR images from previously installed conventional PET scanners is developed, the quality of examinations will improve in many institutes, which will contribute to the early detection of tumors and other diseases. In addition, this technology is expected to play an active role in monitoring the treatment effects with high quantitative accuracy. The advantage of this technology is its ability to generate 2 mm resolution images from 4-mm voxel PET images with minimal additional cost, such as equipment replacement. Many PET scanners still in use—especially those manufactured more than 10 years ago—can only acquire images with voxel sizes of 4 mm. These older scanners are still widely used, particularly in developing countries or in facilities that aim to implement PET at low cost using second-hand equipment. In such contexts, our algorithm can be quite useful. However, in PET scanners that support 2 mm voxels, our algorithm does not directly improve image quality. However, there is a growing demand for techniques that can generate even higher resolution images, such as 1 mm resolution from 2 mm images. Given the low computational cost of this model, it is well-suited for high-resolution super-resolution tasks, where computational demands are typically substantial. Developing a higher resolution model could also provide benefits for PET systems that already achieve 2 mm resolution.

Although 2.5D-SRCNN with logarithmic transformation has shown promising outcomes, substantial room for further advancement remains. First, we validated the model using only SRCNN, the simplest SR model. In the future, we plan to apply 2.5D learning and logarithmic transformation to SR models with more complex structures, such as a SRGAN-based model [20], and vision transformers, such as swin transformer for image restoration [21], to verify their effectiveness. Currently, in 3D learning, training with complete 3D data is difficult due to memory limitations. Consequently, memory consumption was reduced by dividing the 3D data into smaller patches for training. However, this approach presents challenges, such as the loss of boundary information. Therefore, this study aimed to develop a learning method that does not require patch splitting. Second, we performed learning using image output from the same PET scanner as the LR and HR images and did not validate learning with images obtained from multiple scanners. Our proposed method requires corresponding high-resolution and low-resolution images. Therefore, for PET systems that only provide low-resolution images, we will use the predictions of a learned model that has been trained by other PET systems. Accordingly, it is crucial to verify whether the trained super-resolution model can be generalized to different PET systems and reconstruction conditions. Future studies will need to evaluate the effects of applying this model to images acquired from other PET scanners. Third, only FDG-PET images were used for learning. However, whether this method is effective for images from other tracers has not yet been verified. The proposed preprocessing method of applying logarithmic transformation to pixel values may be effective for training modality images that include extremely strong signals from normal tissues, such as prostate-specific membrane antigens, which accumulate significantly in the bladder. Finally, although phantoms were used in the current study, quantitative validation using clinical images and evaluation of the diagnostic performance by physicians will be needed in the future.

We proposed a 2.5D super-resolution method with logarithmic transformation for whole-body PET images, which may be effective for generating super-resolution images with better quantitative accuracy.

Below is the link to the electronic supplementary material.