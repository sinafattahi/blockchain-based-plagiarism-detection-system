This research explores the application of fatty acid ethyl esters (FAEEs) in the pharmaceutical industry due to their biodegradable, renewable nature and versatility as excipients or drug delivery agents. The research seeks to create predictive models utilizing various methods in machine learning to calculate the speed of sound in FAEEs under different temperature, pressure, molar mass, and elemental composition conditions. Laboratory data figures from earlier research were used to train the models. Among the models developed, CNN was recognized as the most accurate model for predicting the speed of sound. This conclusion was drawn from extensive statistical evaluations and visualization techniques. CNN achieved an R² value of 0.9996, with low average absolute relative error and mean squared error, outperforming other tested algorithms. The dataset, consisting of 371 experimental data points, was validated using the Leverage algorithm to ensure reliability. Further analysis showed that pressure is the most influential factor, followed by temperature, as confirmed by sensitivity and SHAP analyses. The proposed framework provides a reliable, cost-effective alternative to experimental methods for estimating sound speed in FAEEs under various physical conditions.

The online version contains supplementary material available at 10.1038/s41598-025-16095-1.

Fatty acid ethyl esters (FAEEs) have received growing attention in pharmaceutical industry due to their favorable physicochemical and biocompatible properties1. They play a crucial role in improving solubility, enhancing bioavailability, and enabling controlled drug delivery in pharmaceutical formulations2. Moreover, their biodegradable and non-toxic nature makes them suitable as carriers for active pharmaceutical ingredients (APIs), contributing to safer and more sustainable therapeutic systems3–5. FAEEs are also involved in the synthesis of bioactive compounds and pharmaceutical excipients, aligning with the increasing demand for eco-friendly and renewable solutions in pharmaceutical research and development6–9.

Several experimental studies have explored sound speed and related thermophysical properties in FAEEs and their derivatives under various temperature and pressure conditions. Ndiaye et al. measured speed sound in ethyl caprate and methyl caprate using the pulse-echo technique at pressures up to 210 MPa and temperatures between 283.15 and 403.15 K. They also conducted density measurements up to 100 MPa and derived isothermal and isentropic compressibilities10. Habrioux et al. applied a dual acoustic sensor pulse-echo method to study sound speed in ethyl laurate and methyl laurate at pressures up to 200 MPa and temperatures from 293.15 to 353.15 K, using the Newton-Laplace equation to evaluate density and compressibility11. Another study by Ndiaye et al.10focused on ethyl myristate, methyl palmitate, and methyl myristate, examining their sound speed and proposing an equation of state based on experimental data12. Tat et al. investigated sound speed and isentropic bulk modulus in biodiesel and its pure esters at temperatures between 20 and 100 °C and pressures up to 34.5 MPa. Their findings suggested that sound speed variations may impact fuel injection timing and NOx emissions13. Zhang et al.12used light scattering to measure thermal diffusivity and sound speed in ethyl myristate across a broad spectrum of temperatures (303.15–560.15 K) and pressures (up to 10 MPa), providing reliable empirical correlations for these properties14.

Despite the benefits of FAEEs, accurately predicting their speed of sound remains a significant challenge owing to the intricate interaction of elements such as temperature, pressure, molar mass, and elemental composition. The sound speed is a critical parameter influencing fuel performance, combustion efficiency, and transport properties, making accurate prediction essential for industrial applications. Experimental methods for measuring speed of sound are reliable but often resource-intensive and time-consuming, highlighting the need for efficient computational models for rapid and precise predictions. This study seeks to tackle this challenge through enhancing advanced machine learning algorithms, comprising Ensemble Learning, Random Forests, Ridge Regression, Decision Trees, AdaBoost, Linear Regression, KNN, CNN, SVM, and MLP-ANN models, to accurately predict the speed of sound for FAEEs. The study employs a refined dataset of 371 data points. The Leverage algorithm was used for anomaly detection to ensure data reliability, and a feature significance an examination was conducted to assess effect of every input variable on speed of sound of FAEEs. Various performance indicators and visualization techniques were employed to evaluate predictive algorithms accuracy. Furthermore, SHapley Additive exPlanations (SHAP) analysis was carried out to analyze influence of key features on speed of sound predictions, offering valuable insights into the roles of elemental composition, molar mass, temperature, and pressure. The overall methodology is depicted in Fig.1.

Process for identifying the highest-performing data-driven model.

DT is a widely used monitored learning algorithm for both classifying and predicting tasks. It recursively divides data based on feature values, creating a tree-shaped framework within which internal nodes symbolize decision criteria and leaves indicate final predictions This approach is valued due to its straightforwardness, creating it especially useful when understanding the decision process is important15,16.

Decision Trees is able to managing categorical and numerical data, require minimal preprocessing, and can model nonlinear relationships. However, they tend to overfit, particularly when overly deep. This can be mitigated through techniques such as pruning or by using methods that combine multiple models, such as Random Forest and Gradient Boosting. Despite not always attaining the maximum precision, Decision Trees remain popular in domains like medicine, finance, and marketing due to their transparency and ease of use17,18. Figure2demonstrates the composition of a typical Decision Tree model, emphasizing its simplicity and interpretability.

AdaBoost is a well-known ensemble learning method that integrates various weak learners to create a robust model predictive model. It works by iteratively modifying the weights of training samples according to the performance of previous learners, placing more emphasis on misclassified examples19. This approach improves both accuracy and robustness by utilizing the advantages of each model while reducing their limitations. AdaBoost is especially proficient at minimizing overfitting and handling complex classification tasks20,21. Figure3illustrates the general structure of an AdaBoost method, highlighting its ensemble-based nature and the integration of multiple weak learners.

RF is a widely used collection of learning techniques that constructs numerous decision trees on arbitrary choices of information and characteristics to enhance predictive performance. For categorization, it employs majority voting, whereas for regression, it calculates the average of all tree predictions. This method enhances precision and generalization while minimizing the likelihood of overfitting. Through utilizing the variety present in individual trees, Random Forest proficiently encompasses intricate patterns and delivers robust and reliable results across various tasks22,23.

Figure4illustrates overall structure of a Random Forest method, emphasizing its ensemble nature and the combination of numerous decision trees. This graphic depiction provides insights into workings of Random Forest, showcasing its capacity to generate accurate and robust forecasts made by capitalizing on the strengths of multiple individual learners. The ensemble approach enables Random Forest to successfully identify patterns and connections within the data, ultimately improving overall forecasting effectiveness24.

Ensemble learning is commonly used employed method in statistical analysis and machine learning that integrates various models to enhance the precision and reliability of predictions25,26. In this approach, several base learners (often of different types) are educated separately, and their personal forecasts are then combined to create a conclusive decision. This strategy leverages the diversity and complementary advantages of specific models to reduce generalization error and increase overall performance.

The most common ensemble strategies include hard voting and soft voting. In strict voting, every model casts a vote for a class label, and label that receives the option with the highest votes is selected as the ultimate forecast. In soft voting, predicted likelihoods for every category from all models are averaged (possibly weighted), and class that has the greatest average likelihood is chosen. Soft voting tends to be more successful when the separate models are properly adjusted.

Ensemble methods can also allocate varying weights to the forecasts of base learners, or embrace sequential strategies where the result of a single model influences the training of the next (e.g., in boosting methods)27,28. These techniques can significantly enhance predictive performance, particularly when handling intricate, chaotic, or elevated-dimensional data29,30.

Figure5depicts the typical architecture of an ensemble learning algorithm, highlighting its multi-model structure and collaborative prediction mechanism. This visual representation emphasizes the adaptive and modular nature of ensemble learning, showcasing its strength in tackling complex prediction problems by integrating diverse models31.

CNNs are a trained architecture within profound learning, originally developed for handling organized data like pictures. They utilize convolutional layers to extract localized designs or attributes similar contours and patterns from input data, rendering them particularly effective for tasks such as image categorization, object identification, and medical image analysis32.

Although CNNs are traditionally used for image data, recent studies have demonstrated their effectiveness in non-image domains such as tabular datasets, particularly when patterns or interactions among input features can be spatially structured. In this study, the tabular dataset containing temperature, pressure, molar mass, and elemental composition was reshaped into a 2D matrix format that mimics the spatial locality of image data. This allowed the convolutional filters to extract interaction patterns between features and learn more complex feature hierarchies compared to traditional fully connected layers33.

And ultimately improve the generalization performance in predicting speed of sound.

This approach reflects a growing trend in machine learning where CNNs are repurposed beyond images, especially in scientific and engineering applications with spatially interpretable feature sets. Figure6illustrates a simplified architecture of CNN model applied in this research34.

KNN method is a simple but effective monitored learning approach utilized for regression and classification jobs. KNN functions based on concept of resemblance, determining value or class of a point by examining the most common category or mean value of its k closest adjacent points in feature space, as illustrated in Fig.735,36. The k parameter in KNN represents nearest neighbors number, which is specified by the user.

Another challenge with KNN is its computational intensity for extensive datasets, since it necessitates determining distances among aim point and supplementary points in dataset. Despite these issues, KNN remains popular in various domains like recommendation systems, and medical diagnostics due to its simple approach and effectiveness in capturing local patterns within data37,38.

SVM is an effective machine learning technique utilized for categorization and estimation jobs. It is based on statistical learning principles and nonlinear optimization. SVM operates by locating an appropriate hyperplane that divides two or more classes of data. The key idea is to find the decision boundary which enhances the gap between data points from various classes, enhancing the model’s generalization ability. The central concept regarding this approach is to maximize distance between training data points and decision boundary (Margin Maximization), improving the model’s ability to generalize.

When data cannot be separated linearly, SVM employs kernel functions like Gaussian or polynomial kernels to convert data into a space of greater dimensions, facilitating better separation. A significant The advantage of SVM is its capacity to handle high-dimensional data, rendering it appropriate for uses such as image identification and natural language understanding39,40.

MLP is a popular ANN architecture commonly used in various machine learning applications. An MLP typically includes a sensory layer, multiple concealed layers, and a resulting layer, all composed of synthetic neurons41,42.

In the MLP-ANN algorithm, input data is initially submitted to the input layer, along with neurons within each hidden layer receive and process outputs from the previous layer, producing new outputs. This sequential data processing continues until the data reaches the output layer, delivering the final prediction or result43,44. Figure8depicts standard design of a MLP-ANN, demonstrating the motion of information through interconnected layers of neurons. This visual representation offers important perspectives on the workings of the MLP-ANN algorithm, emphasizing its ability to comprehend intricate connections and trends within the input data.

Ridge Regression is a regularization technique applied in linear regression to avoid overfitting, especially when there are more variables than data points or when multicollinearity exists. Opposed to ordinary least squares (OLS) regression, which minimizes sum of squared residuals, Ridge Regression modifies cost function by adding a penalty that relates to square of parameters’ magnitude. Lasso Regression eliminates several factors by setting them to zero, whereas Ridge Regression decreases the coefficients closer to zero without fully discarding them, making certain that all features remain part of the model but with reduced weights45,46.

Linear Regression is a fundamental machine algorithm for learning utilized to predict a dependent variable determined by one or more independent characteristics. It represents the connection using a linear equation optimized to minimize prediction error. The method is valued for its simplicity and interpretability but presumes a linear connection, which might not always hold in practice. It is also affected by outliers and multicollinearity. Despite these limitations, Linear Regression remains widely used due to its ease of implementation and broad applicability47,48.

The data for creating machine learning algorithms in this study was sourced from previous research that performed comprehensive experimental analyses on sound speed characteristics of multiple FAEEs under varied laboratory conditions. The dataset comprises 371 data points, encompassing essential input variables like temperature, pressure, molar mass, and elemental composition (oxygen, carbon, and hydrogen content), derived from studies conducted by various researchers10–14. The FAEEs studied include Ethyl caprate, Ethyl linoleate, Ethyl laurate, Ethyl myristate, and Ethyl stearate.

The dataset consists of speed of sound as the primary response variable, with temperature, pressure, molar mass, and elemental composition (oxygen, carbon, and hydrogen content) serving as the main input variables. To achieve a clearer comprehension of how these factors impact the speed of sound in FAEEs and to analyze their distribution, scatter matrix diagrams are shown in Fig.9. These graphics offer an in-depth viewpoint of the dataset, revealing patterns, relationships, and possible outliers. Analyzing these aspects is crucial for grasping the data’s underlying structure and ensuring the creation of a precise forecasting model.

Scatter matrix visualization illustrating connections among variables.

Furthermore, Fig.10exhibits frequency distribution and cumulative distribution for each individual variable, offering a detailed representation of their distributions. These visualizations contribute to a broader insight of the data, enabling a thorough analysis of the variables’ characteristics and their potential influence on sound speed in FAEEs. This knowledge aids in refining the predictive model and interpreting the significance of each input variable.

Representation of frequency and cumulative distribution for data.

K-fold cross-validation is an algorithm utilized to enhance machine learning accuracy algorithms through methodically employing complete dataset for algorithm evaluation over ‘K’ cycles. This method involves dividing dataset into ‘K’ equal parts (or folds), where each part serves as a confirmation set once while additional ‘K-1’ components are employed for training. The results from these ‘K’ validation cycles are subsequently merged into generate an evaluation, diminishing prejudice linked to arbitrary information division into validation and training sets49–51. Furthermore, this technique significantly lowers chances of overfitting. Figure11depicts k-fold cross-validation procedure. In this research, a 5-fold cross-validation technique was employed throughout training period of each machine learning algorithm to improve their predictions effectiveness.

Schematic depiction of k-fold cross-validation algorithm.

In given formulas, subscript ‘i’ denotes index number associated with a particular datapoint in the dataset. The terms ‘pred’ and ‘exp’ refer to predicted and laboratory data linked to every point. Furthermore, ‘N’ denotes the total points tally available in the dataset57.

In this study, the input variables used for developing predictive models include temperature, pressure, molar mass, and elemental composition (oxygen, carbon, and hydrogen content), with the speed of sound for FAEEs acting as the dependent variable. To guarantee a thorough evaluation of the models’ effectiveness, dataset occurs by chance divided into three distinct subsets: training (90% of data), validation, and testing (10% of data).

In this formula, parameters are expressed in this way: n represents the raw, non-normalized point, nmaxindicates the greatest worth in dataset, nminsignifies the lowest value in dataset, and nnormdenotes standardized point.

To better understand contribution of every input parameter to algorithm predictions, we employed SHAP, a consolidated structure for analyzing machine learning algorithms. SHAP is based on cooperative game theory and allocates every feature an importance value indicating its role in a particular prediction. This method allows for local and global understanding model’s actions, providing understanding into which factors have the greatest impact the output across the dataset. In this study, SHAP was utilized in the Random Forest algorithm to quantify impact of temperature, pressure, molar mass, and elemental composition on the predicted speed of sound.

The Leverage approach helps identify potential outliers points by comparing that values in relation to leverage threshold H⁺. Subsequently, Williams’ plot is used to demarcate distinct regions for reliable and dubious information, aiding in the identification of outlier points. Figure12exemplifies how leverage and suspicion thresholds establish reliable region, assisting data integrity assessment and highlighting possible impact of outliers on further evaluations.

Recognition of questionable solubility data through a proven outlier detection technique.

While most solubility measurements are deemed trustworthy, some points (highlighted in red) are indicated as potentially doubtful. This visual depiction assists in evaluating data quality and brings attention to potential influence of anomalous data points on future analysis methods. Importantly, to ensure the development of universally applicable techniques, each point is considered during algorithm development phase.

In above equation, symbol ‘j’ shows specific parameter input being analyzed. The relevance factor ranges from − 1 to + 1, where values nearer to + 1 suggest a more significant advantage correlation among the input and output parameters. Conversely, values close to -1 indicate a stronger inverse relationship. A detrimental relevance factor indicates an inverse association among variables, whereas a positive value implies a straightforward relationship60–63.

As demonstrated in Fig.13, correlation matrix illustrates connections among input parameters temperature, pressure, molar mass, and elemental composition (oxygen, carbon, and hydrogen content) and the output parameter (speed of sound for FAEEs). The correlations of molar mass and elemental composition with the speed of sound are relatively weak, ranging from − 0.39 to 0.42. The analysis shows that pressure has a significant positive correlation (0.84) with speed of sound, implying that an increase in pressure significantly enhances the speed of sound for FEEA. Temperature, on the other hand, shows a moderate negative correlation (-0.66) with the speed of sound, indicating that temperature changes have a noticeable inverse effect on sound speed for FEEA. These results emphasize the prevailing impact of pressure and temperature on sound speed, while contributions of molar mass and elemental composition remain relatively minor.

The computed relevance measure for every input parameter.

This section outlines procedure for optimizing hyperparameters various machine learning models. Figure14a demonstrates that a maximum depth of 15 minimizes MSE for the Decision Tree algorithm, while Fig.14b identifies 17 estimators as optimal for the AdaBoost algorithm. Figure14c illustrates that a maximum depth of 10 reduces the Mean Squared Error (MSE) for Random Forest model. For the K-Nearest Neighbors (KNN) method, Fig.14d shows that 3 neighbors yield the best results. Additionally, Fig.14e and f illustrate the loss function trends for Convolutional Neural Networks (versus epoch) and Support Vector Regression (versus C hyperparameter), respectively, highlighting their convergence to optimal performance. These findings emphasize the significance of hyperparameter tuning in achieving enhanced model accuracy and efficiency.

The optimal hyperparameter value for various machine learning methods.

Table1shows the performance metrics for an array of data-driven algorithms, like DT, RF, KNN, AdaBoost, CNN, Ensemble Learning, SVM, Ridge Regression, Linear Regression, and MLP-ANN. These measurements include MSE, R², and AARE%. Figure15provides a visual depiction of these criteria throughout the testing phase, enabling a more thorough assessment.

The values of the evaluation indices obtained for all developed algorithms concerning all segments.

R2, MSE, and AARE% for all models in this research (testing stage).

Based on the test results, the CNN method outperforms the other algorithms, exhibiting the lowest MSE (30.393282) and the highest R² (0.9995753), indicating its superior predictive accuracy. Additionally, it achieves the lowest AARE% (0.3274238), further confirming its robustness and reliability. In contrast, Linear Regression appears to be the least accurate in this study, yielding the highest MSE (1047.7514) and AARE% (1.9798891) values, along with a relatively low coefficient of determination (R² = 0.9853604).

The research utilizes different graphical evaluations to gauge the predictive accuracy of the trained methods. Preliminary assessments include generating cross charts for every proposed algorithm as illustrated in Fig.16. The CNN algorithm demonstrates high accuracy, evidenced by the tight grouping of data close to line with a unit slope and regression formulas tightly aligning with bisector line. Figure17further visualizes distribution of relative variations for every predictive algorithm. Data points in proximity to y = 0 line indicate increased forecasting precision. This analysis identifies the CNN model as the most proficient predictive tool among those assessed.

Crossplots comparing estimated and actual values for all data in different method.

Relative error percent for all segments (training, testing and validation) for all constructed algorithms in this study.

Figure18demonstrates SHAP values for input features (temperature, pressure, molar mass, and elemental composition) and their respective significance, as assessed by the Random Forest model, in relation to sound speed output. The graph organizes the variables founded on their average absolute SHAP scores, representing average effect of every parameter on algorithm’s predictions. Pressure stands out as the most impactful variable, with greatest SHAP score. This underscores its predominant function in establishing sound speed for FAEEs. Temperature is identified as second most significant factor, with a moderate SHAP score. This indicates that while temperature has a noticeable impact on the speed of sound for FAEEs, its effect is less pronounced compared to pressure. Elemental composition and molar mass appear to have the least impact among the variables considered, as reflected by their lower SHAP score. This examination underscores the comparative importance of each factor in influencing speed of sound for FAEEs, with pressure serving as the key element.

Figure19highlights SHAP assessment for a model forecasting the speed of sound for FAEEs, where molar mass, pressure, temperature, and elemental analysis are used as input variables. The graph demonstrates the effect of every input parameter on algorithm’s results, where positive SHAP values indicate a rise in speed of sound, and values below zero represent a reduction. Pressure has the biggest influence on speed of sound, as evidenced by wide range of SHAP values and temperature exhibits a moderate influence, with a reduced scope of SHAP values, indicating that although it influences speed of sound. This analysis highlights the relative importance of each variable, with pressure the primary influencing factor, succeeded by temperature, elemental composition and molar mass. These results provide valuable insights for guiding future research or real-world uses in speed of sound modeling.

This research developed a comprehensive data-driven approach to predict sound speed in FAEEs, leveraging molecular and physical parameters such as temperature, pressure, molar mass, and elemental composition as inputs. Multiple machine learning techniques were employed, with a focus on constructing reliable predictive models. The workflow involved careful assembly and validation of a dataset sourced from prior experimental studies, alongside the application of the Leverage method for outlier detection to ensure data integrity. Among the tested algorithms, convolutional neural networks (CNN) demonstrated superior predictive performance, achieving high accuracy and low error metrics. To enhance model interpretability, SHAP analysis was used to recognize crucial elements affecting forecasts, revealing temperature and pressure as dominant variables. By integrating domain-specific physical descriptors with rigorous model validation and explainability tools, this structure provides a precise and affordable substitute for conventional experimental procedures for estimating sound velocity in FAEEs across varied conditions. Future investigations are encouraged to extend this framework to a wide array of applications. Potential fields include aerospace engineering, where optimizing flight trajectories and planning adaptive maneuvers for satellite systems can benefit from such modeling64,65; Cutting-edge materials science research can utilize the approach to analyze phase transformations, solidification processes, and advanced ceramic composites66–69; In biomedical and pharmaceutical arenas, the method could enhance therapies based on stem cells and targeted drug delivery systems for inflammatory conditions70,71. Moreover, emerging areas such as wireless communication infrastructure and next-generation 5G antenna design stand to gain from this framework72. Civil engineering applications like acoustic insulation design, structural stress evaluation of cable domes, and ecological monitoring of marine environments are additional promising targets73. Adopting this versatile methodology across disciplines could accelerate knowledge transfer, minimize reliance on resource-intensive experiments, and enable swift, accurate predictive modeling of complex phenomena.

This research created sophisticated models based on data analysis using various machine learning algorithms, including DT, RF, KNN, AdaBoost, CNN, Ensemble Learning, SVM, Ridge Regression, Linear Regression and MLP-ANN, to predict sound speed for FAEEs. The algorithms utilized a comprehensive dataset of 371 points, incorporating pressure, temperature, molar composition, and elemental analysis as input variables. In addition to the typical factors of temperature and pressure, the study also integrates molar mass and elemental analysis as key variables. These additional factors allow for a deeper understanding of the chemical properties and behavior of fatty acid ethyl esters, offering a more accurate and cost-effective solution for predicting speed of sound for FAEEs. The dataset was sourced from experimental studies and provides a comprehensive representation of FAEE characteristics under varying conditions. The analysis of correlation shows weak relationships between the speed of sound and input variables such as molar mass and elemental composition (O, C, H). In contrast, pressure exhibits a strong positive correlation (0.84), while temperature shows a moderate negative correlation (-0.66) with the speed of sound. These results indicate that pressure has a significant enhancing effect, whereas temperature has a noticeable inverse impact on the speed of sound for FAEEs. Among the machine learning models evaluated, the CNN algorithm consistently outperformed others in terms of prediction accuracy. Although pressure and temperature were identified as the dominant factors influencing the speed of sound, the inclusion of molar mass and elemental composition despite their relatively weaker correlations still contributed to the overall model performance. The sensitivity and SHAP analyses confirmed that all input variables played a role, with pressure and temperature having the strongest impact. By utilizing these advanced machine learning techniques, this study provides an efficient, cost-effective tool for accurately predicting FAEEs speed of sound, which can be used as an alternative to expensive and time-consuming laboratory experiments. The tool developed here contributes to the optimization of industrial processes involving FAEE, offering a practical solution for predicting their physical properties. These results illustrate possibility of machine learning models particularly CNNs for accurately predicting sound speed in FAEE under varying conditions. The proposed framework provides a quick and economical substitute for experimental methods, which is highly valuable for industrial applications such as biodiesel quality assessment and pharmaceutical formulation. Given the critical role of acoustic properties in process design and control, the model can support real-time decision-making in manufacturing environments. Future research could explore extending the model to broader classes of esters, incorporating additional physicochemical features, and deploying the trained models into simulation or monitoring tools for industrial use.

Below is the link to the electronic supplementary material.