In recent years, semantic communication based on deep joint source-channel coding (DEEPJSCC) has been demonstrated and widely investigated. However, existing DEEPJSCC schemes suffer from low efficiency in mining latent semantic representations, as well as large model size, high computational complexity, and redundant parameters. To address these issues, we meticulously establish a lightweight DEEPJSCC framework for wireless image semantic transmission, termed STARJSCC. The proposed method achieves flexible wireless image transmission by introducing an improved channel state adaptive module (CSA Mod) to adapt to different channel conditions, combined with a decoupled static semantic compression (SC) mask to control different transmission rates. Experimental results show that the STARJSCC framework outperforms other baseline schemes in terms of performance and adaptability across various transmission rates and signal-to-noise ratio (SNR) levels, achieving up to 2.73 dB improvement on high-resolution test set. Moreover, this solution significantly reduces model parameters, computational complexity, and storage overhead, providing a potential solution for high-quality wireless image transmission in resource-constrained scenarios.

With the iterative innovation of information technology and the disruptive evolution of computational paradigms, the deep integration of communication technology and artificial intelligence (AI) is propelling human society into a new era of intelligent interconnection of all things1–4. In this process, the inevitable strain on bandwidth resources and explosive growth of data transmission present fundamental challenges to traditional transmission frameworks based on Shannon’s information theory5. Semantic communication, an emerging communication paradigm centered on information meaning, aims not merely to ensure accurate transmission of data symbols, but to place greater emphasis on the receiving end’s comprehension and effective utilization of informational intent and semantics6,7. This concept is similar to technologies such as zero-knowledge proof and homomorphic encryption in blockchain8, which attempt to strip away the surface representation of data and directly operate on or convey the underlying logic or meaning. Semantic communication transcends the conventional focus on “signal fidelity” in traditional communication systems, shifting towards “semantic fidelity” as its primary objective, thereby enhancing communication efficiency and intelligent capabilities.

In traditional communication systems, source coding and channel coding are designed as two independent functional modules. For instance, in conventional wireless image transmission systems, the transmission process is typically divided into two primary stages: image compression (employing standards like JPEG9, JPEG200010and BPG11) and data transmission (utilizing error-correcting codes such as LDPC12, Turbo codes13, and Polar codes14). Such a decoupled approach facilitates the flexible design, development, and maintenance of communication systems through modular optimization. However, the separated source-channel coding (SSCC) paradigm inherently prevents synergistic operation to achieve optimal overall communication capacity from the perspective of system optimality. Moreover, under low signal-to-noise ratio (SNR) conditions where channel decoding fails to ensure zero bit error rate (BER), this architecture is prone to the “cliff effect” – a phenomenon characterized by abrupt performance degradation beyond critical SNR thresholds. Consequently, in recent years, as semantic communication technologies have demonstrated their potential in improving wireless network performance, neural/deep learning-based end-to-end optimized joint source-channel coding (JSCC) for data transmission has emerged as an active research domain within semantic communications. This approach demonstrates consistent superiority over the SSCC paradigm across various tasks including text15, speech16, and image processing17–26, particularly in scenarios requiring semantic-aware transmission robustness.

As one of the pioneering works in this domain, a CNN-based deep JSCC scheme (DEEPJSCC) for wireless image transmission was first proposed in Ref17.. By constructing a CNN-parameterized encoder-decoder architecture and adopting an end-to-end jointly optimized training strategy, this approach transcended the theoretical limitations of SSCC designs in conventional systems, demonstrating superior performance over SSCC schemes in both Gaussian and Rayleigh fading channels. Building upon this foundation, Ref18. expanded the research boundaries of deep joint coding by innovatively proposing a feedback-enabled semantic-aware image transmission system (DEEPJSCC-f). Through the introduction of a dual-mode channel feedback mechanism (incorporating ideal noiseless feedback and practical noisy feedback), the system dynamically adjusts encoder-side semantic feature extraction strategies to strengthen receiver-side image quality, though it should be noted that feedback does not theoretically increase the capacity of memoryless communication channels. To mitigate the performance degradation caused by channel SNR mismatch in DEEPJSCC, Xu et al.19proposed attention-enhanced JSCC (ADJSCC), which employed attention modules to recalibrate channel characteristics, enabling dynamic adjustment of source coding compression ratios and channel coding rates according to varying SNR conditions. Subsequently, Yuan et al.20refined the ADJSCC methodology into channel-blind JSCC (CBJSCC), achieving superior performance across different SNR levels without requiring prior channel state information. The work in Ref21. optimized wireless image transmission efficiency and quality through dynamic responses to channel conditions and image content, realizing adaptive rate control in deep learning-based JSCC models. However, these adaptive schemes primarily focus on either rate control or SNR adaptation, but not both concurrently. To address this limitation, a flexible dual-adaptive JSCC scheme (DEEPJSCC-V) was proposed in Ref22., integrating ADJSCC methodologies with adaptive masking mechanisms. This hybrid architecture enables transmission scheme adjustments based on both SNR variations and channel bandwidth ratios (CBR), demonstrating enhanced robustness albeit at the cost of marginal performance degradation compared to single-parameter adaptation approaches. On the other hand, JSCC frameworks based on Transformer architectures23–25and Mamba state-space models26have recently emerged as a prominent research direction due to their exceptional capability in capturing long-term dependencies and semantic representation capacities. However, these methods are excluded from our performance discussion due to their substantially larger model sizes (quantitative comparisons are provided in subsequent sections) compared to CNN-based solutions, which makes them unfavorable for deployment on edge devices.

The aforementioned systems have demonstrated the immense potential of deep learning technologies in semantic communications. However, in DEEPJSCC and its variants17–22, the inherent limitations of conventional convolutions for feature extraction significantly compromise semantic information extraction efficiency, communication resource utilization, and system robustness. Specifically, the quadratic growth of parameters and computational complexity with increasing input channels and kernel dimensions leads to inefficient feature extraction. Furthermore, the lack of explicit model size consideration during design restricts their applicability in resource-constrained semantic communication scenarios (e.g., mobile terminals, IoT devices, edge nodes), where computational and memory budgets are strictly limited. Regarding channel state adaptation, the conventional squeeze-and-excitation (SE) channel attention mechanism27primarily focuses on global information extraction while inadequately exploiting the value of local patterns. Consequently, in semantic communications, designing a channel attention mechanism capable of effectively fusing global and local information without significantly increasing computational complexity remains an unresolved yet critically important research challenge. Such a mechanism demands efficient operation while enabling rational weight allocation to achieve comprehensive yet precise information capture and utilization, particularly crucial for semantic-aware systems requiring balanced performance between feature granularity and processing efficiency.

A encoder-decoder architecture based on star-operation modulation network is designed. Considering the challenges associated with model size and complexity, the proposed scheme retains a convolution-based design approach. Distinctively, we develop a novel JSCC backbone network by integrating the star operation for feature fusion and leveraging the strengths of depthwise convolutions. This architecture is anticipated to enhance the model’s semantic feature representation capacity and transmission performance while significantly reducing model parameters and computational complexity.

A channel state adaptive module (CSA Mod) based on dynamic fine-grained attention mechanism is proposed. To enhance transmission quality and reconstruction fidelity by adapting to real-time channel conditions, we propose a critical plug-in module in STARJSCC, termed CSA Mod. This module refines the SENet architecture by introducing interactions between global and local features while incorporating SNR information as a guidance factor, thereby achieving channel state adaptation.

As a fundamental paradigm in semantic communication systems, the DEEPJSCC framework achieves semantic-level joint source-channel modeling through the synergistic optimization of an end-to-end neural encoder-decoder. The input image is mapped by the joint source-channel encoder into a form suitable for transmission over wireless channels. After undergoing specific CBR and channel conditions, the receiver aims to reconstruct the original semantic information via the decoder, minimizing the semantic discrepancy between the reconstructed and input images. This approach emphasizes semantic fidelity while balancing communication efficiency and robustness.

whereεis the SNR andγis the CBR of the input data, respectively. Before transmission through the wireless channel, the real and imaginary parts of the masked CSCyare extracted and concatenated to form the input signal. After channel processing, the output is reconstructed by concatenating the real and imaginary parts into a single real-valued vector, which serves as the input for the subsequent stage.

Overview of STARJSCC system model for wireless image transmission.

where·denotes dot product operation. The compressed semantic feature dimension is given byk=γ×N. Whenρi=1, thei-th semantic symbol is selected for transmission; whereasρi=0indicates that thei-th symbol is discarded as non-informative. After that, the firstkelements of CSCyare selected by mask tensorρ, generating the masked semantic informationy′.

where k is the number of semantic symbols to be transmitted, andy′∗represents the complex conjugate ofy′.

wherenis zero-mean complex Gaussian noise with varianceσ2, i.e.,n∼CN0,σ2.

wheredx,x^denotes the MSE between the input image and the reconstructed image, andNrepresents the dimensionality of the imagesxiandx^i.

whereθ∗andφ∗denote the optimal encoder and decoder parameters, respectively. During the training of a deep neural network, the network parameters are progressively refined until the loss value stabilizes and ceases to decrease significantly, at which point the neural network attains a convergent state.

Figure2delineates the overall architecture of the proposed STARJSCC for wireless image transmission. Within the joint encoder-decoder of this architecture, the Star Blocks and CSA Mod serve as the pivotal components driving its functionality.

The overall architecture design of our STARJSCC for wireless image transmission.

The encoder takes an RGB imagex∈RH×W×3as input. Initially, the image is processed through a convolutional (Conv) layer withkernel_size=9andstride=2to extract hierarchical features and downsample. This operation reduces the spatial resolution of the input toH2×W2×C1, whereC1denotes the number of output channels after the Conv layer. Subsequently, the semantic features of input image are further learned throughn1successive Star Blocks, during which the dimensionality of it remains unchanged. Finally, the CSA Mod integrates the learned semantic features with the channel state via attention mechanism, adaptively recalibrating the weight distribution to achieve channel state adaptation. The aforementioned steps are collectively referred to as Stage 1. The encoding process comprises three stages, where the operations of Stages 2 and Stage 3 are similar to Stage 1, except that the Conv layer is replaced with a Down Sample layer. At each stage, the spatial resolution of the input is progressively reduced to lower computational complexity and facilitate the extraction of higher-level semantic features.

Following three stages of deep semantic feature extraction and learning, the CSCyof input image is obtained. Prior to transmission over the wireless channel, the SC masking operation and power normalization procedure described in the preceding subsection are applied toyto achieve transmission rate control. Finally, the received semantic informationy~′, transmitted over the wireless channel, is subjected to zero padding to obtain the decoder’s inputy^.

The decoder adopts a symmetrical design to the encoder, with the objective of analyzing and learning the received semantic information. Through a series of feature processing and reconstruction operations, it ultimately outputs a reconstructed imagex^with dimensionsH×W×3, which should closely approximate the original input imagex. Corresponding to the Down Sampling layers in the encoder, the decoder employs Up Sampling layers to progressively restore the spatial resolution of the image. Ultimately, a transposed convolutional (Trans Conv) layer transforms the semantic features into the reconstructed image.

In this paper, we adopt Star Blocks as the primary semantic feature extraction module. As illustrated in Fig.3a, each Star Block consists of two depthwise convolution (DW-Conv) layers and three fully connected (FC) layers. For normalization, we employ generalized divisive normalization (GDN)29, which is better suited for image reconstruction tasks. Regarding the activation function, the parametric rectified linear unit (PReLU)30, which is commonly adopted in the DEEPJSCC framework and its variants, is considered. The two branches within the Star Block leverage element-wise multiplication (i.e., star operation) to fuse semantic features. This design not only enhances the efficiency of the model in extracting image semantic features but also strengthens its representational capacity for latent semantic features. This is because the star operation enables high-dimensional, nonlinear semantic feature mapping while circumventing the limitations of traditional approaches, which typically require a substantial increase in network width or computational overhead28. Additionally, the Down Sample modules used in the Stage 2 and Stage 3 of the encoder, as well as the corresponding Up Sample modules in the decoder, are illustrated in Fig.3b and c, respectively. The Down Sample module consists of a convolutional layer withkernel_size=5andstride=2, followed by a GDN layer. The Up Sampling module comprises a transposed convolutional layer withkernel_size=5andstride=2, followed by an IGDN (Inverse GDN) layer.

(a) A fundamental Star Block. (b) Down Sample module used in the encoder. (c) Up Sample module used in the decoder.

In summary, the proposed model employs a symmetrical encoder-decoder architecture that integrates Conv layer, Trans Conv layer, Star Blocks, CSA Mod, Down Sample and Up Sample operations. This design enables effective extraction, encoding, and decoding of semantic features during wireless channel transmission, thereby achieving efficient image communication and high-fidelity reconstruction.

Constructing a model capable of adapting to diverse channel environments without requiring fine-tuning to achieve efficient image semantic transmission remains a significant challenge in the domain of semantic communication. To optimize transmission quality and reconstruction fidelity, a critical plug-in module is proposed in our work, namely CSA Mod. By leveraging accurate modeling and prediction of input SNR, the CSA Mod dynamically adjusts its parameters and configurations in real time. This optimizes its adaptation to varying channel conditions, ensuring efficient and stable semantic transmission.

In ADJSCC19and DEEPJSCC-V22, the traditional SE channel attention mechanism is employed to achieve SNR adaptation. However, it relies on FC layers to extract global features while lacking effective interaction and fusion with local information, resulting in suboptimal weights allocation for features critical to semantic reconstruction. Inspired by Ref31., we introduce the CSA Mod, which is designed based on dynamic fine-grained attention. This module efficiently integrates global and local features and optimizes weight assignment to extract more discriminative image features, thereby providing precise feature representations for image semantic reconstruction. The design is illustrated in Fig.4. The CSA Mod leverages a correlation matrix to capture interdependencies between global and local semantic information to facilitating their interaction and enabling more effective allocation of feature weights. Additionally, by incorporating channel SNR as a reference factor for attention weight updates through an auxiliary network, the model learns diverse channel states, which in turn improves the robustness of the entire semantic communication system.

At the macro level, the system model takes dataset samples as input, with the final outputs being the optimized model parametersθ∗andφ∗, enabling the model to autonomously learn the joint source-channel encoding process. Specifically, the model parametersθandφare initialized, and the dataset is loaded. Iterative training is then conducted across multiple epochs.

During each training batch, the input data is firstly partitioned intoXb=[x1,x2,x3,⋯,xb]. After that, the CBRγand the SNRϵare randomly generated from the range [1/20, 1/4] and [0, 25] dB, respectively. This stochastic training strategy ensures the model adapts to varying channel states and different CBR values through decoupled attention mechanism and static SC masking scheme. The encoderEθencodesXbusingγandϵto get CSCy=[y1,y2,y3,⋯,yb], and then the masked semantic informationy′is generated by Eq. (2) and Eq. (3). Finally, the decoderDφdecodes the noise-corrupted semantic informationy^=[y^1,y^2,y^3,⋯,y^b], which is transmitted over the wireless channel, into the reconstructed dataX^=[x^1,x^2,x^3,⋯,x^b].

Prior to concluding each training batch, the MSE lossLi=1Nxi-x^i2is computed for each sample, and the epoch-averaged lossL=1b∑ibLiis derived. The gradient information fromLis utilized to update the model parametersθandφvia backpropagation. In the end, the complete training procedure is summarized in Algorithm 1.

In this section, we first provide a comprehensive description of the simulation experimental configuration. Subsequently, we present the results of the simulation experiments, which aim to validate the effectiveness and robustness of our STARJSCC model in executing transmission tasks under diverse channel conditions. Furthermore, ablation study is conducted to investigate the impact of different block designs on the experimental outcomes. Finally, a comparative analysis is carried out between the ADJSCC, DEEPJSCC-V, and our STARJSCC models, focusing on critical metrics such as model parameters and storage requirements.

In our simulation experiments, two datasets with distinct resolutions are considered. For the low-resolution images, the CIFAR10 dataset32is employed, which comprises 32×32-pixel color images spanning 10 different categories with a total of 60,000 images. These images are partitioned into 50,000 training images and 10,000 test images. Since the focus of our study is on communication tasks, category labels are usually ignored during the experiments. The training set is used for model optimization, while the test set serve to evaluate model performance. For high-resolution images, the DIV2K dataset33is adopted, containing 900 images, each exceeding 2000×1000 pixels. As for the testing phase, the Kodak dataset34which includes 24 color images of 768×512 pixels and CLIC2020 testset35with approximate 2 K resolution images are selected to assess the model’s performance on high-resolution images. This dataset selection strategy ensures a comprehensive evaluation of the proposed model’s performance and robustness across varying image resolutions in transmission tasks.

To comprehensively evaluate the end-to-end semantic transmission performance of the proposed STARJSCC model against comparable methods, we employed two well-established evaluation metrics: the pixel-level measurement, peak signal-to-noise ratio (PSNR); and the perception-oriented assessment, structural similarity index (SSIM)36.

whereMSE=d(x,x^)represents the mean squared error between the input imagexand the reconstructed imagex^, andMAXdenotes the maximum possible pixel value. All experiments were conducted on 24-bit-depth RGB images, where each color channel (red, green, blue) is allocated 8 bits. Consequently,MAX=28-1=255.

whereμxandμx^represent the mean values of the original imagexand the reconstructed imagex^, respectively.σx2andσx^2denote the variances of the original and reconstructed images, quantifying their intensity distributions. The constantsc1andc2are stabilization terms introduced to prevent division by near-zero values, ensuring numerical stability during computation.

The Adam optimizer37is employed to update model parameters, where the weight decay and learning rate are set to5×10-4. During the training process, a StepLR scheduler is employed to dynamically adjust the learning rate, thereby mitigating the risk of convergence to local optima. The scheduler configuration utilize a step size of 100 epochs and a multiplicative factor of 0.5. This implementation reduces the learning rate by 50% at every 100-epoch interval, with the maximum training duration set to 400 epochs. After each epoch, validation is performed with gradient updates disabled to evaluate the model’s performance on the validation set.

For training on the CIFAR10 dataset, the batch size is set to 64. For the DIV2K dataset, due to its higher demands on GPU memory and computational resources, the batch size is reduced to 4. Additionally, these images are resized to 256×256 pixels during training to facilitate model optimization. This configuration aims to balance training efficiency and model performance, ensuring optimal training outcomes across datasets of varying resolutions. To maintain architectural consistency, both high and low resolution images are processed using a three-stage STARJSCC scheme with parameters[n1,n2,n3]=[2],[3],[6]. All experiments were conducted on a Linux system utilizing the PyTorch framework and a single NVIDIA RTX 3060 GPU.

The proposed STARJSCC scheme is compared with the ADJSCC and DEEPJSCC-V methods. To mitigate the impact of stochastic channel noise on experimental results during performance evaluation, 10 transmission trials are conducted for each image, and the average PSNR and SSIM values are computed. This approach not only facilitates the acquisition of stable performance metrics but also ensures the reliability and comparability of the experimental outcomes.

The proposed STARJSCC model is comprehensively evaluated under AWGN channel conditions, and its performance is rigorously analyzed through comparative experiments across varying SNR levels. Figure5presents a performance comparison of the CIFAR10 dataset under AWGN channel conditions across varying SNR levels. Specifically, Fig.5a illustrates the PSNR performance of the proposed STARJSCC model against ADJSCC and DEEPJSCC-V at CBR = 1/12 and CBR = 1/6. It is evident that the STARJSCC model outperforms DEEPJSCC-V under both CBR settings. Compared to ADJSCC, the proposed model demonstrates comparable or superior adaptability to channel states. Notably, although STARJSCC does not surpass ADJSCC at low SNR levels, it achieves significant reductions in model complexity (detailed analysis is provided later). Furthermore, STARJSCC can adapt to different CBR values in a single model, a capability absent in ADJSCC. Figure5b depicts the SSIM performance comparison under the same conditions. The SSIM results exhibit a trend consistent with Figure5a, the better channel conditions, the more obvious performance advantage of STARJSCC becomes.

(a) PSNR performance curves versus the SNR over AWGN channel. (b) SSIM performance curves versus the SNR over AWGN channel. Where the CBR = 1/12 and 1/6 for CIFAR10 dataset.

Figure6illustrates the performance comparison of the CIFAR10 test set under AWGN channel conditions across varying CBR. The proposed STARJSCC and DEEPJSCC-V models are evaluated and compared at SNR levels of 1 dB, 4 dB, and 10 dB to assess their adaptability and robustness to different CBR conditions. The results demonstrate that the proposed model significantly outperforms DEEPJSCC-V in terms of both PSNR and SSIM under most conditions, while maintaining comparable performance even under extremely adverse channel scenarios.

(a) PSNR performance curves versus the CBR over AWGN channel. (b) SSIM performance curves versus the CBR over AWGN channel. Where the SNR = 1dB, 4dB and 10dB for CIFAR10 dataset.

To further validate the superiority of the proposed STARJSCC model in high-resolution semantic transmission, additional tests are conducted using the Kodak and CLIC2020 dataset. For high-resolution images, inputs are preprocessed by cropping to 512×512 pixels, with other experimental settings consistent with the low-resolution tests. Figure7presents the performance comparison of the Kodak and CLIC2020 dataset under AWGN channel conditions across varying SNR levels. The results demonstrate that the STARJSCC scheme exhibits even more pronounced advantages in high-resolution image transmission. Specifically, Fig.7a and b illustrates the high-resolution PSNR performance of STARJSCC against ADJSCC and DEEPJSCC-V at CBR = 1/12 and 1/6. The proposed STARJSCC outperforms both DEEPJSCC-V and ADJSCC, with the performance gap widening as SNR increases, reaching a maximum advantage of 2.73 dB. Figure7c and d compares the high-resolution SSIM performance under the same settings. The STARJSCC model consistently achieves higher SSIM values than ADJSCC and DEEPJSCC-V, with a maximum SSIM gain of 0.01577 over DEEPJSCC-V. At CBR = 1/12, STARJSCC demonstrates significant SSIM advantages at lower SNR levels, though this margin diminishes as SNR increases. This phenomenon arises because semantic feature loss becomes more pronounced at lower CBR, inherently limiting the achievable reconstruction fidelity. Conversely, STARJSCC maintains substantial performance gains across all SNR levels at CBR = 1/6, with improvements becoming increasingly prominent as SNR rises.

(a)-(b) PSNR performance curves versus the SNR over AWGN channel. (c)-(d) SSIM performance curves versus the SNR over AWGN channel. Where the CBR = 1/12 and 1/6 for Kodak and CLIC2020 dataset.

Figure8illustrates the performance comparison of the Kodak and CLIC2020 dataset under AWGN channel conditions across varying CBR conditions. Similar to the low-resolution test results, the proposed model exhibits strong adaptability under diverse CBR constraints, consistently outperforming DEEPJSCC-V across three distinct SNR levels. Specifically, for PSNR, the performance advantage of STARJSCC becomes increasingly pronounced as CBR increases, achieving a maximum performance gap of 2.15 dB. For SSIM, the highest improvement reaches 0.02267 under SNR = 1 dB.

(a)-(b) PSNR performance curves versus the CBR over AWGN channel. (c)-(d) SSIM performance curves versus the CBR over AWGN channel. Where the SNR = 1dB, 4dB and 10dB for Kodak and CLIC2020 dataset.

In summary, compared to the low-resolution dataset evaluations, the proposed scheme demonstrates even more substantial improvements in high-resolution testing scenarios. This underscores the superior capability of STARJSCC in high-resolution image semantic transmission tasks relative to other models. This phenomenon arises because STARJSCC’s hybrid attention architecture effectively integrates both local and global information, enhancing the model’s capacity to capture fine-grained semantic features. This architectural strength enables exceptional performance on high-resolution images where abundant structural details exist. In contrast, the inherent loss of finer object boundaries and texture variations in low-resolution images fundamentally limits the full exploitation of STARJSCC’s advantages.

In order to investigate the impact of different Star Block designs on the performance of STARJSCC system, an ablation study is conducted to analyze their architectural variations. Subsequently, we also compare Star Block with standard MobileNet block and EfficientNet-style bottleneck under the same training configurations to justify its use. The four distinct Star Block variants (Block I, Block II, Block III, Block IV) are designed, as illustrated in Fig.9, and tested within the STARJSCC framework. Table1presents the performance metrics of these variants under CBR = 1/12 and SNR = 9 dB, where the Storage metric indicates the storage overhead of models trained with each respective module.

Four distinct Star Block variants are designed, with being Block I adopted as the standard configuration in the proposed STARJSCC framework.

Performance comparison of different block variants under CBR = 1/12 and SNR = 9 dB, tested on the Kodak dataset.

The experimental results demonstrate that models based on Star Block and its variants outperform those using standard MobileNet block and EfficientNet-style bottleneck, justifying the selection of Star Block as the fundamental module for STARJSCC. Among these, Block I achieves the best performance in terms of both PSNR and SSIM, followed by Block II and Block IV, while Block III shows slightly inferior results. Notably, the model trained with Block I exhibits significantly lower storage overhead compared to the other three variants. This substantial reduction in storage requirements, combined with its superior performance, further validates Block I as the optimal design choice for our system.

Generally, the principle of the attention mechanism lies in minimizing the loss by adjusting the model’s focus on different image regions. In STARJSCC, we introduce the CSA mechanism to adapt to varying channel conditions, thereby improving the model’s semantic preservation and transmission capabilities. To understand the impact mechanism of the CSA Mod on semantic features during transmission, we focus on the scaling coefficients it generates. Specifically, we conduct 10 transmissions for images from the Kodak dataset at 5 different SNR values and compute the average of the scaling coefficients produced by the CSA Mod. The detailed distributions are illustrated in Fig.10. We extract the scaling coefficients from the first 48 channels of the first and second CSA Nod in the encoder and visualize their distributions using heatmaps. The results show that after processing by the first CSA Mod, the scaling coefficients exhibit a distinct “stratification” phenomenon, indicating that they still retain noticeable variations at different SNR values. However, after refinement by the second CSA Mod, the differences in scaling coefficients at different SNRs diminish and become nearly identical. This trend aligns with the analysis in Ref19., suggesting that channel noise has a more pronounced impact on low-level features than on high-level features.

The scaling coefficients of the first 48 channels in the encoder of STARJSCC on AWGN channel (CBR = 1/12). (a) the scaling coefficients of the first CSA Mod. (b) the scaling coefficients of the second CSA Mod. The scaling coefficients of each channel are evaluated on the Kodak dataset.

Furthermore, to validate the effectiveness of the proposed CSA mechanism, we conduct comparative experiments with standard attention mechanisms including SE and Convolutional Block Attention Module (CBAM), and investigate the impact of SNR input on the attention module’s performance. Specifically, under the same training configurations, we evaluate different attention mechanisms within the STARJSCC framework for wireless image transmission, with the corresponding performance results summarized in Table2. For ease of understanding, we refer to the attention modules without SNR input as CSA_wo_SNR, SE_wo_SNR, and CBAM_wo_SNR, while those with SNR input are denoted as CSA+SNR, SE+SNR and CBAM+SNR. The results of Table2demonstrate that our proposed CSA attention mechanism achieves the highest PSNR and SSIM values regardless of SNR input incorporation. Moreover, by embedding SNR into the CSA module, our approach achieves performance improvements of 0.26 dB and 0.00213 in PSNR and SSIM, respectively.

The PSNR and SSIM comparison of different attention mechanism under CBR = 1/6 and SNR = 13 dB, tested on the Kodak dataset.

Under the conditions of CBR=1/6, we further evaluate the model parameters, storage requirements, FLOPs, and inference time of conventional BPG + LDPC, ADJSCC, DEEPJSCC-V, SwinJSCC_Base, MambaJSCC and STARJSCC on the Kodak dataset. Notably, metrics such as parameters and storage overhead are independent of input image dimensions, as they solely depend on the model architecture. During testing, kodim02 image is preprocessed by resizing to 256×256 pixels before being fed into the model. We transmit it 10 times and take the average as the final inference time result.

As shown in Table3, the proposed STARJSCC scheme demonstrates significant advantages in FLOPs, parameters, and storage overhead. Specifically, STARJSCC demonstrates significantly faster inference time compared to the BPG+LDPC scheme. In terms of both FLOPs and parameters, STARJSCC achieves reductions of 30.83% and 52.78% compared to MambaJSCC and ADJSCC respectively. Furthermore, the storage cost of STARJSCC is only 59.90% of ADJSCC and 49.03% of DEEPJSCC-V. These improvements stem from its lightweight backbone network design, which replaces standard convolutions with DW-Convs and employs the star operation to fuse features from dual-branch structures, thereby improving the efficiency of semantic feature modeling. Although our model does not hold an advantage over other JSCC schemes, its outstanding advantages in computational efficiency, parameter reduction and storage economy make it particularly suitable for resource-constrained deployment scenarios, demonstrating significant practical value.

Comparison of inference time, FLOPs, parameters, and storage requirements across different codec schemes.

To further validate the effectiveness of the proposed model, a set of visual comparisons is provided using the kodim21 image from the Kodak dataset. As illustrated in Fig.11, the image reconstruction quality of STARJSCC, ADJSCC, and DEEPJSCC-V under AWGN channel conditions is visually compared, demonstrating the robustness and adaptability of the proposed framework. It is important to note that even when the SNR is constant during testing, inherent randomness in channel noise may introduce subtle variations in reconstruction outcomes. From these results, the STARJSCC scheme exhibits significant advantages across all SNR levels. Compared to DEEPJSCC-V, which supports SNR and CBR adaptation, STARJSCC achieves PSNR and SSIM improvements of 4.25 dB and 0.0276, respectively, under SNR = 21 dB. Furthermore, the proposed method effectively mitigates granular artifacts and shadowing distortions, producing reconstructed images with rich details and high fidelity. Consequently, STARJSCC better satisfies human visual perception requirements in semantic communication systems, bring more natural and authentic visual experiences.

Visual comparison of STARJSCC, ADJSCC and DEEPJSCC-V under AWGN channel at SNR=1dB, 5dB, 9dB, 15dB and 21dB.

In practical semantic communication scenarios, images received after wireless transmission are typically utilized for downstream tasks. To validate the usability of transmitted images in subsequent semantic processing, we conduct object detection task using these images. The evaluation is performed with a Yolov8 network initialized with officially released pre-trained weights.Specifically, we utilize images kodim06, kodim11, kodim20, and kodim23 from the Kodak dataset, along with their semantically transmitted versions (processed through ADJSCC, DEEPJSCC-V, and STARJSCC), as input to YOLOv8 to obtain detection results. The transmission parameters are configured with SNR = 6 dB and CBR = 1/12. The comparative results and performance metrics output by Yolov8 are presented in Fig.12. From these results, we observe that the transmitted images of all three models consistently meet the performance standards for object detection. Notably, the transmitted images perform better than the originals in terms of detection success rate in many cases. Furthermore, under conditions with strong background interference (e.g., kodim11), the detection performance of ADJSCC- and STARJSCC-transmitted images is significantly superior to that of both the original images and DEEPJSCC-V-transmitted images. This demonstrates that STARJSCC-transmitted images can be effectively utilized for downstream semantic tasks without compromising accuracy.

Performance comparison of original and transmitted images in object detection task, where SNR = 6dB and CBR = 1/12 during the transmission. (a)-(d) show the detection results of original images. (e)-(h) show the detection results of the images transmitted using ADJSCC. (i)-(l) show the detection results of the images transmitted using DEEPJSCC-V. (m)-(p) show the detection results of the images transmitted using STARJSCC.

This paper proposes STARJSCC, a novel and highly flexible JSCC architecture that demonstrates exceptional adaptability within a single model, dynamically address diverse channel states and CBR conditions. Specifically, we design a star operation-based modulation network for wireless image transmission codec framework, incorporating a plug-in CSA Mod that enables dynamic channel sensing and parameter adjustment while maintaining transmission quality. Extensive experimental results demonstrate that, compared to conventional CNN-based JSCC frameworks, STARJSCC not only significantly reduces model parameters, computational complexity, and storage overhead but also achieves superior image transmission quality. These advancements position STARJSCC as a promising solution for semantic communication systems in resource-constrained wireless scenarios.

In future work, we will explore the deep integration of the proposed framework with SOTA architectures such as Transformer and Mamba, with a focus on developing more efficient rate-adaptive compression algorithms and optimizing the model’s transmission efficiency and generalization capabilities. Furthermore, we have verified the model’s compatibility with mainstream deployment tools such as TensorFlow Lite and ONNX Runtime. We will explore STARJSCC’s generalizability in broader semantic communication scenarios (such as IoT device communication and satellite communication) and conduct cross-modal research to extend it to other data transmission types (such as video streams, voice signals and text information). Through the synergistic integration and refinement of these advanced techniques, we aim to lay the groundwork for novel methodologies and solutions in the design and optimization of next-generation wireless communication systems.

This work was supported in part by National Natural Science Foundation of China under Grant 62361003 and 62261003, the Key Research and Development Program of Guangxi under Grants AD25069071, the Natural Science Foundation of Guangxi under Grant 2025GXNSFAA069672, and in part by the Innovation Project of Guangxi Graduate Education under Grant YCSW2025124.