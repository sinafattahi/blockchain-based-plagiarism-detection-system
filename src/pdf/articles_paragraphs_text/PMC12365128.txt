Video has become a popular medium for socializing and sharing, with 360-degree video offering richer content that delivers a more immersive and engaging experience. In recent years, leading platforms like YouTube and Facebook have been actively promoted 360-degree video services. In particular, virtual reality is a key technology in the new conceptual metaverse that has recently emerged, and 360-degree video is one of its key distribution media1. Unlike traditional videos, 360-degree videos usually require very high video resolution (usually4 K) to ensure satisfactory Quality of Experience (QoE). While mainstream platforms recommend a minimum connection speed of 25 Mbps for UHD streaming2, the 2021 global average bandwidth per user reached only 190.1 kbps3. This disparity underscores the critical need for research into efficient 360-degree video transmission.

An illustration of the tile-based 360-degree video streaming, dividing the video rectangular frame into RxC tiles area, whereR= 4, C = 4.

Work12uses clustering to improve the performance of prediction methods by utilizing cross-user viewing behavior information. Despite improvements in prediction accuracy and time window length, these methods require pre-acquisition of cross-user trajectory information for the corresponding video content, which is not accessible in many scenarios. Examples include new videos that have not yet been viewed and live video streams, demonstrating significant limitations of such methods.

To overcome these limitations, we propose the Cross Modal Multiscale Transformer (CMMST). The CMMST projects both the viewport and 360-degree video content into uniformly-sized rectangular frames, enabling effective Cross Modal feature alignment. This unified representation facilitates dependency extraction between visual content and user behavior modalities. We validate our model on benchmark datasets and experimentally demonstrate that it outperforms baseline approaches.

We formulate the 360-degree video streaming viewport prediction problem as follows: Given a series of 360-degree video frames, wherecorresponds to frame t of the video. The orientation of the user’s head on the corresponding frame is,corresponds to the head orientation of the user i when viewing frame t, where,is unit quaternions representation of head orientation with respect to a fixed reference point. In order to construct the dependence of the user trajectory and the video saliency map, we project the user’s historical head movement trajectory onto the corresponding pixel point location of the video frame, i.e. the center of the user’s viewport. Thus, the motion recordings of the user head are converted into viewing trajectories in rectangular video frames. We project the head orientation quaternion to a 2D coordinate, which corresponds to the viewport position in the corresponding video frames. According to the tile-based 360-degree video streaming framework, we divide the video frames intotile regions on average for selective transmission. Varying the tiling of the video can produce different numbers of tiles; more tiles mean less error tolerance in the viewport prediction. To be fair, we use the same tile strategyas the baseline method PanoSalNet16. According to the viewport coordinate, We can get the index of the tile, where,. The goal of our viewport prediction model is that, based on the viewport trajectory of the past 1 s, and video metadata from one second ago to n seconds in the future. Predict the tile index of the viewport for the next n seconds. Here b denotes the number of frames in one second of video (i.e., the video frame rate).

We employed two widely used datasets encompassing 360-degree videos of diverse categories and corresponding head movement trajectory logs. The first dataset (ds1)17contains nine popular 360-degree videos in four categories, watched by 48 users with an average viewing time of 164 s. The second dataset (ds2)18contains trajectory records of 59 users freely watching five 360-degree videos, each recorded for 70 s. In both datasets, each trace of the head tracking log for both datasets consists of the user’s head position (unit quaternion (w, x, y, z)) along with a timestamp, which is projected to equirectangular frames. Example frames of dataset are shown in Fig.2, where red dots indicate viewport centers. For videos containing salient objects, while most viewports focus on these regions, numerous outliers exist. For videos without salient objects, viewport distributions are more dispersed.

Example frames of dataset, each red dot indicates the center of the user’s viewport.

Pair-wise correlation of each video in dataset. Where the colour of the column indicates the video category.

Structure overview of Cross Modal Multiscale Transformer (CMMST). Input saliency map and user trajectory, CMMST output the probability matrix of each tile. MHPA: Multi-Head Pooling Attention; FFN: Feed-Forward Network; “C” denotes the concatenation operation;.

Saliency detection has long been an important research area in visual attention prediction for image and video viewing. Users are more likely to stay in regions of interest19. In 360-degree videos, saliency information shows strong correlation with user viewing trajectories20, making video saliency features valuable for viewport prediction. While we cannot directly access users’ future viewports, we can infer potential areas of interest through video content analysis. Building upon pioneering work in 360-degree video saliency detection, we employ PanoSalNet16to extract frame-level saliency features, simplifying our problem formulation. Figure5presents the accumulated saliency and fixation maps, revealing distinct spatial exploration patterns across different videos. These patterns are encoded as weight matrices, where higher values in the saliency feature matrices indicate regions more likely to attract user attention.

Accumulate fixations and saliency maps of each video. Accumulate fixation maps are in first line, accumulate saliency maps are in second line.

The standard Transformer9receives as input a 1D sequence of token embeddings. However, our inputs (e.g., video saliency heatmaps) are 3D matrices. To process 360-degree video salience map and user trajectory, we first partition eachsaliency map (where T represents frame count, H denotes height, and W indicates width) into non-overlapping cubes of size. We then apply a linear layer to each cube, equivalent to a 3D convolution with matching kernel and stride sizes, projecting them into the Transformer’s latent dimension D. Formally, we reshape the heatmapinto a sequence of flattened 3D cube, whereis the frames, height and width of the 360-degree video salience map.is the frames, height and width of each video salience map cube, r is the sample rate on time, and.

For trajectory embedding, we apply the same cubing operation to the user’s historical trajectory, mapping it to cube embedding features. These features are then fused with corresponding video saliency heatmapsthrough element-wise multiplication. To capture Cross Modal dependencies, we concatenate the fused hybrid features with future video saliency heatmaps, converting the combined representation into Transformer tokens. Finally, we extract Cross Modal dependencies using multi-head pooling attention.

Multi-head grouping attention is a self-attention operation that enables flexible resolution modeling in transformer blocks21. This mechanism allows our Cross Modal multiscale Transformer to operate at progressively changing spatio-temporal resolutions. Figure6shows the schematic diagram of the multiscale cross-attention module. Specifically, multi-head pooling attention combines sequences of latent tensors to reduce the input sequence length (resolution) while preserving essential features.

Schematic of the computational loop for Multiscale Cross-Attention Module.

Similar to Multi Head Attention for traditional transformers, for a D-dimensional, input vector of length(),.We then project the input tensor to the intermediate query tensorvia aLinear operation. key tensorand value tensor.

Where theis normalizing the inner product matrix row wise.

As in9the computation can be parallelized by considering h heads where each head is performing the pooling attention on a non-overlapping subset ofchannels of the D dimensional input tensor X.

where T is the total number of frames predicted,is the predicted probability matrix for each viewport tile in the i-th frame, andis the corresponding ground truth binary matrix.

The Accuracy and Manhattan Error of head movement prediction are used as the evaluation metric. Accuracy is calculated based on the ratio of the number of overlapping tiles between the predicted and ground truth head orientation map to the total number of predicted and viewed tiles. This accuracy metric is averaged across all frames and all users for each video in the dataset.

Quality of Experience (QoE) metrics: User-perceived quality is quantitatively assessed through multiple QoE metrics, which we define to empirically evaluate our model’s performance.

where,is the number of frames in the chunk x.is the bitrate allocated to tilein chunk x, whileindicates whether the tile is inside theframe’s viewport.represents the total number of tiles displayed in the video player. The normalization constantis derived from the count of unique viewport tiles in chunk x.

We compare our model with three baseline methods: (1) PARIMA14, which extracts motion trajectories of main video objects and predicts them using regression (implemented using publicly available code1 (https://github.com/sarthak-chakraborty/PARIMA(Access Nov 17, 2022))); (2) PanoSalNet16, employing an LSTM architecture to process saliency maps and head movement data for viewpoint prediction (using official public code2 (https://github.com/phananh1010/PanoSalNet.git(Access: March 2, 2021))); and (3) Cluster22, which groups users based on viewport history and performs predictions via quaternion extrapolation (implemented with code1 using a 1-second prediction window).

We train our network using the AdamW optimizer (,) with a learning rate of 0.00025, momentum 0.95, weight decay 0.0005, and batch size 16. Following16, we partition the dataset17using 5 of 9 videos for training and 4 for validation. For each video, we select one segment with a length of 20–45 s. The video segment is selected so that it contains one or more events that introduce new salient regions and cause the user’s head to be moved fast. The default prediction window is set to be 1 s. The default Multiscale cross-attention layer number N is set to be 4. The default Fusion layer number K is set to be 4. We resize the saliency map toto reduce the computation. The final model spent 3 h training 10 epochs on a 4090 GPU. Early stopping criterion: Validation accuracy did not improve for 3 epochs.

We conducted statistical significance tests across five independent runs with different random seeds. As shown in Table1, CMMST demonstrates significant accuracy improvements over baseline methods. PARIMA operates by extracting motion trajectories of dominant video objects and performing regression-based predictions. Although we tested it using shorter video segments containing salient events, PARIMA requires an adaptation period for new input data, ultimately degrading its performance. We evaluated our proposed model on Dataset 2 (ds2), with results presented in Table2. Using a 1-second prediction window across five independent runs with different random seeds, CMMST achieves an average precision of 0.662 ± 0.0021 (mean ± SD). The model maintains strong performance on this dataset, demonstrating robust generalization capabilities across diverse data scenarios.

To validate the practical efficacy of our viewport prediction framework, we implemented an end-to-end streaming pipeline. This experimental setup enables quantitative evaluation of both bandwidth efficiency and perceptual quality through four QoE metric. Benchmark results demonstrate our method achieves superior QoE compared to baseline methods. These QoE gains substantiate our hypothesis that spatial-temporal attention modeling enables more accurate viewport anticipation, thereby reducing wasteful bandwidth allocation and improving rendered quality where users actually look.

To evaluate the contribution of individual modalities, we developed the CMMST-T variant that exclusively processes user trajectory data. Table1. demonstrates that integrating visual saliency information yields a 0.016 improvement in prediction accuracy while reducing average tile error by 0.16, confirming the complementary value of multimodal inputs.

To rigorously validate the contribution of our multiscale design, we have conducted comprehensive ablation experiments as follows: a)CMMST-Small: Only uses fine-scale attention (patch size = 32 × 20).

b)CMMST-Large: Only uses coarse-scale attention (patch size = 16 × 10). As shown in Table3, the complete CMMST maintains no accuracy drop compared to the large variant while requiring only 71% of its FLOPs, establishing an optimal accuracy-efficiency tradeoff.

Ablation experiments of multiscale pooling attention on dataset 1.

To evaluate the model’s robustness over extended prediction horizons, we conducted tests with progressively longer time windows. As evidenced by Fig.7(a), our model maintains consistently low tile error even as the prediction window increases, demonstrating stable performance across varying temporal scales.

(a) The accuracy of proposed viewport prediction model for different time window predictions. (b) The accuracy of the model for different number of multi-scale Transformer blocks. Testing using Cross Attention module and Fusion module with the same number of layers, i.e. N = K.

To evaluate the effects of transformer block number (hyperparameters N and K), we conducted experiments using different layers of blocks, and the results are shown in Fig.7(b). The prediction accuracy is not proportional to the number of block layers. Accuracy decays after increasing to 9 layers, and the computational cost increases sharply with the number of layers.

To understand how the proposed model would adapt to new users for viewport prediction. We retrain the proposed model on 9 videos from17, using the data of 40 users and the rest data of 8 users is used as novel data for validation. We show the accuracy of each video in Fig.8. Despite the fact that the new user’s data has not been trained, our proposed model is still able to achieve a high accuracy rate for the learned videos. The nine videos can be divided into three categories: static scenes with a few scene switches (red), fast-moving scenes (green) and slow-moving scenes (blue)16. It can be seen that our proposed model performs best in static scenes. Prediction accuracy decreases in fast-moving scenes. This may be due to the fact that users’ attention shifts more quickly and less user fixation in fast-moving scenes.

The accuracy of proposed viewport prediction model under different videos.