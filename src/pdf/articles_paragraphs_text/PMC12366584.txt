To evaluate the transferability of BERT (Bidirectional Encoder Representations from Transformers) to patient safety, we use it to classify incident reports characterised by limited data and encompassing multiple imbalanced classes.

BERT was applied to classify 10 incident types and 4 severity levels by (1) fine-tuning and (2) extracting word embeddings for feature representation. Training datasets were collected from a state-wide incident reporting system in Australia (n_type/severity=2860/1160). Transferability was evaluated using three datasets: a balanced dataset (type/severity: n_benchmark=286/116); a real-world imbalanced dataset (n_original=444/4837, rare types/severity<=1%); and an independent hospital-level reporting system (n_independent=6000/5950, imbalanced). Model performance was evaluated by F-score, precision and recall, then compared with convolutional neural networks (CNNs) using BERT embeddings and local embeddings from incident reports.

Fine-tuned BERT outperformed small CNNs trained with BERT embedding and static word embeddings developed from scratch. The default parameters of BERT were found to be the most optimal configuration. For incident type, fine-tuned BERT achieved high F-scores above 89% across all test datasets (CNNs=81%). It effectively generalised to real-world settings, including rare incident types (eg, clinical handover with 11.1% and 30.3% improvement). For ambiguous medium and low severity levels, the F-score improvements ranged from 3.6% to 19.7% across all test datasets.

Fine-tuned BERT led to improved performance, particularly in identifying rare classes and generalising effectively to unseen data, compared with small CNNs.

Fine-tuned BERT may be useful for classification tasks in patient safety where data privacy, scarcity and imbalance are common challenges.

While traditional machine learning methods have been shown to be effective in categorising narrative reports about patient safety incidents, their performance suffers in detecting rare classes and generalising to unseen data due to the complexity of patient safety concepts and terminology, imbalanced datasets with sparse labels and the coexistence of multiple types within individual reports.

BERT (Bidirectional Encoder Representations from Transformers) is a benchmark large language models (LLMs) that prioritises narrative structure over the intricacies of clinical terms but its transferability to identify patient safety incidents has not been previously investigated.

Fine-tuned BERT is transferable to patient safety, accurately identifying rare incident types and untangling complex severity levels with unclear boundaries.

Fine-tuned BERT demonstrated better generalisability on unseen data compared with small convolutional neural networks (CNNs), highlighting its potential for scalable applications.

Future research could validate its feasibility for longitudinally tracking incident reporting data across multiple jurisdictions or countries.

BERT’s reliance on pre-existing knowledge when identifying incidents labelled broadly as ‘Other’ suggests the need for further investigation into balancing the generative capabilities of LLMs with the demands for accuracy, specificity and domain relevance.

While BERT (a classic decoder LLM) provided a strong baseline, future work could explore larger decoder-based models pretrained on extensive corpora, as well as domain-specific pretraining of BERT using large-scale incident data to enhance model understanding in patient safety.

Instead of using clinical or biomedical models, a general BERT that prioritises narrative structure over the intricacies of clinical terms may be more suitable for identifying incidents.21 39BERT remains a benchmark due to its compact size, lower computational requirements and effectiveness in adapting to domain-specific tasks.28 29 34However, the feasibility of applying LLMs to identify incidents has not been explored previously. We investigated two strategies: (1) fine-tuning a pre-trained BERT model with incident reports by leveraging expert report annotations to identify multiple types and severity levels; (2) using BERT word embedding to extract features, with development of a small CNN for incident classification. We compare these two approaches to our prior CNN models with static word embedding.10 20Lastly, we conducted error analysis to investigate how BERT’s pre-trained knowledge identifies intricate patterns and connections in incidents, providing insights for other patient safety tasks.

The Advanced Incident Management System (AIMS) was used in four Australian states and territories (New South Wales, Western Australia, South Australia and Northern Territory).6 40Incidents were categorised into 20 types by reporters, with common types like falls and medication comprising 54% of all reports, while rare types (eg, deteriorating patient) constituted only 5–6%.10On average, 130 000 incidents were reported annually to AIMS in one state. To ensure a representative sample, 300 reports were randomly collected for each type, guided by reporter labels. In total, 6000 reports were collected from AIMS between January and December 2011.

The Riskman system is another tool used in the state of Victoria and several private hospitals across the country.10 11A total of 6000 incident reports were randomly selected from a hospital-level system between January 2005 and July 2012. Any identifiable information, such as names or birth dates, was removed in compliance with jurisdictional privacy requirements.

The reports were reviewed and classified into 20 incident types by patient safety experts using the international classification for patient safety.41,43We focused on 10 priority types for safety and quality improvement (table 1).2 6 10To cover the entire dataset, an ‘Others’ category was created using random sampling approach to ensure representativeness of the 10 remaining types (see details in previous work10). Incident severity was determined by the internationally recognised Severity Assessment Codes (SAC) from the US Veterans Administration.44Four risk ratings (extreme, high, medium, low) were assigned by trained patient safety managers based on severity and recurrence likelihood.

BERT, Bidirectional Encoder Representations from Transformers; CNN, convolutional neural networks; SAC, Severity Assessment Codes; SVM, Support Vector Machine.

To fine-tune BERT, we created a balanced dataset using 260 reports from each incident type and 290 reports for each severity level (table 1).10 20The balanced dataset was split into training (80%), validation (10%) and testing (10%) subsets using 10-fold subsampling cross validation. Training and validation subsets were used to optimise BERT parameters, while the testing subsets served as abenchmarkto assess model generalisability.

To assess real-world performance, models were tested on imbalanced, that is, ‘stratified’ datasets from AIMS (original). These datasets were randomly selected from the remaining AIMS reports, maintaining real-world ratios by type and severity (table 1). To examine generalisability to different reporting systems, the models were tested using a stratified Riskman dataset (independent).

Incident reports consist of structured (eg, ID, date) and free-text fields describing safety events and their consequences. Only the descriptive narratives from text fields were combined and used as model input for experiments, in the following order: incident description/detail, patient outcome, actions taken, prevention steps, investigation findings and results. All codes, punctuation and non-alphanumerical characters were removed, and the text was converted to lowercase.

Leveraging the BERT tokeniser, we tokenised preprocessed text data into subword sequences, assigning each token a corresponding embedding vector.24BERT also adds positional embeddings to understand the sequential order of words in sentences, for example, a classification token (CLS) was used for overall context at the beginning of sequence, and a separator token (SEP) for denoting sentences or segment boundaries.

Unlike traditional models that require fixed-length input, BERT handles text inputs of varying lengths through its attention mechanism, which dynamically assigns weights to different parts of a sequence.24However, for efficient batch processing, consistent sequence lengths are required.45Padding and truncation techniques are employed for length matching. When padding, a (PAD) token is appended at the end of shorter sequences and treated as a neutral element without significance during BERT training. Lengthy reports were truncated from the bottom, prioritising narratives in descriptions over incident management information. To optimise model performance, fine-tuning was conducted with a soft input sequence length ranging from 75 (median report length) to 512 (default BERT’s maximum token limit).

BERT’s word embedding captures the meaning of a word based on its surrounding context in sentences. We used this flexibility to generate word vectors as input features for the subsequent CNNs. The embedding dimensionality (vector size) is treated as a parameter in the word-embedding space, optimised within a range from 256 to 768 (default size in BERT base model).

We customised a pretrained BERT base model architecture to our specific classification tasks by adding a multiclass classification head and modifying its output layers to align with the number of incident types and severity levels as model’s output (n_type/severity=11/4).45Given limited training data, we used a small batch size of 32 and fine-tuned for four epochs over the datasets for the two classification tasks. For each task, we grid searched over lower learning rates in (0.00005, 0.00004, 0.00003, 0.00002 and 0.00001) with Adam optimiser and a weight decay factor of 0.01. Model selection was based on loss function with a validation set and we chose the best performing checkpoint.

F-score, precision and recall measures were used to assess classifier’s performance on individual incident types and severity levels.10 11 20A confusion matrix was used to visualise results.

For incident classification, the most effective configuration to input layer includes a default maximum sequence length (512) and embedding space (768), regardless of whether fine-tuning or using word embedding to extract features. To minimise computational costs, a learning rate of 0.00001 is recommended. These settings achieve the best classification performance across two independent reporting systems (table 2). When using BERT for feature extraction, reducing the dimensionality of embedding spaces generally led to inferior performance, as observed with a fixed report length (online supplemental appendix A, table A2).

BERT, Bidirectional Encoder Representations from Transformers.

Fine-tuned BERT outperformed small CNNs with either BERT and local word embeddings in identifying both incident type and severity levels on all testing datasets (table 3, micro-averaged F-scores;online supplemental appendix B). For incident type, fine-tuned BERT consistently achieved high F-scores of 90.9%, 89.0% and 90.3% on benchmark, original and independent datasets. These scores were 9.8%, 1.4% and 5.2% higher than those of CNNs. For severity level, fine-tuned BERT achieved an F-score of 95.7% on benchmark and generalised well to original (86.5%) and independent datasets (83.3%). Compared with CNNs, it showed substantial gains, improving 11.2%, 4.7% and 17.2% on the benchmark, original and independent datasets, respectively. These results show that BERT is effective for classifying incidents, particularly for unseen reports.

BERT, Bidirectional Encoder Representations from Transformers; CNN, convolutional neural networks; SAC, Severity Assessment Codes.

BERT effectively identified common incident types such as falls, medication and aggression, achieving F-scores consistently above 88% across all testing datasets. For rare types that pose challenges to CNNs, such as patient identification and clinical handover, BERT not only improved performance on the benchmark and original datasets, but also demonstrated remarkable enhancements of 38.4% and 30.3%, respectively, on the independent dataset. This showed BERT’s capability in handling diverse incident types, especially rare types that have proven to be difficult for CNNs. However, BERT struggled with the broad ‘Other’ type, often misclassifying it into specific types, such as blood product and pressure injury. This issue led to the lower precision and F-scores for these categories.

When fine-tuned with a balanced dataset, BERT performed relatively worse in real-world settings (original and independent) when identifying rare incident types such as blood products, patient identification, infection and clinical handover. This issue was also observed with CNNs which performed the worst on the independent dataset; however, BERT achieved comparable F-scores on original and independent datasets. Interestingly, fine-tuned BERT performed better in identifying deteriorating patient in the independent (66.7%) than original dataset (58.3%).

Compared with CNNs, fine-tuned BERT showed significant improvements in identifying severity levels, particularly on the independent dataset where the averaged F-score increased from 66.1% to 83.3%. This improvement stemmed from BERT’s enhanced classification of SAC3 and 4 risk levels. F-scores for SAC3 increased from 84.2% to 98.3% on benchmark, from 86.9% to 90.5% on original, and from 66.4% to 84.3% on the independent dataset. For SAC4, performance improved by 4.5% on the original dataset and 11.7% on the independent dataset, further solidifying BERT’s generalisability to unseen data.

While fine-tuned BERT outperformed CNNs for rare SAC1 (93.1%) and SAC2 (98.2%) events in benchmark, it showed a similar trend to that observed with rare incident types, experiencing a decline in imbalanced settings (table 3). For example, F-scores for SAC1 dropped from 93.1% to 24.9% and 11.9% in the original and independent datasets, respectively.

Fine-tuned BERT demonstrated superior performance compared with CNNs using feature representations from either flexible BERT or local static word embeddings for incident classification. We found BERT’s default configuration was effective in simplifying parameter tuning and improving efficiency. Fine-tuned BERT transferred well to the independent dataset, demonstrating good understanding of patient safety narratives in small and imbalanced data, highlighting potential applicability to other patient safety tasks, like classifying complaints or emails. However, for incidents with broad labels like ‘Other’, BERT tended to rely on pre-existing knowledge to comprehend patient safety concepts and meanings. This necessitates precise task definition and discourages broad labels like ‘Other’.

Although BERT embeddings offer greater contextual flexibility,24they appear to be more suitable for larger AI models, as they do not improve performance when training with small CNNs. We found that decreasing the dimensionality of the embedding space could result in performance decline (online supplemental appendix A). This shows the default setup of BERT, including the size of the embedding space and maximum token limit, performs effectively for brief narratives like incidents (AIMS reports median: 75 words/155 tokens; Riskman median: 57 words/118 tokens).

Fine-tuned BERT was more effective in identifying incidents by severity level than by type. Compared with CNNs, F-scores for severity improved significantly: 11.2% on benchmark, 4.7% on original, a 17.2% on independent dataset. Conversely, F-scores marginally improved for incident type: 9.8%, 1.4% and 5.2% on benchmark, original and independent datasets, respectively. Despite using datasets of the same size for fine-tuning, BERT exhibited better adaptability in classifying severity levels (n=4) compared with incident types (n=11), indicating its transferability to smaller yet more specific tasks.

Previously small text classifiers like CNNs and SVMs struggled to generalise beyond balanced training data; performance dropped on imbalanced real-world data and was worst on independent datasets.10 20In contrast, fine-tuned BERT achieved peak performance on the benchmark dataset and unexpectedly surpassed that performance for specific types on the independent dataset. For instance, patient identification increased from 48.4% (original) to an impressive 72.4% (dataset). Similarly, the performance for medications improved from 87.8% (original) to 94.9% (independent). This suggests BERT’s potential for broader real-world application in patient safety, where data imbalances and unseen data pose common challenges.

CNNs often struggled to identify rare classes externally.10 20However, BERT effectively identified most rare types in the independent dataset. For example, the F-score for patient identification improved from 34.0% with CNN to 72.4% with BERT, clinical handover increased from 44.7% to 75%, and deteriorating patient climbed from 44% to 58.3% on the independent reporting system.

For severity level, fine-tuned BERT consistently outperformed CNNs across all testing datasets, particularly medium-risk SAC3 and low-risk SAC4 incidents.20Despite these two categories representing 98% of reports, the distinction between them is not always clear, leading to inconsistent ratings by patient safety experts.41 42However, BERT offers a promising solution, as demonstrated by its improvement in identifying SAC3 and SAC4 events on the independent dataset (66.4% to 86.1% and 77.5% to 88.2%, compared with CNNs, respectively). These findings confirmed that fine-tuned with domain-specific data, BERT can effectively identify rare types and subjective severity levels, which are challenging even for experienced professionals, making it a valuable tool for initial incident management.

‘Other’ type involving multiple patient safety concepts challenged both small text classifiers (eg, CNNs and SVMs) and fine-tuned BERT.10 20However, their misclassifications differed. CNNs and SVMs often confused ‘Other’ with documentation and clinical handover, while BERT misclassified it as specific types, like falls and pressure injury. For example, BERT misclassified an ‘accident’ (seed pod falling on window screen) as a fall due to its focus on the word ‘descent’. A ‘staff injury’ involving a needle prick was wrongly classified as pressure injury. While the nurse applied pressure to stop bleeding, BERT prioritised keywords like ‘pressure’ and ‘injury’, disregarding patient safety concepts and semantics. Another incident involving an influenza patient missing a mask was misclassified as infection instead of a medical device/equipment. These misclassifications suggest that BERT may have relied on its existing general language knowledge to classify miscellaneous incidents into more specific types it had been trained on. To address this, we propose fine-tuning BERT with more distinct incident types, avoiding the grouping of less frequent incidents into the Other category.

For severity level, we noted that BERT tended to misclassify SAC3 as higher risk SAC4 (online supplemental appendix B, figure B2), reflecting the tendency of reporters to overestimate severity. With AIMS, 60.2% of SAC4 and 32.7% of SAC3 events were ranked into high-risk levels. Reports from health professionals who lack expertise in categorisation often tend towards higher-risk classifications to minimise the risk of overlooking crucial details. BERT has the capability to understand the context, tone and intent of language, explaining why it mirrors reporter behaviour. Refining the model through reinforcement learning is essential to address misclassifications.

Our study underscores the effectiveness of fine-tuned BERT for classifying patient safety incidents by type and severity. Default BERT configurations to input layer proved efficient in both classification tasks, negating the need for extensive parameter tuning. Furthermore, BERT’s enhanced performance in identifying severity levels suggests its suitability for smaller classification tasks, especially when fine-tuned with limited data. Finally, BERT demonstrates improved generalisability on unseen data than small CNNs. However, error analysis emphasised the importance of distinct task definitions and avoiding broad labels.

We thank Bronwyn Shumack, Katrina Pappas and Diana Arachi for assisting with the extraction of the incident reports; and Anita Deakin, Alison Agers and Sara Suffolk for their assistance with labelling reports.