Texture recognition underpins critical applications in industrial quality control, robotic manipulation, and biomedical imaging. Traditional deep dictionary learning methods for texture recognition often emphasize deep feature extraction. However, they tend to lose crucial features as model depth increases, which can reduce their overall effectiveness. To address this issue, we propose a dictionary-reconstruction-based deep learning approach by incorporating a novel hybrid fusion method designed to enhance the accuracy of texture recognition. Our approach involves the successive fusion of multimodality and multi-level features. By reconstructing dictionaries learned at different levels, we integrate both deep and intuitive features. Additionally, we introduce a grouping optimization technique, based on single-sample learning, to train these reconstructed dictionaries, thereby improving feature learning and training efficiency. The proposed approach fuses feature data from various multimodal sources and constructs dictionaries at different learning levels, which enables effective feature fusion across these levels. We evaluate our approach against recent deep learning methods by using the LMT-108 and SpectroVision datasets. The results demonstrate its 97.7% and 89.4% accuracy rates, respectively, outperforming its peers and validating its robustness when handling diverse and challenging data.

Texture is a fundamental attribute of objects1. Recognizing fine-grained textures is essential across multiple domains: in manufacturing lines, automated inspection systems must detect minute surface defects to prevent product failures2; in service and industrial robotics, discerning surface roughness and material properties informs safe and adaptive grasping strategies3; and in medical diagnostics, texture analysis of histological images aids in early disease detection4. Texture recognition, therefore, plays a crucial role in various applications, and significant advancements have been made in this field. Traditional approaches, such as single-layer dictionary learning, can capture shallow features but often overlook deeper, more effective features. On the other hand, deep learning techniques require large datasets and significant computational resources. This has led to a growing interest in combining dictionary learning5,6and deep learning7–9to address texture recognition challenges.

The above-mentioned deep dictionary learning models all emphasize the extraction of deep-level features. In contrast to single-layer dictionary learning, the features extracted from deeper layers are often referred to as hidden features. In the process of layer-by-layer training, it is equivalent to the extraction of hidden features and the elimination of some features, which may cause low utilization of data. Therefore, it is necessary to improve the effective use of data. The proposed method fuses shallow and deep features through a novel dictionary reconstruction process, preventing the loss of key information, and realize the ability of superior recognition accuracy.

Texture recognition often involves multimodal data, such as visual, tactile, and acceleration information, which provides complementary insights into surface characteristics that are difficult to capture with a single modality. This combination of modalities makes texture recognition a complex, yet promising multimodal fusion problem, where integrating information from different sources can significantly enhance classification accuracy. Researchers have explored various fusion strategies to address this challenge17. Feature-level (or early) fusion combines data from different modalities at the initial stages, maximizing the feature set but often requiring more complex preprocessing. In contrast, decision-level (or late) fusion integrates the outputs of individual classifiers, making it more adaptable to real-time applications. Mixed fusion approaches, which blend early and late fusion strategies, have also shown promise by balancing computational cost and accuracy. This study introduces a new method for feature fusion in texture recognition, which can create a more comprehensive feature representation by integrating features from all levels of the network. The method first performs early fusion of multimodal data to form rich initial inputs. Then a new dictionary reconstruction stage is introduced, in which the dictionaries learned at each level are combined.

The development of tactile sensors has furthered research in visual-tactile data fusion, with studies proposing various models and frameworks for effective multimodal integration. These include a hybrid joint group kernel sparse coding model for hybrid fusion18, a visual-tactile fusion framework for object recognition19, and continuous learning frameworks for multimodal integration20. Tactile sensors, particularly through measurements from accelerometers or pressure sensors, are also applied in texture recognition21–23. These approaches have demonstrated notable success in object and texture recognition, especially in applications where the availability of training data is limited.

In this context, the need for a robust and efficient multimodal fusion approach becomes apparent. Existing deep learning models like Multilayer Perceptrons (MLP) show potential in texture recognition but require extensive computational resources and large datasets, which limits their applicability in resource-constrained environments24. In contrast, dictionary learning is more efficient in capturing essential texture features. However, traditional methods like K-SVD25struggle to extract deep-level features, resulting in subpar classification performance. Current methods combining deep learning with dictionary learning13–16,26often prioritize deep features, neglecting some effective features as the model depth increases.

The primary motivation for this work stems from the observation that while deep learning models excel at extracting abstract, high-level features, they often do so at the cost of discarding valuable, low-level textural details learned in the initial layers. This trade-off is a critical bottleneck that limits performance. Our Dictionary-Reconstruction-based Deep Learning (DRDL) method is designed to overcome this limitation. Unlike traditional deep dictionary learning, which only trains dense features for subsequent layers, our approach aims to retain and fully utilize both shallow and deep features. The method involves a multi-stage fusion process: first, multimodal fusion based on feature matching is employed to obtain accurate and comprehensive features. Next, we learn sparse coding and dense feature matrices across different dictionary layers. Instead of simple multiplication, we fuse these features at all levels, ensuring a more comprehensive utilization of learned features, as shown in Fig.1.

DRDL model. The data of the three modes were divided into training set and testing set after feature extraction and reduction. The training was divided into three stages: layer-by-layer training stage, reconstruction stage, and overall loss fine-tuning stage. In the right part of the figure, the dotted line separates the layer-by-layer training stage from the dictionary reconstruction stage and fine-tuning stage.

Proposing a new model that enhances classification ability through successive fusion of features, starting with early fusion of multimodal features and followed by multi-level feature fusion.

Introducing a dictionary reconstruction method that combines deep and intuitive features by reconstructing dictionaries across different levels, thereby improving learning performance.

Developing a grouping optimization method based on single-sample training to enhance the effectiveness of feature learning and dictionary reconstruction.

Conducting extensive experiments and comparative studies with state-of-the-art methods on two challenging texture recognition benchmarks, the LMT-108 and the SpectroVision, to validate the proposed approach.

The following is a summary of the remainder of this paper. Section “Related work” briefs related work and presents the motivation of this work. Section “Methodology” introduces the proposed method. Section "Experimental process and result analysis" gives the experimental results and their analysis. Section “Conclusion” concludes this paper.

Traditional texture classification methods typically rely on a combination of manual feature extraction and machine learning. Common approaches include the gray level co-occurrence matrix (GLCM)27, which examines spatial relationships between pixel intensities to form textures. Gabor filters are also widely used for texture representation, especially in texture classification and segmentation. Selvaraj et al.28and Han et al.29applied Gabor filters to texture images, calculating the mean and variance of filter coefficients for different orientations and scales as features for classification.

Another popular method is the local binary pattern (LBP), valued for its computational simplicity and robustness to illumination changes. Ojala et al.30introduced the LBP algorithm for encoding local neighborhood features of texture images, with these encoded histograms serving as classification features. Shu et al.31advanced this by developing the Local Sorted Difference Refined Pattern (LSDRP), which improved feature representation by ordering local neighborhood points and calculating their differences.

While effective, traditional texture recognition methods27–32that rely on surface-level image features struggle with complex and varied textures, limiting their accuracy and efficiency. In recent years, deep learning has advanced at an unprecedented pace and found applications across a wide range of tasks33,34. These developments have naturally extended to texture recognition, where convolutional and graph-based networks learn hierarchical and relational texture features more effectively than hand-crafted descriptors35,36. However, directly applying such models to multimodal tactile–visual texture data remains challenging, motivating our proposed deep dictionary–driven reconstruction method.

To address these limitations, recent advances have focused on deep learning methods, which have shown remarkable success in texture analysis by automatically learning hierarchical feature representations. For instance, a study37introduced a novel texture feature descriptor using convolutional neural network (CNN) filter banks, demonstrating improved texture classification performance. Another study38developed the locally shifted Fisher vector (LFV) method, enhancing feature learning by utilizing locally shared filters in combination with CNNs. Further advancements include the application of global pooling CNNs (GP-CNN)39, which effectively leverage features from different depth levels of a pre-trained model to obtain high-quality texture information. A ranking-based method, Rank GP-CNN, was also proposed to automatically select and calculate texture feature vectors, enhancing classification accuracy. Finally, a multilevel texture coding and representation network40was developed, which fused features from different levels to preserve texture details and spatial information, further improving classification outcomes.

Sparse representation theory has proven highly effective in feature extraction and automatic learning, with the quality of the dictionary being crucial to the success of sparse coding. Dictionary learning aims to learn an overcomplete dictionary that approximates the original signal through a linear combination of dictionary atoms41. The K-SVD algorithm25, proposed by Aharon et al., is fundamental to this process, iteratively optimizing dictionary atoms to minimize reconstruction error.

As application scenarios have become more complex, dictionary learning methods have evolved. For example, a novel Online Convolutional Dictionary Learning (OCDL) algorithm42extends traditional patch-based online dictionary learning by updating the dictionary in a memory-efficient manner.

Dictionary learning is closely linked with sparse representation, involving two key phases: dictionary update and sparse coding. The goal is to represent the original data by combining dictionary atoms, typically within an overcomplete dictionary where the number of atoms exceeds the data’s dimensionality. Sparse regularization, often achieved by minimizing thenorm, ensures that the coding matrix remains sparse, thereby improving the representation’s efficiency.

Deep learning, inspired by the brain’s cognitive processes, has further enhanced dictionary learning. By integrating deep feature extraction with dictionary learning, new models have emerged that combine the strengths of both approaches, leading to more accurate and robust feature representations43,44.

This paper presents the DRDL method to perform a texture recognition task. Based on the feature matching method, the texture features of different modalities are fused. They are then used as the input of the first layer of dictionary learning to learn the dictionary matrix, sparse representation matrix and dense feature matrix. The dense feature matrix learned from the first layer is used as the input of the second layer to learn the dictionary matrix, sparse representation matrix and dense feature matrix. That learned from the second layer is used as the input of the third layer to learn the dictionary matrix and sparse representation matrix. The third layer introduces an-norm regularization term to reduce the variance among similar sample features. The learned dictionary is reconstructed, i.e., the features learned at different layers, and the sparse representation matrices learned at different levels are fused. In other words, shallow features and deep ones are fused. According to the reconstructed dictionary obtained by training, the sparse representation matrix of the test set is learned, and the test set label is obtained according to the cosine similarity principle to complete the texture recognition task.

Both deep learning and dictionary learning have achieved success in many fields. We propose a Deep Dictionary Learning method based on Dictionary Reconstruction which involves three phases: 1) a pre-training phase to learn multi-level dictionary matrices and features; 2) a dictionary reconstruction phase where the learned matrices are fused into a new, comprehensive dictionary; and 3) a fine-tuning phase to optimize the fused dictionary and generate the final sparse representation.

Deep dictionary learning plays a critical role in dictionary pre-training. Taking a three-layer architecture as an example, each layer can be regarded as an independent shallow dictionary learning module. In the deep dictionary learning framework, feature extraction is performed in a layer-wise manner: the features learned from the first layer serve as the input to the second layer, and the output of the second layer is further used as the input to the final layer.

whereis the coefficient of sparse term.andrepresent the dictionary matrix and sparse coding matrix (vector) of the first layer, respectively. The content of the dictionary matrix calculated here is different from that of. When the original data is represented by the dictionary matrix, the corresponding coding matrix (vector) is sparse.

where,represents the corresponding dictionary matrix and coding sparse matrix (vector) respectively.

Sparse regularization terms are added at the last layer to eliminate redundant features.

In the process of three-layer dictionary training, the original data from different levels is represented in low dimension. Compared to single-layer dictionary learning, deeper dictionary matrices can be learned by multi-layer dictionary learning models, resulting in a deeper representation of the original data. In other words, through different levels of dictionary learning model, the potential representation of data can be learned from different perspectives. Taking face recognition as an example, the corresponding projection matrix as well as the features can be learned from the pose perspective by the first layer dictionary learning model. The projection matrix is the dictionary matrix in the model,. Similarly, the corresponding projection matrix as well as features can be learned from the viewpoint of expressions by dictionary learning models of the second layer. Dictionary matrixtakes the form:. In the third layer, we learn the corresponding projection matrix through the model from an identity perspective. Dictionary matrixtakes the form:. Although machine learning features can be difficult to interpret, the projection matrices learned at different levels can project the original data into different attribute spaces. The whole model uses three levels of dictionary learning to represent data from different levels. The projection matrix of the corresponding attribute can be trained by representing the data from different angles, in which the corresponding dictionary matrix is trained from different levels. The meaning of the basis vectors in these dictionaries is also different.

where,,represents the dictionary matrix of three layers, respectively.Drepresents the dictionary matrix after fusion, and it retains the dictionary matrix learned from different levels. Therefore, the fusion of features vectors learned at each level is realized in the final data representation. The detailed process is shown in Fig.2. The blue and yellow solid line arrows represent the solving process of sparse and dense features, respectively.

The process of dictionary reconstruction. Blue background boxes represent two different ways of dictionary reconstruction.

After the two steps above, dictionaries at various levels are obtained through training and the dictionaries trained at each layer have been fused. Then, according to Eq.(7), the dictionary matrixDand coding matrix (vector) in the objective function are adjusted by alternating minimization method to reduce the total reconstruction error of the model.

whereLrepresents the loss function, andZrepresents the sparse coding matrix (vector).

In the pre-training stage, we adopt the neural network layer-by-layer training method to update and optimize the parameters. Since the optimization functions for each layer are non-convex and cannot be solved directly, an optimal solution is approached through multiple iterations using a traditional dictionary learning method. This is achieved by alternately fixing the dictionary matrix and the coding matrix to update the respective optimal solutions.

The solution of all features and corresponding dictionary matrices can be solved by alternating iterations. Take the first layer as an example to illustrate the solution methods for dense and sparse features and corresponding dictionary matrices, respectively.

whererepresents the regular coefficient, which means to find a norm for all columns and sum thenorm of all columns. Usingnorm as a regularization term can gain the sparse representation45.

After all layers’ training is completed, the dictionaries are reconstructed and fused. To reduce the error after reconstruction, the dictionary matrix is fine-tuned by minimizing the loss function Eq.(3), and the corresponding encoding matrix (vector) is solved. For dictionary update method, dictionary matrixDis updated by fixingZand using gradient descent method. The specific algorithm is shown in Algorithm 1, where t represents the number of iterations. The solution of coding matrix (vector) needs to fix dictionary matrixD, which can be solved by Eq.(10). The iterative procedure in Algorithm 1 terminates when the relative change in the reconstruction errorL(as defined in Eq. (7)), between successive iterations falls below a predefined tolerance, or when a preset maximum number of iterations is reached.

whererepresents another form of original data.uis the coefficient of the inter-class regular term. Inter-class regular is realized by1, 2-norm. The process is to solve the 1-norm for each row of vector of matrixZ, and then solve the2-normfor the vector composed of all1-norm. Its purpose is to maximize the distance between samples of different classes.

Due to the introduction of inter-class regular term, it is necessary to divide the original data into several groups to solve its sparse representation. If there is only one sample for each class of the original sampleX, then, andSis the same as the original sample. If there are multiple samples in each class, the original samples should be divided intongroups according to the principle of selecting one sample from each class, and the sparse coding is solved respectively.

whererepresents the dimension of sparse coding matrix.represents the optimal solution of sparse coding matrix. In the pre-training stage, the Block Gradient Descent (BCD) method is selected to update the dictionary during the optimization process of each layer. The coding matrix (vector) can be solved by Python convex optimization toolkit CVXPY and other tools, no matter it is dense features or sparse features. At the beginning of training, we need to initialize the dictionary, and then solve the convex optimization to obtain the coding matrix. Loss functions converge after a limited number of iterations by iterating the dictionary matrix and the encoding matrix alternately.

In a fine-tuning stage, different levels of dictionaries are fused for dictionary reconstruction. The dictionary matrix and the encoding matrix alternately iterate as described above, until the loss function converges.

whereis the input sample andis the corresponding coding vector.

wherecan be obtained via Eq.(13) or Eq.(14),represents the sub-vector of,lrepresents its related label, and Eq.(15) uses the cosine similarity.

To validate our method, we utilized the LMT-108 dataset46containing 108 different objects which are divided into 9 categories: 1) Meshes; 2) Stones; 3) Glossy; 4) Wood Types; 5) Rubbers; 6) Fibers; 7) Foams; 8) Foils and Papers; and 9) Textiles and Fabrics. The LMT-108 dataset provides various forms of data, including acceleration, friction, image, metal detection, IR reflection and sound. We have applied image, sound and acceleration data to our experiments. Acceleration signals are recorded by a three-axis ADXL335 accelerometer (±3g, 10kHz), and sound is captured using a CMP-MIC8 microphone (44.1kHz). Images have a resolution of 320480. The dataset contains 108 objects, each with 10 samples (totaling 1080). For evaluation, we randomly split each object’s samples in half for training and testing, ensuring no object overlaps between sets.The samples from the LMT-108 dataset are shown in Fig.3.

To verify the robustness and adaptability of DRDL, we need to conduct experiments on another more challenging dataset. We utilized the SpectroVision dataset–a multimodal collection of 14,400 paired samples capturing near-infrared (NIR) spectral measurements and high-resolution texture images (1,6001,200 pixels) from 144 household objects47. Data was gathered non-invasively using a PR2 mobile manipulator equipped with a SCiO spectrometer (740–1,070 nm range) and a 2MP endoscope camera with 12-LED ring lighting for consistent illumination. The objects spanned eight material categories: ceramic, fabric, foam, glass, metal, paper, plastic, and wood. Each object underwent 100 randomized interactions at diverse surface points/orientations (vertical: height/roll variations; horizontal: planar position sampling), ensuring real-world generalizability. This dataset enables robust material recognition without physical contact. Select 4 objects from each material for testing, totaling 32 unseen object data.

SA-x/SA-y/SA-z: Single Axis (SA) is the simplest method, which only respectively represent the acceleration signals measured on the corresponding x, y and z axes.

SoC: The implementation of Shadow o fClustering (SoC) method is relatively simple, which only adds the data from each direction.

Mag: Compared with the former two methods, the complexity of Magnitude (Mag) method is slightly increased, and its implementation process is roughly to find the sum of squares of the data measured by different methods, and then open the root. It is a commonly used method, but it will change some negative data into positive data after processing, which will make part of the process is roughly to find the sum of squares of the data measured by different methods, and then open the root. It is a commonly used method, but it will change some negative data into positive data after processing, which will make part of the original information missing.

PCA: Principal component analysis (PCA) is a widely used dimension reduction method. The main idea of PCA is to map ndimension features to k-dimension, which is a new orthogonal feature, also known as principal component, and a k-dimension feature reconstructed from the original n-dimension features.

In order to select the most appropriate method for dimension reduction, we choose the most effective method SA-z for dimensionality reduction of acceleration signal, combining with our previous work and the comparative experiment of each method in48.

Features of the corresponding three modal data are extracted, which is also a key step. Based on the analysis of previous experimental results in previous work in46, we use the optimal feature extraction algorithm to extract LBP features from the image, and extract Mel-Frequency Cepstral Coefficients (MFCC) features from the sound signal and acceleration signal.

In order to give a performance comparison of our proposed method with the traditional and common methods, ten methods are applied to our experiments.

K-SVD: The dictionary is updated by K-SVD. Orthogonal matching pursuit (OMP) is used to solve the sparse coding problem.

Support vector machine (SVM): It is a class of generalized linear classifiers that classify data by supervised learning. Its decision boundary is the maximum margin hyperplane of learning samples.

MLP: A classic feedforward neural network with at least three layers of nodes that uses backpropagation for training.

Convolutional Neural Network based on Vision (CNN-V): CNN has been used as a common method for texture recognition task in recent years due to its powerful ability to extract texture features. Considering the problem of multimodal fusion, we take the visual modal information as the input in CNN method.

Greedy Deep Dictionary Learning (GDDL)49: A deep dictionary learning method using greedy iteration without any fusion method. Experiments were carried out with visual, sound and acceleration signals as inputs.

Twin-incoherent Self-expressive Latent Dictionary Pair Learning (SLatDPL)50: A dictionary learning model integrates feature extraction and coding coefficient representation, and introduces two non-coherent local constraints with self-expression and self-adaptability.

Robust Adaptive Projective51Dictionary Pair Learning (RA-DPL): A dictionary pair learning model retains the local neighborhood relationship of intra-class sparse coding, and the learned coding has discriminative ability.

Relaxed Block-diagonal Dictionary Pair Learning with a Locality Constraint (RBD-DPL)52: Relaxed block diagonal structure is introduced to improve the discriminability of the dictionary.

One-Shot Learning Method for Texture Recognition (OSL)48: Multimodal fusion and dictionary learning are combined to solve the problem of texture recognition. The model uses only one sample as the training sample, which solves the disadvantage that the model depends on enough samples.

DRDL: We set up a 3-layer deep dictionary learning model, and get the corresponding experimental results through the test set.

Comparison of classification results of each method in LMT-108.

Comparison with Single-Layer Methods: Methods like K-SVD are based on traditional single-layer dictionary learning. While effective at capturing shallow features, they lack the ability to learn deeper, more abstract representations, which limits their accuracy to around 89%. Our DRDL model, by incorporating a deep architecture, significantly outperforms these methods.

Comparison with Standard Deep Learning: The CNN-V model, a standard deep learning approach, achieves around 90% accuracy using only visual data. This demonstrates the power of deep feature extraction. However, our DRDL model surpasses it by not only using deep learning principles but also by fusing multimodal data, providing a richer source of information.

Comparison with Other Deep Dictionary Learning Methods: The GDDL model, another deep dictionary learning approach, shows poor performance when used with single modalities. This highlights a key problem that our model solves: traditional deep dictionary learning can lose important features. Our dictionary reconstruction method explicitly addresses this by fusing features from all levels, which is critical for high performance.

Comparison with a Strong Fusion-Based Model: The OSL method, which also uses multimodal fusion, is a very strong competitor. However, it is still based on single-layer dictionary learning. Our DRDL model gains its final performance edge by combining both early multimodal fusion and a deep architecture with multi-level feature fusion. This unique combination of strategies allows our model to achieve the highest accuracy of 97.7%, demonstrating that both rich input data and a comprehensive feature hierarchy are essential for state-of-the-art performance.

To rigorously validate our performance comparisons, we conducted statistical significance tests. We performed a 10-fold cross-validation to obtain 10 accuracy scores for the top 4 comparison methods. We then used the Wilcoxon signed-rank test to compare our DRDL model against each competing method individually, with a significance level () of 0.05.

The results of the statistical analysis are presented in Table1. The p-values for the comparison between DRDL and all other methods were below 0.05. This indicates that the performance improvement of our proposed model is statistically significant and not due to random chance. This provides strong evidence for the superior feature fusion and dictionary reconstruction capabilities of the DRDL framework.

To further validate the robustness and generalizability of our proposed DRDL model, we conducted experiments on the more challenging SpectroVision dataset. We extract LBP features from texture images and only perform PCA dimensionality reduction on spectral data. As shown in Fig.5, our DRDL method once again achieved state-of-the-art performance, with an accuracy of 89.4%. It surpassed all other compared methods, including deep learning approaches like CNN-V and dictionary learning methods like OSL and K-SVD. Notably, the performance gap between DRDL and the next-best method was 3.3%, which is more pronounced than on the LMT-108 dataset. This demonstrates our model’s superior ability to learn discriminative features from complex and varied texture data.

Comparison of classification results of each method in SpectroVision.

The number of model layers will directly affect the performance of our whole model. When the number of layers is set to 1, the model is a normal single-layer dictionary learning model. We set number of layers to 1, 2, 3 and 4 respectively, and carry out experiments. Additionally, we also set up another experiment, in which only the features learned in the deepest part are utilized without using Dictionary Reconstruction in the 3-layer model. The results are shown in the Table2.

Table of comparison of the effects of different depth models.

The Table2shows that the 3-layer DRDL model has the best performance in this experiment. With the increase of the depth of the model, the performance is improving, but the performance of the 4-layer model is inferior to that of the 3-layer model. The reason is that the increase of the depth will cause the redundancy of the fused features.

It is found that increasing the number of model layers appropriately improves the accuracy of classification. As the number of layers increases, the accuracy improvement decreases. Therefore, it is particularly important to select the appropriate number of layers. Experimental results show that if only considering deep features we fail to achieve the best results, and our proposed DRDL is superior. Because the learning characteristics of all layers are considered in the reconstruction stage, the depth of the model can affect the training speed of the model. As the number of model layers increases, so does the training time. According to the comprehensive consideration of the recognition performance and computational cost of the algorithm, the number of dictionary layers is set to 3 in our following experiments.

The LMT-108 dataset contains objects of nine different materials. Therefore, we have conducted further experiments to test the effect of applying the methods of this paper on surface texture classification of different materials.

In53, the author proposed an ensemble learning method (ELM) with optimized features for multimodal surface material recognition. The method proposed in this paper is compared with the method proposed in53. In this comparative experiment, in order to be consistent with the sample number setting in the experiment in53, the sample number in both the training set and the test set is set to 1080.

The experimental results of the material identification task are shown in Fig.6, and the recognition accuracy is 0.978. The comparison of the experimental results of the two methods is shown in Table3. For all materials, our proposed DRDL for material identification accuracy rate is higher than EML, except for meshes.

Confusion Matrix in Material Identification Tasks Using DRDL. Numbers on the diagonal represent the number of correct classifications, while numbers off the diagonal represent the number of incorrect classifications.

Table of accuracy of methods materials identification task.

We analyzed the effects of parameters of the results, as shown in Fig.7. We performed a joint analysis of parametersand, set from 0.0001 to 100, respectively. In Fig.7, the-axis represents accuracy, while theandaxes areand. When it comes to the other experiments,andare set to 0.15 and 1.8, respectively, and the dictionary sizes of the three-layer model are set to 196, 196 and 256, respectively.

The influence ofandon the classification accuracy, in which the two coordinates of the horizontal plane areandrespectively.

We performed three ablation sub-experiments, including Experiment A, Experiment B and Experiment C. The details are as follows: 1) Experiment A: We separate the multi-modal fusion method from the DRDL, and individually took three kinds of data modes, including images, sound and acceleration, as the input of the model, which were marked as Experiment A-V, Experiment A-S and Experiment A-A respectively. 2) Experiment B: We separate dictionary reconstruction and fine-tuning stages from the DRDL. 3) Experiment C: We separate the fine-tuning stages from the DRDL. The final results are shown in Table4.

DRDL takes 10.57s to classify the test set containing 540 samples. The execution time of all compared methods for classifying test samples is shown in Table5.

Execution time for classifying test samples by different methods.

Compared with CNN-V, MLP and SVM, our DRDL involving dictionary learning requires longer time to classify test samples, because it uses CVXPY to solve a convex optimization problem for dictionary learning. Compared with the single-layer dictionary learning method i.e., OSL and K- SVD, the deep dictionary learning method needs to consider the deep-level characteristics, which leads its longer execution time. Compared with GDDL, a greedy iteration method without any fusion, our DRDL needs longer time because it considers characteristics of multiple layers, which achieves a big increase of accuracy, as shown in Fig.7.

Future work will focus on the following detailed strategies to improve its efficiency and make it more suitable for real-time applications.

Faster Solver with ADMM: Instead of the general-purpose CVXPY, we’ll use the Alternating Direction Method of Multipliers (ADMM). ADMM splits the lasso problem into smaller subproblems that can be solved (often analytically) in parallel, cutting down runtime significantly.

Dictionary Pruning: Once the dictionaryDis trained, many atoms may be redundant. We’ll rank atoms by how often and how much they reduce reconstruction error, then drop the weakest ones. A smaller dictionary speeds up sparse coding by reducing the number of variables.

Quantization for Deployment To save memory and boost speed on limited hardware, we’ll convert 32?bit floats in the dictionary and features to lower precision (e.g. 16?bit floats or 8?bit ints). This slashes storage needs and taps into faster, hardware?accelerated matrix operations.

In our experiments, all methods are implemented in Python 3.8 on a computer platform (2.6-GHz CPU and 16-G RAM).

Although our proposed DRDL model demonstrates state-of-the-art performance, it is important to acknowledge its limitations and consider the context of its evaluation.

The primary limitation of the DRDL model is its computational complexity. The multi-stage process, which includes layer-by-layer pre-training, dictionary reconstruction, and fine-tuning, is inherently more complicated than end-to-end deep learning models or single-layer dictionary methods. The use of CVXPY to solve the convex optimization problem for sparse coding is a significant bottleneck. While this trade-off yields higher accuracy, the model’s current implementation may not be suitable for real-time applications where low latency is critical.

While our evaluation used two established texture recognition benchmarks, their controlled laboratory acquisition may limit the model’s robustness in real-world settings with variable lighting, background clutter, or dynamic contact conditions.

This paper presents a dictionary-reconstruction-based deep learning approach (DRDL) that effectively combines dictionary learning and deep learning. By learning the features of different levels and fusing the dictionaries of different levels, the features learned of the data at different levels are well fused. In this paper, a texture recognition experiment is carried out by using the LMT-108 and SpectroVision dataset combined with multi-modal integration. Our newly proposed method has achieved the highest the highest after its comparison with the state-of-the-art methods. The consistent high performance across two distinct and challenging datasets confirms the robustness and superior feature fusion capabilities of our model.

This work uses visual, sound and acceleration information as the input of the algorithm in our experiments. The uncertainty in pattern recognition is reduced by early multimodal integration and later dictionary reconstruction. Our model takes both intuitive and deep features into account. Therefore, it can be easily extended to perform other related tasks, e.g., expression recognition, face recognition, gestural recognition, and identity recognition. Taking an identity recognition task as an example, the model can fuse data of different modalities, and then learn different levels of features, such as posture and expression. It can potentially improve the accuracy of a recognition task.

Although our proposed DRDL method has achieved state-of-the-art performance in texture recognition tasks, several directions warrant further exploration in future work, including: improving computational efficiency; expanding application domains; and investigating more advanced dictionary fusion strategies.

This work was supported by the National Natural Science Foundation of China (Grant Nos. 62373181, 62163024 and 61903175); and in part by Jiangxi Science Fund for Distinguished Young Scholars (Grant Nos. 20232ACB212002); and in part by Jiangxi Double Thousand Plan Project (Grant Nos. jxsq2023201097); and in part by Jiangxi Key R&D Program Project (Grant Nos.20252BCE310017); and in part by National Key R&D Program of China (Grant Nos.2023YFB4704900).