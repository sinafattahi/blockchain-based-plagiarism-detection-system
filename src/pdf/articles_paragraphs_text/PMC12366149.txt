Calling structural variants (SVs), i.e., genomic alterations of50bp, from whole genome short-read data remains challenging, as existing callers are known to lack accuracy and robustness. Therefore, meta-caller approaches combining the results of multiple standalone tools in a consensus set of reported SV calls, are widely used. Here, SV-MeCa (Structural Variant Meta-Caller) is presented, the first SV meta-caller incorporating variant-specific quality metrics from individual VCF outputs, rather than relying solely on number and combination of tools supporting consensus SV calls. In addition, SV-MeCa offers a suitable score to rank obtained consensus SV calls according to evidence of representing true positive calls, i.e., real-world variants.

SV-MeCa applies seven standalone SV callers and merges resulting deletion and insertion calls into a union VCF file using SURVIVOR. For each entry in the SURVIVOR-generated consensus, caller-specific quality measures are extracted from corresponding standalone VCF files, and serve as input for an either deletion- or insertion-specific XGBoost decision tree classifier, which was previously trained on the HG002 SV benchmark data provided by the Genome in a Bottle consortium. The SV-MeCa XGBoost models assign a probability to (consensus) SV calls to represent true positive calls, which can be used for ranking the final output according to evidence. Performance of SV-MeCa and four previously published meta-caller approaches were evaluated based on autosomal SV calls in samples curated by the Human Genome Structural Variation Consortium, Phase 2. With regard to Fscores, which were 0.58 on average for deletions and 0.42 on average for insertions, SV-MeCa outperformed the other meta-callers. With regard to precision, only ConsensuSV achieved higher values (0.97 versus 0.64 on average for deletions, 0.75 versus 0.53 on average for insertions), and with regard to recall, SV-MeCa was outperformed exclusively by Meta-SV for deletions (0.55 versus 0.53).

SV-MeCa, publicly available athttps://github.com/ccfboc-bioinformatics/SV-MeCa, outperforms existing SV meta-caller approaches by taking variant-specific quality measures into account. Moreover, due to the XGBoost prediction probabilities serving as scores, the output of SV-MeCa can be continuously adjusted to user needs in terms of sensitivity and precision.

The online version contains supplementary material available at 10.1186/s12859-025-06246-6.

Structural variants (SVs) are usually defined as genomic alterations of the types deletions, duplications, insertions, inversions, or translocations affecting50bp [1]. Detection of SVs based on short-read data requires more sophisticated algorithms compared to single nucleotide variants and short indels discovery [2,3]. Current approaches typically pursue one or a combination of four main strategies. Read depth- or read count-based approaches are commonly used for the identification of copy number variants (CNVs), i.e., large deletions or duplications having significant effects on observed read depth in the affected genomic region. On the other hand, read-pair methods consider insert-size and orientation of paired-end reads, whereas the split read-based approaches identify the start and stop positions of putative SVs by scanning for partially mapped (i.e., split) reads. Furthermore, SVs can also be detected based onde novoassembly.

Apart from the inherent limitations of short-read sequencing, the accuracy and robustness of current SV callers limit our ability to discover SVs effectively. Thus, we are not able to detect the full range of existing SVs [2–5]. Meta-caller approaches, combining the results of multiple standalone tools have been developed and shown to achieve improved performance [2,6] in this regard. However, all the currently available approaches applicable to short-read whole-genome sequencing (WGS) data have their specific drawbacks and limitations. MetaSV expects already generated VCF files as input, leaving the execution of standalone SV callers to the responsibility of the user [7]. Furthermore, it is unable to extract insertion calls properly from VCF input (except for Pindel [8]) and therefore includes its own split read- and assembly-based method for insertion detection, which in turn makes the intended meta-approach obsolete. In contrast, Parliament2 [9] and ConsensuSV [10] provide ready-to-use SV meta-caller pipelines that start from raw FASTQ files and allow an SV meta-calling with minimal user effort. Both tools, however, predominantly assess the evidence of an SV call being true positive (TP) based on the consistency with which it is detected by multiple standalone callers or their combination. Parliament2 reports quality values of consensus SV calls based on pre-computed precisions per combination of involved standalone SV callers, SV type and SV length range [9]. ConsensuSV considers and reports only overlapping SV calls of the same type originating from at least a pre-defined number of standalone callers [10]. Similarly, FusorSV uses pre-computed cut-offs per SV caller combination and SV length range to determine the final output of consensus SV calls [11]. Also, the recently published VISTA tool creates the final output from the union of all SV calls of a pre-defined set of standalone callers per SV type and length range [12]. All these strategies share the common fundamental limitation that individual SV calls enter the process of building a final consensus list of putative SVs as virtually binary encodings of their presence. SV calls from only one or very few standalone tools are generally considered to have a low probability to represent TP events, regardless of the qualities assigned to them. Inversely, consistent calls from many approaches are always associated with a high probability of representing TPs, even if consistently reported as low confidence calls.

Here we present SV-MeCa (Structural Variant Meta-Caller), which overcomes these limitations of consensus SV call quality assessment by incorporating variant-specific quality metrics from individual SV caller’s VCF outputs. In addition, SV-MeCa provides a suitable and continuous score to rank consensus SV calls according to evidence of representing TP events.

Starting from a paired-end short-read WGS BAM file including split read information, SV-MeCa incorporates seven SV callers, namely BreakDancer [13], Delly [14], LUMPY [15], Manta [16], Pindel [8], TARDIS [17,18], and INSurVeyor [19] (Fig.1). Resulting VCF files are filtered for insertions (including duplications) and deletions with length50bp. Subsequently, deletions reported by LUMPY, Manta, Delly, Pindel, and TARDIS, and insertions reported by LUMPY, Manta, Delly, TARDIS, and INSurVeyor are merged into a single VCF file using SURVIVOR [20]. For each entry in the SURVIVOR-generated consensus, caller-specific quality measures are extracted from the corresponding VCF files and serve as input for an either deletion- or insertion-specific XGBoost (Extreme Gradient Boost) decision tree classifier [21]. These pre-trained XGBoost models assign probabilities to (consensus) SV calls to represent TPs, which are then used for the ranking of the final output.

We employed the HG002 (NA24385) benchmark data provided by the Genome in a Bottle (GIAB) consortium [22] and SVs identified in the children of three parent–child trios (HG00514, HG00733, NA19240) by the Human Genome Structural Variation Consortium, Phase 2 (HGSVC2) [23,24], Freeze 3, as ground truths for model development and evaluation. Details on data preparation are given in Supplementary Methods.

For the creation of the training data, the GIAB HG002 Tier1 v0.6 SV benchmark [22], including curated SVs in challenging medically relevant genes [25], was restricted to autosomal variants with length50bp in heterozygous or homozygous genotype. We aligned 250bp Illumina paired reads of HG002 against the GRCh37 reference genome, resulting in a mean coverage of 52x. In order to make it more consistent with scientific and clinical practice, we downsampled the mean coverage to 35x.

For evaluation, we employed data mapped to the GRCh38 reference genome exclusively. To assess performance of standalone SV callers and SV-MeCa for SVs in challenging medically relevant genes [25], we used 150bp paired-end PCR-free WGS data, which was already downsampled to approximately 30x and provided by the Human Pangenome Reference Consortium1(HG002-CMRG).

In addition, we retrieved paired-end PCR-free WGS data from HG00514, HG00733, and NA19240 from the 1000 Genomes International Genome Sample Resource (IGSR) data portal2to compare the performance of SV-MeCa against standalone SV callers and existing meta-caller approaches on autosomal chromosomes. Mean coverages of HG00514, HG00733, and NA1940 ranged between 30x and 31x, and read length was 150bp.

Various tools for SV calling based on short-read WGS data in SAM/BAM format were eligible for inclusion in the SV-MeCa workflow. We chose the established tools BreakDancer [13], Delly [14], LUMPY [15], Manta [16], Pindel [8], and TARDIS [17,18], as well as INSurveyor, which was specifically developed for calling insertions [19]. We merged individual SV calls of identical type into consensus calls using the merge utility of SURVIVOR v1.0.7 [20]. A short description of each tool, settings for the SV calling procedure and running SURVIVOR, are given in Supplementary Methods.

Finally, for XGBoost classifier training, we compared SURVIVOR’s output against the GIAB HG002 benchmark VCF using the Truvari v3.5.0 bench utility [26] with default parameters, but restricted to autosomes, specifically autosomal regions specified in GIAB Tier v0.6 BED, and setting –pctseq to 0 to avoid sequence comparisons.

and #TP denoting the number of TP, #FP the number of false positive, and #FN of false negative predictions. Therefore, we used quality metrics retrieved from the VCF output of BreakDancer, Delly, LUMPY, Manta, Pindel, and TARDIS only for the deletion-specific model, and of Delly, Manta, Pindel, TARDIS, and INSurVeyor only for the insertion-specific model.

We provide details on extraction and transformation of features that are retrieved from SURVIVOR and individual VCF output files in Supplementary Methods, and a summary of all covariates considered in model development in Supplementary Table S1.

The selection of features to be incorporated into the XGBoost classifiers aimed to provide a model applicable to input data within a broad range in terms of read lenghts and observed sequencing coverages.

To achieve this, we normalized the quality metrics that refer to (local) coverage or counts of supporting reads by observed mean coverage (Supplementary Table S1). Further, we also normalized the average edit distances of reads supporting insertion calls (so-called stable reads), as reported by INSurVeyor, by read length.

To assess the dependence of reported quality scores on mean coverage, we analyzed corresponding values from concordant calls based on the 52x and the 35x 250bp HG002 sample (see Supplementary Methods, Supplementary Figures S1 and S2 for details).

Finally, we applied a correlation-based feature selection to determine the initial set of covariates to be considered for the SV-MeCa XGBoost classifier training (Supplementary Methods).

This filtering strategy resulted in a set of 35 covariates as initial input for XGBoost training of the deletion-specific SV-MeCa model, namely 3 derived from BreakDancer’s VCF output, 10 from Delly, 7 from LUMPY, 7 from Manta, 3 from PINDEL, and 4 from TARDIS. To train the insertion-specific SV-MeCa model, 36 features were initially included, 6 of which were specific to Delly, 18 to INSurVeyor, 6 to Manta, 3 to Pindel, and 2 to TARDIS (Supplementary Table S1).

For each SV type, two classifier models were trained on autosomal 35x GIAB HG002 data and evaluated, namely the final SV-MeCa XGBoost model, hereafter referred to as full model (FM), including the final subset of covariates to consider, and a basic model (BM) incorporating SV length and binary encoding of the presence of corresponding SV calls per standalone caller exclusively, to demonstrate the benefit of incorporating VCF-derived, SV call-specific quality metrics.

Python implementation of XGBoost Classifier,3an ensemble learner of decision trees, was chosen as supervised learning approach and subsequent predictor due to its scalability, robustness and handling of sparse data [21]. The algorithm works by iteratively fitting decision trees on the residuals of the previous tree, and adjusting the weights of the misclassified samples to prioritize their correct classification in subsequent trees. For the final prediction, the results of individual trees in the entire ensemble are combined.

We employed the exact greedy method with default gbtree booster for individual tree construction, and according to the aim of generating a binary classifier, we used binary logistic regression as objective. Details on hyperparameter tuning are given in Supplementary Methods.

We evaluated model performances based on precision, recalland(Eq.1). SV calls were predicted as positive, if the classifier’s prediction probability was0.5. For calculating #FN, also those SVs were included that were not called by any of the standalone callers applied.

If covariates in trained FMs showed zero feature importance (due to the feature_importances_ attribute), which is defined as the relative contribution of the corresponding feature to the model (gain) per default, we repeated hyperparameter tuning and model training with the corresponding covariates discarded.

SV-MeCa is implemented as a Nextflow v23.10 [27] pipeline, and uses a conda v23.1.0 environment to ensure consistent package management. It is available as a Docker container, ensuring streamlined deployment, portability, and reproducibility.

SV-MeCa expects BAM or VCF files as input. Given BAM files, SV-MeCa employs the CollectWGSMetrics utility of GATK v4.3 to assess read coverages. Read lengths are retrieved from BreakDancer’s CFG output. If VCF files are given as input, mean coverage and read length are required as additional arguments.

SV-MeCa provides one VCF per executed SV caller and a final consensus VCF as output. In the consensus VCF, prediction probabilities, i.e., final ranking scores, are encoded as QUAL values. Furthermore, to enable straightforward lookup of originating calls in standalone VCFs, all identifiers contributing to a joined call are listed in the final consensus VCF.

We evaluated the performance of SV-MeCa and four other meta-caller approaches for SV calling, namely ConsensuSV [10], MetaSV [7], Parliament2 [9], and recently published VISTA [12], using the samples HG00514, HG00733, and NA19240 from HGSVC2, considering precision, recall andscore. Details on execution of the meta-caller approaches can be found in Supplementary Methods. We were unable to resolve the dependencies of the requirements for running FusorSV [11]. Thus, FusorSV could not be included in our evaluation. Further, the VISTA framework provides only pre-trained models and corresponding scripts for deletions, and hence, was only included in our evaluation for deletions.

From the GIAB HG002 reference, 9264 SVs (4051 deletions and 5213 insertions) passed all preprocessing filters and were further considered in SV-MeCa model development and evaluation. Considering the 35x HG002 sample, 20.83% (844/4051) of the deletions and 51.69% (2695/5213) of the insertions were not included in the SURVIVOR-generated union of the results of all standalone SV callers under investigation (Table1). All SV callers performed noticeably better for deletions than for insertions, withvalues for deletions ranging from 0.33 (BreakDancer) to 0.78 (Manta) and for insertions from <0.01 (BreakDancer) to 0.31 (Pindel), respectively 0.50 for the insertion-only caller INSurVeyor.

Considering the 52x HG002 sample, 18.04% (731/4051) of the deletions and 44.91% (2341/5213) of the insertions were not included in the SURVIVOR-generated union of the results of all standalone SV callers under investigation (Supplementary Table S2). However, besides #TP, also the amounts of false positive calls in the SURVIVOR union increased with the increase in sequencing coverage from 35x to 52x, namely from 23.16% (967/4174) to 50.11% (3335/6655) for deletions and from 33.24% (1254/3772) to 56.64% (3751/6623) for insertions (Table1and Supplementary Table S2), resulting in decreasedvalues (for deletions: 0.78 versus 0.62, for insertions: 0.56 versus 0.49).

We show hyperparameters retrieved by grid search in Supplementary Table S3. For the deletion-specific FM, a feature importance of zero was reported for 4 of the 35 covariates initially included in XGBoost training, as well as for 2 of the 36 covariates initially included in the insertion-specific FM training (Supplementary Table S1). Omitting these features did not affect the results of hyperparameter tuning (Supplementary Table S3), nor training accuracies (Supplementary Table S4).

Tables2and3present comprehensive summaries of the features considered for each standalone SV caller in the final SV-MeCa approach per SV type.

Within the 31 covariates of the deletion-specific FM, the (normalized) quality values QUAL from Manta showed by far the highest feature importance, i.e., relative contribution to the model, directly followed by (normalized) counts of paired reads provided by Delly, variant fraction estimates from Pindel, and allele balance reported by LUMPY (Supplementary Figure S2). The covariate with the highest contribution provided by TARDIS, namely reported quality CNVL, was ranked seventh. The SV length and all features retrieved from BreakDancer were listed within the eight features with lowest feature importances.

Of the 34 covariates of the insertion-specific FM, INSurVeyor provided the two features with highest feature importances, directly followed by Manta’s (normalized) count of supporting paired reads, TARDIS’ (normalized) count of supporting split reads, and Pindel’s local coverage estimate (Supplementary Figure S3). The feature importances of the four covariates provided by Delly were consistently below the one of SV length, which was ranked 12th.

The number of reference deletions and insertions (including duplications) per HGSVC2 benchmark sample (Table4) was more than double compared to the HG002 GIAB reference (Table1).

Values ofachieved by the SURVIVOR union (Table4) of deletion calls retrieved from BreakDancer, Delly, LUMPY, Manta, Pindel, and TARDIS ranged from 0.39 to 0.43. Values ofachieved by the SURVIVOR union of insertion (including duplication) calls retrieved from Delly, INSurVeyor, Manta, Pindel, and TARDIS ranged from 0.27 to 0.30. Numbers of exclusive calls, i.e., TP calls that were uniquely identified by a single tool, are shown for HGSVC2 reference samples in Supplementary Figure S5.

The BMs and the final FMs included in SV-MeCa were applied to the deletion and insertion calls in the SURVIVOR union per HGSVC2 sample in order to evaluate XGBoost model performances. The BMs achieved model accuracies between 0.71 and 0.72 (AUC = 0.90 – 0.91) for deletions and between 0.63 and 0.65 (AUC = 0.85 – 0.87) for insertions (Supplementary Table S5). In contrast, the FMs incorporated into SV-MeCa achieved accuracies ranging from 0.79 to 0.81 (AUC = 0.93 each) for deletions and from 0.76 to 0.80 (AUC = 0.88 – 0.89) for insertions. Corresponding ROC curves are shown in Fig.2.

Overall performance metrics of the BMs and the FMs under inclusion of the entire set of false negative calls, i.e., reference SVs that were not detected by any of the incorporated standalone callers, are visualized in Fig.3and listed in Supplementary Table S6. SV-MeCa’s FMs showed slightly decreased recall, but outperformed the BMs with regard to precision and.

We compared the performance of SV-MeCa with its incorporated standalone callers using two datasets, namely SVs in challenging medically relevant genes in HG002, HG002-CMRG, and SVs in autosomes of samples HG00514, HG00733, and NA19240 (Fig.4, Supplementary Table S7). For deletions, Manta, LUMPY, and TARDIS achieved consistently higher precision values than SV-MeCa, as well as Delly for HGSVC2 reference samples. For insertions, SV-MeCa was outperformed by INSurVeyor und Manta, and for HG002-CMRG also by Delly. With respect to recall, SV-MeCa consistently outperformed all standalone caller approaches under consideration. Regardingscores, Manta achieved slightly higher values than SV-MeCa for deletions in the HGSVC2 reference samples (0.60 on average versus 0.58 on average), but was outperformed by SV-MeCa for deletions in the HG002-CMRG reference (0.55 versus 0.58). For insertions, SV-MeCa consistently achieved highervalues than all standalone caller approaches under consideration.

A key feature of SV-MeCa is the provision of quality values, which are retrieved from the XGBoost predicition probabilites and can be used to sort SV calls according to evidence to represent TPs. To evaluate the performance of resulting rankings, we considered SV-MeCa calls for samples HG00514, HG00733, and NA19240 per decile of assigned XGBoost prediction probabilities. Between 5550 and 5939 deletion calls per sample were assigned a probability <0.1, corresponding to 32.5134.32% of all calls (Fig.5, Supplementary Table S8). The proportion of TP calls was 0.02 for each sample. The second most frequently assigned probability decile was the highest one with between 4038 and 4817 calls, corresponding to 23.9526.37% of all deletion calls. In this subgroup, the proportion of TP calls was 0.86 for each sample.

The lowest probability decile was also the most frequently assigned regarding insertions, with 5000 to 7185 calls per sample, corresponding to 20.7725.74% of all calls (Fig.5, Supplementary Table S9). The proportion of TP calls in this subgroup ranged from 0.02 to 0.03. The highest probability decile comprised between 3557 and 4236 calls (13.0115.19%). In this subgroup, the proportion of TP calls ranged from 0.77 to 0.79.

We compared the performance of SV-MeCa with respect to recall, precision, andagainst ConsensuSV [10], MetaSV [7], Parliament2 [9], and VISTA [12] (deletions only) on samples HG00514, HG00733, and NA19240 (Fig.6).

ConsensuSV consistently achieved the highest precision, with 0.960.97 for deletions and 0.730.77 for insertions (Supplementary Table S10). On the other hand, MetaSV achieved the highest recall for deletions (0.55 for each sample), whereas for insertions, SV-MeCa outperformed the other meta-callers with recall 0.35 on average. Finally, based on thescore, SV-MeCa was superior to all the other meta-callers for both deletions (0.58 for each sample) and insertions (0.42 on average).

We presented SV-MeCa, which is to our knowledge the first meta-caller approach for SVs that incorporates individual, VCF-retrieved quality metrics into a comprehensive model for generation of the final list of consensus calls. We demonstrated the benefit of the introduced approach by comparing the results of SV-MeCa to the results of a basic XGBoost model, considering caller agreement and SV length exclusively. Further, SV-MeCa outperformed existing SV meta-caller approaches consistently with respect to.

Based on the prediction probabilities assigned by deletion- and insertion-specific XGBoost classifiers, SV-MeCa provides a score for each SV call that indicates the evidence of representing a TP, real-world finding. The models may also allow conclusions to be drawn about which metrics must be prioritized from VCF output of established SV callers in order to distinguish TP and artifact calls as reliably as possible. For example, information about microhomologies was consistently included for both SV types (Tables2and3), again emphasizing the role of microhomologies as prominent mediators of SV generation [28]. Further, Delly’s QUAL (at least indirectly as binary encoding of the PASS flag) and TARDIS’ CNVL values were consistently considered for both deletion and insertion calls, indicating the general ability of these metrics to appropriately aggregate the overall evidence for a TP SV call. A comprehensive investigation of VCF-retrieved quality metrics and their contribution to the assement of SV calls could be the subject of a future study.

It is undisputed that SV calling on short-read data is principally limited. A comprehensive evaluation by the HGSVC2 consortium revealed that approximately 30% of SVs detectable in long-read sequencing data could not be identified in Illumina short-read data [24]. Due to the known limitations, it is often argued that meaningful SV calling can and should only be applied to so-called 3rd generation, i.e., long-read sequencing data. However, in large-scale sequencing projects, and particularly in routine clinical diagnostics, short-read sequencing still represents the standard for reasons of cost, efficiency, and expertise. Nevertheless, it should be noted that consistently more than 40% of the known SVs in our benchmark data were not identified by any of the integrated standalone SV callers, and thus, contribute the vast majority of observed #FNs, rather than those misclassified by the SV-MeCa model. However, the introduced approach allows for the easy inclusion of additional (future) SV callers for recall improvement, while controlling the rate of false positives.

The development of SV callers is principally hampered by limited availability of appropriate benchmark data. All standalone SV callers included in SV-MeCa, as well as the meta-callers against which we compared its performance, were also developed and evaluated, at least in part, on the basis of data from 1000 Genomes samples. Therefore, overfitting effects can not be ruled out, besides a bias towards recurrent SVs in high confidence regions. With the rise of WGS in routine diagnostics, we will be able to broaden the data basis for SV-MeCa model training as well as its evaluation by real-world data from clinical practice. The robustness of future SV-MeCa models will benefit in particular from more variable mean coverages and read lengths in training data. The current training data from GIAB HG002 is characterized by its high quality and quite long reads (2x250), which might be uncommon in practice. The following further improvements of the SV-MeCa approach are aimed for by the developers: In addition to its extension to further SV types, such as inversions and genomic rearrangements, the objective is to assess whether the development and application of specific models for insertions and duplications will further improve SV-MeCa’s performance. Finally, we are aware that the accuracy of SV meta-callers crucially relies on the accuracy of the tools employed for SV comparison and merging. Limitations of existing tools have been reported consistently [29]. Therefore, we plan to evaluate if and how SV-MeCa’s performance may be further improved by employing alternative approaches to SV merging.

SV-MeCa, which is publicly available athttps://github.com/ccfboc-bioinformatics/SV-MeCaas a ready-to-use Docker container, outperforms existing SV meta-caller approaches by taking variant-specific quality metrics into account. These metrics are retrieved from individual VCF outputs of seven incorporated SV callers. The XGBoost prediction probabilities serving as scores in SV-MeCa’s VCF output, provide a suitable filter criterion to adjust the final consensus of SV calls straightforwardly and continuously to user needs in terms of sensitivity and precision.

We thank the ITCC (IT Center University of Cologne) for providing compute resources on the DFG-funded HPC (High Performance Computing) system RAMSES (Research Accelerator for Modeling and Simulation with Enhanced Security) as well as support (DFG funding number: INST 216/512-1 FUGG).