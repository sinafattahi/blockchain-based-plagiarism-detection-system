Trauma-induced rib fractures are common injuries. The gold standard for diagnosing rib fractures is computed tomography (CT), but the sensitivity in the acute setting is low, and interpreting CT slices is labor-intensive. This has led to the development of new diagnostic approaches leveraging deep learning (DL) models. This systematic review and pooled analysis aimed to compare the performance of DL models in the detection, segmentation, and classification of rib fractures based on CT scans.

A literature search was performed using various databases for studies describing DL models detecting, segmenting, or classifying rib fractures from CT data. Reported performance metrics included sensitivity, false-positive rate, F1-score, precision, accuracy, and mean average precision. A meta-analysis was performed on the sensitivity scores to compare the DL models with clinicians.

Of the 323 identified records, 25 were included. Twenty-one studies reported on detection, four on segmentation, and 10 on classification. Twenty studies had adequate data for meta-analysis. The gold standard labels were provided by clinicians who were radiologists and orthopedic surgeons. For detecting rib fractures, DL models had a higher sensitivity (86.7%; 95% CI: 82.6%-90.2%) than clinicians (75.4%; 95% CI: 68.1%-82.1%). In classification, the sensitivity of DL models for displaced rib fractures (97.3%; 95% CI: 95.6%-98.5%) was significantly better than that of clinicians (88.2%; 95% CI: 84.8%-91.3%).

DL models for rib fracture detection and classification achieved promising results. With better sensitivities than clinicians for detecting and classifying displaced rib fractures, the future should focus on implementing DL models in daily clinics.

Three types of DL models are described: detection, segmentation, and classification. A detection model often uses a bounding box or only notes the presence of a rib fracture, without further spatial information. Segmentation models provide more spatial information by delineating the contour of rib fractures. In this review, a model was marked as a segmentation model when the segmentation was centered around the fracture, instead of segmenting the entire ribcage. Classification models classify rib fractures into various classes or based on the presence of fractures.

Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed in this review.19A systematic literature search was performed on March 27, 2023, using the following databases: Medline ALL, Embase, Web of Science Core Collection, Cochrane Central Register of Controlled Trials databases, and an additional search in Google Scholar. The search strategy was formulated and performed using a librarian (supplemental digital content (SDC) 1,http://links.lww.com/JTI/A334). This protocol was developed before this study.

The inclusion criteria were publications that contain: (1) rib fractures, (2) CT data, and (3) description of a DL model for imaging. The exclusion criteria were publications that were commentaries, conference abstracts, studies not conducted on living humans, nonoriginal studies, and studies not published in English or Dutch. After removing duplicates, titles and abstracts of potentially eligible records were independently screened by 2 reviewers (N.B. and S.D.H.). Subsequently, full-text screening was performed using the inclusion criteria to check for eligibility. A third reviewer (M.M.E.W.) was consulted in cases in which the 2 reviewers did not reach an agreement until a consensus was reached.

Two reviewers (N.B. and S.D.H.) independently assessed the quality of all included articles according to a modified version of the Methodological Index for Non-Randomized Studies (MINORS).20The modified version of MINORS includes the following items: study aim, data selection/input features, (type of) AI model, external test data set, gold standard, demographic report, data set distribution, and adequate statistical analysis/performance metric. The items were scored 0 (not reported) or 1 (reported), with a maximum score of 8. In case of disagreement, a consensus meeting was organized, in which inconsistencies were resolved under the supervision of a third reviewer (M.M.E.W.).

Two reviewers (N.B. and S.D.H.) independently extracted the data from the included studies. The extracted data for each study were year of publication, number of patients and fractures, type of AI model used, train-validation-test split of the data set, presence of testing on an external data set (yes/no/not reported), reports about demographics (yes/no/not reported), data selection performed (yes/no/not reported), number of different CT scanners used, slice thickness (in mm), and gold standard label assignment.

The primary outcomes for the detection DL model were performance metrics such as sensitivity, precision, false positives (FP) per patient with the rib fracture as a 3D object or per slice with the rib fracture as a 2D image, F1-score, accuracy, and mean average precision (mAP). The mean sensitivity with SD was calculated. The outcome measures for the segmentation models were sensitivity, specificity, Dice Similarity Coefficient (DSC), and Intersection-over-Union (IoU). The classification models were evaluated for sensitivity, precision, F1-score, and accuracy. All outcome measures were taken from the test set whenever possible. If the sensitivity, precision, or specificity was not given, confusion matrices were noted, and the mentioned values were calculated (SDC 2, Supplemental Digital Content 1,http://links.lww.com/JTI/A334).

To benchmark the accuracy of the DL models, the outcome parameters obtained by the clinicians were extracted from the studies.

The search identified a total of 495 records. After removing duplicates, 323 records were screened on title and abstract. Thirty-six records were eligible for the full-text screening. This resulted in 25 studies meeting the inclusion criteria14,23-46(Fig.1). Twenty studies were included in the meta-analysis14,23,25,27–34,36–42,44,45and 5 were excluded because of a lack of data.24,26,35,43,46The included studies scored between 4 and 8 points on the MINORS quality assessment (SDC 3, Supplemental Digital Content 1,http://links.lww.com/JTI/A334). Four studies were labeled as low-quality, with a score of 4 (23 to 26).

The commercially available model was trained on this data set.

Images of rib fractures; multiple images can exist for one rib fracture.

For the detection, a data set of 105 patients was used.

Number of radiologists depended on the test set used.

No explanation of where the external data comes from.

For the detection of rib fractures, 43,834 patients from 21 studies were included14,23-38,40-44,46(SDC 4, Supplemental Digital Content 1,http://links.lww.com/JTI/A334). These studies reported sensitivities ranging from 0.645 (27) to 0.971 (44). The pooled proportion for sensitivity was 86.7% (95% CI: 82.6%-90.2%) and that of the clinicians was 75.4% (95% CI: 68.1%-82.1%; Table2; SDC 7, Supplemental Digital Content 1,http://links.lww.com/JTI/A334). The lack of overlap between the 2 CIs implies that the DL model performs significantly better than clinicians in detecting rib fractures. The confidence intervals for sensitivity when comparing the external data set to clinicians show a pooled proportion of 86.6% (95% CI: 81.7%-90.8%) and 78.0% (95% CI: 68.9%-85.8%), respectively (Table3; SDC 8, Supplemental Digital Content 1,http://links.lww.com/JTI/A334). These confidence intervals do overlap; the difference in performance between the detection DL model and clinicians remains undecided. Two studies did not mention the sensitivity of their model but reported mAP values of 42.3% (23) and 89.2% (25). When sorting the studies on highest sensitivity, there is no correlation between superior performance DL models and type of DL algorithm employed, MINORS score of the studies, or distribution of patients across the training, validating, and testing data set.

See SDC 7, Supplemental Digital Content 1,http://links.lww.com/JTI/A334for forest and funnel plots.

See SDC 8, Supplemental Digital Content 1,http://links.lww.com/JTI/A334for forest and funnel plots.

Eighteen studies reported either on FPs per scan or the F1-score, ranging from 0.14 (29) to 2.71 (36) and 0.652 (38) to 0.97 (35), respectively. The precision values ranged from 0.602 (38) to 0.96 (35), and the accuracy from 0.814 (24) to 0.96 (37). The reported mean with SD of clinicians’ sensitivity, precision, and F1-score were 0.74±0.12, 0.91±0.07, and 0.85±0.02, respectively.

Eleven out of 19 studies reported higher mean sensitivity for the DL model than for clinicians.14,24,29,30,32,34,35,37,40,43,46One study showed a lower sensitivity for rib fracture detection using a DL model than clinicians, 0.794 versus 0.834 (95% CI: 80.7-85.8; 33).

For the segmentation of rib fractures, 2578 patients from 4 studies were included39,42,43,45(SDC 5, Supplemental Digital Content 1,http://links.lww.com/JTI/A334). These studies showed sensitivities ranging from 0.813 (43) to 0.95 (42). The pooled proportion was 92.4% (95% CI: 88.9%-95.3%), and there was no comparison with clinicians (Table2; SDC 7, Supplemental Digital Content 1,http://links.lww.com/JTI/A334). Three studies reported the DSC and IoU,39,42,45ranging from 0.628 (42) to 0.854 (45) and 0.488 (42) to 0.804 (45), respectively. Zhou et al43described the outcome as “not high” in the absence of further quantification. One study reported a specificity and accuracy of 0.876 and 0.881, respectively.45Jin et al39compared their DL model results with those of clinicians who achieved a sensitivity, DSC, and IoU of 0.831, 0.647, and 0.478, respectively. This is lower than their DL model, with a sensitivity, DSC, and IoU of 0.920, 0.751, and 0.556, respectively.

For the classification of rib fractures, 26,359 patients from ten studies were included (28 to 30, 32 to 34, 36, 37, 41, 44; SDC 6, Supplemental Digital Content 1,http://links.lww.com/JTI/A334). Cumulative labels that were described in various studies were “old fracture,” “acute fracture,” healing, displaced, nondisplaced, and buckle.

Four studies reported the classification of acute fractures with sensitivities ranging from 0.677 (41) to 0.947 (30).30,34,37,41,44Eight studies reported old fractures, with sensitivities ranging from 0.587 (41) to 0.968 (34).28,29,32–34,37,41,44Displaced fractures were classified in 5 studies,28,29,32,33,36with a sensitivity ranging from 0.92433to 0.995 (36). Six studies reported nondisplaced fractures,28,29,32,33,36,41with sensitivity ranging from 0.732 (36) to 0.853 (28). Meng et al28–30,33classified buckle fractures with sensitivities ranging from 0.581 (33) to 0.896 (30).

For clinicians, the sensitivity for classifying acute fractures ranged from 0.677 (33) to 0.98 (37), old fractures from 0.533 (44) to 0.943 (33), displaced fractures from 0.83 (36) to 0.934 (33), nondisplaced fractures from 0.35 (36) to 0.849 (29), and buckle fractures from 0.554 (28) to 0.756 (29). Zhou et al34,44reported acute, old, and healing fractures. Healing fractures were defined as those more than 3 weeks old. The sensitivities for classifying healing fractures were 1.000 (34) and 0.859 (44), and for the clinicians, 0.751 (34) and 0.614 (44).

The pooled proportions for the sensitivity for the classification of displaced rib fractures for both the DL models and clinicians were 97.3% (95% CI: 95.6%-98.5%) and 88.2% (95% CI: 84.8%-91.3%), respectively. Thus, DL models performed significantly better. Nondisplaced fractures had sensitivities of the DL models of 77.4% (95% CI: 66.2%-86.9%) and of the clinicians of 71.7% (95% CI: 62.2%-80.3%). Acute fractures had a pooled sensitivity of 86.2% (95% CI: 76.9%-93.4%) and 78.9% (95% CI: 60.0%-93.0%) for DL models and clinicians, respectively. For old fractures, the sensitivities of DL models were 89.3% (95% CI: 81.8%-95.0%) and of the clinicians were 80.2 (95% CI: 67.0%-90.7%; Table2; SDC 7, Supplemental Digital Content 1,http://links.lww.com/JTI/A334). Because the CIs overlap, the difference between the DL models and clinicians for nondisplaced, acute, and old fractures is better, but undecided.

This review evaluates the performance of DL models in the detection, segmention, and classification of rib fractures using CT data. Based on the included studies, most DL models have higher sensitivities than clinicians for the detection, segmentation, and classification of rib fractures.

The majority of studies reported detection either as a primary outcome or secondary to classification. In daily practice, the high number of missed fractures may have negative clinical consequences, such as inadequate pain management and pulmonary complications.1,6Future research will have to show if this shortcoming could lead to undertreatment of the patient, with potential negative consequences, and if DL models might be helpful in overcoming this.

The delineation of rib fractures is technically challenging because the fracture parts can vary in size, position, and orientation. This may be the reason why a segmentation model cannot perform this task perfectly. None of the studies that reported segmentation used an external test set to validate the results. Possible explanations might be the lower clinical added value, the higher computational power required, or the labor-intensive process of creating a suitable data set for segmentation.

Several studies have reported on the classification of rib fractures.28–30,32–34,36,37,41,44Unfortunately, there was no uniform definition of the different fracture types among the included studies. This could be a reason why the pooled proportions of the sensitivity for the DL models were not significantly higher than those for the clinicians across different rib fracture types, except for displaced fractures. Recently, efforts have been made by the Chest Wall Injury Society (CWIS) to standardize the classification of rib fractures. The CWIS taxonomy defines fractures based on their location, degree of displacement, fracture type, and association with other fractures based on the Delphi consensus.15This might be a starting point to uniformly define fracture types. However, future studies must focus on the accuracy of DL models while using this Delphi-based classification system.

In 5 studies, a tradeoff between sensitivity and FP rate was achieved by setting a threshold.27,30,35,36,44The studies in this review showed a preference for high sensitivity, while accepting a higher FP rate. This seems reasonable from a clinical perspective to prevent undertreatment.

Thus, DL models have the potential to improve current practice due to their high sensitivity, but clinicians have to interpret DL outcomes cautiously because various shortcomings still exist. High FP rates might result in overtreatment, with consequent increase in health care costs. Implementation studies must evaluate this socioeconomic risk.

DL models are highly dependent on the data used for training purposes and tend to perform better with homogeneous data sets.47,48This is also shown in the nonsignificantly different pooled proportions of sensitivity for the detection DL models using external data sets compared with clinicians. Real-world (external) data sets are heterogeneous because of factors such as differences in CT scanners, slice thicknesses, imaging protocols, and populations. A large number of studies have not reported this information in their data sets. The lack of transparency can limit the reproducibility of such studies and hinder the generalizability of the findings. Collaborations between institutions interested in both DL models and patient care might overcome this knowledge gap while testing shared patient data.

In addition, while most studies used 2 or more clinicians to define the gold standard, it is important to maintain a critical attitude toward the results. If the gold standard and the ground-truth for the DL models were assessed by the same clinicians, this could lead to confirmation bias. In most studies, these were assessed by different clinicians. Furthermore, it is difficult to determine the accuracy of the gold standard and whether this mimics daily practice. The assignment of the gold standard was done by radiologists and other clinicians with varying years of experience in assessing radiographs, and should be taken into consideration when interpreting the gold standard. The vulnerability of the gold standard is indicated by the high frequency of missed fractures in clinical practice and limited interobserver agreement. The findings in this study suggest that DL models can be helpful to come to more consistent conclusions in the near future compared with current practice.

A shared thought in the field is that DL model performance improves with increasing amounts of data.49However, this trend was not evident in this review. This might indicate that the data sets were homogeneous and that overfitting was possible. Nevertheless, three out of 5 studies showing the highest sensitivities used >1 CT scanner30,32,44and 4 out of 5 used an external test set,29,30,32,35indicating heterogeneous data input. One study showed that a larger data set resulted in an increase in performance of ~4%.30These authors used the model of Azuma et al36and performed training with an additional 333 CT scans. This increased the sensitivity from 89.4% to 93.5%, with a decreased FP per case from 2.5 to 1.9. However, the authors did not report on the data set characteristics, used a small test set, and excluded data with confusing artifacts. Thus, in this literature review, the performance of the DL models cannot be directly related to the data sets used.

This study has some limitations that need to be acknowledged. First, not every study reported the same outcomes, making part of the intended pooled analysis impossible. Furthermore, the pooled results should be interpreted with caution owing to the large heterogeneity across the studies, and several studies had to be excluded based on a lack of reported outcomes of interest. Furthermore, other factors associated with the usefulness of DL models, such as time efficiency and technical specifications, have not been addressed. Although sensitivity was reported by all studies except 2,23,25no uniform measure was used for the FPs, if mentioned at all. Studies have used FP ratios per scan and F1-scores, making it difficult to quantitatively compare the performance of models using these metrics. The definition of an FP remains unclear in several studies. This makes the comparison between the results of these studies difficult and raises the need for a unanimous way of presenting results in AI studies. Future research should aim to expand the clinical added value of DL models by focusing on more advanced classification and prediction models that could provide important information for patient management and treatment planning. With further advancements in DL and increased collaboration between researchers and clinicians, DL models have the potential to become indispensable tools for the diagnosis and management of rib fractures. After implementation, socioeconomic consequences must be evaluated carefully.

In conclusion, DL models can detect, segment, and classify rib fractures and have shown significant improvements over the last few years. DL models outperform clinicians in rib fracture detection and classification of displaced rib fractures. Fewer data are available regarding the evaluation of segmentation models. The currently available data lacks validation on external test sets.

The authors thank Maarten Engel and Christa Niehot from the Erasmus MC Medical Library (Rotterdam, the Netherlands) for developing and updating the search strategies.