When individuals encounter observations that violate their expectations, when will they adjust their expectations and when will they maintain them despite these observations? For example, when individuals expect objects of type A to be smaller than objects B, but observe the opposite, when will they adjust their expectation about the relationship between the two objects (to A being larger than B)? Naively, one would predict that the larger the violation, the greater the adaptation. However, experiments reveal that when violations are extreme, individuals are more likely to hold on to their prior expectations rather than adjust them. To address this puzzle, we tested the adaptation of artificial neural networks (ANNs) capable of relational learning and found a similar phenomenon: Standard learning dynamics dictates that small violations would lead to adjustments of expected relations while larger ones would be resolved using a different mechanism—a change in object representation that bypasses the need for adaptation of the relational expectations. These results suggest that the experimentally-observed stability of prior expectations when facing large expectation violations is a natural consequence of learning dynamics and does not require any additional mechanisms. We conclude by discussing the effect of intermediate adaptation steps on this stability.

Imagine strolling through an art museum, expecting awe-inspiring masterpieces. Suddenly, disrupting your expectations, you encounter, displayed in the vitrine, ... a banana. This can be framed as a violation of a relational expectation: On the one hand, the artistic value of museum displays is expected to begreaterthan the artistic value of mundane objects. On the other hand, a banana seems to have a limited artistic value. There are two ways to resolve this inconsistency: recalibrate the expected relationship between the museum displays and mundane objects, or maintain this expectation and find an alternative explanation for the observation (e.g., find a deeper appreciation for the artistic value of bananas).

Previous studies suggested the existence of distinct cognitive modules associated with the generation of representations and the encoding of relations both in humans and other species1–7. In the banana example, therepresentationalmodule, which extracts task-relevant features from inputs, determines the artistic value of objects, whether a Rodin sculpture or a banana. Adaptation of this module would correspond to finding merit in bananas. Therelationalmodule encodes the expected relationship between representations, or between these representations and a predefined anchor. For instance, the relational module in the banana scenario encodes the expectation that objects displayed in an art museum surpass a certain threshold of artistic value, and its adaptation would result in decreasing this threshold. While in the banana example both modules can simultaneously adapt—slightly changing the representation of bananas and the expectation from museums—this is not the case in all violations of relational expectations.

Consider a scientist who has consistently observed that particles of type B are larger than particles of type A. With new experimental techniques, experiments suggest the opposite: particle A is actually larger than particle B. This unexpected finding forces the scientist to choose between two alternative adaptation pathways: maintain the view that B is larger than A by, for example, questioning the validity of the experimental results (adapt the representational module), or alternatively, update their view and conclude that A is, indeed, larger than B (adapt the relational module). This scenario sets two distinct possible adaptation pathways.

Our study was motivated by recent experiments that found that the choice between resolution pathways exhibits an inverted U-shaped dependence on the size of the violation8–11: When the violation is minute, its effect on the expectations is small. Larger violations have a larger effect on the expectations. Even larger violations (extremeviolations), however, fail to alter expectations. Somewhat similar findings were also observed in non-human animals in the framework of associative learning. When moderate, stronger unconditional stimuli elicit stronger conditional response. However, when extreme, the magnitude of the resultant conditional responsedecreaseswith the magnitude of unconditional stimulus12–14. These results are surprising because naively, operant learning, predictive coding and Bayesian models assert that the larger the prediction error, the greater the adaptation15–19. Therefore, gating mechanisms that modulate the magnitude of the adaptation10, for example, “immunization” mechanisms that prevent adaptation in extreme violation settings have been proposed20–23.

In this paper we show that the inverted U-shaped dependence of adaptation on the size of the violation naturally emerges in artificial neural networks (ANNs) that are trained to identify relations using standard (gradient) learning. We first present the results from a behavioral perspective using deep networks. Then, we explain these results by mathematically analyzing a simplified model.

We study adaptation using the order discrimination task: Agents are presented with image pairs and are instructed to determine, for each pair, if it was presented in the correct or reverse order. Each image depicts shapes arranged on agrid and is characterized by five features: the grayscale color of the shapes, their number, size, grid arrangement, and shape type (Fig.1). The first three features—color, number, and size—are described by a scalar number, establishing natural possible order relations between images. The “correct” order in the task depends on the identity of the relevant feature (color, size, or number), termed thepredictive feature, and whether this feature increases or decreases from left to right.

Order discrimination task. (a) Training set that demonstrates an underlying rule between images. The shapes in the right images are larger than those in the left images. The difference is characterized by. In these examples,, corresponding to half of the maximal size difference possible in our simulations. (b) The task is to determine whether two images are in the correct order according to the rule that characterizes the training set.

To teach the underlying rule, the agents are presented with a series of “correctly” ordered image pairs (Fig.1a), such that the predictive feature changes according to the underlying rule, while the remaining features, irrelevant to deciding the correct order, are randomly chosen for each pair but remain constant within the pair (as in the test images). In the main text of this paper we present the results when the predictive feature was the size. We demonstrate the generality of our findings by presenting similar results in the Supplementary Information, when the predictive features were color or number (Figures S1–S4).

A key parameter in this task is the difference in the predictive feature values between the two images, a quantity that we denote by. A positiveimplies that the predictive feature increases from left to right, whereas a negativeimplies that it decreases. The absolute value ofdetermines the magnitude of change:implies that the feature does not change between the two images, whereassignifies maximal difference. In the real world,would correspond to the (scaled) objective difference between objects. In the example of the scientist measuring particle sizes,is the (scaled) objective difference between the sizes of particleAand particleB.

As agents, we used ANNs that were designed to emulate relational learning24–27. Specifically, our networks were comprised of two modules. The first is a representational module, an encoder which we denote byZ. Its goal is to extract a relevant feature from inputs. For each of the two images of a pair,and(left image and right image, respectively), it maps theimage to one-dimensional variablesand, whereare trainable parameters. We implemented this mapping using a multilayered convolutional network. A similar architecture was shown in a previous study to be able to extract the relevant features in an intelligence test28. The difference between the representations of the two images is given by.

The relational module, which we denote as, characterizes the expected relation between the representations of the two images of the pair. In general, a relational module could be any function. We used a constant, single-parameter function that encodes the expected difference between representations,.

andrare hyper-parameters. With this additional regularization term, solutions such that, if attainable, would reside in the points where the lineintersects with the ring,and.

The model parametersandare trainable and we used Stochastic Gradient Descent (SGD) onto learn them (see Methods). To evaluate the performance of the ANN, we measured its ability to determine the order of novel test images. Specifically, we presented the ANN with two images, measured the values of the loss function associated with the two possible orders of these images (left-right or right-left), and chose the order that minimized the loss function. Figure2a, depicts the average performance of 100 ANNs as a function of the size of the training set when, showing that the networks successfully learned to solve the task after less than 40 image pairs. The high performance holds for other values of. We testedvalues ranging from 0.1 to 1, training them on 160 image pairs, and found that throughout this range, the ANNs performance exceededaccuracy (Fig.2b). Larger differences in the predicted feature between the two images () were associated with higher performance, signifying that the magnitude ofis a measure of the difficulty of the task. This robust learning performance, in a model with distinct representational and relational modules, reflects the ability shown in humans and non-human animals to learn relationships regardless of absolute attributes. Moreover, the higher performance with largervalues align with observations from those studies, where clear distinctions between stimuli support more robust relational behavior3,4,29,30.

Task performance and solutions. (a) The average test accuracy of 100 networks trained on a task where the predictive feature, size, changed by. (b) The final test accuracies for various values of, averaged over 100 networks per. Error shades and bars correspond toCI. (c,d) The model can solve the task using two different internal strategies. In one strategy, (c) a representative network learned to measure “largeness,” where it perceived the shapes getting larger (positive) and expected them to get larger (positive). In an equally effective strategy, (d) another network learned to measure “smallness,” where it perceived the shapes becoming less small (negative) and expected “smallness” to decrease (negative). Both solutions are equally valid for solving the task.

It is instructive to separately consider how each of the two modules adapt in the process of learning. This is depicted in Fig.2c for one representative network, where we plot the values ofandas a function of example pairs. According to Eq. (2), the regularized loss is minimized when. In this simulation,. While the values ofandvary between example pairs, they approach. Figure2d depicts another example network. In this simulation, which differed only in the initial parameters,andconverged to a negative solution, where. From a point of view of task performance, the positive and negative solutions () are identical. In the positive solution, the representational module learns to extract the size of the shapes, that is, how large they are, and the relational module learns that this size increases between the two images (from left to right). In the negative solution, the representational module learns to extract the “smallness” of the shapes (the negative of the size) and the relational module learns that the “smallness” decreases between the two images.

To study the violation of relational expectations in the ANNs, we trained them with sequences of image pairs, characterized by a specific predictive feature that changes bybetween the two images of the pair. After learning, we changed the training set’s relation rule towhere. That is, the shapes’ sizes decreased rather than increased from left to right, violating the relational expectation. We continued training the ANNs with the new rule and measured the performance of the ANNs as a function of examples.

The rule sizes before and after reversal,and, determine the magnitude of the violation. Specifically, we expect that largeandwould correspond to a large violation while smallandcorrespond to a small violation. To illustrate the core phenomenon, we begin our analysis by considering the specific case of a symmetric rule reversal:. To simplify notations, we writeand, therefore analyzing the case of rule reversal,. Later, we study howandindependently affect the adaptation pathway.

Crucially, there are two ways of resolving a rule reversal violation. Recall that in Fig.2we saw two different solutions that networks trained on the task identified. In one, bothandwere positive, while in the other, both were negative. There, we discussed the fact that the solution that minimizes the loss function is not unique:andboth minimize the loss function. These two solutions are depicted schematically in Fig.3a. Following rule reversal,necessarily changes its sign, violating the expectation set by. As illustrated in Fig.3b, one possibility for resolving the expectation violation is by changing the weights of the representational module,, so thatwould return to its pre-reversal sign keeping the relational module unchanged. The other possibility is that the representational module retains its post-reversal sign ofwhile the relational modulechanges its sign. Both can lead to exactly the same level of performance. We hypothesized that magnitude ofwould determine which of the two adaptation pathways would be taken by the ANNs.

Illustration of the two adaptation pathways. (a) The two equivalent solutions for the relational task. One solution is to encode the size of the objects and expect that the size increases (positiveand, marked by a star). In the other solution, the smallness decreases (negativeand). The diagonal line all solutions in which. (b) Adaptation pathways following rule reversal. Without loss of generality, we consider an agent that has learned the positive solution. After learning the initial rule, the rule is reversed, flipping the sign of, making it inconsistent with positive(marked by a star). There are two adaptation pathways:relational adaptation—maintaining the sign ofbut reversing the sign of the expectation; orrepresentational adaptation- changing the sign ofto encode the smallness of objects, while maintaining the expectation that the relevant feature increases (). The dashed diagonal line represent expectations that are opposite to the observations ().

Going back to the ANNs, we first considered two violation magnitudes: a larger violation (, Fig.4a left) and a smaller violation (, Fig.4a right). We measured the ANNs performance as a function of examples before and after reversing the rule (Fig.4b). In both the larger and the smaller violation conditions, performance levels just before the sign reversal (image pair 160) were almost perfect. The first image pairs immediately following the reversal were almost always incorrectly ordered by the ANNs (performance level close to 0). With examples, however, the networks adapted to the new rule, achieving almost perfect performance after additional 160 image pairs. Notably, learning was slower for the more difficult task associated with the smaller value of. Also, adaptation to the reversal was slower than initial learning, a phenomenon that has been previously reported in human learning31.

Dual adaptation pathways following rule reversal. Demonstration of the two adaptation pathways, depending on the violation magnitude, when the rule changes fromto. (a) Example image pairs before and after the rule reversal for a large violation (, left) and a small violation (, right). (b) Performance across 100 networks as a function of examples (shaded regions: 95% CI). (c) Representative demonstration of the evolution of(red) and(blue) for each violation magnitude. Initially, both networks converge to the positive solution (). After reversal,flips immediately due to the change in the image order. Optimization drivesandtowards each other, approachingand. For large violations (left), the representation module “wins” and adaptation restoreswhile keepingunchanged. For small violations (right), adaptation eventually occurs via the relational module, flipping the sign ofto match the negative.

To dissect the roles of the two modules in this reversal adaptation, Fig.4c depicts the values of(red) and(blue) in representative networks adapting to the large and small violations. Before reversal, both networks converged to comparable values ofand. Immediately after the rule reversal,flipped. This is because the image order was reversed – the sizes of the shapes decreased, rather than increased between the two images – and the representation module reflected it. The value of, however, remained unchanged. This is because the network “expected” the sizes of the shapes to increase rather than decrease. With training, the system resolves this violation.

In our simulations, we found that when the violation was large (Fig.4c left), the representational moduleZadapted so thatreturned to its pre-reversal values. By contrast, when the violation was small (Fig.4c right),remained negative and the violation was resolved by a change in the sign of the relational module.

Are these results representative? We simulated reversal adaptation for different values of, each time simulating 100 randomly-initialized networks, and measuring the fraction of times in which the adaptation involved a change in the sign of, which indicates that the relational module dominates the adaptation. In line with the examples of Fig.4c, the smaller, the larger was the fraction of networks in which the relational module flipped its sign in response to the rule reversal (Fig.5a). The transition between the two adaptation pathways, an inflection point marked by, was at. These results demonstrate that large violations () can inhibit adjustments to relational expectations, leading to adaptation in object representations instead.

Adaptation pathway dependence on violation magnitude. (a) Fraction of networks () adapting via relational module (sign change) for rule reversal, fitted with logistic function (green) to estimate inflection point. Arrows mark thevalues from Fig.4. The logistic function fit has two parameters, the inflection point,, and the slope parameter,(see Methods). Error bars: 95% CI. (b) Adaptation pathways for general rule changes. Color indicates fraction (out of) adapting via relational module (). Squares: representational module (Z) dominant (ratio); triangles: relational module dominant (ratio).

The results of this section are reminiscent of the surprising part of the experimentally-observed inverted U-shaped dependence of relational adaptation to the size of the violation discussed in the Introduction. When the violation is modest (is small), the relational expectation adapts:changes its sign when the order of images is reversed. By contrast, it remains unchanged when the violation is large. Instead, the violation is resolved by the network changing its representation. This behavior does not require any explicit immunization mechanism. Rather, it naturally emerges from the dynamics of learning.

In the analysis above, we considered the special case of symmetric rule reversal, in which. Now, we study the specific contribution of these two parameters to the choice of an adaptation pathway. To that end, we studied the adaptation of ANNs to a different pairs ofand, as depicted in Fig.5b. For each pairwe measured the fraction of networks in which adaptation was associated with a change in the sign of(color coded). To better visualize the transition point (which was denoted byin the case of), the symbol (square vs. triangle) denotes whether this fraction was smaller or larger than. These simulations show that the adaptation pathway depends both onand. The largerand the larger, the more likely it is that the representational module will change its sign. However, a largecan compensate for by a smalland vice versa. The boundary between the two adaptation pathways resembles a hyperbolic curve. This is not a coincidence. Below we prove, in a simplified model that in the limit of weak regularization, the boundary is, indeed, a hyperbolic function in theplane.

Next we study the implications regarding our ability to use shaping to facilitate or inhibit adaptation of relational expectations.

Understanding how individuals adapt to violations of their expectations is important for clinical psychology, as expectation persistence and change are central to mental health interventions. Clinical research has shown that maladaptive expectations contribute to disorders such as anxiety and depression, where individuals often maintain dysfunctional expectations even in the face of disconfirming evidence23. Effective psychological treatments leverage expectation violations to induce cognitive and behavioral change. Yet, while moderate expectation violations are most effective in altering beliefs, extreme violations risk reinforcing rigid mental models, preventing adaptation10,23. In this section we show that adding an intermediate rule in the reversal task can alter the adaptation pathway, thereby steering the adaptation process toward a preferred adaptation strategy.

We compared adaptation in reversal task in which the rule is reversed in one step, as before () to adaptation when our agents also adapt to an intermediate step (). We hypothesized that the value ofwould inversely depend on the magnitude of the intermediate step: larger intermediate rules would lower, making relational adaptation less likely, whereas smaller intermediate steps would increase, favoring relational adaptation. The intuition behind this hypothesis is that the relational module can change its sign only when the rule reverses. Therefore, when, ifthen the intermediate step enhances the violation (increasesof Fig.5b) and therefore decreases the probability of a relational adaptation. By contrast,decreases the magnitude of the violation and therefore, increases the probability of a relational adaptation. The effect of a negativeis similar. This time, the focus is on the transition (), wheretakes the role ofof Figure5b.

To test this hypothesis, we trained ANNs using theparadigm, using 160 image pairs for each rule (totalimage pairs). For each pairwe trained 50 ANNs and computed the fraction of networks in which the sign ofchanged from image pair 160 (after the network learned therule) to image pair 480 (after the network learned therule). The results are depicted in Fig.6a. Then, for each value ofand, we computedby fitting a logistic function to the computed fraction as a function of, as in Fig.5a. The values ofas a function ofare presented in Fig.6b. Indeed, for large values of,was smaller than that computed in theparadigm (dashed horizontal line, computed in Fig.5a), while small values ofincreased. These findings demonstrate that adaptation pathways can be influenced by the sequence of changes between them. By strategically introducing an intermediate step, we can shift the boundary between relational and representational adaptation, effectively shaping the learning process. Specifically, small values ofincrease the probability that the relational network would adapt.

Intermediate learning step: influence on adaptation pathways. The threshold between relational and representational adaptation () is modulated by the magnitude of an intermediate step. (a) The fraction of networks that adaptfor training with an intermediate stepfor various pairs of. (b) Large values oflower, promoting representational adaptation, while smallincreases, favoring relational adaptation. The gray dashed line representsin the absence of an intermediate step, highlighting how structured transitions can shape the adaptation process.

To gain an analytical understanding as to why small and large violations lead to qualitatively different adaptation mechanisms, and the extent to which these results depend on the particularities of the model that we studied, we considered adaptation to violation in a simplified model, in which the pair of images was replaced by a pair of scalars, denoting the predictive features in the pair of images. That is,andand their relation is their difference,.

This loss function captures two key components: (1) the squared error between the representational differenceand the relational expectation, and (2) a regularization term ensuring that solutions remain on a ring.

Recall that when studying the adaptation of the ANN to the image pairs, we identified two solutions that minimize the loss:. The two solutions are associated with the same level of performance, they differ in how the system encodes the relationship: when, the predictive feature increases, whereas when, it decreases, as discussed in Fig.2.

In gradient-based systems, learning dynamics converge to a stable fixed point where. In the Methods section we prove thatare the only stable fixed points of the dynamics, Eq. (5). Thus in general, the dynamics would converge to either the positive or the negative fixed point.

To investigate the dynamics following rule reversal, we consider a system that has converged to the positive fixed point. Following the reversal of the rule, the sign of the representational differenceflips to. As a result, the value ofin the loss becomes non-zero, reflecting the violation of the expected relationship.

At this point, the gradient dynamics would resolve the violation by either driving the system back to the positive fixed point, adapting the representational module, or to the negative fixed point, which adapts the relational expectation (as illustrated in Fig.3).

To understand how the magnitude ofaffects this adaptation pathway, we first considered the dynamics of a weakly regularized system, where. In this case, the dynamics first minimize the unregularized part of the loss,, driving the system to, and then the regularization kicks in to set the system on the ring. We show below that this sequential adaptation pattern – first aligningand, then enforcing the regularization constraint – provides a key insight into the behavior of the more complex ANN.

whereanddenote the values ofandbefore the reversal.

Substituting the initial conditionsand, we find that after reversal,will change its sign if and only if.

Considering now the full model (Eq. (5)), we identify the following symmetry: Immediately after reversal. Considering the equations forand, they are also symmetric to swappingand, as long asis replaced by. Consequently, if for a particular value of,would change its sign in the reversal protocol, it would bewhich changes its sign ifis used, indicating that also in the full model,is the transition point between the two, qualitatively-different modes of adaptation.

Figure7depicts these dynamics in the reversal paradigm using a phase portrait for two values of. Whenis large (left) adaptation is dominated by a change in. The opposite, a dynamics that is dominated by a change ofmanifests whenis small (right). Notably, in simulating the model, we used a relatively weak regularization,. As a result, initially, the dynamics drive the system to the line, minimizing the unregularized term by either changing the sign ofor, depending on the size of the violation. Then, when, the regularization term pushes the system towards one of the two fixed points, where.

The adaptation dynamics in theplane. Visualization of the two adaptation pathways as two trajectories in the simplified model’s internal state represented byand. The arrows represent the “downhill” direction for learning: the opposite direction of the loss gradient with respect toand. The blue and red nullclines represent the loci whereand, respectively. Their intersections correspond to fixed points in which learning halts. They intersect at the stable fixed points at, and at the unstable fixed point. Before reversing the rule, the network start with positiveand. Following the reversal of the rule,flips its sign (yellow circles). Adaptation whenis large (left panel) leads to reversing the sign of, returning to the positive fixed point, whereas whenis small (right panel) it isthat changes its sign to match the negative.

We also studied the system’s dynamics in the more general adaptation of. In this case, the initial statewould generally not belike in the symmetric rule reversal. Instead, it would be(see Methods). This is whileremains the same. Therefore, the ratioadjusts the distance of the system from the ordinates (y-axis), whose crossing corresponds to changing the sign of. The larger, the longer is the path required for a change in the sign of(Fig.8a). Another modification to the dynamics is thatin Eq. (6), which sets the gradient ofcompared with those of, is replaced by. Therefore, the value ofdetermines the direction of the gradient, or the relative speed of adaptation of the two modules: The larger, the more the gradient is aligned withcompared to(more horizontal) (Fig.8b).

How initial and new rule strengths determine the adaptation pathway. This figure explores how the network adapts when the rule changes from an initial strength () to an opposite rule of a different strength (). The choice of adaptation pathway can be understood as a “race” between the representational module () and the relational module (). Two factors determine the winner. First, as illustrated in panel (a), the ratio of the new rule’s strength to the old one () determines the “starting position” for the race (here,is fixed at 1, whileis 2 for the blue trajectory and 0.5 for the red). Second, as shown in panel (b), the strength of the new rule () influences the relative “speed” of adaptation, changing the direction of the learning process (we usedfor the blue trajectory andfor the red, while fixing the starting points by setting). (c) The final race outcome for various combinations ofand. The boundary between the two pathways is determined by the combined strength of the rules. Specifically, the line(black line) is a good fit for this boundary.

Together, we can view the dynamics as a “race” between the two adaptation pathways. The ratiodetermines the starting points of the two adaptation pathways, and hence the relative distance to the finish line. The larger this ratio, the smaller is the relative distance required for adaptation by the relation module. The larger, the faster is the dynamics of, compared to that of. Together, the relation module is more likely to adapt when the ratiois large and whenis small (Fig.8c).

As discussed above, when regularization is weak, we can analytically predict the “winner” from the ratio between the relative distances ofandto the finish line,, and the ratio between their speeds,. If, then the representational module would adapt; whenthe relational module adapts (see Methods). This prediction is a good fit for the regularized simplified model of Fig.8c, in which. This prediction also fits the adaptation pattern of the ANN in the more complex images task, when adjusted such thatis the condition for adapting the representational module (Fig.5b and S5). Overall, our results suggest that the multiplication of, which precisely determines the adaptation pathway in the limit of weak regularization, is a good fit to to howandaffect the adaptation pathway in this model.

In this study, we explored how artificial neural networks (ANNs) resolve relational inconsistencies when faced with violations of expected relationships. Our findings reveal two distinct adaptation pathways that naturally emerge from gradient-based learning dynamics: when violations are small, networks primarily adjust their relational expectations, whereas extreme violations lead to modifications in object representations, preserving the initial relational expectation. This dichotomy can account for the experimentally observed inverted U-shaped dependence of expectation adaptation on violation magnitude, where moderate expectation violations lead to learning, but extreme violations often result in resistance to change.

The key component of our theoretical finding is that a violation of expectation can be resolved in more than just one way. Therefore, in case of relational inconsistency, whether or not the relational component will eventually adapt depends on whether adaptation of this component is “sufficiently fast”, compared with the alternatives route for adaptation—the adaptations of the representational module. In response to the violation of expectation, a “race” between the two adaptation pathways begins. The ratio between the new rule and the original rule determines the starting points of the two adaptation pathways, and hence the distance to the finish line. The larger this ratio, the smaller is the distance required for adaptation by the relation module, relative to the representational module. Moreover, despite the fact that we used the same learning rule for the representational and relation modules, SGD, which is characterized by a single learning rate, the “speed” of adaptation of the two modules is not equal. The larger the magnitude of the “new” rule () the slower the relation module adapts relative to the representational module.

Our results indicate that the product of the initial and final rulesis a crucial parameter in determining the adaptation pathway. When this product is large, the representational module adapts and when it is small the relational module adapts. In the real world, the rule sizes correspond to an objective difference between objects. Extrapolating our results to the scientist example, if particles A are expected to be larger than particles B by, and the novel observation is that they are smaller by, then the observation would be dismissed ifwould be larger than some threshold that depends on the scale.

In our study, the core relationships we investigated were defined by changes in specific predictive features: shapes size in the main manuscript, and grayscale color and number of shapes in the Supplementary Materials. All other features varied randomly between image pairs and were categorized as irrelevant features. Our prior research has demonstrated that the quantity of these irrelevant features directly correlates with the difficulty of similar tasks34. Consequently, we integrated these irrelevant features into our experimental design to slightly increase task complexity, aiming for a more realistic scenario. We hypothesize that as the networks achieve high performance, they learn to effectively disregard these irrelevant features. Crucially, when the rule is reversed in our experiments, only the predictive feature’s rule is altered. Therefore, we do not anticipate that the number or identity of these irrelevant features will influence our reported results.

Reconciling prior beliefs with conflicting evidence is commonly framed within a Bayesian framework17,18. To connect our findings to this approach, consider a scenario with two competing hypotheses: one suggesting the predictive feature (e.g., size) increases, and the other suggesting it decreases. When an agent begins with a stronger belief in the “increase” hypothesis and then encounters evidence challenging this belief, Bayesian updating provides a formal way to revise these beliefs. This revision depends on two key factors: the initial confidence in each hypothesis (prior beliefs) and how well each hypothesis explains the new observation (likelihood of the evidence). The Bayesian framework provides a clear decision threshold – the point at which belief should shift from one hypothesis to the other. For the Bayesian model to account for the inverted U-shaped dependence of relational adaptation to the size of the violation, observed experimentally as well as in our ANN model, we need a model in which largeandsupport belief consistency. If we interpret the magnitudes ofandas measures of the strengths of evidence they provide, it is easy to see why a strong initial evidence, in the form of a largewould do that. However, it is challenging to interpret belief consistency whenis large. This is because stronger contradictory evidence (larger) is expected to increase the likelihood of hypothesis revision rather than decrease it. It is possible to account for the inverted U-shaped dependence in a Bayesian framework if we add a measure of confidence in the observations, and posit that the confidence in the first set of observations (associated with) is larger than that of the second set of observations (associated with).

In the paper, we found that intermediate adaptation steps can be used to reshape the adaptation pathway. Specifically, a small intermediate step promoted the adaptation of the relational expectation. This result aligns with findings in cognitive-behavioral therapy and education, where gradual exposure to conflicting information enhances adaptive outcomes. For example, therapies designed to modify dysfunctional expectations, such as exposure therapy for anxiety disorders, benefit from structured interventions that introduce moderate violations instead of extreme ones37,38. Similarly, in education, conceptual change is more effective when scaffolded gradually rather than introduced through abrupt contradiction19,39,40.

The two adaptation pathways discussed in this work are categorical (representational vs. relational). But notably, there are multiple ways to adapt, even within each category. Specifically, the representational module is characterized by a large number of parameters, more parameters than examples. Therefore, there can be multiple combinations of parameters that, for the same set of examples, yield the same representational adaptation. The relational module in our model is rather simple, but in a more general model we expect a similar multiplicity of possible adaptation pathways within the relational pathway. Indeed, different adaptation pathways within a module have been a subject of research in the cognitive sciences. For example, an inconsistency between a person’s unhealthy habit of smoking and the warning against the harmful effects of smoking presented on a tobacco package can be resolved in several ways (that do not change the habit): The smoker can posit that benefits associated with appetite suppression outweigh the cancer-related health risks, or alternatively, question the research that links smoking to increased mortality41. Both solutions are consistent with a change to the representation of smoking. A major limitation of our model is that it is not informative about the determinants ofwithin-categoryadaptation pathways.

Another limitation of our model is that it modeled passive agents who get a sequence of pairs of inputs and are required to learn their relationships. However, humans actively engage in the learning process by performing various comparison and manipulation patterns of stimuli29,30. This active participation is useful for establishing relational behavior. The active manipulation of stimuli can be interpreted as a form of representational adaptation, where the learner actively re-frames or re-processes the sensory input by physically interacting with it, allowing the learner to adjust their internal representations of objects to resolve inconsistencies and make new information compatible with existing or emerging relational rules.

By examining the intrinsic learning dynamics of neural systems, our work provides insights into the mechanisms that govern adaptation to inconsistencies. The competition between representational and relational adaptation pathways naturally produces the non-monotonic patterns of belief updating observed in human cognition. These findings have implications for understanding learning processes across cognitive, educational, and therapeutic contexts. Further work is needed to explore the nuances of adaptation within the representational and relational categories, as well as to experimentally test the model’s predictions.

A PyTorch42code that generates the results and figures of this paper is available at:https://github.com/Tomer-Barak/relational_expectation_violations.

The order discrimination task was designed to assess the ability of ANNs to determine the correct order of image pairs based on a specific feature. As written in the main text, each image in the pair depicted shapes arranged on agrid and was characterized by five features: grayscale color, number of shapes, size, grid arrangement, and shape type. The images werepixels in size and grayscale (they consisted of 3 channels with identical values). The size of the shapes was defined as the diameter of the circle enclosing them.

The “correct” order in the task was determined by the identity of the relevant feature (color, size, or number), termed thepredictive feature, and whether this feature increased or decreased from left to right. To construct the training set, the predictive feature values were randomly selected from a uniform distribution over possible values for the left image. The corresponding values for the right image were then calculated by applying the rule parameterto the left image’s values. Non-predictive features were randomly selected from a uniform distribution for each image pair, remaining constant within the pair.

For eachand tested network, we constructed a training set that consisted of 160 image pairs that demonstrated that rule. To evaluate the performance of an ANN in this task, we tested its ability to classify the correct order of 32 novel image pairs. In Figs.2a-b and4b, we averaged the classification accuracy of 100 ANNs and estimated the confidence interval based on the standard error.

The representational moduleconsisted of three convolutional layers (number of filters: 16, 32, 32; kernel sizes: 2, 2, 3; strides: all 1; padding: all 1) followed by one fully-connected linear layer (taking a 2592-dimensional vector to a one-dimensional output). Three ReLU activation functions were applied after each convolutional layer, and two Max-Pool layers (kernels: 4 and 6, strides: all 1) were applied after the second and third convolutional (+ReLU) layers. The parameters ofwere randomly initialized using PyTorch’s42default initialization (uniform distribution scaled bywhereNis the number of the layer’s input neurons).

Given a training set, we optimized the randomly initialized ANN’s parameters to minimize the regularized loss function (2) with the vanilla SGD optimizer (). For the regularization term, we used the hyperparametersand. We used a batch size of 2 image pairs and applied 20 optimization steps per batch.

To assess the adaptation pathway of an ANN, we measured its parameterduring training. To complement this measure, we also measured the averageof the ANN over 32 test image pairs from the same training set distribution (with the same). A network that changed its sign ofand kept the sign ofafter rule reversal was classified as adapting its relational module. A network that kept the sign ofwhile changing the sign ofhas adapted its representational module. We excluded networks that kept or changed the signs of bothandtogether. These networks necessarily failed the task. The fraction of excluded networks was less than: Fig.5a: 3/1800. The fraction of networks that adapted their relational module (e.g., in Fig.5a) was obtained bywhereis the number of networks that adapted, andis the number of networks that adapted.

To calculate the inflection point, we fitted a logistic function to the results of how many networks adapted their relational module as a function of. Specifically, we fitted the two parameterscanddof the logistic function. The inflection point was defined as. TheCIs ofcorrespond towhereSE(d) is the standard deviation error of the estimation ofdusing SciPy’s43curve fitting function.

Subtracting the equations, we get that. Ifthen from Eq. (5),at the fixed point. Therefore together,.

The nullclines are depicted in Fig.7. The fixed points can be computed analytically by substitutingin the nullclines equations. We find that there is a trivial fixed point at. A linear stability analysis reveals that this fixed point is unstable. Additionally, there are two additional fixed points. These fixed points satisfy bothand the regularization term. We will discuss their stability shortly. When the regularization term is large,, there are two additional fixed points,, but a linear stability analysis reveals that they are unstable. Because the dynamics is driven by a gradient of a loss function, then it necessarily converges to a fixed point. Because theare the only non-unstable fixed points, they are necessarily the only attractors of the dynamics.

To understand how the magnitude ofaffects this adaptation pathway, it is useful to consider the dynamics of a weakly regularized system, where. In this case, the dynamics first minimize the unregularized part of the loss,, driving the system to, and then the regularization kicks in to set the system on the ring.

When approaching the the line attractor,becomes small, comparable to the regularization term in Eq. (5). Therefore, the regularization would then become more dominant and drive the system towards. The result we arrived at, Eq. (11), shows that whetheris smaller or larger than 1 determines the sign of the fixed point.corresponds to a fixed point where bothandare positive, keeping the original sign of, whereasleads to a negative fixed point, changing the sign of. Therefore, the value ofdistinguished between the two adaptation pathways, and the inflection point is at. We verified this analysis by simulating the simplified model with weak regularization. For example, Fig.7demonstrates the dynamics whenfor a strong violationand a weak violation. Initially, the dynamics drive the system to the line, minimizing the unregularized term by either changing the sign ofor, depending on the size of the violation. Then, when, the regularization term pushes the system towards one of the two fixed points, where.

This equation shows that whenever, the system adapts its representational module, while forit would adapt its relational module. We verified this prediction in the weakly regularized simplified model in Figures5b and S5.

This work was supported by the Gatsby Charitable Foundation. Y.L. holds the David and Inez Myres Chair in Neural Computation. We thank David Hansel and Ilya Nemenman for insightful discussions.