This study aims to compare the diagnostic abilities of humans in wound image assessment with those of an AI-based model, examine how “expertise” affects clinicians’ diagnostic performance, and investigate the heterogeneity in clinical judgments.

A total of 481 healthcare professionals completed a diagnostic task involving 30 chronic wound images with and without maceration. A convolutional neural network (CNN) classification model performed the same task. To predict human accuracy, participants’ “expertise,” ie, pertinent formal qualification, work experience, self-confidence, and wound focus, was analyzed in a regression analysis. Human interrater reliability was calculated.

Human participants achieved an average accuracy of 79.3% and a maximum accuracy of 85% in the formally qualified group. Achieving 90% accuracy, the CNN performed better but not significantly. Pertinent formal qualification (β  =  0.083,P< .001) and diagnostic self-confidence (β  =  0.015,P= .002) significantly predicted human accuracy, while work experience and focus on wound care had no effect (R2= 24.3%). Overall interrater reliability was “fair” (Kappa = 0.391).

Among the “expertise”-related factors, only the qualification and self-confidence variables influenced diagnostic accuracy. These findings challenge previous assumptions about work experience or job titles defining “expertise” and influencing human diagnostic performance.

This study offers guidance to future studies when comparing human expert and AI task performance. However, to explain human diagnostic accuracy, “expertise” may only serve as one correlate, while additional factors need further research.

The development of artificial intelligence (AI)-powered clinical decision support systems (CDSS) in highly visual medical fields has experienced significant growth in recent years1–4—for analyzing images or videos,5supporting clinical diagnosis,6and detecting diseases.7,8With the improvement of deep learning approaches and the use of larger datasets, algorithms are now able to achieve the same results as experts in certain medical tasks.9In dermatology, where visual judgment is key, AI systems have become increasingly reliable in classifying dermatological findings, eg, various types of skin cancer as well as inflammatory diseases and chronic wounds.10–16Studies that have been conducted on wounds have shown that AI models are able to detect visual signs of potential healing complications, such as maceration (the swelling and softening of tissue due to prolonged contact with fluid17) or infection.15,18–20Therefore, AI opens up new possibilities for wound assessments where, unlike skin cancer, the findings cannot be or are usually not histologically verified.

While the advances of AI enabled diagnostics are undeniable, the question remains as to whether and when they are performing better than humans in certain tasks. A systematic review and meta-analysis by Liu et al., including 82 studies, compared the diagnostic accuracy of medical image-based disease classification by deep learning algorithms and healthcare professionals. They found their performance to be comparable (sensitivity of 87% for AI vs 86.4% for humans).21The systematic review by Shen et al.—based on 9 articles—concluded that AI models performed comparably to medical experts and outperformed less experienced clinicians.22The validity of these comparisons, however, suffered from the fact that human performance was only investigated in very small groups of clinicians, ranging from 2 to 42. Only the study by Brinker et al. established a benchmark for human melanoma detection in dermoscopic images including 157 dermatologists.23In a subsequent study, they also compared the diagnostic results of a convolutional neural network (CNN) with those of 145 dermatologists, showing that the CNN performed on par with the average dermatologist. Still, 19 out of 145 human experts achieved a higher sensitivity than the CNN,24thereby indicating a differential effect.

Expressing human diagnostic ability as the mean of expert opinions overlooks the group’s diversity and assuming job title to represent expertise may simplify matters. Experts can greatly differ in education, training, years of experience, work focus, and diagnostic self-confidence as well as demographic factors such as age, gender, and healthcare setting. To our knowledge, these characteristics have not yet been analyzed in the context of comparing human and AI-based diagnostic performance. Knowledge about group differences would yield more nuanced insights and would help identify those individuals who can benefit the most from the assistance of an AI-based system.

Therefore, the aim of the present study is to compare human diagnostic ability in wound image assessment with that of an AI-based model. Hereby, we also seek to find the characteristics that are associated with human diagnostic judgments. Furthermore, we are interested in the overall interrater reliability, and particularly between different groups, to better understand the homogeneity or heterogeneity regarding their judgment.

To address the research questions, we conducted a clinical study requiring clinicians and an AI system to determine the presence or absence of maceration in images of chronic wounds. Our aim was to operationalize human expertise by the set of the following variables:pertinent formal qualification,work experience, anddiagnostic self-confidencein detecting maceration. The variable as to whether there had ever been afocus on wound carein their professional life (eg, working as a specialist in a wound care center or with a home care provider focused on wound care) rounded out this set. The selection of these variables was informed by the Theory of Expert Competence,28,29which involves among others domain knowledge and psychological expert traits, including self-confidence. To choose the variables, other studies that point at extensive deliberate practice30and formal qualification31were also drawn upon. In wound care, there is a wealth of studies, recommendations, and guidelines for assessing and treating wounds.32,33This knowledge underpins the understanding ofpertinent formal qualificationin wound care.

In addition, clinicians were also characterized byage,gender, andhealthcare setting(outpatient or inpatient). The complete questionnaire is shown inAppendix S1.

Each clinician received 30 wound images via an electronic form (LimeSurvey version 6.6.6) capturing the diagnostic decisions and participant characteristics. To avoid sequence effects, the images were displayed in random order. There were no time constraints to complete the task.

The same diagnostic task with the same 30 wound images was presented to a CNN image classification system that had been developed based on a pre-trained MobileNetV2 architecture in a previous study. This model, which was trained on 458 images, demonstrated superior performance in classifying wound images with and without maceration compared to other models. It was selected due to its high efficiency, achieving the highest accuracy (77%) despite having significantly fewer parameters (3.5 million) than more complex architectures. Transfer learning with ImageNet weights, data augmentation, dropout, and early stopping had been applied to reduce overfitting and enhance the generalizability.15The images that the model was trained on were sourced from the same hospital as the images for the maceration detection task, but the 2 datasets were completely distinct.

The participants were recruited from among the physicians, nurses, and other healthcare professionals across Germany between May 2024 and July 2024 through online advertisements in 1893 German hospitals, at a wound congress 2024 with 4628 attendees,34via the professional association “Initiative Chronic Wounds e. V.” (ICW), and the Department of Dermatology at Erlangen University Hospital. None of the clinicians who determined the ground truth took part in the study. The inclusion of participants without pertinent formal qualification in wound care was essential to adequately and realistically reflect the situation in providing wound care. Not uncommonly, patients with chronic skin problems contact this group of health professionals first before they are referred to a specialist.

The study protocol was approved by the Ethics Committee of Osnabrück University of Applied Sciences (no. HSOS/2024/1/1).

To establish the ground truth for the maceration classification, an ICW-certified wound expert initially selected 110 leg ulcer images with various diagnoses from the University Hospital Erlangen’s EHR, all with a clearly recognizable maceration status. Five additional wound experts (ICW-qualified nursing and medical staff) independently assessed these images. From their unanimous decisions, 15 images with and 15 without maceration were chosen, yielding 30 images in total. All of the images were reviewed by the hospital’s Data Integration Center for anonymity and by a certified wound expert for high image quality.

All the characteristics of the participants were treated as binary variables to compare the performance metrics and the interrater reliability between different groups. Professional background was classified either as withpertinent formal qualification(dermatology residents/specialists or ICW-certified nurses) or without (other physicians/nurses without specialized wound care training).Work experiencewas categorized into participants with up to 5 years and those with more than 5 years of experience.26Agewas binarized by median split anddiagnostic self-confidence, assessed using a 10-point scale, and binarized intolowandhigh. For the regression analysis,work experience,age, anddiagnostic self-confidencewere treated as continuous variables. Participants with non-patient-care qualifications or incomplete responses were excluded.

Diagnostic performance was evaluated by accuracy, sensitivity (recall), and specificity (mean and 95% confidence interval35). Accuracy served as the primary outcome and was tested for significance in multiple paired t-tests. Alpha (0.05) was, therefore, corrected according to the Holm-Bonferroni method. Positive and negative predictive values can be found inAppendix S2.

A multiple linear regression analysis was performed to identify the correlates that might influence the participants’ accuracy in decision making when detecting maceration. Demographic characteristics (age,gender,healthcare setting) as well as objective and subjective professional expertise (pertinent formal qualification,years of work experience,focus on wound care, anddiagnostic self-confidence) were used as independent variables. All of the indicators met the requirements of multiple linear regression analysis.Appendix S3contains the diagnostic tests and their results. The results of the statistical analyses were tested at a significance level ofP< .05.

To evaluate the interrater reliability between the different participant groups in the assessment of maceration, Fleiss’ kappa was calculated for the entire sample and separately by groups.

The statistical analyses were conducted using R (version 4.4.1) with additional packages, including lmtest (version 0.0-40), irr (version 0.84.1), and car (version 3.1-3), to perform advanced modeling, reliability assessments, and diagnostic testing. Diagrams were created using the ggplot2 package (version 3.5.1).

A total sample of 481 participants was included in the study (Table  1). The sample is balanced with the exception of work experience (more persons with long experience), gender (more females), and healthcare setting (more persons from inpatient care). Bivariate descriptive statistics (phi-coefficient) are shown inAppendix S4. Variables that moderately correlate with each other are age and work experience (phi = 0.35), pertinent formal qualification with diagnostic self-confidence (phi = 0.40) and a focus on wound care (phi = 0.30), and diagnostic self-confidence with a focus on wound care (phi = 0.40).

Overall, human participants achieved an accuracy of 79.3%, a sensitivity of 76.4%, and a specificity of 83%. In comparison, the AI classification model demonstrated an accuracy of 90%, a sensitivity of 93.3%, and a specificity of 86.7%, thereby outperforming the human results on a descriptive level. However, when tested for significance, the accuracy values of humans were only significantly lower than those of the CNN in the groups withoutpertinent formal qualification, lowdiagnostic self-confidence, nofocus on wound care, and shortwork experience(Table 2).

Selected metrics of human and AI diagnostic decisions.

P-values are derived from paired t-tests comparing each group to the CNN. The significance threshold was adjusted for multiple comparisons using the Bonferroni correction (α = 0.0034). Significant values are shown in bold.

Figure 1shows the confusion matrix for the clinicians and the AI model.

Confusion matrix for human and AI model decisions.

Participants with apertinent formal qualificationdemonstrated the highest values in all metrics with a higher accuracy (85% vs 73.2%), sensitivity (82.2% vs 70.5%), and specificity (88.3% vs 76.9%) compared to those without such a qualification (Table 2).

A similar pattern emerged for participants with highself-confidence in diagnosing macerationcompared to those who lacked this confidence (Table 2). Furthermore, the analysis revealed that the subgroup (n = 160) with thewound care qualification plus high self-confidenceexhibited a sensitivity of 83.3%, specificity of 89.7%, and accuracy of 86.3%, thereby reaching the highest values in the groups of humans on a descriptive level. The negative predictive value (NPV) and positive predictive value (PPV) results for the CNN and all groups can be found inAppendix S2.

To better understand which group differences were significant, a multiple linear regression analysis on the overall diagnostic accuracy was conducted. The findings (Table 3) show thatpertinent formal qualification(coefficient of 0.083;P< .000) anddiagnostic self-confidence(coefficient of 0.015;P= <.001) were significant, while other factors, such as group differences inwork experienceand afocus on wound care, were not. The model accounted for 24.3% of the overall variance (R2= 0.243,P< .001).

Influencing factors on the overall correct answers: multiple linear regression results.

Results of the inter-rater reliability Fleiss’ kappa for more than 2 raters based on 30 votes per rater.

Health professionals with apertinent formal qualificationachieved the highest agreement (kappa = 0.531; “moderate”) compared to those without such qualifications (kappa = 0.277; “fair”) followed by persons who wereself-confidentof their diagnostic ability (kappa = 0.508; “moderate”) versus those who were not (kappa = 0.310; “fair”). Other groups that achieved a “moderate” agreement comprised persons with longwork experiencewith afocus on wound care, working in anoutpatient setting, andolderandmalepersons.

The present study is the first to compare detailed human characteristics with AI-based diagnostic performance in detecting maceration in chronic wound images. To this end, a large and heterogeneous sample of healthcare professionals was recruited, outnumbering the previous sample sizes.21Based on the selected images, the clinical task was designed to be straightforward and without ambiguities. Although the diagnostic accuracy of the AI model was rather high (90%) and outperformed the humans (79.3%), this difference was not statistically meaningful. In contrast, the poor performing groups, ie, withoutpertinent formal qualification, lowdiagnostic self-confidence, nofocus on wound care, and shortwork experience, revealed a significantly lower accuracy than the CNN. The best performing groups were those with apertinent formal qualificationand highdiagnostic self-confidence, whereby the ones that combined these 2 characteristics achieved the highest accuracy values among humans on a descriptive level. Significant differences were found in the diagnostic accuracy between the groups with and without apertinent formal qualificationand those with a high and lowdiagnostic self-confidencein assessing maceration.Work experienceandfocus on wound carehad no significant influence. Overall, the inter-rater reliability was found to be “fair,” whereby groups of individuals with apertinent formal qualificationand highdiagnostic self-confidencealso reached a higher agreement among themselves than any of the other groups. However, no group agreed at a “substantial” or higher level.

These results challenge the notion that visual wound assessment is a simple task for clinicians, highlighting the impact of individual knowledge and skill. They also open room for AI assisted wound diagnostic tools to augment the diagnostic capabilities of clinicians demonstrating a lower diagnostic performance.

To our knowledge, there are no similar studies in wound care for serving as a comparison with our results. Studies in melanoma detection show either the superior or on par performance of an AI model compared to humans.23,24,37,38For melanoma, Brinker et al. found a sensitivity value of 67.2% and specificity of 62.2% for dermatologists in lesion detection compared to the CNN achieving 82.3% sensitivity and 77.9% specificity.39Esteva et al., when evaluating a CNN trained to differentiate malignant melanomas from benign nevi and keratinocyte carcinomas from seborrheic keratoses, found an accuracy of the CNN of 72.1% compared to 2 dermatologists reaching an accuracy of 65.6% and 66%.16Judging by the absolute height of these values, they are lower than the ones we found, which could be explained by the relatively straightforward task in our study.

There was considerable variability in diagnostic accuracy, ranging from 73.1% (shortwork experience) to 85% (withpertinent formal qualification). However, onlyformal qualificationanddiagnostic self-confidencesignificantly affected human accuracy, thereby suggesting that specialized training and trust in one’s abilities make the difference.

Similar to accuracy, these 2 groups exhibited the largest interrater agreement, again demonstrating their decisive influence. In contrast to intuitive assumptions, neitherwork experiencenorwork focusin wound care were significant factors influencing the quality of the clinical judgment nor showed the highest interrater agreements.

This raises the question of what defines “expertise.” Earlier studies used job title or years of work experience,21–24,26but our findings indicate thatformal qualificationsplusself-confidenceare key, while years in healthcare or a wound care focus play secondary roles.

Potentially confounding factors such asage,gender, andhealthcare settingappear to have an equally low effect. The significant characteristics of expertise are expressed not only in the accuracy of the diagnostic judgment but also at a descriptive level in the interrater reliability as well as in other quality metrics of human judgment such as sensitivity and specificity.

While the significant superiority of AI in this task compared to the low performing humans was demonstrated by the findings, the underlying reasons can only be speculated. The superior performance could be attributed to the fact that it relies on a single training source and, therefore, a single ground truth—albeit heterogeneous regarding the images showing wounds of different diagnoses. In contrast, human experts are subject to their own individual experiences that are grounded on education and the exposition to relevant cases, which is mirrored by the variability of human judgments. A deeper understanding of these performance differences is essential for curating training datasets and designing AI systems that specifically compensate for the human weaknesses observed in everyday clinical practice. Despite this variability, the diagnostic decisions converged, and interrater reliability increased with a high level of relevant education and training plus a good amount of self-confidence.

Maceration is an example of a clinical condition that is not easy to verify by means of an objective test. Therefore, the ground truth determined by a panel of experts may still contain some degree of subjectivity. The same holds true for the labeling of the wound images that were used for training the AI model.

The fact that all of the wound images were sourced from a single wound care center might have had an unintended impact on the outcomes, potentially affecting the variability of the dataset.

On the other hand, specialized university wound centers—from where the data originate—see a great diversity of cases from a wide geographical range.

Due to the same provenance of the images, a potential limitation of this study is that the model’s performance may be overestimated due to the similarities in imaging protocols and backgrounds, which could limit its generalizability to other settings with different workflows or standards. Furthermore, we only considered dermatology residents or specialists and nurses with specialized wound care training as having pertinent formal qualifications. This may represent a bias, as it is conceivable that board-certified surgeons could also possess relevant qualifications in this area, although this is likely to apply to only a small number of individuals.

Our approach deliberately did not account for time pressure and other situational factors of clinical environments that can influence diagnostic accuracy. Therefore, future research should consider evaluating diagnostic performance in settings that more closely replicate the real-world nature of clinical decision making to ensure validity. Detecting maceration in wound images is generally considered relatively straightforward. On top of this, the 30 images selected represented clear cases of maceration and no maceration, making it a rather simple task for both humans and the AI model. It can be hypothesized that the differences in accuracy and reliability among the health professionals become more pronounced as the task and the maceration cases become more difficult. In addition, the response times were not measured, which might have provided further insight into the decision-making process.

The potential of AI-assisted tools to enhance assessment consistency in clinical practice is worth further investigation. When developing AI models and labeling data, it is essential to acknowledge the inherent variability in human judgment, even among experts. Although this study contributed to clarifying the role of human expertise when comparing AI with human decisions, there remains the need to further investigate factors beyond expertise explaining the variance in diagnostic accuracy.

Our results indicate that while clinicians generally demonstrate a rather high degree of accuracy in diagnosing wound maceration, individual factors significantly influence their performance. In particular,pertinent formal qualificationsandself-confidencein their abilities impact the clinician’s accuracy. It is noteworthy that these 2 characteristics are only moderately correlated, pointing at two more or less distinct constructs.

Insights of this study may also have practical implications. Considering AI systems as sparring partners for clinicians, poor performing clinicians in this seemingly straightforward task might be good candidates for improving their skills with AI.

The authors would like to express their sincere gratitude to the participating physicians, nurses, and their respective hospitals for their valuable contributions to this study. The authors would also like to thank Cornelia Erfurt-Berge, Sybille Kahnt, Alexandra Kätscher, Kerstin Wolf, and Beata Zschieschang for rating the wound images in order to establish the ground truth as well as Leila Malihi for providing and executing the CNN algorithm.

Florian Kücking, 
Research Center of Health and Social Informatics, Osnabrück University of Applied Sciences, 49009 Osnabrück, Germany.

Ursula H Hübner, 
Research Center of Health and Social Informatics, Osnabrück University of Applied Sciences, 49009 Osnabrück, Germany.

Dorothee Busch, 
Research Center of Health and Social Informatics, Osnabrück University of Applied Sciences, 49009 Osnabrück, Germany; 
Department of Dermatology, University Hospital, Friedrich-Alexander University Erlangen-Nürnberg, 91012 Erlangen, Germany.