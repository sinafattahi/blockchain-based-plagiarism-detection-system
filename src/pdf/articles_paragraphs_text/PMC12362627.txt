Female depression is a prevalent and increasingly recognized mental health issue. Due to cultural and social factors, many female patients still face challenges in diagnosis and treatment, and traditional assessment methods often fail to identify high-risk individuals accurately. This highlights the necessity of developing more precise predictive tools. Utilizing machine learning (ML) algorithms to construct predictive models may overcome the limitations of traditional methods, providing more comprehensive support for women’s mental health.

To construct an ML-nomogram hybrid model that translates multivariate risk predictors of female depressive symptoms into actionable clinical scoring thresholds, optimizing predictive accuracy and interpretability for healthcare applications.

We analyzed data from 7609 female participants aged 18 to 85 years from the Guangdong Provincial Sleep and Psychosomatic Health Survey. Sixteen variables, including anxiety symptoms, insomnia, chronic diseases, exercise habits, and age, were selected based on prior literature and comprehensively incorporated into ML models to maximize predictive information utilization. Three ML algorithms, extreme gradient boosting, support vector machine, and light gradient boosting machine, were employed to construct predictive models. Model performance was evaluated using accuracy, precision, recall, F1 score, and area under the curve (AUC). Feature importance was interpreted using SHapley Additive exPlanations (SHAP), with ablation studies validating the impact of the top five SHAP-derived features on predictive performance, and a nomogram was constructed based on these prioritized predictors. Clinical utility was assessed through decision curve analysis.

The prevalence of depressive symptoms was 6.8% among the sample. The evaluation of predictive models revealed that light gradient boosting machine achieved a top-performing AUC of 0.867, placing it ahead of extreme gradient boosting (AUC = 0.862) and support vector machine (AUC = 0.849). SHAP analysis identified insomnia, anxiety symptoms, age, chronic disease, and exercise as the top five predictors. The nomogram based on these features demonstrated excellent discrimination (AUC = 0.910) and calibration, with significant net benefits in decision curve analysis compared to baseline strategies. The model effectively stratifies depressive symptoms risk, facilitating personalized and quantitative assessments in clinical settings. We also developed an interactive digital version of the nomogram to facilitate its application in clinical practice.

The ML-based model effectively predicts depressive symptoms in women, identifying insomnia, anxiety symptoms, age, chronic diseases, and exercise as key predictors, offering a practical tool for early detection and intervention.

Core Tip:This study leverages machine learning to develop a highly accurate predictive model for depressive symptoms in women. Ablation studies systematically validated the critical contributions of these top-ranked SHapley Additive exPlanations features, demonstrating significant performance degradation upon their removal. The light gradient boosting machine model achieved superior performance, supported by SHapley Additive exPlanations for interpretability and a nomogram for clinical application. This innovative approach offers a practical tool for early detection and personalized intervention, addressing the limitations of traditional methods. Findings highlight the potential of machine learning to enhance women’s mental health outcomes, with implications for improving diagnostic precision and treatment strategies in diverse clinical settings.

Depressive disorder, also known as depression, is a leading cause of global burden[1], disproportionately affecting women, with a prevalence of approximately 5% among adults and 6% among women. Female gender is considered a significant risk factor for depression[2-4]; the prevalence of major depressive disorder in women is twice that in men after reaching adulthood[5]. Depression, characterized by persistent depressed mood or loss of pleasure in activities, is associated with elevated risks of premature mortality[6,7], chronic diseases, reduced work capacity, and impaired social functioning[8,9]. Although reproductive health and maternal care have been prioritized to date, women’s healthcare needs extend far beyond the areas prioritized to date[8].

In China, rapid economic development and urbanization over the past three decades have contributed to an increase in depressive symptoms and mood disorders[10,11]. Guangdong province, with its dense population and high economic status, reported a GDP exceeding 14 trillion yuan in 2024. While economic growth has improved living standards, it has also intensified social pressures and family role conflicts, potentially heightening the risk of depression among women[12]. Female patients with depression face challenges in diagnosis and treatment due to cultural and social factors.

Advanced machine learning (ML) algorithms has significantly improved depression prediction by processing more variables and capturing complex nonlinear relationships, enhancing prediction accuracy and early diagnosis. However, ML models face dual implementation barriers in primary care: Limited interpretability of algorithmic outputs and low clinician trust due to unverified decision logic[13]. To address these challenges, recent studies have explored ML-nomogram hybrid approaches, demonstrating significant value in bridging the algorithmic precision of multivariate risk prediction with clinical interpretability. These hybrids transform complex variable interactions into actionable scoring thresholds, facilitating individualized risk stratification and treatment optimization[13]. Studies have shown the effectiveness of such hybrids in predicting depression in various populations, including cross-cultural models for adolescent self-injury[14] and multimodal algorithms integrating biopsychosocial predictors for perinatal depression screening[15].

However, research on ML-based depression prediction, especially among women in China, remains limited. While previous studies have focused on specific subgroups such as postpartum women[15], the elderly[16], and college students[17], there is a paucity of research on general women populations. This study proposed an ML-driven nomogram that translates complex risk factors of depressive symptoms in women into a clinically actionable scoring system, enabling dynamic risk assessment and personalized prevention strategies, using data from the Guangdong Provincial Sleep and Psychosomatic Health Survey (GSPHS). This study adopts rigorous data processing techniques, including feature engineering, as endorsed by recent ML studies on depression prediction[18], ensuring the robustness and interpretability of the predictive model.

The GSPHS was a population-based cross-sectional study conducted from September to November 2019 in Guangdong Province, China. It targeted a representative sample of adults aged 18 to 85 years. It was conducted under the guidance and oversight of the Guangdong Mental Health Center. Using a multi-stage sampling method, 17132 adults were randomly selected from 180 communities across 72 districts or counties in 21 prefecture-level cities, with 100 residents chosen from each community. Of these, 13768 individuals responded to the survey, yielding a response rate of 80.4%. The survey methodology for this study is elaborated upon in our previous research[19]. The GSPHS dataset served as the source for the data utilized in the machine learning model. We extracted 7609 female participants from the GSPHS to conduct this study. The study used a complete dataset with no missing values for the variables included. The data processing and model-building processes of ML are displayed in Figure1.

The flow diagram of data processing and model building process.XGBoost: Extreme gradient boosting; SVM: Support vector machine; LightGBM: Light gradient boosting machine; AUC: Area under the curve.

The Patient Health Questionnaire-9 (PHQ-9), containing nine items, was applied to assess depressive symptoms during the last two weeks, which was founded on the major depression standard of Diagnostic and Statistical Manual for Mental Disorders[20-22]. A total score ranges from 0 to 27, with greater values indicating higher levels of depressive symptoms. The Chinese PHQ-9 is universally verified with favorable psychometric properties for screening depression in the general population[23,24]. In this study, the total score of PHQ-9 > 10 was defined as clinical depressive symptoms[20]. Based on previous research[25,26], the outcome was defined as whether the score on the PHQ-9 was 10 or above.

The Generalized Anxiety Disorder-2 (GAD-2) scale demonstrates robust psychometric validity and reliability for screening generalized anxiety symptoms, though its diagnostic accuracy varies across cultural contexts[27,28]. Subjects were inquired how often they were disturbed by the two symptoms of GAD in the last two weeks. The GAD-2 scale assesses two core symptoms of GAD: (1) “Feeling nervous, anxious, or on edge”; and (2) “Not being able to stop or control worrying” over the past two weeks, with each item scored 0-3 based on frequency[29]. Studies in Chinese populations found that at its optimal cutoff, the GAD-2 scale demonstrated strong diagnostic performance, with high sensitivity and specificity, as well as a notably elevated diagnostic odds ratio[30]. In this study, a total score of GAD-2 ≥ 3 was representative of anxiety symptoms. To evaluate sleep quality and disorders over the previous few weeks, the Pittsburgh Sleep Quality Index (PSQI), a 19-item self-report survey, is utilized. PSQI is suitable for clinical psychiatry and sleep quality evaluation studies in China, offering reliability and effectiveness in assessing and screening sleep disorders in the Chinese population[31]. Sleep quality was assessed using the PSQI, with higher scores indicating poorer sleep quality. A total PSQI score > 7 is representative of sleep problems[32]. Additionally, data were collected on body mass index (BMI), chronic diseases, and social-demographic and lifestyle factors, including place of residence, marital status, education level, income class, drink, tea consumption, smoking status, exercise frequency, sleep latency, sleep duration, and sleep efficiency. Detailed measurement protocols for these variables are provided inSupplementary material.

Drawing upon predictors of depressive symptoms identified in prior research[33,34] and accounting for the variables contained within our data source, we selected 16 factors for analysis. These included: Age, residence, BMI, education year, marital status, income class, smoke, drink, tea, exercise, chronic diseases, insomnia, anxiety symptoms, sleep latency, sleep duration, and sleep efficiency. All variables are presented in Table1.

Data was analyzed using Statistical Package for the social sciences software (version 25.0) and R language (version 4.2.0). Categorical variables are summarized as frequencies and proportions. Categorical variables are expressed asn(%) and compared between groups using theχ2test. Results were considered significant whenP< 0.05 (two-sided). All ML algorithms were operated in Python (version 3.10). The “shinydashboard” and ”DynNom” packages in R were utilized to build an interactive online nomogram, which serves to simplify the clinical use of our prediction model.

Three separate risk prediction models were constructed using the most popular ML methods commonly applied for classification problems. The analysis was conducted using three ML algorithms: Extreme gradient boosting (XGBoost), support vector machine (SVM), and Light gradient boosting machine (LightGBM), with subsequent accuracy comparisons conducted. These algorithms were selected due to their proven effectiveness in handling complex, high-dimensional categorical data and their ability to capture non-linear relationships and feature interactions, which are critical for accurately predicting depressive symptoms. XGBoost works by building a series of decision trees, where each tree improves upon the predictions of the previous one[35]. It continuously refines its accuracy through this process, making it fast and highly adaptable for handling the large number of variables and data points in this study. Capable of both classification and regression, SVM functions by defining a distinct separation between different classes and utilizes “kernel” methods to handle complex data structures[36]. To solve large problems efficiently, it breaks them down into smaller parts, allowing for faster analysis and better adaptation to new data[37]. Designed for large datasets, LightGBM is an efficient algorithm that trains models quickly, uses minimal memory, and handles categorical variables effectively[38]. It also supports parallel processing, making it ideal for the large-scale data in this study.

The systematic comparison of three ML architectures is necessitated by the need to identify the optimal model that balances predictive performance, computational efficiency, and generalizability in clinical decision-making scenarios. Each model’s performance was evaluated using multiple metrics, including accuracy, precision, recall, F1 score, and area under the curve (AUC). The receiver operating characteristic (ROC) curve offers a visual depiction of the trade-offs between a classifier’s true positive rate and false positive rate, thereby serving as a key evaluation tool[39]. It is a unit square plot for simultaneously displaying the tradeoff between the true positive rate (TPR) and the false positive rate (FPR). TPR is the probability that the model incorrectly predicts the positive class. FPR is the probability that the model incorrectly predicts the positive class for a binary classifier at different classification thresholds[39,40]. A model’s classification performance can be quantified by the AUC, which is calculated as the total area beneath the ROC curve. It provides a numerical value for comparing the performance of different models or indicators. A perfectly calibrated model is characterized by a calibration curve that follows the diagonal y = x line, indicating a perfect correspondence between predicted probabilities and actual outcomes[41]. Consequently, any deviation from this diagonal, in terms of the curve’s shape or position, is indicative of the model’s predictive bias and consistency[41]. Decision curve analysis (DCA) evaluates predictive models by calculating net benefit across various threshold probabilities. It plots net benefit against threshold probability, considering clinical consequences[42]. DCA determines the clinical net benefit of a model by benchmarking it against the default strategies of treating all or treating no patients, thereby providing a more clinically relevant assessment than traditional accuracy metrics[43].

where FP and FN represent false positive and false negative, respectively.

Specificity reflects a model’s ability to correctly identify negative examples. It is calculated as the fraction of all true negative instances that are successfully classified as such. This metric is paramount in situations where the consequences of a false positive, incorrectly classifying a negative case as positive, are particularly high.

F1score= 2 × (PPV × sensitivity)/(PPV + sensitivity).

In order to combat overfitting, we randomly divided the sample into training and testing sets in a 7:3 ratio, ensuring the balance of positive and negative samples was preserved. The training set provides the data for the model to learn from, whereas the validation set is used iteratively to optimize hyperparameters and ultimately select the model with superior performance. No oversampling techniques were applied, as preliminary tests with synthetic minority oversampling technique yielded no performance gains, likely due to the categorical nature of the data and the inherent robustness of the classifiers to imbalance[44].

For model validation, we used an internal test cohort, separate from the training set, and performed cross-validation to ensure the robustness and generalizability of the models. After applying data normalization, the feature set was refined by removing predictors with very low variance, those with a low correlation to the target label, and those exhibiting strong inter-correlations. This process was undertaken to improve the overall quality of the data for modeling.

In this study, we utilized three ML algorithms: LightGBM, SVM, and XGBoost. To optimize their performance, we employed grid search with cross-validation to tune the hyperparameters for each model. This process involved systematically evaluating various combinations of hyperparameters on the training dataset and selecting the best set based on their average performance across the cross-validation folds. The detailed process of hyperparameter grid optimization, including the specific ranges explored for each model and the selection criteria, is presented inSupplementary material. Using these optimized hyperparameters, we trained the final models on the entire training dataset and evaluated their performance on the independent test set, as presented in the results section.

We utilized a repeated five-fold cross-validation method (5 repeats) on the training data to train and optimize the hyperparameters for three classification algorithms. Each model’s performance was quantified by its average accuracy across the validation folds, and the algorithm with the superior AUC was ultimately chosen as our final predictive model. In this study, we evaluated model performance using only the test set and presented the ROC calibration, and DCA based on this set. While the training set is crucial for model development, the test set is essential for assessing the model’s generalizability and real-world performance. The test set contains data the model has not seen during training, allowing for an unbiased evaluation of its ability to generalize to new and unseen data. Focusing on the test set helps mitigate overfitting, ensuring that the reported performance metrics accurately reflect the model’s expected performance in real-world scenarios.

Model interpretation is crucial for understanding the factors driving model predictions. We utilized SHapley Additive exPlanations (SHAP) for model interpretation, specifically to determine the importance of each feature in predicting the risk of depressive symptoms. Rooted in cooperative game theory, SHAP equitably assigns a Shapley value to each feature[45], representing its marginal contribution to the ML model’s prediction. This is achieved by averaging its impact across all feature permutations, resulting in clear explanations for the model’s output[46].

An ablation study was conducted to determine the impact of the five most influential features, as identified by SHAP, on the predictive performance of the optimal model. This systematic approach quantifies the necessity of key predictors by iteratively removing them, enabling clinically interpretable feature prioritization while maintaining predictive accuracy. The selected model, determined by the highest AUC across LightGBM, XGBoost, and SVM candidates, was retrained iteratively on progressively reduced feature subsets. Specifically, each of the top five SHAP-derived features was sequentially excluded from the original feature set, and the model was retrained using identical hyperparameters and preprocessing steps to ensure comparability. To quantify the impact of individual features, the relative decrease in AUC between the full model and each ablated model was computed, with statistical significance assessedviaDelong’s test for paired AUC comparisons. Additionally, a comprehensive ablation was performed by simultaneously excluding all five features to evaluate their collective contribution. All analyses were repeated with 1000 bootstrap resamples to estimate 95% confidence intervals (CIs) for performance metrics, ensuring robustness against sampling variability. This approach not only validated the necessity of the selected features but also provided insights into their clinical interpretability, aligning with the goal of developing a parsimonious, interpretable nomogram for real-world depression risk stratification.

We constructed a nomogram based on the top five important features to enhance prediction accuracy and interpretability. The selection of the top five SHAP-derived features for nomogram construction optimally balanced predictive accuracy, clinical practicality, and statistical robustness, adhering to the Pareto principle where these features captured the majority of predictive power while avoiding overfitting risks inherent in higher-dimensional models. The nomogram enabled the calculation of the total risk score for individual patients and predicted the probability of depressive symptoms. Each feature factor is represented by an independent scale line in the figure. We performed DCA to assess the clinical utility of the predictive model, evaluating the net benefit of using the model at different threshold probabilities[43].

The study included 7601 participants, with 93.1% (7090) classified as non-depressive and 6.8% (519) exhibiting depressive symptoms (Table1). The mean age was 47.52 years (± 14.00), showing a significant intergroup difference (P< 0.001). Notably, 73.0% of the total cohort were aged 18-39 years, with a higher proportion in the depressive group (85.9%) compared to the non-depressive group (72.0%,P< 0.001). Geographically, 78.0% resided in urban areas, with minimal difference between groups (79.2% depressivevs77.9% non-depressive,P= 0.488). The depressive symptoms group had more single/unmarried individuals (47.2%vs30.8%), higher alcohol consumption (15.2%vs6.3% consuming ≥ 1 time/week), lower exercise frequency (47.0%vs29.7% hardly exercising), and higher smoking rates (6.9%vs1.3%) (allP< 0.001). Clinically, they showed a higher prevalence of underweight BMI (< 18.5 kg/m²; 22.5%vs17.1%,P= 0.0032), anxiety (62.2%vs2.5%), insomnia (74.6%vs14.7%), and poorer sleep quality, with lower sleep efficiency (> 85%; 65.9%vs79.1%) and fewer achieving ≥ 7 hours of sleep (22.9%vs40.8%) (allP< 0.001). Notably, chronic diseases were less prevalent in the depressive symptoms group (47.0%vs77.6%,P< 0.001). Educational attainment and income class were similar between groups.

The calibration of the various models is visually assessed using the calibration curves presented in Figure2. The calibration curves revealed distinct model behaviors in aligning with the ideal diagonal. LightGBM exhibited the closest overall fit, though systematic underprediction occurred in mid-range probabilities (0.4-0.7), where predicted probabilities corresponded to 0.4-0.5 observed frequencies. XGBoost showed larger deviations at approximately 0.4 and approximately 0.7 probabilities, indicating suboptimal calibration compared to LightGBM. SVM displayed severe miscalibration with erratic fluctuations, notably overpredicting outcomes (observed fraction = 1.0) at approximately 0.5 predicted probabilities, reflecting potential overfitting. All models partially aligned with the diagonal in high-probability ranges (0.8-1.0), with LightGBM converging most smoothly.

The receiver operating characteristic curve of all models.ROC: Receiver operating characteristic; XGBoost: Extreme gradient boosting; SVM: Support vector machine; LightGBM: Light gradient boosting machine; AUC: Area under the curve.

The performance of three ML models, XGBoost, SVM, and LightGBM, was evaluated using the test set, as presented in the ROC curve (Figure3). The models’ ability to distinguish between the two classes was assessed using the AUC. The testing set ROC curves presented in the right subplot demonstrate robust discriminatory performance across three ML models. LightGBM (blue line) achieved the highest AUC of 0.867, with its trajectory closely paralleling that of XGBoost (green line, AUC = 0.863), indicating comparable classification efficacy between these two ensemble methods. The SVM classifier (orange line) exhibited marginally inferior performance with an AUC of 0.850. The tight clustering of LightGBM and XGBoost trajectories (AUC difference < 0.004) suggests technical parity in generalization capability. The ROC curves for all three models were plotted, exhibiting a consistent trend of high TPR as the FPR increases. The diagonal dashed line represents the performance of a random classifier, which serves as a baseline for comparison. All three models significantly outperformed the random classifier, evidenced by the curves being well above the diagonal, further confirming the robustness of the models. These results suggested that LightGBM demonstrated the best performance among the three algorithms, followed by XGBoost and SVM.

Calibration curve of all models.XGBoost: Extreme gradient boosting; SVM: Support vector machine; LightGBM: Light gradient boosting machine.

A summary of the performance indicators for the three ML models on the test set is provided in Table2. The assessment included accuracy, precision, recall, F1 score, and AUC. The comparative performance analysis of XGBoost, SVM, and LightGBM models reveals nuanced differences in their predictive capabilities across key evaluation metrics. XGBoost demonstrates balanced performance with an accuracy of 0.941, slightly lower than SVM (0.946) and LightGBM (0.944), though the marginal differences suggest comparable classification reliability among all three models. Notably, SVM achieves the highest precision (0.707), outperforming XGBoost (0.603) and LightGBM (0.670), which aligns with its theoretical strength in maximizing margin separation to reduce false positives. However, XGBoost exhibits superior recall (0.429) compared to both SVM (0.371) and LightGBM (0.378), indicating its relatively stronger ability to identify true positive instances in class-imbalanced scenarios.

XGBoost: Extreme gradient boosting; SVM: Support vector machine; LightGBM: Light gradient boosting machine; AUC: Area under the curve.

The F1-score, harmonizing precision and recall, further highlights XGBoost’s advantage (0.501) over SVM (0.487) and LightGBM (0.483), suggesting its effectiveness in balancing type I and type II errors for this task. In terms of discriminative power measured by AUC, LightGBM marginally outperforms others with 0.867, followed by XGBoost (0.862) and SVM (0.849), reflecting LightGBM’s optimized tree-growing strategy and histogram-based acceleration in capturing complex feature interactions. These observations corroborate existing literature where XGBoost’s robustness on smaller datasets and LightGBM’s efficiency in handling feature heterogeneity contribute to their respective strengths. While SVM excels in precision-critical applications, the ensemble methods (XGBoost and LightGBM) show more balanced overall performance, particularly in scenarios requiring trade-off optimization between sensitivity and specificity. In this study, the LightGBM model achieved a high AUC score, indicating its strong reliability in predicting depressive symptoms. Given that accurate risk prediction is crucial for effective intervention, we selected the LightGBM model as the optimal choice to further conduct SHAP-based feature importance analysis.

Feature importance was assessed using SHAP values (Figure4). In this study, we analyzed the key features influencing the presence of depressive symptoms using SHAP values. The results revealed significant differences in the predictive contributions of various features to depressive symptoms. The figure illustrates the mean absolute SHAP values of these features; blue bars represent the contributions from individuals without depressive symptoms, and purple bars indicate contributions from those with depressive symptoms. The horizontal axis quantifies feature impact magnitude, while vertical axis ranks features by overall importance. Among the features, insomnia was the most critical factor, with significantly higher SHAP values in individuals with depressive symptoms compared to those without, highlighting insomnia as a strong predictor of depressive symptoms. Anxiety symptoms demonstrated a strong association with depressive symptoms. Individuals with depression exhibited markedly higher SHAP values for anxiety symptoms. The SHAP analysis showed wider distribution of mean absolute SHAP values for age in non-depressive (blue bar)vsdepressive groups (purple bar). Chronic diseases showed balanced contributions across groups. Exercise emerged as a key predictor, with non-depressive individuals displaying higher mean absolute SHAP values, suggesting stronger protective effects of habitual exercise in non-clinical populations.

Feature contributions by class using SHapley Additive exPlanations values.LightGBM: Light gradient boosting machine; SHAP: SHapley Additive exPlanations.

Based on the SHAP values, we identified the top five variables with the highest impact on the outcome, which were insomnia, anxiety, age, chronic disease, and exercise. To systematically evaluate the contribution of each feature to model performance, we conducted an ablation study comparing the complete model (with all features) against models with individual features removed (insomnia, anxiety, age, chronic disease, exercise), as illustrated in Figure5. The baseline model (all features) achieved an AUC of 0.875. Experimental results demonstrated: Removing the age feature increased AUC to 0.884 (ΔAUC = +0.009), while removing exercise elevated AUC to 0.881 (ΔAUC = +0.006), suggesting potential redundancy or noise introduction from these features. Elimination of chronic disease decreased AUC to 0.869 (ΔAUC = -0.006), confirming its marginal positive contribution. Notably, removing insomnia caused a dramatic AUC decline to 0.804 (ΔAUC = -0.071), and anxiety removal reduced AUC to 0.850 (ΔAUC = -0.025), establishing both as critical predictors, with insomnia exhibiting 2.8-fold greater impact than anxiety. This graduated performance variation reveals asymmetric feature importance distribution.

Ablation study on key features using the receiver operating characteristic curve.LightGBM: Light gradient boosting machine; ROC: Receiver operating characteristic; AUC: Area under the curve.

A model was established for predicting the risk of depressive symptoms in women using a nomogram, as illustrated in Figure6. The model incorporates multiple predictive variables to calculate a total risk score based on the points assigned to each variable. Scoring can be performed for each patient according to these established risk factors; a higher total score indicates a greater likelihood of depressive symptoms. We have developed an online digital version of this nomogram, which is accessibleviaSupplementary material.

Nomogram for predicting the risk of depression based on insomnia, anxiety symptoms, chronic disease, exercise, and age.The total points are used to calculate the linear predictor and risk probability.

ROC curves for the training and internal test cohorts are given in Figure7, evaluating the model’s performance. The X-axis represents the FPR (1-specificity), while the Y-axis represents the TPR (sensitivity). The ROC analysis revealed excellent model discrimination, with the internal test cohort achieving an AUC of 0.910 (95%CI: 0.885-0.935), closely matching the training cohort’s 0.906 (0.889-0.922). Both cohorts’ curves showed near-identical trajectories tightly clustered near the upper-left quadrant, consistently outperforming the random-chance diagonal across the full FPR spectrum (0-1.00). Overlapping CIs and parallel curve progression confirmed minimal performance divergence, indicating strong generalization without overfitting. At clinically actionable thresholds, the model maintained a sensitivity > 0.75 while controlling FPR < 0.25 in both cohorts. These findings demonstrate robust predictive consistency for depression risk stratification in unseen validation data, supporting clinical translation potential.

The receiver operating characteristic curves for the predictive model in the training and internal test cohorts.AUC: Area under the curve.

The clinical utility of the model was evaluated using DCA, which assessed the net benefit of the model at various probability thresholds for predicting depressive symptoms in women. The calibration plot for the logistic regression model (Figure8) demonstrates moderate miscalibration with systematic underprediction across low-to-moderate risk ranges. Between 25% and 75% predicted risk (X-axis), the observed risk curve lies below the ideal calibration diagonal while exhibiting pronounced local fluctuations. This systematic deviation peaks in the mid-range probabilities, suggesting conservative risk estimation for intermediate-risk subgroups. Improved alignment emerges in higher risk strata (75%-100% predicted risk), where the curve closely follows the diagonal despite a transient downward deflection near 85% predicted risk. Notably, the model achieves near-perfect calibration convergence in the extreme upper decile (90%-100% predicted risk), with minimal residual fluctuations closely tracking the diagonal. The preserved discriminative capacity is evidenced by an AUC of 91.0 (95%CI: 88.5-93.5), complemented by a Brier score of 4.3 (95%CI: 3.7-5.0), indicating both strong predictive accuracy and acceptable calibration error magnitude. These patterns collectively reveal context-dependent calibration performance, with particular clinical relevance in high-risk stratification where prediction-reality concordance becomes critical for intervention decisions.

Calibration plot for the logistic regression model.The plot shows the relationship between predicted and observed risks. AUC: Area under the curve.

Decision curve analysis for the predictive model.The plot shows the net benefit of the model across varying thresholds of high risk, compared to strategies of treating all patients or treating none.

Population characteristics: Depressive groups showed higher proportions of younger adults (18-39 years) and unmarried individuals, with elevated unhealthy behaviors (smoking, alcohol use) and poorer sleep quality. Paradoxically, chronic disease prevalence was lower in this group, potentially linked to their younger demographic. Model performance: LightGBM outperformed XGBoost and SVM in predictive accuracy (AUC = 0.867), demonstrating superior calibration and balance between precision and recall. Its mid-range probability predictions aligned more closely with observed outcomes than other models. Predictive determinants: Insomnia and anxiety emerged as dominant predictors, with SHAP analysis revealing insomnia’s disproportionate influence compared to other factors. Chronic diseases contributed minimally, suggesting sample selection bias may obscure their role. Clinical utility: The model achieved robust discrimination in high-risk stratification and outperformed blanket intervention strategies in DCA. Its calibration near extreme-risk thresholds supports precision in clinical decision-making. Limitations: Age-related confounders in chronic disease patterns and conservative mid-risk calibration highlight areas for refinement. While SHAP clarified feature importance, causal pathways remain unverified.

Recent research on ML approaches for female depression prediction has demonstrated evolving multidisciplinary frameworks. Studies have integrated electronic health records with psychological assessments to develop composite risk models, enhancing identification of high-risk populations[47]. Biomarker discovery through proteomic analysis has revealed potential blood-based indicators linked to postpartum depression pathophysiology, with ML validating their predictive relevance[48]. Multicenter investigations employing various algorithms emphasized the effectiveness of combining biological, psychological, and social predictors, particularly highlighting postnatal factors’ critical role in model optimization[15]. Cross-cultural examinations revealed how family dynamics and regional caregiving patterns influence depression risk stratification in female populations, with ensemble learning methods showing particular efficacy in culturally contextualized prediction[14]. These developments collectively advance personalized intervention strategies while underscoring the necessity of incorporating sociocultural dimensions into computational prediction frameworks for women’s mental health. The SHAP-driven selection of top 5 features coupled with nomogram visualization aligns with emerging trends in translational bioinformatics, where model transparency enhances clinical utility while maintaining competitive performance comparable to complex ensembles[49,50]. Despite existing research employing ML methods for depression prediction, studies specifically targeting the female population remain scarce. Developing models particularly for females addresses the unique impacts of depression on women and provides clinicians with more personalized and precise diagnostic and therapeutic support.

In this study, the prevalence of moderate and above depressive symptoms was 6.8%, close to the results of previous studies[51,52]. The observed lower prevalence of chronic diseases in the depressive group may reflect age-related selection bias, as the cohort predominantly comprised younger individuals (85.9% aged 18-39) who were less likely to develop age-dependent conditions like cardiovascular or metabolic disorders[53]. This paradox also stems from cross-sectional study limitations, where younger depressive populations were captured before accumulating comorbidities linked to aging trajectories.

Our study systematically evaluated their performance, filling the research gap in parallel comparisons of these models. We ensured high external validity and generalizability by employing GSPHS data, which includes a diverse population of women across various ages, health statuses, and social backgrounds. Innovations in data preprocessing, including feature selection and data balancing, further enhanced model performance.

A large body of literature exists on the application of ML models for predicting depression; yet, a predominant focus on single-model evaluations is evident in many of these studies[54,55]. To predict depressive symptoms in women, we compared and evaluated three mainstream ML methods: XGBoost, SVM, and LightGBM. After evaluating multiple ML models, we selected the optimal model based on the AUC. AUC is particularly suitable for medical classification tasks given its robustness to class imbalance[56]. In classification tasks within medicine and psychology, AUC is widely adopted due to its robustness to class imbalance. Selecting this model indicated that it performed best in ranking positive instances higher than negative ones, which is critical for clinical applications[57].

The outstanding performance of LightGBM, as evidenced by its higher AUC compared to XGBoost and SVM, could be attributed to its efficient handling of high-dimensional categorical features, adaptability to sparse data patterns, and unique algorithmic design for capturing complex feature interactions. Our dataset consisted exclusively of categorical variables spanning demographic characteristics, behavioral factors, and clinical variables. A key advantage lay in LightGBM’s native support for categorical features without one-hot encoding, avoiding sparse matrix issues. For example, variables like “exercise frequency” were processedviaoptimal splitting, while SVM required binary encoding that inflated dimensionality. LightGBM further optimized categorical splits using specialized parameters, enhancing performance. The prediction of depressive symptoms often involves complex interactions among multiple variables[58,59]. For example, marital status and income level may jointly influence depression risk[60], while sleep quality and anxiety levels may exhibit interaction effects[61]. LightGBM’s leaf-wise tree growth and hierarchical partitioning effectively captured variable interactions, a characteristic that explains its superior performance. In medical and psychological data, features often exhibit non-linear relationships with the target variable. For example, BMI may show a U-shaped relationship with depressive symptoms, where both underweight and overweight individuals are at higher risk[62]. LightGBM demonstrates enhanced capability in identifying complex nonlinear patterns through its gradient-boosting framework and tree-based architecture[63]. This algorithmic advantage typically yields superior performance in AUC metrics compared to linear models or suboptimally configured SVM implementations. This characteristic likely explains LightGBM’s outperformance in our dataset, particularly when analyzing data containing such intricate pattern structures. With low depression prevalence (< 10%), AUC robustly assessed imbalanced data performance[64]. LightGBM addressed class imbalanceviaparameters like scale_pos_weight and leveraged histogram-based learning for computational efficiency. In contrast, SVM struggled with high dimensionality from one-hot encoding. These findings underscore LightGBM’s suitability for medical data with complex patterns. Future studies should employ feature importance analysis to elucidate critical interactions and enhance interpretability.

We applied SHAP to identify the top five predictive variables, improving the clinical interpretability of these “black-box” algorithms. To better understand the contribution of each factor and refine the model accordingly, we incorporated the top five predictive variables into the ablation study. The ablation study results revealed deeper insights into the relative importance of the top five SHAP-ranked features in predicting depressive symptoms. The removal of the insomnia feature led to a significant decline in AUC, underscoring its critical role in predicting depressive symptoms. Similarly, excluding the anxiety feature resulted in reduced AUC, confirming its importance as a predictive factor. In contrast, eliminating the chronic disease feature caused only a marginal AUC decrease, suggesting its relatively minor contribution to model performance. This may reflect either an indirect relationship between chronic diseases and depression or partial mediation of its effects through other features. Interestingly, removing age and physical activity features resulted in slight AUC improvements, a counterintuitive finding. Although SHAP values identified these features as important, their exclusion may indicate that they introduce noise or overfitting in the model. The improved performance without these features could imply better generalization or potential redundancy, where their predictive effects are already captured by other variables. This phenomenon warrants further investigation to explore whether age and physical activity contribute redundant signals or noisy patterns in depression prediction.

These variables, insomnia, anxiety symptoms, chronic disease, exercise, and age, were used to construct a nomogram. The nomogram demonstrated excellent discrimination, calibration, and clinical utility, providing clinicians with a quantifiable tool for depression risk stratification. DCA confirmed its superior net benefit compared to “treat-all” and “treat-none” strategies. The identified predictors align with established roles in depression pathogenesis and offer practical insights for risk assessment. Our model’s strong performance highlights its potential as a clinical screening instrument, supporting personalized risk assessment and early intervention. This data-driven approach could enhance screening protocols, improve evidence-based decision-making, and boost patient outcomesviatargeted prevention strategies.

Insomnia emerged as the top variable influencing the model’s predictive performance. Poor sleep quality or sleep disorders are commonly observed in individuals with depression, often co-occurring with a higher prevalence of chronic illnesses, indicating worse overall health in this population. The relationship between sleep and depression is complex and bidirectional. Insomnia and sleep disturbances exacerbate depressive symptoms by increasing stress, impairing emotional regulation, and disrupting neurobiological pathways, involving the hypothalamic-pituitary-adrenal (HPA) axis. Depression itself can lead to sleep dysfunction, creating a vicious cycle that perpetuates both conditions. Improved sleep quality, including more days per week of restful sleep and reduced sleep dysfunction, is associated with decreased depression[65-68]. Accordingly, it is critical to address sleep disturbances as a key component in managing depression, specifically in women. Women are more likely to experience insomnia due to hormonal fluctuations, caregiving responsibilities, and higher stress levels; their vulnerability to depression may be further heightened by poor sleep.

The second most significant variable identified for anxiety symptoms was consistent with previous literature, indicating that these factors are crucial in the onset and progression of depressive symptoms[69]. Our study found that individuals with anxiety demonstrated a higher risk of experiencing symptoms of depression, supporting their frequent comorbidity and potential shared mechanisms, with anxiety being China’s most prevalent lifetime disorder[70]. Major depressive disorder is associated with anxiety disorders, particularly panic disorder, GAD and post-traumatic stress disorder[71,72], with emerging evidence highlighting shared neurobiological mechanisms. Both disorders involve dysregulation of the serotonin (5-HT) and norepinephrine systems, as well as HPA axis hyperactivity[73-75]. Reduced 5-HT function in the prefrontal cortex and limbic system is linked to heightened anxiety, emotional reactivity, and impaired stress coping[74,76], while elevated norepinephrine amplifies arousal and vigilance[73]. Chronic stress drives HPA axis hyperactivity, elevating cortisol levels and causing hippocampal atrophy, prefrontal cortex dysfunction, and amygdala hyperactivity, key contributors to emotional dysregulation and symptom persistence[77-79]. Cortisol further disrupts 5-HT/norepinephrine balance, exacerbating pathophysiology[80]. The bidirectional relationship between anxiety and depression underscores their interdependence, necessitating integrated prevention and treatment[72]. Sexual dimorphism in stress responses (e.g.,women’s “tend and befriend” pattern) and sex hormones’ regulatory effects on HPA axis activity may heighten women’s vulnerability to anxiety[73], emphasizing the need for sex-specific approaches in research and clinical practice.

Age emerged as a significant predictor of depressive symptoms, with the depression group exhibiting a younger mean age in this study. This finding may reflect the influence of modern social stressors, which disproportionately impact younger individuals, including academic or career pressures, financial instability, and social isolation[81]. Additionally, life stage transitions, such as entering the workforce, building relationships, or starting families, may heighten vulnerability to depression during early adulthood. Hormonal fluctuations, particularly in women, may further contribute to this risk[82]. Younger women face unique stressors, like societal expectations and role balancing. These findings accentuated the need for targeted interventions and further research into age-related social, biological, and psychological factors to guide mental health strategies.

Chronic disease was a key predictor of depressive symptoms in our model, representing a close association between physical and mental health. Chronic activation of the HPA axis commonly observed in individuals with chronic diseases, is linked to the development and progression of major depressive disorder and various physical conditions, such as cardiovascular disease[73,79]. The persistent physiological stress associated with chronic illness, including inflammation, hormonal dysregulation, and immune dysfunction, may induce or exacerbate depressive symptoms. Addressing chronic disease as part of depression management is therefore critical.

In this study, we found that women who exercise four or more times a month demonstrated less likelihood of experiencing depressive symptoms. Future research should identify the most effective strategies to break the cycle of chronic disease and depression to develop more personalized and holistic treatment approaches. Exercise stimulates dopamine and endorphin production, elevates mood, and improves depression and anxiety across diverse populations, including those with chronic diseases[83-85]. Exercise reduces inflammation, improves sleep, and enhances cognitive function, supporting better mental health[86]. These effects are particularly relevant for depression, where biological, psychological, and social factors interact to worsen symptoms. Given its multifaceted benefits, incorporating exercise into depression treatment strategies is essential. Future research should explore the optimal exercise types, intensities, and frequencies to maximize its therapeutic potential.

Nomograms are predictive tools based on statistical models that integrate multiple factors to estimate the probability of a specific clinical event[42]. The SHAP-derived predictors could enable healthcare providers to stratify populations based on individualized risk profiles, directing screening efforts toward subgroups with elevated biomarker scores or socioeconomic vulnerabilities. In this study, a nomogram was constructed to predict the probability of clinical depressive symptoms in patients. This tool not only demonstrated robust predictive performance and substantial positive net benefits but also provides a practical way for healthcare providers to prioritize screening efforts. Healthcare providers could use the nomogram to stratify patients based on risk levels, directing screening and follow-up resources toward those with elevated scores to optimize care delivery and improve patient outcomes. By incorporating these variables into nomograms, providers may efficiently identify high-risk individuals during routine visits, particularly in resource-limited settings where comprehensive mental health evaluations are challenging.

There are several limitations in this study. The sample predominantly consisted of women, which, while valuable for understanding gender-specific aspects of depression, limits generalizability. Geographic sampling biases and limited population diversity require validation across broader socioeconomic/regional contexts. The reliance on self-reported data for smoking, drinking, and anxiety symptoms may introduce bias, particularly regarding sensitive topics. More objective data collection methods, such as wearable devices, could help mitigate this limitation. This cross-sectional study lacks specific analysis of depression during pregnancy and perimenopause periods, potentially reducing model accuracy for these high-risk groups. While the LightGBM model exhibited strong predictive performance, potential overfitting concerns necessitate further validation in independent samples. The complex interactions between sleep quality, marital status, and anxiety symptoms in depression onset require further investigation through longitudinal cohort studies to understand causal relationships better. Future research should employ more detailed variable categorization and broader population sampling to enhance result generalizability.

This study successfully developed a robust predictive model for the risk of depressive symptoms in women using ML algorithms, including XGBoost, SVM and LightGBM. The model demonstrated excellent discriminative ability and calibration, displaying its potential for application in clinical practice. Insomnia, anxiety symptoms, age, chronic disease and exercise were identified as key predictors of depressive symptoms in women. These findings highlight that clinical and public health strategies must target these specific factors to enable the early detection of at-risk individuals and the delivery of personalized interventions for women’s mental health. The integration of SHAP enhanced model interpretability, and interactive digital version of nomogram provided a practical tool for clinical application. Future research will address the current limitations to further refine these predictive models.

We sincerely thank all participants and researchers of the Guangdong Provincial Sleep and Psychosomatic Health Survey for their contributions, and we would like to acknowledge everyone who helped us in this study.