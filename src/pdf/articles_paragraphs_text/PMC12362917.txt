In cancer research, different levels of high-dimensional data are often collected for the same subjects. Effective integration of these data by considering the shared and specific information from each data source can help us better understand different types of cancer.

In this study we propose a novel autoencoder (AE) structure with explicitly defined orthogonal loss between the shared and specific embeddings to integrate different data sources. We compare our model with previously proposed AE structures based on simulated data and real cancer data from The Cancer Genome Atlas. Using simulations with different proportions of differentially expressed genes, we compare the performance of AE methods for subsequent classification tasks. We also compare the model performance with a commonly used dimension reduction method, joint and individual variance explained (JIVE). In terms of reconstruction loss, our proposed AE models with orthogonal constraints have a slightly better reconstruction loss. All AE models achieve higher classification accuracy than the original features, demonstrating the usefulness of the embeddings extracted by the model.

We show that the proposed models have consistently high classification accuracy on both training and testing sets. In comparison, the recently proposed MOCSS model that imposes an orthogonality penalty in the post-processing step has lower classification accuracy that is on par with JIVE.

Data integration is the process of combining data from different modalities or sources that each provide a separate view of the common phenomenon. Compared with the analysis of the individual data modality, different data sources can provide complementary information thus promising a more comprehensive understanding of the system [11]. This is particularly important in the biomedical field, where multi-omics data including genomics, transcriptomics, proteomics, epigenomics, metabolomics, and meta-genomics are increasingly made available for different study objectives. One of the most important examples is The Cancer Genome Atlas (TCGA), a project that has collected different types of omics data for over 20,000 tumor and normal samples, which collectively cover 33 different types of cancers with clinical information for a subset of subjects [12]. In addition to an effective integration strategy to account for the distributional difference of the data source, each modality of data is often also high dimensional. For example, gene expression profiles for each subject in the TCGA database consist of 60,483 measurements (FPKM: fragments per kilobase of transcript per million mapped reads) corresponding to the expression measurement for each gene at the transcript level. DNA methylation profile for each subject was determined by Infinium HumanMethylation 450 BeadChip Arrays containing 485,578 probes that measure the methylation level for a small part of the human genome [16]. The high dimensionality can pose significant analysis challenges for classic statistical methods and machine-learning techniques since the number of features in each modality is larger than the number of observations (). Consequently, dimension reduction such as principal component analysis (PCA) or customized filtering (e.g., only considering differentially expressed genes or methylation probes in the analysis) are often needed either before or during the data integration process.

The majority of the data integration methods can be grouped into four different categories: dimension reduction, kernel, network, and deep learning methods [1]. Among these, joint dimension reduction methods are probably the most heavily explored, with integrative extensions of commonly used dimension reduction methods like principal component analysis [3,4], canonical correlation analysis [14], and non-negative matrix factorization [13,15].

However, there has recently been more interest in deep learning methods for data integration. Deep learning methods are more flexible, and they can identify non-linear patterns much more easily than most dimension reduction methods, due to the non-linear activation functions of most deep learning methods. Deep learning models can learn the hierarchical representation of data automatically without the constraints of linear relationships, making them particularly suitable and flexible for the integration of multi-omics data in biomedical applications [11].

An autoencoder (AE) is a deep-learning approach to find the latent representation of the input with a lower dimension that contains the necessary information to reconstruct the original input. An AE consists of an encoder functionparameterized byand a decoder functionparameterized bysuch that for a single input,and theis the embedding of the original input andis the reconstructed input. The goal of the AE is to capture the essential information in the data by minimizing the reconstruction error. Different metrics can be used to measure the error such as mean squared error:, whereis the entire dataset andis the corresponding reconstruction of the entire dataset after applying the AE model [10]. Ifandare linear functions thenwill lie in the principle component subspace making the AE similar to PCA. Ifandare nonlinear, the input will then be mapped onto a lower dimensional manifold that is more informative than principle components when there are non-linear interactions in the data [11]. In practice, AEs are neural network architecture that often consists of fully connected layers of encoder and decoder. The encoder takes the input data and compresses them into lower dimensional representations and the decoder takes this lower dimensional representation and reconstructs the input at the final output layer.

A number of different AE architectures with variational components were previously proposed for multi-omics data integration by [10]. The most straightforward method is the Autoencoder with Concatenated Inputs (CNC_AE), which simply concatenates the scaled data sources as the input for AE. For this AE architecture, the encoder consists of a number of fully connected layers including the embedding layer to transform the concatenated input of different data sources (i.e., concatenation of X1 and X2) from a higher dimension to a lower dimensional representation. The corresponding decoder then takes this lower dimensional representation as the input aiming to reconstruct the original input for different data sources separately [10]. The X-shaped Autoencoder (X_AE) is a slightly modified version of the CNC_AE, in which the individual data sources are first preprocessed separately before being joined together to be further processed by the model [10]. Finally, instead of directly concatenating the preprocessed data source, a pair-wise mutual concatenation of the input is used in the Mixed-Modal Autoencoder (MM_AE) model to take advantage of the potentially shared information among the different data sources [10]. Despite these different architectures, all these AE structures fail to take advantage of the potentially joint and individual components/information across the data source in their architectural design.

A recent study implemented the notion of joint and individual components in AE for multi-omics data integration with the multi-omics data clustering and cancer subtyping via shared and specific representation learning (MOCSS) model [2]. Specifically, two separate AEs were created for each data source and designated as the shared and specific respectively. With 2 different data sources, all 4 AE have the same architecture. However, instead of using the concatenated input from different data sources to extract the potentially shared information, 2 separate shared embedding vectors were extracted from each data source based on the aforementioned shared AEs. To encourage these embeddings to be shared from data source to data source, these vectors were first transformed through another fully connected neural network (MLP: multiple linear perceptrons) before a contrastive learning process was used to align them by reducing the pair-wise contrastive loss between these vectors. The orthogonal penalties were applied between the shared embedding vector and the specific embedding vector for each data source individually and then added together as the final orthogonal loss. Despite taking advantage of the potentially shared and specific information from the data source, the fact that MOCSS model aligns the shared information and imposed the orthogonal penalty during the post-processing process may loss some important information.

In this study, we propose a novel architecture of AE model for multi-omics data integration, where the joint component is derived from the concatenated data sources and the individual component come from the corresponding individual data source. To encourage the model to separate and extract the joint/shared information contained between different omic data and the specific information contained in each data source, an additional orthogonal penalty is applied between the joint and the individual embedding layers. For comprehensiveness, 3 different version of orthogonal penalty was defined and compared, and the model without orthogonal penalty was also compared. Unlike MOCSS model, the joint component and orthogonal penalty are natural components of our model structure. For comparison, we implemented aforementioned AE structures and compared them on a comprehensive simulation setting in terms of input reconstruction and classification accuracy. For further validation, we also applied them on a real-world cancer dataset from TCGA and compared their performance with a dimension reduction model: Joint and Individual Variation Explained (JIVE), which is based on PCA [4]. We chose to compare with JIVE because it similarly imposes an orthogonality constraint between the joint and individual components. It should be noted that, for a fair comparison, all AE models implemented in this study do not contain variational components.

To compare the effectiveness of multi-omics data integration methods, different AE models were first applied to the simulated data and further validated on the real-world cancer data. We implemented the previously proposed AE models for omics data integration, CNC_AE, X_AE, MM_AE and MOCSS, as shown in Fig.1a–d. The proposed novel AE structure, Joint and Individual Simultaneous Autoencoder (JISAE) is shown in Fig.1e, where the encoder starts with the individual omics data as well as their concatenation. These three inputs were separately processed by 4 fully connected layers including the separate final embedding layers for three inputs. To encourage the model to separate and extract the joint (shared) information between the 2 different omics data sources and the individual (specific) information unique to each data source, in addition to the reconstruction loss, an additional orthogonal penalty is applied between the embedding layer of joint and the individual inputs as shown in the orthogonal loss diagram in Fig.1e. We refer to the models with an orthogonality penalty as JISAE-O. To evaluate the importance of the orthogonality penalties for the model performance, the JISAE model without orthogonal constraints is implemented for comparison, as shown in Fig.1f.

In the MOCSS model, the following penalty was imposed between the shared and specific embeddings for each data source. Let, andto represent the shared and specific embeddings for, andto represent the shared and specific embeddings foras shown in Fig.1d (All of these matrices are normalized). Then the orthogonal loss is calculated as shown in (3), where therepresents the element-wise product and avg represents the average.

To compare the effectiveness of different orthogonal loss penalties in our proposed AE structure, 3 different orthogonal loss functions were implemented resulting in three different JISAE-O models (distinguished as JISAE-O1, JISAE-O2, and JISAE-O3). Specifically, letto represent the embedding from joint inputand,andto represent the embedding from individual inputandrespectively (All of these matrices are normalized). JISAE-O1 is similar to the implementation of the MOCSS method, applying an element-wise penalty then taking the overall average. This penalty was chosen for consistency with comparing to the MOCSS method. The orthogonal loss for JISAE-O1 is defined in (4).

The rows in,, andrepresent individual subjects in a given batch of training examples. The columns from each matrix represent elements of features from the low dimensional representation for the shared and individual input respectively. Therefore the orthogonal loss defined for JISAE-O2 (see equation 5) imposes the orthogonality constraints among shared and specific embedding for each subject (taking the squared sum of the diagonal from theandrespectively or squared dot product between shared and specific embeddings for each subject) whereas the orthogonal loss in JISAE-O3 (see equation 6) imposes the orthogonality constraints among shared and specific embedding across all subjects in a given batch of training examples for every feature in the low dimensional representation. Equations (4) and (5) both calculate the dot product between shared and specific embeddings for a given subject. When taking the average, equation (4) also normalize the dimension of embeddings, while equation (5) will scale with the dimension of embeddings. Therefore, the total loss for the MOCSS and JISAE-O models is defined as. For a fair comparison, we also kept the original contrastive loss for MOCSS model.is a hyperparameter that controls the relative contribution of the orthogonal loss to the total loss in these models. The total loss for other AE models is simply. With the total loss function defined above, the gradient-based optimization algorithm was used to reduce the loss by iteratively optimizing the model weights.

The R package r.JIVE [6] was used to compare the AE methods with JIVE. All AE models in this study were created using the Pytorch framework 1.13.1 [7] with CUDA Toolkit 11.4. The KFold from sklearn 1.0.2 [8] was used to split the dataset randomly for cross-validation (CV). The Optuna package (https://github.com/optuna/optuna) was used for hyperparameter searching based on the average reconstruction loss on the validation set during the 5-fold cross-validation.

For the cancer dataset, multi-omics TCGA preprocessed datasets were downloaded fromhttps://acgt.cs.tau.ac.il/multi_omic_benchmark/download.html[9], which includes gene expression, miRNA expression, and patient clinical data from 10 different cancers (AML: Acute Myeloid Leukemia, BIC: Breast cancer, COAD: Colon adenocarcinoma, GBM: Glioblastoma multiforme, KIRC: Kidney renal clear cell carcinoma, LIHC: Liver hepatocellular carcinoma, LUSC: Lung squamous cell carcinoma, SKCM: Skin Cutaneous Melanoma, OV: Ovarian serous cystadenocarcinoma and SARC: Sarcoma). Subjects were limited to only those with both gene expression and miRNA expression data. Since the number of measured features can be different across the cancer types, we selected the cancer types that have the most consistent feature dimensions across both omics sources. We also restricted our analysis to primary tumors only. After this filtering, 6 cancer types: BIC, LUSC, SKCM, LIHC, SARC, and KIRC were left for subsequent analysis. The resulting data set contained 1866 subjects with 20,531 genes and 1046 miRNAs. The resulting sample sizes for each cancer type were 621 BIC, 341 LUSC, 97 SKCM, 367 LIHC, 257 SARC, and 183 KIRC, as shown in Fig.2a.

To compare the effectiveness of multi-omics data integration with different models, we benchmarked different AE performance first on a simulation dataset generated by using R package MOSim [5] and further validated on a dataset based on TCGA. For the simulation, RNA-seq (with a total 20,531 features) and miRNA-seq (with a total 1,046 features) were jointly generated from a series of settings. Specifically, data of 2, 3, 4, and 5 groups were generated separately and for each group setting, 20%, 40%, 60%, 80% and 100% of the genes were set to be differentially expressed. For the miRNA data, 50% of the miRNAs were set to be activators and 50% were set to be repressors. Sequencing depth was set to 10 for all features, and the replicate variability (replicateParams) was set toand. All other parameters used the defaults from the MOSim package. Thus a total of 20 different dataset were simulated representing different level of difficulty to test the model and the total number of samples were kept to 600 in each dataset as shown in Table1.

To choose the optimal hyperparameters for different AE models,of the data was allocated for training while the remainingwas used for final model evaluation as shown in Fig.3a. 5-fold cross-validation (CV) was used on the training set to choose the optimal hyperparameters for each AE model based on the lowest average reconstruction loss on the validation set. It is computationally infeasible to cover the entire hyperparameter space, so Optuna (https://github.com/optuna/optuna), a hyperparameter optimization framework for hyperparameter searching, was used by conducting 50 random trails for each model in the simulation setting and 100 random trials for cancer dataset, where each trial corresponds to a different combination of hyperparameters. The selected optimal hyperparameters were then used to retrain the model on the entire training set in order to extract the embeddings for subsequent analysis. For the cancer data, after excluding solid tissue normal and metastatic samples, as shown in Fig.2a, the primary tumor samples were divided into non-overlapping training and testing sets following the same partition rule as shown in Fig.2b with the same training strategy shown in Fig.3a.

The number of nodes for all hidden layers across for AE model was chosen from. Except for the MOCSS model, batch normalization followed by dropout layers was included in all other AE models, and the dropout rate was selected from. The MOCSS model is symmetric where the number of nodes across the encoder mirrors the number of the nodes across the decoder. In terms of nonlinear activation functions, instead of ReLu, hyperbolic tangent activation was used in the MOCSS model. The learning rate was chosen fromtoon a logarithmic scale; l2 penalty for model weights is chosen fromtoon a logarithmic scale; number of training epoch is chosen from; batch size was chosen fromand the Adam optimizer was used for model weights optimization. Finally, for the three AE models with orthogonal loss constraints, as mentioned above,was chosen fromto control the relative contribution of the orthogonal loss to the total loss.

To assess whether the AE models can compress the data and retain the essential information for the data, reconstruction loss was calculated for the training and testing sets for each AE model except MOCSS. MOCSS has separate reconstruction loss for joint and individual embeddings, which is difficult to compare with other AE models. The reconstruction loss was based on the individual subject (contains two data modalities) in order to assess the uncertainty associated with average reconstruction loss. To investigate whether the omics data integration via AE can be useful for subsequent analysis, three classifiers were tested using the embeddings from the models. The first was a Gaussian naive Bayes classifier that did not require any additional hyperparameters. The other two classifiers were a random forest (RF) model and a support vector machine (SVM). All three classifiers were implemented using the sklearn package in Python with default parameters [8].

The classification was conducted with 5-fold CV in order to assess the classification uncertainty. This comparison was conducted for both simulation and cancer datasets. For the cancer dataset, we additionally compared the AE model with JIVE. The concatenated joint and individual components extracted from the JIVE model were also used for classification. To assess the effectiveness of AE and JIVE models, the original given features as well as their concatenation were also used as the baseline. The gene expression and miRNA expression data in the training and testing set were normalized by using the MinMaxScaler from the sklearn package [8] separately. MinMaxScaler scaled the data set such that all feature values are in the range of [0, 1]. For a fair comparison, the input for all AE and JIVE models was also scaled by MinMaxScaler for training and testing sets separately.

Data sets with different levels of difficulty were simulated from 20 settings. 600 samples were simulated from each setting, with 2, 3, 4, or 5 groups containing 300, 200, 150 or 120 samples respectively, as shown in Table1. The total number of features for gene and miRNA expression in the simulation data was chosen to match the total number of features in original TCGA dataset (20,531 genes and 1046 miRNAs).

The TCGA dataset contained 685 BIC, 221 COAD, 344 LUSC, 291 OV, 450 SKCM, 410 LIHC, 261 SARC, 170 AML, 274 GBM, and 208 KIRC subjects that had both omics measurements. In the gene expression data across these 10 cancer types, GBM contained expression measurements for a total of 12,042 genes while the rest of the cancer types all contained the expression level measurements of 20,531 genes. In the miRNA expression data, six cancer types, BIC, LUSC, SKCM, LIHC, SARC, and KIRC, had expression measurements of 1,046 miRNAs. Three cancer types, COAD, OV, and AML, had the expression measurements of 705 miRNAs, and GBM contained expression measurements of 534 miRNAs. To make sure each omics data had the same number of features across the selected cancer types, 6 cancer types including BIC, LUSC, SKCM, LIHC, SARC, and KIRC were selected for subsequent analysis. Some of these cancer types include multiple sample types (e.g., primary tumor, solid tissue normal, and metastatic samples), but we restricted our analysis to primary tumors only (i.e., a total of 1,866 subjects) with 621, 341, 97, 367, 257, and 183 for breast, lung, melanoma, liver, sarcoma and kidney tumor samples respectively, as shown in Fig.2a.

The optimized hyperparameters based on the lowest average reconstruction loss were used to retrain all the AE models on the entire training set separately (see Figs.S1andS2in supplementary material). To evaluate the generalization of AE models for data reconstruction, the reconstruction loss on the training and testing set was calculated for all AE models other than MOCSS based on the retrained model weights. This calculation is based on the reconstruction error for each individual subject including both data modalities (i.e., miRNA and gene expression). The average reconstruction loss and the corresponding standard deviation on the simulation training dataset is shown in Fig.S3a while the reconstruction results on the testing dataset is shown in Fig.4. As expected, as the difficulty of the simulation settings increased, the averaged reconstruction loss also increased. The extracted embeddings were used as the new features in a Gaussian naive Bayes classifier for classification task with 5-fold CV. As shown in Figs.S3b andS4for the simulation training and testing dataset respectively, almost all the data points in both datasets were classified with accuracy of 100% (average accuracy with corresponding standard deviation shown in parentheses).

For the TCGA cancer dataset, the optimal dimensions of embeddings and corresponding reconstruction loss for each AE model except MOCSS are shown in Table2. After hyperparameter selection, CNC_AE reduced the dimension the most from 21,577 (including 2,0531 genes and 1,046 miRNA sequences) to 32 embeddings, followed by JISAE-O3 with 192 embeddings. JISAE and JISAE-O1 achieved relatively lower training reconstruction loss on average compared to the other AE models. JISAE had the lowest testing reconstruction error, with X_AE close behind.

For classification, the model performance was also compared with JIVE. Using the permutation test in the r.jive package [6], we selected a rank of 6 for the joint matrix and ranks of 160 and 100 for individual matrices. The score matrices for joint and individual components were then extracted with PCA as mentioned earlier. On the testing set, by directly applying the loadings from the training set to the testing set, we obtained the corresponding joint and individual score matrices with the same ranks as the training set.

To evaluate the effectiveness of the embeddings from the AE models and the joint and individual components from JIVE model on the TCGA cancer dataset in the subsequent analysis, three different classifiers were used for cancer type classification: a Gaussian naive Bayes classifier, an RF classifier, and an SVM. We performed a 5-fold CV and reported the average classification accuracy and also showed the corresponding standard deviations on both the training and testing sets. The inputs for the classifiers were either the original data (gene expression, methylation, or both), the embeddings from each of the AE models, or the JIVE components, as illustrated in Fig.3a. As shown in Fig.3b, the joint and individual components from the JIVE model were concatenated together for classification.

Training classification results are shown in supplementary figures (Figs.S5,S6,S7). For the naive Bayes classifier, the MM_AE model resulted in the highest average training classification accuracy (95.18%), followed by JISAE (93.51%), JISAE-O1 (92.77%) and JISAE-O2 (91.68%). The JIVE model produced the lowest average training classification accuracy (67.2%), which was even lower than the original features. MM_AE also produced the highest average training classification accuracy for the RF and SVM classifiers among the dimension reduction methods, but these were lower than the original feature sets.

For each AE model, by applying the trained model weights from the entire training set to the testing set, we obtained the embeddings for the testing set. The classification accuracy using the naive Bayes classifier on the testing set is shown in Fig.5. The original features generated much lower classification accuracy (all around 58%) compared with AE and JIVE models. While the CNC_AE model achieved 87% average classification accuracy on the training set, it achieved the highest average accuracy of 95.7% on the testing set. In comparison, JISAE-O1 and JISAE-O3 achieved similar performance (around 91%) on the testing sets. The joint and individual components from the JIVE model produced a classification accuracy of 85.74%, despite having considerably lower training accuracy.

For the RF and SVM models, the original features produced higher classification accuracies than the dimension reduction methods. This is not entirely surprising, since RF and SVM are supervised methods, while the AE models and JIVE are all unsupervised dimension reduction methods. The performance of all of the dimension reduction methods was close to the original features for the RF classifier (Fig.6). JISAE-O1 had the highest performance among the AE models (97.0%), which was still lower than the testing classification accuracy for full feature set (98.9%). For the SVM classifier, JISAE with no orthogonality penalty performed as well as the full feature set (97.3%). However, for the SVM classifier, JISAE-O2 (72.9%) and JISAE-O3 (33.6%) performed very poorly (Fig.7).

In this study, we proposed a flexible new AE model structure for data integration in multi-omics data. Compared to previous integrative AE models, the proposed models use both shared and specific inputs and allow flexibility to incorporate orthogonality in the loss function. We compared the reconstruction loss and classification accuracy of the novel model structures to four existing integrative autoencoders: CNC_AE, X_AE, MM_AE, and MOCSS [2,10]. Although we focused on the setting with two data sources in this manuscript, the proposed methods should easily extend to three or more data sources by simply adding additional inputs into the AE.

Among the simulated datasets, the most difficult setting for the models contained 5 groups with 20% differentially expressed genes (DEGs). In this setting, X_AE and MM_AE showed slightly higher reconstruction loss on both training and testing set compared with other AE models. Despite the slightly difference in reconstruction loss among different AE models, the embeddings extracted from AE models are all informative in terms of classification of different groups (see Fig.S4).

On the TCGA application, JISAE-O1 achieved the lowest training reconstruction loss among all AE models, indicating the effectiveness of this AE model in reconstructing the model input with the learned embeddings. More importantly, the JISAE model achieved the lowest reconstruction loss on the testing set. The models with an orthogonality penalty performed worse on the testing set, indicating possible overfitting. Table2shows that the test reconstruction loss is considerably higher than the training reconstruction loss for JISAE-O1. This suggests possible overfitting that is not as severe without an orthogonality penalty. The orthogonality penalty increases the model complexity, as it prevents information from overlapping between the joint and individual components. So it is more likely to overfit the data using the orthogonality penalties. Based on the optimal embedding dimensions, CNC_AE reduced the dimension of the input the most. Overall, the different AE architectures constructed in this study showed slightly different capacities to capture the essential input information and generalize that to the new data.

Comparing the model performance in terms of reconstruction loss derived from the embeddings and subsequent classification tasks with these embeddings, it is clear that the low reconstruction loss is not directly translatable to subsequent higher model performance. For instance, CNC_AE is a simple structure, that has the lowest dimension of embeddings and is performing the best for testing set classification, nevertheless, the reconstruction loss for CNC_AE is not the lowest one among all AE models constructed in this study. Although the highest average accuracy of 95.7% on the testing set was achieved by the CNC_AE model, JISAE-O1 achieved consistently high model performance on both training (92.77%) and testing set (91.94%) indicating the greater robustness of this model. Although not as highly accurate as the JISAE-O1, the MOCSS model also demonstrated consistently higher model performance between the training and testing set (86.21% on the training set and 85.73% on the testing set). The fact that the same orthogonal loss function was used in JISAE-O1 and MOCSS models (see equations (3) and (4) respectively) indicates that this orthogonal loss might be the optimal one compared to the other 2 orthogonal loss (see equations (5) and (6)) defined in this study for the classification tasks.

In theory, deep-learning-based multi-omics data integration methods such as AEs can have more flexible architectures with the possibility of designing specific structures for processing each data source. This flexibility and relatively higher model performance in this study demonstrated its potential superiority over the JIVE model for data integration. Although all AE models considered in this study achieved similarly high model performance on the simulation dataset in terms of reconstruction loss and classification accuracy, with the real-world TCGA dataset, the novel AE model structure coupled with orthogonal penalties proposed in this study showed relatively higher model performance compared with previous MOCSS model (see Fig.5).

There are some limitations to the current study. Although 5-fold CV was used to select the optimal hyperparameters for each AE model, L2 loss and dropout layers were also implemented in the models, a relatively large discrepancy still appears between the classification accuracy on the training and testing set for some of the AE models with the TCGA data. To mitigate this, more data could be collected for training and testing in the future, and variational AE models that contain intrinsic model regularization can be used to replace the static AE structures in this study. The AE methods introduced in this analysis are unsupervised. This was clearest for the RF and SVM classifiers, where supervised classifiers for the full feature set outperformed the AE models. For classification, supervised neural networks that use the classes directly as outputs could perform even better. Alternatively, in addition to the reconstruction loss, a loss function that is directly related to the subsequent analysis can be added to the model training process (e.g., classification error), so the extracted embeddings are also directed towards better performance in that context. Finally, although the feature dimension of the simulated data was the same as the original TCGA data, the sample size for the simulation study was smaller for easier computation. On a larger sample, it is possible that the performances of each of the methods would be different.