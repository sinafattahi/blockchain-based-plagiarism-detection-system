Grid cells, with hexagonal spatial firing patterns, are thought critical to the brain’s spatial representation. High-speed movement challenges accurate localization as self-location constantly changes. Previous studies of speed modulation focus on individual grid cells, yet population-level noise covariance can significantly impact information coding. Here, we introduce a Gaussian Process with Kernel Regression (GKR) method to study neural population representation geometry. We show that increased running speed dilates the grid cell toroidal-like representational manifold and elevates noise strength, and together they yield higher Fisher information at faster speeds, suggesting improved spatial decoding accuracy. Moreover, we show that noise correlations impair information encoding by projecting excess noise onto the manifold. Overall, our results demonstrate that grid cell spatial coding improves with speed, and GKR provides an intuitive tool for characterizing neural population codes.

The influence of an animal’s running speed on grid cell spatial representation is unclear. Here, the authors present a Gaussian Process with Kernel Regression method to infer neural representational geometry from data and show that faster movement dilates grid cell representational manifolds, enhancing spatial representation.

In navigation, it is crucial that the brain forms a certain internal representation of the external space1. Grid cells are widely regarded as an essential component of internal spatial representation2,3. Their hexagonal spatial firing patterns are thought to form a coordinate system of external space4and support the downstream hippocampal spatial representations (e.g., place cells)5–9. Yet maintaining precise spatial coding is particularly challenging at high running speeds, when self-location changes rapidly10. The effect of running speed modulation on grid cell population coding remains unclear.

Previous literature offers dual possible predictions about running speed modulation on grid cell codes. On the one hand, speed may support grid cell spatial coding. Running speed is known to mostly increase grid cell firing rates11–13. Rats running at high speeds (10 cm/s to 50 cm/s) are also known to have more medial entorhinal cortex (MEC) cells coding spatial information than when running at a low speeds (2 cm/s to 10 cm/s)10. On the other hand, speed signals disrupt the phase differences between pairs of grid cells14. Increasing speed may also lead to larger input noise (possibly from the medial septum11,15or speed cells16,17), causing larger noise error to accumulate over time, thus degrading spatial coding fidelity18–21.

While these previous studies provide insights into speed modulations of grid cells, their analyses were limited to individual or pairs of grid cells11–14(although decoding analysis has been performed on MEC cell population10). Neurons in the brain represent information through their collective population activity. Population noise covariance can significantly impact information coding22–28. Grid cells’ activities are especially known to be tightly coupled and change coherently14,29. To study the speed modulation of grid cell code, it is important to analyze simultaneously recorded grid cell population activities, including the effect of noise covariance. Yet such a study is still lacking.

Because neural data is intrinsically high-dimensional, inferring the noise-covariance matrix can be difficult. A standard approach for discretely valued information is to compute the sample covariance across trial responses24. To handle continuously varying stimuli (e.g., orientations of static grating stimuli), one typically first bins continuous parameters, then collected repeated trials at each bin30–33. From these trial-based data, the noise covariance can be estimated using the sample-covariance estimator or, more recently, via a Wishart-process model34.

However, discretizing continuous stimuli and collecting trial-based data can be impractical for two main reasons. First, high-dimensional inputs—such as natural images—require an exponentially large set of discretized values27. Second, many naturalistic experimental paradigms (e.g., navigation tasks) lack repeated trials31,33,35. A study on retinal representations of natural images addressed these challenges by substituting retinal data with convolutional neural network (CNN) units, explicitly formulating the noise covariance27. However, this approach relies on the observed similarities between retinal neurons and CNN units36. There’s a trend in neuroscience to move beyond trial-based experiments, towards trial-free naturalistic experiments31,33. Yet, to our knowledge, it remains challenging to reliably estimate noise covariance from high-dimensional neural data in naturalistic tasks without repeated trials.

In this paper, we introduce Gaussian Process with Kernel Regression (GKR), a method for inferring both the smooth mean (manifold) and noise covariance from high-dimensional neural data, including recordings from naturalistic tasks. The study of manifolds and noise covariance falls within the framework of information geometry27,37. We applied GKR to simultaneously recorded grid cell activities35. We found that: (1) Running speed both dilates the grid cells’ toroidal-like manifold and increases noise; (2) Nevertheless, the effect of manifold dilation outpaces the effect of noise increase, as indicated by the overall higher Fisher information at increasing speeds, and further supported by improved spatial coding accuracy at higher speeds; (3) Furthermore, compared to hypothetical independently firing grid cells, we found that noise correlations in real grid cells “shape “ the noise structure such that more noise is projected onto the manifold surface, indicating that noise correlation in grid cells is information-detrimental. Overall, our results indicate that running speed enhances grid cell spatial coding through geometric modulations. GKR provides a useful tool to interpret noisy neural data from an intuitive information geometry perspective.

We analyzed grid cell recordings from Gardner et al.35, obtained as rats foraged in an open-field (OF). The dataset included approximately 60–200 simultaneously recorded grid cells per module, with the exact number varying by rat, recording day, and module (see Methods). The experiment comprised nine configurations, denoted using a notation system, for example, “R1M2” refers to rat R on day 1, specifically from grid cell module 2. Grid cells within the same module shared a similar spatial period but differed in phase. Raw spiking data were converted into firing rates using a kernel averaging (see Methods), with example rate maps shown in Fig.1Aand Supplementary Fig.1.

ATop: Rats were performing open-field foraging (OF) tasks with grid cell activity collected (data from Gardner et al.35). Nine experimental configurations were analyzed, varying across rats, recording days, and grid cell modules. For example, the configuration “R1M2” represents rat “R”, day 1, and grid cell module 2. Each experimental configuration was subsampled, generating 50 sampled datasetsDs, ensuring a similar number of data points across different speed values (see text and Methods). Bottom: Example rate maps of grid cells in R1M2; with additional examples provided in Supplementary Fig.1.BSCA quantifies the fidelity of grid cell spatial coding. Random locations were sampled within the open field, and for each location (star), two conjugate boxes were defined, separated by a fixed distance of 10 cm. Neural activity within these boxes was collected and classified using logistic regression (circles and crosses). The SCA is defined as the average classification accuracy across all sampled locations (see Methods).CSCA as a function of the rat’s speed. Each dot represents the SCA computed from a sampled datasetDsat one speed bin (see Methods, fifty sampledDswith eight speed bins = four hundred data points). The solid line and error band show the best-fitting line and 95% confidence interval (CI), estimated using Bayesian linear ensemble averaging (BLEA, see text and Methods).DSCA-speed slopes of different experimental configurations. Dots and error bars represent mean and 95% CI of the slopes estimated using BLEA (panel C shows the case of R1M2). Numbers above error bars are linear models’ r-squared values. Asterisks below the x-axis denote the statistical significance of the slope computed from the original datasetsDscompared to label-shuffled data (two-sided, Bayesian method, see Methods). ***p< 0.001; **p< 0.01; *p< 0.05; NS no significance. Source data are provided as a Source Data file.

We analyzed the rats’ behavioral data and found a predominance of low-speed movement, resulting in a concentration of observations in the slow-speed range (Supplementary Fig.2). Since the primary goal of this study is to compare grid cell representation properties across different speeds, it is essential to control nuisance factors—specifically, ensuring an equal number of data points across speed conditions. To achieve this, we randomly sampled data within discrete speed bins (bin width = 5 cm/s, see Methods) to create a balanced dataset,Ds, with an equal number of data points per bin. We generated fifty suchDsdatasets per experimental configuration to estimate uncertainty.

One straightforward approach to evaluate the quality of neural spatial coding is by decoding location information from neural states. Good decoding performance indicates good neural spatial coding. We designed a locally linear classification accuracy to evaluate the quality of spatial coding, formally referred to as spatial coding accuracy (SCA) (Fig.1B, see Methods). Specifically, at each speed bin value, several locations were randomly sampled. For each sampled location, we created two conjugate boxes centered near the sampled location but positioned in opposite directions, separated by a fixed distance of 10 cm. Data from these two boxes were collected, relabeled as class 1 and class 2, and then split into training and test sets. A logistic regression model was then trained to classify the data and evaluated on the test set. The classification accuracy, averaged over all randomly sampled spatial locations, is referred to as the SCA. SCA quantifies how well neural states corresponding to two nearby spatial locations are discriminable.

For each sampled datasetDs, we computed SCA across eight speed bins as described above. FiftyDs—each yielding eight metric-speed pairs (where metric is SCA in this case)—produced a metric-speed dataset of 400 points (dots in Fig.1C). A natural idea is to fit a simple linear regression (e.g., least-squares regression) to these 400 points and use the slope to quantify how metric varies with speed. However, this approach is problematic as it assumes all observations are independent, which here is violated: our 400 points come from fiftyDsdrawn from the same original datasetD, so they are statistically related. If one were to increase the number of sampled datasetsDs, a naïve linear regression would misleadingly drive the slope’s estimation uncertainty toward zero.

To address this, we introduced Bayesian Linear Ensemble Averaging (BLEA, see Methods). BLEA proceeds in two stages: first, it fits a Bayesian linear regression separately to the metric-speed data from eachDs, yielding fifty posterior distributions over the regression weights; then it combines these posteriors via Bayesian model averaging38,39. The result is a Gaussian-approximated posterior over the slope (and intercept) together with predictive distributions, from which confidence intervals (CIs),p-values, and other statistical measures can be estimated using a Bayesian framework (see Methods)40.

Applying BLEA to SCA (Fig.1C, D), we found that SCA increases with speed, with a slope significantly larger than that of the label-shuffled dataset. This result holds across different classifiers used for computing the SCA (support vector machine and perceptron, see Supplementary Fig.3), indicating better self-location representations in grid cell population at higher speeds.

What are the underlying neural mechanisms contributing to the improved spatial coding in grid cells? To explore this question, we need a tool to analyze the high-dimensional noisy neural states. A recent popular neural population geometry framework suggests that, instead of analyzing noisy high-dimensional data, it will be more intuitive to use certain methods to extract the data’s underlying smooth manifold along with the noise covariance34,41,42. Wishart process is such a method that can infer the smooth manifold and covariance matrix34. However, the recent implementation of Wishart process requires trial-based experimental paradigm, which forbids this method to be used in broader and complex natural behaving experiments34. The OF task (Fig.1A) is one of such natural behaving experiments without strict repeated trials.

Therefore, we developed Gaussian Process with Kernel Regression (GKR) method. The main purpose of GKR is to estimate the representation manifold relevant to the (known) labels of interest42. Response fluctuations arising from nuisance latent variables (e.g., emotional states) are captured as a noise covariance term. For example, in this paper, locations and speeds are the labels of interest, while neural response fluctuations due to other factors are treated as “noise,” summarized in the noise covariance matrix.

whereμ(x) is assumed to be a smooth-varying mean, also called a manifold in this paper; Σ(x) is a smooth-varying covariance. The combination of manifold and noise covariance is referred to as a statistical manifold of neural response. The goal of GKR is to infer the manifold and covariance from a dataset{r,x}.

GKR solves this inference problem by two steps (Fig.2A, see Methods): (1) inferring smooth manifoldμxvia Gaussian process regression38; (2) inferring the covariance matrix by applying a kernel averaging to residuesrxi−μxi(indexirepresents thei-th data point). Kernel parameters are optimized to maximize data log-likelihood.

AGiven neural statesrand labelsx, the goal of GKR is to infer the conditional distributionpr∣xas a smooth function ofx. GKR approximatespr∣xas a Gaussian distribution and separates the inference problem into two steps: inferring meanμx, and inferring noise covarianceΣx.BApplication of GKR to a synthetic dataset. The synthetic dataset consists ofNsynthetic neurons with heterogeneous tuning curves to a circular labelθranging from 0 to2π. The ground truthμθandΣθare visualized in the first two principal components plane (via PCA, left panel). Ellipses indicate the principal axes of covariance, with lengths proportional to the eigenvalues. In this example, the dataset consists of 10 neurons and 100 data points, which were used to fit manifolds via GKR, bin averaging, and Ledoit-Wolf (LW) methods, respectively (right panel; see Methods). The default dataset consists of 300 data points and 10 neurons, with the number of data points (C) or neurons varying (D) as indicated by x axes. Dataset was used for estimating geometric metrics. The estimated geometric metrics were compared against ground truth values and evaluated using relative estimation error, defined as the normalized difference between the estimated and true values. Dots and error bars represent the median, first, and third quartiles of relative estimation error across 10 randomly generated synthetic datasets (see Methods). A similar analysis for a 2D synthetic manifold is presented in Supplementary Fig.4. Source data are provided as a Source Data file.

We evaluated GKR on both a one-dimensional synthetic model (Fig.2B–D) and a two-dimensional synthetic model (Supplementary Fig.4). Each model consisted of a ground truth manifold,μx, where each component represented a synthetic neural tuning curve, and a covariance matrix,Σx. We generated data from these models using a Gaussian distribution (Eq.1) and applied GKR to infer the manifold and covariance matrix.

For comparison, we also applied bin averaging and the Ledoit-Wolf (LW) method. In bin averaging, we discretized the label spacexinto small bins, treating all data within a bin as having the same label. The sample mean and covariance within each bin served as estimates of the inferred manifold and covariance matrix. The LW method builds on bin averaging by incorporating shrinkage regularization to improve covariance estimation (see Methods)43.

Using the inferred manifold and covariance matrix, we computed other geometric quantities, including the Riemannian metric, precision matrix, and Fisher information (see Methods). To assess the inference performance, we compared these inferred quantities to their ground truth values by computing the relative estimation error, defined as the difference between the estimate and the ground truth, normalized by the ground truth. Across various experimental conditions and in both one-dimensional and two-dimensional synthetic datasets, GKR consistently outperformed bin averaging and LW methods (Fig.2B–D, Supplementary Fig.4).

We then applied GKR to the grid cell sampled datasetDs. The inferred manifold is intrinsically three-dimensional, as it is parameterized by three label variables of interest: two spatial locations and one speed (notably, these three labels are largely uncorrelated, see Supplementary Fig.5). To characterize the fitted manifold’s topology, we conducted a persistent homology analysis (see Methods) and found that the manifold exhibits possibly toroidal topology (Supplementary Figs.6A, B), consistent with previous findings35.

Next, we examined how spatial locations were represented by grid cells. For each speed value, we defined a speed-slice manifold (SSM) as a cross-section of the full manifold, obtained by fixing speed while varying location (Fig.3A). To visualize the SSM, we randomly sampled points from the manifold at a fixed speed of 20 cm/s, projected them onto the first six principal components (PCs) using principal component analysis (PCA), and further reduced the dimensionality to three using Uniform Manifold Approximation and Projection (UMAP)44. The resulting visualization (Fig.3B), along with persistent homology analysis (Supplementary Fig.6C), suggests that the SSM exhibits possibly toroidal topology.

AA speed-slice manifold is a cross-section of the full manifold, obtained by holding speed constant while varying location.BWe visualized an example SSM (speed = 20 cm/s) by first projecting it onto the first six PCs, then further reducing it to three latent dimensions using UMAP44. Color represents the third UMAP component value only for better visualization. The upper and bottom panels show two views of the same SSM.CWe also visualized the representation of four fixed spatial positions (i.e., lattice) but varying speed values.DFor visualization, the lattice manifold was projected into the first three PCs (upper) and two PCs (bottom) respectively (see Supplementary Fig.7for cumulative variance explained ratio).ESSM size was measured in the original high-dimensional space (dimension equals the number of neurons). SSM radius is the average distance from points on the manifold to the manifold center. The lattice area measures the parallelogram area formed by two tangent vectors (i.e., Tan. vec., differentiated along x and y labels respectively, see Methods). Left: Each dot represents the measured quantity from one sampled datasetDsat one speed bin (see Methods, fifty sampledDswith eight speed bins = four hundred data points). The solid line and error band show the best-fitting line and 95% CI estimated using BLEA. Right: Dots and error bars represent mean and 95% CI of the slopes estimated using BLEA (left panel shows the case of R1M2). The numbers above error bars are r-squared scores. The texts below x axis tick label represent the significance level whether the slope fitted from original dataDsdiffers from that fitted from label-shuffled data (two-sided, Bayesian method, see Methods); ***p< 0.001; **p< 0.01; *p< 0.05; NS not significant. Source data are provided as a Source Data file.

A key question is how speed modulates the geometry of the SSM. Direct visualization of SSMs at different speeds is challenging, so we instead examined speed modulation using an example lattice on the SSM. This lattice consists of four spatially adjacent points, centered at the OF center, with an inter-point distance of 2 cm (Fig.3C). While keeping these four spatial locations fixed, we varied the speed components to construct a lattice manifold. PCA analysis revealed that this manifold is low-dimensional, with three principal components accounting for over 90% of the variance (Supplementary Fig.7). Based on this, we projected the lattice manifold into three or two dimensions for visualization (Fig.3D). The results indicate that the lattice expands as speed increases.

In addition to the lattice manifold, we also visualized other manifold slices, including those obtained by fixing the rat’s x-coordinate and using a larger lattice. All visualizations consistently imply that the SSM dilates with increasing speed (Supplementary Fig.7).

To quantify changes in SSM size, we used two metrics: (1) SSM radius, defined as the average distance from the SSM surface to its center, providing a global measure of SSM size; and (2) Lattice area, computed as the area of a parallelogram whose sides are tangent vectors of the SSM, capturing local manifold surface size. Across all experimental configurations examined, both SSM radius and lattice area increase with running speed, indicating that the grid cell SSM dilates as speed increases (Fig.3E).

To investigate this, we fitted manifolds and noise covariances for the sampled datasetsDsusing GKR. Total noise is the trace of the covariance matrix (Fig.4A). We found that total noise increases with increasing speed (Fig.4B, C). Compared to total noise, noise projected onto the manifold may be more relevant to information coding26. Therefore, we projected the covariance matrix onto the tangent plane of the SSM, and computed the trace of the projected covariance matrix as the projected noise. Consistent with total noise, projected noise also increases with speed (Fig.4B, C).

ATotal noise is the trace of the covariance matrix. Projected noise is the trace of the covariance matrix projected onto the SSM tangent plane.BEach dot is the measured quantity from one sampled datasetDsat a speed value (see Methods, fifty sampledDswith eight speed bins = four hundred data points). The line and error band show the best linear fitting line and 95% CI using BLEA.CDots and error bars represent mean and 95% CI of the slopes estimated using BLEA (panel B shows the case of R1M2). The numbers above error bars arer-squared scores. The texts belowxaxis tick label represent the significance level whether the slope fitted from original dataDsdiffers from that fitted from label-shuffled data (two-sided, Bayesian method, see Methods); ***p< 0.001; **p< 0.01; *p< 0.05; NS not significant. Source data are provided as a Source Data file.

Running speed has opposing effects on spatial coding. On the one hand, it expands the smooth spatial manifold (SSM), pushing neural representations of nearby locations further apart (Fig.3E), thereby improving spatial coding. On the other hand, it increases grid cell population noise (Fig.4C), which degrades spatial coding. To assess the overall impact, we examined (linear) Fisher information, a metric that quantifies the local discriminability of neural population representations, defined as∂μ/∂xTΣ−1∂μ/∂x, which incorporates both the noise factor (Σ) and the lattice area factor (lattice area is formed by tangent vectors∂μ/∂x) (Fig.5A). Fisher information is a commonly used metric for assessing the local discriminability of neural population representations. The total Fisher information, given by the trace of the Fisher information matrix, measures the overall precision of the representation, with higher values indicating better spatial coding45.

A(Linear) Fisher information mathematically combines noise covariance and tangent vectors (which determine lattice area), which is commonly used to measure the local discriminability of information from noisy neural states45. Total Fisher information is the trace of the Fisher information matrix. Each dot represents the measured total Fisher information from one dimensionally reduced sampled datasetDs6(Dsprojected into its first six PC subspace) at a specific speed value (fifty sampledDs6with eight speed bins = four hundred data points). The line and error band show the best linear fit and 95% CI using BLEA.BDots and error bars show the mean and 95% CI of the estimated slopes using BLEA (see Methods, panel A shows the case of R1M2). The numbers above the error bars represent r-squared scores. The texts below x-axis tick labels indicate the significance level of whether the slope fitted from the sampled datasetDs6differs from that of label-shuffled data (two-sided, Bayesian method, see Methods); ***p< 0.001; **p< 0.01; *p< 0.05; NS not significant.CWe computed theoretical SCA upper bounds derived from the Fisher information (red, see Methods), and also showed the actual SCA computed directly from data (blue, similar as Fig.1B, C, but usingDs6rather thanDs). Dots and error bands have the same meaning as in (A).DCorrelation between upper bounds and SCA. For each experimental configuration, fiftyDs6and eight speed bins were used, thus four hundred data points for upper bound and actual SCA, respectively (seeC). We then computed the correlation (black dots) between these two sets of four hundred data points, with error bars indicating the 95% CI obtained via Fisher transformation87. The texts below the error bars indicate the significance levels of whether the correlation differs from zero: ***p< 0.001; **p< 0.01; *p< 0.05; NS not significant (Pearson correlation test, two-sided). The results of the same analysis but using the original datasetsDsare similar (Supplementary Fig.8). Source data are provided as a Source Data file.

It is well known that Fisher information is hard to estimate in a high-dimensional space24. Therefore, besides using the originalDs, we also projectedDsinto the first six PCs, denoted asDs6. EachDs6was then fed into GKR for fitting a GKR model. GKRs fitted from bothDsandDs6were analyzed identically to double-check our results on Fisher information.

We computed the total Fisher information from the fitted GKRs (see Methods, Supplementary Fig.8AforDsand Fig.5AforDs6). Slope analyses indicate that Fisher information increases with running speed in bothDsandDs6(Fig.5BforDs6, Supplementary Fig.8BforDs, although four out of nine experimental configurations in the results ofDsdo not show statistical significance, possibly due to the curse of dimensionality). This increase in Fisher information with running speed suggests that the effect of SSM dilation outweighs that of increasing noise, leading to improved spatial coding at higher speeds, which is qualitatively consistent with the results obtained using SCA (Fig.1B–D).

Fisher information, derived from a purely bottom-up geometric approach, in fact is intrinsically linked to SCA (Fig.1B). Specifically, we established a theoretical upper bound for SCA directly from Fisher information (see Methods). This bound is approximately a linear function of the square root of total Fisher information. Therefore, an increase in Fisher information implies a corresponding increase in SCA.

We first tested this theoretical upper bound in synthetic datasets (Supplementary Fig.9). We showed that the SCA computed directly from the synthetic datasets is well bounded by the theoretical upper bound predicted by the Fisher information. Moreover, the upper bound exhibits trends consistent with actual SCA across different dataset parameters (e.g., number of data points, dimensionality).

We then applied this analysis to grid cell datasets, computing the theoretical SCA upper bounds using Fisher information fitted from GKR. The computed upper bounds are well above the actual SCA values (Fig.5C, Supplementary Fig.8C). More importantly, both SCA and its upper bound exhibit similar speed modulation effects. The correlations between the predicted upper bounds and actual SCA are positive (Fig.5D, Supplementary Fig.8D).

Overall, Fisher information, derived from the geometric properties of the SSM and noise, quantitatively aligns with decoding performance measured via SCA. Both approaches support the conclusion that grid cell spatial coding improves with increasing running speed.

GKR approximates the distributionpr∣xas a normal distribution, which might not always valid (see Methods and Supplementary Fig.10). To verify the validity of our previous sections’ results, we propose a modified approach—Gaussian process regression with kernel sampling (GKR-S)—that does not assume data normality. GKR-S contains two steps: first, it uses Gaussian process regression to estimate the manifold (as in GKR); second, it resamples from neighboring labelsxito generate pseudo-samples and thereby estimate the conditional distribution (see Methods).

We evaluated GKR-S on a synthetic dataset and found that it performs well only for low-dimensional data (Supplementary Fig.11A). Therefore, we projected the grid cell datasets onto their first six PCs and applied both GKR-S and GKR to estimate geometric properties (e.g., manifold size, Fisher information etc.). The two methods yielded closely matching results, both revealing better spatial representation at higher running speeds, indicating that, despite its normality assumption, GKR remains a reasonable method to estimate these geometric properties (Supplementary Fig.11).

What do these population results imply about individual grid cells? Here, we propose a simple model—the Independent Poisson Speed-Gain (IPSG) grid cell population, which contains three key assumptions: (1) grid cell firings are independent; (2) grid cells are Poisson neurons; and (3) the effect of running speed on individual grid cell firing is a monotonically increasing gain factor11.

Using these assumptions, we derived analytical expressions for manifold size, noise strength, and Fisher information as functions of running speed. Both these analytical results and additional numerical simulations demonstrate that the IPSG can qualitatively reproduce the observed positive speed modulation effects in Figs.3E,4C,5B(Supplementary Fig.12, see Methods).

Intuitively, in IPSG, increasing speed amplifies each cell’s firing without altering its spatial tuning. Because manifold size scales roughly with firing rate, it too grows with speed (see Methods, Fig.3E, Supplementary Fig.12). Poisson-neuron assumption implies that noise (the standard deviation) scales as the square root of the firing rate, so noise increases more slowly than firing rate. As a result, each cell’s signal-to-noise ratio—and hence its Fisher information—rises with speed under the independent-firing assumption (see Methods). More loosely, IPSG suggests that faster running increases grid-cell firing (without disrupting the rate map too much) more than noise and the effects of noise correlations are negligible, explaining the observed speed modulations on the manifold (Figs.3E,4C,5B). Whether this picture holds in real grid-cell data remains unclear. We leave more precise theoretically modeling and validations for future work, while this paper focuses more on data-driven descriptive analysis.

Our results indicate that grid cell spatial coding improves at high speeds, based on our analysis of simultaneously recorded grid cell population activity. A key advantage of analyzing population activity, as opposed to individual neural responses, is that it inherently accounts for the effects of noise correlation on spatial coding. Here, we use the term ‘noise correlation’ specifically to refer to the cell-to-cell noise covariance (i.e., between two different cells). Noise correlation can be information-beneficial or information-detrimental, depending on the geometric relation between noise covariance and the information encoding manifold (Fig.6A, also see a two-neuron toy example as an illustration in Supplementary Fig.13)24,26. In this section, we explicitly examine how noise correlation influences grid cell spatial coding.

ANoise correlation can either enhance (“information‐beneficial”) or impair (“information‐detrimental”) coding relative to a model of independent firing grid cells (IFGC).BIFGC’s GKR is identical to the GKR fitted from the original sampled datasetDs, except that the off-diagonal elements of the covariance matrix are set to zero (see texts and Methods). Total noise is the trace of the covariance matrix. Left: Each dot represents the total noise from one GKR model fitted to a specificDsat a given speed (fifty sampledDswith eight speed bins = four hundred data points). The lines and error bands show the best linear fitting and 95% CI using BLEA. Right: We computed the speed-averaged total noise as the average total noise across all speeds (from 5 cm/s to 45 cm/s) perDs. Since fiftyDswere used, we have fifty speed-averaged total noise values per experimental configuration, allowing fitting a normal distribution. Dots and error bars show the mean and 95% CI of the estimated speed-averaged total noise distributions (see Methods). The texts below each x-axis tick label indicate the significance level of whether the speed-averaged total noise fitted from the original GKR differs from that of IFGC GKR (two-sided, Bayesian method, see Methods). ***p< 0.001; **p< 0.01; *p< 0.05; NS not significant.C,DSame as (B), but measuring projected noise and total Fisher. Statistical testing (Bayesian Methods) on projected noise (or total Fisher) is one-sided—whether the projected noise (or total Fisher) obtained from the original GKR is greater (or smaller) than IFGC GKR (see Methods).EThe key idea of SCA is to assess classification accuracy between data points drawn from two spatial boxes. To compute IFGC’s SCA, we generated a “trial‐shuffled” dataset by permuting each cell’s firing activity across all data points within the same box, allowing eliminates intercellular noise correlations. Illustrations are the same as panel B, except showing SCA at different speeds. Statistical testing is one-sided (Bayesian method, see Methods), indicating whether the SCA from the original dataset is smaller than IFGC’s SCA. Source data are provided as a Source Data file.

To explore the role of noise correlation in spatial coding, we compared outcomes from the original grid cell dataset (Ds) with those from a hypothetical population of independent firing grid cells (IFGC)26. A conventional method to eliminate noise correlation is trial shuffling. For neural states recorded under identical conditions (e.g., the same spatial location) across different trials, one can randomly permute the firing profile of each neuron across trials. This shuffling preserves single-cell statistics while effectively disrupting intercellular noise correlation.

Although the OF task does not include repeated trials, the GKR serves as a generative model that can produce multiple data points for each condition. Specifically, for a given conditionx, the fitted GKR generates (theoretically) an infinite number of data points drawn from a Gaussian distribution (Eq.1). After the trial-shuffling procedure is applied to these generated data points, the mean remains unchanged, but the covariance matrix becomes diagonal, with all off-diagonal elements set to zero. Thus, the IFGC’s GKR model is equivalent to the original GKR model with a purely diagonal covariance matrix.

We computed the total noise levels for both the GKR and IFGC GKR models (Fig.6B). As expected, removing the off-diagonal elements of the covariance matrix does not alter its trace, leaving the overall noise unchanged. However, this manipulation affects the projected noise: the GKR model exhibits a higher projected noise than the IFGC GKR model (Fig.6C). This finding indicates that cell-to-cell noise correlations in grid cell activity “reshape” the noise structure, directing a larger fraction of noise onto the torus surface. Consistent with this, increasing noise correlation leads to smaller Fisher information (Fig.6D), underscoring their detrimental impact on information encoding.

To further validate that the correlation is information-detrimental, we employed a top-down decoding approach based on linear classification accuracy of neural states from two spatial boxes (i.e., SCA in Fig.1B), without using GKR. For the IFGC SCA, we randomly permuted each neuron’s firing rates within each box. This procedure preserves the single-cell statistics while effectively disrupting the noise correlations. Analysis of the permuted data showed that the IFGC SCA exceeded the SCA from the original datasets (Fig.6E), thereby confirming that noise correlation is information detrimental.

Accurate internal spatial representation is essential for navigation, and grid cells are widely regarded as a fundamental component of this process2,3. Previous analyses of speed modulation on grid cell coding have predominantly focused on individual cells or cell pairs11–14, thereby neglecting the influence of population noise covariance—a factor that can significantly impact coding fidelity26. Here, we developed GKR to study the population coding from an information geometry perspective. We demonstrated that the grid cell manifold expands in size as speed increases. This manifold dilation effect exceeds the increase in noise, as indicated by the higher Fisher information observed at high speeds. Overall, our results favor the hypothesis that increasing running speed increases grid cell spatial coding accuracy. GKR can be a powerful tool to study neural population representation from an intuitive information geometric perspective.

However, GKR has its limitations. First, it does not perform well in very high-dimensional spaces, which may require a larger number of data points (Fig.2and Supplementary Fig.4). This issue may be mitigated by first applying a dimensionality reduction method to the data46. Second, GKR assumes that the data follows a normal distribution. While the normal distribution is a commonly used approximation34,38—GKR may produce unreliable results if the true distribution deviates significantly from normality. It is advisable to perform a normality test (Supplementary Fig.10), compute test data’s likelihood34, or use an alternative method (e.g., GKR-S, Supplementary Fig.11) to verify the results. Finally, GKR is only applicable when the data explicitly contains labels. In other words, its purpose is to evaluate the geometric representation properties of known labels of interest. For example, in vision42, navigation, or working memory47studies, the labels of interest are often defined by stimulus parameters. However, in cases where one aims to evaluate the representation structure of latent variables, it is necessary to first apply latent variable inference methods46,48, and then apply GKR. Overall, future improvements to GKR could focus on enhancing its performance in high-dimensional settings, adapting it for non-normal data, and extending its applicability to scenarios without explicit labels.

We analyzed the population-level properties of grid cell activities, but what are the implications of these findings for individual grid cells? There are two major models of grid cells49: the rate-based model and the oscillatory-interference model. First, in some rate-based models (continuous attractor networks20), running speed serves as an input to the grid-cell network. Faster speeds may elevate firing rates11,12, which can sharpen the spatial rate map, enhance the signal-to-noise ratio, and thus boost population Fisher information (Fig.5). Although some other rate-based models contain normalization mechanisms implying the opposite—that the population mean firing rate should not change with speed (e.g., self-organizing models50). Secondly, in the oscillatory-interference model, running speed modulates the frequency of membrane potential oscillation which might lead to more accurate spatial fields49(although a study suggests that MEC theta frequency is modulated by acceleration rather than speed51). Finally, higher running speeds imply more frequent encounters with environmental boundaries, allowing for more frequent corrections in grid coding18. Despite these conjectures, it should be noted that real grid cells are complicated, involving correlated noise. Models based solely on individual grid cells, without accounting for noise correlations, may result in substantial estimation errors, as shown by the pronounced discrepancies between the original data and the IFGC model in Fig.6. The connection between population results and individual grid cells remains for future exploration.

Rats typically exhibit higher running speeds in novel environments52, as implied by this paper, which might enhance grid coding thus supporting more effective adaptation to novel surroundings. Grid cell representations are known to change in novel environments53–55. Some studies suggest that the grid pattern rescales in a novel environment (e.g., Barry et al. 201256); others propose that rats refine their grid coding by learning the environment’s boundaries18,55. These alterations in individual grid cell patterns may reflect corresponding changes in the representation geometry, such as a rescaling of the toroidal structure or localized distortions near environmental boundaries. The effects of environmental modulation on population-level representations remain an open question for future investigation.

Beyond grid cells, how does running speed influence other cell types in the navigation system? Hardcastle et al. found that the spatial decoding accuracy of MEC neurons improves at higher speeds, suggesting that increased speed generally benefits MEC spatial representation10. This aligns with findings that, similar to grid cells, running speed predominantly increases the firing rates of other MEC cell types, including head direction cells, speed cells, and conjunctive cells11,17. However, the modulation effects of running speed on hippocampal cells may be more complex. Grid cells are modeled as a primary feedforward input to the hippocampus8,57, suggesting that running speed should also enhance place cell representation as well. However, this feedforward model is a simplification, as the hippocampus sends feedback projections to the MEC58. Moreover, place cells receive inputs not only from grid cells but also from other sources, such as head direction cells57. These additional mechanistic factors obscure how running speed modulates place cell activity. Indeed, earlier research suggests that the majority of place cells are not strongly modulated by running speed, at least not in an obvious manner59. Nevertheless, beyond speed modulation, movement direction could influence the representational geometry of place cells, as it has been shown to reshape place fields60. Extending geometric analyses to other cell types remains an interesting avenue.

Running speed modulation effects have been widely observed across other brain regions as well61. For example, locomotion primarily suppresses neural activities in the auditory cortex62,63. In contrast, locomotion generally enhances V1 neuron activity64,65, but may turn to suppression after certain high running speeds66. In fact, the effect of locomotion modulation is usually entangled with other modulation factors61,66,67. For example, V1 neural activities are jointly influenced by both animal’s running speed and visual stimuli movement speed65. This influence can be mathematically expressed as a weighted sum of the two speed contributions, with weights varying diversely across neurons. The geometric approach has been shown to be a practically effective method to assist in understanding the diversity of individual neurons from a comprehensive population-level perspective41,68,69. GKR can be a useful tool to understand the diversity of running speed modulations in different brain areas.

One advantage of GKR is its ability to provide detailed inspection of location geometry. Local geometry reveals the intricacies of information coding within a small range of values, which is particularly useful for comparing the representation bias of different information values. Representation bias has been observed in the navigation system60,70. For instance, place and grid cells’ fields tend to shift towards reward locations, which has been interpreted as an overrepresentation of rewarded locations70–73. From a geometric perspective, overrepresentation implies larger Fisher information, which can be attributed to either local manifold dilation, reduced projected noise, or both (Figs.3,4,5). The concepts of local manifold dilation and reduced noise have been supported in working memory studies: (1) The working memory system may use attractors to reduce noise47,74,75. (2) Recurrent neural networks (RNNs) trained on working memory tasks utilize larger state spaces to represent common values, thus yield improved Fisher information47. In RNNs, manifolds are often observed to be quite simple, usually taking the form of a low-dimensional ring structure47. This simplicity allows the size of the encoding space to be measured using straightforward methods. However, in the actual brain, manifolds can be highly complex and high-dimensional69. The GKR method illustrated in this paper can be particularly helpful in studying the local structure of these complex, high-dimensional manifolds, assisting the analysis of representation bias.

Experimental data were collected by Gardner et al.35. Rats performed open-field foraging (OF) tasks in a 150 cm wide OF box. Three-dimensional motion capture tracked the rats’ head positions and orientations using five retroreflective markers attached to the implant during recordings. The 3D marker positions were then projected onto the horizontal plane to determine the rats’ 2D positions. Neuropixel probes recorded neural activity in the MEC. Neural activity were then processed using a clustering method to classify neurons into grid cells and non-grid cells35. In total, these procedures yielded nine sets of simultaneously recorded grid cell population activities (i.e., nine experimental configurations): rat ‘R’ day 1 modules 1, 2, 3; rat ‘R’ day 2 modules 1, 2, 3; rat ‘S’ module 1; and rat ‘Q’ modules 1, 2. We used a shorthand notation, e.g., “R1M2”, to represent rat R (“R”) on day 1 (“1”) and grid cell module two (“M2”). Note that “R1” does not necessarily refer to the same day as “S1”. Day labels are used solely to distinguish recordings from the same rat. These processed data are available from Gardner et al. 202235.

The grid cell rate maps shown in Fig.1Aand Supplementary Fig.1were computed as follows. Firing rate was estimated by dividing spike counts by 10‐ms time bins and then convolving the result with a Gaussian filter with a standard deviation of 20 ms. To estimate the averaged firing rate at different locations, the OF box (150×150cm) was digitized into small spatial bins of3×3cm. Firing rates at each visited spatial bin were averaged, and those at each unvisited bin were set to 0. To correct the effect of unvisited bins, we created a mask(M0)with a value of 1 at the visited bins and 0 at unvisited bins. Next, both the firing rate and maskM0were spatially convolved with a 2D Gaussian filter with a standard deviation σ = 8.25cm. The convolved firing rate was divided by the convolvedM0to obtain the final corrected rate map for each cell.

Gridness measures how well a grid cell’s rate map conforms to a hexagonal pattern12. Some grid cells’ rate maps have incomplete peaks at the OF box boundaries. To correct this boundary effect, the rate map was first padded by 30 cm on each side. This padding was performed by linearly ramping the firing rate at the edges to zero over the outer 30 cm of the padded area. (implemented using the ‘numpy.pad‘ function in Python, with ‘mode = ‘linear_ramp’‘). Autocorrelating the padded rate map produced an autocorrelation map. The boundary effect of the autocorrelation was corrected by padding with zeros on all sides (implemented using ‘scipy.signal.correlate2d (padded_rate_map, padded_rate_map, mode = ‘same’, boundary = ‘fill’, fillvalue = 0)‘).

The autocorrelation map was masked by two circles centered at the map’s center. The outer circle’s diameter matched the edge length of the autocorrelation map. The inner circle’s area was 15% of the outer circle’s area to filter out center peaks on the map. Only the regions between the two circles were kept; the rest were set to 0. Next, the masked autocorrelation map was correlated with its rotated versions (rotated by 30, 60, 90, 120, and 150 degrees, respectively). A well-defined grid cell should have peak correlation values at 60 and 120 degrees, and valleys at 30, 90, and 150 degrees. Gridness was calculated by subtracting the average valley values (30, 90, and 150 degrees) from the average peak values (60 and 120 degrees).

Time was binned in 10‑ms intervals. Spikes count at each time bin was computed, and then divided by 10 ms as an estimate of firing rate. The firing rate was then temporally smoothed using a Gaussian kernel with a standard deviation of 20 ms. To estimate the rat’s speed, velocity was first computed by computing the finite differences of the rat’s positions, i.e.,pi+1−pi−1/20wherepiis the rat’s position at time bini. The velocity’s L2 norm is the speed. This procedure provided a feature map indicating grid cell firing rates, with rows representing time bins and columns representing grid cell IDs; and a label with rows representing time bins and three columns indicatingxlocation,ylocation, and speed. Data with speeds lower than 5 cm/s or higher than 45 cm/s were excluded. Grid cells with low gridness (below 0.1) were also excluded. The combined feature map and label are termed as a grid cell dataset, denoted asD. There are 9 grid cell datasets corresponding to different experimental configurations (different rats, grid cell modules, and different days). The number of grid cells in each dataset is: 113 in R1M1, 132 in R1M2, 51 in R1M3, 140 in R2M1, 153 in R2M2, 62 in R2M3, 96 in S1M1, 81 in Q1M1, 53 in Q1M2.

The speed distribution inDis highly biased. It has more data in low-speed region than in the high-speed region (Supplementary Fig.2). This biased distribution of data may cause potentially biased evaluation. To avoid this, we performed resampling on the dataset as follows. Speeds ranging from 5 cm/s to 45 cm/s were binned into 5 cm/s bins. Data in each speed bin was collected. LetNspmindenote the minimum number of data points among all speed bins, we definedKas themin{Nspmin,10, 000}. In each speed bin, we sampledKdata points (without replacement). Sampled data points from different speed bins were combined to create a single sampled dataset, denoted asDs.Dshas a roughly equal amount of data at each speed value. The above sampling procedure was repeated 50 times, resulting in 50 sampled datasetsDsper experimental configuration.

As a baseline comparison, we also shuffled the dataDby permuting the label timestamps, thereby disrupting the relationship between neural states and labels. This permuted data was then processed using the same sampling procedure as described above, yielding 50 label-shuffled-sampled datasets.

The dimensionality ofDsis the number of grid cell, which can be more than 100. This can pose a challenge in accurately estimating covariance and Fisher information24. Therefore, we also performed PCA onDs, projecting onto the first 6 principal components to obtainDs6. The same projection procedure was applied to the shuffled datasets. These projected datasets were used for estimating Fisher information in Fig.5, and comparing to GKR-S in Figure 11.

A common way to evaluate the quality of neural population representation is to assess how accurately a simple linear classifier can distinguish between neural population representations of two adjacent experimental conditions (e.g., stimulus parameters or locations in this paper)22. In this paper, this type of classification accuracy is referred to as spatial coding accuracy (SCA, Fig.1B).

For a given sampled datasetDs, we split it into 8 speed-split datasets (SSD) based on speed values. Specifically, data with speed values withinvi,vi+5cm/swere collected as one SSD, wherevi=5, 10,…,40cm/s. For each SSD, we randomly sampled 300 spatial locationsxc. For each locationxc, we constructed two adjacent locationsx±=xc±δle^, wheree^is a unit vector with a random angle,δl=5cm. Eachx±defines a small spatial box, centered atx±with an edge length of 10 cm. Data from the two boxes were collected. To ensure fair classification, the data from the box with the larger number of data points were subsampled (without replacement) so that both boxes had an equal number of data points. The data from the two boxes were then concatenated. If the total number of data points was less than 50, thisxcwas discarded due to insufficient data. Otherwise, the concatenated data was split into train and test sets (0.67:0.33). A logistic classifier (with an L2 regularization coefficient C = 1, using the scikit-learn package) was then trained on the train set and evaluated on the test set. The classification accuracy averaged across all validxcis defined as the SCA of that speed binvi,vi+5cm/s. This procedure was applied to all speed bins,Ds(orDs6in Fig.5C), and the label-shuffled dataset.

Besides using logistic regression for classification (Fig.1), we also computed SCA using perception and support vector machines, see the results in Supplementary Fig.3.

Metrics considered in this paper include SCA (e.g., Figs.1C, D,5C), torus radius, lattice area (e.g., Fig.3E), total and projected noise (e.g., Fig.4B, C), and Fisher information (e.g., Fig.5, Supplementary Figs.8A, B), etc. For each sampled datasetDs, we computed the metric values at different speed bins, forming a metric-speed dataset consisting of metric valuetiand the corresponding speed valuevi, whereiindexes theith data point in the metric-speed dataset. For example, one dot in Fig.1Cis one data point in the SCA-speed dataset (with a correspondingDs). For convenience, we definexi=vi,1, which includes speed and a constant for a bias parameter. Currently, we limit our discussion to oneDs, and later we will ensemble results from differentDsby Bayesian model averaging39.

Given a metric-speed dataset from oneDs, we used Bayesian linear regression (BLR) to fit the linear relationship between a metric and speed38. The benefit of BLR over ordinary least squares is that BLR naturally provides a way to set the regularization parameter (by setting the prior) and offers the posterior distribution of inferred parameters (e.g., slope), thus enabling a pure Bayesian analysis of the data. We follow the implementation of BLR in Bishop 200638.

whereϵ~Nϵ∣0,β−1,βis a scalar representing precision, andy=wTx. This equation indicates that the conditional distributionpt∣x,wis a Gaussian distribution with meanyand varianceβ−1. The prior for parameterwisw~Nw∣0,α−1IwhereIis a2×2identity matrix, andαis a scalar. Given the prior and conditional distribution, we can derive the posterior distributionpw∣t,Xand predictive distributionptq∣xq,t,X, wherexqis the query label,tandXare the data points in the metric-speed dataset,tqis the prediction. Both posterior and predictive distributions are Gaussian distributions. Hyperparametersαandβwere estimated by maximizing the marginal likelihoodpt∣α,β,Xthrough an iterative method38.

Overall, given a metric-speed dataset (obtained from one sampled datasetDs), BLR provides the posterior distributionpw∣t,Xand predictive distributionptq∣xq,t,X.To simplify the notation, the two distributions are written aspw∣Dsandptq∣xq,Ds, which followNw∣mw,s,Σw,sandNtq∣mt,s,Σt,s, respectively.

where the first term is the average of covariances, second term represents bias.

wherew~Nw∣mw,Σw,ϵ~Nϵ;0,∑sβs−1/B, andβsis the best hyperparameter fitted using an iteration method from a sampled datasetDs(see above). Overall, we obtainedpw∣Dandptq∣xq,D, which are both Gaussian distributions. This overall method pipeline is called Bayesian linear ensemble averaging (BLEA) in this paper. Mathematical details can be found in Supplementary Methods.

pw∣Dandptq∣xq,Dallow us to estimate the confidence interval (CI) and assess statistical significance from Bayesian framework40. First, since the predictive distributionptq∣xq,Dis a Gaussian, the 95% CI of the prediction (two-tailed) is given by an interval [a, b] such thatΦa−μ/σ=0.025andΦb−μ/σ=0.975, whereΦ⋅is a cumulative distribution function of a standard Gaussian distribution,μandσare the predictive distribution mean and standard deviation. One example CI can be found in Fig.1C, which well covers most of data points. 95% CI is also called a credible interval in the Bayesian framework40.

Similarly, knowing the posterior distribution of slopepw∣D, we can also compute its 95% CI. For example, this is illustrated by the error bars in Fig.1D.

We are interested in whether the slope fitted fromDis statistically different from that fitted from the label-shuffled dataset (e.g., Fig.1D). Therefore, we also prepared label-shuffledDsfromD(see Methods: Data Preprocessing), and ran the above analysis to obtain their label-shuffled posterior and predictive distributions. Both of the posterior distributions of the original and label-shuffled datasets are Gaussian, hence we defined the slope differenced=wdata−wshuffle, which also follows a Gaussian distribution with a mean equal to the difference between the two slope means and a variance equal to the sum of the two variances. Based on the distribution ofd, probability of direction,pd, can be computed as the maximum ofPd>0andPd<0.pdrepresents the probability thatdto be positive or negative (depending on which is the most probable). It directly relates top-value (from a frequentist framework, two-sided) byp=2×1−pd, where the null hypothesis is thatd=0and alternative hypothesis is thatd≠040. Thus, statistical statements can be made based on thep-values. One-sidepvalue ispdor1−pddepending on the one-side direction.

We are also interested in whether the speed-averaged metric computed under theDis statistically different from that computed under the hypothetical independent firing grid cell assumption (IFGC, Fig.6). For eachDs, we averaged the metric value across speed values. This gives onet¯s. Fiftyt¯swere concatenated and fitted by a Gaussian distribution using the maximum log-likelihood method, as an approximation ofpt¯s∣D. Therefore, we can use the same method above to determine whether the speed-averaged metric computed under original dataset is statistically different from that computed from the IFGC (Fig.6B–E).

wherer∈RNrepresents a neural state containingNneurons, andx∈RMrepresentsMlabels. Labels are defined broadly. They can be stimulus parameters (e.g., grating image orientation, object positions), an agent’s latent state (e.g., latent dynamics factor, emotion), or an agent’s behavioral labels (e.g., agent speed, agent position).μis the mean of neural state, modeled as a continuous function of the labels.μis also referred to as a manifold.ϵis white noise with a covarianceΣx. Given noisy neural statesrand corresponding labelsx, our goal is to infer the smoothly varying manifoldμand covarianceΣ.

Bin averaging is a straightforward estimation method. This approach divides the entire range of labelxinto small bins. Data pointsriwithin each bin are considered to have the same labelxi. Hence, the manifold can be estimated by sample averageμxi=rxi, where⋅xidenotes averaging over the data points within binxi. Similarly, the covarianceΣcan be estimated by the sample covariance matrix.

whereSis the sample covariance,λis the shrinkage coefficient estimated by the Ledoit-Wolf (LW) shrinkage algorithm43,Nis the number of neurons, andIis an identity matrix. This algorithm is implemented by a Python function ‘sklearn.covariance.LedoitWolf‘.

One disadvantage of the bin average and LW methods is that the estimation of one bin’s covariance does not use data from adjacent bins. Ideally, the manifold and covariance matrix are smooth over label values. Data in adjacent bins provide certain information about the current bin. Therefore, we developed the Gaussian Process with Kernel Regression (GKR) method to infer smoothly varying manifold and covariance from noisy neural states. GKR has two major steps: (1) inferring the manifold via a Gaussian process, and (2) inferring the covariance matrix. Note that while Gaussian processes have been used in previous studies to infer the firing of individual grid cells76,77, our method, GKR, which is partially based on Gaussian processes, focuses on cell-to-cell statistics.

wherepirepresents the period of the circular variable. Based on all these modeling, the problem of inferringμ~from noisy datar~,xbecomes a classical Gaussian process regression problem, where the parameters{β,li,σi,ci}are optimized to maximize the log-likelihood of a joint Gaussian distribution forμ~~GPN0,kμ. Finally,μ~is unstandardized back toμ.

In many scenarios, the labelxspans a large continuous range rather than a few discretized values (e.g., possible positions of a rat in a navigation task). In this case, Gaussian process regression requires computing a large kernel matrix, leading to expensive matrix manipulations78. To reduce this, we employed a variational inducing variable method78. It approximates training label values with a smaller set of inducing pointsz, thereby reducing the time complexity. In this paper, inducing points were initialized as a randomly sampled subset of the original training labels (200 inducing points), and were optimized during the optimization of Gaussian process regression. Gaussian process regression with inducing variables method is implemented in the Python GPflow package79.

whereisums over all training data points, andη=10−6is a small number for numerical stability (keeping the covariance invertible even in the first term is small).kLx,xiis a weight kernel that represents the contribution ofCxiin estimating the covariance matrix at labelx. It is normalized such that∑ikLx,xi=1. To gain an intuition of this method, consider a simple case where (up to a normalization)kLx,xi=1ifxi−x<δand zero otherwise, step two is simply a sample covariance in a small bin of half-widthδ.

withκas a normalization factor ensuring∑ikLx,xi=1, andLis anM×Mupper triangular matrix, interpreted as the Cholesky decomposition of a semi-positive definite precision matrixLLT. Note that the precision matrix has non-diagonal terms, hence the interactions between different label components are considered.

where terms irrelevant to covariance are omitted. Notably, whileΣis the weighted average of the Gram matrices from the training set, the log-likelihood functionLshould be evaluated from the validation set, where we used different indicesi,jto distinguish (Eq.10using training set). Setting the log-likelihood function on the training set would result inΣxconverging to the Gram matrixCx. This can be demonstrated by computingΣxto satisfy the condition∂L/∂Σ=0. Therefore, splitting between training for computing covariance and validation for computing likelihood is necessary.

Overall, we use the following procedure to fit the manifold and covariance from a dataset. In step one, the entire train dataset was used for Gaussian process regression, obtaining a continuous manifold functionμ. In step two, the train dataset was split into batches, each containing 3000 data points (except for the final batch). Each batch was further split into train and validation sets (0.66:0.33). The train set was used for computing the covariance matrix given anL(initialized as an identity matrix), and the validation set was used to compute the log-likelihood function. The log-likelihood was maximized by an Adam optimizer (gradient applied onL). This batch training was repeated for 30 epochs. Finally, with the optimizedL, the whole dataset was used for computing covariance (Eq.10). The computer code for implementing GKR is provided athttps://github.com/AgeYY/speed_grid_cell_information.git.

We are interested in whetherpr∣xfollows a normal distribution, as assumed by GKR. We first inspected the case of an example cube that centered at (11 cm, 37 cm, 18 cm/s) with edge lengths of (10 cm, 10 cm, 10 cm/s) in the label space. Data (from aDssampled from R1M2) within the cube were collected and projected into their PC1-PC2 plane, as well as PC1 axis and PC2 axis for visualization. Using the projected data, we fitted the optimal 2D and 1D normal distributions using maximum likelihood estimation and overlapped the optimal normal distributions with the projected data for direct visual comparisons. To formally assess normality, we performed Henze–Zirkler test to the projected 2D data, and Shapiro–Wilk tests to the projected 1D data (see Supplementary Fig.10). This example cubic data shown in Supplementary Fig.10does not follow a normal distribution.

Next, we examined normality across sampled cubes. For eachDs(oneDswas sampled for one experimental configuration, e.g., R1M1, R1M2 etc.), we sampled 300 cubes. Data within each cube were projected onto their first 20 PCs. For each cube, we randomly sampled one PC, and applied Shapiro–Wilk test to assess normality. Ap-value below 0.05 was considered indicative of non-normality. The percentage of non-normal cubes is shown in Supplementary Fig.10. We performed normality tests on the projected PC rather than the full high-dimensional space, as testing in the full high-dimensional space would require significantly more data and could lead to unreliable results.

GKR assumes the conditional probabilitypr∣xfollows a normal distribution, whereas GKR-S employs a non-parametric approach that does not impose this assumption. The first step of GKR-S is identical to GKR, using Gaussian process to infer the meanμx. We then computed the residue asϵxi=ri−μxi, where the subscriptidenotes a training data point. The residual vectorϵxiis also referred to asϵi.

withκas a normalization factor ensuring∑ikx,xi=1, and for simplicity,Σis a diagonal matrix. We determined the diagonal elementsΣjjheuristically using Silverman’s rule-of-thumb80:Σjj=4d+11d+4n−1d+4σjwhereσjis the standard deviation of thej-th label,nis the number of training data, anddis the dimensionality of the labelx.

For a given query labelxq, this kernel function assigns a weight for each training data point. These weights serve as priors for resampling the training data. The sampled data are thought to be drawn from the conditional distributionpϵ∣x. Consequently, the statistics ofpϵ∣xandpr∣xcan be empirically computed.

whereδ⋅is the Dirac delta function. Consequently, the mean ofpr∣xcan be computed analytically asμx+∑ikxi,xϵi.

When estimating the covariance, for simplicity, we assume thatμxis a close estimation of the true mean, then∑ikxi,xϵi≈0. This significantly simplifies the expression of noise covariance to∑ikxi,xϵiϵiT. For computational efficiency, we dropped training data points for whichkxi,x<10−6. It is worth noting that this mathematical form of covariance estimation is similar to the one used in GKR (Eq.10), implying that the second step of the GKR may be thought as a case of a resampling method.

Using the above formula, we obtained both the mean and noise covariance, that allow further computation of other quantities as described in Methods: Computing Geometric Properties of the Speed-Slice Manifold at Different Speeds. In particular, the projected noise is estimated using Eq. (21), which is mathematically equivalent to resampling an infinite amount of data, projecting it onto the tangent plane, and then computing the projected data’s covariance matrix. This paper focuses on the linear Fisher information which sets a fundamental limit on the variance of any unbiased linear estimator81. Both projected noise and linear Fisher information (referred to simply as Fisher information in this paper) are fully determined by the mean and covariance matrix.

withνrepresenting a constant for stimuli-independent noise, andγas the non-diagonal decay rate. Note that the covariance matrixΣdepends on the manifoldμ.

whereVM⋅denotes a von Mises function,gi~U0.5, 1.5is a random gain,zi=2iπ/Nis the preferred label value for thei-th neuron, andσ=0.3is the tuning width.xis a circular scalar label ranging from 0 to2π. Parameters for generating covariance matrix (Eq.16) are:α=0.2,ν=0.05,γ=1.

wherezi~U−1, 1,−1, 1is the center of the receptive field of neuroni,σ=0.3, andλi~U0.5, 1.5controls the tuning width. The labelxis two-dimensional, with each component ranging from −1 to 1. Parameters for generating covariance matrix (Eq.16) are:α=0.5,ν=0.1,γ=1.

To generate a synthetic dataset of sizeT,Tlabelsxwere uniformly sampled from the entire range. Each sampled labelxwas then used to compute one manifold pointμand one covariance matrixΣ, thus generating onerusing a Gaussian distribution (Eq.15).Tlabels generateTdata points.

When visualizing the ground truth of synthetic datasets in Fig.2B, 100 labelsxwere randomly sampled. Then manifold pointsμand covariance matrices were computed. Manifold points were then fed into a PCA, dimensionally reduced to the first 2/3 dimensions (two for Fig.2and three for Supplementary Fig.4). The covariance matrices were also projected onto the PCA subspace, transforming to a 2 × 2/3 × 3 matrix. The eigenvalues of this 2×2/3×3 matrix were visualized as the lengths of the ellipsoid’s major axes (Fig.2, Supplementary Fig.4); and eigenvectors were visualized as the ellipsoid’s major axes directions.

We evaluated the performances of estimators (Bin average, LW, GKR) by comparing their predictions to ground truth. We evaluated several metrics: (1) manifoldμ(2) covariance matrixΣ(3) Riemannian metric∂μ/∂xT∂μ/∂x(4) Linear Fisher information∂μ/∂xTΣ−1∂μ/∂x(5) Precision matrixΣ−1.∂μ/∂xwas estimated numerically by finite difference.

For each condition (number of data points or number of neurons, Fig.2), ten synthetic datasets were sampled. For each dataset, all data were used for training the estimator. Trained estimator predicts the values of metrics at other 100 randomly sampled labels. The relative estimation error is the mean of∥Mi−M^i∥F/∥Mi∥Fover all 100 labeli, whereMiis the ground truth quantity whileM^iis the estimated quantity,∣∣⋅∣∣Fis the Frobenius norm.

GKR was applied toDsto fit a manifold and a covariance matrix. SinceDshas three labels (two for locations and one is the speed), the full manifold is an intrinsically three-dimensional object. For the ease of visualization, we visualized slices of the manifold instead.

First, we visualized the manifold representing different locations but fixing the speed at 20 cm/s, i.e., speed-slice manifold (SSM). A 30 by 30 grid of positions was sampled from the entire OF space. Along with the fixed speed of 20 cm/s, these labels were fed into the fitted GKR to predict the manifold points on SSM. These 900 predictions were first reduced to 6 dimensions using PCA, then projected non-linearly into 3 dimensions using Uniform Manifold Approximation and Projection (UMAP), implemented by the Python umap-learn package. The parameters used were ‘n_neighbors’ = 100, ‘min_dist’ = 0.8, ‘metric’ = ‘cosine’, and ‘init’ = ‘spectral’. To visualize the continuous manifolds, we interpolated small surfaces of adjacent x, y coordinate predictions using the plot_surface function in the Matplotlib Python package (Fig.3B).

We visualized other manifold slices similarly. Figure3Dconsidered four adjacent spatial points (centered atx=75cm,y=75cm, with an adjacent points’ distance of 2 cm) and varying speed values. Supplementary Fig.7Cconsidered four distant spatial points (centered atx=75cm,y=75cm, with an adjacent distance of 20 cm). Supplementary Fig.7Dconsidered a slice with a fixedx=75cm. PCA analysis on these slice manifolds suggested they are low-dimensional (3 PCs are sufficient to explain 80 percent of the variance). Hence, these manifold slices were directly visualized in the space of the first two/three principal components.

Persistent homology is a method to analyze the topological structure of data clouds82. Each point in the data cloud was replaced by a small ball of radiusr. If the distance of two points is smaller than2r, then they would be connected. Roughly speaking, a graph with dots and connected lines is called a simplicial complex. Simplicial complex can have several holes of different dimensions (0D hole means a single component connecting all points; 1D hole is a loop; 2D hole is a cavity). As the dot ball radius increases from 0 to infinity, different dots would be connected, resulting in different simplicial complexes. During this process, some holes emerge while some holes die out. The birth and dead time of different holes can be collected and represented as bars. All bars of the same hole dimensions form a barcode for that dimension. Usually most bars are short, they likely represent noise, while long-life bars indicate non-trivial topological structure of the data cloud. The number of long-life bars in each dimension is counted as a Betti number, written inβi. For example, a loop manifold should have one long-life zero-D hole, one one-D hole and no 2D holes. Hence the corresponding Betti number should beβ0,β1,β2=1, 1, 0. In particular, a torus should have Betti numbersβ0,β1,β2=1, 2, 1. We used the software package Ripser to compute the barcode, accompanied with approximated sparse filtrations to increase computational efficiency83(epsilon approximation constant = 0.2, see more detail in Ripser84). Intuitively, instead of computing the distance matrix of all points in the data cloud, approximated sparse filtrations discard balls which are completely covered by other balls (under certainr).

To build an objective procedure counting the number of long-life bars in the barcode, we defined a bar-length threshold to distinguish long-life bars. Here we defined the length threshold heuristically same as previous study35. A data point (e.g., a neural state) is a n-dimensional array where n is the number of grid cells. All data points form a m-by-n matrix, where m is the number of data points. We then randomly rolled (periodic boundary) each column of the matrix. This shuffled dataset was then fed into persistent homology, the maximum bar length was collected. This shuffling procedure repeated 20 times and obtained the final maximum bar length among 20 shuffling. This is the bar-length threshold.

Specifically,Dswas used to fit the GKR. When estimating the topological structure of the full three-dimensional manifold, 6,400 random labels were randomly sampled, and input to GKR to generate 6,400 manifold points. To simplify these data points, in accordance with Gardner et al. 202235, we firstly projected these data points into six PC subspace, and then used k-means to compute 1,200 cluster centers. These centers were then fed into Ripser, and Betti numbers were estimated from the above procedure.β0,β1,β2=1, 2, 1suggests a signature of possible toroidal-like topology.

When estimating the topological structure of SSM (Fig.3B, speed = 20 cm/s), 30-by-30 grid locations were collected, fed into the GKR to make predictions. These 900 manifold points were then projected into the first six PC subspace, and then fed into Ripser to compute Betti numbers (there’s no need to use k-means approximation in this case, because 900 data points is a good number to computationally handle, unlike the full-manifold case above).

Dswas used to fit the GKR. The fitted manifold is a function ofx,ylocations and speedv. For each fixed speed value, we randomly sample 500 locations denoted asxi,yi.

The SSM center is the averaged 500 manifold points, denoted asμcv.

The average of lattice area over 500 random points is the (averaged) lattice area of a SSM, shown in Fig.3E.

Fisher information matrix is defined asJTΣ−1J, whereJis the Jacobian matrix in respect to spatial location. Total Fisher information is the trace of Fisher information matrix, averaged over all 500 random points.

Project noise is the trace of projected noise matrix, averaged across 500 randomly sample points.

whereAis the area of the open fieldX;M¯=∫XMxdx/A;μis the vector of mean firing rates withμix,v=fivMix, andμ¯vis its spatial average overX. Therefore, Eq. (23) states that the manifold size increases with running speed.

To further evaluate whether IPSG grid cells can reproduce the increase in manifold size, total noise and Fisher information with speed (Figs.3,4, and5), we simulated simple IPSG grid cells. The rate map of each grid cell isMix=∑acos2πLkax+ϕia+4, whereka(a=1, 2, 3) represents unit vectors pointing at 0, 60, and 120 degrees, respectively.L=πis the spatial period, with the simulated field spans−π,π×−π,π. The phase offsetϕiawas randomly sampled from a uniform distribution over0, 2π; The additive factor of 4 ensures positive firing. All grid cells share the same speed gain functionfiv≡fv=20−20exp−v/10, which models a saturating speed gain, as implied by Hinman et al. 201611.

We simulated 10 IPSG grid cells, and randomly sampled 10,000xandvfrom uniform distributions, withxvalues ranging from the whole field and speed values ranging from 0 to 40. For each sampledxandv, we computed the mean firing rate of each grid cell, and then generated Poisson spikes, resulting in a simulated dataset containing spike counts along with their correspondingxandv.

To estimate uncertainty, we performed bootstrap resampling on the dataset 10 times (with replacement), generating 10 resampled datasets. For each resampled dataset, we used GKR to fit the data and estimate the manifold size, total noise and Fisher information, as shown in Supplementary Fig.12.

Consider a classification problem involving data from two small boxes centered atx±=xc±δx. Denote the two classes asC1andC2. In the process of evaluating SCA, we subsampled the data points so that two boxes have equal data set sizes. In line with this, the prior probabilities of a data point belonging to either class are equal,pC1=pC2=1/2. We also assume the neural staterin boxiis approximately given byNr;μi,Σ, whereican be 1 or 2. Here we derive the optimal classification accuracy if the classification boundary is linear (as used by the logistic classifier). A linear classification boundary means that the class isC1ify=wTr−w0<0, andC2otherwise.

Classification accuracy is the probability of a correct classification.

whereΦ⋅is the cumulative density function of a standard normal distribution. We used the fact that, ifpr∣Ciis a Gaussian,wTris also a Gaussian with meanwTμiand variancewTΣw.

Next, we find the optimalPcorrect. Let∂Pcorrect/∂w0=0, we getw0=wTμ1+μ2/2. Let∂Pcorrect/∂w=0, we get an equationΔμ2wTΣw=2ΣwwTΔμwhereΔμ=μ2−μ1. This equation has a general solutionw∝Σ−1Δμ. Substituting this back into accuracy, we get the optimal accuracy of the two boxes isΦΔμTΣ−1Δμ/2.

The relationship between optimal accuracy and the total Fisher information becomes intuitive if we assume the Fisher information is unbiased in all directions, which, biologically speaking, means that a rat has no directional bias in an open field. Under this assumption, the integral is trivial because the function inside is independent of direction. Second, the Fisher information becomes proportional to the identity matrixTrII. Without loss of generality, letδx=δl1, 0T. The optimal accuracy becomes (Taylor expanded around zero)0.5+δlϕ0TrIwhereϕ0=1/2π. Therefore, optimal accuracy asymptotically increases with the square root of the total Fisher information.

We used Monte Carlo method to estimate the integral in Eq. (27). Specifically, we sampled 2D vectors from a 2-dimensional standard Gaussian distribution, then rescaled the 2D vectors to have a length equal toδl, resulting the sampledδx. This sampling is unbiased with respect to angle because the 2-dimensional standard Gaussian distribution is isometric. Given Fisher informationI, the upper bound was estimated as the average ofΦδxTIδxacross allδx.

We tested upper bounds in both 1D and 2D synthetic data sets. For each parameter configuration (number of neuronsN, number of data pointsK, and noise levelα; the other parameters were fixed as described in Methods: One-Dimensional and Two-Dimensional Synthetic Datasets), we generatedKdata points. SCA was computed as described in ‘Methods: Spatial classification accuracy’. On the other side, the generatedKdata points were also used for fitting GKR. Fitted GKR make predictions of Fisher information, which then converted to upper bound (Eq.27). Finally, the ground truth Fisher information of the synthetic datasets was also used to compute the upper bound. Results are shown in Supplementary Fig.9.

We also inspected the upper bounds on the Grid cell datasets. GKR provides predictions of Fisher information, which were then used to compute the upper bounds. The upper bounds and SCAs of the R1M2 were shown in Supplementary Fig.8Cand Fig.5CforDsandDs6(projection to six PCs, see Methods), respectively. Upper bound-speed/SCA-speed array has50×8=400data points (fifty samplingDs/Ds6times 8 speed bins). To have a quantitative comparison between upper bounds and SCA, we computed the Pearson correlations between the two arrays, denoted asr. Thep-value (two-sided) and confidence interval (via Fisher z-transform) can be computed accordingly via Python package stats.peasonr85,86.

We present a simple two-neuron system to illustrate that the neuronal noise correlation can have either beneficial or detrimental effects, depending on the geometry of the noise covariance.

whereΔμ=μ2−μ1represents the vector difference between the class-conditional firing means. The baseline Fisher information, when there is no correlation (ρ=0), is 2. Positive correlation (ρ>0) increases the Fisher information, hence it is information-beneficial; while negative correlation (ρ<0) reduces the Fisher information, hence it is information detrimental. In general, negative correlation is not necessarily detrimental to information—it depends on the geometric relationship between the noise covariance and the signal. In this simple example, positive noise “reshapes” the noise covariance so that more noise aligns with the classification decision boundary, whereas negative noise “reshapes” it such that more noise is orthogonal to the decision boundary (Supplementary Fig.13).

To show these effects in simulation, we generated 1000 total samples (500 per class) under three different noise conditions (see Supplementary Fig.13): (1) beneficial correlationρ=0.8>0: noise aligns with the decision boundary; (2) detrimental correlationρ=−0.8<0: noise projects onto the signal axisΔμ; and (3) independent noiseρ=0: no correlation between neurons. Note that the total noises (trace of the covariance matrix) of three conditions are the same. With these generated data, we trained a logistic regression classifier to distinguish two classes for each condition. The classification accuracy quantifies the “goodness of information coding”. We also indicate the theoretical Fisher information value in Supplementary Fig.13.

We investigated the effect of grid cell activity correlation by comparing results from the original datasetDsto those from hypothetical independent firing grid cells (IFGC). A classic method for generating independent firing cells involves shuffling trials within the same condition. Specifically, each cell’s firing profiles are randomly permuted across trials within a condition. This approach preserves single-cell firing statistics while disrupting cell-to-cell firing correlation. We adapted this method when computing SCA of the IFGC (Fig.6E). Recall that the key idea of SCA is to compute the classification performance on data within two nearby (spatial) boxes. We treat data within each box as a single condition, where each data point (an N-dimensional vector of single-cell firing rates) represents one trial. We then randomly permute each cell’s firing rate across all data points within the box, breaking the cell-to-cell correlation. The SCA of this “trial-shuffled” data is called the SCA of IFGC (Fig.6E).

We also adapted this “trial-shuffling” idea to compute the geometric metrics (total noise, projected noise, and Fisher information) for IFGC. Specifically, after fittingDs, GKR can predict mean and covariance at a conditionx. Consider GKR as a generative model, it generates infinite data points under the same conditionx. If we applied the above “trial-shuffling” procedure on these data points, and recompute the mean and covariance matrix, the mean remains unchanged, while the covariance matrix retains only the diagonal components of the original covariance matrix, with all off-diagonal components set to zero. Therefore, IFGC’s GKR is same as the original GKR except only having the diagonal covariance matrix. With IFGC’s GKR, the geometric metrics can be computed as previously described.

We compared speed-averaged metrics obtained from the original datasets to those obtained from IFGC. Methods of computing speed-averaged metrics along with statistical analysis can be found in the Methods: Bayesian linear ensemble averaging and statistical testing section.

Further information on research design is available in theNature Portfolio Reporting Summarylinked to this article.

The authors gratefully acknowledge Robert Wang for reviewing the manuscript and providing insightful comments; Dr. Richard J. Gardner for clarifying details regarding the publicly available grid cell spiking dataset35; and Dr. Haoran Li for testing our computer code. This work was supported by grants from Incubator for Transdisciplinary Futures: Toward a Synergy Between Artificial Intelligence and Neuroscience (RW).