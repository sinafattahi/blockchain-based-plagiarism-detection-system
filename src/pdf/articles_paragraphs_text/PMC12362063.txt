The diagnosis and prognosis of Prostate cancer (PCa) have undergone a significant transformation with the advent of prostate-specific membrane antigen (PSMA)-targeted positron emission tomography (PET) imaging. PSMA-PET imaging has demonstrated superior performance compared to conventional imaging methods by detecting PCa, its biochemical recurrence, and sites of metastasis with higher sensitivity and specificity. That transformation now intersects with rapid advances in artificial intelligence (AI) – including the emergence of generative AI. However, there are unique clinical challenges associated with PSMA-PET imaging that still need to be addressed to ensure its continued widespread integration into clinical care and research trials. Some of those challenges are the very wide dynamic range of lesion uptake, benign uptake in organs that may be adjacent to sites of disease, insufficient large datasets for training AI models, as well as artifacts in the images. Generative AI models, e.g., generative adversarial networks, variational autoencoders, diffusion models, and large language models have played crucial roles in overcoming many such challenges across various imaging modalities, including PET, computed tomography, magnetic resonance imaging, ultrasound, etc. In this review article, we delve into the potential role of generative AI in enhancing the robustness and widespread utilization of PSMA-PET imaging and image analysis, drawing insights from existing literature while also exploring current limitations and future directions in this domain.

Generative AI techniques show promise in addressing clinical challenges in PSMA-PET imaging for prostate cancer, enhancing image quality, automating lesion detection and segmentation, and aiding radiologists in improving diagnosis and prognosis.

Prostate-specific membrane antigen (PSMA)-targeted positron emission tomography (PET) imaging significantly advances prostate cancer detection and management but faces numerous clinical challenges such as dynamic lesion uptake ranges, benign uptake issues, limited datasets, and image artifacts.

Generative artificial intelligence (AI) models have already demonstrated their value in performing tasks like image enhancement, lesion detection, segmentation, and data augmentation in medical imaging.

The application of generative AI in PSMA-PET imaging is still in its infancy but shows potential to address its clinical challenges by generating synthetic training images, improving image quality, automating lesion analysis, and mitigating artifacts.

The introduction of small-molecule positron emission tomography (PET) radiotracers targeted against prostate-specific membrane antigen (PSMA) initiated a radical change in the way men with prostate cancer (PCa) are staged, re-staged, and treated [1], [2]. PSMA is a transmembrane glycoprotein that is expressed in approximately 95% of PCa tumors [3]. Pivotal clinical trials have demonstrated the utility of PSMA radioligands in evaluating men with newly diagnosed PCa with high- or very-high-risk disease [4] and men with evidence of biochemical recurrence (BCR) on the basis of a rising prostate-specific antigen (PSA) level [5]. PSMA-PET imaging also holds promise for other indications, including patient selection for PSMA-targeted radioligand therapy [6] and identification of individuals with oligometastatic disease who may benefit from metastasis-directed therapy [7]. As research progresses, new applications for PSMA-targeted PET are likely to emerge.

However, the integration of targeted radiotracers like PSMA-PET into clinical practice presents unique challenges, including a wide dynamic range in lesion uptake, variability in lesion morphology, heterogeneity in image quality, and the intricate interplay between PSMA expression levels, initiation of androgen-axis-targeted therapies, and determination of progression of disease. The intersection of targeted radiotracers and artificial intelligence (AI) has been proposed as a potential solution, leveraging AI algorithms to enhance imaging quality, automate lesion detection, segmentation, and characterization, and facilitate PCa staging, restaging, and metastasis detection [8], [9], [10], [11]. Nonetheless, conventional AI approaches are constrained by their dependency on large, labeled datasets and predefined features, limiting their ability to address the challenges associated with PSMA-PET.

Generative AI offers a transformative alternative by overcoming several limitations inherent in conventional AI methods. Generative AI models, such as generative adversarial networks (GAN), variational autoencoders (VAEs), diffusion models, and large language models (LLM) possess the capability to generate synthetic data that closely resemble real-world objects. That ability is especially pertinent in PSMA-PET imaging, where labeled datasets may be scarce and difficult to generate, and variability among patient populations can pose challenges. In this manuscript, we will delve into the application of generative AI in diagnostic imaging and discuss its potential applications in PSMA-PET imaging, focusing on the ability to identify, automatically segment, and characterize individual lesions, as well as provide comprehensive tumor burden assessment and prognostication.

One of the prime clinical challenges associated with PSMA-PET imaging for PCa is the wide dynamic range of PSMA uptakes in lesions. PSMA expressions can vary among different lesions and within the same lesion. This variability can lead to limitations in accurately detecting and characterizing lesions. In advanced, metastatic castration-resistant prostate cancer (mCRPC), some metastatic lesions may have low PSMA expression. Additionally, in some cases with early-stage or low-volume disease, lesions may be missed, leading tofalse-negativeresults [12]. Overall, true-positive lesions can vary in their extent of uptake from barely perceptible above background [13] to having maximum standardized uptake values (SUVmax) greater than 100 (i.e. approximately 4 orders of magnitude of radioactivity concentration) [14] (Figure 1). As such, typical methods for whole-body tumor segmentation will tend to be either overly specific for high-uptake lesions and will miss subtle findings – or will be overly sensitive for subtle sites of uptake and will include regions of confluent noise or non-specific uptake.

Demonstration of the wide dynamic range of PSMA PET uptake in two men with anatomically similar findings. (A) Fused PSMA PET/CT image from an 81-year-old man presenting for initial staging with Gleason 4+5 = 9 grade group 5 prostate cancer with serum PSA 3.6. (B) Fused PSMA PET/CT from a 73-year-old man who was found to have a serum PSA of 62.1 and then underwent a prostate biopsy that demonstrated Gleason 5+4 = 9, grade group 5 prostate cancer. Note the significant differences in uptake in metastatic left peri-aortic lymph nodes (arrows), especially in comparison to other areas of normal uptake on the images. PSMA, prostate-specific membrane antigen; PET, positron emission tomography; CT, computed tomography.

Distinguishing between benign and malignant lesions solely based on PSMA uptake can be challenging as well, especially in regions with physiological PSMA expression, such as the urinary bladder and kidneys. This can result infalse-positivefindings and unnecessary interventions or treatments [12]. In the context of existing phase II prospective data on the deleterious effects of leaving any true-positive lesions untreated, the definitive characterization of potential false-positive lesions is of paramount importance to avoid unnecessarily aggressive treatment plans [7]. However, definitive characterization often belies the radiologist’s interpretation as some types of lesions may be uncommonly true-positive [15].

While PSMA PET imaging is highly sensitive relative to conventional imaging in detecting BCR (rising PSA levels) after primary treatment, its performance may decrease at very low PSA levels [5]. This can limit its effectiveness in detecting recurrent disease in patients with minimal PSA elevation, potentially leading tofalse-negativeresults, as everything from the reconstruction algorithm to the imaging time can affect lesion detectability [16], [17].

Different PSMA-targeted PET tracers, i.e.,68Ga- and18F-labeled tracers, introduce further challenges due to their distinct physiological uptake patterns and biodistributions. For instance, some of the recently introduced18F-labeled radiotracers can exhibit higher liver uptake, which may impact lesion detectability in mCRPC patients and also tend to lead to higher rates of non-specific uptake in non-cancer bone lesions [18]. Similarly, the available68Ga-labeled tracers demonstrate higher excretion into the urinary bladder, which can complicate uptake interpretation during primary staging or BCR [19], [20] . Those differences also result in varying distributions of false-positive lesions, such as in the skeleton, due to differences in radiotracer kinetics and off-target binding potential [21]. Consequently, AI models trained on data from one type of PSMA-targeted tracer may not generalize well to another tracer data without further adaptation. However, we should also bear in mind that many aspects of radiotracer distribution may still have commonalities (such as the locations of normal organs), potentially still allowing for significant overlap of data from different radiotracers and the effective use of transfer learning in this context [22].

Limited access to large-scale datasets containing annotated PSMA-PET images compared to other modalities makes it challenging to train robust deep-learning models for image analysis and decision-making. Moreover, PCa is a heterogeneous disease, exhibiting variability in tumor characteristics, morphology, and biological behavior [23]. Further, in current academic radiology, leadership decisions are often driven by productivity measures such as relative-value units (RVU), which quantify the value of medical services based on factors like time, expertise, and resources. These measures prioritize clinical productivity over tasks like detailed image annotation. As a result, sub-specialty trained radiologists and nuclear medicine physicians frequently face significant time constraints, creating an acute need for innovative solutions [24].

PSMA-PET images may not provide detailed-enough anatomic information of the patient, which is essential for accurate diagnosis and lesion localization [23], [25]. That dependency on additional imaging modalities such as diagnostic computed tomography (CT) or magnetic resonance imaging (MRI) presents significant challenges in accurately assessing and localizing abnormalities.

Noise and motion artifacts from different sources can affect PSMA-PET imaging quality, impacting the accuracy and reliability of diagnostic interpretation. Noise, stemming from factors such as lower injected radiation dose, reduced scan time, and scatter or attenuation from tissue can degrade image clarity and reduce the signal-to-noise ratio [26]. Given that primary tumors, local recurrences, and pelvic nodal involvement will often be in the same plane as the urinary bladder, scatter over-correction artifacts may be particularly bothersome to the interpreting physician [27]. Similarly, motion artifacts, caused by patient movements or physiological processes like respiratory or cardiac motion, can lead to blurring or misregistration of image structures [28].

Additionally, there is currently no consensus on the optimal threshold for segmenting metastatic lesions in PSMA-PET imaging. Commonly used approaches include relative thresholds like a percentage of SUVmax, fixed thresholds, or pre-defined reference organ thresholds, such as the liver or spleen [29]. The choice of threshold can significantly affect lesion delineation and, consequently, the reliability of AI-based lesion detection and characterization. Moreover, conventional semi-automatic or threshold-based methods for tumor burden evaluation in mCRPC patients under RLT therapy are time-consuming and prone to inaccuracies. For example, para-aortic lymph node metastases located near physiological bowel activity often result in false positives, requiring labor-intensive manual corrections. Further, even in the context of primary PCa, where cubic-millimeter-level volumetric validation of tumors is possible, there seems to be significant interpatient variability in the most accurate segmentation method. That raises significant concerns that almost no segmentation method will have the widespread applicability to adequately delineate metastatic disease volumetrically. Instead, we suggest that the focus should be on the test-retest repeatability of tumor segmentations in the context of there being no ground-state truth of tumor volume (Figure 2).

A demonstration of the extent of metastatic prostate cancer that would preclude reasonable manual tumor segmentation. (A) Maximum intensity project image and (B) sagittal fused PSMA PET/CT images from a 63-year-old man with widespread metastatic prostate cancer. The accurate delineation of the extent of prostate cancer would be very time consuming for a human observer and there would be significant inter- and intra-reader variability in the assigned segmentations. Properly validated generative AI methods might improve both the accuracy and the repeatability of segmentations in this context. PSMA, prostate-specific membrane antigen; PET, positron emission tomography; CT, computed tomography; AI, artificial intelligence.

Efforts to harmonize PSMA-PET reporting standards have resulted in frameworks such as PROMISE (Prostate Cancer Molecular Imaging Standardized Evaluation), PSMA-RADS (PSMA Reporting and Data System), and E-PSMA (European PSMA Guidelines), which aim to ensure consistency and reliability in PSMA-PET interpretation [30]. These frameworks are particularly critical as PSMA-PET imaging is increasingly integrated into clinical trials, where validated methods are essential for assessing treatment responses and ensuring reproducibility. However, there can be significant trade-offs in terms of clinical applicability and complexity as shown by the E-PSMA effort to harmonize various approaches into a universally accepted system. One way to ensure continued clinical applicability while also allowing for nuance of clinical interpretation is to train models to provide decisions that are not always just “positive” or “negative”, but can also be “indeterminate”. There has been some progress made with traditional AI methods to incorporate that idea [31], although a more thorough incorporation of generative AI would likely lead to significant improvements in the reliability of such methods.

Lastly, the inherent complexity of PSMA-PET imagingin combination with poorly understood correlations between findings, impacts on patient management, and long-term patient outcomes – has led to the proposal of multiple different systematic approaches for lesion characterization [32], anatomic delineation of disease extent [33], confidence in findings of primary disease in the pre-cancer-diagnosis setting [34], and response to therapy [35]. Traditional AI has already been leveraged into a U.S. Food and Drug Administration-cleared product that can identify sites of abnormal radiotracer uptake – however, lesion characterization has proven more difficult, as will be discussed later.

Addressing all the challenges described is essential for ensuring high-quality, AI-driven PSMA-PET image analysis and accurate clinical assessments of the extent of PCa.

Generative AI encompasses a variety of sophisticated models designed to create new data instances that resemble those in the training dataset. Among those models, GAN stands out as a widely recognized and powerful approach [36]. The architecture of a standard GAN comprises two key components: a generator and a discriminator. Both are typically implemented as deep neural networks, with the generator tasked with producing realistic samples and the discriminator learning to distinguish between genuine and generated data. This framework operates in a competitive manner, with the generator striving to improve its output while the discriminator seeks to become more discerning. Over time, this dynamic results in the generation of increasingly convincing data distributions. There are numerous variants of GANs depending on their architecture and the loss functions they utilize, such as deep convolutional GAN (DCGAN), conditional GAN (CGAN), cycle-consistent GAN (CycleGAN), auxiliary classifier GAN (ACGAN), etc. Various aspects of notable GAN variants in the context of medical imaging are summarized inTable 1.

Another major generative AI model is a variant of the traditional autoencoder network, known as VAE [37]. An autoencoder consists of an encoder network, followed by a decoder network, both typically implemented through deep neural network technology. The encoder network learns the latent representation of the input data through some lower-dimensional latent variables. The decoder expands upon the latent representation to generate the same or similar data as its output. In a VAE, the encoder learns the probability distributions of the latent variables and thus captures the nature and uncertainty of the input data in a stochastic way. The generative aspect lies in the decoder taking samples from the learned distributions and generating novel and realistic data instances.

Diffusion models have emerged as another significant class of generative AI models, characterized by a two-step process involving forward and reverse transformations [38], [39]. In the forward diffusion process, Gaussian random noise is incrementally added over several steps, gradually degrading the data until it becomes indistinguishable from random noise. During the reverse process, a neural network, often referred to as the backbone or denoiser, is trained to predict the noise added during the forward process, given the noisy image and the noise level. This approach can indirectly generate images through an iterative process of feeding a noisy input and subtracting off the noise prediction output. Repeated for the same number of steps as the forward process, this allows the trained backbone model to generate novel, realistic data by denoising samples drawn from a Gaussian distribution.

In recent years, there has been a surge in the adoption of LLMs, primarily for tasks involving text generation and natural language processing. Those models are based on transformers, a deep learning architecture consisting of encoders and decoders [40]. Within the encoder module, each input word/token is transformed into a contextualized high-dimensional representation, capturing the semantic and syntactic information present in the input sequence through a ‘self-attention’ mechanism. Based on the encoder’s representations and preceding output tokens, the decoder module generates the output sequence such as translated sentence or next word predictions. LLMs leverage these transformers by pre-training them on extensive corpora of text data, followed by fine-tuning them for specific applications [41].

GAN variants have been extensively utilized to generate synthetic images that closely mimic real patient data, thereby facilitating more robust AI model training. This is particularly beneficial when dealing with rare diseases, where the number of available training cases may never allow for adequate use of traditional AI, or in the context of limited patient data from retrospective studies. For instance, DCGAN was utilized to generate artificial brain-MRI images that even experienced neuroradiologists struggled to distinguish from authentic scans [42]. Similarly, DCGAN found application in PET imaging by synthesizing brain images for different stages of Alzheimer’s disease [43]. Additionally, a variant of CycleGAN was employed to generate multi-modality MRI images to improve brain lesion segmentation accuracy [44]. Chest X-ray data augmentation using ACGAN raised Covid-19 detection accuracy from 85% to 95% [45].

Medical image quality can frequently be compromised by noise and artifacts stemming from various factors, including low radiation dosage, limited acquisition time, or patient motion during image acquisition. In such instances, generative AI, particularly GAN-based approaches, can rectify noise and add missing information, thus enabling safer and faster image acquisition [46]. A 3-dimensional (3D) CGAN network was employed to generate high-quality full-dose PET images of the human brain from their noisy low-dose counterparts [47], while CycleGAN was utilized for achieving similar outcome with full-body PET images [48]. Low-dose PET image denoising by GAN proved superior to traditional and deep learning-based denoising methods in regard to SUVmeanand SUVmaxbias for lesions and normal tissues [49]. GANs have also found various other applications in medical imaging, including cross-modality image synthesis, attenuation and scatter correction, image fusion, image registration, super-resolution image generation and anomaly detection [50], [51], [52].

In addition to GANs, VAEs have found substantial utility in medical image analysis. The synthetic image generation capability of VAEs has been utilized to enhance deep AI model training for tumor identification and its impact analysis from MR images [53]. Data augmentation using the generative aspects of VAE proved crucial in improving the accuracies of classification and segmentation of medical image data across various modalities including CT, MR, X-ray and ultrasound [54]. Furthermore, VAEs were extensively utilized forunsupervisedanomaly detection in medical images, through learning the latent representation of healthy images in a probabilistic way, followed by evaluating the reconstruction loss, which was typically higher for images depicting abnormal conditions [55], [56]. The capabilities of VAE and GAN were combined by implementing a VAE-GAN architecture, where a VAE network was used to learn the inherent representation of a limited number of chest X-ray image data and this representation was used by the generator network of a GAN to generate novel synthetic data instances [57]. By combining the probabilistic encoding capabilities of VAEs with the adversarial training strategy of GANs, VAE-GANs offer a powerful framework for generating high-fidelity medical images while preserving the underlying data distribution characteristics.

Diffusion models have recently demonstrated significant potential in various medical imaging tasks. They were employed to augment high-resolution 3D brain MRI image dataset for training neural networks effectively [58]. A denoising diffusion model with transformer-based network in the reverse generative process was proposed and evaluated to synthesize 2D medical images of various modalities, including X-ray, MRI and CT [59]. Besides data augmentation, diffusion models found application in medical image segmentation through generating denoised segmentation mask conditioned on the features of the input image [60], [61]. Furthermore, diffusion models were used in anomaly detection by generating anomaly maps through the translation of patient images to healthy images and then comparing the two [62]. Similar to GANs, diffusion models were employed in many other medical image analysis tasks, such as image reconstruction, registration, classification and enhancement [63].

Finally, LLMs have demonstrated remarkable adaptability and utility in analyzing and interpreting medical data. LLMs possess pre-trained knowledge of vast textual data that can be fine-tuned with medical and radiology literature to streamline, comprehend, and summarize radiology reports [64]. In addition to text-based inputs, LLMs can also accommodate medical images by encoding them into a format suitable for processing. The latest advancements in LLM technology have given rise to multimodal LLMs (M-LLMs), capable of handling and generating data across multiple modalities including text, image, audio, and video. These M-LLMs have been successfully deployed to generate descriptive captions and impressions from medical images [65]. Moreover, text-to-image transformers, often built on pre-trained LLM, have emerged as a significant innovation within this framework, demonstrating their ability to generate synthetic medical images from textual descriptions. These transformers have been effectively utilized to create synthetic chest X-ray [66] and brain MRI [67] images, which show their potential to enhance diagnostic processes and augment training datasets.

Although a great deal of work has been done to understand the pitfalls and limitations of PSMA-PET, and the field of generative AI is rapidly making inroads into solving near-impossible problems in image interpretation, to date the direct application of generative AI to PSMA-PET has been limited. However, some of the intrinsic difficulties with PSMA-PET are potentially addressable by generative AI in a manner that may not be possible with traditional AI. Of note, we should remember that generative AI algorithms do not “understand” the presence of the physical world – however, they are still able to provide conclusions that have applicability in the physical world.

The utilization of generative AI for the conditional and unconditional synthesis of data holds promise in addressing the challenges of PSMA-PET imaging associated with wide dynamic range of lesion uptake, benign and/or physiological uptake, BCR with very low PSA level, and cross-tracer variability. The synthetic images can incorporate the heterogeneity of PSMA uptake within the prostate gland and surrounding organs in the training data, thereby facilitating robust AI model training that minimizes false-positive and false-negative outcomes. This approach not only augments data in scenarios where images from specific tracers are scarce but also eliminates the need for additional image acquisition and manual annotation, offering time and cost savings. Additionally, generative AI models, such as GANs and VAEs, can perform domain translation between tracers, like68Ga and18F, enabling cross-tracer generalization without extensive retraining. These models can also adapt AI systems to extract invariant features across tracers, such as lesion morphology and PSMA expression, which leads to better accuracy and generalization. Furthermore, generative AI can also synthesize complementary imaging modalities, like CT, from PET data which can provide more comprehensive insights for lesion detection [68]. These advancements can lead to improved diagnostic precision, more effective lesion localization in PSMA-PET imaging, and a more integrated approach to multimodal imaging.

Similar to other imaging modalities, GAN holds the place of being the most utilized generative AI model in PSMA-PET imaging analysis. PSMA-PET image acquisition suffers from attenuation from epithelial tissue that can be corrected using corresponding CT images as an attenuation correction map. However, in PET/MRI systems, the MR images cannot serve as attenuation coefficient maps by themselves. PET/MRI has broad applicability in PCa as a “one-stop-shop” for initial staging and BCR. GAN was successfully utilized to generate the µ-map for attenuation and scatter correction in68Ga-PSMA-PET/MRI prostate imaging, which improved image quality and quantitative accuracy [69].

The lack of clarity in PSMA-PET images resulting from shorter scan duration can be compensated by GAN variants through utilizing their conditional synthesis capability. CycleGAN, for instance, has been utilized to predict standard scan18F-FDG and68Ga-PSMA-PET images using only 1/8 and 1/16 short-duration scans [70]. Combining such advances from GAN with the move towards long-z-axis field-of-view scanners could allow for transformative changes in decreasing the amount of injected radioactivity that is required and in reducing the scan time. Diffusion models often outperform GANs and VAEs by offering more stable training, avoiding mode collapse associated with adversarial training, and producing higher-quality image data. Their ability to capture complex data distributions and refine details progressively can make them a robust choice for PSMA-PET image analysis, especially since they have already been utilized in image reconstruction and enhancement [71], [72] as well as denoising [73] of traditional PET images.

Traditional deep learning methods have proven effective at identifying sites of PSMA uptake, delineating their anatomic locations, and providing information such as whole-body tumor burden and staging [8]. However, individual lesion characterization remains challenging, partly because of the rarity with which some types of lesions will arise and partly because of a lack of annotated data with pathologic confirmation. Important aspects of natural language processing, such as interpreting radiologists’ descriptions, could be leveraged for enhanced lesion characterization [31]. For example, solitary rib lesions have a small, but measurable, rate of being true positive metastases in patients presenting for initial staging or recurrent disease – and radiologists and nuclear medicine physicians may vary significantly in their interpretive approach [15]. A generative AI model that incorporates the language used to describe solitary rib lesions could serve as a decision aid for less experienced interpreters. Moreover, embedding statistical properties from radiology reports into text-to-image transformer models as conditionals during the generative AI training process could further improve the characterization of lesions in scans that are not included in the training images, thereby enriching the model’s ability to generate contextually relevant and accurate diagnostic insights.

Generative AI models can suffer from a few technical and clinical limitations when applied to PSMA-PET imaging. These models are highly data-intensive, as they require high-quality, unbiased, and sufficiently large training datasets to generalize effectively. Their effectiveness is contingent upon the quality and diversity of the training dataset, as they can only synthesize data within the statistical boundaries of the true data distribution represented by the training images. As discussed before, PSMA-PET images contain heterogeneity in tumor type, volume and uptake that may not be captured rigorously in the dataset used for training generative AI models. This limitation can lead to suboptimal model performance and reduced reliability in handling rare or complex cases. One example might be a rare tumor type that has PSMA radiotracer uptake but is present in a distribution that is incompatible with PCa; even a very large dataset would be unlikely to contain enough such examples to adequately train the model. Additionally, the ethical challenge of ensuring fairness and generalizability arises when training datasets lack adequate demographic representation, potentially leading to biased outcomes.

Further, the generation of large datasets might best be undertaken through shared repositories that include both the scan data and any available clinical data. While the inclusion of clinical data may or may not improve human interpretations of PSMA PET scan [74], it is likely that such data could be effectively leveraged for more accurate prognostic and predictive biomarker development through federated learning or ensemble approaches [75]. However, the more clinical data required by any repository, the more the treating institution is likely to place significant barriers to data sharing due to concerns regarding patient privacy. The policies for anonymizing data and methods for how the data will be stored must therefore be rock solid and incontrovertibly secure.

In the case of GANs, adversarial training can sometimes lead to mode collapse or mode-hopping, where the model generates a limited variety of outputs, reducing the diversity of synthetic images [76]. This is a critical issue for PSMA-PET imaging, where variability in lesion morphology and tracer uptake is essential for robust AI training. Additionally, deep generative models like GANs and VAEs are susceptible to vanishing or exploding gradient problems, which can result in slow learning or failure to converge during the training process [50].

Training and inference processes of generative AI models require substantial computational resources, especially for high-resolution 3D PSMA-PET images. This creates challenges for integrating these models into clinical workflows, particularly in resource-limited settings [76], [77]. The “black-box” nature of generative AI further raises concerns about their reliability and interpretability in clinical practice. Clinicians may hesitate to adopt such models without clear explanations of how synthesized images or features are derived [76]. Regulatory approval for the use of generative models in clinical practice also requires extensive validation, especially when synthetic images are used for diagnostic or therapeutic purposes [78].

A significant limitation of LLMs is their lack of contextual understanding in medical imaging. LLMs, trained primarily on textual data, do not fully grasp the nuances of PSMA-PET images and their clinical context which may lead to potential misinterpretations and lack of explainability of the data. This can be especially problematic when LLMs are used to generate or assist in interpreting synthetic images, where a deep understanding of the image’s clinical relevance is critical.

Addressing the limitations of generative AI in PSMA-PET imaging requires strategic advancements in model development and application. One promising approach is the use of transfer learning, which allows pre-trained generative models to be adapted to the specific requirements of PSMA-PET imaging [79]. By leveraging knowledge from existing datasets, these models can be fine-tuned to achieve more precise characterization of individual lesions, thereby supporting personalized treatment decisions.

To address training instabilities, commonly associated with GAN and VAE, stabilization techniques such as spectral normalization or the application of Wasserstein loss have shown promise [68], [82]. These methods can help reduce issues like mode collapse and improve the diversity of synthetic images, as demonstrated in advanced GAN variants outlined inTable 1. Complementing these efforts, the development of hybrid models that combine the strengths of different generative AI architectures, such as integrating GANs with VAEs or incorporating attention mechanisms, can enhance model performance by focusing on clinically critical regions of interest within the images [83].

Finally, integrating synthetic data into radiomics pipelines could significantly enhance feature extraction and downstream clinical decision-making. By incorporating synthetic images into radiomic analysis workflows, researchers can evaluate the impact of generative AI on the robustness of feature extraction, crucial for PSMA-PET imaging. This approach can help improve the accuracy and repeatability of radiomic signatures used for early diagnosis, prognosis, and treatment response assessment, ultimately contributing to better patient outcomes. Of course, given the limitations in PET evaluation of high-frequency imaging features, the need for carefully chosen radiomic features that are centered around low-frequency, whole-tumor features is emphasized [84]. Collectively, these research directions aim to address the technical, computational, and clinical challenges of generative AI in PSMA-PET imaging to ensure its safe and effective adoption in clinical workflows.

In conclusion, the integration of generative AI techniques with PSMA-PET imaging holds tremendous potential to address the unique challenges associated with PCa diagnosis, staging, and treatment. While traditional AI methods have made significant strides in lesion detection and anatomic delineation, they often fall short in characterizing individual lesion and addressing the variability inherent in PSMA-PET images. Generative AI models, such as GANs, VAEs, diffusion models, and LLMs, can offer innovative solutions to these challenges by generating synthetic data, enhancing image quality, automating lesion detection, segmentation, and characterization, and augmenting radiologists’ interpretations.

Despite its promise, the application of generative AI in PSMA-PET imaging is still in its infancy, and several technical, clinical, and ethical challenges remain. Issues such as the need for large, unbiased datasets, computational resource demands, training instabilities, and the “black-box” nature of generative models must be addressed to facilitate clinical adoption. Moreover, ensuring fairness and generalizability across diverse patient populations and achieving regulatory approval are critical for translating these technologies into practice.

Looking ahead, future research directions at the intersection of generative AI and PSMA-PET imaging include the development of robust deep learning models with transfer learning capabilities, the creation of diverse, well-annotated datasets for training and validation through federated learning, the integration of multi-modal imaging for comprehensive lesion characterization, and the translation of generative AI-driven insights into clinical practice to improve patient outcomes. Additionally, regulatory frameworks must evolve to provide guidelines for validating and deploying generative AI models in clinical settings. In summary, the synergistic application of generative AI with PSMA-PET imaging represents a promising avenue for advancing precision medicine in management of PCa, paving the way for more accurate diagnosis, personalized treatment strategies, and improved patient care. Continued research and collaboration in this field will be crucial for realizing the full potential of generative AI in revolutionizing imaging and therapeutics of PCa.