Electroencephalography (EEG) recordings with visual stimuli require detailed coding to determine the periods of participant’s attention. Here we propose to use a supervised machine learning model and off-the-shelf video cameras only. We extract computer vision-based features such as head pose, gaze, and face landmarks from the video of the participant, and train the machine learning model (multi-layer perceptron) on an initial dataset, then adapt it with a small subset of data from a new participant. Using a sample size of 23 autistic children with and without co-occurring ADHD (attention-deficit/hyperactivity disorder) aged 49–95 months, and training on additional 2560 labeled frames (equivalent to 85.3 s of the video) of a new participant, the median area under the receiver operating characteristic curve for inattention detection was 0.989 (IQR 0.984–0.993) and the median inter-rater reliability (Cohen’s kappa) with a trained human annotator was 0.888. Agreement with human annotations for nine participants was in the 0.616–0.944 range. Our results demonstrate the feasibility of automatic tools to detect inattention during EEG recordings, and its potential to reduce the subjectivity and time burden of human attention coding. The tool for model adaptation and visualization of the computer vision features is made publicly available to the research community.

The online version contains supplementary material available at 10.1038/s41598-025-10511-2.

Electroencephalography (EEG) is a widely used method for studying brain-behavior relations. A typical EEG recording session includes visual and/or auditory tasks, which can be presented in an event-related potential (ERP) paradigm or during spontaneous EEG recording. Collecting data using visual tasks in children is significantly more challenging due to their reduced ability to sustain their attention to visual stimuli1,2. The ability to sustain attention during EEG tasks can be especially challenging for children with neurodevelopmental disorders, such as autism and ADHD (attention-deficit/hyperactivity disorder)3,4. A meta-analysis by Stets et al. (2012)5reports that studies involving visual tasks in infants have significantly higher attrition rates than auditory or combined visual and auditory tasks. While reports of attrition rates in different studies vary1,5,6, a general recommendation is to design tasks that will be engaging for children, thereby facilitating the maintenance of visual attention6. To facilitate visual attention children may be asked to provide a behavioral response (e.g., press a button)7,8or an experimenter may gently redirect a child to the screen when noticing signs of disengagement3,8,9.

Removing segments of the data during which a participant did not look at the screen is often the first stage of EEG data processing in recordings with visual stimuli. Typically, researchers either code the participant’s attention on-line by pressing a button which sends a marker to the EEG recording when the participant was not attending to the stimulus7,10,11or by recording the video of the participant’s behavior synchronously with the EEG recording and marking periods of inattention post-hoc upon reviewing the video offline3,12. This is a burdensome manual process requiring significant time and effort. It is also highly subjective; for example, the annotator might only see the participant’s face and must guess whether the participant’s gaze is directed to the area inside or outside of the screen. That is, the target of the child’s gaze is inferred based on the angle of the eyes because the recording camera is adjacent to the target screen, so the target screen is not actually captured in the video frame. For this reason, we expect human annotators coding inattention using EEG videos to vary more and have less reliability with one another than traditional video annotating. For example, a previous non-EEG study measuring inattention in young children and infants required human raters was able to obtain at least 90% frame-by-frame agreement before allowing them to work on the videos in the dataset, and their data had a range of agreement from 89.57 to 98.51% across multiple datasets13.

Subjectivity during this first stage of data processing poses an obstacle for EEG studies, in particular for multi-center ones, since reproducibility and constancy of EEG data quality in multi-center studies are critical14,15.

In addition to its value for data curation, information about inattention periods can be useful for creating clinical biomarkers. There is evidence of alterations in orienting, disengagement from, and sustaining attention to relevant stimuli in autistic children16–19, which undoubtedly influences the amount of inattention during the EEG study. Though a typical EEG study excludes from analysis time periods where the participant is not engaged with the visual stimulus11,20,21inattentiveness during EEG in social/nonsocial stimuli can be a measure that distinguishes autistic and neurotypical children, used alone or in conjunction with EEG power features3.

Inattention detection has been studied not just within the field of medicine, but also in other contexts such as autonomous driving safety and assisted driving tools. Within the field of autonomous driving, inattention has been operationalized as “insufficient or no attention, to activities critical for safe driving.” Inattention can be divided into five subtypes: restricted attention (due to physical obstructions or blinks), misprioritized attention (attending to a less important feature instead of a potential safety concern), neglected attention (not checking the blind spot), cursory attention (looking in the right direction but failing to process the information), and diverted attention (distraction by driving-related or non-driving-related tasks and events)22,23. Computer vision can detect restricted attention, misprioritized attention, and diverted attention, but not cursory attention. Thus, more research is needed to accurately measure and predict all types of attention automatically.

Modern computer vision techniques now enable accurate, real-time inattention detection—often by analyzing facial expressions, eye gaze, and head pose, eschewing use of expensive additional sensors24. To date, conventional eye-tracking technologies has been most commonly used for detecting inattention. Simultaneously presenting a stimulus on the eye-tracker screen while recording both eye-tracking and EEG signals enables the detection of a participant’s visual attention directed towards the screen25. For example, a study by Maguire et al. (2014)26used an eye-tracker synchronized with EEG to present an “attention-getter” animation in an experiment with 6–8 year old children. They reported increased retention of EEG data compared to the condition where children were asked to provide a behavioral response (button pressing) to facilitate attention. However, eye-tracking equipment can be expensive and requires calibration. Advances in inattention detection solely based on computer vision technology have a promise of substantially reducing the cost and effort of data preprocessing in lab experiments.

Here we propose a solution for monitoring attention during EEG acquisition based on computer vision analysis (CVA), which is scalable and less expensive than eye-tracking equipment, requiring only off-the-shelf cameras to objectively measure children’s movement behavior. This is largely enabled by the progress in face detection and estimation of facial landmarks, head pose, and gaze27–30. In non-EEG settings, these tools have been able to detect head turns in response to name31, and capture patterns of gaze in a low-cost setting without additional calibration13,32. In the work of Qian et al. (2022)33, supervised machine learning in combination with CVA approaches were applied to detect blink and head movement artifacts detection in a minimally constrained portable EEG setting.

A similar tool to the present CVA methodology, iCatcher13is a publicly available supervised deep learning model trained to classify infants’ gaze into three categories (‘left’, ‘right’, and ‘away’) based on facial appearance. iCatcher offers a low-cost, automated alternative to traditional eye-tracking systems and is particularly useful for studies involving young children. iCatcher works best on short to moderately long videos with a single person in the frame. Clear visibility of the face is important, as obstructions like hats or coverings can affect accuracy. While development of toolboxes such as iCatcher have undoubtedly moved the field forward, there are some limitations that do not make it well-suited for videos recorded during EEG sessions. First, the EEG net/cap will obstruct part of the face, possibly below and above the eyes depending on what system is used. Secondly, when running EEG sessions with young children from neurodevelopmental populations, typically multiple research staff and/or parents are involved in helping the child sit still and complete the task, which would invalidate a tool like iCatcher which is optimized for videos with only one human face detectable.

In this work, we developed a combination of CVA and a supervised machine learning model to detect inattention periods during the EEG recordings. This is computed from the videos of the child’s head and upper body captured synchronously with EEG and with simple off-the-shelf cameras. While these videos were recorded during EEG sessions, the EEG data itself is not utilized in the present analysis. We hypothesized that automatic CVA codes of eye gaze coordinates, head pose descriptors (pitch, yaw, and roll), and nose landmarks could reliably detect periods of visual distraction from the screen using a supervised machine learning model. The proposed method requires minimal involvement by human annotators to fine-tune the model to a new participant. In this process, a small number of frames from the new participant’s video are labeled by a human, followed by an additional round of model training. Minor human involvement is critical since head poses and facial expressions of children vary significantly in clinical populations, justifying the need and opportunity for tuning the pre-trained model to new participants. Recent work based on iCatcher provides evidence that the lowest agreement between human annotators and automatic models occurs on the label ‘looking away from the screen13. Thus, we developed a graphical user interface (GUI) allowing users to label data for fine-tuning, visualize video and corresponding time series of CVA features, and post-process the model results. The post-processing stage gives an opportunity for additional quality control of inattention periods proposed by the model.

The proposed approach reduces subjectivity by providing the CVA features for human reference in the labeling process, thus standardizing the information an individual uses in their labeling. It also significantly reduces the coding time by decreasing the number of frames to be labeled manually. We therefore trained the model on an annotated dataset of children’s videos synchronized with EEG recordings, and then fine-tuned it to a new child by labeling a limited amount of randomly selected additional frames on the new video. To evaluate our approach, we trained and fine-tuned on a dataset of 23 videos in the leave-one-subject-out cross-validation setting, training on a dataset of 22 videos, and fine-tuning to a holdout video on each cross-validation iteration. Due to the length of the EEG sessions and the size of the videos created, a similar tool, iCatcher, was unable to handle the current data set. We have shared online the GUI for the video and CVA features inspection, model retraining, and predictions post-processing.

Participants were 23 children (16 males), ranging from 49 to 95 months of age who were part of a study funded by the National Institutes of Health (NICHD 2P50HD093074, Dawson, PI). The ethnic and racial composition of the sample was as follows: White, 17; Black, 0; Asian, 2; other and mixed race, 4; Hispanic, 4. All 23 children met DSM-5 criteria for autism spectrum disorder (ASD) based on the Autism Diagnostic Observation Schedule-2nd Edition34by an experienced, research reliable psychologist. Eleven of the 23 children were diagnosed with co-occurring attention deficit/hyperactivity disorder (ADHD) based on a comprehensive clinical evaluation by a clinical psychologist with expertise in ADHD. Children had a mean Full-Scale IQ of 78.5 (SD = 25.5) based on the GCA Standard Score derived from Differential Ability Scales Second Edition35.

All caregivers/legal guardians of participants gave written, informed consent and the study protocol was approved by the Duke University Health System Institutional Review Board (Protocol numbers Pro00085435 and Pro00085156). Informed consent was obtained from the subjects and/or their legal guardian(s) for publication of identifying information/images in an online open-access publication. Methods were carried out in accordance with institutional, State, and Federal guidelines and regulations. The procedures in these studies adhere to the tenants of the Declaration of Helsinki. Additionally, the caregiver of the participant whose video was used in the Supplementary Materials, as well as blurred in the Figures, provided consent to use the materials in publication. All other data presented have been anonymized.

Continuous EEG and event-related potentials (ERPs) were recorded as part of an EEG study while simultaneous video recording of the session was underway. EEG sessions were aborted early if the child could not comply with study procedures. Videos recorded during the EEG sessions were 00:04:15 to 00:31:09 in duration. One or two clinical research assistants were present in the room during the EEG recording to ensure the quality of the session and to gently redirect the participant’s attention back to the screen in case they were distracted. The child’s face was recorded from a Basler ACE acA1300-30uc camera below the screen synchronized with the EEG. The camera resolution was 1296 × 966 pixels and the frame rate was 30 fps. To synchronize the camera and EEG, an in-house software code was used, based on the Baslerpylonlibrary and Cedrus StimTracker hardware device used to set markers on the EEG recording. A diagram of the recording setup is shown in Fig.1.

Recording setup. Video from the camera is recorded on Video Recording Computer, which sends a marker to the EEG Recording Computer via Cedrus Stimtracker every 100 frames. This allows for synchronization between the EEG and video recordings.

To extract the CVA features, we used in-house code involving three steps: (a) face detection and disambiguation, (b) extraction of landmarks and head pose angles, and (c) gaze estimation. The raw set of extracted features per frame included nosex(horizontal) andy(vertical) coordinates in the frame, gazexandycoordinates in the presentation screen plane, and head pose angles (pitch, yaw, and roll).

Code for face detection and disambiguation used theface_recognitionpython library based on thedlibC + + library36. Every time the algorithm detected more than one face on the video (which happened either due to ambiguity of face detection – one face was detected twice, or when another person, e.g., research assistant(s) or parent(s) entered the frame), the algorithm showed the frame with a bounding box and prompted the user to select the correct participant’s face.

After the faces were detected, an algorithm for facial landmark extraction based on theintrafacesoftware library30was applied to the detected faces. As a result, facial landmark pixel coordinates, as well as pitch, yaw, and roll head pose angles were obtained.

Gaze estimation.The iTracker software28was used for gaze estimation, providing gazexand gazeycoordinates in the screen plane. Even though iTracker was trained to predict gaze coordinates on a mobile device screen using the mobile device frontal camera, we used the output of iTracker as a proxy for gaze coordinates in the presentation screen plane. The software package was modular and this component can be easily replaced by others as preferred by the user.

Since theintrafacelibrary was not currently available to the general public, for the convenience of potential users we make publicly available an alternative processing pipeline which consists of our original face estimation and disambiguation code, and a code for landmarks, head pose and gaze extraction using the popular OpenFace software package27.

Due to pauses between EEG/ERP recordings where the behavior of participants varied significantly, inattention detection was restricted only to the periods during the actual stimuli presentation, excluding breaks between stimuli, and the training set for the machine learning (ML) model included only data from frames inside those periods. Frames where the face could not be detected (hence there was no information on landmarks and head pose) were excluded from the analysis.

The final set of features for the analysis are reported in Table1.

List of input features per frame for the machine learning model.

After pre-processing the features, the participant identifier was one-hot encoded and added to the feature list. This allowed learning a separate bias term in the first layer of the trained neural network, resembling the design of mixed models. The number of categories for one-hot encoding was one more than the number of participants, with the assumption that the identifier of the participant whose data was used for model fine-tuning and prediction was encoded in the last category.

Data for all 23 participants was labeled by one of the co-authors using the Elan v. 6.3 software. Nine (39%) participants were selected for independent annotation by another co-author. Neither annotator participated in data analysis. Annotators labeled data using the recorded video as ‘gaze off screen’ if the participant looked away from the screen, and/or as ‘head turn’ if the participant turned their head. For the purpose of inattention detection, a frame was labeled as ‘inattention’ if it either was labeled as a head turn or gaze off screen. Annotators included eye blinks within periods of inattention and ignored eye-blinks when the participant was visually attending; eye blinks did not interrupt or break-up inattention events. Agreement on inattention labels between independent annotators was assessed with Cohen’s kappa37.

Model inputs were features extracted from each video frame. We utilized a multi-layer perceptron (MLP) model with an input size of 37 features, two hidden layers (layer dimensions 512 and 14 were selected empirically following information bottleneck principle)38, and a temperature scaling layer for model calibration39. –40The target variable for model training was each video frame’s inattention label, namelyinattention-presentorinattention-absent. The output was a one-hot encoding of binary inattention signal (dimension = 2). Cross-entropy loss was a cost function. Adam optimizer was used for model training41. We used weighted sampling for model training to allow each batch to have approximately equal amounts of positive and negative samples (inattention and attention respectively). Models were trained in thepytorchframework42. Evaluation was done using the leave-one-subject-out cross-validation (LOSO CV) method. To evaluate the model performance, we assessed average precision (AP, also known as area under precision-recall curve), area under the ROC curve (AUC), and maximal Cohen’s kappa (MK) between the human annotator and the machine learning predictions per participant across different thresholds. Additionally, we evaluated median Cohen’s kappa across the entire distribution at the range of thresholds between 0 and 1. This allowed us to assess the value of the threshold needed to achieve the best agreement between the model and the human coder over the entire distribution, without adjusting the threshold for each individual participant.

Predict probabilities of sample being positive in each frame.

If the approach is Random sampling, randomly sample N frames into the batch from the participant’s data.

If the approach is Sequential sampling, sample nextNframes from the beginning of the participant’s data into the batch.

Remove frames included in the batch from the participant’s data.

Add batch to the labeled dataset (for training in LOSO CV framework we used the labels from the dataset for the participant the algorithm was being trained on).

We used Cohen’s kappa as a metric of quality assessment for the human annotations. We selected nine participants and performed independent labeling by a second human annotator. We then computed Cohen’s kappa to measure agreement between both human annotators. We additionally computed Cohen’s kappa between the primary human annotator and the model prediction on a threshold level corresponding to maximal median kappa at iteration 20.

We created a web-based GUI which allows for visualizing the data, labeling the data frame-by-frame and re-training the model in the random sampling framework, and post-processing the data (see Fig.2for screenshot, and Supplementary Materials online for video (Supplementary Video S1) of how the tool works). The tool is based on open-source tools ‘plotly’ (https://plotly.com/python/) and ‘dash’ (https://dash.plotly.com/).

A: Visualization of CVA features together with the video of the participant. B: Interface for labeling the frames. Image is included for educational purposes and the individual’s diagnostic cohort is not specified.

The full dataset consisted of 566,043 frames (videos were 00:04:15 to 00:31:09 in duration). After excluding frames where the face or gaze were not detected, 535,539 frames were retained (5.38% of frames were invalid), with an average of 23,284 and a standard deviation of 6,193 frames per participant. Of all the frames, 79,629 were labeled as inattention (14.86% of the dataset).

The results of transfer learning can be seen in Table2; Fig.3. The sequential sampling approach performed substantially worse than the random sampling approach. Median AP, AUC and MK were 0.855, 0.965, 0.742 respectively at the start of the training (no adaptation to the participants yet). By iteration 20, median AP was 0.962, AUC 0.989, and MK 0.888 on random sampling approach as compared to median AP 0.640, AUC 0.862, and MK 0.548 in sequential sampling approach.

Average precision, AUC, and maximal cohen’s kappa percentiles at different iterations with two sampling/adaptation alternatives. The random sampling approach outperforms the sequential sampling one on all three metrics on each listed iteration.

Average precision, Maximal Cohen’s kappa and AUC per each iteration using different sampling/adaptation methods. Line color is median, and shaded area is interquartile range per each iteration.

Cohen’s kappa at different levels of prediction threshold for both sampling approaches (random and sequential) at iterations 5, 10, and 20 are shown in Fig.4. Thresholds at the highest median kappa and the corresponding median kappa values are shown in Tables2and3. The highest median kappa ranged between 0.792 and 0.888 in the random sampling approach, and between 0.223 and 0.426 in the sequential one. Figure4shows that the median Cohen’s kappa stayed relatively stable and high in the range of thresholds between 0.2 and 0.8, allowing for a general threshold for the model predictions to be set in this range.

Median (thick line) and Interquartile Range (shaded area) of Cohen’s kappa at different threshold levels at iterations 5, 10, and 20.

Thresholds and cohen’s kappa levels at highest median value of kappa in the two sampling approaches at iterations 5,10,20.

A second independent annotator labeled videos from nine participants, which in total accounted for 209,556 frames or 39% of the data set. It took the second annotator approximately 33 h to label nine videos, resulting in an average of 0.57 s spent per frame. Cohen’s kappa values between the two human annotators ranged between 0.548 and 0.859 (see Table4).

Agreement level (Cohen’s kappa) between human annotators, and between the primary annotator and the models adapted by random sampling at iterations 5, 10 and 20.

Agreement between the primary annotator and the model adapted by random sampling increased with each iteration of additional training and was in the ranges [0.498–0.954] at iteration 5, [0.541–0.953] at iteration 10, and [0.616–0.963] at iteration 20 (Table4).

We developed a web-based GUI which may be used for reviewing the CVA features of the video, additional labeling of frames and retraining the model, and post-processing of the data, including setting the model decision threshold and rejection of falsely detected inattention events. We make publicly available a pipeline for data pre-processing based on in-house code for face detection and OpenFace framework for head pose and gaze estimation27.

In this work we proposed a method for detection of periods of inattention to visual stimuli during EEG recordings. The tool was based on the CVA of videos of participants’ movement behavior which were synchronously recorded with EEG. We outlined a data processing pipeline, including face and facial landmarks detection, head pose computation, and gaze estimation. We proposed a MLP model for predicting inattention from these CVA features, and random sampling as a means for fine-tuning the model for each participant. We made publicly available a GUI that allows for visualization of the CVA features, model fine-tuning, prediction thresholds adjustment, and results post-processing. While we utilized a sample of children with neurodevelopmental conditions to test the tool, we expect it will also work well on other research populations, including neurotypical children and adults. Features included gaze coordinates, pitch, yaw, roll, and nose coordinates; the nose landmark was particularly useful for detecting inattention that included a head turn to the side.

The proposed random frame sampling approach for model adaptation to the participant outperformed the sequential sampling approach. For the non-fine-tuned model, maximal Cohen’s kappa was 0.742, placing the best potential agreement with the human rater in the ‘substantial’ range40. Compared to the initial non-fine-tuned model prediction, the model trained on additional 2560 labeled frames (equivalent to labeling only about 85 s of the video) significantly improved performance, as indicated by all quality metrics. On the other hand, sequential frame sampling performance decreased in the initial five iterations (see Fig.3), then gradually improved, but did not reach the performance of the random sampling approach. The reasons behind this included the strong temporal correlation of the features, hence low variability in the new input data, and the rare occurrence of inattention (prevalence of inattention is 14.86%), causing the absence of positive labels in many batches.

In line with a previous study13, we found that agreement on inattention labeling by human coders was in the ‘moderate’ to ‘substantial’ ranges in seven participants, and in the ‘perfect’ range for two participants43. Labeling inattention is a challenging task for humans, likely because annotators need to make a subjective judgement regarding the boundaries of the stimulus presentation screen. The provided GUI tool allows for visualization the raw CVA features together with the participant’s video, also enabling coders to label frames for the fine-tuning or post-processing stage. When the annotator needs to make a decision on an ambiguous frame, they can play the video to compare the frame in question with neighboring frames, which may help to better evaluate whether the participant was attending to the screen.

Our results show that the proposed approach makes annotation substantially more efficient. The existing human labeling system takes on average 220 min per video. Given that additional labeling takes about 0.57 s per frame, the need to label only about 2560 frames for a high quality labeling can significantly increase efficiency, reducing the effort to 24 min on average. Modularity of the tool we developed allows users to utilize any input/output compatible CVA pipeline and machine learning model, while keeping the same GUI. The initial model can be retrained as the amount of labeled data increase.

Using the same prediction model and tool for discarding inattention periods may facilitate multi-center studies by unifying the data pre-processing pipelines. Another way to facilitate multi-center studies is to perform pre-processing and labeling of the data in each center separately, and then share only the CVA features and annotations for training of the model with larger amounts of data. Such an approach helps to preserve the privacy of the data in each center, allowing centers to share only specific de-identified CVA features.

A limitation of the present study is the absence of a published model and our original full pre-processing pipeline. The reason for this is the removal of theintrafacelibrary30from public access. We provided the code for an alternative pre-processing pipeline predicting the same features based on the publicly available OpenFace library, and the model structure and interface needed for full integration into the GUI. Additionally, another limitation of the current study is that we calculated performance using annotations from a primary annotator who coded all the videos; while we were able to bolster confidence in the primary annotator’s performance by recruiting a second annotator to re-code 39% of the videos, full confidence could only be achieved by having multiple annotators code all available videos. However, in pediatric psychology research, it has been suggested that having 10–25% of videos re-coded by a second annotator is sufficient, depending on how variable the behavior being coded is44,45. A potential future direction is to work with the missing data caused by an inability to detect a face in the video. CVA could not detect the face in 5.38% of the frames in our dataset, likely due to either extreme angles of the head with respect to the camera or because of face occlusions. Future studies may attempt to associate these periods with attention/inattention to the screen by using imputation/interpolation methods.

We presented a low-cost scalable approach to inattention detection during EEG recordings using CVA, and made a publicly available tool for visualization, model fine-tuning, and post-processing of the system’s results. We also made publicly available an example of the computer vision analysis pipeline which can be used in future studies. We showed that fine-tuning the model on small amounts of new data by labeling the data on a per-frame basis substantially increased the model performance. Our work demonstrated that computer vision analysis was feasible for detecting inattention in EEG studies. We hope that by providing a scalable method for assessing inattention during EEG experiments, EEG studies will be more reproducible, and the feasibility of studying early brain development in populations in which sustained attention during EEG experiments can be challenging will increase. Such populations include infants and children with and without neurodevelopmental conditions, among others.

Below is the link to the electronic supplementary material.

This research was supported by a grant from the National Institutes of Health (NIH; NICHD 2P50HD093074, Dawson, PI). We thank the NIH and the children that participated in the research studies and their families.