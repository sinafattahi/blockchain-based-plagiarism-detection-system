The Philippines’ high tuberculosis (TB) burden calls for effective point-of-care screening. Systematic TB case finding using chest X-ray (CXR) with computer-aided detection powered by deep learning-based artificial intelligence (AI-CAD) provided this opportunity. We aimed to comprehensively review AI-CAD’s real-life performance in the local context to support refining its integration into the country’s programmatic TB elimination efforts.

Retrospective cross-sectional data analysis was done on case-finding activities conducted in four regions of the Philippines between May 2021 and March 2024. Individuals 15 years and older with complete CXR and molecular World Health Organization-recommended rapid diagnostic (mWRD) test results were included. TB presumptive was detected either by CXR or TB signs and symptoms and/or official radiologist readings. The overall diagnostic accuracy of CXR with AI-CAD, stratified by different factors, was assessed using a fixed abnormality threshold and mWRD as the standard reference. Given the imbalanced dataset, we evaluated both precision-recall (PRC) and receiver operating characteristic (ROC) plots. Due to limited verification of CAD-negative individuals, we used “pseudo-sensitivity” and “pseudo-specificity” to reflect estimates based on partial testing. We identified potential factors that may affect performance metrics.

Using a 0.5 abnormality threshold in analyzing 5740 individuals, the AI-CAD model showed high pseudo-sensitivity at 95.6% (95% CI, 95.1–96.1) but low pseudo-specificity at 28.1% (26.9–29.2) and positive predictive value (PPV) at 18.4% (16.4–20.4). The area under the operating characteristic curve was 0.820, whereas the area under the precision-recall curve was 0.489. Pseudo-sensitivity was higher among males, younger individuals, and newly diagnosed TB. Threshold analysis revealed trade-offs, as increasing the threshold score to 0.68 saved more mWRD tests (42%) but led to an increase in missed cases (10%). Threshold adjustments affected PPV, tests saved, and case detection differently across settings.

Scaling up AI-CAD use in TB screening to improve TB elimination efforts could be beneficial. There is a need to calibrate threshold scores based on resource availability, prevalence, and program goals. ROC and PRC plots, which specify PPV, could serve as valuable metrics for capturing the best estimate of model performance and cost–benefit ratios within the context-specific implementation of resource-limited settings.

The global tuberculosis (TB) report of 2024 indicates that the Philippines has one of the highest gaps between TB notifications and estimated TB incidence, reflecting the country’s need for improved case detection strategies [1]. In response to the increasing number of estimated people with TB and intermittent delays in TB service delivery, the Department of Health promoted a promising strategy to enhance case finding (CF) efforts by utilizing innovative and more sensitive screening tools to detect TB presumptively. Computer-aided detection powered by deep learning-based artificial intelligence (AI-CAD) was introduced in chest X-ray (CXR) screening across communities, health facilities, and workplaces. The AI-CAD aids in analyzing and interpreting digital CXR images for abnormalities and provides quantitative scores for TB-related lung conditions [2,3]. Several studies have evaluated the performance of CAD software in international contexts. One study concluded that AI-assisted diagnosis improved accuracy and work efficiency by saving time and accelerating TB screening [4]. Additionally, various studies found CAD to exhibit high sensitivity yet a broad range of specificity estimates (i.e., 26.6–95.0%) [5–10].

Based on a World Health Organization (WHO) CAD toolkit [2] and other studies [8,11,12], CAD accuracy may vary depending on the population setting or sources of patients, which include individuals identified in triage settings at health facilities and those in TB screening settings within community CF activities. Earlier studies indicate that the performance of AI-CAD differs across age groups, TB history, and sex [6,8,9,11,13]. A sub-analysis from a study evaluating five AI algorithms for TB detection revealed that the accuracy of CAD software was significantly lower among individuals with a history of TB treatment and varied according to the population setting, performing better among walk-ins than referrals from public and private facilities, and was less effective in older age groups [11]. Several studies corroborate the findings that CAD demonstrates lower specificity among those with a history of TB treatment [6,8,13,14]. While two studies [9,13] found no significant difference between males and females, Murphy et al. [7] reported a substantial difference in CAD software accuracy by sex, with specificity being lower in men than in women [6,8]. Variations in the threshold abnormality score distribution may be attributed to differences in TB prevalence, intervention settings, and the methodologies applied in the software [11].

In addition to discrepancies between population subgroups, intrinsically, some studies also showed variable CAD diagnostic performance with different software versions. However, it is not only the intrinsic properties of software that affect the performance and ultimately the diagnostic yield; the screening and testing algorithms used by implementers must also be considered. Despite high sensitivity, some studies report low specificity, which may be attributed to the screening algorithm used during implementation. Participants identified as high risk for being TB presumptive by CXR may be more likely to have their TB disease status evaluated using the confirmatory test compared with those considered at lower risk or with negative CXR results. This highlights the occurrence of partial verification, where not all screened participants undergo the confirmatory test, leading to an overestimation of sensitivity and an underestimation of specificity, as there would be more positive test results than negative ones [15,16]. This concern is exemplified by at least two studies [17,18], which reported estimated sensitivity of 83% with specificity of 62.6% and 12.7%, respectively. Neither study collected sputum from participants who were not presumptive for TB by CXR, which depended on established threshold scores. Fehr et al. [17] recognized that determining the best score for triaging should involve testing the sputum of all participants to capture all cases of active TB disease and provide unbiased data to determine an optimal triage threshold. However, the practical choice of threshold needs to balance maximal case finding and cost-effectiveness. Nishtar et al. [18] also mentioned that their study found low specificity due to the limited use of GeneXpert cartridges. In both studies, confirmatory testing was not performed for individuals who tested negative for TB by CXR or who showed no signs or symptoms, as such testing was not programmatically feasible for these negative results. Given the findings of earlier studies, reviewing the real-life performance of AI-CAD in CF activities within the Philippine context [6] is important. Such valuable information can contribute to refining AI-CAD integration into the country’s programmatic efforts toward TB elimination. Hence, this current study presents an assessment of the estimated diagnostic accuracy of CXR with AI-CAD, the factors affecting its screening performance, estimated abnormality threshold scores for different high-risk groups, and some relevant major performance indicators.

The United States Agency for International Development’s TB Innovations and Health Systems Strengthening (TBIHSS) project conducted CF activities in four prominent regions of the Philippines between May 2021 and March 2024. All screened individuals received a postero-anterior CXR DICOM image using qXR version 3 (Qure.ai, India), which was selected as the AI-CAD software following the project’s stringent procurement procedures based on the technical and financial requirements of the CF activities. qXR provided an abnormality score ranging from 0 to 1. Images with a score of 0.50 or higher were classified as TB presumptive, whereas those with a lower score were categorized as TB negative. This threshold score was established based on the project’s previous experience in 2020, during which the software’s performance was evaluated throughout the pilot implementation. The threshold score was set at 0.50 to align with the national context. After acquiring a software license, TBIHSS applied this cut-off score in its scale-up implementation in 2021. This threshold (≥ 0.5) remained unchanged throughout the scale-up implementation from 2021 to 2024. Active case-finding activities were conducted in communities (ACF-comm) and workplaces (ACF-WP), whereas intensified case-finding activities (ICF) were carried out in hospitals in Regions 3, 4A, and 7 and the National Capital Region.

Those TB presumptive individuals identified through either CXR with AI-CAD or by presenting TB signs and symptoms, as assessed by a trained health worker on site, were advised to undergo testing for bacteriological confirmation (Fig.1). The screening algorithm began by securing consent and collecting demographic data, as well as information on TB signs and symptoms (persistent cough lasting 2 weeks or more, unexplained fever, unexplained weight loss, and night sweats), along with risk factors of the screened clients. After the interview, all clients who provided their consent, regardless of the presence of TB symptoms, were asked to undergo CXR. If identified as TB presumptive by CXR, the participant would then be advised to undergo mWRD testing. However, those with TB-negative results from CXR were advised to wait for the official radiologist reading. If found to have findings suggestive of TB by either symptom screening and/or CXR, they were asked to submit sputum for mWRD testing. Those determined to be TB negative, both by symptoms and CXR (AI-CAD and official radiologist reading), did not undergo mWRD testing per national TB program policy and algorithm but received TB education and counseling. A positive mWRD result indicates the presence of MTB, which may be either rifampicin-resistant or rifampicin-susceptible. Full resistance patterns were not identified through this study. During implementation, some clients were identified as TB presumptive by AI-CAD but did not submit their sputum for testing.

This retrospective cross-sectional study utilized data from the above CF implementation. We applied the formula presented by Hajian-Tilaki [19] to compute the minimum sample size, using a sensitivity of 95.3 from TBIHSS’s pilot ICF implementation in 2019. The alpha significance level was set at 0.05, and the margin of error used was 3%. Based on the 11.5% TB presumptive yield by CXR with AI-CAD from the pilot ICF implementation, we estimated a minimum sample size of 1663.

To evaluate the estimated diagnostic performance of AI-CAD, we computed metrics such as pseudo-sensitivity, pseudo-specificity, PPV, and pseudo-negative predictive value (NPV) with a 95% confidence interval (CI) using mWRD test results as the standard reference [20]. To accommodate the insufficient or minimal representation of CAD-negative, as most CAD-negatives were not referred for mWRD testing due to the screening algorithm of the activity, we used the term “pseudo” for the estimates in this study. We calculated the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (PRAUC) to evaluate the prediction performance of the tool [21–23]. Due to our imbalanced dataset of people with and without TB, we decided to include PRC curves, which provide additional information and evidence about their performance [21,24,25]. We also computed theF1 score, as it balances the trade-offs between precision and recall. To assess differences in screening performance among pre-specified factors like age, sex, signs and symptoms, and history of TB treatment, we used Pearson’s chi-squared test [8,9,11]. To determine the association between these factors and the likelihood of CAD detection, we employed multiple logistic regression, adjusting for the covariates [8] and stratified by true TB status. Missing covariate data were imputed using multiple imputation by chained equations, and estimates were pooled across datasets using Rubin’s rules. Determining the threshold scores, positivity rate, mWRD tests saved, and number needed to test (NNT) also helps to optimize sensitivity and PPV for screening and triaging in programmatic implementation. NNT is the number of individuals with presumptive TB that must be tested with a bacteriological test to identify one person with TB, whereas mWRD tests saved (with 0% representing the Xpert-test-for-all scenario) may be used to infer the cost–benefit ratio [11]. All statistical analyses were conducted using STATA (Stata v.18, StataCorp, 2023, College Station, TX: Stata Press). The data and analytic codes used for statistical analyses in this study are publicly available at 10.5281/zenodo.16732568 [26].

Overall, TBIHSS’s case finding activities from May 2021 to March 2024 screened 52,840 individuals, with 47,817 (90.5%) undergoing chest X-ray (Fig.1). Reasons for not having a CXR performed included recent external CXR completed, individuals under the eligible age of 15, and refusal to undergo CXR after the screening interview. The majority (40,486) of those assessed were both CAD-negative and TB-negative according to the physician’s evaluation, whereas the number of CAD-positive individuals who were actually without TB (false positives) is 3592 (83%). Following the screening algorithms, these participants were not advised to undergo mWRD testing. There were 629 CAD-positive individuals who could not submit a sputum specimen for testing. After excluding all these ineligible participants’ data, a total of 5740 individuals had CXR and mWRD results, with 749 (13%) microbiologically confirmed people with TB, of which 44 (5.9%) were resistant to Rifampicin.

Compared with non-TB individuals, confirmed cases were significantly younger, and the majority had no history of TB treatment (Table1). Data showed no significant difference in proportions of people with TB between those with and without symptoms (p= 0.269). People with TB were more likely to be male (16%). During the implementation period, a downward trend was observed in detected TB yields, with the highest rate at 15% in 2021. Among those tested by mWRD, ICF yielded the highest proportion of cases (24%), followed by ACF-comm (13%), and workplaces (10%). The mWRD positivity rate among CAD-positive individuals was 17% compared with 2% among those CAD-negative.

With a fixed abnormality threshold of 0.5, the overall pseudo-sensitivity of CAD was high at 95.6% (95% CI, 95.1–96.1), but the pseudo-specificity was low at 28.1% (26.9–29.2), and the PPV was at 16.6% (Table2). In various population settings, AI-CAD maintained a comparably high pseudo-sensitivity above 90%, achieving the highest pseudo-specificity in ACF-comm (35%) and the highest PPV in ICF (29.6%), both with similarp-values of < 0.0001. Pseudo-sensitivity showed non-significant differences among risk groups, except for those with TB symptoms (p= 0.022). It was higher among males (96.4%) and those without a history of TB treatment (96.9%). Notably, the highest pseudo-sensitivity was recorded in 2022 (97.1%), compared with 2023 (95.2%) and 2024 (94.2%). Pseudo-specificity was significantly higher among females (34.4%), younger age groups, and those without a history of TB treatment (19.2%). There was an increase in PPV performance in 2024 (17.6%) compared with previous years. NPV was lower among those with a history of TB treatment and in ICF.

Table2also displays the AI-CAD model’s AUROC, PRAUC, andF1 score based on key demographic and clinical factors that may influence the tool’s screening performance. Overall, CXR performed moderately with an AUROC of 0.82 (95% CI, 0.80, 0.84), indicating that the AI-CAD model could differentiate people with TB from those without TB, with an 82% probability in the study population. Although AUROC varied across different risk groups, a significant difference was found among various age groups (p= 0.0002) and those with TB symptoms (p< 0.0001). The CAD model’s AUROC for females was relatively higher than for males, with values of 0.83 and 0.81, respectively. The AUROC among those with no history of TB treatment was higher at 0.82. These results suggest that the CAD model may exhibit a better ability to identify people with TB among females and individuals without a history of TB treatment.

With a PRAUC of 0.489 and a baseline classifier of 0.131, the CAD model demonstrated a relatively good balance between precision (PPV) and recall (sensitivity). The CAD performance in ICF (0.655), the youngest age group (0.692), those with TB treatment history (0.411), and during the 2022 implementation (0.494) showed the highest PRAUC among subgroups, compared to their baseline classifiers. The higher the PRAUC, the better the CAD can balance precision and recall for people with TB. However, theF1 score of the CAD model on this imbalanced dataset was low at 0.28.

Performance metrics for the CAD threshold scores closest to pseudo-sensitivities of 95%, 90%, and 85% were obtained (Table3) and compared with the pseudo-sensitivity at a 0.5 cut-off score, alongside a counterfactual scenario where no screening tool was used. Table3describes the relationships among the estimated number of mWRD tests saved, the number of missed cases, and the NNT for the respective threshold scores of pseudo-sensitivities that were close to the WHO-recommended TPP of 90% sensitivity. Lowering the pseudo-sensitivity to 90% resulted in a higher threshold score of 0.68, which led to an increased PPV of 20.2%, thereby resulting in 79.8% false positives among CAD-positive individuals. This threshold score also reflected a 42% mWRD tests saved, with a NNT of five individuals while sacrificing 10% of missed cases. A 0.5 cut-off score would yield a lower percentage of mWRD tests saved (25%) and a higher NNT of six individuals, but with a lower rate of missed cases (4.4%). Based on a sub-analysis in ICF, increasing the score to 0.64 would raise the PPV to 33.5% with 9.6% missed cases, which was higher than the missed cases (5.5%) at the 0.5 score. However, this 0.64 score also provided 36% mWRD tests saved compared with 24.5% mWRD tests saved if a 0.5 score was used, noting that the respective NNTs were close to each other, 3.0 and 3.4, respectively. In ACF-comm, lowering the sensitivity to nearly 90% and increasing its PPV to 20.4% would yield 44.4% mWRD tests saved and a NNT of 5. With almost the same percentage of missed cases compared to ICF at 9.7%, a higher proportion of mWRD tests were saved in ACF-comm (44.4%). Notably, in ACF-WP, increasing the score to 0.65 from 0.5 would still maintain 95% pseudo-sensitivity and increase the PPV to 14%; more importantly, the mWRD tests saved would rise from 12.7 to 30%, with a lower NNT of 7 than a NNT of 8.6 at the 0.5 cut-off. However, this trade-off would significantly raise the rate of missed cases from 1.1 to 5%. All population settings in CF reduced mWRD tests required by more than 35% while maintaining a pseudo-sensitivity above 90%.

Figure2presents various performance metrics of AI-CAD during implementation. An AUROC of 0.82 indicated a moderate to high discriminative ability for TB triage; however, it was important to note that there was high pseudo-sensitivity, which did not fully represent the entire population, and many true negatives were excluded from the analysis. For recall values greater than 90%, the computed precision would be 20% or less. This resulted in a PRAUC of 0.489, which seemed low, but it was 3 to 4 times higher than the random-classifier baseline (0.131). This PRAUC (Fig.2B) suggested that the model had a moderate ability to detect people with TB and appeared suitable for supporting TB screening in clinical settings, although it may still require further validation. With pseudo-sensitivity exceeding 90%, the curve (Fig.2C) indicated that the number of mWRD tests saved consistently decreased below 40%. The slope of the curve increased sharply in the trade-off between pseudo-sensitivity and mWRD tests saved when the pseudo-sensitivity dropped below 90%. Figure2D displayed a steady direct curve that indicated a positive correlation between PPV and mWRD tests saved, following a brief plateau in the estimated mWRD tests saved when the PPV was around 20%. The model’s ability to improve the PPV appeared to stagnate in this short range, possibly because some radiologic images of TB disease were more challenging to classify. This scenario suggested that the balance between true and false positives stabilized within this proportion of saved mWRD tests. The lower threshold (Fig.2E) indicated a high number needed to test and suggested higher sensitivity. At an implementation threshold of 0.5, the NNT was 6, and at a threshold range of 0.3 to under 0.5, the curve flattened or reached a plateau. This indicated that a further increase in the threshold within that range did not significantly reduce the NNT, risking the loss of some true positives. However, if the implementation threshold was raised to 0.68 or higher, the NNT would decrease to 5 or lower.

Multivariate analyses revealed no association between CAD positivity among confirmed people with TB and the factors included in the model. Although not statistically significant, males were twice as likely to be classified as actual cases by the AI-CAD tool.

The association between ACF in WP and AI-CAD positivity was marginally non-significant (p= 0.052), indicating the need for further investigation. The likelihood of a negative CAD result among people without TB was significantly associated with sex, age, TB treatment history, TB symptoms, and population setting (Table4). The CAD model showed significantly higher pseudo-specificity in individuals without TB treatment history (OR = 0.7), indicating they were more likely to receive a correct negative result compared with those with TB treatment history (also see Table2). Flagging negative results by CAD in ICF and ACF-comm also exhibited higher performance, with ORs of 2.6 and 1.8, respectively, compared with ACF-WP. The tool also performed better in terms of pseudo-specificity with decreasing age.

The results of this case-finding study provided additional insights into the performance of AI-CAD in specific populations in the Philippines and its potential role in the TB elimination efforts as we have limited studies in the country that explore this field. Its application in TB case finding activities increased CXR imaging coverage by screening more individuals in a shorter time, resulting in a higher TB yield of 1.57% compared with the national TB prevalence [27]. The CAD model demonstrated high pseudo-sensitivity at the fixed abnormality threshold score. As diagnostic confirmation was not conducted for all CAD-negative individuals, this undoubtedly affected the overall model performance estimates because the true specificity may be underrepresented. To compare with previous findings with similar screening methodology, the AUROC (0.82) in the current study was higher than the AUROC of 0.75 reported among migrants by Gelaw et al. However, it is relatively lower than the 0.85 reported by Tavaziva et al. [6] and 0.86 reported by Kagujje et al. [13]. In subgroup analysis, our ICF AUROC of 0.83 was higher than the AUROC of 0.82 evaluated from Kasturba Hospital in India by Nash et al. [14]. However, compared with the findings of Qin et al. [11] in their study of three tuberculosis screening centers in Bangladesh, our study had an AUROC lower than their 0.91 (usingqXR), 0.90 (using CAD4TB), and 0.85 (using InferRead DR).

Nevertheless, our results indicated that the performance of AI-CAD varied with population settings and across different contexts, influenced by various demographic and clinical factors. Beyond the prevalence rate, CAD performance differed across contexts and participants’ profiles [28,29]. In this study, the likelihood of detecting CAD-positive among people with TB did not significantly vary by sex, age, history of previous treatment, or TB signs and symptoms. Despite the marginally non-significant higher odds ratio (OR = 4.8) of detecting CAD-positive individuals among those screened in workplace settings compared to those in the ICF, this suggests that the determination of CAD performance may also depend on the interaction of risk factors and requires continuous evaluation. Moreover, when determining the likelihood of tagging the CXR image as CAD-negative among people without TB, our study found that all the factors examined were significantly associated. These findings align with the findings of Khan et al. [18], who also analyzed the odds ratio of software correctly classifying CXR as abnormal.

We also assessed the performance of CAD over the years of implementation, anticipating improved model performance with newer versions. The data revealed that the best performance occurred in 2022, evidenced by the highest pseudo-sensitivity, AUROC, and PRAUC, although the highest PPV was recorded in 2024. The range of AUROC values (0.79 to 0.84) is relatively low compared with the study findings on qXR versions 2 and 3, which had AUROC values of 0.87 and 0.90, respectively, as reported by Qin et al. [31].

The different AUROC and PRAUC among risk groups helped determine differences in CAD performance and aided in adjusting its threshold cut-off score. Aside from overall screening performance, no specific high-risk group demonstrated high pseudo-sensitivity, which may also be attributed to the programmatic algorithm and the performance metrics’ inherent characteristics. On the other hand, disease prevalence in the tested population did not affect the ROC curves, whereas TB yields influenced the PRC in the study populations [32]. To make this more general, sensitivity and specificity remain stable across prevalence values, whereas PPV and NPV are highly prevalence-dependent [33,34]. A lower TB yield within the oldest age group in our data may explain why its PRAUC was much lower than that of other age groups (Table2). When dealing with an imbalanced dataset, it was often necessary to use both PRC and ROC metrics to visualize better and capture the actual CAD performance [21]. The importance of the PRAUC was emphasized when analyzing imbalanced data, as it provided a more nuanced assessment of model performance due to its prevalence-sensitive nature [25]. When using PRAUC, a perfect model would exhibit a PRAUC of 1.0, whereas a baseline random classifier may have a PRAUC value equal to the proportion of cases among those tested. Because the random-classifier baseline in PR curves was influenced by the ratio of cases to non-cases of the disease, PRAUC can subsequently help estimate the performance of a CAD model by determining whether the PRAUC value remains above the disease prevalence in the population. In highly imbalanced datasets, even a small number of false positives can significantly reduce precision [25,32,35]. Applying this with a health program perspective, ensuring that the PRAUC value of an AI model remains above the estimated or even actual proportion of cases in the target population may help guarantee that the AI model is not merely guessing or acting as a random classifier.

We used 0.50 as the acceptable cut-off during implementation. Despite PRAUC,F1 score, and AUROC being highly recommended tools for performance evaluation, our programmatic data may require additional metrics and analyses, such as the number of mWRD tests saved, missed cases, and NNT, to assist implementers in making informed decisions on threshold score calibration through a more robust assessment [11,36]. These additional implementation indicators may be beneficial when considering the cost–benefit ratio and screening ability. Table3provides options for threshold scores designed to maximize sensitivity, PPV, and the aforementioned metrics, as different programmatic goals may necessitate different threshold scores. For example, for a healthcare program aiming to identify as many people with TB as possible, one option would be to maximize sensitivity, even at the expense of targeted specificity [37]. Lowering the threshold scores would lead to reduced specificity, increased sensitivity, and a decreased PPV. This change would also result in more diagnostic tests, fewer mWRD tests saved, increased NNT, and a reduced number of missed cases. However, among these scores, we could focus on identifying a level that achieved a favorable balance between sensitivity and PPV [25,36]. Based on Table3, if the program had utilized 0.68 as the cut-off, sensitivity would still be at 90%, with a resulting 4% increase in PPV due to an increased number of saved mWRD tests to 42%, alongside a decrease in NNT to five individuals. This approach would result in 74 missed cases, compared with only 33 missed cases if using the 0.5 cut-off. This miss was attributed to more individuals failing to receive a diagnosis, as they would fall into the false-negative category of the screening tool, leading to a trade-off between identified people with TB and the proportion of mWRD tests saved. This aligned with the authors’ message in their evaluative analysis of trade-offs in screening programs, in scenarios where a more sensitive screening test was being considered [36,38]. This means increasing the number of individuals who are true positives while minimizing false positives and balancing this against the number of false negatives.

The impact of adjusting the threshold scores would be evident in a subgroup analysis of different patient settings. ACF-comm would save more mWRD tests than ICF, but both would miss a similar proportion of cases (9.6%/9.7%), and both would achieve a PPV of more than 20% had pseudo-sensitivity been reduced to 90% at the 0.65 and 0.64 scores, respectively. Exploring this option might require further evaluation and support from the programmatic stakeholders. Regarding ACF-WP, setting the threshold score at 0.65 for a sensitivity of 95% would be preferable to increasing it further to 0.71 for a sensitivity of 90%. Both scores would lead to a similar PPV of 14%, but the difference in mWRD tests saved and NNT was observed to be less significant compared with the difference in the proportion of missed cases, with 10% at 0.71 and only 5% at 0.65.

The potential value of PPV and the PRC as metrics for model performance in actual clinical practice and programmatic implementation could also be seen in Table3. One limitation of the study was the underrepresentation of predicted negative cases by the AI-CAD, as these cases did not have the opportunity for mWRD testing due to the programmatic algorithm employed; only those presumed to have TB by CXR, signs and symptoms, and/or official radiologist reading were included. Partial verification bias has been considered because not all participants underwent the mWRD test [15,16]. As mentioned above, diagnosing more CAD-positive than CAD-negative would lead to an overestimation of sensitivity and an underestimation of specificity. Thus, estimates such as pseudo-specificity and pseudo-sensitivity values, which are used as true negatives, may be underrepresented. Testing more CAD-negative individuals might influence the overall metrics and provide a more precise overview of the actual performance of the AI-CAD model. Additionally, by utilizing precision and recall, the impact of the underrepresentation of true negatives is reduced, if not entirely mitigated. This may help alleviate the burden of achieving the target specificity profile set by the WHO. However, rather than completely disregarding specificity and lacking the means to balance sensitivity, Table3illustrates how PPV and PRC could be used instead of specificity. To provide a more concrete theoretical example for programmatic implementation, one could begin with a much lower threshold to include as many cases of PTB as possible at the outset of the program. PPV values may be recorded at baseline. Even if the PPV is low, the number of confirmatory laboratory tests available at the start of the program may still be substantial and could offset this performance. As the program progresses, resources may become limited. When confirmatory laboratory tests become scarce, it may be more crucial to maintain a higher PPV. This can serve as a trigger or condition to adjust the threshold score, reducing sensitivity while still staying within the WHO target profile of 90% sensitivity [37] and maximizing PPV, thereby ensuring that the AUPRC remains above that of a baseline classifier.

In our study, examining the positivity rate of individuals classified as CAD-negative, we found an estimated 2%. Given that most CAD-negative individuals were not referred for confirmatory testing, the likelihood of identifying the true positive rate among all CAD-negative individuals exceeding 2% would be low, as many untested CAD-negative individuals likely had no symptoms, no risk factors, or no clinical suspicion. Support for this reasonable assumption can be found in these studies [11,34,39,40], by emphasizing the relationship between disease prevalence and the positivity rate among false-negative results of diagnostic tests. In relatively low-prevalence settings like in our study, with less than 2% TB yield and high NPV, it may indicate that a negative result confidently rules out the presence of disease. Nevertheless, while disease prevalence can influence the positivity rate among false negatives, the exact relationship depends on multiple factors, including the nature of the test and the population characteristics [39]. Further studies with expanded confirmatory testing among those identified as CAD-negative would help refine the estimate of false-negative rates and better characterize the screening tool’s performance [41].

Another limitation was that testing all individuals in a real-world implementation would be impractical and would undermine the purpose of efficient screening and triaging. Mycobacterial culture has been prioritized as a reference standard, but we utilized mWRD testing. Some people without TB identified by mWRD could, in fact, be culture-positive cases if testing were undertaken.

We also did not assess the operational and logistical performance of the AI-CAD. We did not have sufficient data to evaluate troubleshooting frequency, processing time, infrastructure requirements in health facilities and communities, and other technical issues. As we acknowledge, the impact of these concerns may also contribute to informed decision-making regarding the optimal balance between metrics and the required threshold score.

Given the above findings and limitations, this study recommends that AI-CAD be retrained using specific demographic data for actual community or healthcare program implementation. The AI model intended for a specific community or hospital can be fine-tuned or retrained using previous data sources from the population. In cases where the AI model has achieved an acceptable AUROC or PRAUC and meets the WHO TPP for sensitivity but falls short of the target specificity, the authors suggest the following steps. Initially calibrate the threshold value to maximize sensitivity, even at the expense of target specificity. This approach is feasible in the early stages of implementation when confirmatory testing is still widely available. However, as laboratory tests become scarce in the later stages of implementation, a higher PPV may be more optimal. This approach may enhance the cost–benefit ratio regarding the potential overutilization of confirmatory laboratory tests during the early phases [37]. If threshold calibration is ineffective or fails to achieve the desired clinical outcome for the program implementer, retraining the AI model with local data using transfer learning can be considered as a second option.

Analyzing AI-CAD performance in TB screening and triaging demonstrated its potential to enhance efficiency and yield in local programmatic implementation. The tool showed promising performance metrics, such as high pseudo-sensitivity and acceptable AUROC and PRAUC scores. However, accurately estimating specificity was challenging due to limited confirmation and evaluation among screened CAD-negative. Another highlighted point was the importance of adapting threshold scores based on available resources, prevalence rates, and program goals. The analysis also indicated that performance variations across risk subgroups, including sex, history of TB treatment, signs and symptoms, and population setting, underscore the need for context-specific calibration. These factors were identified as potential factors affecting the performance of pseudo-sensitivity and pseudo-specificity in TB screening. Methodologically, this study illustrated the importance of addressing imbalanced data by using PRC and PPV as supplementary metrics to help mitigate the potential effects of underrepresented true negatives. Given our limitations, such as the absence of complete logistic evaluations, retraining the AI-CAD model using local demographic and clinical data is recommended, and this approach may better balance diagnostic performance, operational feasibility, and optimal resource utilization in identifying people with TB in the country.

We thank FHI 360 for providing us with their CF data. We also thank all those implementing and supporting partners both in the national (Department of Health, Global Fund’s ACCESS TB Project, Innovations for Community Health (ICH), and Tropical Disease Foundation, Inc. (TDFI)), and sub-national levels (Centers for Health Development, City Health Offices/Provincial Health Offices, hospitals, COVID-19 referral centers, and public/private workplaces).

NM led the design and is behind the research conception, the programmatic data analysis, the interpretation of results, the acquisition of data, and provided overall oversight. EJC worked on analyzing and interpreting results and recommendations based on valuable data insights. JC supported the study’s conception and interpretation of results. MRS and ROC contributed to the acquisition and analysis of programmatic data and the interpretation of results. SS supported the study conception, drafting, and revision of the report. LS provided valuable guidance on the study design and contributed to the writing and reviewing of the final draft. All of the authors read and approved the final manuscript.

Partial support for this study was provided by FHI 360 with funds from the Ward Cates Emerging Scientific Leader grant (award number 2021). The funder had no role in the study design, data analysis, interpretation, or writing of this manuscript, and the views expressed in this publication do not necessarily reflect those of FHI 360.