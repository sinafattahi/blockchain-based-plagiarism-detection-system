Personalising psychotherapies for depression may enhance their efficacy. We conducted a randomised controlled trial of smartphone cognitive-behavioural therapy (CBT) among 4,469 adults in Japan (RESiLIENT trial, UMIN-CTR UMIN000047124). Participants received one of nine CBT skills or combinations, or a health information control (HI), over six weeks. All interventions were found efficacious. We developed prescriptive models using machine learning to forecast changes on the Patient Health Questionnaire-9 (PHQ-9) at week 26 and created a personalised and optimised therapy (POT) algorithm that recommended the most suitable CBT for each participant. In a simulated randomised comparison, the effect of POTs over HI was a difference by −1.41 (95%CI: −1.91 to −0.90) points on the PHQ-9 corresponding with a standardised mean difference of −0.37 (−0.49 to −0.23), which was 35% greater than that of the group-average best intervention. A new randomized trial to confirm the external validity and applicability of the algorithm is warranted.

Depression exists on a continuum. Towards the severer end, depressive disorders above the diagnostic threshold for major depression or dysthymia constitute the second largest cause of the years lived with disability worldwide1. Subthreshold depression, while not satisfying the official diagnostic criteria, is prevalent in the general population and associated with impaired social functioning, decreased quality of life, increased use of health services and mortality2,3. Although the individual-level burden of subthreshold depression may not match that of full-syndrome major depressive disorder, its high prevalence means that the societal economic cost is of a similar magnitude4. Furthermore, individuals experiencing subthreshold depression have a threefold increased risk of progressing to a major depressive episode compared to those without such symptoms5.

Psychotherapies are effective across the whole spectrum of depression6. An individual participant data meta-analysis of various psychological interventions for people with subthreshold depression showed an effect size of −0.27 (95%CI: −0.40 to −0.14) in depressive symptom reduction up to 12 months7. Current guidelines unanimously recommend psychotherapies for people with subthreshold to mild major depression8.

However, the meta-analyses also show that nearly half the participants receiving the psychotherapies may remain symptomatic through the follow-ups9. There is a clear need to improve the treatments. We now have a wide array of empirically supported psychotherapies, and one promising approach is to match these various psychotherapies with individuals’ characteristics and thereby increase the efficacy both at the individual and the total cohort level10,11. There is high hope and need for empirically supported personalised therapies, or precision medicine, which are based on effect modifiers (EM), i.e., patient characteristics that influence the effect of the treatments over controls, in addition to prognostic factors (PF), i.e., characteristics that influence the course of the disease both in the treatment and control arms.

As trustworthy knowledge about effect modification can come only from randomised trials, there have been attempts to analyse randomised data to construct precision treatment rules based on EMs and PFs. However, attempts so far at precision medicine in psychotherapies suffer from several limitations. First, most of these studies are based on studies comparing one intervention against one non-treatment control12. While such algorithms may indicate which subgroup may benefit from the intervention and thereby decrease futile administration of the intervention outside this subgroup, it cannot improve the outcome for the whole sample who all need some treatments. Second, there are some other studies that compared two active interventions13. These studies estimate how much individual patients benefit more from one treatment (indicated treatment) over another (non-indicated treatment) and compare the outcomes between patients who received the indicated treatment and those who received the non-indicated treatment. This knowledge is important when an individual has to choose between the interventions for themselves. However, the more critical question for public health is whether providing the interventions as recommended by the precision medicine algorithms can improve the outcome of the whole cohort over administering the group average best treatment. But this question seems to have never been answered or even asked for psychotherapies. As a matter of fact, we currently have more than two variations of psychotherapy. The pragmatic question then is which from among the many available empirically-supported psychotherapies and their skills are the best for individual patients, and whether personalising psychotherapies this way can improve the outcomes of the entire cohort over and above administering the group average best treatment to everyone14,15.

We have conducted the largest individually randomised controlled trial of internet cognitive behavioural therapy (iCBT) among adults with or without subthreshold depression (Resilience Enhancement with Smartphone in LIving ENvironmenTs (RESiLIENT) trial)16. The trial examined five cognitive and behavioural skills commonly used in iCBT programmes, namely behavioural activation (BA), cognitive restructuring (CR), problem solving (PS), assertion training (AT), behaviour therapy for insomnia (BI), and their combinations, and found that these interventions were differentially efficacious in reducing depressive symptoms up to half a year17. The current report presents the results of subsequent analyses of the trial aimed at determining the extent to which the personalised & optimised therapy (POT) algorithm utilising the machine learning method could improve the outcome for the entire cohort over administering the intervention found to be the best on average in the trial to everyone in the cohort.

Figure1shows the overall flow of the participants. We screened a total of 34,123 people for eligibility between September 2022 and February 2024, and finally entered 4469 participants as the intention-to-treat cohort for the present analyses. (Table1) The follow-up proportions were 96.8% at week 6 and 90.1% at week 26.

Of 34,123 assessed for eligibility, 28,759 were excluded, and 5364 were randomly allocated to one of the 12 groups. Excluding those who withdrew consent or were allocated to self-check or delayed treatment controls, 4469 participants constituted the intention-to-treat cohort for the present study. AT: assertion training, BA: behavioural activation, BI: behaviour therapy for insomnia, CR: cognitive restructuring, PHQ-9: Patient Health Questionnaire-9, PS: problem solving.

PHQ-9Patient Health Questionnaire-9,GAD-7Generalised Anxiety Disorder-7,ISIInsomnia Severity Index,WSASWork and Social Adjustment Scale,HPQHealth and Work Performance Questionnaire,UWESUtrecht Work Engagement Scale,EQ-5D-5L: EuroQOL-5D-5L,SWEMWBSShort Warwick Edinburgh Mental Well-Being Scale.

Table2shows the estimated least squares mean change scores of the PHQ-9 from baseline to week 26 for each intervention and control arm. The effect sizes of the intervention arms ranged between -0.25 (95%CI: −0.37 to −0.13) for BA + CR to −0.10 (−0.23 to 0.02) for PS. Supplementary Tables1and2show the same for those with baseline PHQ-9 scores ≤4 and for those with scores ≥5 separately.

†Minus value indicates depression reduction. () indicates 95% confidence intervals.

‡Effect sizes (standardised mean differences) were calculated using the observed baseline SD for within-group change score and the observed week 26 SD for group differences.

Supplementary Tables3,4and5shows the estimated regression coefficients in the three LASSO models.

Table3presents the prediction accuracy measures of Models 1–3. R were around 0.46 for Model 1 and 0.57 for Models 2 and 3, respectively. The post-randomisation variables made important contributions to the predictive accuracy. The biases of the predictions were nearly 0 and all the three models were well calibrated.

However, the values of around 2.5 for mean absolute errors and 3.2 for root mean squared errors indicated the prediction errors of individual participants were variable and might be relatively large compared with the average changes around −2.5.

Table3also shows the internal-external validity of the three models. The three models appeared to fit the population regardless of their sources of recruitment.

Supplementary Table6shows the treatments recommended for the 4469 participants according to PrBest (probability to be the best treatment) by Model 1. Because Model 1 uses only baseline variables, these recommendations apply to both one-skill and two-skill interventions. Looking at this table, however, we note that the top three recommendations were AT, BI or BA + CR for those with baseline PHQ-9 scores of 4 or less, while BA + CR, BA + PS and BA + AT were recommended for those with baseline PHQ-9 scores of 5 or more.

These results indicate that for those with baseline PHQ-9 scores of 5 or more, we could use Model 2 or 3, which show stronger prediction performance than Model 1, to make recommendations from among the four two-skill interventions. Supplementary Table6also tabulates the recommended treatments by applying Model 2 and 3, respectively, for those with elevated baseline depression, while using Model 1 for those with less baseline depression.

Actual administration of the intervention while using the POT algorithm calls for simpler interventions, i.e., fewer intervention options, if appropriate. First, people with less symptoms at baseline may be less motivated to pursue longer interventions. Second, there will always be some people who do not respond to any interventions, however optimised, and they may benefit from second-line interventions. It will be easier to arrange the latter when the first-line interventions are not complicated.

Given these considerations we simulated RCTs of various POT algorithms for those with baseline PHQ-9 of 4 or less and for those of 5 or more (Supplementary Tables7and8). For the former group, choosing a single-skill intervention, AT or BI according to each individual’s PrBest by Model 1 improved the total effects of the interventions by 36.1% over the group average best; for the latter group, administering one of the best three two-skill interventions (BA + CR, BA + PS or BA + AT) according to each individual’s PrBest by Model 3 produced the largest gain (25.6%) in effectiveness over the group average best. Table4shows the final number of participants who would be recommended AT or BI according to Model 1 among those with 4 or less, and BA + CR, BA + PS or BA + AT according to Model 3 among those with 5 or more.

Figure2illustrates this simulated RCT by cross-tabulating the random allocations vs POT recommendations. Those randomised to the HI control (green cells,n= 447) showed on average a reduction in the PHQ-9 scores by −1.66, while those randomised to the group average best arm (BA + CR, red and purple cells,n= 447) a reduction by −2.70. Among those who would be recommended their POT, the blue and purple cells represent those who happened to be randomised to their POT (n= 469) and would show a reduction by −3.07 at week 26. The difference in observed PHQ-9 change scores from baseline to week 26 over the HI control improved from −1.04 (or SMD of −0.27) on the group average best to −1.41 (or SMD of -0.37) on the POT, or 35% increase overall (Table5).

Among the 4469 participants in the intention-to-treat cohort, AT had the largest PrBest in 595, BI in 594, BA + CR in 1139, BA + PS in 1296 and BA + AT in 845 according to the POT algorithm. 595 for whom AT had the largest PrBest were randomly allocated to BA (65), to CR (56), PS (58), AT (55), BI (62), BA + CR (60), BA + PS (69), BA + AT (56), BA + BI (61), and HI (53). Blue cells then represent those allocated to their POT, red cells those allocated to the group average best, and the purple cell those whose POT was BA + CR and happened to be randomly allocated to BA + CR. AT assertion training, BA behavioural activation, BI behaviour therapy for insomnia, CR cognitive restructuring, HI health information, POT personalised & optimised therapy, PS problem solving.

† Estimates based on the MID (multiple imputation then deletion) model. The values may be slightly different from those obtained in the.

BAbehavioural activation,CRcognitive restructuring,HIhealth information,PHQ-9 Patient Health Questionnaire-9,POTpersonalised & optimised therapy.

This 35% increase is an underestimate for those for whom the group average best treatment may not be their best fit, because both the denominator and the numerator included 125 whose recommended treatment was BA + CR and who happened to be actually randomised to BA + CR (the purple cell in Fig.2). If we exclude the latter people (hence limiting the comparison to those whose POT is not BA + CR), the difference in observed PHQ-9 change scores from baseline to week 26 over the HI control grew from −0.78 (−1.34 to −0.22) on the group average best to −1.29 (−1.83 to −0.74) on the POT, or 64% increase. This increase corresponded with an effect size of −0.13 (−0.28 to 0.02,p= 0.090).

Using data from the largest individually randomised trial of smartphone CBT, which compared five different behavioural and cognitive skills and their combinations, and which had measured a rich array of variables at baseline and achieved satisfactory follow-up of the cohort through 26 weeks, we built prediction models that identified PFs and EMs through LASSO and estimated ITE (individual treatment effect) for each individual on each treatment. The models showed little bias, good discrimination and calibration, albeit with relatively large random errors. Applying these models and choosing the treatment with the largest probability to be the best treatment for each individual, we constructed the POT algorithm, which would recommend AT or BI for those with baseline PHQ-9 scores of 4 or less and BA + CR, BA + PS or BA + AT for those with baseline PHQ-9 scores of 5 or more. A simulated randomised comparison demonstrated that this POT algorithm can lead to an increase by 35% in the effect of the smartphone CBT for the entire cohort, in comparison to allocating the group average best treatment to everyone.

The construction of such prediction models was possible because we had measured a rich array of baseline variables. Currently, there is growing emphasis on the individual participant data (network) meta-analysis to construct personalised treatment6,7. They allow the accumulation of a large sample of participants, which would usually not be possible with individual trials, and should increase the statistical power to identify PFs and EMs. However, pooling data from different trials, each of which had its own study hypothesis and measured different variables, often results in smaller sets of variables measured in common across the included trials, as if the more trials you have, the less baseline variables you have.

In the current trial, we measured over 30 baseline variables with validated scales, as informed by the review of the relevant literature18. However, when we examine the selected predictors (Supplementary Tables3,4,5), we note that (i) many variables remained PFs but (ii) relatively few emerged as EMs.

Several EMs differed among the interventions and therefore led to the recommendation of different interventions for individuals. Baseline or early PHQ-9 scores were not only consistently strong PFs across the prediction models but were also weak EMs for many interventions. This finding is consistent with prior studies6,19. It is interesting to note that this effect modification was mostly negative, meaning that the higher the baseline depression, the more effective the intervention was. However, for AT it was positive, meaning that AT may be more effective for people with lower baseline depression. The recommended POT algorithm reflected this trend.

It is encouraging that this relatively small number of variables could form prediction models with little bias and robust discrimination and calibration. At the same time, the current models still showed large measurement errors. The current findings suggest that we could drop some of the baseline questionnaire and increase the accessibility and feasibility of the programme. They also mean that we should introduce some completely new baseline variables in future studies, test out if they acted as PFs or EMs, and eventually construct a more powerful prediction model.

Our POT algorithm used the bootstrap estimates of the probability as the best method to select the optimal intervention for each individual. In the preliminary analysis phase, we used the predicted value to select the best treatment. However, this approach led to disappointing results when we validated the algorithm in the simulated RCTs, presumably because this approach did not factor in the imprecision in the estimations. The probability of being the best reflected precision of the estimated effects. Even with this approach, however, limiting the final selections to the better interventions resulted in greater improvement than letting each individual use their idiosyncratic treatment with the highest PrBest. In other words, it appeared rare that, if the best treatment was among the exceptional ones (e.g., PS in Supplementary Table6according to Model 1, which was judged to be the best treatment for only 5% (236/4469)), it actually led to worse results when they were compared in a simulated RCT using actually observed outcomes. Exceptionally good options for a small portion of the sample appeared spurious, at least in the current prediction models.

The augmentation by the POT algorithm over the group-average best treatment (BA + CR) was 35% for the entire cohort, and 64% for those who were judged to do better if they received treatments other than BA + CR. These increases corresponded with SMD differences of 0.10–0.13 and may not be transformative. However, given that the recommended POTs (AT, BI, BA + CR, BA + PS or BA + AT) are at most as effortful as, and potentially less so than, the one-size-fits-all recommendation of BA + CR for all, we judge that the POT algorithm may well be clinically valuable, if confirmed in an independent randomised trial. The current algorithm suggests that people with higher levels of depressive symptoms benefit from learning more skills but goes beyond this intuitive rule by indicating specific single skills or specific combinations of skills for particular persons with particular constellations of symptoms and characteristics.

The benefit of the POT algorithm for smartphone CBT is that it can be implemented within the app. The baseline questionnaire takes 15–20 min to complete on the app. The algorithm itself remains a black box but the app can explain why specific skills were chosen for each person by identifying the individual’s strengths and weaknesses that may be addressed through learning those particular skills.

Several caveats are in order before we can confidently apply the current findings to the community and scale up the intervention. First, the final POT appeared to increase the effect size by 35 or 64% over the policy to administer the group average best intervention to everyone. The estimated improvement is internally cross-validated through 10-fold cross-validation in each of the training sets. However, the 95% confidence interval of the SMD between the two policies included zero, and the external validity of the model was inferred only through internal-external validation. The actual benefits of the POT algorithm need to be confirmed in a new randomised trial that compares it with the group average best. Second, the current optimisation is concerned only with the very early phase of the intervention, up to week 2. However, in the daily life, it is to be expected that unexpected things will happen. We will need longitudinal optimisation, such as modifying the intervention when the participants are not on track20, when they do not respond, or when they do respond but are still at risk of relapses, to further augment the benefits of the interventions. Third, the current trial only dealt with variations within smartphone CBT. POTs involving other schools of psychotherapies are desirable. Given the practical challenges in obtaining sufficiently large samples to develop reproducible prediction models, empirical research may necessarily focus on scalable interventions. Lastly, the RESiLIENT trial did not include participants with moderate to severe depressive symptoms, especially if they had suicidal ideations. We need a new study to establish the POT algorithms for these people.

In conclusion, large and rich randomised data for the five CBT skills led to the construction of the POT algorithm for iCBT, which recommends specific skills in accordance with an individual’s baseline characteristics and early treatment responses and which may be able to increase the efficacy of the intervention by 35% for the entire cohort over the group-average best intervention. This is the first precision medicine algorithm to recommend the optimal treatment for individuals from among many treatment alternatives, each of which has been shown to be more effective than the control conditions. It is time to test out this POT algorithm in a newly planned randomised trial to confirm its external validity and applicability.

The protocol paper16and the primary analysis paper17provide detailed information about the RESiLIENT trial (UMIN Clinical Trials Registry UMIN000047124, registered on 1 August 2022). The study was approved by Kyoto University Ethics Committee (C1556) on 3 March 2022.

We recruited participants aged 18 years or older, of any sex, with the screening Patient Health Questionnaire-9 (PHQ-9) total scores of 0-14 inclusive but excluding those with total scores 10–14 and with suicidality (scoring 2 or 3 on its item 9). Scores of 0-4 on the PHQ-9 represent no to minimal, 5–9 mild, 10–14 moderate, 15–19 moderately severe, and 20–27 severe depressive symptoms.

The RESiLIENT trial allocated participants to the following 12 arms in equal proportions. The Resilience Training App, developed for this trial (Supplementary Fig.1), provided each arm as a six-week programme.

BA (Behavioural activation): BA consists of psychoeducation about pleasurable activities and recommends personal experiments to test out new activities to see if they can bring some pleasure and/or sense of mastery.

CR (Cognitive restructuring): CR teaches participants how to monitor their reactions to situations in terms of feelings, thoughts, body reactions and behaviours by filling in mind maps. The participant uses these mind maps to find alternative thoughts.

PS (Problem solving): PS teaches participants how to solve a problem at hand through structured steps.

AT (Assertion training): AT teaches the participant concrete methods to express their true feelings and wishes without hurting others or sacrificing themselves.

BI (Behaviour therapy for insomnia): BI invites the participant to keep daily sleep records, based on which the participant would start applying sleep restriction and stimulus control techniques.

The original master protocol consisted of four 2 × 2 factorial trials16. In this study, however, we put together data from the nine active arms and one control arm to build prediction models and POT algorithms, because the clinical question for this secondary analysis was which of the nine active interventions were to be recommended in the POT algorithm. For the control condition we used the HI arm, because we reasoned that this would represent the “informed, health-conscious life as usual” and is arguably the most reasonable counterfactual condition against which we would like to know the added values of learning some cognitive or behavioural skills17.

All the assessments in this study have been completed by the participants on the app.

We measured the following baseline variables as potential PFs and EMs for iCBT skills, based on the literature and expert opinions18.

The outcome measure of interest in this study was the change in PHQ-9 scores from baseline to week 26.

Participants in the intervention arms filled in the PHQ-9 at weekly intervals between week 0 and week 6, and then every four weeks between week 6 and week 26. We chose week 26 as the target for optimisation, because the previous analyses found the effects of the interventions were maintained up to week 2617. When considering potential second-line interventions for individuals who do not respond to optimised treatments, week 26 represents the most reasonable and practical timing, and therefore a good target date for effect maximisation.

We asked interested potential participants to log onto the web specifically prepared for the trial and fill in the screening questionnaire. Once we ascertained the minimum eligibility criteria, we invited all those scoring 5 or greater and one in ten of those scoring 4 or less on the screening PHQ-9 to the online informed consent session. Once individuals have agreed and signed their digital consent forms, we allow them access to the app by using their ID and password. Consenting individuals then had to complete all the baseline questionnaires within one week. After they completed all the baseline questionnaires, the app automatically randomised the participants to one of the ten intervention or control arms, stratified by the PHQ-9 scores (4 or less vs 5 or more) and their employment status (yes vs no), using the pre-installed permuted block randomisation sequence prepared by an independent statistician. Due to the nature of the intervention and the study procedure, neither the participants nor the trial management team were blinded to the intervention.

We used mixed-effects models for repeated-measures (MMRM) to estimate the least-squares means of change scores on the PHQ-9 for the nine interventions and the HI control up to week 26. The model included fixed effects of treatments, week (as categorical), and treatment-by-week interactions, adjusted for baseline PHQ-9 scores, stratification variables, age and gender. We adopted the unstructured correlation structure for the within-participant correlation coefficient matrix.

We developed prediction models for predicting change scores of PHQ-9 at week 26 and individual treatment effects (ITEs) for the nine treatment regimens against the HI control.

An alternative strategy to improve the prediction is to add early post-randomisation variables to the prediction models (Model 2). Particularly, information about early responses to treatment has been shown repeatedly to improve the prediction performances39,40. In this study, four of the nine potential treatment alternatives (BA + CR, BA + PS, BA + AT, BA + BI) used the same component up to week 2. We could therefore add the PHQ-9 scores at weeks 1 and 2 as predictors for the week 26 outcome to recommend one of these four alternatives in the POT algorithms.

To evaluate the performance of alternative prediction models, we performed an internal validation using 10-fold cross-validation and calculated predictive accuracy measures including R2, R, bias, mean absolute error, root mean squared error, calibration-in-the-large, and calibration slope42.

Our multivariable prediction models incorporated a large set of covariates and their interactions with the treatments. To handle this large number of parameters, we need effective variable selection and adjustment of the model complexity43. We therefore tried various machine learning approaches, including RIDGE, LASSO, elastic-net, support vector machine, and causal forest, for the predictive analyses. We compared the internal validity via ten-fold cross-validation of these approaches for Model 1, and found that the LASSO regularisation approach performed the best for the RESiLIENT trial (Supplementary Table9). In the following analyses we therefore adopted this approach for the primary analyses44.

There was little missing data among the baseline covariates (47 (1.1%) for exercise habits, 1 (0.0%) for number of cohabitants, 1 (0.0%) for education) but there was more missingness for weeks 1 and 2 PHQ-9 scores. Moreover, for the HI group, PHQ-9 scores were not measured at weeks 1 and 2. In addition, 443 or 9.9% of the intention-to-treat cohort (n= 4469) did not report their week 26 outcome.

We used multiple imputation using repeated measurements of the PHQ-9 between week 1 and week 22 as auxiliary variables to impute week 26 outcomes (for all models) as well as weeks 1 and 2 outcomes (for Models 2 and 3). We used the chained equation approach to generate imputation data45and performed 100 imputations for all of the analyses.

To evaluate the generalisability of the model, we conducted internal-external cross-validation that splits the overall population via non-random mechanism, incorporating some characteristics of external validation43. In the RESiLIENT trial, we recruited participants from various sources, and they differed in participant characteristics, measured as well as unmeasured, from one source to another. Typically we advertised our trial at new sites each month, and the recruitment from each site lasted for one to two months. In other words, participant characteristics changed every month. We therefore deemed it possible to test the internal-external validity of our model by dividing the sample into ten temporarily consecutive subsamples.

To select the best treatment for each individual, we could use the LASSO-based ITE estimates of individual participants when they received each of the nine interventions in comparison with the control46. However, the point estimates of ITEs do not reflect precision information, and relying solely on the ITE point estimates can lead to suboptimal treatments47. To incorporate the statistical errors of information around ITE point estimates, we calculated the probability to be the best treatment (PrBest) for each of the nine treatments for each participant. The PrBest is estimated as a relative frequency for the ITE estimates to be the largest among the nine treatments on the bootstrap replications (we performed 1000 replications) for each participant.

We then constructed three tables of POTs for participants with baseline PHQ-9 scores ≤4 and ≥5 separately, using the prediction model with baseline variables only (Model 1) or the prediction model with data up to week 2 (Model 2 or 3).

We then compared the week 26 outcomes among (i) those who had been randomly allocated to the HI arm in the original RESiLIENT trial, (ii) those who had been allocated to the group average best arm, and (iii) those who had been allocated to their POT arm, and calculated the improvement in the week 26 PHQ-9 change scores as (iii) – (i) / (ii) – (i).

Because in the RESiLIENT trial allocation to (i) through (iii) is randomised, these comparisons can be considered a simulated randomised trial. We could use information up to week 2 (Model 2 or 3) and still maintain randomisation if we used such models to choose only from among two-skill interventions because all two-skill interventions used the same BA lessons up to week 2. Because we used the 10-fold cross-validation framework to calculate the PrBest, in which we determined the best treatment for each individual as the treatment that has the largest PrBest among the nine treatments according to the model developed with the training set, these comparisons are corrected for optimism. We avoided data leakage because we conducted the tuning of LASSO prediction models with 10-fold cross-validations in the training sets. All missing data were addressed by multiple imputations in the model development phase, and we adopted the multiple-imputation-then-deletion (MID) strategy in these analyses48. We used robust variances to calculate confidence intervals of the effect sizes due to the potential heterogeneities.

We restricted the interventions to be included in the POT algorithms in several patterns. We determined the final POT algorithm, taking into consideration not only the increase in the total efficacy by administering the interventions as per the POT algorithm over administering the group-average best for everyone in the group, but also the feasibility and simplicity of the interventions to be recommended. Simplicity and feasibility are important considerations for scalable interventions.

We used the R packagesglmnetver. 4.1-8 for RIDGE, LASSO, and elastic-net,e1071ver. 1.7-16 for support vector machine,grfver. 2.4.0 for causal forest, andmicever. 3.17.0 for the multiple imputation analyses.

This study was supported by the Japan Agency for Medical Research and Development (grant number JP21de0107005), awarded to T.A.F. (principal investigator), A.T., R.T., M.S., L.Y., M.H., T.A., N.Ka., T.N., N.Ko. and S.F. The funder had no role in study design, data collection, data analysis, data interpretation or writing of the report.