Ovarian hyperstimulation syndrome (OHSS) is a serious complication of controlled ovarian stimulation (COS). The main clinical manifestation of OHSS is increased ovarian volume. OHSS can cause local and systemic tissue oedema, electrolyte disturbances, cardiorespiratory dysfunction, coagulation dysfunction, and other symptoms. These symptoms greatly affect patients’ quality of life. As infertility rates rise and assisted reproductive technology (ART) becomes more common, the risk of OHSS increases. Therefore, early identification of high-risk patients and timely intervention are crucial.

The PubMed, Embase, Cochrane Library, Web of Science, CINAHL, China National Knowledge Internet (CNKI), Wanfang, China Science and Technology Journal Database (VIP), and China Biology Medicine (CBM) databases were systematically searched from inception to March 30, 2025. Two researchers independently screened the literature, extracted data, and evaluated the quality of included studies using the updated prediction model risk of bias assessment tool (PROBAST + AI). We conducted a meta-analysis of predictors from the developed models using Stata 15.0 software.

A total of 16 studies were included, comprising 29 OHSS risk prediction models. The area under the curve (AUC) ranged from 0.628 to 0.998, with 23 models demonstrating AUC > 0.700. Model calibration was performed in 10 studies, internal validation in 14 studies, and 2 studies conducted both internal and external validation. The PROBAST + AI assessment identified a high risk of bias across the included studies, primarily in the research design and statistical analysis domains. The most common predictors identified across the models included: antral follicle count (AFC), estrogen (E2) levels on the day of human chorionic gonadotrophin (hCG) injection, number of oocytes retrieved, polycystic ovary syndrome (PCOS), age, anti-mullerian hormone (AMH), gonadotropin (Gn) days, initial dose of Gn, and body mass index (BMI).

Our findings indicate substantial variation in OHSS incidence. Interpretation of the results should be with caution due to the limitations of the current evidence. Current OHSS risk prediction models remain under development and require further refinement. Future efforts to build and improve these models should focus on key areas, including research design, sample size, handling of missing data, model calibration and validation, and detailed reporting.

The online version contains supplementary material available at 10.1186/s12884-025-07971-9.

OHSS is one of the most serious complications of ART. The core pathophysiological mechanism involves vasodilation of small arteries and increased capillary permeability, which trigger massive protein extravasation into the extravascular interstitium. This increases tissue colloid osmolality, resulting in fluid shift from the intravascular compartment to the interstitium with subsequent accumulation, ultimately manifesting as a constellation of clinical symptoms [1,2]. The diagnosis of OHSS is based on clinical manifestations and imaging. By severity, OHSS is classified as mild, moderate, or severe [2–4]. A Cochrane systematic review, which included a total of 27 reviews reporting on OHSS occurrence during ART cycles (involving 85,497 participants), showed an estimated prevalence of 20–33% in its mild form and 3–8% in its moderate or severe form [5,6]. OHSS is a self-limiting condition. Patients with mild OHSS generally do not require treatment, while most with moderate to severe OHSS require hospitalization. Studies indicate that these patients are more likely to develop gestational diabetes mellitus, gestational hypertension, preterm labor, miscarriage, preeclampsia, and thromboembolism. OHSS is also associated with increased incidences of fetal growth restriction, low birth weight, and intrauterine fetal death [7]. As the pathogenesis of OHSS remains incompletely understood, hospitalized patients primarily receive symptomatic management, which causes physical pain, increases time and economic burdens, and adversely impacts psychological well-being [3,8].

There can be controversy relative to the growing reliance on ART, underscoring the importance of mitigating associated risks. Identifying high-risk factors is critical for preventing OHSS. Although numerous OHSS risk prediction models have been developed and validated, significant heterogeneity persists in their predictive performance, generalizability, and accuracy, with a lack of systematic synthesis. This study conducted a comprehensive systematic evaluation of OHSS risk prediction models, assessing their methodological bias risk (using the updated PROBAST + AI tool) and applicability to target populations and clinical settings. The findings aim to guide clinical decision-making by facilitating early screening, prevention, and intervention strategies, while providing evidence-based recommendations for developing more robust and widely applicable prediction models in future research.

The key questions for the systematic review of risk prediction models were constructed using the PICOTS framework established by the Cochrane Prognosis Methods Group [9]. Population (P): Patients aged ≥ 18 years undergoing ART treatment; Index prediction model (I): OHSS risk prediction models that were developed, validated, or updated; Comparative model (C): None; Outcome (O): The predicted outcome is the occurrence of OHSS; Timing (T): Before, during, or after the ovarian stimulation cycle; Setting (S): Reproductive centers.

The systematic review and meta-analysis followed the checklist outlined by transparent reporting of multivariable prediction models for individual prognosis or diagnosis: checklist for systematic reviews and meta-analyses (TRIPOD-SRMA) guidance [10] (Additional file 1) and was registered in the PROSPERO database (ID: CRD420251025876).

The inclusion criteria of studies included the following: (1) Study participants were patients aged ≥ 18 years undergoing ART treatment; (2) Studies focused on the development, validation, or update of OHSS risk prediction models; (3) Studies explicitly describing the model construction, comparison, evaluation process, and statistical methods; (4) Study types including cohort studies, case-control studies, or cross-sectional studies; (5) Studies including at least two predictive factors. Exclusion criteria of studies included the following: (1) Studies analyzing risk factors without constructing a risk prediction model; (2) Unavailability of full texts or incomplete data; (3) Duplicate publications; (4) Non-English or non-Chinese publications; (5) Publications reported as titles, abstracts, letters, conference announcements, or intervention protocols.

Two researchers independently conducted a systematic literature search and imported the results into the EndNote X9 reference manager software to remove duplicates and streamline the screening process. After screening abstracts and titles, they examined full texts to identify papers that met the predetermined inclusion and exclusion criteria. Any disagreements between the two researchers were resolved through discussion with a third researcher until a consensus was reached.

A standardized data extraction form was developed based on the critical appraisal and data extraction for systematic reviews of prediction modelling studies (CHARMS) checklist [11] and implemented in Excel 2019. The extracted data included: the name of the first author, publication year, country, study design, study population, number of models developed, candidate variables, methods for handling continuous variables, sample size, number of outcome events, missing data and handling methods, modelling approaches, model performance, calibration methods, validation methods, model presentation format, total number of predictors, and final predictors. Literature screening and data extraction were conducted independently by two researchers, followed by cross-checking. Any discrepancies were resolved through discussion with a third researcher until a consensus was reached.

The methodological quality, risk of bias, and applicability of the included studies were assessed independently by two researchers using the updated prediction model risk of bias assessment tool (PROBAST + AI) [12]. PROBAST + AI is an upgraded version of PROBAST [13,14]. It allows all key stakeholders to examine the quality, risk of bias, and applicability of any type of healthcare prediction models and is applicable regardless of the modelling approach, prevailing statistical methods, or AI/machine learning techniques used for model development. The assessments were cross-verified, with any arising discrepancies resolved through discussion or adjudication by a third researcher.

PROBAST + AI includes four core domains: participants and data sources, predictors, outcome, and analysis. It incorporates 34 signaling questions (16 for model development and 18 for model evaluation) alongside 6 applicability items (three each for development and evaluation phases). The first three domains utilize identical signaling questions to assess either the quality of model development or the risk of bias in model evaluation. All domains focus on concerns of quality and applicability (for model development) and concerns of risk of bias and applicability (for model evaluation).

As with PROBAST-2019 [13,14], signalling questions are answered as “yes”, “probably yes”, “no”, “probably no”, “no information”, or, when appropriate, “not applicable”. Quality concerns (for model development) are judged as “low”, “high”, or “unclear”, and the risk of bias (for model evaluation) is judged as “low”, “high”, or “unclear”.

All signalling questions are phrased such that “yes” or “probably yes” answers indicate high quality or low risk of bias, while “no” or “probably no” answers indicate potential quality concerns or bias. Subsequently, assessors need to use their judgment to determine the entire domain rating, assigning low, high, or unclear for either quality concerns (model development) or risk of bias (model evaluation). Crucially, a single “no” response does not automatically result in a high concern for quality or a risk of bias rating of the entire domain. The “no information” category applies exclusively when reported evidence is insufficient for assessment. The “not applicable” category may be available for items that are not applicable for certain types of prediction models or situations.

Applicability refers to either a study’s relevance to the reviewer’s question or a model’s utility for the assessor’s intended purpose. For instance, a model may demonstrate low quality concerns when data/participant selection aligns with the developer’s objectives, yet show high applicability concerns if these elements diverge from the reviewer’s implementation context.

The overall assessment synthesizes ratings across model development (quality and applicability) and model evaluation (risk of bias and applicability) to form three key judgments, each rated as low, high, or unclear. Specifically, a high overall concern rating for model development indicates low quality in the development process. A low overall risk of bias rating for model evaluation indicates that the reported model performance estimates are valid. A high overall concern rating for applicability (in both model development and model evaluation) indicates that the model’s applicability is limited or poor for the review question or the assessor’s intended use.

Statistical analysis was performed using Stata software (version 15.0). The data synthesis process was guided by several methodological reference articles, related guidelines, and existing meta-analyses of prediction model performance [15–21]. Odds ratios (OR) and 95% confidence intervals (CIs) of predictors in the studies were extracted, and studies with ≥ 3 predictors were meta-merged. Heterogeneity among studies was assessed using Cochran’sQtest and theI²statistic. A fixed-effects model was applied if heterogeneity was low (P> 0.1 orI²< 50%), while a random-effects model was adopted if significant heterogeneity existed (P≤ 0.1 orI²≥ 50%). To further evaluate the robustness of the research results, subgroup analysis and sensitivity analysis were conducted to identify sources of heterogeneity. Sensitivity analysis was performed using the one-by-one removal of individual studies method to examine whether any single study had a significant influence on the overall effect size. Since the number of included studies for each predictor in this research was less than 10, funnel plots and publication bias tests were not performed. AP-value of less than 0.05 was considered statistically significant.

A total of 7,328 articles were found through database searching. After removing duplicates, 4,536 articles remained. Two researchers independently screened the titles and abstracts of these articles, excluding 4,490 based on the predefined exclusion criteria. Full texts of 46 articles were assessed for eligibility, of which 30 were excluded for the following reasons: ineligible study type (n= 4), ineligible study population (n= 5), unavailability of full text (n= 6), and absence of model development/validation (n= 15). Ultimately, 16 studies were included (providing 29 models for analysis) in the systematic review and meta-analysis. The flow diagram of the search results is shown in Fig.1.

A total of 16 studies [22–37] were published between 2011 and 2025, with 11 articles published in the past 5 years. These studies originated from China (n= 13) [22,33,37], Denmark (n= 1) [34], Australia (n= 1) [35], and Iran (n= 1) [36]. The study designs included: prospective cohort studies (n= 2) [33,34], retrospective cohort studies (n= 7) [24,25,27–29,33,37], retrospective case-control studies (n= 5) [22,23,26,35,36], and retrospective studies (n= 3) [30–32]. Among the 29 models developed, Wei et al. [22] contributed 3 models, Mo et al. [23] 3 models, Fan [25] 2 models, Yu [26] 3 models, Cao et al. [28] 4 models, and Cao et al. [33] 4 models, while the remaining studies [24,27,29–32,33-37] each developed 1 model. The basic characteristics of the included studies are summarized in Table1.

The studies incorporated candidate variables ranging from 4 to 39, with sample sizes spanning 150 to 17,408 cases, outcome events varying from 15 to 4,143 cases, and OHSS incidence rates of 2.40–35.48%. For model development, logistic regression was applied in 12 studies [22,24–27,30-34,36,37], machine learning algorithms in 2 studies [23,26], stepwise regression in 2 studies [22,28], Lasso regression in 3 studies [22,28,29], and multivariate Poisson regression in 1 study [35]. Regarding missing data, 7 studies [26,28–30,33,35,37] reported handling missing values, including random forest regression (n= 1) [26], multiple imputation (n= 1) [33], mean value interpolation (n= 1) [37], and direct deletion (n= 4) [28–30,35], while the remaining studies [22–25,27,31,32,34,36] stated no missing data. Regarding model validation, two studies [26,33] performed both internal and external validation, 11 studies [22–25,27,28,30–32,34,37] reported only internal validation, and 3 studies [29,35,36] did not describe validation methods. Detailed information is provided in Table2.

The 16 included studies comprised 29 risk prediction models. All 16 studies reported discrimination metrics during model development and/or validation. The model by Cao et al. [33] presented the C-index, and La et al. [34] reported the Brier score. The AUC of the 28 models ranged from 0.628 to 0.998, with 23 models demonstrating AUC > 0.700. While this indicates favorable statistical discrimination performance in the reported settings, we acknowledge that the absence of external validation in many studies limits generalizability and could lead to overoptimistic performance estimates. Furthermore, although 10 studies [22,24,26–33] reported calibration assessments (using calibration curves, Hosmer-Lemeshow goodness-of-fit tests) and some included decision curve analysis (DCA), a more comprehensive evaluation of clinical impact or net benefit beyond DCA was limited across the included studies. We agree that discrimination and calibration are necessary but not sufficient to establish clinical utility, which depends on additional factors such as rigorous external validation and direct assessment of clinical impact. The model results were presented in various formats: nomograms (10 studies [22,24–27,29–32,37]), variable importance plots (2 studies [23,25]), risk equations (6 studies [24,25,31,35–37]), a web-based visualization calculator (1 study [26]), a combination of multi-stage nomograms and risk calculators (1 study [28]), a multi-stage risk scale (1 study [33]), and a risk chart (1 study [34]). Each model incorporated 1–11 predictors, ranked by frequency as follows: AFC, E2levels on the day of hCG injection, number of oocytes retrieved, PCOS, age, AMH, Gn days, initial dose of Gn, and BMI (Fig.2). Model performance and predictor details are provided in Table3.

The quality, risk of bias, and applicability of the included studies were assessed using the PROBAST + AI criteria [12]. Overall assessment results indicated that two studies [33,34] demonstrated high overall quality and low risk of bias. In contrast, the remaining studies were characterized by a high risk of bias and methodological flaws. While their clinical applicability (e.g., alignment of population and setting) was acceptable to some extent, it should be noted that the applicability of the study by Mo et al. [23] remains unclear. Given the significant risk of bias present in most models, the reliability of their clinical application remains questionable.

Detailed assessment of the quality dimension: (1) Participants and data sources: 14 studies [22–32,35–37] were rated as low quality. The main reasons included: (a) Retrospective, case-control, or cross-sectional designs resulted in a long time interval between data collection and the occurrence of the predicted outcome, affecting the accuracy and completeness of the predicted results. These design flaws increase the risk of measurement and recording bias [13]. (b) Five studies [22,23,26,35,36] utilized existing data, making the measurement process susceptible to interference from outcomes that had already occurred. (c) Most studies were single-center designs, with limited representativeness of the study population, thereby restricting the generalizability of the results (e.g., to different geographical regions or ethnic groups). (2) Predictor domain: 16 studies [22–37] were rated as high quality, with well-defined predictors, assessment methods, and availability in the intended clinical setting/application. (3) Outcome domain: 2 studies [23,35] were rated as unclear quality due to insufficient clarity in reporting outcome measure definitions. (4) Statistical analysis domain: Issues in this domain are most prominent. Only three studies [31,33,34] were rated as high quality, with the remainder classified as low quality. Core quality issues include: (a) The widespread issue of the minimum event count for each predictor variable being < 10, leading to a high risk of model overfitting and insufficient stability. According to PROBAST guidance, an events per variable (EPV) value of at least 20 per predictor is recommended to ensure model stability and reduce overfitting risk [13]. However, only 5 included studies met this criterion [25,30,31,33,35]. (b) Inappropriate handling of missing data—seven studies [26,28–30,33,35,37] reported missing data, with four studies [28–30,35] employing inappropriate handling methods (e.g., directly deleting samples with missing values without using reasonable methods such as multiple imputation), while the remaining studies did not report on data missingness or handling methods. Improper handling (e.g., direct deletion) can introduce bias and lead to model overfitting [38]. A detailed summary of the assessments is provided in Table4.

Beyond the quality domain assessment, methodological limitations and potential sources of bias include the following aspects: (a) Predominant reliance on logistic regression as the core modelling technique. This approach is susceptible to underfitting, struggles to capture complex nonlinear relationships, and requires data preprocessing to accommodate missing values. (b) Lack of reporting regarding standardized criteria used to assess the performance of the developed predictive models, which increases uncertainty in interpreting and comparing results. In summary, the retrospective design, single-center bias, statistical analysis flaws (low EPV, inadequate handling/reporting of missing data), and methodological constraints constitute significant sources of potential bias. These factors may compromise the accuracy, stability, and generalizability of the predictive models developed in the included studies.

This study identified 49 potential factors associated with OHSS in the included studies. Because four studies [22,23,30,34] did not reportORvalues and 95%CIsfor their predictive factors, factors investigated in these four studies were excluded from the meta-analysis. The selection process was as follows: For each potential predictor, if it was reported in at least 3 studies, a meta-analysis was performed to pool the effect sizes; predictors reported in fewer than 3 studies were deemed to have insufficient evidence and excluded. Following this exclusion and screening process, 9 predictive factors met the inclusion criteria for meta-analysis. However, high heterogeneity was observed (I²>50%), and sensitivity analyses were performed for each predictor. After sequentially removing each study, combined effect sizes were calculated, with no significant changes observed in effect sizes or heterogeneity. Among these 9 factors, AFC, Age, AMH, and FSH demonstrated statistically significant associations in the pooled analyses (P< 0.05). These results should be interpreted with caution. Detailed heterogeneity results are presented in Table5.

Due to limitations in the included studies, we attempted subgroup analyses for predictors with sufficient data (AFC, Age, and AMH) to investigate potential sources of heterogeneity. The results showed that no significant moderators were found for the AFC predictor. For the age predictor, the age ≤ 30 years subgroup exhibited extremely high heterogeneity (I²= 94.0%,P< 0.001), whereas the age > 30 years subgroup showed very low heterogeneity (I²= 0%,P= 0.762). For the AMH predictor, cohort studies demonstrated extremely high heterogeneity (I²= 98.4%,P< 0.001), while non-cohort studies showed low heterogeneity (I²= 0%,P= 0.401). These findings may stem from the limited number of studies within each subgroup and substantial methodological heterogeneity across the original studies (e.g., inconsistent definitions of OHSS, variations in biomarker detection methods). The persistent heterogeneity suggests that the presence of unmeasured clinical or methodological factors, or complex biological interactions, indicates that current evidence remains insufficient to explain between-study differences. This highlights the necessity of implementing standardized reporting frameworks and conducting Individual Participant Data (IPD) meta-analyses in future research. The results are illustrated in the additional File 3.

This systematic review, which included 16 studies (comprising 29 OHSS risk prediction models), revealed significant methodological limitations in current research. The included studies exhibited prevalent methodological deficiencies and an overall high risk of bias. Key issues include: (a) insufficient application of advanced modelling techniques; (b) development of models primarily based on retrospective, single-center data with a lack of rigorous external validation, severely limiting their external generalizability; (c) inadequate or incomplete reporting of data processing procedures (especially regarding missing data handling); and (d) insufficient sample size, compromising model stability and reliability. In summary, current research on OHSS risk prediction models demonstrates significant shortcomings in methodological rigor and adequate validation. Consequently, these models urgently require rigorous validation through prospective multicenter studies before clinical application, and require cautious interpretation and evaluation within specific contexts for any clinical decision-making.

A robust prediction model should undergo rigorous internal and external validation. To enhance model accuracy and generalization capabilities, future efforts should prioritize the use of prospective data, explore diverse modelling approaches (such as decision trees, artificial neural networks), enhance external validation, and reduce overfitting risks. The selection of appropriate predictors is crucial for achieving the best model fit and enabling accurate predictions [39]. Machine learning techniques—such as random forest, regularized random forests, support vector machine, Boruta, and gradient-boosted feature selection [40,41] are increasingly used for multi-factor risk screening and analysis. These methods can reduce the risk of overfitting. Furthermore, it is suggested that variable screening should integrate clinical practice considerations. Adopting such approaches can improve screening accuracy and enhance prediction performance. Future studies should adopt standardized methods to handle missing data. These methods include single imputation, multiple imputation, or model-based approaches (e.g., random forest regression). The processing procedures used must be reported in research papers. Adopting these practices will help reduce the adverse impact of missing data on the analysis [13]. In terms of performance assessment, consideration should be given to core indicators, internal and external validation, and overfitting and underfitting of the model. The core indicators include discrimination and calibration, and the discrimination is often expressed by the AUC value or C statistic; when the AUC is > 0.700, the model is more discriminative; the calibration is assessed by the Hosmer-Lemeshow goodness-of-fit test or calibration curves [13]. However, the P-value derived from applying the H-L test may be slightly biased in evaluating the calibration degree, and the Brier score is recommended. A lower Brier score (closer to 0) indicates higher calibration [42]. To standardize these assessment frameworks, the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD) published by Collins et al. [43] in 2015 provides comprehensive guidelines; it is recommended that TRIPOD’s rules be used for updating and validating relevant risk prediction models in the future.

The 16 studies included in this review had 1–11 predictors for each predictive model. Meta-analysis of predictors reported in ≥ 3 studies identified AFC, Age, AMH, and FSH as key influencing factors for OHSS occurrence (P< 0.05). However, these findings should be interpreted with caution due to limited studies within subgroups and significant methodological heterogeneity across original studies. AFC and AMH are ovarian reserve markers that can respond to exogenous Gn [44]. Studies have shown that AFC and AMH are among the best indicators for evaluating the sensitivity and specificity of ovarian hyper-responsiveness and OHSS risk [23]. AFC ≥ 24 is associated with an increased risk of moderately severe OHSS. In a prospective analysis of 1,012 first ART cycles, the risk of OHSS increased from 2.2% in women with AFC < 24 to 8.6% in women with AFC ≥ 24 [44]. Compared with patients with mild OHSS, patients with moderate OHSS had a higher mean AFC (22.80 ± 2.84 vs. 23.13 ± 2.42). Thus, knowledge of AFC levels is essential in planning and managing the risk of over-response and OHSS. Serum AMH concentration reflects the number of growing follicles and the ability of granulosa cells to produce AMH. Elevated AMH levels are associated with the cumulative effect of increased AFC, as well as the biological characteristics of ovarian granulosa cells (such as their ability to synthesize and secrete AMH) and an increase in AMH production by individual follicles [22]. Notably, although higher AMH levels indicate higher ovarian reserve function, abnormally high AMH levels often suggest a supraphysiological response of the ovary to ovulation-promoting drugs. This response may significantly increase the risk of OHSS.

OHSS is more frequently observed in younger women, particularly those with elevated AMH levels, higher serum E2concentrations, and a greater number of retrieved oocytes. Under equivalent Gn stimulation, younger women typically exhibit a higher density of Gn receptors (FSH/LH receptors) on the surface of follicular granulosa cells. This enhances follicular sensitivity to Gn, leading to more rapid follicular growth and an exaggerated ovarian response to the medication. Consequently, the risk of OHSS is significantly elevated [45]. Moreover, younger patients generally present with lower basal FSH levels, which further elevates OHSS risk [23]. Ovarian function is regulated by the hypothalamic-pituitary-ovarian (H-P-O) axis. Elevated basal FSH levels indicate diminished ovarian reserve and indirectly reflect reduced ovarian responsiveness [25]. Conversely, individuals with lower basal FSH typically demonstrate better ovarian reserve, higher AFC, and increased sensitivity to ovarian stimulation medications. These characteristics heighten the risk of OHSS during ART treatments. Consistent with this mechanism, OHSS incidence is significantly lower in populations with elevated basal FSH due to poor ovarian response [27]. It is suggested that future predictive modelling studies should focus on the above predictors, implementing early assessment and intervention for specific risk factors to reduce the occurrence of OHSS.

Among the included studies, some models demonstrated notable strengths in their methodological or applied aspects. Models developed by Yu [26] and Cao et al. [33] underwent both internal validation (random splitting) and external validation (independent cohorts), demonstrating superior generalizability. Models by Cao et al. [33] (prospective/retrospective cohorts) and La et al. [34] (prospective cohort) exhibited lower risk of bias and higher evidence quality. Fan [25] constructed models at two distinct timepoints: pre-stimulation and post-oocyte retrieval, while Yu [26] established a multi-phase modelling framework covering the pre-stimulation phase, the hCG injection day, and the pre-embryo transfer stage, enabling real-time clinical decision adjustment during treatment. Cao et al. [28] and Cao et al. [33] developed comprehensive multi-stage models spanning ovarian stimulation to embryo transfer, fully capturing dynamic OHSS risk evolution. Models by Mo et al. [23] (neural network) and Yu [26] (random forest/XGBoost) outperformed others in detecting complex interactions, showcasing enhanced nonlinear predictive capability. However, insufficient transparency regarding data details limits the clinical applicability assessment of Mo et al.‘s model [23]. In summary, the models developed by Cao et al. [28] and Cao et al. [33] are suitable for personalized dynamic assessment of full-cycle OHSS risk; those by Fan [25] and Yu [26] apply to risk stratification at critical decision points during ART treatment, while La et al.‘s model [34] is designed for baseline risk screening.

However, the practical implementation of these models still faces several challenges. Firstly, population heterogeneity limits model generalizability; among the 16 included studies, 13 focused on Chinese populations, while only three included populations from other regions/countries (Denmark, Australia, Iran). Racial differences may cause model calibration bias, restricting their applicability across diverse populations. Secondly, variations in clinical practice hinder the models’ universal applicability, manifesting in three key aspects: (a) Differences in ovarian stimulation protocols: While the included studies covered protocols like the long protocol and antagonist protocol, the specific drug brands and dosages lacked standardization. (b) Inconsistent definitions of OHSS: Varying diagnostic criteria for mild/moderate-to-severe OHSS affected the comparability of prediction outcomes. (c) Lack of standardization in biomarker testing: Insufficient reporting of the testing methods and reagent brands used for key predictors, such as AMH and E2,impedes cross-center application of the models. Thirdly, technical barriers impede clinical adoption: Eleven studies used nomograms to build their models, but only two [26,28] provided web-based calculators; the lack of digitization for most models constrains their efficiency in clinical practice.

Future model development should prioritize the following: (1) Adherence to standardized reporting guidelines by rigorously following TRIPOD statement requirements to enhance reporting details, and prioritizing the adoption of prospective cohort designs with large sample sizes combined with multi-center external validation. An example of this approach is found in Cao et al. [33]. This is crucial to mitigate the risk of measurement bias inherent in retrospective data. (2) Standardization of missing data handling, moving beyond the direct deletion of missing values (as seen in 7 studies that failed to address missing data), and instead recommending the use of multiple imputation or random forest imputation for data imputation. (3) Integration of dynamic predictive variables, including time-dependent factors such as the rate of change in E2on day 5 of ovarian stimulation, and implementation mechanisms for dynamic risk reassessment through multi-phase models, as exemplified by Cao et al. [33]. (4) For future research directions, efforts should focus on enhancing model generalizability through standardized reporting frameworks, multi-center collaborative networks, and real-time dynamic calibration. A key breakthrough area lies in the deep integration of machine learning models into clinical workflows.

The results of the analysis of the current study should be interpreted with caution because of the following limitations: First, this research field is still exploratory, with limited literature. Although some models are built on large datasets, most studies are single-center and retrospective. They vary significantly in target populations, predictor/outcome selection, and definitions. Predictor selection and modeling methods are often simplistic, and widespread clinical validation is lacking, limiting the models’ broad applicability. PROBAST + AI tool assessment indicates low overall study quality and a high risk of bias. Furthermore, the included data primarily comes from developing countries in Asia, with scarce data from developed countries, reflecting global disparities in healthcare resources and technology development. Finally, although significant heterogeneity was observed, the meta-analysis could not fully determine its exact source. This suggests the presence of unmeasured clinical or methodological factors or complex biological interactions.

Although these limitations should be carefully considered, their existence does not render the current systematic review and meta-analysis inappropriate. As the first systematic analysis focusing on OHSS prediction models, this study successfully summarizes the existing models’ limitations and areas for improvement. We rigorously screened literature based on inclusion criteria, and the analysis precisely highlights critical flaws in current OHSS prediction model research: the scarcity of high-quality studies, inadequate application of advanced modeling methods, lack of standardized reporting details, and the widespread absence of external validation. These findings are, in themselves, highly insightful. They provide crucial guidance and a reference basis for developing high-quality, generalizable prediction models in the future.

This systematic review and meta-analysis demonstrates that the risk of OHSS in ART is significant and cannot be overlooked; moreover, the performance of existing prediction models varies substantially. The analysis identified several factors associated with OHSS occurrence. Given the reality of uneven global distribution of healthcare resources, implementing tailored interventions across different regions is imperative to reduce OHSS incidence. This includes enhancing preoperative care to prevent severe complications and optimizing postoperative management. Future research should prioritize the inclusion of data from both highly developed and resource-limited countries to enhance the generalizability and robustness of the findings.

Below is the link to the electronic supplementary material.

We hereby thank and appreciate all the researchers whose studies were used in the present systematic review and meta-analysis.