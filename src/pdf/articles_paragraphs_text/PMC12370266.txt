Traditional Chinese Medicine (TCM) emphasizes the concept of medicine food homology (MFH), which integrates dietary therapy into health care. However, the practical application of MFH principles relies heavily on expert knowledge and manual interpretation, posing challenges for automating MFH-based dietary recommendations. Although large language models (LLMs) have shown potential in health care decision support, their performance in specialized domains such as TCM is often hindered by hallucinations and a lack of domain knowledge. The integration of uncertain knowledge graphs (UKGs) with LLMs via retrieval-augmented generation (RAG) offers a promising solution to overcome these limitations by enabling a structured and faithful representation of MFH principles while enhancing LLMs’ ability to understand the inherent uncertainty and heterogeneity of TCM knowledge. Consequently, it holds potential to improve the reliability and accuracy of MFH-based dietary recommendations generated by LLMs.

This study aimed to introduce Yaoshi-RAG, a framework that leverages UKGs to enhance LLMs' capabilities in generating accurate and personalized MFH-based dietary recommendations.

The proposed framework began by constructing a comprehensive MFH knowledge graph (KG) through LLM-driven open information extraction, which extracted structured knowledge from multiple sources. To address the incompleteness and uncertainty within the MFH KG, UKG reasoning was used to measure the confidence of existing triples and to complete missing triples. When processing user queries, query entities were identified and linked to the MFH KG, enabling retrieval of relevant reasoning paths. These reasoning paths were then ranked based on triple confidence scores and entity importance. Finally, the most informative reasoning paths were encoded into prompts using prompt engineering, enabling the LLM to generate personalized dietary recommendations that aligned with both individual health needs and MFH principles. The effectiveness of Yaoshi-RAG was evaluated through both automated metrics and human evaluation.

The constructed MFH KG comprised 24,984 entities, 22 relations, and 29,292 triples. Extensive experiments demonstrate the superiority of Yaoshi-RAG in different evaluation metrics. Integrating the MFH KG significantly improved the performance of LLMs, yielding an average increase of 14.5% in Hits@1 and 8.7% inF1-score, respectively. Among the evaluated LLMs, DeepSeek-R1 achieved the best performance, with 84.2% in Hits@1 and 71.5% inF1-score, respectively. Human evaluation further validated these results, confirming that Yaoshi-RAG consistently outperformed baseline models across all assessed quality dimensions.

This study shows Yaoshi-RAG, a new framework that enhances LLMs’ capabilities in generating MFH-based dietary recommendations through the knowledge retrieved from a UKG. By constructing a comprehensive TCM knowledge representation, our framework effectively extracts and uses MFH principles. Experimental results demonstrate the effectiveness of our framework in synthesizing traditional wisdom with advanced language models, facilitating personalized dietary recommendations that address individual health conditions while providing evidence-based explanations.

The concept of “medicine food homology” (MFH) in Traditional Chinese Medicine (TCM) means that certain substances can function as both nutritional food and therapeutic medicines depending on their application and dosage [1]. This ancient philosophy has gained considerable traction in modern health care systems [2], offering approaches to disease prevention and management through dietary interventions [3]. However, the practical implementation of MFH principles remains largely dependent on the expert knowledge of TCM practitioners, leading to inconsistent application and limited accessibility. The absence of automated and systematic approaches to using MFH principles for dietary recommendation generation has hindered its broader integration into contemporary health care frameworks and constrained its potential benefits for public health.

Recent advances in large language models (LLMs) [4] have demonstrated their remarkable performance across a variety of medical tasks [5], including automated clinical decision support [6], medical education [7], and health care information retrieval [8]. These capabilities are primarily derived from extensive pretraining on large-scale corpora, which encode rich knowledge from diverse textual sources. The application of MFH principles heavily depends on the domain knowledge documented in TCM literature, including foods, herbs, and formulations. Given that many LLMs have been pretrained on the corpora, which incorporate such TCM literature, combined with their strong natural language generation abilities, they present promising potential for automating MFH-based dietary recommendations. Nonetheless, LLMs exhibit substantial limitations as they often produce hallucinations that refer to factually incorrect information [9]. This problem is especially evident when generating information related to the medicinal properties of food in the context of TCM. For example, when querying with “I have stomach discomfort recently, can you recommend some summer drinks?” the LLMs respond: “Mung bean soup clears heat and is suitable for heat-related discomfort.” However, according to MFH principles, mung beans are cold (yin), and their consumption may aggravate stomach discomfort. Such inaccuracies significantly affect the safety and reliability of LLMs in practical health care applications [10].

Retrieval-augmented generation (RAG) has emerged as a promising strategy to mitigate these limitations by incorporating external knowledge into LLMs, thereby enhancing factual accuracy and reducing hallucinations [11]. However, using TCM knowledge as the retrieval source of RAG presents unique challenges. TCM knowledge is sourced from diverse and heterogeneous materials, including ancient and classical literature, modern TCM literature, food and herbs information, regulatory documents, research papers, and TCM formulations. This multisource nature results in inconsistent knowledge representation, terminological ambiguities, and varying levels of evidence quality. Furthermore, MFH principles inherently involve uncertainties arising from dosage effects, individual variability, and context-dependent applications [12]. Currently, there is no comprehensive and structured knowledge base specifically tailored for supporting RAG grounded in MFH principles, creating a significant gap between traditional wisdom and modern technologies.

However, we still do not have methods to integrate MFH principles with LLMs for dietary recommendation. Our framework bridges this gap by combining MFH KG with LLMs to develop more comprehensive and culturally informed dietary recommendations.

KGs serve as structured representations of factual knowledge, providing an excellent complement to parameterized language models [19,20]. With the emergence of LLMs, research has evolved along 2 complementary trajectories: leveraging LLMs to construct KGs and enhancing LLMs with KG integration. Several approaches have emerged for LLM-assisted KG construction. Extract-define-canonicalize [21] addresses scalability through open information extraction (OpenIE) and post hoc canonicalization. BEAR [22] leverages LLMs for zero-shot knowledge extraction in service-oriented domains. LLM-TIKG [23] applies few-shot learning for constructing threat intelligence KGs, while Yang et al [24] presented an approach for medical ontology expansion using LLMs to generate competency questions.

Researchers have also explored integrating KGs to enhance LLMs’ capabilities. GraphGPT [25] combines LLMs with graph structural knowledge through instruction tuning. GaLM [26] transforms KGs into text representations to improve reasoning while reducing hallucinations. Think-on-Graph [27] is a training-free framework that uses iterative beam search to enable interactive reasoning, while StructGPT [28] enhances zero-shot reasoning through an iterative Reading-then-Reasoning approach. Our work builds upon these dual research trajectories by proposing a framework that leverages LLMs for KG construction and uses KGs to enhance the capabilities of LLMs specifically within the domain of TCM, thereby providing reliable dietary recommendations grounded in MFH principles.

To overcome the limitations of existing methods, we propose a new framework that enhances LLMs’ capability to generate MFH dietary recommendations by using an uncertain knowledge graph (UKG) as an external resource. This framework uses multistep LLM calls for OpenIE [29] to automatically construct an MFH KG from unstructured and heterogeneous literature sources. It then implements UKG reasoning to measure the confidence of existing triples and complete missing triples. To improve knowledge retrieval, the framework adopts a reasoning path–based knowledge retrieval strategy, extracting relevant knowledge from the MFH KG and refining the results through ranking and filtering based on triple confidence scores and entity importance. By encoding domain-specific knowledge and addressing uncertainties related to MFH, the MFH KG empowers LLMs to deliver more personalized, evidence-based, and reliable dietary recommendations.

The key contributions of this study are as follows: (1) an integrated framework that leverages KG-augmented LLMs to generate personalized and evidence-based MFH dietary recommendations, (2) an LLM-driven OpenIE methodology for automatically constructing a UKG specifically tailored to TCM’s MFH from heterogeneous multisource data, (3) a systematic evaluation of several mainstream LLMs for the generation of MFH dietary recommendations, and (4) experimental results and case studies that demonstrate the capability of the proposed framework to deliver personalized and professional MFH-based dietary recommendations.

Knowledge graph construction: A comprehensive corpus was compiled from multiple sources, and an MFH KG was constructed using LLM-driven OpenIE. Then, UKG reasoning was applied to measure the confidence of extracted triples and complete missing triples.

Retrieval-augmented generation: Given a user query, the framework first identified and linked query entities to the MFH KG. The LLM then constructed relation paths to facilitate the retrieval of reasoning paths. Subsequently, these candidate reasoning paths were refined through postprocessing, including ranking and filtering, to extract the top-kmost relevant reasoning paths, thereby improving retrieval accuracy. The retrieved knowledge was integrated with the user query through prompt engineering, guiding the LLM to generate personalized dietary recommendations that aligned with both the user’s needs and MFH principles.

A comprehensive corpus was compiled from multiple sources, and an MFH KG was constructed using LLM-driven OpenIE. Then, UKG reasoning was applied to measure the confidence of extracted triples and complete missing triples. In the following sections, we provide a detailed explanation of both modules, outlining their design, implementation, and contributions to the overall framework.

To construct an MFH KG, we systematically constructed a corpus from diverse sources, including ancient and classical literature, modern TCM literature, food and herbs information, regulatory documents, research papers, and TCM formulations. This process results in a corpus of 1359 relevant documents.Table 1shows the statistical distribution of documents across different source categories.

Based on the collected corpus, we constructed an MFH KGG, as shown inFigure 2, which illustrates the detailed process. Formally, the graph is defined as a tripleG=(E,R,F), whereErepresents the set of entities,Rrepresents the set of relations, andF⊆E×R×Erepresents the fact set. Each fact inFis represented as a quadruple(h,r,t,s), wherehis the head entity,ris the relation,tis the tail entity, andsis the confidence score reflecting the likelihood that the triple(h,r,t)holds true. For instance, consider the quadruple (h: Insomnia and Vivid Dreams,r: Related Constitution,t: Yin-Deficient Constitution,s: 0.87). This quadruple indicates that Insomnia and Vivid Dreams are associated with the Yin-Deficient Constitution, with a confidence score of 0.87.

We used LLM-driven OpenIE to extract triplesh,r,tfrom each passage within the corpus. Specifically, we first integrated predefined relation definitions into a prompt (Textbox S2 inMultimedia Appendix 1), guiding the LLM to identify and extract potential relations. These candidate relations, along with existing ones, were subsequently processed using another prompt (Textbox S3 inMultimedia Appendix 1), where the LLM determined whether to retain new relations or filter out redundant ones. This process resulted in the construction of a relation setrsel, which was incorporated into OpenIE prompts, instructing the LLM to extract entity-relation triples. The relations of extracted triples were strictly constrained to the relation setrsel.

Specifically, during the OpenIE, we instructed the LLM to extract attributes of food entities and dish entities (Textboxes S4 and S5 inMultimedia Appendix 1). For food entities, LLMs referenced their source passages to update attributes, including name, primary effects, consumption methods, indications, and contraindications. Similarly, for dish entities, attributes such as name, ingredient composition, cooking method, and dietary benefits were updated.

The automated construction of the MFH KG inevitably introduced noise, incompleteness, and factual inaccuracies, making absolute factual correctness unattainable. To mitigate these issues, we adopted UKG reasoning to measure the confidence of extracted triples and complete missing triples. Specifically, we first used unKR [30], an open-source Python library for UKG reasoning, to learn vector representations of entities and relations. Based on these representations, we applied the triple confidence measurement method [31], integrating heterogeneous evidences to measure confidence of each triple. For each given head entityhand relationr, candidate tail entities were enumerated to construct new triples absent from the current graph. The triples with confidence scores larger than 0.85 were added to the MFH KG.

Given a user queryQ, we first constructed a prompt (Textbox 1) to guide the LLM to extract a set of relevant keywordsEfromQ. We then encoded both the keywords and all entities from the MFH KG into dense vector representations, denoted asHRandHG, respectively. Considering that SBERT [32] has demonstrated superior performance over BERT [33] and RoBERTa [34] on semantic textual similarity tasks while maintaining lower computational overhead [35], we adopted SBERT for embedding both the extracted keywords and entities. Subsequently, we computed the cosine similarities between the embeddings of each keyword and all entities. For each keyword, we retrieved the top-10 most similar entities from the MFH KG based on cosine similarity scores. These candidate entities were then presented to the LLM, which was instructed to identify the matched entity (Textbox 2). The selected entity corresponding to each keyword constitutes the query entity setEQ={e1,e2,…,en}, which serves as the starting point for subsequent reasoning path retrieval.

You are a medical knowledge assistant specialized in Traditional Chinese Medicine. Given a user query, extract a concise set of keywords that represent the user's core health-related intent and information needs. These keywords will be used to retrieve relevant knowledge from a medicine-food homology knowledge graph to answer the user query.

This study used path-based retrieval from the MFH KG rather than retrieving contextual subgraphs for linked entities. This is motivated by the advantages reasoning paths offer in generating MFH-based dietary recommendations. Specifically, reasoning paths provide structured knowledge by capturing direct relations between query entities and answer entities. For example, the reasoning path “(Insomnia and Vivid Dreams, Related Constitution, Qi-deficient Constitution); (Qi-deficient Constitution, Suitable Food, Codonopsis)”enables LLMs to clearly infer the relations between a user’s symptoms, their constitution, and the recommended food, thereby enhancing the interpretability. In contrast, subgraph-based retrieval in long-hop reasoning tasks often introduces irrelevant information [36]. This not only leads to significant computational overhead but also hinders LLMs from effectively extracting relevant information from the retrieval results.

Built upon the previous work [37], we first provided the LLM with a structured prompt (Textbox 3) listing all relations inG. The LLM then generated multiple relation paths{prel1,prel2,…,prelk}, where eachpreliis an ordered sequence of relationsr1,r2,…,rl. These relation paths serve as guides for retrieving knowledge essential to answering user queries. Given that the MFH KG contained considerably fewer relations compared with large-scale KGs such as FreeBase [38], we did not fine-tune the LLM for relation path generation. Instead, we used few-shot prompting to enhance the LLM’s ability to generate meaningful relation paths.

Please generate multiple relation paths (sequences of relations) based on the existing relation types. Each relation path should contain relations that are connected through the same entity.

The purpose of the relation paths is to assist in retrieving relevant entities that can answer the user query.

Subsequently, starting from each entity in theEQ, we followed each generated relation pathprelito retrieve a set of reasoning pathsPrea​={prea1,prea2,…,pream}fromG, where each reasoning pathpreai={(e1,r1,e2),(e2,r2,e3),…,(el,rl,el+1)}consists of multiple triplesej,rj,ej+1and represents an instance of one of the relation paths.

In our initial experiment, we noticed that the reasoning paths inPrea​contained redundant and irrelevant information. To enhance the quality of these reasoning paths, we computed a score for each reasoning path inPrea​by considering both the entity importance and the triple confidence within the reasoning path. These scores enabled us to filter out less relevant reasoning paths.

wherewi-1→irepresents the confidence score of the relation from entityeitoej,ndenotes the total number of entities in the path, andPR(ej)is the weighted PageRank score of entityejwithin the reasoning path. This scoring methodology captured both the confidence of triples within the reasoning path and the importance of the entities involved, striking a balance between path reliability and informational importance. Finally, we ranked the reasoning paths based on their scores and retained only the top-kreasoning paths for subsequent inference.

We used prompt engineering to improve the accuracy and relevance of generated answers of LLM. Prompt engineering involves constructing carefully designed prompts to guide LLMs toward generating high-quality and task-specific responses. Specifically, we constructed a prompt template (Textbox 4) comprising four key components: (1) task specification, (2) user query, (3) retrieved reasoning paths, and (4) attributes of food and dish entities. This structured prompt facilitates the integration of external knowledge, improves the LLMs’ comprehension of MFH principles, and supports the generation of more accurate responses.

You are a professional Traditional Chinese Medicine doctor, providing dietary recommendations using the concept of “medicine food homology.” Given a user query and the relevant reasoning paths and entity attributes, generate an answer.

Please provide a detailed, accurate, and concise response based on the information provided.

The response should be well-structured and ensure logical consistency with the given reasoning paths and entity attributes.

Incorporate medicine food homology principles in your recommendations.

Output format: {“answer”: “(Your answer here.)”}. The answer must be written in Chinese.

To identify the most suitable LLM for MFH dietary recommendation tasks, we evaluated several mainstream LLMs, including GPT-4 [40], LLaMA2-Chat-7B [41], Qwen2.5-7B [42], and DeepSeek-R1 [43]. Additionally, we considered the influence of model size, incorporating both large-scale models (GPT-4 and DeepSeek-R1) and 7B-parameter models (LLaMA2-Chat-7B and Qwen2.5-7B). During the experiment, we implemented the same prompt set across all models to ensure comparative evaluation under identical input conditions. All models were integrated with our proposed framework for dietary recommendation generation based on the MFH principles.

In this context,TP(true positives) represents the number of dietary recommendations correctly suggested,FP(false positives) indicates inappropriate dietary recommendations incorrectly suggested, andFN(false negatives) refers to appropriate dietary recommendations that the model failed to suggest.

While automated evaluation methods are widely applied in natural language processing tasks, they have certain limitations in the context of dietary recommendations. Automated metrics struggle to assess the effectiveness of recommendations, specifically whether the suggested food and their combinations align with MFH principles. Moreover, user acceptability and explainability of the generated dietary recommendations are difficult to accurately assess using automated methods. Therefore, we incorporated human evaluation, integrating subjective ratings from both experts and general users to provide a more comprehensive assessment of the generated dietary recommendations.

Among these evaluation criteria, rationality, explainability, user acceptability, and personalization were assessed using a 5-point Likert scale (1‐5, where 1 represented the lowest score and 5 represented the highest). Consistency was evaluated using a binary judgment (Yes=5points andNo=0points).

To conduct the evaluation, we recruited 5 experts with backgrounds in TCM or nutrition, along with 20 general users. Participants followed a structured human evaluation guideline to rate the generated dietary recommendations. The evaluation was conducted using a survey system, where assessors were provided with the user queries along with the corresponding dietary recommendations generated by LLMs. Finally, we computed the mean scores for each evaluation criterion.

Institutional review board approval was not required for this study, as it did not involve direct research with human participants. All corpora used, such as research papers and TCM formulations, were publicly available [44-46], and no identifiable private or sensitive information was accessed or recorded. Annotators involved in the human evaluation participated voluntarily, provided informed consent prior to participation, and were informed of their right to withdraw at any time. No compensation was provided, and no private information was collected.

We constructed an MFH KG comprising 24,984 entities, 22 relations, and 29,292 triples. To evaluate the LLMs’ performance in dietary recommendation tasks, we developed a specialized question answering dataset consisting of 2000 queries focused on MFH principles. This dataset was designed to ensure balanced coverage across diverse health conditions and TCM constitutional types.

Table 2shows a comparative analysis of LLMs with or with no augmentation by the MFH KG on dietary recommendation tasks. Overall, the integration of the MFH KG substantially improved model performance across all evaluated metrics, with an average increase of 14.5% in Hits@1 and 8.7% inF1-score, respectively. Among the baseline LLMs, DeepSeek-R1 achieved the best performance (Hits@1: 70.4%,F1-score: 64.1%), followed by GPT-4 (Hits@1: 68.6%,F1-score: 55.8%), Qwen2.5-7B (Hits@1: 58.7%,F1-score: 53.2%), and LLaMA2-Chat-7B (Hits@1: 54.1%,F1-score: 49.3%). The performance ranking remained consistent after MFH KG augmentation, with DeepSeek-R1 outperforming the others (Hits@1: 84.2%,F1-score: 71.5%). Due to its superior performance in the above automated evaluation, DeepSeek-R1 was selected as the LLM for all subsequent experiments.

Values in italics indicate the best performance for each metric.

In human evaluation, we conducted a comparative assessment between LLMs and MFH KG–augmented LLMs across 5 key metrics. As shown inTable 3, the MFH KG–augmented DeepSeek-R1 consistently outperformed the baseline DeepSeek-R1 model across all evaluation criteria. The MFH KG–augmented model achieved substantially higher scores in rationality (4.1 vs 3.2), explainability (4.3 vs 2.9), user acceptability (4.4 vs 3.6), personalization (4.2 vs 3.3), and consistency (4.6 vs 3.4). These results provide compelling evidence that augmenting DeepSeek-R1 with the specialized MFH KG significantly improves the quality of dietary recommendations based on MFH principles.

We analyzed the impact of parameterk, which controls the number of retrieved reasoning paths in the RAG module. The evaluation acrosskvalues [1, 3, 5, 10, 20, and 30] is shown inFigure 3. TheF1-scores indicate that small increases inkinitially improved performance, but after reaching an optimal point (k=10), further increases led to gradual accuracy decline. This pattern demonstrates the effectiveness of our reasoning path postprocessing, as it successfully prioritizes the most relevant reasoning paths while filtering out potentially misleading or irrelevant information.

To study the impact of the damping factord, where 1-dis the probability of randomly jumping to other nodes, we varieddwithin the range of [0.1, 0.9]. As shown inFigure 4, the performance of Yaoshi-RAG initially improved with increasingd, reached the peak (d=0.8), and subsequently declined. This trend aligns with the commonly adopted value (d=0.85) proposed by Page et al [47] and is consistent with previous findings [48,49]. Specifically, whendis too small, the resulting PageRank distribution is dominated by random jumps, which weakens the influence of the graph structure and yields less meaningful entity rankings. Conversely, whendapproaches 1, the distribution becomes nearly uniform, and the convergence of the power iteration method slows significantly, posing computational challenges.

We evaluated the effectiveness of Yaoshi-RAG’s retrieval strategy against multiple baselines. For sparse retrieval, we used BM25 [50] and followed DecAF [51] by transforming 1-hop subgraphs of topic entities into text. In KG embedding–based methods (TransE [52] RAG and RotatE [53] RAG), we ranked triples by embedding similarity. We also extracted the 2-hop subgraph to prioritize semantically relevant triples from query entities’ neighborhood. All baseline methods retained the top 30 triples as retrieval results.Table 4shows the performance comparison of different retrieval strategies. Yaoshi-RAG achieved superior performance with the highest Hits@1 (84.2%) andF1-score (71.5%).

Values in italics indicate the best performance for each metric.

To evaluate the impact of key modules in Yaoshi-RAG, we compared three variants: (1) a variant replacing the UKG with a deterministic KG, in which all triples were assumed to be fully reliable (ie, confidence scores were set to 1); (2) a variant removing the postprocessing of retrieval results, in which all retrieved reasoning paths from the UKG were directly input into the LLM without ranking or filtering; and (3) a variant randomly generating relation paths to guide the retrieval of reasoning paths.

As shown inTable 5, using a deterministic KG leads to a performance decline, highlighting the importance of incorporating uncertainty information derived from the UKG. Performance degrades further when postprocessing is removed, due to the inclusion of noisy and irrelevant reasoning paths. This confirms the effectiveness of leveraging weighted PageRank and triple confidence scores in identifying relevant reasoning paths. The variant using randomly generated relation paths for retrieval performs the worst among all settings, emphasizing the necessity of a retrieval strategy that generates faithful relation paths.

Values in italics indicate the best performance for each metric.

Textbox 5demonstrates an actual case of a dietary question and the answer generated by our proposed framework. The framework delivers a comprehensive response to a patient presenting peripheral heat sensations and sleep disturbances. By leveraging reasoning paths derived from the MFH KG, the framework establishes clear relations between reported symptoms, underlying constitutional patterns, and appropriate dietary recommendations. The framework also enriches its recommendations by incorporating detailed food entities’ attributes from the KG into the LLM, thereby enhancing the specificity of guidance regarding consumption methods and other relevant details. This case demonstrates how Yaoshi-RAG effectively translates MFH principles into clear and practical guidance, helping patients better understand the recommendations and adhere to the suggested treatments.

Lily, lotus seed, and white fungus soup: Efficacy: Lily calms the mind, Tremella nourishes Yin and moistens dryness, and lotus seed strengthens the kidney and astringes essence. Together, they create a “Metal-Water Mutual Generation” effect. Preparation: Soak 15 grams of Tremella in cold water until fully expanded, then tear it into small pieces. Add 30 grams of fresh lily and 20 grams of decored lotus seeds to 800 mL of water and simmer for 1 hour. Add rock sugar before serving. Best time to consume: Between 3 and 5 PM (Shen Hour), when the bladder meridian is active, enhancing Yin nourishment.

Mulberry and black sesame paste: Efficacy: Mulberries nourish the liver and kidneys, while black sesame replenishes blood and moisturizes dryness, making them an ideal pairing for dual liver-kidney nourishment. Preparation: Lightly toast 50 grams of black sesame over low heat and then blend it with 20 grams of dried mulberries and 30 grams of glutinous rice with 600 mL of water into a smooth paste. Best time to consume: About 2 hours before bedtime for a calming effect.

Dietary restrictions: Avoid spicy and heat-inducing food (eg, chili and Sichuan pepper), warming tonics (eg, lamb and longan), and stimulants such as strong tea and coffee.

Lifestyle recommendations: Prepare for sleep before 9 and 11 PM (Hai Hour). Practice the “Shaking Head and Wagging Tail to Reduce Heart Fire” movement from the Eight-Section Brocade (Ba Duan Jin). Engage in calligraphy or meditation to calm the mind.

The above plan should be followed continuously for at least 2 weeks to see effects. If diarrhea occurs, discontinue the use of Poria. For patients with diabetes, it is recommended to substitute rock sugar with xylitol. If symptoms persist, seek medical attention promptly for tongue and pulse diagnosis.

Evaluation results indicate that our framework improved the performance of all evaluated LLMs, showcasing its robust adaptability. High-performing models, such as DeepSeek-R1, exhibited marked improvements when enhanced by the MFH KG. This suggests that the MFH KG offers valuable context about ingredient relations, therapeutic properties, and dietary principles, which might not be fully captured during the models’ pretraining. Remarkably, even LLMs with fewer parameters showed performance improvement when integrated with the MFH KG. These results highlight the effectiveness of the proposed framework, which combines the contextual reasoning capabilities of LLMs with the rich and structured knowledge of KGs, making it a powerful tool for MFH-based dietary recommendations.

The human evaluation results revealed that the KG-augmented LLM demonstrated comprehensive improvements. The augmented LLM exhibited improved rationality through the relations between symptoms and dietary recommendations provided by the MFH KG. It also achieved superior explainability by integrating theoretical reasoning paths. Furthermore, the framework demonstrated increased user acceptability with more clinically applicable suggestions. The augmented LLM also maintained greater consistency across varying inputs. These findings collectively indicate that RAG can effectively enhance LLMs’ capabilities with domain-specific knowledge.

The parameter analysis and ablation studies validate the effectiveness of our postprocessing strategy applied to the retrieved reasoning paths. Retrieving more information does not necessarily lead to performance improvement and may instead introduce irrelevant or noisy information [54], thereby degrading the quality of recommendations. By incorporating triple confidence scores from UKG reasoning, the framework prioritizes informative reasoning paths for dietary recommendations. The performance decline beyondk=10 indicates that confidence-based ranking effectively distinguishes informative reasoning paths from less relevant ones. This underscores the importance of uncertainty modeling in TCM knowledge representation, particularly given the heterogeneous quality of evidence. By measuring confidence scores, the inherent uncertainty in MFH principles can be better managed, prioritizing reliable relations while mitigating the influence of less trustworthy ones.

Several limitations of this study should be acknowledged. The MFH KG was constructed using OpenIE, which may introduce inaccuracies and cannot cover all knowledge on MFH principles. Besides, we did not fine-tune the LLMs specifically for relation path generation, which may have constrained the retrieval effectiveness of our framework, particularly for complex cases involving multiple symptoms or conditions. Addressing these limitations in the future would likely further enhance the LLMs’ clinical uses and recommendation quality.

This study shows Yaoshi-RAG, a new framework that enhances LLMs’ capabilities in generating MFH dietary recommendations through the integration of a UKG. This framework uses multistep LLM calls for OpenIE to automatically construct an MFH KG, incorporating UKG reasoning to measure the confidence of existing triples and complete missing triples. Upon receiving user queries, the framework implements a reasoning path–based knowledge retrieval strategy, extracting relevant reasoning paths from the MFH KG and optimizing the retrieval results through a postprocessing mechanism. Experimental evaluations demonstrate that DeepSeek-R1 is the best-performing base model for MFH-based dietary recommendation generation. This framework facilitates dietary recommendations that adapt to individual health conditions and symptom requirements while considering diverse ingredients and dishes, accompanied by comprehensive explanations grounded in MFH principles. In the future, we plan to focus on implementing more effective knowledge extraction methodologies, expanding our KG through additional MFH literature, exploring optimized retrieval strategies, and fine-tuning open-source LLMs to further improve the accuracy and reliability of the generated dietary recommendations.

This paper is supported by the National Natural Science Foundation of China (grants 82104786, 62376058, 52378009, and 62276063), the Southeast University Interdisciplinary Research Program for Young Scholars, and the Big Data Computing Center of Southeast University.