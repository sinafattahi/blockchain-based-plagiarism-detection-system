To conduct a survey of radiotherapy linear accelerator quality control (QC) across the United Kingdom (UK) on behalf of the Institute of Physics and Engineering in Medicine (IPEM) Radiotherapy Special Interest Group and Interdepartmental Dosimetry Audit (IDA) Sub-committee. To update results from a similar survey published in 2012 and compare to the latest guidance from IPEM Report 81 (2018). There have been significant developments of equipment and clinical practice since the previous survey and IPEM publication, requiring an updated review and benchmark of QC practice.

All UK radiotherapy centres were invited to complete a comprehensive survey of their local QC practice, with questions on c-arm gantry, ring-gantry, linac ancillary equipment, and patient-specific QC.

63% (n= 43/68) of the UK radiotherapy centres responded. IPEM Report 81 was used to inform QC practice in 91% of centres. For the majority of tests studied centres were meeting or exceeding the recommendations of this report. Standard output was still performed weekly in 26% of centres compared to monthly recommendation in Report 81. Comprehensive tables of frequency and tolerances of QC tests were collated for c-arm and ring gantry linacs and ancillary equipment.

Updated data are presented on current practice for linac QC in the UK.

Quality control (QC) testing is an essential component of the system for assurance of accuracy and safety in radiotherapy. As the complexity of equipment and clinical techniques continue to evolve, it is essential that QC testing is optimized for maximum value and efficiency, while meeting safety requirements and assuring best achievable accuracy. Since it falls to the responsibility of the local Medical Physics Expert (MPE)1to decide the scope of QC testing in radiotherapy departments, it is particularly useful to have recommendations, guidance, comparative data, and surveys of peer practice when producing QC testing schedules.

Previous surveys of QC practice in the UK have been valuable, with positive responses from physicists, conducted between 1999 and 20122–4helping to shape and standardize practice and give confidence in approach to QC. In the United Kingdom (UK), the professional body for medical physicists, the Institute of Physics and Engineering in Medicine (IPEM), published guidance on physics aspects of QC in radiotherapy, Report 81, second edition, in 2018,5which also provided a valuable resource for design of QC schedules. However, the previous surveys and published professional guidance in the UK are now several years old and reference data may be in need of update, particularly to reflect changes in treatment technology and clinical practice; specifically, expansion of image guidance and online/offline plan adaptation, adoption of higher-precision techniques, ring-gantry linacs, and evolution of patient specific QC practice, initially for volumetric techniques (VMAT) and later for more complex stereotactic approaches (SABR).

The IPEM Interdepartmental Dosimetry Audit (IDA) Group on behalf of the IPEM Radiotherapy Special Interest Group, commissioned a survey of UK radiotherapy centres to establish current consensus practice for QC of radiotherapy linacs and associated equipment.

A survey questionnaire on QC practice of radiotherapy linacs, was sent to all UK radiotherapy centres in August 2024. Responses were collected up until ∼70% of centres had responded, which was through to November 2024.

The questionnaire was piloted at 6 UK radiotherapy centres, to determine whether questions were explicit and interpreted correctly. Responses from the pilot survey were used to refine the final set of questions and pilot centres updated their responses to align with this official version.

A comprehensive list of all QC tests undertaken across all the responding centres in the UK was collated. To manage the amount of data and improve readability, only those tests conducted in at least 5 centres for c-arm gantry linacs, and 3 centres for ring-gantry linacs (due to a smaller sample size) were analysed and presented in this report. This empirical approach to data presentation maintained a true reflection of the full data. QC tolerances can be expressed in different units, such as % or mm, therefore the data was separately analysed and presented for each unit reported.

The results of the QC survey are presented in 4 sections: QC management, frequency, tolerances, and patient specific QC (PSQC) considerations. The results for c-arm linacs are presented in this paper with the results for ring gantry and ancillary equipment presented in theSupplementary Material.

Forty-three UK centres responded to the survey providing comprehensive data on their QC schedules, procedures, and practice. Satellite centres were not separately invited but were asked to be included separately if QC processes differed to the lead centre.

The results are presented as a percentage of the total number of centres responding to the survey (n= 43 out of a possible 68) not as a percentage of centres responding to specific questions. Also, centres may have responded to more than one option in a question. Therefore, totals may be greater or less than 100%. Data from the previous survey4is also presented in this format (which differs from the original publication4) for ease of comparison. Survey responses that were unclear and could not be validated were not presented in the data tables.

Linacs manufactured by Varian Medical Systems (Palo Alto, CA) were installed in 58% of responding centres, with Elekta (Stockholm, Sweden) linacs installed in 37%, and linacs from both vendors in 5% of centres. The responses for Varian and Elekta linacs have been combined for this publication as there were no significant differences and to manage the size of data tables presented.

The stated main purpose of a QC schedule was to “demonstrate safe use of the machine” for 91% of centres. Other common themes were to “monitor performance” (21%), “prioritise measuring components that are known to drift” (21%), a “statutory requirement” (14%), and to “predict failing components” (12%). Only 5% of centres stated to test potential failure modes.

The periods within which linac QC is performed in UK centres is shown inTable 1(ordered by frequency). There is a wide variety in how QC is scheduled, with almost twice as many (26%) centres using a combination of early morning and normal working day for their QC, compared to centres using evenings and weekends (14%).Table 2presents a wide variation in the time required for performing linac QC at centres across the UK, with the total time, including offline analysis, ranging from 6 to 37 hours per linac per month.

Periods in which linac QC is performed in UK radiotherapy centres.

Duration of linac QC performed per linac per month in UK radiotherapy centres (it is unclear if all centres included daily QC in their total).

A service efficiency machine (SEM) was available in 16% of centres. 37% of centres stated they have a managed equipment service. When asked who provides the preventative maintenance inspection (PMIs) 42% of centres stated they had a partnership with the vendor, 28% stated in house and 26% stated vendor only. 19% of centres performed PMIs monthly, 21% quarterly, and 21% 3 times/year with an overall range of monthly to 6 monthly. The time taken for PMIs is not included in the QC time data (Table 2).

IPEM Report 81 (2018)5was stated as being used in 91% of centres to derive their QC schedule with 67% of centres stating experience is also used. Only 5% of centres mentioned near misses inform QC schedules. 26% of centres review QC schedules annually, 26% stated with the introduction of new treatment techniques or equipment, and 23% stated when new guidance is published. 19% of centres stated they have no formal review schedule and 14% of centres stated they continually review. 12% of centres stated they had not completed a review since the publication of IPEM Report 81 (2018).5Only 1 centre stated a complete change of process moving from IPEM Report 81 (2018)5approach to a failure mode and effects analysis (FMEA) approach. 51% of centres stated their QC schedule is in need of update.

86% of centres stated they have improved the efficiency of their QC approach: 16% by reducing frequency, 16% by moving to online analysis, 16% by reordering tests. Half of centres stated they undertake the right amount of QC, 16% stated too much, 5% felt too much but uncomfortable reducing below IPEM Report 81 (2018)5recommendations and 5% stated too little.

Software was used in 2/3 of centres for recording QC results: most commonly QATrack+ (RADformation) (26%). Local spreadsheets and databases were used in 53% of centres and only 7% of centres reported they still use paper records for some or all their QC. Stated benefits of electronic recording included improved trending (47%), identification of out of tolerance results (28%), remote access (19%), and reduced human error (19%). One centre stated that trending of the results in this way helped to identify degradation of the target before breakdown.

Full completion of planned monthly QC was reported in 47% of centres, 35% stated above 95% completion, and 9% reported less than 75%. 37% of centres stated they routinely record compliance data as a key performance indicator, 49% of centres stated they do not, and 5% stated not currently but soon.

A linac would be removed from clinical use to complete monthly QC in only 19% of centres, and of these, half had a SEM. 44% stated they may remove functionality from a machine to return to use if QC had not been completed, in particular this related to electron energies being taken out of service.

IPEM Report 81 (2018)5was used to determine the frequency of performing QC in 86% of centres, 84% stated local experience/past trends/reviewing trends, 26% stated AAPM 142 Report6and 26% risk assessments. Most centres reported using published guidance as a starting point and making centre specific modifications from local experience and trending. 7% stated frequency is based upon what is practically achievable, only 1 centre stated using FMEA.

The frequency at which linac QC measurements were made at UK centres responding to the survey is presented inTable 3for conventional c-arm gantry linacs. The data shows the current survey results compared to IPEM Report 81 (2018) recommendations5and the previous UK survey results.4Data for ring-gantry linacs and ancillary equipment are presented in theSupplementary Material.

Frequency at which QC is performed at UK centres, for c-arm gantry linacs.

D, daily; W, weekly; M, monthly; Q, quarterly; 6, 6 monthly; A, annually; C, commissioning; R, repair.

CBCT = cone beam computed tomography; DMLC = dynamic multi leaf collimator; DRGS = dose rate gantry speed; F&S = flatness and symmetry; FSD = focus to skin distance; FFF = flattening filter free; HU = Hounsfield unit; kV = kilovoltage; MLC = multileaf collimator; MU = monitor unit; MV = megavoltage; PDD = percentage depth dose; TPR = tissue phantom ratio; VMAT = volumetric modulated arc therapy.

As close to as possible previous survey results included in table. Underlined text indicates the most common value for each parameter.

The survey used a definition of “notification” level, being the ideal operating performance above which investigations and rectification would be planned, and “suspension” level at which equipment is likely to be removed from clinical use. Terminology varied between centres with over 15 variations.

Published guidance was used in 95% of centres to determine the notification and suspension levels for the QC tests. 74% of centres also stated that tolerances were locally derived based on experience of expected machine performance. 49% of centres stated these tolerances are regularly reviewed and updated.

The variation of tolerance levels for QC tests is given inTable 4for c-arm linacs. Ring-gantry linacs and linac ancillary equipment is presented in theSupplementary Material. The modal (most frequent) tolerance values are given, with multi-modal result if appropriate, in the formatn=x/ywherexis the numbers of centres reporting modal value andythe total number of centres responding to each question in the stated units. "No consensus" is stated where no mode in the data and “functional” includes similar wording, eg, working, pass, on, yes/no.

Notification and Suspension tolerance levels for QC results at UK centres, for c-arm linacs.

n=x/ywherexis the number of centres reporting modal value andyis the number of centres who responded to the question in the stated units.

0.5% variation between independent measurements and 1% from reference value.

The survey question was ambiguous as to whether “through leaves” meant “between leaves” or “below leaves” and centres may have answered differently.

No unexpected image artefacts/no uncorrected defective pixels.

CBCT, cone beam computed tomography; DMLC, dynamic multi leaf collimator; DRGS, dose rate gantry speed; F&S, flatness and symmetry; FFF, flattening filter free; FSD, focus to skin distance; HU, Hounsfield unit; kV, kilo voltage; MLC, multileaf collimator; MU, monitor unit; MV, megavoltage; PDD, percentage depth dose; SABR, stereotactic body radiation therapy; SRS, stereotactic radiosurgery; TPR, tissue phantom ratio; VMAT, volumetric modulated arc therapy.

Independent monitor unit (MU) checks or point dose calculations were used in 41% of responding centres. The most commonly used software was RadCalc (Lifeline Software Inc., Tyler, United States) (35%), DoseCHECK (SNC, Mirion Medical, Florida, United States) (16%), and in-house solution (23%). 49% stated that an MPE would review failing plans to decide the course of action, 23% stated this may include sending the plan for measured PSQC and 16% stated it may include using a different measurement point.

Independent 3D dose calculations were completed for all (treatment planning system) TPS plans in 28% of centres, 23% stated all VMAT/IMRT plans, other centres restricting to specific categories, eg, stereotactic ablative body radiotherapy (SABR) or flattening filter free (FFF). The most commonly used software was SNC Patient (30%) and RadCalc (26%). 49% stated an MPE would be involved in the decision of how to proceed with a failing calculation with 42% stating their decision likely includes sending the plan for measurement.

The proportion of patient plans undergoing measured PSQC varied considerably between centres, but in the majority, it was a relatively small percentage of the total number of plans. 14% of centres stated 10% of all plans have measured PSQC, 9% stated 5% of plans, the remaining centres estimated values in the range 1%-100% of plans. 49% of centres reported that plans undergo measured PSQC when new sites/techniques/prescriptions/class solutions have been implemented, fail software PQSC and/or are for SABR/SRS. Other common responses stated randomly sampled plans (30%), particularly complex plans (21%) and plans falling outside MU or MU/Gy limits (19%).

The most common equipment used for measurement was Delta4 (ScandiDos, Sweden) (40%), EPID panel (mixed vendors) (30%), point dose (26%), and ArcCHECK (Mirion Medical, Melbourne, United States) (23%). 91% stated gamma criteria is used for measured PSQC 60% of which stated 3%/3 mm >95% is used as the passing tolerance, it was not always specified whether this was using local or global normalization. SABR/SRS tolerances were typically tighter and more variable between centres, but modal response was 3%/2 mm with no specified pass percentage, and again was unclear whether this was using local or global normalization. For point dose measurements the most common pass criteria was 3% plan dose and 5% per beam. 60% stated that they deliver a set of reference plans for PSQC with centres most commonly stating they perform these checks monthly or 6 weekly. 23% stated they do not perform these plans and 2% stated they were planning on introducing it.

Failing measured PSQC results would have a review by an MPE in 91% of centres, with 35% stating they would consider a replan and 28% would remeasure often stating by an independent operator and on a different linac. Other common responses included, involving a clinician (19%) and relaxing tolerances (9%). However, 23% of centres reported zero plans are replanned per year due to failing measured PSQC results and a further 44% stated ≤5 plans per year. Other centres stated qualitatively very low or <1%. Three centres reported higher frequencies of replanning as a result of failed measurements:<5% (of 300-400 plans), 12 plans/year and 18 plans/year. Clinical scientists were the most common staff group to perform PSQC (63% of responding centres). 7% stated radiographers perform the portal dosimetry measurements.

In vivomeasurements were performed in 58% of centres. EPID panel was used in 37% of centres, entrance diodes in 30% and TLDs in 12%. Common reasons given by centres who do not performin vivomeasurements included; diodes being phased out, false negative/positive rate too high, rely on imaging instead, diodes not appropriate for VMAT techniques, risk assessments indicated able to reduce, rely on PSQC and routine QC instead. 14% stated that while they do not currently performin vivomeasurements they are in the process of commissioning EPID dosimetry.

End to end tests were performed in 49% of centres with 21% clarifying these are performed as part of commissioning new techniques/equipment/class solutions, after upgrades or during external audits. Only 5% stated they are routinely performed annually and 2% stated monthly.

Table 1indicates a shift towards more QC being performed outside of normal working hours (09:00-17:30) compared to the 2012 survey. Previously, 30% of centres performed all QC during the normal day,4which is now only undertaken in 12% of centres.Table 2highlights a large range in the time required for performing linac QC which potentially indicates large variations in the quantity, complexity, or efficiency of QC tasks between centres but could also be a result of some centres including daily run up tasks in this number.Table 2shows a large difference between the time required to perform QC of c-arm gantry linacs compared to ring gantry linacs, with the former reported to take twice as long (median values).

Nearly all QC schedules, 91%, were found to be derived from IPEM Report 81 (2018)5with changes based upon a centres own experience in 67% of cases. 100% of centres cited IPEM Report 81 (1999)2in the previous survey and 54% stated machine reliability and historic data, suggesting an increased variation between centres. Interestingly, both surveys had only 7% of centres stating they use FMEA, indicating minimal uptake of the previous survey’s recommendation to move to more considered design of QC schedules. However, IPEM Report 81 (2018)5was published after the publication of the previous survey and therefore would imply that these IPEM Report 81 (2018)5recommendations take into account the risks of not performing QC at the specified frequency and take precedence over the previous survey recommendations. This is a likely case why few centres have adopted the FMEA approach. If a full FMEA study on linac QC became available and was supported in the same way as IPEM publications, it is possible we would see more of a shift to that approach of QC. Previously, 37% of centres mentioned near misses informed schedules compared to only 5% in this survey. 20% of centres previously said the plan-do-study-act cycle, no centres reported that in this survey.

There was consistency in when centres last performed a review of their QC schedule, with 56% stating within the last year for this survey compared to 54% in the previous survey. Five centres reported they had not completed a review since the publication of IPEM Report 81 (2018),5the most recent UK guidance. The most reported changes made as a result of reviewing QC schedules were found to be changes in QC frequency and tolerance, not a complete redesign. This is reflected in minimal deviation from an IPEM Report 81 (2018)5approach. This could explain why despite high rates of recent review, 51% of centres still responded “Yes” when asked if they believe their QC schedule is in need of a review and update, compared to only 33% of centres in the previous survey.4This is supported with 9% of centres reporting they wish to introduce more automated QC, 7% reporting they wish to introduce uncertainty models and 5% wishing to move to paperless QC. The desire to overhaul the QC approach has existed since the previous survey but this has not yet been achieved by most centres.

Just over a quarter of centres reported using QATrack+ for QC record keeping (ceased maintenance from January 2025, after data collection completed). Only a few centres reported using paper methods, citing funding as an issue in making the transition to electronic recording.

Compared to the previous survey results4there has been an improvement in the number of centres achieving 100% of monthly QC completion, previously <30% but now 47%. However, more centres reported less than 80% of monthly QC achieved, previously 4% but now 9%.

A similar number of centres reported the use of risk assessments to determine QC in this survey (26%) compared to the previous survey (30%). 7% of centres also specified that the frequency of QC performed was based upon what was practicably achievable, this highlights the conflicting demands radiotherapy departments face. 16% of centres stated that they were unable to review the frequency of QC due to such demands. This highlights a circular problem; radiotherapy departments may be too busy to improve efficiency which in turn results in inefficiency. Some centres mentioned an apprehension of reducing tolerances below IPEM Report 81 (2018)5or have not seen a need to. This is reflected inTable 3where typically centres are performing tests at least as frequently as IPEM Report 81 (2018)5recommends. Outliers are largely electron tests, for example, electron energy ratio at different gantry angles, linearity of dose with MU, and constancy of dose output QC are no longer being performed in 44%, 28%, and 26% of centres respectively, a reduction in frequency from IPEM Report 81(2018)5recommendations.

The frequency at which centres reported performing TPR20:10 has also decreased below previous survey results, 74% of responding centres,4to a bimodal split between monthly (30%) and annually (30%). Standard output was recommended to be performed monthly in IPEM Report 81 (2018)5and found to be performed monthly in 52% of centres in the previous survey and monthly in 58% in the current survey. Weekly output was still performed in 26% of centres in the current survey. However, all centres perform daily output constancy checks, 40% with a manufacturer-integrated device and 67% with an external device.

There was a lower level of consistency for QC tests of imaging systems, such as for 3D kV imaging where equal proportion of centres measured at monthly, quarterly, and 6 monthly, as well as intermediate periods.

The least consensus for c-arm linac QC was reported for tolerance levels of imaging QC tests, due in part to different measurement methods and measurement units, and likely a lack of guidance in publications. A quarter of centres have separate radiation protection or diagnostic departments perform the annual imaging QC tests which could also be contributing to the difference in types of imaging tests performed and their tolerances.

Ancillary equipment has potential to directly affect patient treatment, and as such has comparable status to linac performance. Little consensus was found for gating implementation, although monthly or less frequent QC was reported.

The frequency at which diode calibration check is performed has not changed since the previous survey with 17% of centres stating it is performed monthly previously compared to 19% in this survey (Table S7). This is interesting as it appeared that the use of diode measurements were being reduced from centres responses to PSQC questions yet the percentage of centres performing this check monthly has remained constant.

Tolerances for linac ancillary equipment, in particular gating and surface guided equipment, showed ranges with a 10-fold difference (Table S8). In room respiratory monitoring system 0.2-2 mm notification, 0.3-3 mm suspension, temporal accuracy 0.1-1 second notification, and stability 0.2-2 mm suspension. This could be because these are relative new technologies and consensus methodologies have not yet been established.

There was a lack of consistency in the type and quantity of patient-specific quality control (PSQC) performed at centres across the UK. PSQC testing may be interpreted as calculations independent of the treatment planning system used for the plan creation or physical dosimetric measurements made prior to treatment commencement.

Most centres said they would consider a replan if failing PSQC measurements occurred, yet this translated to very few actual replans; almost 70% of centres replanned between 0 and 5 plans per year. It is unclear whether this is a result of very few plans failing measurement or from MPE decision to proceed to treatment with the original plan. The survey did not ask the percentage of plans that fail PSQC per year. Two of the three centres that stated a much higher percentage of replans in comparison to the rest of the cohort also reported a much higher percentage of the number that are measured. This indicates an inconsistent approach to PSQC which may have significant impact on the clinical workload.

The linac QC survey was sent to all UK radiotherapy centres with 63% responding (n= 43/68). This has provided an update to the UK consensus practice of linear accelerator QC. Topics covered were c-arm linacs, ring gantry linacs as well as linac ancillary equipment and PSQC, which have not previously been surveyed in the UK on this scale. Findings include that among the main stated reasons QC is undertaken is to “demonstrate safe use.” Almost all centres stated IPEM Report 81 (2018)5as a main source or starting point to structure their QC schedules including tolerances and frequency with adjustments based upon local experience, evolution of clinical techniques and available QC equipment.

This work is not intended to be used as professional advice but to offer an update to the previous review of consensus practice in the UK.

Thank you to all UK radiotherapy centres who took part in this survey, especially the 6 centres who took part in the pilot survey.

Elisha Tassano-Smith, 
Medical Physics Department, Portsmouth Hospitals University NHS Trust, Queen Alexandra Hospital, Portsmouth, Hampshire PO63LY, United Kingdom; 
Department of Medical Physics and Biomedical Engineering, University College London, London WC1E6BT, United Kingdom.

Antony L Palmer, 
Medical Physics Department, Portsmouth Hospitals University NHS Trust, Queen Alexandra Hospital, Portsmouth, Hampshire PO63LY, United Kingdom; 
Department of Medical Physics and Biomedical Engineering, University College London, London WC1E6BT, United Kingdom.

Wojciech Polak, 
Medical Physics Department, Portsmouth Hospitals University NHS Trust, Queen Alexandra Hospital, Portsmouth, Hampshire PO63LY, United Kingdom.

Andrew Nisbet, 
Department of Medical Physics and Biomedical Engineering, University College London, London WC1E6BT, United Kingdom.