The Internet of Things has proliferated, and the number of devices integrated into intelligent networks has made resource management and allocation one of the most critical challenges. The intrinsic constraints of IoT devices, such as energy consumption, limited bandwidth, and reduced computational power, have increased the demand for more intelligent and efficient resource allocation strategies. Numerous current resource allocation methods, such as evolutionary algorithms and multi-agent reinforcement learning, are grossly inefficient at adapting well to IoT networks in light of dynamic and rapid changes due to the inherent computational complexity and high cost. This paper proposes an intelligent resource allocation approach for Internet of Things (IoT) networks that integrates clustering and machine learning techniques. Initially, IoT devices are grouped using the K-Means algorithm based on features such as energy consumption and bandwidth requirements. A Random Forest model is then trained to accurately predict the resource needs of each cluster, enabling optimal allocation. Simulation results show that the proposed approach improves prediction accuracy to 94%, reduces energy consumption by 20%, and decreases response time by 10% compared to existing methods. These results highlight the effectiveness of the approach in managing resources in dynamic and scalable IoT environments.

With the rapid expansion of digital technologies, IoT has become one of the key elements transforming modern life. It interconnects billions of devices, enabling smooth data exchange and the creation of smart networks. However, such rapid growth has also brought several challenges in resource management, energy optimization, and quality of service. In this context, inventive methods of resource optimization and effective data management have gained high attention. The rapid development of these devices generating vast volumes of data while requiring relatively constrained resources in terms of energy, bandwidth, and processing capabilities has made fog and edge computing environments imperative for efficient resource management1–3.

Besides, data protection has become a concern at all levels4. Traditional resource allocation methods usually aim at the minimization of cost or execution time and do not take into account multiple objectives at the same time, such as energy consumption reduction5,6, service quality assurance, and adaptation to dynamic condition changes1. Further, due to dynamic changes in the needs of IoT devices and the limitations of resource allocation7,8in dynamic environments, sudden changes in user resource requirements make resource allocation even more complex6,9. The high energy consumption of devices and the need to reduce the energy consumption of IoT devices is one of the most critical problems in prolonging the device’s lifespan5,9. Delay management: Delay reduction is a critical issue for real-time applications in intelligent transportation10. Although some research works on resource allocation in IoT, the necessity of integrating advanced techniques such as deep learning, metaheuristic optimization, and fault tolerance has been explored to a lesser extent. Equally important is the requirement for integrated approaches aimed at simultaneously reducing delay and energy costs and improving fault tolerance11–13. The purpose of this paper is to dynamically enhance resource allocation using online learning methodologies1. The methodology presented here proposes a hybrid approach in which machine learning—more precisely, Random Forest—and clustering techniques are combined to allocate resources. First, IoT devices are clustered according to standard features to handle similar needs in different classes efficiently.

Then, the Random Forest model is applied to predict future demands and allocate optimal resources. This scheme allows for energy efficiency improvement, delay reduction, and better network performance overall14, and it also enables efficient resource allocation between users5,6. Hence, it proposes combining the Random Forest model and K-Means clustering for resource allocation in IoT networks. First, IoT devices are clustered by the K-Means clustering algorithm depending on energy consumption and bandwidth demands. Afterward, the Random Forest model is used to predict the resource needs for every cluster. This method, which is of high accuracy, identifies complex patterns in the use of resources and allocates resources effectively based on the actual needs of the devices. The proposed methodology also shows the ability to adapt in real time to environmental changes and aims to maximize efficiency while reducing energy consumption1,14.

A hybrid framework was proposed for resource allocation in IoT networks using K-Means clustering and Random Forest prediction.

Device-level features such as energy consumption and bandwidth requirements were considered in both clustering and prediction processes.

The algorithmic structure of the proposed model was elaborated through a step-by-step analysis of clustering and decision-making based on Random Forest.

A mathematical formulation of the resource allocation process was provided, incorporating both cluster-level and device-level features to enhance analytical clarity and generalizability.

The rest of the paper is organized as follows: “Related works” describes related works, “Proposed method” introduces the proposed method, “Evaluation of the proposed algorithm” presents the simulation results, and the final section is dedicated to the proposed method’s conclusion.

Limiting the resource availability in IoT networks, for instance, energy, bandwidth, and computation power, optimizes resource allocation. Many researchers have thus put forth various methods for improving such networks’ resource management and allocation mechanisms. In14,15, some researchers, based on some research work focusing on the application of edge and fog computing, reduce latency further to increase efficiency in IoT networks. This approach relocates part of the processing to locations closer to the data source, reducing bandwidth consumption and transmission delay. For instance, in one study, researchers employed fog computing to manage resources in intelligent transportation, which reduced latency and further improved system efficiency14.

Evolutionary algorithm-based methods have also been widely used for resource allocation in IoT networks. Bat and whale optimization have been applied to solve resource allocation problems in IoT’s dynamic and complex environments. Although advantageous in resource allocation optimization, these methods generally require high computational resources and long computation times, which is unrealistic for most IoT applications because of resource limitations. Conversely, reinforcement learning and multi-agent approaches have also been proposed for resource management in IoT. Examples include multi-agent reinforcement learning-based methods16proposed for different scenarios of resource allocation in IoT to enhance efficiency and resource management. However, these approaches require further improvement since they often present profound computational complexities and incur high costs; hence, they are not optimal for all cases.

The remaining parts of the literature were related to machine learning models for resource allocation in IoTs; classification algorithms like Random Forest help identify resource requirements and thereby apply the intelligent resource allocation selection policy. Examples include using the Random Forest model to make predictions about the energy consumption pattern of IoT devices in order to plan accordingly for resource allocation. Although many papers have been published regarding resource allocation in IoT, there are still some gaps in this area. Most of the developed methods are concerned with the quality of service or reduction of delay while giving less consideration to energy optimization as an important factor.Most of the listed algorithms are single-point or device-based resource allocation methods. There is still a lack of scalable and holistic methods for performing resource management when facing many IoT devices. Herein, we propose integrating Random Forest and K-Means clustering algorithms as a new IoT network resource allocation approach. The method presented will enable the classification of similar devices using the K-Means algorithm and the Random Forest model for predicting resource needs with high accuracy to achieve optimality and scalability in resource allocation, contributing to enhanced efficiency, reduced energy consumption, and adaptation to dynamic changes1. Some of these methods are presented in Table1.

The following paper, therefore, proposes an intelligent technique for resource allocation in IoT networks using machine learning algorithms and clustering techniques. The proposed approach will contribute to improving efficiency, reducing energy consumption, and optimizing resources by IoT devices. The proposed system uses the latest machine learning techniques coupled with clustering techniques to efficiently manage resources in these IoT networks. It solves most of the complexities related to large-scale dynamic networks and provides an invaluable contribution to reducing energy consumption and enhancing the performance of IoT networks. The proposed architecture is based on a few steps forged for this particular resource allocation activity in dynamic and complex IoT environments. Random Forest resource allocation can be used to develop the efficiency of resource distribution in an IoT network. Also, clustering enables better pattern identification in data by aggregating devices or nodes based on their common characteristics, such as energy consumption, request type, or communication quality. The combination of both eventually leads to wiser resource allocation and network optimization. Figure1illustrates proposed method’s workflow.

The goal is to optimize resource distribution by minimizing energy consumption and response time while maximizing prediction accuracy. Each IoT device is first described by a set of features, including energy consumption, data volume, network quality (such as bandwidth and latency), request priority, and the device’s maximum tolerable delay. These features form a feature vectorfor each device, which serves as input to the K-Means algorithm. This clustering algorithm groups devices with similar behavioral patterns into clusters, thereby reducing data complexity and facilitating moreeffective resource prediction.

whereis the center of the cluster to which deviceiis assigned. The Random Forest model is then used to predict the required resources and The overall objective function is defined as.

whereis the response time for devicei,is the prediction accuracy of the Random Forest model, andare weighting coefficients that control the trade-offs among objectives.This optimization is subject to the following constraints, Cluster-level resource limits Equation, QoS latency constraint for each device as Equation, Maximum energy consumption per device as Equation .

low are the fundamental resource allocation steps for IoT networks using the Random Forest algorithm.

Identification and Characterization of Attributes: The first step in resource optimization in IoT networks is identifying features that may impact resource allocation. These features typically contain information related to device status and network requirements. The main features often include device energy consumption, request priorities, data transmission volume, processing usage, and network conditions.

Data Collection and Preparation: The Random Forest model needs historical data for training, which includes feature engineering like collecting data in the past and labeling. With properly prepared datasets, this model will learn the pattern and make the best predictions possible on resource allocations.

Training the Random Forest Model: The prepared data is used to train the Random Forest model at this stage. The Random Forest algorithm will build multiple decision trees that cooperatively decide on resource allocation by considering various conditions. Each decision tree can focus on specific resource allocations, including bandwidth, processing power, or energy distribution. In turn, the Random Forest learns how to adapt resource allocation based on changes in input features, such as device and network status. Using metrics such as feature importance allows the model to identify which types of features—energy consumption in this case—will have the most value in resource prioritization, optimizing decisions with elevated accuracy.

Predicting Resource Allocation Using the Trained Model: After training, the model can be used to predict resource allocation under new conditions. For example, after adding a new device to the network, the random forest model will predict how much processing power, memory, and bandwidth this device will require.

Resource Allocation Evaluation and Improvement: After resource allocations, the system’s performance must be analyzed, and the results must be fed into model improvements. Evaluation entails latency, energy efficiency, and service quality guarantees. This will involve checking instances when the allocation is not properly realized, finding out why it has happened, and performing adjustments after training a model. These adjustments can then be turned into forecasts regarding resource allocations under new conditions, such as adding to the network a new device that requires random forest modeling to predict its computational processing, memory, and required bandwidth. Predictions using the random forest model will give better results to optimize resource allocations for future use.

Dynamic and Real-Time Allocation: Resource allocation must be performed dynamically and in real-time in some scenarios. For this, online machine learning models can be used, allowing the model to continuously update with new data and perform optimal resource allocation. The random forest model can be updated in real time with new data, continuously optimizing resource allocation. The model can provide immediate predictions for new resource allocation if network or device conditions change. Although using random forests requires historical data and precise information for model training, as well as the high processing cost and adjustments to the number and depth of trees, it can improve prediction accuracy by using multiple decision trees. Furthermore, it can simulate complex and nonlinear relationships between various features (such as energy consumption, network load, and device status). It can process large datasets, typically present in an IoT network.

The significant steps for resource allocation using clustering and random forests in IoT are outlined below, along with a mathematical analysis.

This step is similar to the previous one, and it needs to collect and preprocess the data required for resource allocation. Energy consumption, network status, data volume, request priorities, and data types can be the input features. The features of devicei, including energy consumption, data volume, request priority, latency, and network status, are represented in the formula (1).

yi: Deviceiresource allocation may aim to ascertain the amount of each resource needed, such as computation resources, energy, bandwidth, or memory. Data collection involves gathering information in discrete time intervals for each device.Ei: Energy consumption,Di: Data volume,Qi: Network connection quality, including bandwidth and Latency.Pi: Request priority. Data preprocessing should also be performed. Features should be normalized in order to avoid the impact brought about by different scales. The incomplete or missing data should be deleted or completed using methods such as averaging and prediction. Based on formula (2), whereµjis the mean andσjis the standard deviation of featurej.

Device-specific features, including request priorities, latency, and device status.

Network features, for example, network connection quality, bandwidth, and Network latency.

K-means clustering can be mathematically represented as follows: First, the cluster centersC1,C2,…,and CKare randomly initialized. Then, each devicexiis assigned to the nearest cluster center, according to formula (3), where ||xi − Ck||2is the Euclidean distance between deviceiand cluster centerk.

Then, according to formula (4), the cluster centers are updated. Here,NKrepresents the number of devices in clusterk. This process is repeated until the cluster centers converge.

N denotes the total number of devices,Cirepresents the cluster assigned to devicei, andCkis the center of clusterk. An indicator function is used, which equals one if deviceibelongs to clusterk(Ci = Ck) and zero otherwise. In DBSCAN, the density requirement to form a cluster is defined by the relationship whereNis the number of devices within the distanceepsilon, andTis the total number of devices in the region.

It groups devices of high density and labels scattered points as noise. Clustering enables us to find similar devices, facilitating resource allocation more efficiently. Each cluster may have its requirements. For example, a cluster with high energy consumption may require allocating more computational resources.

The Random Forest model is a machine learning-based approach for predicting resource allocation. During training, cluster-level features and device-level features are used as inputs to the Random Forest model. In this model, multiple decision trees are constructed. Each decision tree establishes a set of rules for resource allocation. These decision trees are randomly built from different data features and then used to predict resource allocation. The algorithm operates as follows: For each tree, random sampling of the data is performed. Each tree then randomly selects features and splits the data based on the best feature. Finally, resource allocation predictions are made based on the majority vote of the predictions from the trees. Assuming the data includesNdevices andMfeatures, the Random Forest generates a prediction functionf(x)for resource allocation, as shown in formula (7). Here,Trepresents the number of decision trees in the Random Forest, andft(xi)denotes the resource allocation prediction made by treet.

The Random Forest model was trained and can now be used to predict resource allocation under new conditions.Yirepresents the predicted resource allocation for devicei. These resources may include computational resources, energy, memory, or bandwidth.

After resource allocation is completed, the system’s performance should be evaluated. This involves checking the quality of service and assessing how efficiently the resource allocation meets device requirements. Additionally, delays in resource allocation and data transmission are reviewed. For model evaluation, metrics such as the prediction accuracy of resource allocation, shown in formula (9), are used. Here,Πis an indicator function that returns 1 for correct predictions and zero otherwise. Energy consumption is evaluated using formula (10), whereEirepresents the energy consumption of devicei.

Based on the model’s predictions, resources are allocated to the clusters. Higher-demand clusters are prioritized, receiving a larger share of resources. The resource allocation to clusters is determined by formula (11), whereRjrepresents the resources allocated to clusterj, and α denotes the allocation coefficient.

If a cluster predicts high consumption, it can decide to reduce the activity of other clusters or put low-priority devices into sleep mode.

Continuous monitoring at this stage means checking the model performance and feedback on resource allocation. According to formulas (9) and (10) in the model optimization stage, various criteria such as accuracy, positive accuracy, and recall are used for evaluation. As seen, adding a clustering step to the resource allocation process using the random forest in the Internet of Things networks, with precise mathematical analysis, can help improve prediction accuracy and effective resource allocation. This method allocates resources more optimally by creating similar clusters of devices and prevents waste of energy and resources.

A flowchart, shown in Fig.2, is presented to explain the structure and process of the proposed method. This flowchart graphically displays the different stages of this method and provides a comprehensive view of the main steps, their sequence, and the interaction between key components of the system or method. It aims to facilitate the conceptual understanding of the method and clarify its implementation path. Table2is presented Symbols and Descriptions.

In the proposed model, resource allocation in IoT networks is carried out using a hybrid framework that combines K-Means clustering and Random Forest prediction. Initially, each IoT device is characterized by features such as energy consumption, bandwidth requirement, data transmission latency, and computational capacity. These features are used as input to the K-Means algorithm to group devices into clusters with similar behavior patterns. Then, the data from each cluster are fed into a Random Forest model to predict the corresponding resource demands. The Random Forest model is configured with 100 decision trees, a maximum depth of 15, and uses the Gini impurity as the splitting criterion. K-Means was selected for its simplicity, speed, and effectiveness in clustering numerical data, while Random Forest was chosen due to its robust performance on nonlinear data, resistance to overfitting, and high generalization ability. The interaction between these two algorithms is designed to reduce data complexity through clustering and improve prediction accuracy in the allocation phase. Finally, based on the model output, resources such as bandwidth, energy, and processing power are optimally allocated to devices according to their predicted requirements. The overall structure of this process is also illustrated in a block diagram within the paper to provide a clear visual representation of the workflow and the interaction among model components.

Clustering Accuracy: After clustering, using the following formulas.

The similarity within clusters and the difference between clusters are measured.

Prediction Accuracy: It reflects the accuracy of the random forest model in predicting the resource requirements of IoT devices. The prediction is evaluated using the following formulas:andi.

Response Time: Refers to the time taken to cluster the data and predict resources.

Energy Consumption: Indicates the total energy consumed by the system during the resource allocation process, analyzed using simulation data presented in Table3.

These data are structured in a table containing multiple features and samples, with each sample representing the state of an IoT device at a specific time interval. The dataset includes 1000 samples characterized by five key features. The experimental environment comprises an Intel i7 processor, 16GB of memory, Python programming language, and Scikit-Learn libraries. The MNIST dataset, a standard collection of handwritten digit images (0 to 9), is utilized in this study. It includes 60,000 images for training and 10,000 images for testing. Widely employed for evaluating machine learning algorithms and neural networks, MNIST’s diversity and complexity enable comprehensive testing of various methods, simulating real-world data scenarios effectively.

In this paper, the Random Forest model was trained using a set of optimized hyperparameters. Table4displays the key settings of the model, including the number of trees, the splitting criterion, the maximum depth of the trees, and other related parameters.

Shutter hyperparameters of the random forest model in the proposed method.

Implementation platform: The proposed method was implemented with the aim of optimizing resource allocation in the Internet of Things environment using Python language and Scikit-learn, Pandas, and Matplotlib libraries. The K-Means clustering algorithm was used to group the initial data into homogeneous clusters. Then the Random Forest machine learning model was trained to predict the optimal resource allocation in each cluster. All processes were implemented on a system with Intel Core i7 processor specifications, 16 GB RAM, and Ubuntu 22.04 operating system. This implementation was done offline, but the algorithms were designed in such a way that they can also be implemented in distributed environments such as Edge Computing. Compared to the methods available in studies3,11,12,16,24,25,28, which are mainly based on deep and reinforcement learning, the proposed approach, with its computational simplicity and execution speed, shows significant performance in prediction accuracy, response time, and energy consumption.

Dataset details: In this section, in order to improve the quality of the evaluation and provide a more accurate basis for analyzing the performance of the proposed model, the MNIST dataset was changed to the GWA1dataset. The main reason for this change was the need for a real, diverse, and feature-rich dataset in the field of resource allocation and task scheduling in cloud and IoT environments. The GWA dataset contains accurate and comprehensive information on distributed workloads, processing resources, users, and task volumes recorded in real environments such as Grid5000 and SHARCNET. This dataset, with its features such as a large number of sites, large data volume, diversity in user types, and the ability to analyze task execution time, resource consumption, and processing load, allows for a more accurate evaluation of key parameters such as prediction accuracy, energy consumption, and response time, and provides a suitable basis for comparing the proposed model with reference methods. Therefore, a series of numerical experiments were conducted using the new dataset and in the Python runtime (Python 3.10) and the matplotlib library.

In this section, three independent variables, including the number of clusters, the input data volume, and the number of features, were examined. For each of these variables, as in section (a), three main criteria of prediction accuracy, energy consumption, and response time were considered as evaluation indicators. For each independent variable, the performance of the proposed method was compared with seven reference methods, including3,11,12,16,24,25,28. Comparative graphs were designed in Excel software as line and bar graphs based on the implemented numerical outputs. As can be seen in Fig.12, the proposed method has stable performance with increasing the number of clusters and maintains higher accuracy compared to the reference methods. The accuracy of the model in the range of 5 to 50 clusters usually remains around 94%. In Fig.13, as the input data volume increases from 500 to 5000, the proposed method still maintains high accuracy and stability.

The comparative diagram in Fig.18shows the total energy consumption in a specific scenario for the proposed method and seven reference methods. As can be seen, the proposed method, with a total energy consumption of about 330 mJ, has shown a more optimal performance than other methods. This result is the result of using optimal clustering and utilizing a low-power random forest model, which has led to a reduction in the need for expensive processing.

Then, to check the depth of the model, four different graphs were plotted for training accuracy, testing accuracy, training error, and testing error against other methods, confirming the stable behavior of the model in the training and testing processes. These four criteria are typically drawn as a line graph in terms of the number of epochs (in deep models) or the number of samples or different parameters (e.g., the number of clusters or features). In training and evaluating machine learning models (including the proposed model based on random forest), applying accuracy and error in the two steps of training and testing is a valid and precise technique for measuring the performance and investigating the phenomenon of overfitting or underfitting in a way that the training and testing accuracy are compared to examine the proximity of the model’s behavior on the training and testing data. Conversely, the training and testing error is also employed to indicate the distance between the model’s behavior on the two sets and to identify Overfitting or Underfitting.

If the training accuracy is good and the testing accuracy is poor, overfitting is present.

If both are close and high, the model is good and generalizable.

Training Accuracy: As per formula (12), the percent of samples from the training data that were predicted correctly by the model. If this figure is exceptionally high and test accuracy is low, it is an indication of Overfitting.

Figure22reveals how successfully the model has learned the training data. A rise in the value of this graph shows that the model has learned from the existing data successfully. If the training accuracy is perfect, but the test accuracy is still low, it can be a sign of overfitting. As Fig.21shows, the training accuracy of the suggested model has risen constantly and reached around 97% at the end of training. This amount shows that the model has been able to learn the relations in the training data effectively. The ascending and smooth trend of the graph shows efficient learning without disruption during the training.

Test Accuracy: As per formula (13), the proportion of samples from the test data (which is unseen by the model) that are predicted correctly. This metric reflects the generalization of the model. The desire is for this value to be as close as possible to the train accuracy. It demonstrates the model’s capability to generalize the acquired knowledge to unseen data (test). The model being accurate in this graph is high, meaning the model performs well in the real world. The proximity of the test accuracy to the train accuracy reflects the desired level of balance between learnability and generalizability of the model.

As per Fig.23, the test accuracy also rose progressively with the rise in the number of training sessions and ultimately reached around 95%. This proximity of the test accuracy to the training accuracy reflects that the model possesses high generalizability, and the overfitting phenomenon is absent in it. The slight gap between the two accuracies demonstrates the presence of a good balance between learning and generalization.

Train Loss: Percentage of incorrect predictions of the model on the training data according to formulasandIf the training error is near zero but the test error is high, the model is overfitted. This comparison reveals the percentage or degree of error in the model’s predictions on the training data. The fewer this error, the more successfully the model learned the training data. But if the training error is zero or very small and the test error is high, this is a danger signal and an indication of overfitting.

Test Loss: The model’s rate of wrong predictions on the test data based on formulas (14) and (15). This time for error calculation indicates how accurate the model is on the unseen data. In Fig.25, the graph of the test error also illustrates a proper decline and has reached around 5%. This low value, as well as the training error near it, signifies that the model is not overfitted or underfitted. The concurrent decline of the training and test errors represents the high effectiveness and generalization capability of the suggested model in handling unseen data.

As shown in Fig.24, the training error was reduced consistently over time and finally reached around 3%. This consistent and steady reduction shows that the model is well fitted to the training data, and the learning process has been going on without fluctuations or harsh deviations. The lack of sudden spikes in the graph shows the stability of the model.

Evaluating compatibility with real-time scenarios. In order to investigate the usability of the suggested approach in real and dynamic IoT environments, three typical scenarios were examined: device failure, bandwidth variation, and inference time. This research, as an assessment in the following three types of graphs, indicates how prepared and solid the model is for real-time applications and working conditions.

Inference time compared to data size. In IoT applications, one of the key characteristics of machine learning models is their capability to give quick responses to new inputs under varying conditions. With the increase in the volume of data, the time of inference (Inference Time) might grow. This graph aims to investigate the scalability of the model and its responsiveness in real time. If the inference time grows steadily or linearly, it is a sign of the stability of the model under real conditions. According to Fig.26a, it illustrates that by increasing the number of clusters from 5 to 50, the inference time of the suggested model has increased gradually. This relation reflects the effect of the complexity of data partitioning on the time performance of the system, and by increasing the volume of input data from 500 to 5000 records, according to Fig.26b, the inference time also rises. This is normal because processing a larger volume of data needs more time for analysis and resource allocation. Finally, Fig.26c illustrates the relation between the number of data features and inference time. By growing the number of features from 5 to 50, the inference time increases considerably, reflecting the direct effect of feature dimensions on the computational load of the model.

Model accuracy relative to the percentage of device malfunctions. In IoT environments, the outage or failure of some nodes (devices) is a natural occurrence. Therefore, a model that still performs accurately in the face of a slight reduction in input data is more suitable in terms of reliability. In this diagram, the model’s resilience to node failures is evaluated by simulating the removal of a percentage of input data (e.g., 10%, 20%, etc.). The stability of the model’s accuracy in this scenario indicates its resilience in real-world conditions. Fig.26d shows how the proposed model’s performance in terms of accuracy is affected by the failure rate of devices in an IoT environment. To evaluate the model’s real-time robustness, different failure rates (from 0 to 30%) are simulated, and their impact on the model’s accuracy is investigated. As can be seen, as the failure rate increases, the accuracy of the model gradually decreases, but this decrease remains within a controlled range. This indicates that the proposed model has a good ability to maintain accuracy even under minor disturbance conditions, which is an essential feature for real-time systems in the IoT.

Relative response time to data rate or bandwidth. For the majority of IoT applications, network bandwidth is variable and may impact how fast data is sent to the model. This chart is to look at the impact of data rate (or bandwidth/delay variance) on the response time of the model. If the model continues to perform well at low or variable rates, its applicability to real-world networks is proven. Figure26e shows the relation between network bandwidth and response time of the system. The x-axis is the bandwidth (measured in megabits per second), and the y-axis is the response time of the system (measured in milliseconds). The response time drops significantly with a rise in bandwidth since the system is able to transfer and process data faster. This decline is mainly found at lower bandwidth levels, and if the bandwidth is increased to high levels, the response time decline approaches a saturation point. This graph illustrates that the provided approach is practical in responsiveness for increasing bandwidth situations and can remain stable and effective for IoT networks with fluctuating bandwidth.

a Inference time graph based on the number of clusters.bInference time graph based on input data volume.cInference time graph based on number of features.dModel accuracy graph versus percentage of device malfunction.eResponse time versus bandwidth graph.

This study proposed a hybrid method combining K-Means clustering and Random Forest prediction to address resource allocation challenges in intelligent systems, particularly within IoT environments. The approach was designed to utilize multiple device-level features, enabling more accurate predictions and more efficient resource distribution. The necessity of this research stems from the increasing complexity and dynamism of IoT networks, where conventional allocation methods often fall short due to computational limitations and lack of adaptability.

The experimental results confirmed the effectiveness of the proposed method in improving clustering quality, enhancing prediction accuracy, and reducing energy consumption and response time. These outcomes demonstrate that the model can meet the demands of real-time, resource-constrained environments while maintaining scalability and robustness. The authors believe that the integration of clustering and prediction mechanisms played a pivotal role in optimizing system performance.

Given its promising performance, the proposed method shows strong potential for deployment in areas such as smart grids, distributed AI systems, and large-scale data processing frameworks. Future work may involve extending the model to operate with deep learning-based frameworks, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), and applying it to more diverse and realistic datasets to further validate its generalizability and performance under dynamic network conditions.