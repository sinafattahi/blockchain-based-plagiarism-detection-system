In real-world data (RWD), defining the observation period—the time during which a patient is considered observable—is critical for estimating incidence rates (IRs) and other outcomes. Yet, in the absence of explicit enrollment information, this period must often be inferred, introducing potential bias.

This study evaluates methods for defining observation periods and their impact on IR estimates across multiple database types. We applied 3 methods for defining observation periods: (1) a persistence + surveillance window approach, (2) an age- and gender-adjusted method based on time between healthcare events, and (3) the min/max method. These were tested across 11 RWD databases, including both enrollment-based and encounter-based sources. Enrollment time was used as the reference standard in eligible databases. To assess the impact on epidemiologic results, we replicated a prior study of adverse event incidence, comparing IRs and calculating mean squared error between methods.

Incidence rates decreased as observation periods lengthened, driven by increases in the person-time denominator. The persistence + surveillance method produced estimates closest to enrollment-based rates when appropriately balanced. The min/max approach yielded inconsistent results, particularly in encounter-based databases, with greater error observed in databases with longer time spans.

These findings suggest that assumptions about data completeness and population observability significantly affect incidence estimates. Observation period definitions substantially influence outcome measurement in RWD studies.

Standardized, transparent approaches are necessary to ensure valid, reproducible results—especially in databases lacking defined enrollment.

Unlike randomized controlled trials and other prospective studies, which involve direct data collection on patients from registration through follow-up, re-using existing healthcare data presents a unique challenge: it is often unclear when a patient is actually “observed.”1In real-world data (RWD), an observation period reflects the time during which a patient is expected to have healthcare events recorded. A lack of data during this period may imply either that no clinical events occurred or that there was simply a failure to capture these events. Defining observable time—when we can confidently interpret the absence of data as the absence of clinical events, rather than as missing data—is, therefore, essential to avoid bias in observational studies.2Observable time definitions underpin key outcomes, including incidence rates (IRs), time-to-event analyses, and other measures foundational to reliable evidence generation.

With the growing use of Common Data Models (CDMs), the definition of observable time is becoming increasingly standardized. CDMs enable the sharing of analytical tools and even entire study implementations across different data sources, thereby enhancing transparency and reproducibility.19Converting data to a CDM often requires restructuring the data to meet minimum research-readiness standards. For instance, the OMOP CDM necessitates an explicit definition of the observation period, which dictates the span of time in which healthcare events should be recorded.20To be clear, the choice of approach to defining observable time is a critical decision that should be well documented for all observational studies. However, it is notable that increasing prominence of CDMs in health research has elevated this issue of defining observable time from a hidden, often undocumented study-specific parameter to an important underlying assumption applied to an entire database during the extract, transform, and load (ETL) process and thus applied across all study questions relying on those data.

This study is motivated by this shift in data processing standards. As defining observable time now requires explicit decisions and documentation during the ETL process, there is an urgent need to assess the implications of various methods. Here we examine multiple approaches to defining observable time across different types of databases with a focus on their impact on evidence generation by applying these methods to replicate a large study on background IRs of adverse events of special interest.

We use 3 different methods of defining eras of observable time based on observed health events, and 1 reference standard. Here we define observable time as the time during which any interaction with the healthcare system is captured.

The creation of observation periods, based on patient interactions with the healthcare system requires the identification of periods of time when a person received care. To start, we identify unique dates with any healthcare event(s) per person. These events include visits, diagnoses, procedures, observations, device exposures, laboratory measurements, prescriptions written, drugs administered, and pharmacy drug dispensings. If a person was cared for in a tertiary setting for a length of time, then the start and end dates of the interaction are identified.

Often encounter-based databases are made up of EHR records that reflect interactions with a care provider in the office/hospital/outpatient center but do not extend to pharmacy interactions. In addition, some prescriptions are refilled automatically or by mail and do not necessarily represent a time when a person is actively receiving care. To address this fundamental difference and facilitate comparison between enrollment-based and encounter-based databases, we identify a second set of events (medical only) where those solely representing pharmacy drug dispensings are removed.

We then create observation periods for each person using the identified dates with any healthcare event and dates with any medical event (a medical event is any healthcare event excluding pharmacy dispensings) by patient (hereafter referred to as “all events” and “medical events,” respectively), by applying a combination of persistence and surveillance windows to the events. A persistence window is the maximum allowed number of days between event records that are then strung together to create an era of persistent observation. The surveillance window is the number of days added to the end of the persistent observation era as an additional period of surveillance prior to the end of the observation period. This is demonstrated inFigure 1, which shows the difference in the observation periods between a surveillance window of 0 vs 90 days and persistence windows of 180 vs 365 days.

How persistence and surveillance windows are applied to health events to create observation periods.

We apply 8 persistence windows of 180, 365, 548, 730, 1095, 1460, 1825, 2190 days combined with 5 surveillance windows of 0, 30, 90, 180, 270 days across each of the 11 databases for the 2 event types “all” and “medical.” Overall, there are 80 combinations per database, 880 in total. Persistence and surveillance window values were chosen based on common patterns of healthcare utilization. For example, annual health checkups are required in Japan’s JMDC database, and yearly physicals are common in U.S. clinical practice. These empirically informed intervals were selected to reflect realistic follow-up patterns and ensure consistency across databases, rather than through formal tuning or algorithmic selection.

We created observation periods using dynamic persistence and surveillance windows tailored by age and gender, leveraging the distribution of time between healthcare events to account for demographic differences in healthcare utilization. For enrollment-based databases with near-complete capture of health events, we calculated the time in days between successive events for each individual, stratified by age and gender. The average time between events was computed per person and summarized to generate age- and gender-specific distributions (mean, median, and 10th to 99th percentiles). This approach ensured equal weighting of individuals rather than events, preventing bias from chronically ill individuals with frequent visits. Observation periods were then inferred by applying the 99th percentile of the age- and gender-stratified distribution as the persistence window, while the 10th, 25th, 50th, 75th, and 90th percentiles were used as surveillance windows. This method was applied to both all healthcare events and medical events like the persistence + surveillance approach.

We also include the era defined as the earliest date a person was observed to the latest date a person was observed in the 5 databases without defined enrollment. This approach, otherwise known as “min/max,” is the most commonly suggested approach for defining the observation period in encounter-based databases.

We use the enrollment time as the reference standard for comparison in the 6 databases with defined enrollment. Due to the nature of enrollment-based databases it is the case that some percentage of the persons enrolled do not have any records in the database at all; these are known as non-care seeking individuals. These individuals still contribute person-time when calculating statistics like IRs but do not have any observed interactions or outcomes. Encounter-based databases only record patient interactions with the healthcare system and as such, do not have any non-care seeking individuals. To account for this fundamental difference in the makeup of the databases, we studied 2 more patient populations for the purposes of comparison in the study replication. One includes enrollment time for those patients with at least 1 health event (medical or pharmacy) and the other includes enrollment time for patients with at least 1 medical event.

In this study, we use Merative Marketscan Commercial Database (CCAE), Merative Marketscan Multi-State Medicaid Database (MDCD), Merative Marketscan Medicare Database (MDCR), Optum’s Clinformatics De-Identified Data Mart (Optum Extended), Optum De-Identified Electronic Health Records (Optum EHR), JMDC, IQVIA Adjudicated Health Plan Claims Data (Pharmetrics), IQVIA Longitudinal Patient Database in Australia (Australia LPD), IQVIA Disease Analyzer France (France DA), IQVIA Disease Analyzer Germany (German DA), and Premier. These databases all include feeds from multiple providers or are administrative claims that capture all billed healthcare encounters for covered persons and all databases used were standardized to the OMOP CDM v5.3 or higher. Full text descriptions of these databases can be found inAppendix S1. CCAE, MDCD, MDCR, Optum Extended, JMDC, and Pharmetrics are all enrollment-based while Optum EHR, Australia LPD, France DA, German DA, and Premier are encounter-based.Table 1gives high-level characteristics of each database used in the analysis including average age of patients, number of persons with no events (any and medical only as described in Defining Eras of Observable Time section), the provenance of the data and the time periods the databases span.

Summary statistics for the enrollment-based databases; values are median (IQR) unless stated otherwise.

Reflects the observation period based on enrollment time in a health plan.

Reflects the total time persons are observed in the database  from their first event to their last event (min/max).

This study was originally designed to characterize the IRs of 15 different adverse events (AE) from 1 January 2017 to 31 December 2019. Persons with at least 1 observation period that included the dates 1 January 2017, 1 January 2018, or 1 January 2019 with at least 365 days of observation prior to index were considered the target at risk population. The index date was defined as 1 January of each year. Of the original 15, 5 AEs were chosen for this replication: acute myocardial infarction, deep vein thrombosis, pulmonary embolism, anaphylaxis, and narcolepsy to represent rare, prevalent, acute, and less acute conditions. Links to the phenotype definitions can be found inAppendix S2.

A “clean window” was defined as the time period prior to the index date during which the AE could not be observed. If the AE was observed during that window, the participant was removed from the study cohort. The clean window for the replicated AEs was 365 days except for anaphylaxis, which was 30 days.

A person’s time at risk was defined as 365 days after the index, censored on an event during the clean window, death, or the end of their observation period in the database. Incidence rates were calculated as the total number of events divided by the total time at risk per 100 000 person years.

We then use the IRs generated using enrollment time and compare them to the IRs generated using persistence + surveillance and the age + gender methods by calculating the mean squared error (MSE). This allows us to determine the IRs that are the closest to the reference standard.

Tables 1and2provide summary statistics for the enrollment-based and encounter-based databases, respectively. Across the 6 enrollment databases, the total number of persons ranged from 11 168 574 (MDCR) to 169 538 013 (CCAE). Between 94.58% (MDCR) and 82.89% (MDCD) had at least 1 event (medical or pharmacy) observed while 92.68% (JMDC) to 81.31% (Pharmetrics) had at least 1medicalevent observed. In contrast, the total number of persons in the encounter databases ranged from 334 266 216 (Premier) to 1 258 464 (Australia LPD). Between 98.73% (France DA) and 100% (Premier) had at least 1 event observed (medical or pharmacy).

Reflects the time persons are observed in the database, from their first event to their last event (min/max).

The median number of observation periods per person for all enrollment-based databases was 1 with median length in days from 545 in Optum Extended to 1399 in JMDC. In the encounter-based databases, Premier had the lowest median days from earliest event to latest event at 20 days while Optum EHR had the highest at 1170 days.

Australia LPD, France DA, and Germany DA have slightly less than 100% of persons with at least 1 event in the database with 98.84%, 98.73%, and 99.82%, respectively. Due to privacy-preserving measures implemented by the data vendor, death dates are removed from the dataset. In a small number of cases, death was the only recorded event for an individual. As a result, these individuals have no observable events in the data.

Figures 2and3show how the IR in CCAE and MDCR changed based on the persistence and surveillance window used as compared to that using enrollment time and the min/max approach. There is a clear decreasing trend in the IRs as both the persistence and surveillance windows increase, however, there is diminishing impact of modifying the persistence window once it exceeds 500 days. This holds across almost all databases and outcomes, with few exceptions as seen inAppendix S3.

Incidence rates in Merative CCAE across multiple persistence and surveillance windows, 5 outcomes, and 2 event types.

Incidence rates in Merative MDCR across multiple persistence and surveillance windows, 5 outcomes, and 2 event types.

Comparing the IRs generated using enrollment time and the min/max, in Pharmetrics, MDCD and CCAE they are very similar across the 5 AEs. In MDCR and Optum Extended the min/max IR consistently underestimates the enrollment IR while in JMDC the opposite is seen with the min/max IR consistently overestimating the enrollment IR.

In Optum EHR, Premier, and IQVIA Germany the min/max IR is low, corresponding to the estimates using 2 190 day persistence windows and 180 or 270 day surveillance windows. In contrast, the Min/Max estimates in IQVIA Australia LPD and IQVIA France fall around the midpoint in the range of IRs calculated using the persistence and surveillance approach.

For the enrollment-based databases, the IRs produced through the persistence + surveillance method were compared to the IRs produced using the enrollment time as the observation period by calculating the MSE. The MSE was then plotted as a heat map by outcome and event type with the min/max IR represented by a 9999-day persistence window and 0-day surveillance window. The darkest blue color on the graph corresponds with the lowest MSE. The MSE heat map for the Acute Myocardial Infarction outcome with the observation period defined using medical events only can be seen inFigure 4; the additional heat map plots are available inAppendix S4. For all enrollment databases except JMDC, the IRs closest to those using enrollment time show an inverse relationship between persistence and surveillance, with the lowest persistence windows paired with the highest surveillance windows, and vice-versa. In Merative MDCR, the IRs produced using higher surveillance windows do not come as close to the enrollment IR as the IRs produced using 0- to 90-day surveillance windows though there is still a clear inverse relationship between persistence and surveillance.

Heatmap of mean squared error between enrollment IR and persistence + surveillance IR using medical events only for the outcome acute myocardial infarction.

This same MSE approach was used to compare the enrollment IR to that using the age + gender approach, the results of which can be seen inFigure 5for the deep vein thrombosis outcome with the observation period defined using medical events only. Additional plots for the age + gender approach can be found inAppendix S5. The 99th percentile of average time between events was the only persistence window used and across all outcomes and databases, the surveillance window using the 90th percentile of average time between events produced IRs that came closest to the IRs produced using the enrollment time, especially when only medical events were used (except for Merative MDCR). A similar pattern in Merative MDCR observed in the persistence + surveillance approach was also observed in the age + gender approach, with the lower surveillance windows coming closer to approximating the IR using enrollment time. The age + gender approach did not perform as well in the rarer outcomes, specifically narcolepsy and anaphylaxis.

Heatmap of mean squared error between enrollment IR and age + gender IR using medical events only for the outcome deep vein thrombosis.

When using RWD for epidemiologic evidence generation the definition of observable time is critical and can lead to biased estimates if done incorrectly. Underlying almost every study relying on the secondary use of health data is a phenotype with some notion of “prior observation time” or a covariate balancing method that requires accurate look-back periods to produce balanced treatment and comparator arms.22–24Despite this, the calculation used to determine observable time per person is rarely discussed.

Without explicit enrollment or registration information inferring eras of observable time is especially difficult. The question is less about the completeness of the data, for example, if the patient sought care with a different provider not captured by the data source, and more about the time span after an encounter when a patient is still considered observed. In other words, when a patient leaves 1 health visit, what is the span of time until their next expected visit? In real-world data, it is possible to observe large spans of time with no health events and it is unknown whether these spans of time are truly devoid of events or if they are periods of incomplete data capture. Many encounter-based databases like EHRs use a first event to last event or min/max approach, but this assumes continuous observation during that time which may not be accurate.

To investigate how differing definitions of observable time in observational health databases impact evidence we conducted a large methodological experiment. By applying different persistence and surveillance windows (either static or based on age and gender) to string together health events into observation periods, we were able to test how IRs generated using these observation periods differed from each other as well as those generated using enrollment-time as a reference standard (enrollment-based databases only) and the min/max approach (encounter-based databases).

Our findings demonstrate a clear inverse relationship between observation window length and IRs: shorter windows resulted in higher IRs, while longer windows reduced them. This effect is intuitive, as increasing the denominator by extending observation time naturally lowers IRs. However, this phenomenon has not been empirically explored in detail before. Comparing the persistence + surveillance approach with enrollment time, we found that in U.S. claims databases (excluding Merative MDCR), the lowest persistence windows combined with the highest surveillance windows produced IRs closest to those obtained using enrollment time, and vice versa.

The Merative MDCR database showed differing trends when using the persistence + surveillance method and when comparing the IRs generated using this method to the enrollment IR. The requirement of a wash-out period of 365 days for many of the outcomes studied caused the IRs to increase between the 180 and 365 persistence window estimates. This indicates that age plays a factor in how observation periods should be defined in encounter-based databases, but the age and gender model did not perform as well as expected. In future the age and gender model should be refined to better represent the healthcare utilization patterns of patients.

Ultimately, our findings emphasize that the assumptions made about a database and its underlying population play a pivotal role in shaping IR estimates. More research is needed to determine the most appropriate approach for defining observation periods, particularly in databases without explicit enrollment information. Developing standardized methodologies for inferring observable time, while considering the specific characteristics of different database types, will be essential to improving the reliability and comparability of IR estimates in real-world evidence generation.

The observation period is a critical determinant of study validity, affecting both transparency and reproducibility in epidemiologic research. To improve data fitness-for-use, observation periods should be systematically assessed to ensure that the data capture process adequately supports answering clinical questions. Clearly defined observation period methodologies, tailored to specific database types, are essential to mitigating bias and enhancing the reliability of real-world evidence.

Clair Blacketer, 
Coordinating Center, Observational Health Data Sciences and Informatics (OHDSI), New York, NY, 10032, United States; 
Department of Medical Informatics, Erasmus University Medical Center, Rotterdam, NL, 3015 GD, United States; 
Johnson & Johnson, Raritan, NJ, 08869, United States.

Frank J DeFalco, 
Coordinating Center, Observational Health Data Sciences and Informatics (OHDSI), New York, NY, 10032, United States; 
Johnson & Johnson, Raritan, NJ, 08869, United States.

Mitchell M Conover, 
Coordinating Center, Observational Health Data Sciences and Informatics (OHDSI), New York, NY, 10032, United States; 
Johnson & Johnson, Raritan, NJ, 08869, United States.

Patrick B Ryan, 
Coordinating Center, Observational Health Data Sciences and Informatics (OHDSI), New York, NY, 10032, United States; 
Johnson & Johnson, Raritan, NJ, 08869, United States; 
Department of Biomedical Informatics, Columbia University, New York, NY, 10032, United States.

Martijn J Schuemie, 
Coordinating Center, Observational Health Data Sciences and Informatics (OHDSI), New York, NY, 10032, United States; 
Department of Biostatistics, University of California, Los Angeles, Los Angeles, CA, 90095, United States.

Peter R Rijnbeek, 
Coordinating Center, Observational Health Data Sciences and Informatics (OHDSI), New York, NY, 10032, United States; 
Department of Medical Informatics, Erasmus University Medical Center, Rotterdam, NL, 3015 GD, United States.