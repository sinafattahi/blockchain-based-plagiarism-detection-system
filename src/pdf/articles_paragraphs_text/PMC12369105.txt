YouTube has become a widely used platform for surgical education, especially in otolaryngology. Despite its popularity, the educational quality of available content remains largely unregulated. This study aimed to evaluate the quality of endoscopic type 1 tympanoplasty and myringoplasty videos on YouTube using two validated assessment tools: the modified IVORY and the LAP-VEGaS systems.

A systematic search was conducted on YouTube in November 2024 using specific keywords. A total of 105 English-language videos demonstrating endoscopic type 1 tympanoplasty or myringoplasty were included based on predefined inclusion criteria (view count ≥ 1.000, duration 1–60 min, and accessible engagement metrics). The IVORY scoring system was modified to better fit the procedure and the YouTube platform. Two independent otolaryngologists evaluated all videos using the modified IVORY and standard LAP-VEGaS scoring systems. Inter-rater reliability was assessed with Cohen’s Kappa and Intraclass Correlation Coefficients (ICC). Relationships between video characteristics and educational quality were analyzed using Spearman correlation and linear regression.

The mean LAP-VEGaS score was 5.9 ± 2.8, with 63.8% of videos rated as low quality (score < 7). The mean IVORY score was 16.4 ± 3.8, and 95.2% of videos received an F grade academically. A strong positive correlation was observed between LAP-VEGaS scores and video likes (r= 0.474,p< 0.001), view rate (r= 0.411,p< 0.001), and views (r= 0.299,p= 0.002), while a negative correlation was found with video upload age (r= − 0.435,p< 0.001). Similarly, IVORY scores were positively correlated with likes (r= 0.349,p< 0.001) and view rate (r= 0.258,p= 0.008), and negatively correlated with upload age (r= − 0.346,p< 0.001). Linear regression showed that the number of likes (β = 0.465,p= 0.002) and upload age (β = − 0.339,p< 0.001) significantly predicted LAP-VEGaS scores. For IVORY scores, upload age was a significant negative predictor (β = − 0.001,p< 0.001). Inter-rater reliability was excellent for both scoring systems, with ICC values of 0.896 for LAP-VEGaS and 0.910 for IVORY.

Most YouTube videos on endoscopic type 1 tympanoplasty and myringoplasty lack high educational quality. Implementing structured guidelines and specialty-specific video evaluation tools is crucial to enhance the educational value of online surgical content. Promoting adherence to instructional design principles may improve the effectiveness of freely available video resources.

The online version contains supplementary material available at 10.1186/s12909-025-07775-7.

In recent years, internet use has become more critical for acquiring surgical skills. Surgeons frequently use YouTube, and the number of surgical videos on the platform is steadily increasing. These videos are accessible to both patients and healthcare professionals. Nevertheless, since there is no review process to evaluate the accuracy of information, video quality, and reliability before upload, the quality of these videos may be significantly poor [1,2]. Various general evaluation criteria have been established for assessing videos on video-sharing platforms. Although video quality scales like the Journal of the American Medical Association (JAMA) and DISCERN are useful, they have been found to be inadequate for evaluating otolaryngology (ENT) surgical training videos. In response, the IVORY guidelines were introduced in 2020, specifically designed for evaluating ENT educational videos [3–7]. Additionally, the LAP-VEGaS grading system, originally created for assessing laparoscopic surgical videos, has also been used for evaluating endoscopic surgical videos [8].

In recent years, the design of surgical videos has gained attention from both medical and educational researchers. To understand how video features influence learning, Mayer’s Cognitive Theory of Multimedia Learning provides a useful theoretical framework. According to this theory, people learn more effectively from a combination of words and visuals than from words alone. Videos that reduce extraneous cognitive load, manage intrinsic cognitive load, and encourage active processing are more likely to support meaningful learning. Simply presenting information isn’t enough for learning to happen; the learner must form a mental connection with the material. To facilitate this, certain video features are important: high visual quality, no distracting background noise, synchronized narration or on-screen text, and content segmented to highlight key ideas [9]. Although the main purpose of the IVORY and LAP-VEGaS scoring systems is to assess the quality of surgical videos, many of their criteria closely match the core principles of Mayer’s theory. This alignment between educational video design and multimedia learning principles may become especially important when applied to procedure-based training such as otologic surgery.

Tympanoplasty and myringoplasty are among the most common otologic surgical procedures performed in the field of otolaryngology. While the learning curve for these procedures varies from person to person, it is also influenced by whether the procedure is performed endoscopically or microscopically. Literature has shown that the learning curve for endoscopic tympanoplasty is more challenging than for microscopically performed surgeries [10,11].

Learning ear anatomy and ear surgery during otolaryngology residency can be challenging. Endoscopic techniques have helped accelerate learning by allowing simultaneous visualization of the anatomy while making procedures less invasive (i.e., non-incisional) [12]. Recent studies demonstrate that surgical training videos can enhance surgical skills, and video-based learning offers a valuable supplement to traditional surgical education [13–17]. As a result, video-based learning methods are increasingly incorporated into residency curricula to speed up the training of young surgeons [18]. While previous studies have evaluated the quality of videos related to general otolaryngology or other surgical procedures, there is a notable lack of focused assessment on videos demonstrating endoscopic type 1 tympanoplasty and myringoplasty — two commonly performed middle ear surgeries. To our knowledge, this is the first study to utilize both LAP-VEGaS and a modified IVORY system to assess the educational quality of endoscopic type 1 tympanoplasty and myringoplasty videos on YouTube.

The aim of this study is to objectively analyze the quality of videos on YouTube as an educational tool in learning how to perform endoscopic type 1 tympanoplasty and myringoplasty, one of the key indicator cases for trainees in otolaryngology.

Videos were accessed through searches on the YouTube website (www.youtube.com). The searches took place between November 1 st and 3rd, 2024. The search terms “endoscopic tympanoplasty,” “endoscopic type 1 tympanoplasty,” and “endoscopic myringoplasty” were used, and videos were sorted by the highest number of views. Videos with at least 1.000 views, lasting more than 1 min but less than 1 h, and with visible like and dislike counts were included in the study. Only videos featuring endoscopic cases were considered; videos showing microscopic surgeries were excluded. The URLs, video durations, like/dislike counts, comments, publication dates, and countries of origin were recorded. Only English-language videos were evaluated. To prevent counting the same video more than once, video titles were noted, and only one video per title was included. Videos with disabled comments were excluded from the comment statistic analysis.

Like rate was calculated as the number of likes/[number of likes + number of dislikes]; view rate was measured as the number of views/number of days since the video was uploaded; and video power index was assessed as [like rate × view rate]/100 [1].

Videos were evaluated using both the LAP-VEGaS and IVORY assessment guides. Each video was scored by two otolaryngologists. One had three years of specialization, and the other had ten years of experience as a specialist. The two otolaryngologists watched the videos and reached a consensus during their evaluations.

The LAP-VEGaS system was used to evaluate each video. The nine items of the system were scored from 0 (not present) to 2 (fully presented). Supplemental Table1displays the LAP-VEGaS scoring system. Based on the total score, videos were divided into three educational quality groups: low (0–6), medium (7–12), and high (13–18) [8].

Item 9: “When available, peer review of the video is recommended before publication, assessing the procedure (e.g., scientific validity, safety) as well as the quality of educational editing.” This question was removed, as YouTube does not have a peer-review system for videos.

Item 12: “Relevant preoperative workup should be shown. Imagery should be explained by arrows/overlays.” In tympanoplasty surgeries, preoperative workups were considered to include audiometry and CT scans. These items were scored separately in the organ-specific section. This item was excluded to avoid duplication.

Item 17: “Relevant pathology shown during the film should be identified and named. A picture of the specimen (with ruler) may be included if applicable.” This item was removed, as there is no pathological specimen in type 1 tympanoplasty or myringoplasty surgeries.

The organ-specific section has been updated for endoscopic type 1 tympanoplasty and myringoplasty. Two new items have been added to highlight key operative steps for these procedures and otology-specific recommendations.

In the IVORY rating system, each item is scored on a scale of 0 to 2. After adjustments, the scoring system was revised to a maximum of 40 and a minimum of 0 (Supplemental Table 3).

Although both the LAP-VEGaS and IVORY grading systems are structured tools designed to evaluate the educational quality of surgical videos, they differ in scope and target audience. LAP-VEGaS, initially created for laparoscopic surgery videos, focuses on general principles of educational video production, including structure, visual clarity, and instructional content. IVORY, on the other hand, was specifically developed for otolaryngology surgical videos and incorporates organ-specific items relevant to ENT procedures. By using both systems in our study, we aimed to compare a general framework with a specialty-specific one to see if they offer complementary insights. Additionally, we analyzed the correlation between scores to examine how closely the two systems align in assessing video quality. Our modifications to the IVORY system further tailored it for otologic procedures, providing an opportunity to evaluate the relevance and adaptability of both frameworks within a specialized surgical context.

SPSS version 24.0 (IBM Corp., Armonk, NY, USA) was used for statistical analysis. Descriptive data are presented as frequencies (percentages), means, standard deviations (SD), medians, and minimum–maximum (min–max) values. The distribution of the data was evaluated using the Kolmogorov–Smirnov test. After determining that the continuous variables did not show a normal distribution, the relationship between video quality scores (LAP-VEGaS and IVORY) and video features was assessed using the Spearman correlation test. A p-value of < 0.05 was considered statistically significant.

To evaluate inter-rater reliability, Cohen’s Kappa coefficient was calculated for each item. Kappa values were interpreted as follows: 0.00–0.20 indicating slight reliability, 0.21–0.40 fair reliability, 0.41–0.60 moderate reliability, 0.61–0.80 substantial reliability, and 0.81–1.00 almost perfect reliability.

To assess overall inter-rater reliability at the test level, the Intraclass Correlation Coefficient (ICC) was calculated for the total LAP-VEGaS and IVORY scores. ICC values above 0.75 were considered indicative of high reliability.

Linear regression analysis was conducted separately for LAP-VEGaS and IVORY using two different modeling approaches.

This study was prepared in accordance with the principles of the Declaration of Helsinki and does not require ethics committee approval as it does not involve studies on patients or animals.

A total of 105 endoscopic type 1 tympanoplasty and myringoplasty videos were included in the study. The upload dates ranged from June 23, 2011, to November 9, 2024. The average video duration was 8.4 min. The most viewed video had 254,178 views, while the least viewed video had 1,015 views (Table1).

The mean LAP-VEGaS score was 5.9 (± SD 2.8) (Table2). According to the LAP-VEGaS grading, a total of 67 (63.8%) videos were classified as low quality, 37 (35.2%) as medium quality, and 1 (1.0%) as high quality (Table3). The LAP-VEGaS score showed a significant correlation with the number of views (r= 0.299,p< 0.002), view rate (r= 0.411,p< 0.001), video power index (r= 0.143,p< 0.001), likes (r= 0.474,p< 0.001), and dislikes (r= 0.223,p= 0.022). It also had a negative correlation with the video upload date (r= −0.435,p< 0.001) (Table4). The IVORY score was positively correlated with the LAP-VEGaS score across all sections except Section A (p< 0.001). Since both scales contain similar questions, this explains the observed correlation. Using both scales in our study aimed to assess their complementary roles and facilitate a comparative analysis.

The IVORY grading system had a maximum obtainable score of 40 points. The mean total IVORY score was 16.4 (± SD 3.8). The highest total score was 32, and the lowest was 10 (Table2). When classified according to the academic grading system used in the USA, 1 video (1%) received a B grade, 4 videos (3.8%) received a D grade, and 100 videos (95.2%) received an F grade [20] (Table3).

The total IVORY score was negatively correlated with both video length (r= −0.207,p< 0.034) and video upload date (r= −0.406,p< 0.001). The like rate was positively correlated with Section E (p< 0.003), while the number of dislikes was negatively correlated with Section E (r= −0.264,p< 0.006). The number of likes was correlated with Section B (r= 0.221,p< 0.023) and Section D (r= 0.307,p< 0.001), whereas video length was correlated with Section D (p< 0.040) but negatively correlated with Section B (r= −0.321,p< 0.001). Video Power Index (VPI) demonstrated a weak but statistically significant positive correlation with IVORY Section B (r= 0.220,p= 0.024) and total IVORY scores (r= 0.198,p= 0.043) (Table4).

There was no significant correlation between the number of comments and the quality scores on either the IVORY (p= 0.707) or LAP-VEGaS (p= 0.196) scales.

URLs of selected videos, video statistics, and score evaluations are shared in Supplemental Table 4.

Cohen’s Kappa coefficient was calculated as 0.794 for LAP-VEGaS item 6, 0.716 for LAP-VEGaS item 7, and 0.799 for LAP-VEGaS item 9. Among the IVORY scoring items, the Kappa coefficient was 0.783 for IVORY item 6 and 0.596 for IVORY item 14. For all other evaluation items, a high level of inter-rater agreement was observed. These results indicate overall good to excellent inter-rater reliability, with only IVORY item 14 showing a moderate level of agreement. The Intraclass Correlation Coefficient (ICC) for the total LAP-VEGaS score was 0.896 (95% CI: 0.892–0.900), reflecting very high consistency between raters. The ICC for the total IVORY score was 0.910 (95% CI: 0.906–0.914), demonstrating that the IVORY system can be reliably used by different raters when assessed as a whole.

The effects of likes, dislikes, number of views, and video age (number of days since upload) on LAP-VEGaS scores were examined using linear regression analysis. The model was statistically significant (F(4, 100) = 9.228,p< 0.001) and had a moderate level of explanatory power (R² = 0.270). The regression results showed that the variables significantly predicting the LAP-VEGaS score were the number of likes (B = 0.008,p= 0.002) and the number of days since upload (B = − 0.001,p< 0.001). As the number of likes increased, LAP-VEGaS scores also tended to rise, while older videos were linked to lower scores. Other variables (dislikes and views) were not significant predictors (p> 0.05).

A separate linear regression analysis was conducted to predict IVORY scores using video duration (in minutes) and number of days since upload as independent variables. The model was statistically significant (F(2, 102) = 10.050,p< 0.001), though it showed a low level of explanatory power (R² = 0.165). Among the predictors, only the number of days since upload was found to significantly predict the IVORY score (B = − 0.001,p< 0.001), while video duration was not statistically significant (B = − 0.081,p= 0.120). The results suggest that older videos tend to have lower IVORY scores.

Surgical specialization training has evolved considerably in recent years [21]. Especially during the Covid-19 period, the number of assisted surgeries decreased along with the total number of surgeries performed by residents continuing their training, leading to gaps in resident education [22–24]. Recently, surgical training has become more challenging due to the rise in resident numbers and the decline in available educators. For these reasons, the use of surgical videos in training has increased more frequently. Therefore, it is crucial to evaluate these videos, which serve as educational tools, objectively. Additionally, analyzing parameters such as the number of views, likes, and the length of these videos by correlating them with IVORY and LAP-VEGaS scores can provide insights for creating new videos. In our study, we found that the quality of endoscopic type 1 tympanoplasty and myringoplasty surgery videos uploaded to YouTube was generally low in terms of education. Previous studies assessing the quality of surgical videos in otolaryngology often used grading systems that were not specific to the field. These studies typically employed general quality scales like JAMA, DISCERN, or the Global Quality Score (GQS) [25–27]. Recently, some studies have evaluated otologic surgical videos using the IVORY grading system for procedures such as DSR, stapedectomy, and parotidectomy [1,2,18]. A common finding across these studies is that the videos assessed did not meet the expected educational quality standards. Similarly, in our study, the majority of the evaluated videos were scored as low-quality according to the IVORY scale.

In Mayer et al.‘s study, a correlation was found between the number of views and the total IVORY score videos [18]. However, in Yıldırım and Özdilek’s study, no such correlation was found [2]. In our study, the number of views was not significantly correlated with the IVORY score (p= 0.940), but there was a significant positive correlation between the number of views and the LAP-VEGaS score. (r= 0.299,p< 0.002).

In our analysis, we found that increasing video length had a negative impact on the IVORY score in Section B (technical aspects). This section includes questions that assess the video based on its duration, so longer videos are expected to have lower scores in Section B. Conversely, scores for Section D (surgical procedure) increased significantly with video length, likely because longer videos allowed for more detailed descriptions of the surgical process. However, the total IVORY score was negatively affected by longer video length in the correlation analysis, suggesting that the extended duration of some videos may not have been used effectively. In contrast, video duration was not identified as a significant predictor in the regression model, which may indicate the influence of potential interactions or the effect of other confounding variables.

Videos with a higher number of likes performed better in Sections B and D of the IVORY score, suggesting that providing technical details and thoroughly explaining the surgical procedure may enhance a video’s appeal. Furthermore, a significant positive correlation was observed between the number of likes and LAP-VEGaS scores, indicating that higher-quality videos tend to receive more viewer approval. This relationship was also confirmed in the linear regression model, where the number of likes emerged as a significant positive predictor of LAP-VEGaS scores (B = 0.008,p= 0.002), reinforcing the association between perceived video quality and viewer engagement.

A significant negative correlation was found between dislikes and Section E, indicating that videos lacking critical organ-specific components (such as the demonstration of essential steps in the surgical procedure) were more likely to attract dislikes. The like rate was positively correlated with Section E, meaning that videos with well-executed organ-specific sections were associated with higher like rates and fewer dislikes.

Older videos generally received lower quality scores, and this trend was supported by both correlation and regression analyses. In our study, the video upload date was significantly negatively associated with both IVORY and LAP-VEGaS scores (p< 0.001). Linear regression analyses further confirmed this relationship: video upload time was a significant negative predictor for both LAP-VEGaS (B = − 0.001,p< 0.001) and IVORY (B = − 0.001,p< 0.001) scores. These findings suggest that older videos may have lower technical and pedagogical quality, possibly due to limitations in recording equipment, editing capabilities, and the absence of standardized educational video guidelines at the time they were made. Interestingly, this pattern had not been consistently observed in previous IVORY-based studies, highlighting the potential impact of changing multimedia standards on perceived educational quality [1,2,18].

Inter-rater reliability is essential when assessing scoring systems such as IVORY and LAP-VEGaS, especially in studies involving subjective evaluations. In our research, Cohen’s Kappa values showed good agreement for items 6, 7, and 9 of the LAP-VEGaS score, while the other items demonstrated excellent agreement. For the IVORY score, item 6 had good agreement, item 14 showed moderate agreement, and all remaining items showed excellent agreement. The variability observed in Item 14 could result from differences in how raters interpreted and segmented the videos into distinct surgical steps, which may reflect the subjective nature of the scoring criteria. Overall, both scoring systems demonstrated high inter-rater reliability, with intraclass correlation coefficients (ICCs) above 0.75. These results align with previous studies using similar video assessment tools, which have reported acceptable reliability levels when raters are properly trained and scoring criteria are well-defined [24,28].

A review of the literature on YouTube and otolaryngology reveals that the focus has primarily been on evaluating patient informational videos [29–31]. The evaluation of surgical training videos has only recently gained attention [1,2,18,28,32]. While research on surgical education videos is fairly common in urology and general surgery, there have been fewer studies in otolaryngology.

In our study, the IVORY scoring system was modified to better suit the context of YouTube-based educational videos and the specific features of endoscopic type 1 tympanoplasty and myringoplasty procedures. For example, organ-specific criteria were adjusted to emphasize key video quality elements relevant to these surgical techniques. Similar modifications have been made in previous studies evaluating videos from various surgical fields [1,2,18]. Although such adjustments may reduce direct comparability with research using the original IVORY framework, they enhance the relevance and usefulness of the scoring criteria for our particular procedure. This customized approach allowed for a more detailed and context-aware assessment of video quality.

While this study primarily centered on objectively assessing video quality, it is also crucial to consider the broader context of content creation and user behavior on platforms like YouTube. Although content creators do not disclose their specific motivations for uploading surgical videos, it can be assumed that the videos—many of which are excerpts from live surgeries—mainly target an audience of otolaryngology residents and specialist physicians rather than patients seeking general information. These videos may serve various purposes, such as educational uses, showcasing surgical techniques, or generating income or visibility through video content. Regardless of the original intent, such videos can still offer educational value by demonstrating surgical techniques, anatomical landmarks, the use of new surgical instruments, procedural tips, or aspects of training that may have been insufficiently covered during residency. In a survey of 70 surgical residents, medical students, and faculty surgeons, 95% of participants reported regularly watching surgical videos before performing surgeries, with YouTube being the most popular platform. YouTube’s widespread accessibility—available as a mobile app on nearly all smartphones, free of charge, and open to global content upload and viewing—likely explains its popularity among surgeons as an on-demand educational resource [33].

Within the framework of widely accepted educational theories such as Bloom’s taxonomy and Miller’s pyramid of clinical competence, watching high-quality surgical videos can support lower-level cognitive objectives, including knowledge acquisition, comprehension, and recognition of procedural steps. Visually well-designed, structured, and pedagogically aligned videos help develop mental models, especially for learners in the early stages of surgical training [34–36]. The LAP-VEGaS and IVORY video assessment systems are valuable tools for evaluating the presentation quality of such materials and generally align with approaches like Mayer’s Cognitive Theory of Multimedia Learning. For instance, items related to the structured presentation of surgical steps and use of visual aids reflect Mayer’s segmenting and signaling principles, which facilitate learner attention and retention. Similarly, videos with minimal extraneous content and clear narration adhere to the coherence principle, reducing cognitive overload. However, these systems seem to fall short in capturing more comprehensive pedagogical dimensions, such as clearly defined learning outcomes, assessment of learners’ cognitive engagement, and measurement of educational effectiveness. Therefore, we believe that to more holistically define and evaluate educational quality, these scoring systems should be more closely integrated with advanced learning theories. Future scoring frameworks may benefit from incorporating validated educational metrics that align with broader instructional design and multimedia learning principles.

In European countries, otolaryngology residency training is based on the core curricula and logbooks outlined by the UEMS ORL Section and Board training requirements [37]. Currently, these programs do not include educational objectives related to using videos as instructional materials, nor do they cover training in video assessment, editing, or production. However, every resident is also a potential peer educator and future trainer. Active participation in the learning process is essential for reaching higher cognitive levels of learning. Activities like producing, editing, and narrating educational videos can foster deeper understanding by encouraging learners to synthesize and organize knowledge while taking on a teaching role. Familiarity with video assessment criteria is also helpful for creating new, high-quality content. Supervised video creation integrated into surgical education curricula may effectively bridge passive observation and active development of clinical skills. We believe future research should examine the role and impact of such video-based instructional strategies within structured training programs.

This study, to the best of our knowledge, is the first to evaluate the quality of endoscopic type 1 tympanoplasty and myringoplasty videos. Previous studies have generally focused on total IVORY scores, with little attention to section-specific analysis. We believe that analyzing videos segment by segment is essential, as viewer behaviors, view rates, likes, dislikes, and comments have been linked to specific parts of the videos. By focusing on these sections, video creators can potentially reach a larger audience and enhance video quality. We also recommend developing and widely adopting a specialized grading system for otolaryngology, instead of relying solely on general video scoring systems.

There are some limitations to our study. One limitation is that videos were selected solely based on their English language and a minimum view count. While these criteria were chosen to ensure accessibility, relevance, and comparability of content, they may have introduced selection bias by excluding potentially high-quality videos in other languages or with limited exposure. Using only English content may especially underrepresent surgical techniques from non-English-speaking countries. Similarly, restricting inclusion to videos with over 1.000 views might exclude recently uploaded or niche instructional videos that have not yet gained widespread visibility. Our research focused only on videos from a single platform (YouTube), and other platforms were not considered. Although YouTube offers broad accessibility and convenience for learners, it also has inherent limitations as an educational platform. Content on YouTube is not peer- reviewed, and the qualifications of content creators are often not disclosed, raising concerns about the accuracy, safety, and educational quality of surgical videos. YouTube’s engagement-based algorithm tends to favor videos with high watch time or user interaction, metrics that do not necessarily reflect educational value. As a result, newly uploaded yet high-quality videos, especially those made by academic institutions or experienced educators, may be overshadowed by more popular but less pedagogically sound content. While all analyzed videos were recorded during surgeries, they are likely to be clicked, liked, and commented on by viewers who are not necessarily healthcare professionals. This cycle, where popularity is driven by engagement rather than instructional merit, may limit learners’ access to the most relevant and well-prepared material. We did not formally assess or verify the credentials of video uploaders. It is plausible that content uploaded by experienced surgeons or academic institutions may differ in quality from that shared by non-experts or laypersons. Additionally, the motivations behind uploading these videos may not always be educational. As Luu et al. noted, the likelihood of uploaders knowing the IVORY and LAP-VEGaS guidelines is quite low [24]. Video uploaders were evaluated based on criteria they probably did not consider when creating their content. Another limitation is the lack of external validation for the modified IVORY scoring system. While modifications were tailored to better reflect the characteristics of endoscopic tympanoplasty and myringoplasty videos on YouTube, the newly added or omitted items have not yet undergone independent validation. Future studies should aim to assess the construct validity, reliability, and applicability of these modifications across different datasets and surgical procedures, while also expanding to multilingual content and exploring other platforms or institutional video libraries to gain a more comprehensive understanding.

Although the quality of endoscopic type 1 tympanoplasty and myringoplasty videos was found to be low in this study, platforms like YouTube can still make significant contributions to the educational process. By popularizing quality guidelines and adapting the evaluation scales to the specific field, surgeons can be taught how to improve video editing, thereby increasing the number of high-quality videos. These platforms can then play a more prominent role in surgical education.

All authors contributed to the study conception and design. Material preparation, data collection and analysis were performed by [Kerimcan Çakıcı], [Sabri Köseoğlu],. The first draft of the manuscript was written by [Kerimcan Çakıcı] and both authors commented on previous versions of the manuscript. Both authors read and approved the final manuscript.