Artificial intelligence (AI) has become a transformative tool in medical education and assessment. Despite advancements, AI models such as GPT-4o demonstrate variable performance on high-stakes examinations. This study compared the performance of four AI models (Llama-3, Gemini, GPT-4o, and Copilot) with specialists and residents on European General Surgery Board test questions, focusing on accuracy across question formats, lengths, and difficulty levels.

A total of 120 multiple-choice questions were systematically sampled from theGeneral Surgery Examination and Board Reviewquestion bank using a structured randomization protocol. The questions were administered via Google Forms to four large language models (Llama-3, GPT-4o, Gemini, and Copilot) and 30 surgeons (15 board-certified specialists and 15 residents) under timed, single-session conditions. Participant demographics (age, gender, years of experience) were recorded. Questions were categorized by word count (short, medium, long) and by difficulty level (easy, moderate, hard), rated by three independent board-certified surgeons. Group accuracy rates were compared using ANOVA with appropriate post-hoc tests, and 95% confidence intervals were reported.

Board-certified surgeons achieved the highest accuracy rate at 81.6% (95% CI: 78.9–84.3), followed by surgical residents at 69.9% (95% CI: 66.7–73.1). Among large language models (LLMs), Llama-3 demonstrated the best performance with an accuracy of 65.8% (95% CI: 62.4–69.2), whereas Copilot showed the lowest performance at 51.7% (95% CI: 48.1–55.3). LLM performance declined significantly as item difficulty and length increased, particularly for Copilot (68.3% on short vs. 36.4% on long questions,p< 0.001). In contrast, human participants maintained relatively stable accuracy across difficulty levels. Notably, only Llama-3 ranked within the human performance range, placing 26th among 30 surgeons, while all other LLMs failed to surpass the 60% accuracy threshold (p< 0.001).

Current LLMs underperform compared to human specialists when faced with questions requiring high-level medical knowledge, reinforcing their current role as supplementary tools in surgical education rather than replacements for expert clinical judgment.

The online version contains supplementary material available at 10.1186/s12909-025-07856-7.

The influence of artificial intelligence (AI) on healthcare is becoming increasingly evident, particularly in the domain of surgical education and practice. The applications of AI encompass a wide range of functions, from enhancing the accuracy of diagnostic procedures to improving surgical outcomes through advanced data analysis and decision support systems. In the field of surgical education, AI-driven technologies such as virtual reality (VR) simulations and AI-assisted diagnostic tools are being integrated to provide immersive learning experiences for medical students and residents [1,2]. However, the question remains as to whether AI can match or surpass human expertise in complex, high-stakes environments such as surgical board examinations.Surgical board examinations, including the European General Surgery Board examination, demand an in-depth understanding of complex clinical scenarios, procedural knowledge, and decision-making under uncertainty.These competencies are shaped by years of clinical practice, making them challenging for even the most advanced AI models to replicate [3,4]. While AI systems have been shown to excel at processing large datasets and recognizing patterns, they often lack the contextual understanding and adaptability inherent to human cognition [5,6]. A plethora of studies have indicated that artificial intelligence (AI) possesses considerable potential in enhancing decision-making processes, optimising surgical outcomes, and optimising resource allocation. Nevertheless, these studies have also underscored the indispensable role of human expertise in addressing complex, context-specific challenges that AI is yet to fully comprehend [7]. Furthermore, as AI becomes more integrated into surgical practice, understanding its limitations and potential areas for improvement is critical [8]. For instance, although AI models such as GPT-4 (OpenAI), Gemini (Google DeepMind), GitHub Copilot (Microsoft) and Llama-3 (Meta AI) have demonstrated proficiency in processing substantial volumes of medical data, they are deficient in addressing queries that necessitate sophisticated clinical reasoning and experience-driven decision-making [9,10]. Previous research underscores that while AI can facilitate the standardisation of educational practices, it cannot yet substitute for the nuanced assessment of experienced surgeons. A long-term perspective on the role of AI in medicine suggests a collaborative approach, where AI augments rather than replaces human expertise, ensuring that patient care benefits from both technological advances and human intuition [11,12].

Moreover, recent investigations into the use of large language models (LLMs) across foundational medical disciplines such as gross anatomy [13], medical embryology [14], and neuroscience education [15] have demonstrated encouraging yet limited outcomes. These models can support educational delivery and improve student interaction but still fall short of fully replicating human instructional depth and adaptability.

This study evaluates the performance of four state-of-the-art AI models on a set of European General Surgery Board exam questions, comparing AI-generated responses with those from board-certified surgerys and residents to assess the capabilities and limitations of AI in replicating human expertise. The focus is on understanding how AI ranks among humans in exams requiring complex surgical knowledge and clinical experience.

This comparative, cross-sectional study evaluated the accuracy of four large language models (LLMs)—Llama-3 (Meta), Gemini (Google), GPT-4o (OpenAI), and Copilot (Microsoft)—against human surgeons. A total of 120 multiple-choice questions (MCQs) were selected from the most recent edition of the General Surgery Examination and Board Review question bank, which contains 700 questions commonly used for European Board exam preparation [16]. The study received ethical approval from the Non-Intervention Scientific Research Ethics Committee of Afyonkarahisar Health Sciences University (approval number: 2024/500; meeting date: 13/12/2024; number: 2024/11).

For human participants, an invitation was sent to 65 general surgeons via professional social media platforms after ethical approval. A total of 42 surgeons consented to participate. To ensure exam realism and transparency, all participants received the questions at the same time, on January 5, 2025, at 10:00 AM, via a timed Google Forms link with a 120-minute limit. Prior notification was provided 5 days in advance via email. Of the 42 volunteers, 30 completed all 120 questions and were included in the final analysis: 15 board-certified surgeons and 15 surgical residents. Based on institutional email records collected during the pre-exam invitation process, none of the participants were affiliated with the author’s institution. The exam was administered on a weekend outside of working hours using personal devices, which further minimized the risk of inter-participant interaction or institutional bias.

To ensure representativeness, a systematic random sampling method was employed. The reference booklet contained blocks of six consecutive questions. For each block, a number between 1 and 6 was written on slips of paper and placed into a pouch by the study author. A hospital staff member from an unrelated department, who was not involved in the study, blindly drew one slip for each block. The selected number indicated which of the six questions within that block would be included. This procedure was repeated until 120 questions were obtained, ensuring balanced representation across the syllabus.

Each of the 120 MCQs was copied verbatim from a Word document and pasted one by one into the user interface of each LLM with the standard prompt: “Please answer the following multiple-choice question.” No additional context was provided, and each question was asked only once. All AI responses were collected on January 5, 2025, between 6:00 PM and 12:00 AM to maintain temporal consistency across models.

Item difficulty was independently assessed by three board-certified general surgeons with over 10 years of experience, all of whom had previously passed the European Board examination and were not affiliated with the author. Each item was rated on a three-point scale (Easy, Moderate, Hard), and final categorization was determined based on majority agreement. Although formal inter-rater reliability metrics were not calculated, question difficulty was categorized based on majority consensus among these three board-certified surgeons.

Question length was determined by total word count. All items were imported into a Word document, and word counts were calculated. Quartile values were then used to classify items as follows: those in the 1 st quartile were considered Short, the 2nd and 3rd quartiles Medium, and the 4th quartile Long.

The minimum number of questions was determined via power analysis using effect size data from a similar study on ChatGPT’s performance on pediatric surgery board questions, which reported an effect size of d = 0.33 [17]. With α = 0.05 and 80% power, 118 questions were deemed sufficient; thus, 120 questions were selected to ensure statistical adequacy.

Statistical analyses were conducted using IBM SPSS version 20.0. Normality of distribution was assessed using the Kolmogorov–Smirnov test, histograms, and skewness–kurtosis coefficients. For two-group comparisons, Student’s t-test was applied to normally distributed data, while the Mann–Whitney U test was used for non-normally distributed data. Categorical data were analyzed using the Chi-square or Fisher’s exact test.

Accuracy rates for each group followed a normal distribution and were compared across all six groups using one-way ANOVA. Homogeneity of variances was assessed using Levene’s test. When significant group differences were identified, post-hoc comparisons were performed. Tukey’s HSD test was used when variances were equal; Tamhane’s T2 was applied when variances were unequal. The choice of post-hoc test was explicitly based on the outcome of the variance homogeneity assessment. For example, age-based comparisons used Tamhane’s T2 due to unequal variances, while specialization-year comparisons used Tukey HSD due to variance equality.

Effect sizes were calculated using Cohen’s d for two-group comparisons and eta squared (η²) for ANOVA results. For all primary outcomes, 95% confidence intervals were reported. Statistical significance was set atp< 0.05.

All human participants provided informed electronic consent after being clearly informed that the test was voluntary, unrelated to any academic or economic consequences, and designed solely for scientific purposes. All questions were selected from publicly available resources, ensuring no copyright infringement. Generative AI tools were used in accordance with their standard license agreements; therefore, no additional permissions were required.

The present study incorporated a total of four artificial intelligence (AI) models (Llama-3 (Meta), Gemini (Google), GPT-4o (OpenAI), and Copilot (Microsoft) and 30 human participants. The demographic characteristics of the human participants, including gender, age, professional status, and years of specialization, were analysed. Each participant was administered 120 questions selected for the study, and their responses were recorded as correct or incorrect. The accuracy rates were then compared with the aforementioned demographic variables.

When comparing the accuracy rates of human participants with other independent variables, no statistically significant difference was found between gender and accuracy rates (p= 0.507). However, a statistically significant difference was observed between age groups; participants aged 36–40 performed better than those aged 18–25 (p= 0.012). With regard to years of specialisation, non-specialist participants demonstrated significantly lower accuracy rates compared to other groups (p< 0.001). No statistically significant difference was found between accuracy rates and years of specialisation within the specialist group (p= 0.574). When participants were grouped according to years of specialisation, specialists were found to have significantly higher accuracy rates (81.6 ± 7.3) than residents (69.3 ± 5.1) (p< 0.001)(Table1).

A comparative analysis of responses to all questions by both AI models and human participants revealed that specialists demonstrated the highest accuracy rate (81.6%), while Copilot exhibited the lowest accuracy rate (51.7%). Notably, residents achieved an average accuracy rate of 69.9%, which surpassed that of all AI models. Among the AI models, Llama-3 attained the highest accuracy rate (65.8%)(Table2and Fig.1). Among the 30 human participants, all of whom surpassed the accuracy threshold required for correct responses, only Llama-3 managed to secure a position, ranking 26th among surgeons. In contrast, the other AI models failed to qualify for the ranking and were unable to exceed the 60% accuracy threshold, highlighting their limited performance compared to human participants in this examination.

Questions were categorised according to their length as short, medium, or long. The accuracy rates of the AI models, residents, and specialists were then analysed for these categories. Llama-3, Gemini, and GPT-4o demonstrated their highest accuracy rates on medium-length questions (67.4%, 60.9%, and 63.0%, respectively). Conversely, Copilot attained its maximum accuracy of 68.3% on short questions and its minimum accuracy of 36.4% on long questions, indicating a significant disparity compared to other AI models and human participants. Among human participants, both residents and specialists achieved the highest accuracy rates on short questions (85.6% and 74.4%, respectively)(Table3; Fig.2).

Questions were categorised according to difficulty level as follows: easy, moderate, and difficult. It was observed that for all AI models and human participants, the highest accuracy rates were achieved on easy questions, while the lowest accuracy rates were observed on difficult questions. This distribution indicates that the question difficulty levels were appropriately calibrated. A comparison of the accuracy rates for easy and difficult questions revealed that Gemini exhibited the most significant disparity (42.9%, 81.0%−38.1%), while specialists demonstrated the least pronounced gap (25%, 93.6%−68.6%). For easy and moderate questions, Gemini once again exhibited the largest gap (28.2%, 81.0%−52.8%), while GPT-4o demonstrated the smallest gap (2.8%, 66.7%−63.9%). Conversely, for questions of a moderate and difficult nature, the largest gap was observed in GPT-4o (28.2%, 63.9%−35.7%), while specialists demonstrated the smallest gap (14.2%, 82.8%−68.6%)(Table4; Fig.3).

This study evaluates the performance of four AI models (Llama-3 (Meta), Gemini (Google), GPT-4o (OpenAI), and Copilot (Microsoft)) against human participants (including specialists and residents) on preparation questions for the written examination of the European Board of General Surgery.To the best of my knowledge, this is the first study to compare the performance of real surgeons and multiple artificial intelligence models in this context. The findings revealed that human participants, particularly specialists, outperformed AI models across various metrics, underscoring the current limitations of AI in replicating the expertise required for such high-stakes examinations.

The field of Artificial Intelligence (AI) has undergone rapid advancements, significantly impacting various disciplines and demonstrating a remarkable capacity for innovation and transformation in the acquisition and utilisation of knowledge [18]. Previous studies in the field of medical question-answering have predominantly focused on specific, narrowly defined tasks, placing a higher priority on the optimisation of models than on their broader applicability [19,20].

The performance of artificial intelligence models on board examination preparation questions, particularly in medical specialties, has become a subject of interest, with studies conducted across several disciplines. However, the majority of these studies have primarily focused on the most widely used AI model, ChatGPT (OpenAl) [21,22]. Utilising a dataset comprising 12,723 questions derived from Chinese medical licensing examinations, Jin et al. Reported an accuracy rate of 36.7% [23]. Similarly, Bicknell et al. Reported an accuracy of only 60% on a dataset of 750 USMLE Step 1 and Step 2 questions in 2024 [24]. In the study conducted by Benjamin et al. on questions prepared for neurosurgical written board examinations, ChatGPT-4(OpenAl) demonstrated an accuracy rate of 53.2%. While this result indicates suboptimal performance in comparison to that of surgical residents and specialists, it is noteworthy that ChatGPT-4 (OpenAl) outperformed medical students [25]. In a similar vein, a study by Tzu-Ling Weng et al. on family medicine board preparation questions in Taiwan found that ChatGPT exhibited a 41.6% accuracy rate, underscoring its limited proficiency in answering exam questions [26]. In their study on the American College of Gastroenterology Self-Assessment Test, Suchman et al. Reported that ChatGPT-3 (OpenAl) achieved an overall accuracy of 65.1% on 455 questions, while GPT-4 (OpenAl) scored 62.4%. The authors concluded that ChatGPT failed to pass the test and emphasized that such AI models should not be used for medical education in gastroenterology [27].

In the present study, the highest accuracy rate was exhibited by specialists (81.6%), whilst Copilot (Microsoft) demonstrated the lowest performance with an accuracy rate of 51.7%. It is noteworthy that residents achieved an average accuracy of 69.9%, which surpassed the performance of all AI models. Among the AI systems evaluated, Llama-3 (Meta AI) demonstrated the highest accuracy, attaining a rate of 65.8%.

Copilot’s inferior performance may be explained by its design architecture, as it is primarily optimized for code generation tasks rather than complex medical reasoning. Unlike other models fine-tuned with clinical or instructional datasets, Copilot lacks domain-specific contextual understanding, making it more vulnerable to semantic complexity and multi-step logic demands present in longer clinical questions [28,29].

Gilson et al. Evaluated the performance of ChatGPT (OpenAI) on the USMLE, analysing its ability to answer Step 1 and Step 2 questions for interpretability. Using two sets of 120 free questions from AMBOSS and the National Board of Medical Examiners, the study revealed a notable decline in ChatGPT’s performance as question difficulty increased, particularly in the AMBOSS Step 1 dataset [29]. In a related study, Kollitsch et al. Compared ChatGPT-3.5, ChatGPT-4 (OpenAI), and Bing AI on questions from the European Urology Board examinations, revealing that the accuracy of these AI models decreased significantly as question difficulty increased [30].

In the present study, the difficulty of the questions was found to have a significant impact on the performance of both human participants and AI models. It was observed that both groups demonstrated optimal performance on questions of low difficulty, while accuracy levels exhibited a decline as the difficulty of the questions increased. Notably, specialists maintained relatively high accuracy levels, even on questions that were considered to be difficult (68.6%), while Gemini’s accuracy exhibited a substantial decrease from 81.0% on easy questions to 38.1% on difficult ones.

Beyond question-answering accuracy, the application of large language models in medical education may extend to automated question generation. Emekli and Karahan demonstrated that AI-generated multiple-choice questions in radiography education exhibited acceptable levels of difficulty and discrimination, comparable to faculty-generated items [31]. In this regard, LLMs could be leveraged not only to answer but also to create clinically relevant questions, supporting the development of reasoning skills and expanding the scope of question banks used in surgical education.

Another issue that has been highlighted in studies involving artificial intelligence models is their tendency to provide inconsistent answers when the same questions are repeated, as well as variations in response quality following updates to the AI systems [32]. Consequently, a plethora of studies have been conducted to compare the responses provided by ChatGPT’s 3.5 (OpenAI) and 4 versions, with the objective of evaluating differences in performance and consistency [33]. A substantial number of studies have determined that ChatGPT-4 (OpenAI) has been found to provide responses that are more accurate in comparison to its predecessor, ChatGPT-3.5. This enhancement in performance can be attributed primarily to the advanced and updated version of the model. For instance, Azizoğlu et al. Reported that on pediatric surgery board examination questions, GPT-3.5(OpenAI) correctly answered 46 out of 105 questions (43.8%), whereas GPT-4 demonstrated significantly improved performance, correctly answering 80 questions (76.2%) [34]. In a related investigation, Gencer et al., in their study on thoracic surgery exam questions, reported that ChatGPT-4 (OpenAI) achieved an accuracy rate of 93.33%, surpassing the performance of both medical interns and ChatGPT-3.5 (OpenAI) [35].

In the present study, Llama-3, the most recently developed AI model in the LLaMA series, was included. Human participants consistently demonstrated higher accuracy, even on challenging questions. Notably, Llama-3(Meta AI) was the only AI model to secure the 26th rank among surgeons, whereas the other AI models failed to qualify for ranking and did not surpass the 60% accuracy threshold. The comparatively superior performance of Llama-3 may be attributed to its extensive pretraining on diverse biomedical and general data, as well as its robust instruction tuning techniques. These features enhance its capacity to process multi-step reasoning and adapt to question complexity in a manner more aligned with clinical decision-making frameworks [29,36].

Although current AI models fall short in matching expert performance, their educational utility remains significant. Peláez-Sánchez et al. emphasized that large language models can facilitate more autonomous and interactive learning experiences by adapting to student needs and promoting Education 4.0 principles [37]. From this perspective, even suboptimal-performing models may serve as supplemental tools for self-directed learning, patient education, and simplifying complex surgical concepts for lay audiences. By tailoring explanations and offering interactive guidance, such tools can support both student comprehension and patient engagement.

The present study is subject to several limitations. Firstly, the questions were sourced from a European General Surgery Board preparation book rather than from actual past examinations, as official board questions are not publicly accessible. Secondly, the test lacked oral and visual components, which are integral to real-life board examinations, limiting the ecological validity of the results. Thirdly, the exam was administered via Google Forms under self-directed conditions without live proctoring, which may have introduced variability in participant focus or environment. Additionally, all participants were required to complete the test on the same weekend day and at the same time, which, while necessary to minimise information sharing, reduced the number of surgeons who could ultimately participate. Item difficulty was independently rated as Easy, Moderate, or Hard by three board-certified general surgeons with over 10 years of experience, and final classification was based on majority agreement. Although inter-rater reliability statistics (e.g., Fleiss’ kappa) were not formally calculated, this is acknowledged. Moreover, the study concentrated on quantitative measures of accuracy and did not assess qualitative aspects of clinical reasoning, contextual judgement, or adaptive response. Lastly, as AI models continue to evolve rapidly, their performance may improve significantly in future iterations, and these findings should be interpreted within that context.

The results revealed that LLMs performed significantly worse than both human groups, underscoring their current limitations in supporting knowledge-intensive surgical board examinations. While these models demonstrate notable promise in augmenting medical education, they presently lack the advanced clinical reasoning and contextual comprehension required for such cognitively demanding tasks. Continued refinement of LLM architectures is essential before they can reliably assist in high-level assessment or decision-making in surgical training.

The authors would like to express their sincere appreciation to Dr. Mustafa Azizoğlu for his conceptual contributions during the early design of this study. We also thank Dr. İsmail Zihni, Dr. Sinan Şener, and Assoc. Prof. Dr. İsa Sözen for their valuable assistance in the classification of item difficulty levels. We are grateful to Assoc. Prof. Dr. Fatih Akkuş and Dr. Osman Gerçek for their support in the statistical analysis. Finally, we extend our thanks to the reviewers and the editorial team of BMC Medical Education for their constructive feedback and valuable suggestions that helped improve the quality of this manuscript.

All procedures followed were in accordance with the ethical standards of the responsible committee on human experimentation (institutional and national) and with the Helsinki Declaration of 1975, as revised in 2008. Informed consent was obtained from all participants.

During the preparation of this work, I, Melih Can Gul, used ChatGPT (OpenAI) to support language editing and improving the readability of the manuscript. After using this tool, I reviewed and edited the content as needed and take full responsibility for the content of the publication.

Conceptualization: M.C.G.; methodology: M.C.G.; Software, M.C.G.; validation, M.C.G., formal analysis, M.C.G.; writing—original draft preparation, M.C.G., writing— review and editing, M.C.G., supervision, M.C.G. The author reviewed the manuscript.

There is no specific funding related to this research.