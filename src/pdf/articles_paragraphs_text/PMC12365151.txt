Accurately identifying the stages of lung adenocarcinoma is essential for selecting the most appropriate treatment plans. Nonetheless, this task is complicated due to challenges such as integrating diverse data, similarities among subtypes, and the need to capture contextual features, making precise differentiation difficult. We address these challenges and propose a multimodal deep neural network that integrates computed tomography (CT) images, annotated lesion bounding boxes, and electronic health records. Our model first combines bounding boxes with precise lesion location data and CT scans, generating a richer semantic representation through feature extraction from regions of interest to enhance localization accuracy using a vision transformer module. Beyond imaging data, the model also incorporates clinical information encoded using a fully connected encoder. Features extracted from both CT and clinical data are optimized for cosine similarity using a contrastive language-image pre-training module, ensuring they are cohesively integrated. In addition, we introduce an attention-based feature fusion module that harmonizes these features into a unified representation to fuse features from different modalities further. This integrated feature set is then fed into a classifier that effectively distinguishes among the three types of adenocarcinomas. Finally, we employ focal loss to mitigate the effects of unbalanced classes and contrastive learning loss to enhance feature representation and improve the model’s performance. Our experiments on public and proprietary datasets demonstrate the efficiency of our model, achieving a superior validation accuracy of 81.42% and an area under the curve of 0.9120. These results significantly outperform recent multimodal classification approaches. The code is available athttps://github.com/fancccc/LungCancerDC.

Lung cancer stands as one of the most prevalent cancers worldwide and is the leading cause of cancer-related deaths among men. It ranks as the second leading cause of cancer deaths among women, with its incidence rising each year1,2. Lung cancer is primarily categorized into two types: small cell lung cancer (SCLC) and non-small cell lung cancer (NSCLC), with NSCLC accounting for about 85% of all cases3. Within NSCLC, lung adenocarcinoma (LUAD) emerges as the most common subtype4. LUAD is further divided based on histological characteristics and the degree of tumor invasion into three categories: adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive adenocarcinoma (IA)5,6. AIS is localized within the alveoli, and early detection often leads to successful surgical intervention, resulting in a high cure rate and a positive prognosis. Likewise, recognizing MIA at an early stage enables timely treatment, which can halt its progression to more aggressive invasive adenocarcinoma, thus enhancing survival rates. Even though IA is already invasive, early detection significantly improves patient outcomes by reducing the risk of recurrence through surgical and adjuvant therapies. Therefore, accurately differentiating among these types of adenocarcinoma is essential for healthcare professionals to ensure effective early diagnosis and treatment.

Low-dose computed tomography (CT) is the main method for early screening and diagnosis of lung cancer7. It enables doctors to assess vital tumor characteristics such as size, shape, location, metastasis, and heterogeneity, which are crucial for identifying the type of lung cancer. However, this process depends heavily on the expertise of experienced physicians, which can lead to variability in diagnostic opinions. Furthermore, the subtle histological differences among adenocarcinoma subtypes make distinguishing them based solely on visual CT features difficult. To address these challenges, diagnosing adenocarcinoma subtypes frequently requires integrating clinical information, including patients’ electronic health records (EHR). Physicians often combine this clinical data with CT findings to enhance diagnostic accuracy1. While this approach improves the precision of diagnoses, it also significantly increases the workload for healthcare professionals and reduces efficiency.

The advancement of machine learning and radiomics has facilitated automated diagnosis using CT images8. Radiomics involves extracting histological and morphological features from these images based on established knowledge and specific rules, with these features often demonstrating strong associations with clinical outcomes. However, raw images need pre-processing to identify regions of interest (ROI), such as cancerous areas, which typically involves manual annotation by skilled radiologists or segmentation algorithms. Once features are extracted, they usually undergo further selection to retain only the most relevant ones, often employing statistical methods like LASSO and principal component analysis9. Subsequently, machine learning models, including supervised methods like random forest10, support vector machine11, and generalized linear models12, or unsupervised methods like k-nearest neighbors13are applied14. Nonetheless, radiomics-based approaches sometimes fail to fully utilize information from CT images because significant data is lost during feature extraction, including regions adjacent to lesions that might correlate with the disease type. Additionally, the feature selection process can lead to further information loss, and traditional machine learning models, with their limited capacity for parameters, often face challenges in achieving optimal model fitting and performance.

In the past decade, developments in deep learning technologies have helped address challenges in the early detection of lung cancer15. Convolutional neural networks (CNNs), such as the ResNet series16, have greatly enhanced the accuracy of recognizing features in CT scans. ResNet addresses the vanishing gradient problem in deep network training by using residual connections, which allows for the creation of deeper networks with improved accuracy and performance. More recently, the Transformer architecture17, especially the Vision Transformer (ViT)18, has outperformed CNN-based models while also reducing computational demands. Additionally, the contrastive language-image pre-training (CLIP) model19has shown significant promise as a vision-language tool. Deep learning automates the extraction of visual features from CT scans, serving as a valuable quantification tool. This automation facilitates the identification of various lesions, offering faster and more precise diagnostic support for physicians20,21, and alleviates the workload on radiologists22. In multi-view, Zhou et al.23introduced an ensemble multi-view 3D CNN model that excels in risk stratification of lung adenocarcinoma, Luo et al.24proposed cross-aligned representation learning, even surpassing experienced doctors in evaluating invasive adenocarcinoma. Despite these advancements, accurately differentiating cancer subtypes remains challenging. Wang et al.25combined different models, which enhanced preoperative diagnosis. Recent neurological studies have significantly advanced the development of Spiking Neural Networks (SNNs). Despite these advancements, the learning methods for SNNs are not yet fully understood26. In addressing this gap, Agarwal et al. introduced a dual Encoder-Decoder framework specifically designed for processing CT images27. Building on this work, they also developed a multi-scale dual-channel feature embedding decoder aimed at improving biomedical image segmentation28. Furthermore, Mandal et al. explored optimization techniques by implementing a Real Coded Genetic Algorithm to effectively reduce errors29. Existing models often struggle with classification accuracy because lesions are confined to small areas within CT scans, which contain considerable redundant information. Furthermore, effectively distinguishing cancer subtypes necessitates integrating both visual data and patient clinical information, a task that traditional network models find difficult to manage.

The development of multimodal approaches has also significantly enhanced the capabilities of deep learning in the medical field. Yang et al.30examined the CT features of lung adenocarcinoma across different demographic groups and found that these features vary between genders and age groups. Studies by Yu et al.31and Guo et al.32have demonstrated notable advancements. Wang et al.33proposed an approach that integrates multiple imaging modalities but only visional modal. Integrating features from multiple sources often yields superior results compared to relying on a single feature set34. Vale-Silva et al.35introduced the MultiSurv, a model applied across 33 different cancer types, which extracts features from visual data using CNN models like ResNeXt-5036while processing clinical data through fully connected networks. The model employs a max-pooling method to combine these feature representations. However, this straightforward fusion might overlook important information within each modality and their interactions. Similarly, TMSS37utilizes a ViT encoder to process multimodal data by integrating CT, positron emission tomography (PET) scans, and EHR for tasks such as segmentation and prognosis. Another approach, CLIP-Lung38, uses a textual knowledge-guided framework to predict lung nodule malignancy. The LLM-guided model39employs large language models to analyze clinical notes and align them with image data. Although this model effectively integrates multimodal data and examines the relationships between different features, it struggles with information redundancy in large CT datasets, potentially overlooking critical histological features in lesion areas.

We enhance the semantic representation of CT images by leveraging bounding boxes around lesions to focus feature extraction on the ROI, which improves localization accuracy and ensures more precise lesion characterization.

We propose a two-stage attention mechanism that integrates multi-head self-attention and channel attention, effectively coupling features across different modalities and assigning appropriate weights to each one for improved model performance.

Our model generates a rich contextual feature representation by incorporating CT scans of varying resolutions, enabling more accurate differentiation of cancer subtypes.

We evaluate the proposed CMMFNet model on public and proprietary datasets to verify its superiority over state-of-the-art methods.

This study introduces the CMMFNet, a multimodal fusion network designed to diagnose lung adenocarcinoma by integrating CT images, positional data, and clinical information. As depicted in Fig.1, the process begins with the merger of three key inputs:. Here,represents the small-sized images cropped from the original CT scans using lesion bounding boxes,are the corresponding large-sized CT images, andrepresents the clinical data. These inputs are processed through the CLIP module, which extracts features from these diverse modalities. To enhance feature alignment, we calculate the cosine similarity between the extracted features, which informs the contrastive loss calculation. The resulting fused features, referred to as, are further refined through the Deep Fusion Framework (DFF), an attention-based module that facilitates deeper integration of the features. Ultimately, the integrated features are passed through a classification head to produce the diagnostic outcome. In the following subsection, we discuss each component of the CMMFNet in detail.

The overall architecture of the proposedCMMFNet. It contains(a) CLIPand(b) DFFmodule. CT with bounding boxes are processed by an encoder based on the ViT module,(c) CTE, to capture and integrate multi-scale image information. EHR data is encoded using an FC-Base module,(d) CLE, to extract and integrate clinical text features.

We employ a CLIP-based architecture to extract features from CT scans and clinical data, effectively. This architecture processes two sizes of CT images, each associated with lesion-bounding boxes, alongside structured clinical information. This process results in the extraction of features denoted as, representing each modality, respectively. The primary components of this model are as follows.

The CT Encoder (CTE)module, depicted in Fig.1(c), is built on the ViT framework. Initially, a CT imageis processed through a 3D patch embedding technique. For an image with dimensions, it is divided into patches using a patch size of 8, resulting inpatches. Each patch is embedded in a vector space ofdimensional, forming a matrix, where.

The bounding box, centered atwith dimensions, is adjusted through a fully connected layer to match this vector space, resulting in a matrix. The matricesandare concatenated into. Positional encoding with learnable parameters is then applied to this concatenated matrix. The matrix is subsequently processed through a standard 12-layer ViT module for encoding. The output from the final layer of this module is averaged across the channel dimension to produce the final output.

The Clinical Encoder (CLE)module, shown in Fig.1(d), utilizes a fully connected network to encode clinical information. This process starts with EHR data, represented as. The EHR data is initially transformed through a fully connected layer to reshape it to. Positional encoding with learnable parameters is then applied to enhance the model’s comprehension of the data’s structural relationships and uncover latent interactions among features. The transformed vector is subsequently processed through another fully connected layer, adjusting its dimensions to produce. This final transformation aligns the clinical feature length with that of the visual modality feature.

The logitsandderived from both image modalities and clinical data are then used to calculate the contrastive loss, as detailed in SectionDynamic loss regulation.

To achieve feature fusion, we develop an attention-based module, referred to asDFF, as shown in Fig.1(b). Initially, the featureundergoes a two-step sequential process using the attention-based fusion modules. This approach effectively integrates the features. The fused features are then passed to the classification head, which produces the final category output.

Phase 1:To integrate the concatenated multimodal features, we treat them as a unified entity and utilize an-layer Transformer block to identify intrinsic correlations among these features. Each Transformer block is composed of multi-head self-attention and a feed-forward network. The input features pass through multiple attention layers, enabling the model to capture global dependencies within the data. After processing through the final Transformer block, layer normalization is applied to the output to ensure stable training.

whereare the projection matrices, andis the dimension of each attention head. Here,refers to one of theattention heads, and the relationholds. The matrixis the output projection matrix.

This normalized output,, is then prepared for the subsequent phase, Phase 2.

whereandrepresent the average and max pooling processes, respectively, andspecifies the pooling region indices. The weightsandare learnable parameters of the fully connected layers, withas the reduction ratio. The sigmoid functionis used for normalization.

This dynamic adjustment ensures that if one loss component becomes larger, its weight is reduced, balancing the two loss values.

whereandare the logits from the image and text models, respectively, andis the true label for the-th image-text pair. The functionrepresents the cross-entropy loss between logitsand label. This approach ensures equal consideration of image-to-text and text-to-image relationships during training, with the final loss averaged over all pairs, thereby improving alignment between corresponding representations.

whererepresents the predicted probability for the true class label,acts as a balancing factor for the class, and the parameteris the focusing parameter. A highervalue increases the emphasis on difficult examples, improving performance in the presence of class imbalance. We setbased on the class distribution (e.g., inversely proportional to class frequencies) to help address the imbalance between classes.

The open-source LPCD dataset42, comprises DICOM images from CT and PET-CT scans, featuring bounding boxes that mark tumor locations and include associated clinical data. This dataset consists of 342 lung cancer instances, which are classified into 246 cases of adenocarcinoma, 55 cases of small cell carcinoma, 36 cases of large cell carcinoma, and 5 cases of squamous cell carcinoma. Given the limited number of squamous cell carcinoma instances, the large cell and squamous cell carcinoma categories are combined, resulting in a three-class dataset with the following distributions: 70.7% adenocarcinoma, 17.3% small cell carcinoma, and 12.0% for the merged category.

The private LUNA-M dataset, sourced from the Cancer Hospital & Shenzhen Hospital in Shenzhen, China, includes 1,614 cases of lung adenocarcinoma from 1,430 anonymized patients, comprising 545 males and 885 females. Each case contains CT scans, clinical data, and bounding boxes marking tumor locations. Patients’ ages range from 19 to 86, with half between 46 and 64. The dataset is divided into 63 cases of AAH, 299 of AIS, 389 of MIA, and 863 of IA. Due to the low occurrence and pathological similarity of AAH to adenocarcinoma, AAH and AIS are merged into a single category, creating a three-class dataset with 53.5% IA, 24.1% MA, and 22.4% for the combined AAH and AIS. Initial bounding boxes, which provide approximate 2D annotations of the largest lesion cross-section, are refined to accurately define the lesion’s physical location and depth. This refinement results in a 3D bounding box that fully encapsulates the lesion.

Dataset Processing:All CT scans initially have dimensions ofin a 3D volume, whereindicates the number of scans. The slice intervals range from 0.625 mm to 5 mm, but all scans are resampled to a uniform resolution of 1. To ensure the images are within a relevant intensity range, windowing and truncation are applied to the pixel values, restricting them to the rangeHU, a common practice in medical imaging to focus on the most relevant tissue densities. Each lesion is then center-cropped into 3D blocks of sizesand, with bounding boxes adjusted to relative positions. The accuracy of this cropped data is validated using ITK-SNAP43. For data augmentation, random flips are applied to the CT scans along each axis (i.e., axial, sagittal, and coronal), enhancing the model’s robustness and generalization capabilities. Corresponding adjustments are made to the bounding boxes to maintain accurate lesion localization after the flip. Additionally, the EHR data undergoes preprocessing by standardizing numerical variables and applying one-hot encoding to categorical variables.

Implementation Details:Our experiments were conducted using a 48G NVIDIA A40 GPU with CUDA Version 12.2. The computational setup included Python 3.11 and PyTorch 2.4. To train the model, we employed five-fold cross-validation. This process involved dividing the dataset into five subsets using stratified sampling, where each subset was used once as a validation set while the remaining four subsets were used for training. This approach ensured that class distributions remained consistent across all folds by employing stratified splitting. We employed the Adam optimizer, starting with a learning rate of, a weight decay of 0.001, and beta set to (0.9, 0.99) to handle momentum and adjust the learning rate efficiently. Furthermore, the ExponentialLR scheduler was applied to gradually reduce the learning rate during training, using a gamma value of 0.99. During the training phase, we configured the number of epochs to 500 and used a batch size of 24. For the loss function parameters, we set the initial weight of the contrastive loss to 0.5, as specified in Eq. (13). Additionally, in Eq. (17), thehyperparameter for focal loss was adjusted to be inversely proportional to class frequencies, while thevalue was fixed at 0.25.

Evaluation criteria:The evaluation of all methods was conducted using widely recognized metrics to ensure a fair and comprehensive comparison. These metrics include Accuracy, Precision, Recall, F1-Score, and Area Under the ROC Curve (AUC). These metrics collectively offer a holistic view of the models’ classification performance, ensuring that improvements are not biased towards a single metric but reflect an overall enhancement in prediction quality.

In our experiments with the LUNA-M and LPCD datasets, as indicated in Table1, our method consistently surpassed other approaches. The baseline experiments, drawing from previous studies16,18, focused on performance analysis, particularly with the LUNA-M dataset. This focus was necessary due to the highly imbalanced class distribution in the LPCD dataset, which led to predictions overly favoring a single dominant class across all samples. ResNet18 showed superior accuracy compared to the ViT model, achieving improvements of 1.24% and 13.00%. We believe this is due to the ability of CNNs to effectively capture subtle lesion regions. In contrast, Transformer-based models like ViT tend to focus on the overall structure of the image, which can limit their effectiveness in highlighting small, localized areas. Further experiments were conducted using images of varying resolutions. Results from the LUNA-M dataset indicated that CT32 images outperformed CT128 images. This finding underscores the negative impact of redundant information outside the lesion areas on network performance. Specifically, using CT32 images resulted in accuracy improvements of 4.03% and 15.79%. These outcomes highlight the critical importance of concentrating on the region of interest (ROI) to enhance the performance of the network.

Comparison of the proposed method with baseline methods and state-of-the-art approaches on the LPCD and LUNA-M datasets.Boldtext highlights the best indicator, whileunderlinedtext represents the second-best.indicates an increase, andindicates a decrease.

We compared our proposed method to several state-of-the-art multimodal approaches35,37–39. While these methods typically use single-size CT images () and EHR as inputs, our approach integrates a more comprehensive set of inputs, including dual-size CT images, BBOX, and EHR. On the LUNA-M dataset, our method achieved the highest performance metrics: Accuracy at 81.42%, Precision at 78.12%, Recall at 79.39%, F1-Score at 78.63%, and AUC at 0.9120. These results represent significant improvements over the second-best scores, with increases of 2.47% in Accuracy, 3.33% in Precision, 2.81% in Recall, and 3.18% in F1-Score, in addition to a modest AUC gain of 0.0073. The ROC curves presented in Fig.2illustrate that our method, CMMFNet, achieves the most optimal curve position, reflecting its excellent discriminative capacity across various classification thresholds.

Similarly, our method ranked highest on the LPCD dataset, with an Accuracy of 81.16%, Precision of 87.89%, F1-Score of 64.23%, and AUC of 0.8567. These metrics also showed notable improvements over the next best results, with gains of 1.45% in Accuracy, 11.22% in Precision, 0.90% in F1-Score, and 0.0036 in AUC. The results on external datasets such as LPCD42further validate the effectiveness of our proposed method and demonstrate its strong generalization capability, indicating its applicability to different datasets with similar modalities.

Overall, our method achieved leading performance across both datasets. By leveraging state-of-the-art multimodal approaches35,37–39, rather than relying solely on CT images, we substantially enhanced accuracy. Furthermore, our proposed method outperformed existing techniques across all evaluated metrics, underscoring its effectiveness and robustness.

To evaluate the effectiveness of each component, we conducted ablation studies on the LUNA-M dataset, with the results presented in Table2. The findings reveal that utilizing two scales of CT inputs increases accuracy by 1.24% compared to single-scale inputs, highlighting the benefit of multi-scale data in improving model accuracy. For 128-size CT scans, theenhanced with bounding boxes showed improvements of 15.17% in accuracy, 45.96% in precision, 30.96% in recall, 40.69% in F1-Score and 0.3057 in AUC compared toCT128, which is using original CT data. This underscores the advantage of incorporating lesion location information for CT enhancements. OurCT32,CT128 model is designed to focus on cropped lesion regions, or Regions of Interest (RoI), to reduce the inclusion of non-lesion areas. As shown in Table2, ablation experiments were performed using images with bounding boxes. In contrast, experiments involvingCT32 andCT128 were conducted without the use of bounding boxes. Figure3illustrates how ROI-based feature extraction enhances semantic representation across different CT scan sizes, directing more attention to tumor areas.

Results of ablation experiment on LUNA-M.Boldtext highlights the best indicator.indicates an increase, andindicates a decrease.

Enhancing semantic representation through ROI-based feature extraction.

The integration of clinical information further boosted the model’s performance. Compared to the best-performing single modality (), incorporating multimodal data resulted in improvements of 4.03% in accuracy, 6.49% in precision, 6.96% in recall, 6.65% in F1-Score, and 0.0236 in AUC. These results highlight the significant advantage of our approach in effectively integrating multimodal data for enhanced predictive performance. The results demonstrate that the combination ofyields a consistent improvement of approximately 0.6% across all metrics compared to. This suggests that including images with bounding boxes enhances performance.

The integration of clinical information further boosted the model’s performance. Compared to the best-performing single modality (), incorporating multimodal data resulted in improvements of 4.03% in accuracy, 6.49% in precision, 6.96% in recall, 6.65% in F1-Score, and 0.0236 in AUC. These results highlight the significant advantage of our approach in effectively integrating multimodal data for enhanced predictive performance. The results show thatleads to a consistent improvement of approximately 0.6% across all metrics compared to. This indicates that incorporating images with bounding boxes contributes to the performance boost.

Experiments with the DFF module revealed that attention-based fusion significantly enhanced overall model performance. The model achieved peak accuracy and AUC scores of 81.42% and 0.9120, respectively, with the DFF module contributing improvements of 2.78% in accuracy, 2.56% in precision, 2.44% in recall, 2.68% in F1-Score, and 0.0133 in AUC across all metrics. These results indicate that each component of our approach, including multi-scale input, lesion-aware enhancement, and deep feature fusion, is crucial for enhancing model performance, working synergistically to optimize effectiveness. The AUC primarily assesses the model’s ability to rank positive and negative samples globally, offering a broad evaluation across various classification thresholds. It is not sensitive to class distribution. On the other hand, the DFF module is designed to enhance classification performance at a specific threshold, typically set at 0.5. Consequently, the DFF module markedly improves metrics like accuracy and F1-score, but its influence on the overall ranking performance, as indicated by AUC, is limited. This suggests that while the DFF module strengthens the model’s ability to distinguish samples around the decision threshold, its impact on global ranking capability is relatively minor.

Student’s t-test:Statistical hypothesis testing is crucial for determining whether observed performance improvements are due to actual model enhancements or merely random variations in the data44. The Student’s t-test is a widely used method for comparing the means of two related groups to assess whether their differences are statistically significant. To determine if the improvements in experimental metrics were due to random fluctuations or genuine model enhancements, we conducted a t-test on various result sets. Specifically, we used a paired t-test, suitable for comparing performance metrics before and after model modifications. The calculatedp-value for our approach in the comparative experiment is 0.0059, and in the ablation experiment, it is 0.0009. Since both values are statistically significant (), these results confirm that the observed improvements in performance are indeed due to the proposed model enhancements.

The confusion matrix, as depicted in Fig.4, highlights the model’s performance in predicting lung adenocarcinoma subtypes (IA, MA, AIS). The model demonstrates high accuracy for IA, with 149 cases correctly classified and minimal misclassification into MA (18 cases) and AIS (6 cases). This suggests that the distinct features of IA are effectively captured by the model. However, there is significant confusion between MA and AIS, indicative of their overlapping radiological and pathological characteristics. Specifically, 7 cases of MA were misclassified as IA, and 12 as AIS, while 6 AIS cases were predicted as IA, and 11 as MA. These misclassifications underscore the challenge of distinguishing these subtypes, emphasizing the need for more refined feature extraction or model adjustments. Improving the model’s performance could involve incorporating richer imaging features or leveraging multimodal data, such as clinical and molecular biomarkers. These enhancements would better differentiate between the subtypes and reduce ambiguity, thereby improving the model’s clinical applicability and decision-making reliability.

In this study, we evaluate various models based on inference computation, memory usage, and parameter size, as shown in Table3. We use multiply-accumulate operations (MACs) as the key measure for computational complexity. Memory usage indicates the GPU memory required during inference, and parameter size reflects the model’s complexity.

Comparison of Model Inference Computation Across Different Datasets (Measured by MACs).

In the performance comparison, CMMFNet stands out, especially in achieving an AUC score of 91.20, outperforming all other models. Despite having relatively higher computational complexity, memory usage, and parameter count, these attributes contribute to its outstanding performance, particularly in managing complex tasks and enhancing classification accuracy. Models like TMSS and ViT (CT128) are similar to CMMFNet in terms of computational requirements, with MACs and memory usage figures of 87.54G and 141.23MiB, respectively. Nevertheless, they lag behind in AUC scores, achieving only 88.96 and 52.41 compared to CMMFNet’s superior results. Conversely, methods such as ResNet18, which demand less computational power, exhibit poor AUC performance when measured against CMMFNet. Consequently, CMMFNet makes efficient use of computational resources to deliver significant improvements across various performance metrics, underscoring its potential and advantages for real-world applications.

This paper introduces an innovative deep multimodal network, CMMFNet, designed to classify lung adenocarcinoma subtypes effectively. The network employs the CLIP module for feature extraction and contrastive learning, while the DFF module integrates features from different modalities. Comprehensive experiments on both in-house and public datasets show that CMMFNet outperforms several leading multimodal models across various metrics. By applying ROI-based feature extraction, the network can focus more effectively on the tumor areas, which is especially useful when dealing with adenocarcinoma subtypes that exhibit subtle histological differences. Contrastive learning further enhances the consistency between visual and text-based features, improving the model’s ability to align multimodal information. An attention-based feature fusion mechanism strengthens the coupling of features across different modalities, ensuring that the model can leverage both visual and textual data more effectively.

Nevertheless, this study has several limitations. First, the method relies on initial annotations of lesion bounding boxes for CT cropping and ROI-based feature extraction. Accurate localization of the tumor is crucial for the network to capture both local and global information of the tumor areas. Second, in our approach to EHR data, we focused only on the values, without fully utilizing the semantic meaning of the EHR fields. Different EHR fields contain diverse indicators, and the network’s weights can only be applied to datasets that share the same EHR field structure. For datasets with different field configurations, additional training is required. However, training with additional fields necessitates a large amount of data, which can be a limiting factor. Future work will aim to improve lesion localization, facilitating automated image cropping and enabling the model to capture more detailed lesion features. We also plan to evolve the model into a more comprehensive end-to-end solution. Additionally, we will gather more EHR data to train on specific fields, improving the model’s ability to generate text feature representations in medical terms, ultimately enabling zero-shot transfer similar to CLIP.

In conclusion, the proposed CMMFNet demonstrates reliability in accurately predicting lung adenocarcinoma subtypes. Additionally, its robust performance and integration of multimodal data highlight its potential as a valuable tool to assist doctors in clinical decision-making and treatment planning. In conclusion, the proposed CMMFNet demonstrates reliability in accurately predicting lung adenocarcinoma subtypes. Additionally, its robust performance and integration of multimodal data highlight its potential as a valuable tool to assist doctors in clinical decision-making and treatment planning.

This work was supported by Guangdong Basic and Applied Basic Research Foundation (No. 2025A1515011617, 2022A1515110570), the China Ministry of Education Humanities and Social Sciences Research-Planning Fund Project (No. 23YJA910007), Zhejiang Provincial Philosophy and Social Sciences Planning Project-Major Project (No. 40023SYS12ZD), First Class Discipline of Zhejiang - A (Zhejiang University of Finance and Economics-Statistics), Innovation Teams of Youth Innovation in Science and Technology of High Education Institutions of Shandong Province (No. 2021KJ088), Shenzhen Science and Technology Program (No.KCXFZ20201221173008022), Shenzhen High-level Hospital Construction Fund, Shenzhen Clinical Research Center for Cancer (No. 287).