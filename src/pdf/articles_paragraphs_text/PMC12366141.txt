Infant mortality rate (IMR) is a critical indicator of a nation’s health and socio-economic development. It represents the number of deaths per thousand live births during the first year of life. It is widely recognized as an essential metric for assessing the overall well-being of a population, especially in the context of maternal and child health. The study attempts to analyze infant mortality rate data using one of the well-known time series models known as the auto-regressive integrated moving average (ARIMA) model. This article mainly focused on classical as well as Bayesian estimation of the parameters of the model considered. To write the likelihood of the ARIMA model, we have used the approach of Kalman filtering. A Random Walk Metropolis algorithm has been used to deal with analytically intractable posterior results from the ARIMA model. After performing the Bayesian analysis of competent ARIMA models, we have selected the most appropriate model using Akaike’s information criterion (AIC), the Bayesian information criterion (BIC) and K-fold Cross Validation. Kalman forecast has been performed for infant mortality growth rate data to attain the prospective predictions. Finally, a numerical illustration has been provided for the annual IMR growth rate data of India from 1950-2023. Among the competing models, ARIMA(5,1,0) is identified as the best-fitting model with minimum AIC, BIC, mean squared error (MSE) and some other error metrics. Forecasts based on this model predict a steady decline in IMR from 2024 to 2033. These findings underscore the utility of Bayesian ARIMA modeling in demographic forecasting and public health planning.

Concern about the demographic characteristics (like fertility, mortality, migration etc.) is an important aspect of the advancement of the society. These characteristics always impact the social and economic growth of the nation. The infant mortality rate (IMR) is a vital sign of the socioeconomic progress and general health of a country. IMR, which stands for the number of fatalities per 1,000 live births during the first year of life, is widely accepted as a crucial indicator for evaluating a population’s general health, particularly when it comes to mother and child health.

Selecting the appropriate values forpandqis crucial after differencing data is an important part of fitting ARIMA model. This process typically involves drawing and analyzing the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots of the time series data. ACF is a plot of the correlation between a series and its lagged values, accounting for the correlation contributed by intervening lags. PACF is a plot of the partial correlation of a series with its own lagged values. This method gives a crude value ofpandq. Other methods of determiningpandqinclude AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).

Checking for stationarity is an essential step in building a good time series model. Differencing the data is used to make the time series stationary. Stationarity in time series data can be checked via visual inspection. However, the augmented Dickey-Fuller (ADF) Test (see [5]) is an important test to confirm the Stationarity. We will discuss the ADF Test later in detail.

While reviewing the literature, we have focused on classical as well as Bayesian time series literature. In classical literature, [4,6,7] are few important literature. These three books summarized most of the important articles in the classical time series. Anderson [6] played a pivotal role in the development and popularization of ARIMA models. Their methodology emphasized model identification, estimation, diagnostic checking, and model selection. Box et al. [4] one of the trusted literature in time series. They popularized the Box-Jenkins method to analyze time series data. Hamilton [7] discussed various recent concepts of time series with its detailed proof and theory. Since our objective is perform the Bayesian analysis of the time series data, we have reviewed Bayesian literature in more details.

In contrast to the classical approach, the Bayesian approach, the model parameters (andcoefficients) in the ARIMA model are treated as random variables with associated prior distributions. These prior distributions reflect the subject beliefs of experts about the parameters before observing the data. The likelihood of the parameters is then updated using prior distribution via the Bayes Theorem. The updated distribution (i.e. posterior distribution) thus combines the prior information and the likelihood of the data given the parameters. Like the classical approach, in time series literature, the Bayesian approach is also well established. Some important articles include [8–14]. Most of these articles can be considered breakthroughs in the Bayesian analysis of ARIMA models, starting from sophisticated numerical integration techniques. Among these articles [9,10] introduces Markov chain Monte Carlo simulation in the Bayesian analysis of time series models.

Although the ARIMA model has a variety of benefits, it is also not free from the drawback. One of the major drawbacks of the ARIMA model is the complicity in writing its exact likelihood. To handle this situation, the idea is to express the model in a particular form called state space representation. This representation provides a way to write the exact likelihood of the ARIMA model using an algorithm called Kalman Filter (see, for example, [15]). For a given model and set of observed variables, the Kalman filter produces successive one-step-ahead predictions conditional on the past and concurrent observations. Once we have point estimates for the one-step ahead predictions for the filtered state, we can then also calculate the variance of this filtered state variable.

ARIMA models are widely used in time series forecasting, their application in demographic contexts particularly using Bayesian methods, remains limited. This underutilization is primarily due to two reasons: (1) the analytical complexity of deriving the exact likelihood function for ARIMA models, especially in the presence of differencing and latent states; and (2) the computational challenges associated with Bayesian estimation, which often requires advanced sampling techniques like Markov Chain Monte Carlo (MCMC). Consequently, most demographic studies are based on classical ARIMA or alternative trend-based models.

This study contributes to bridging this gap by applying Bayesian ARIMA modeling to infant mortality data in India. We utilize Kalman filtering to compute the exact likelihood and implement the Random Walk Metropolis algorithm for posterior estimation. By doing so, we demonstrate that Bayesian ARIMA models can provide robust parameter estimates and credible predictive intervals, making them a valuable tool for demographic forecasting and public health planning.

There are a number of articles that present the Bayesian analysis of the ARIMA model for econometric and financial data. When we went through the literature review, we saw a few articles related to Bayesian analysis of ARIMA models for demographic data. However, none of these articles used the exact likelihood for the Bayesian analysis. To fill this gap, in this article, we will provide the Bayesian analysis of the ARIMA model using the exact likelihood. To obtain the exact likelihood, we have executed the Kalman filter algorithm given by [15]. The ARIMA model aims to capture trends and seasonal patterns that influence IMRs over time. Besides capturing the IMR pattern, our objective is also to forecast the infant mortality growth rate of India using the Bayesian approach. Many works have been written in this area. Few important ones are [16–18].

The Auto-regressive Integrated Moving Average (ARIMA) model is a reliable and popular way to extract patterns from time series data. An effective method for analyzing and projecting the behaviour of a stationary time series is the ARIMA(p,d,q) model. The model incorporates auto-regression (AR), Integrated (I) and moving average (MA) components to capture the error terms in the time series data as well as the influence of previous values (see, [4] for details). The AR component considers the relationship between a current observation and its past observations. The AR component involves regressing the current value on its own lagged values. The integrated (I) component involves differencing the time series data to make it stationary. It is important in time series analysis to ensure constant mean and variance over time. The MA component considers the relationship between a current observation and a residual error from a moving average model applied to lagged observations.

In the above equation,are independent Gaussian noise term.andare the AR and MA coefficients, respectively. To perform the analysis in an easy way and write the complete likelihood of the ARIMA model, we can represent it as a state space model. The next subsection will do the same.

Before analyzing time series data, one has to perform two important tasks. First is checking the stationarity and invertibility of the dataset, and second is determining the order of time series. These two are related because only after checking stationarity can we determine the orderd,pandq.

If the null hypothesis is not rejected, it suggests that differencing the time series (e.g., first difference) may be appropriate to achieve stationarity. If the null hypothesis is rejected, it implies that the time series is stationary and may require further analysis or modeling. And after calculating the test statistic and corresponding p-value, we can make an appropriate decision by accepting or rejecting the null hypothesis.

If the time series is not stationary, we need to make it stationary by differencing the series. If it is stationary, then the order; otherwise, we can make it stationary by takingddifferences. In this way, we can determined. After checking the stationarity and determiningd, our next objective is to determine the orderpandqof the ARIMA model. ACF and PACF plots of the time series data play an essential role in this case. The ACF plot helps us to select the moving average partqof the ARIMA model. Similarly, the PACF part helps us to determine the order of the auto-regressive part (p) of the ARIMA model. We will discuss this plot and its interpretation in theNumerical illustrationsection.

The state space representation is a compatible way to describe the time series models. It expresses a time series model regarding unobserved states and their evolution over time. This representation serves as a convenient form for implementing the ARIMA model. Besides, it also helps to write the exact likelihood of the ARIMA model.

This is a basic structure using diffuse initialisation to estimate the parameter of interest.

In the above equationis a vector of unobserved states at time,is an m x m state transition matrix of fixed coefficients that represents how the states evolve over time. The notationis avector of disturbances assumed to follow a multivariate normal distribution with zero mean and covariance matrix. The dimension ofcorresponds to that of the state vector, as determined by the state-space formulation andis a matrix that maps the white noise errors to the state space.

The plan is to find the conditional distribution ofandforbased on all the information given=(). Since the data is assumed to be Gaussian, we are able to carry out prediction and update steps in a straightforward way. With these understandings, we can progress our methodology by employing the Kalman filtering algorithm to obtain the full likelihood of the ARIMA model.

The Kalman filter is a recursive algorithm that estimates the hidden state of a dynamic system from noisy and incomplete observations. It operates in two key steps: prediction, where past information is used to estimate the next state, and update, where the new observation refines this estimate by weighing uncertainties. This iterative process allows adaptive learning and improved forecasting over time. In time series modeling, particularly in state-space ARIMA models, the Kalman filter enables exact likelihood computation and efficient forecasting, even with partial observations or random noise. Fundamentally, it is a Bayesian inference-based filter that performs state estimation in stochastic processes using observed data.

whereand,is prediction error ofgivenor innovation term.is the variance ofgiven.is the backwards Kalman gain matrix, which ranges between 0 and 1. This recursion is Kalman filter and once theis established, the predicting statetends to follow the mean and variance as,.

The algorithm can be initialised from, whereis the mean vector of the initial state distribution, andis the covariance matrix of the initial state distribution. This backwards pass does not need, and uses less memory. Here, Algorithm 1 is provided below. It is self-explanatory and very easy to apply.

In the state space representation of the ARIMA time series model, the state space form allows for structuring a model in terms of components, each of which holds a clear and direct interpretation. Also, every element of the model in the state vector involves randomness, and a priori information arises naturally. Therefore, adapting the Kalman filter theory is necessary to address practical challenges in constructing the likelihood function.

Bayesian formulation of any model depends on two functions: likelihood and prior. Likelihood contains all experimental information, while prior contains subjective information. The posterior distribution is proportional to the likelihood times prior distribution. In the next three subsections, we will explain the likelihood prior and posterior distribution of the ARIMA model.

In time series data analysis and forecasting, the ARIMA model plays a crucial role. This is perhaps because the use of the ARIMA model during the study period for observable events is more reliable, and one can find a more accurate prediction of an event. To deal with time series data using the ARIMA model, one should carefully consider the likelihood function, taking into account the AR and MA model parameters.

Evaluation of the exact likelihood function of the ARIMA model is not simple. However, one can write the partial likelihood of the ARIMA model in an easy manner. To write the exact likelihood, one can use the results of the Kalman Filter (see, for example, [20]).

The termis explained in10. The q is typically related to the degree of differencing in models.is not a parameter to estimate, it’s a technical device to express infinite prior variance (a very large number, e.g. 1e5). Moreover, comparable descriptions of the diffuse loglikelihood function have been embraced by [22,23]. MLE can simply be obtained by maximising the likelihood using a good optimisation method (say, for example, Newton-Raphson Method or Downhill Simplex method). Since our objective is to perform a Bayesian analysis, we will next determine the prior distribution for each parameter.

In the Bayesian framework, it’s essential to define the prior distribution. The rationale behind the Bayesian approach is to select a prior distribution that aligns with the assumed model and the parameters of interest. The process of selecting a proper prior distribution is a nuanced one that requires careful consideration of available information, expert opinions, and empirical evidence.

By leveraging these insights effectively, one can ensure prior distribution that leads to more informative and robust Bayesian analyses. Selecting a suitable prior entails incorporating pertinent knowledge or beliefs about the parameters, ensuring that the prior distribution conveys this information effectively while also acknowledging the associated uncertainty [24]. One can see [25,26] for details.

To elicit these hyperparameters of the distribution, we use multivariate normal priors for the AR coefficientsand the moving average coefficients. The hyperparameters, i.e., the mean vectors and covariance matrices, are selected using an empirical Bayes strategy in the absence of strong subjective prior information. For more details, readers can go through [25].

The prior mean vectorsandrepresent the initial estimates of the parameters, whileandare their respective prior covariance matrices. In this study,andare set equal to the corresponding maximum likelihood estimates (MLEs) obtained from classical analysis to incorporate prior knowledge. For the covariance matrices, we assume diagonal structures to reflect prior independence among parameters. Specifically, the diagonal entries ofandare carefully chosen to quantify uncertainty in each parameter while maintaining a simplified dependence structure.

Whereandare the MLEs of the corresponding parameters. Off-diagonal elements are set to zero. This structure represents a weakly informative prior that allows sufficient flexibility in posterior learning while anchoring the priors around empirically supported values.

In the above equation,is the vector of ARIMA parameter. The direct calculation of the posterior as described above presents challenges in obtaining a closed-form analytical solution. Consequently, Bayesian computational techniques offer an alternative approach for numerically extracting relevant information. The Metropolis-Hastings MH) method (in particular, the Random Walk Metropolis Algorithm) is employed to obtain posterior inferences by generating random samples and summary statistics from these samples. In the following section, we delve into the details of this algorithm, focusing on its effectiveness.

If the algorithm rejects the proposed value, the old value () is accepted with probability. After running thisNnumber of times, we get the Markov Chain with equilibrium distribution. The generated chain gradually converges to the desired distributionfor a largeN. Its straightforward implementation is one of the strengths of the RWM algorithm. The detailed algorithm is given in Algorithm 2.

Algorithm 2 illustrates the mechanism of the Random Walk Metropolis (RWM) algorithm. The step size () in the Random Walk Metropolis (RWM) algorithm is crucial for balancing efficiency. The optimal acceptance rate of the RWM algorithm is 0.23, and we should adjust the step size to achieve it. After the generation of the chain, one can easily get the posterior summary. For further details of this algorithm, we refer readers to [28–30].

The notation table provided here offers a clear reference for the time series model in Table1, enhancing readers’ understanding.

After the analysis of the model, it is crucial to select the best model among the different competing models. Obviously, the best model should to used to make predictions and forecast. This step is essential for ensuring that the chosen model effectively reflects the data generation process, enabling reliable insights and predictions. In this article, we have used two well-known model selection criteria for selecting the best model among the other competing models. These are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). We will discuss these methods in the next subsections.

In the above equation,kis the number of estimated parameters. The symbolindicates the maximum likelihood estimate of. A model with a minimum value of AIC is proffered among the other competing models.

In the above equation,is the maximum likelihood estimate of parameter vector. Moreover,kis the number of effective (independent) parameters, andNis the sample size. Similar to AIC, a model with the minimum BIC value is the optimal model among the competing models.

Here,is the total number of observations across all individuals and folds,is the actual observation for individualion variablejin foldk, andis the predicted value from the model trained without that fold.

Some suggested ARIMA models and Bayes estimates for real-world time series data sets are numerically illustrated in this section. The example serves as a key demonstration of how our models work in finding a true state of the system by showcasing the method’s practical utility and relevance to real-life problems. In addition to the analysis, we have also mentioned the forecast for future purposes.

We have taken a real data set of the IMR for India over the period of 73 years from 1950 to 2023 annually. The data set is given in the form of a time series from World Population Prospects. World Population Prospects is the twenty-seventh edition of official United Nations population estimates and projections that have been prepared by the Population Division of the Department of Economic and Social Affairs of the United Nations Secretariat. It presents population estimates from 1950 to the present for 237 countries or areas underpinned by analyses of historical demographic trends. This latest assessment considers the results of 1,758 national population censuses conducted between 1950 and 2023, as well as information from vital registration systems and 2,890 nationally representative sample surveys (UN-WPP). Table2shows the IMR values, and Table3shows the IMR growth rate in percentage.

After understanding the dataset, we have drawn the time series plot of IMR growth rate data and differenced IMR growth rate. These plots are given in Fig.1.

After plotting the IMR growth data, it can be observed that it is not stationary (see Fig.1a). However, after differencing it once, we obtain stationarity in Fig.1b. This shows that we can set. The ADF test also shows that unit root is not present for the first difference. The p-value (=0.31) is also greater than 0.05.

Selecting the appropriate values forpandqis crucial in building an effective ARIMA model for a given time series [7]. To determinepandq, we have drawn the ACF and PACF plots as mentioned inDetermining the ordersection. This plotting involves computing the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots of the time series data. ACF is a plot of the correlation of a series with its own lagged values. PACF plot is a plot of the partial correlation between a series and its lagged values, regressed the values of the time series at all shorter lags. ACF and PACF plots of the data are given in Fig.2.

The above ACF (see in Fig.2a) and PACF plots (see in Fig.2b) are shown with details. Significant autocorrelation spikes at specific lags may indicate periodic behaviour or a strong dependence on past values, as seen at lag 5, which is the highest. Significant spikes at multiple lags may suggest a mix of autoregressive and moving average components, indicating a more complex time series structure in the partial autocorrelation at lags 5 and 10, respectively. Furthermore, these plots provide valuable insights into the temporal dependencies within a time series, aiding in model selection and forecasting. Using all the nearest possible combinations of AR lag, fixing the difference at one time, and other nearest lag possible combinations of MA order, we go for the likelihood estimation and Bayesian estimation as well in the next section.

The primary aim of the study is to emphasise Bayesian analysis, a crucial aspect of establishing initial values to compute the MLE using the Newton-Raphson method. This initial value helps us to run the algorithm3.4. In this study, the ARIMA model results from the specified combinations of (p, d, q), namely (5,1,0), (5,1,1), (5,1,2),(5,1,3),(5,1,4),(5,1,5), (0,1,5), (1,1,5), (2,1,5),(3,1,5), (4,1,5) and (5,1,5). Since we have stationarity at the first lag, we have selected d = 1. Although the ACF and PACF plots suggest a lag of 5, we are not very sure about it. Therefore, we have selected these combinations ofpandq. We have computed the MLE for the mentioned models, along with their respective standard errors (SE), for the above-selected combinations ofpandqin the ARIMA (p,d,q) model, and their AIC and BIC values. The results are shown in Table4.

From Table4, it can be seen that the ARIMA models generally show consistent parameter estimates across different parameter specifications, with varying degrees of uncertainty (as indicated by the standard error, SE). Notably, the order (5,1,0) and (0,1,5) models have more stable parameter estimates with relatively lower standard errors compared to higher-order models like (5,1,3), (5,1,4), (5,1,5), where standard errors are larger, indicating less precise estimates. Additionally, the ARIMA models with moving average terms (e.g., order (5,1,1) and (5,1,2)) show slightly higher parameter variability, suggesting increased model complexity that may lead to poor precision.

Besides classical estimates, Table4presents AIC and BIC values. Based on these values, we can say that ARIMA models with orders (5,1,0), (5,1,1), (0,1,5) and (1,1,5) are performing better than the others. Also, it is well known that Bayesian analysis is computationally costly, due to the need for repeated likelihood evaluations and high-dimensional sampling using Markov Chain Monte Carlo (MCMC) methods. Each iteration of the Random Walk Metropolis algorithm requires evaluating the full likelihood via the Kalman filter, which increases computational load significantly. Therefore, to ensure tractability and focus on the most promising configurations, we restrict the Bayesian analysis to four models that showed the best performance in the classical model selection phase. Therefore, we plan to perform a Bayesian analysis for these four models only. In the next subsection, we will provide the Bayesian analysis of these four models.

To conduct Bayesian analysis, the initial step involves determining the prior hyperparameters. Since the Bayesian framework relies heavily on prior information, carefully selecting priors is crucial to avoid misleading results. As suggested inPrior distributionsection, the most appropriate prior for both theandparameters is the Multivariate Normal (MVN) distribution. We choose the hyperparameters as follows: The MLEofis considered as the prior mean for the respective AR models. The diagonal elements of the prior variance-covariance matrixis 2abs [,,...,]. The non-diagonal elements ofare considered to be zero. In the same way, we choose the MLE and diagonal elements of the prior variance-covariance matrix for theparameter of the MA model (details mention in3.2).

We now proceed to run the RWM algorithm, as discussed inRandom walk Metropolis (RWM) algorithmsection. The proposal scale,, has been chosen in the algorithm to keep the acceptance rate optimal. The initial values of the chain are set to the corresponding MLE. The algorithm has been run for 5e5 iterations. Under the aforementioned conditions, the optimal acceptance rate ranged fromto, indicating a low rejection rate of the algorithm.

Table5presents estimated posterior characteristics for different configurations of the models, which have been chosen according to the minimum AIC and BIC values. So, we have chosen four models of order (5,1,0). (5,1,1), (0,1,5), (1,1,5). The posterior summary includes the posterior mean, median, mode, and highest posterior density (HPD) intervals with a 0.95 probability.

By varying the hyperparameter of prior distributions and comparing the resulting posterior summaries, we observed that the estimates remained largely consistent, indicating robustness of the Bayesian inference. Across the models, the parameter estimates indicate variability in both magnitude and uncertainty. In general, the fifth lag of the AR or MA terms shows relatively larger means and wider HPD intervals, suggesting a stronger or more uncertain contribution at that lag. Comparing models, the ARIMA(5,1,1) and ARIMA(1,1,5) appear to capture richer dynamics due to the inclusion of both AR and MA terms, although some parameters show wide HPD intervals, implying caution in their interpretation.

From Fig.3shows the trace plots from 5e5 MCMC iterations show well-mixed, stationary chains for all five parameters, with no visible trends or drifts. The rapid fluctuations indicate low autocorrelation, suggesting good convergence and reliable posterior sampling. This analysis helps identify the most suitable model structure for forecasting while highlighting parameter uncertainty. Further details of this combination of models has been discussed in the next subsection.

In order to proceed with model selection, which is to make a comparison among models, and wish to know the best one among four models considered for Bayesian Analysis. We have used the AIC score, BIC score for model selection as discussed inBayesian Model selectionsection. Also, K-fold cross-validation (CV) is used to assess the predictive performance of the selected models on simulated time series data. For practical compatibility, K is commonly set to 5 or 10; this study chose K = 10, following [34]. The results are shown in Table6.

The Table6presents a comparative analysis of four Bayesian ARIMA model configurations (5,1,0), (5,1,1), (0,1,5), and (1,1,5)—based on evaluation metrics such as AIC, BIC, MSE, RMSE, and MAE. Among them, the ARIMA(5,1,0) model demonstrates the best performance, having the lowest AIC (18.68), BIC (30.14), and the most favorable error values (MSE = 4.72, RMSE = 2.17, MAE = 1.66). To provide a more comprehensive evaluation of model performance, we also computed Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) for each selected ARIMA model. These metrics offer additional insight into forecast accuracy, with RMSE being sensitive to large errors and MAE providing a more robust view of average forecast deviations. The ARIMA(5,1,0) model outperforms the others across all four criteria—AIC, BIC, RMSE, and MAE—reinforcing its status as the most reliable model for forecasting India’s IMR growth.

However, ARIMA(5,1,0) offers several additional advantages that justify its selection. First, it has a simpler structure with fewer parameters than ARIMA(1,1,5) or ARIMA(0,1,5), which reduces the risk of overfitting and enhances model interpretability. Second, in the Bayesian estimation process, the ARIMA(5,1,0) model demonstrated better convergence diagnostics (e.g., well-mixed trace plots and stable posterior distributions), indicating numerical stability and robustness. These practical and computational considerations, along with its marginally better predictive accuracy, make ARIMA(5,1,0) the most appropriate model for forecasting India’s infant mortality rate in this study.

This retrospective study examines the trend in interval estimates over the period 2015 to 2023. The intervals represent credible intervals, reflecting changes observed year-over-year. By systematically analysing these intervals, the study aims to understand the longitudinal behaviour of IMR growth rate, offering insights that may support future forecasting or decision-making.

The Table7presents forecasted IMR growth rates from 2015 to 2023, each accompanied by a 95% confidence interval. While the predicted values consistently show a decline in IMR growth( in %) over time, the widening confidence intervals (CI), especially in later years, indicate increasing uncertainty in the forecasts. This suggests that although the model anticipates continued improvement, the reliability of long-term predictions decreases as the forecast horizon extends.

Again, for validating the model’s accuracy and reliability we are comparing the forecasted IMR growth rates and their confidence intervals with actual observed data is essential. It shows how well the model reflects real trends, whether its uncertainty estimates are appropriate, and helps identify over or underestimations. This comparison also supports model refinement and builds credibility, making the forecasts more meaningful for evidence-based decision-making.

The Table8compares actual and forecasted IMR values from 2015 to 2023, along with the absolute error for each year. The results show that the model consistently overestimated IMR across all years, with forecasted values slightly higher than actual figures. While the forecast closely matched actual IMR in 2015 (smallest error: 0.22), the accuracy gradually declined over time, reaching the highest discrepancy in 2023 (1.51). This pattern suggests the model performs better for short-term predictions, but its accuracy diminishes as the forecast horizon extends. Overall, the model demonstrates a reasonable fit, though its tendency to over-predict should be noted for future refinement.

As the purpose of this article is to forecast the IMR growth data using a Bayesian ARIMA model. But for the simplicity of this study, we can go for the Autoregressive(AR) model to predict the same dataset. And also make a comparison of the predictive performance between the common forecasting model and the Bayesian ARIMA model. Since, AR models are ideal for small, stationary datasets, capturing temporal dependence through past values. They’re simple, interpretable, and effective for short-term forecasts, requiring no external inputs.

Bayesian ARIMA provides probability distributions for forecasts, and also requires a more iterative process to find the estimates. Bayesian ARIMA offers enhanced precision by incorporating uncertainty and adaptability. The choice between the two depends on the complexity of the dataset and the need for probabilistic forecasting. To evaluate the comparative performance of the two models, we present Table9, which reports the IMR values and their growth rates for the same year, along with the corresponding 95% CI for both the AR model and the Bayesian ARIMA model.

The Bayesian ARIMA model demonstrates superior forecasting performance compared to the AR model, as shown by significantly lower MSE values averaging 4.3 across 10 folds versus 18.5 for the AR model. Also, its ability to provide probabilistic confidence intervals enhances its reliability in uncertain environments. Compared to the AR model, Bayesian ARIMA delivers more accurate and informative forecasts, especially when accounting for uncertainty and evolving data patterns.

To fulfil the second objective of our study, we have generated forecasts for the subsequent periods based on the fitted time series model. The model captured the underlying patterns and trends in the historical data, and the forecasted values provide an estimate of the expected behaviour moving forward. The result of the point forecast for IMR and IMR growth (in %) with their respective credible interval are summarised in Table10for the next decade.

To obtain these patterns, we initially simulated the corresponding posterior and obtained a posterior sample of size 1e5 for the ARIMA(5,1,0) model using available data values. Subsequently, we simulated predictive samples for the remaining unobserved datasets for each value in the simulated posterior sample. The predictive estimates are provided as the corresponding posterior modes based on 100 predictive samples. These samples are used to apply the Kalman filter to predict future observations by using the model’s estimated parameters and current state information.

The forecasted IMR from 2024 to 2033 in Table10indicates a consistent downward trend, highlighting gradual improvements in infant mortality rates (IMR). Starting from an IMR of 25.21 in 2024, the rate steadily declines to 15.68 by 2033. The year-over-year growth rate remains negative throughout the period, with the highest reduction observed in 2033 (-5.81%). The output typically includes forecasted values along with their associated uncertainties, which helps with short-term time series prediction.

From Fig.4shows that the trend is promising as this persistent decline reflects the potential impact of public health interventions, improved healthcare services, and socio-economic development. The model effectively captures this trend, offering valuable projections for health policy planning and evaluation.

This paper analyses Bayesian analysis of the ARIMA model using the likelihood and proper prior distribution. The likelihood is obtained using the Kalman filter algorithm. Obviously, the posterior obtained using Bayes’ theorem is analytically intractable. To deal with this issue, we have used the Metropolis algorithm. To determine the order of the ARIMA model, we have used the ADF test, ACF and PACF plots. The model selection has been done for different ARIMA model configurations using AIC, BIC and K-fold Cross-Validation. With the help of this technique, we find the best model for forecasting. Numerical illustration is provided for infant mortality growth data. In this section, we have estimated parameters and selected the best model for the data at hand. After getting the model, we have made retrospective and prospective predictions for IMR growth.

The authors thank the anonymous reviewers for their careful reading of our manuscript and their insightful comments and suggestions. The research facility of Dr. Rakesh Ranjan is supported by a seed grant under the IoE Scheme of BHU.

Conceived and designed the research paper: RR and AS; Analyzed the data: TT and RR; Contributed agents/materials/analysis tools: AS, TT, RR and AKT; Wrote the manuscript: TT, AS; All authors read, reviewed and approved the manuscript.

Not applicable. No funding was received for conducting this study.