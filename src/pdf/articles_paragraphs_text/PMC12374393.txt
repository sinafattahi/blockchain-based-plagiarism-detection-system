Abdominal ultrasound is non-invasive and efficient, yet acquiring standard planes remains challenging due to operator dependency and procedural complexity. We propose AbVLM-Q, a vision-language framework for automated quality assessment of abdominal ultrasound standard planes.

In this study, we assembled a multi-center dataset comprising 7,766 abdominal ultrasound scans, which were randomly divided into training (70%), validation (15%), and testing (15%) subsets. The proposed method, AbVLM-Q, was developed using a three-step approach: (1) hierarchical prompting that incorporates spatially aware querying and sequential reasoning; (2) a quantifiable scoring mechanism based on multi-level clinical penalty criteria; and (3) LoRA (Low-Rank Adaptation)-based fine-tuning of a pretrained vision-language model. Performance was evaluated using mean recall, precision, label accuracy, subset accuracy, and confusion matrix analysis.

The system achieved key structure detection with 88.90% mean recall and 98.10% precision, showing higher precision and comparable recall to Faster R-CNN (89.77% recall, 88.64% precision at a 0.5 confidence threshold). Plane classification yielded 98.96% label accuracy and 96.28% subset accuracy, surpassing the best CNN (97.84%, 94.29%;P< 0.05). Image scoring accuracy for the clinically critical “Excellent” grade (scores 8–10) reached 85.11% with the best-performing backbone. Confusion matrix analysis confirmed consistent performance across different backbones, with discrepancies primarily observed at grade boundaries.

AbVLM-Q provides a novel method for automated ultrasound quality assessment, functioning as both an evaluation tool and a training platform for standardized scanning. It bridges AI-driven imaging analysis with clinical workflows, enhancing quality control in ultrasound diagnostics.

The online version contains supplementary material available at 10.1186/s12880-025-01885-w.

Abdominal ultrasound, a non-invasive and radiation-free modality, is widely utilized for assessing abdominal organs in clinical diagnosis, disease monitoring, and preventive care. Its efficiency establishes it as the preferred imaging choice in emergency medicine and hepatobiliary screening [1,2]. Standard planes, defined by anatomical landmarks, enable systematic evaluation of organ structure and function [3]. However, their clinical utility is limited by inconsistent image quality caused by inter-operator variability (particularly among residents), patient variability, equipment heterogeneity [4], and the anatomical characteristics of specific organs. For instance, in imaging assessments of anatomical structures such as the left branch of the portal vein and the section where hepatic veins drain into the inferior vena cava, inexperienced residents may struggle to accurately identify key anatomical landmarks. This can result in section deviation or omission of critical structures during imaging acquisition. In obese patients, acoustic shadowing caused by adipose tissue, along with challenges in coordinating respiratory movements in elderly patients or those with pain or impaired consciousness, can lead to failure in obtaining standardized anatomical planes. These issues underscore the need for automated quality assessment systems to standardize image interpretation.

Recent advances in deep learning have expanded the methodological repertoire for abdominal ultrasound image analysis. CNN-based transfer learning (e.g., VGGNet, ResNet) has demonstrated efficacy in organ classification and discriminative feature extraction [5–7]. Building on these foundations, multi-task architectures combining view classification with landmark detection improve diagnostic speed and accuracy [8]. Hybrid frameworks combining deep learning with interpretable classifiers (e.g., KNN) advance diagnostic automation pipelines [9]. These studies emphasize two key success factors: (1) effective use of pre-trained models through transfer learning, and (2) architectural innovations that balance model complexity with clinical interpretability [10,11]. Despite improvements in CNN-based standard plane analysis, challenges in cross-institutional adaptability and clinical integration remain.

Multimodal integration represents the next frontier, with vision-language models (VLMs) demonstrating particular promise [12–14]. Unlike conventional CNN-based methods, which rely exclusively on visual feature extraction and manual encoding of clinical heuristics, vision–language models (VLMs) support multimodal inference through the joint processing of image data and textual clinical knowledge. This dual-modality design enables the parsing and operationalization of standardized protocols expressed in natural language, thereby enhancing interpretability and clinician trust. Moreover, by incorporating clinical guidelines directly into prompt inputs, VLMs reduce implementation complexity and offer greater flexibility in adapting to diverse clinical requirements. The open-source availability of certain models has further accelerated the development of large language models and general artificial intelligence. Recent studies indicate that vision-language models can enhance diagnostic accuracy by processing both imaging features and clinical context [15,16]. However, the absence of a targeted alignment mechanism between ultrasound image features and clinical quality assessment limits their direct applicability to standard plane analysis in abdominal ultrasound.

This study investigates the feasibility of applying Vision-Language Models to analyze abdominal ultrasound standard plane images. We propose AbVLM-Q (Abdominal Vision-Language Model for Quality Assessment), an end-to-end framework for automated quality assessment. The development of the AbVLM-Q system incorporates three key elements: (1) hierarchical prompting and structured image-text dialogues to align ultrasound features with clinical context, enabling efficient structure detection and plane classification; (2) a 10-point scoring system based on established expert guidelines for interpretable quality assessment; (3) a multi-center dataset of 11 standard planes, with supervised fine-tuning to improve model adaptability. AbVLM-Q facilitates the quantitative evaluation of ultrasound image quality, enhancing diagnostic consistency in clinical practice and demonstrating its potential for standard plane analysis.

This multicenter retrospective study was conducted across two large tertiary hospitals in China. Institutional review board approval was obtained from both centers, and the requirement for informed consent was waived due to the retrospective study design. All procedures adhered to ethical guidelines.

This study included images from abdominal ultrasound examinations conducted at two independent ultrasound medicine centers between August 2023 and March 2024, using mainstream ultrasound devices (e.g., GE, Philips, Siemens, CHISON, Mindray) to enhance model generalizability. A total of 7,766 images were initially collected. The inclusion criteria were: (1) acquisition in predefined abdominal standard plane positions; and (2) complete ultrasound scans without visible artifacts or noise. Exclusion criteria were: (1) images with anatomical abnormalities or quality issues that compromised interpretation; (2) absence of key anatomical structures essential for plane classification or presence of irrelevant anatomical structures; and (3) non-abdominal ultrasound images. After applying these criteria, 3,766 standard plane images and 4,000 non-standard plane images (i.e., non-abdominal ultrasound images or ultrasound scans that did not meet the standard plane definition) were collated as comparison data to enhance the robustness of the model in a real clinical setting.

The final dataset was randomly partitioned by a ratio of 70%:15%:15% into training, validation, and test sets. Supplementary TableA1summarizes plane types and abbreviations, while Supplementary FigureA1provides schematic illustrations. The distribution of each plane type is detailed in Supplementary TableA2. Score distribution across intervals (0–4, 4–6, 6–8, and 8–10) is shown in Supplementary FigureA2. The workflow for the dataset construction process is illustrated in Fig.1.

During the data annotation phase, two ultrasound specialists annotated the images, and their work was reviewed by an expert to ensure accuracy. The annotation process consisted of several steps. First, each image was evaluated to confirm it represented a valid abdominal ultrasound scan; images deemed invalid were excluded from the dataset. Next, the appropriate plane type for each image was identified. For plane types sharing common abdominal structures, images were labeled with two corresponding sets of tags to reflect overlapping anatomical features. Key structures were then marked with rectangular bounding boxes. Finally, quality scores were assigned based on predefined deduction criteria. All annotations, conducted using custom annotation software, included category labels, structural labels, and quality scores for each image.

In the data preprocessing phase, an internally trained region recognition model was applied to extract key regions from the images. The images were then cropped to remove irrelevant content, such as background, patient privacy information, and non-ultrasound elements, retaining only the central ultrasound scan area to focus on clinically relevant regions. Since the Vision-Language Model (VLM) used in this study supports arbitrary input resolutions, resizing was not necessary, thus preventing any potential distortion or deformation.

In clinical practice, the assessment of abdominal ultrasound images requires a structured approach to address their complexity. To mirror this process, we designed the task flow in three sequential steps. First, key anatomical structures are detected, as accurate identification of critical structures is essential for confirming the standard plane. Next, standard plane classification assigns the image to its corresponding plane type based on the detected structures.

Finally, image quality is scored using predefined clinical criteria, providing an interpretable quality analysis. This hierarchical approach enables the model to capture key steps in clinical decision-making, translating abstract tasks into concrete actions for image quality analysis.

Our quality assessment system is based on a Vision-Language Model backbone, specifically tailored for the analysis of abdominal ultrasound standard planes. This section provides an overview of the key components of our approach, including model architecture, hierarchical prompting, the clinical scoring mechanism, and fine-tuning strategies—all of which contribute to an end-to-end framework for the comprehensive evaluation of abdominal ultrasound standard planes.

As illustrated in Fig.2, the model architecture consists of four key components: a vision encoder, a text encoder, a multimodal fusion module, and a large language model (LLM). The vision encoder processes abdominal ultrasound images, extracting key visual features that capture both local and global anatomical information. Meanwhile, the text encoder processes hierarchical prompts, generating contextualized embeddings that align with clinical standards. To bridge the disparity between visual and textual features, an input projector transforms the outputs of the vision and text encoders into a shared latent space, ensuring coherent cross-modal feature alignment. This fusion enables the LLM to integrate visual and textual information, supporting advanced semantic understanding, reasoning, and decision-making.

By combining extracted visual features with text-based information, the model facilitates image-text interactions, supporting three core tasks: key structure detection, where the vision encoder identifies critical structures and the LLM outputs their coordinates; standard plane classification, where the model determines the appropriate standard plane type based on the detected structures and hierarchical cues; and image scoring, where the model assesses image quality by evaluating anatomical structure integrity and ultrasound image clarity, generating an interpretable score aligned with clinical guidelines.

To ensure comprehensive and accurate image quality assignment, we referenced clinical guidelines and translated the judgment rules for 11 standard planes into detailed deduction strategies. Figure3illustrates the development process of this clinical scoring mechanism. For instance, the guideline for the gallbladder longitudinal view specifies that “the gallbladder neck, body, and bottom must be fully displayed.” We converted this into: “if any structure is not displayed, deduct 3 points; if incomplete, deduct 2 points.” The specific deduction values are determined through discussion and consensus among clinical experts in the field. This precise deduction strategy ensures that different quality issues are quantified, making the image quality assessment standardized and actionable. Additionally, we introduced global deduction labels to address common image quality issues across all standard planes. For example, “acoustic attenuation” is penalized based on its area ratio within the scanning range: 2 points for mild, 4 points for moderate, and 6 points for severe attenuation.

To enhance the clinical reasoning capabilities of our model, we developed a three-layer hierarchical prompting framework, as illustrated in Fig.4. This integrated framework combines task specification, structured querying, and scoring reasoning into a unified workflow, reflecting the sequential progression of clinical logic.

Task Specification Layer: We define the model’s role in quality control analysis of abdominal ultrasound images, establishing the context and directing its focus on assessing standard plane image quality. This layer incorporates clinical directives based on established standards, providing broad guidance for the model’s operation.

Structured Query Layer: We integrate spatial awareness queries and bounding box annotations to describe the positions and spatial relationships of anatomical structures within the image. Special tokens, such as (<|object ref start|>, <|object ref end|>) and (<|box start|>, <|box end|>), enable the LLM to interpret the vision encoder’s coordinate outputs, facilitating precise localization of anatomical features.

Scoring Reasoning Layer: We implement a Chain-of-Thought (CoT) approach to break down the scoring process into clinically interpretable steps. The model applies structured reasoning, generating detailed justifications for each deduction by linking them to specific image regions via bounding box annotations. This ensures transparency, traceability, and reliability, allowing the LLM to articulate its decisions in clinical terms based on guideline knowledge.

For this task we selected Qwen2-vl-2B [17] as the backbone and benchmarked its performance against other open-source vision-language models. Direct application of pretrained models proved infeasible because of domain-specific challenges, such as variability in imaging conditions and anatomical diversity across abdominal ultrasound standard planes; therefore, domain adaptation was required for reliable quality assessment. We employed the SWIFT framework [18] for supervised fine-tuning and applied LoRA [19] to adapt a subset of model parameters. The LoRA configuration used a rank of 16, an alpha of 32, a dropout rate of 0.05, a non-trainable bias setting, and bfloat16 (bf16) precision. Adaptation was performed on key linear projection layers within both the attention mechanisms and the feed-forward networks of the Transformer architecture, and the same LoRA configuration was applied across all experiments reported in this work. This strategy enabled efficient task-specific adaptation while reducing computational cost and mitigating overfitting to pretraining patterns.

Fine-tuning was performed on a dual-machine setup equipped with sixteen NVIDIA GeForce RTX 3090 GPUs using a learning rate of 4e − 5. We did not freeze the Vision Transformer (ViT) backbone parameters, allowing continued refinement of visual representations for abdominal ultrasound images. Fine-tuning the model of approximately 2 billion parameters with a batch size of 2 for 20,460 steps required approximately 7.3 h. Fine-tuning the model of approximately 7 billion parameters was constrained to a batch size of 1 due to GPU memory limitations; this run required 40,920 steps and took approximately 15.2 h on the same hardware.

The model demonstrated stable convergence during fine-tuning (see Supplementary FigureA3). The fine-tuning loss decreased rapidly in the early iterations and continued to decline, while token-level accuracy reached 96.65% at approximately 4,000 iterations (corresponding fine-tuning loss = 0.036) and thereafter remained at approximately 96.6%. The fine-tuning loss attained a minimum of 3.2e − 6 at step 20,180 and had effectively leveled off by approximately 20,000 iterations. Because token-level accuracy correlates more directly with our phrase- and sentence-level evaluation metrics than next-token validation loss, the combination of a leveled loss and a high, stable token-level accuracy supports our assessment that the model had converged for the downstream tasks.

The independent test set consists of 1,155 abdominal ultrasound scans. The distribution of each plane type within the dataset is presented in Supplementary TableA2. To address class imbalance, we employed random sampling for each plane type, partitioning the dataset into training, validation, and test sets with a 70%:15%:15% split. This strategy ensures a consistent distribution of plane types across all three sets.

In this section, we evaluate key structure detection, a critical step for accurately localizing anatomical landmarks in abdominal ultrasound standard planes. For this task, we employed VLMs to generate bounding box predictions, with performance assessed using standard metrics: precision, recall, false positive count (FP), and mean intersection over union (Mean-IoU).

whereNis the total number of true positive detections in the test dataset.

False Positive Count (Num of FP) indicates the number of erroneous detections, providing insight into potential diagnostic distractions.

We evaluated key structure detection in abdominal ultrasound standard planes, presenting performance metrics in two tables. Table1compares AbVLM-Q’s performance across different backbones. AbVLM-Q with Qwen2-vl-2B achieves high precision (98.10%) and Mean-IoU (0.786) with 25 false positives (FP), while Qwen2-vl-7B shows a slight recall increase (88.90% vs. 88.30%) but a minor precision drop (97.90%) and reduced Mean-IoU (0.770) with 28 FP, potentially due to increased model complexity. In contrast, models such as InternVL2, Deepseek-VL, and Minicpm-V2.6 perform worse, with precision below 89% and FP exceeding 140, likely due to the limited relevance of their pre-training datasets to medical imaging.

Table2evaluates the Faster R-CNN [20] baseline across varying confidence thresholds, revealing its sensitivity to threshold adjustments. At low thresholds (0.00–0.20), we observed high recall (97.95–96.12%) but low precision (54.54–75.46%) and a high number of FP (1157–443), with Mean-IoU ranging from 0.519 to 0.536. At a threshold of 0.99, precision improved to 98.85% and FP dropped to 6, with Mean-IoU reaching 0.626; however, recall decreased to 36.56%. This trade-off demonstrates challenges in Faster R-CNN’s robustness to the anatomical variability in abdominal ultrasound imaging.

whereNis the total number of samples,is the true label set of thei-th sample,is the predicted label set, and(·) is the indicator function.

whereLis the number of labels per sample, andanddenote thel-th true and predicted labels of thei-th sample, respectively.

Table3compares the performance of AbVLM-Q with CNN- and transformer-based models in standard plane classification. AbVLM-Q outperforms all models across key metrics. Qwen-VL-2B achieves a Label ACC of 98.96%, surpassing ConvNeXt-T (97.84%) by 1.12% points, while the difference with Qwen-VL-7B (98.87%) is not statistically significant due to overlapping CIs. In multi-view diagnostic scenarios, Qwen-VL-7B achieves a Subset ACC of 96.28%, exceeding ViT-B/32 (93.68%) and ConvNeXt-T (94.29%). For F1-score, Qwen-VL-7B attains 94.61%, outperforming ConvNeXt-T (93.71%) and ViT-B/32 (89.86%), with Qwen-VL-2B scoring 94.18%. These results highlight the framework’s robustness in standard plane identification, which is essential for ultrasound diagnosis.

To assess the potential impact of imaging equipment characteristics on classification performance, subgroup analyses were conducted stratifying cases by ultrasound vendor/model and probe type. Categories with limited sample sizes or incomplete metadata were combined into an “Others/Unknown” group to ensure adequate statistical power. Performance metrics, including Label Accuracy and Subset Accuracy with corresponding 95% confidence intervals, are detailed in Supplementary TablesA3andA4. The point estimates and confidence intervals for the majority of subgroups were consistent with those of the overall dataset, and substantial overlap of confidence intervals was observed. These findings indicate no clear evidence of systematic performance variation attributable to specific vendors or probe types within the constraints of the available data.

To evaluate AbVLM-Q’s scoring performance on abdominal ultrasound standard planes, we tested five VLM backbones. Scores were assigned on a 0–10 scale and categorized as Poor (0–4), Moderate (4–6), Good (6–8), and Excellent (8–10). As shown in Fig.5, all models exhibited a strong diagonal trend, indicating good agreement with the ground truth. Qwen2-vl-2B achieved the highest accuracy in the Excellent category (263/309, 85.11%), while Qwen2-vl-7B performed best in the Good category. InternVL2-2B showed similar performance but tended to overestimate quality, often misclassifying Good images as Excellent. DeepSeek-VL-1.3B and MiniCPM-V2.6 exhibited more off-diagonal misclassifications, particularly between Moderate and Good, reflecting greater uncertainty in borderline cases.

Discrete boundaries in deduction criteria contribute significantly to misclassification between the Poor and Moderate categories. For example, the grading of acoustic attenuation affecting the scan sector is divided by thresholds at one-quarter and one-half of the sector area. When the affected region was close to the one-quarter boundary, small variations in the estimated proportion could lead to a two-point difference in the sub-score, shifting the total score from 6 to 4 (Fig.6, a1–a2). Additionally, the AI occasionally failed to identify subtle anatomical discontinuities that experienced sonographers could detect, such as incomplete anterior or posterior gallbladder walls (indicated by red and green lines in Fig.6, b2), resulting in under-penalization. However, more prominent features, such as the gallbladder neck, were generally recognized and penalized correctly. Moreover, some discrepancies appear to stem from poor image quality, where the AI conflated multiple penalty items into a single category. For instance, in Fig.6, c2(Right Subcostal Right Lobe of the Liver and Right Kidney Longitudinal View), the model omitted the ground-truth penalty for incomplete visualization of the right kidney lower pole, possibly because this region overlapped with an area affected by acoustic attenuation.

Inference benchmarking was conducted on a single NVIDIA GeForce RTX 3090 GPU under conditions consistent with those described in Sect. 2.4.4 (Task-Specific Fine-Tuning). The model achieved a mean latency of 1.2 s per image in single-image mode and 0.4 s per image with optimized batch processing, with throughput varying modestly depending on the length of the generated textual output. These results indicate that AbVLM-Q is suitable for retrospective, centralized quality-control workflows and large-scale processing of archived examinations.

Due to the growing demand for high-quality standard plane images in abdominal ultrasound, AI-driven automated analysis has become increasingly important. In this study, we developed AbVLM-Q—an automated quality assessment system that integrates a vision-language model with efficient fine-tuning and hierarchical prompting. AbVLM-Q decomposes the quality assessment task into three interrelated subtasks: key structure detection, standard plane classification, and image scoring, thereby closely mimicking clinical decision-making. Experimental results indicate that AbVLM-Q outperforms conventional models across all three tasks. Importantly, this end-to-end, integrated multi-task system reduces reliance on operator expertise while enhancing the consistency of quality evaluations, underscoring its potential for broad clinical application.

Accurate detection of key anatomical structures is essential for reliable landmark localization in abdominal ultrasound. Leveraging the Qwen2-vl-2B backbone, AbVLM-Q achieved a precision of 98.10% and a recall of 88.30%, reflecting a balanced and robust performance. In contrast, traditional models—such as Faster R-CNN—suffer from high false-positive rates and significant drops in recall at higher thresholds. Moreover, while Faster R-CNN attained a maximum mean IoU of 0.626 at a confidence score of 0.99, AbVLM-Q reached 0.786, indicating superior capability in capturing fine details and accurately predicting anatomical boundaries. This robust performance without manual threshold adjustments underscores the practical applicability of AbVLM-Q in complex clinical settings.

Classifying abdominal ultrasound standard planes remains challenging—particularly when similar plane types share overlapping anatomical features. In our experiments, AbVLM-Q improved Label Accuracy by 1.12% and Subset Accuracy by 1.99% over conventional models, a notable achievement given the stringent requirement to distinguish among 12 distinct planes simultaneously. Furthermore, in resource-constrained configurations (e.g., using MobileNetV2), traditional models recorded a Subset Accuracy as low as 79.05%, whereas even the lightweight DeepSeek-VL-1.3B backbone enabled AbVLM-Q to achieve a Label Accuracy of 91.43%. These findings indicate that AbVLM-Q is robust to domain shifts and can be effectively deployed on embedded devices without significantly compromising diagnostic accuracy.

Previous studies on abdominal ultrasound standard plane classification have reported varying accuracies. For example, Cheng et al. achieved 77.9% accuracy using transfer learning—surpassing radiologists’ 71.7% accuracy [5]—while Lawley et al. reported 83.9% accuracy with InceptionV3 [7], and Wu et al. reached 92.31% accuracy in liver ultrasound plane classification [6]. However, these studies often relied on proprietary datasets and private test sets, which were constrained by patient privacy and ethical considerations, making direct comparisons challenging. In contrast, our study achieves 98.96% label accuracy and 96.28% subset accuracy, thereby providing strong evidence of the high performance and robustness of AbVLM-Q relative to existing approaches.

To facilitate clinical integration, we developed a scoring mechanism grounded in established clinical guidelines for abdominal ultrasound. This mechanism aligns the model’s scoring with quality control standards, reduces subjectivity, and enhances clinician acceptance. By capturing subtle image quality details and supporting flexible scoring strategies, the system can adapt to various clinical tasks and remain applicable over the long term. Confusion matrix analysis further demonstrates that AbVLM-Q maintains consistent scoring patterns across different backbones. Notably, for quality classifications (poor, moderate, good, and excellent), all backbones perform best on excellent images—a finding that corresponds with real-world observations where most scans are rated as good or excellent (see Supplementary FigureA2). However, distinguishing between poor and moderate quality images remains challenging due to the inherent ambiguity of image-based scoring.

In Sect. “Image scoring”, analysis of misclassified cases showed that indistinct deduction point boundaries, limited sensitivity to subtle anatomical details, and image quality variations contributed to confusion between Poor and Moderate grades. These findings underscore the need to improve the imaging encoder’s fine-grained feature extraction and subtle discontinuity detection. Future work could focus on optimizing encoder design, integrating multi-scale features, and incorporating domain-specific attention or high-resolution inputs. Refining scoring criteria near deduction boundaries may further enhance handling of borderline cases, thereby reducing misclassifications and improving the reliability of AI-assisted ultrasound quality assessment in clinical practice.

The measured inference latency demonstrates that AbVLM-Q is well suited for retrospective, centralized quality control of large ultrasound datasets. Real-time deployment on ultrasound consoles presents additional challenges, including limited computational resources of embedded hardware, strict latency requirements, and integration complexities with proprietary software systems. Therefore, near-term clinical implementation is most feasible through centralized server-based processing coupled with low-latency network connectivity. Achieving on-device, point-of-care inference will necessitate further engineering efforts such as model compression, quantization, knowledge distillation, and hardware-aware optimization, alongside comprehensive clinical validation and regulatory approval.

Despite these promising results, our study has several limitations. First, the dataset was primarily derived from routine clinical examinations and may not adequately represent rare anatomical variants or pathological conditions. Second, since the ultrasound scan data were collected from video screenshots, comprehensive patient demographic information (e.g., age, gender, medical history) was not fully captured, thereby limiting our ability to analyze correlations between patient characteristics and image quality. Future work should address these issues by incorporating a more diverse dataset and complete patient profiles to further validate the system’s performance.

In conclusion, this study is the first to apply vision-language models to standard plane analysis of abdominal ultrasound, filling a critical gap in automated quality assessment. AbVLM-Q demonstrates performance that is comparable to or exceeds that of traditional models, while its cross-modal transfer learning effectively associates imaging biomarkers with clinical decision-making. This integration may enhance diagnostic consistency in primary healthcare settings. Moreover, further investigation into the application of vision-language models for related tasks, such as organ classification and tumor characterization, could contribute to meaningful advancements in radiology AI, ultimately improving diagnostic accuracy, decision-making efficiency, and patient care.

Below is the link to the electronic supplementary material.

The authors would like to acknowledge the Department of Ultrasound Medicine, The First Affiliated Hospital, Zhejiang University School of Medicine, and Nanchang People’s Hospital for data provision, and gratefully acknowledge the algorithmic support from Yizhun Medical AI Co.