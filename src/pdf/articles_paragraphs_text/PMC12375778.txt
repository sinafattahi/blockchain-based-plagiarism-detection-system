The primary objective of modern healthcare systems is to enhance public health by providing efficient, reliable, and well-structured solutions. Improving patient satisfaction through tailored medical services has driven rapid advancements in healthcare, leading to increased competition and system complexity. However, the expansion of healthcare services introduces challenges such as high data volume, latency, response time constraints, and security vulnerabilities. To address these issues, fog computing offers an effective solution by processing data closer to end devices, reducing latency, and enabling real-time responses. This research proposes a robust brain tumor detection framework within a fog-based smart healthcare infrastructure. The process begins with data placement leveraging an improved evolutionary technique for Image Processing (HETS-IP) to optimize fog node placement based on key parameters such as energy efficiency and latency. Specifically, the Particle Swarm Optimization (PSO) algorithm is enhanced with a direct binary encoding technique, in which solutions are represented as binary strings, making it suitable for problems where decisions are discrete. This approach allows efficient optimization in binary decision spaces and improves adaptability for complex placement problems. Once data placement is committed, the tumor detection framework is performed directly at fog nodes to enhance real-time processing. This phase will begin with preprocessing, where a bilateral filter is applied to reduce noise while preserving critical edge details. Next, feature extraction is utilized to derive statistical texture features, which capture diagnostic information essential for distinguishing between tumor types. The process continues by classification using a deep Convolutional Neural Network (CNN) with sequential architecture to classify tumors. Simulation results demonstrate that HETS-IP outperforms traditional evolutionary algorithms, including Ant Colony Optimization (ACO), Genetic Algorithm-Simulated Annealing (GASA), and Genetic Algorithm (GA). On average, HETS-IP reduces energy consumption by 5%, 9%, and 14% and decreases makespan by 4%, 6%, and 11%, respectively. Additionally, the proposed approach achieves an accuracy of 97% and a precision of 96%, ensuring highly reliable brain tumor detection.

The healthcare sector has grown quickly recently and has significantly increased both employment and revenue1. Only a few years ago, a hospital physical examination was required to diagnose illnesses and abnormalities in the human body. For the duration of their therapy, the majority of the patients had to remain in the hospital. In addition to raising healthcare costs, this put pressure on healthcare facilities in rural and isolated areas. The development of technology over the years has made it possible to use small devices, such as smartwatches, for health monitoring and the diagnosis of a variety of illnesses. Additionally, technology has changed the healthcare system from being hospital-centric to being patient-centric2,3. Many clinical studies, for instance, can be carried out at home without a healthcare provider’s assistance, including blood pressure, glucose, and so forth. Furthermore, sophisticated telecommunication technologies enable the transmission of clinical data from distant locations to healthcare facilities4. Healthcare facilities are now more accessible because of the utilization of these communication services and the quickly developing technology (such as fog computing, wireless sensing, IoT, big data analysis, machine learning, and mobile computing).

The fog computing paradigm, an extension of cloud computing, plays a crucial role in the functioning of IoT applications by addressing key healthcare challenges such as latency in data processing, limited bandwidth in remote areas, and the need for real-time decision-making. Specifically, fog computing decentralizes computation and storage to the network edge, closer to data sources like IoT devices, thereby reducing delays and alleviating pressure on centralized fog-cloud systems5. This directly impacts healthcare delivery by enabling faster diagnostics (e.g., real-time monitoring of vital signs), improving accessibility for patients in underserved regions, and supporting cost-effective, patient-centric care through efficient data handling6,7. In order to establish a connection between IoT devices and the cloud layer, the solution architect intends to deploy fog nodes at the network edge, as seen in Fig.1of the hierarchical model. These fog nodes possess distinct computational and storage capabilities.

The structure of a healthcare system with using fog and cloud computing.

The Internet of Things has not only increased human independence but also expanded human interaction with the outside world. With the aid of cutting-edge algorithms and protocols, IoT has emerged as a significant force in international communication. Numerous gadgets, wireless sensors, household appliances, and technological devices are connected to the Internet using it4. IoT applications are seen in the following domains: healthcare1,8, homes9, cars, and agriculture5. The Internet of Things is becoming more and more popular because of its advantages, which include greater future event prediction, reduced costs, and improved accuracy. The quick IoT revolution has also been aided by greater understanding of software and applications, the advancement of computer and mobile technologies, the accessibility of wireless technology, and the growing digital economy10. Using various communication protocols like Bluetooth, Zigbee, IEEE 802.11 (Wi-Fi), and others, the Internet of Things devices have been combined with other physical devices to monitor and share data. Sensors, whether wearable or installed on the human body, are utilized in healthcare applications to gather physiological data from patients’ bodies, including temperature, pressure, electrocardiogram (ECG), electroencephalogram (EEG), and more11. It is also possible to capture environmental data like temperature, humidity, time, and date. This data aids in drawing accurate and significant conclusions about the patients’ medical situations. Since a lot of data is collected and recorded from a range of sources, data storage and accessibility are also crucial in IoT systems. Physicians, caregivers, and other authorized individuals have access to the data from the aforementioned sensing devices. Sharing this data with healthcare providers via a server or cloud enables prompt patient diagnosis and, if required, medical action. The communication module, patients, and users all work together to provide safe and efficient transmission. The majority of IoT systems execute user control, data visualization, and apprehension through a user interface that serves as a dashboard for medical providers. The literature that has documented the advancements of IoT systems in healthcare monitoring, control, security, and privacy has found a wealth of research12. These achievements demonstrate the IoT’s efficacy and promising future in the healthcare industry. However, preserving the quality-of-service matrices, which include availability, pricing, security, privacy of information sharing, and reliability, are the primary considerations while creating an IoT device.

Even though smart healthcare and IoT systems are growing quickly, current data placement tactics in fog computing settings still have problems with optimizing latency, saving energy, and finding tumors in real time. Many methods only look at resource allocation or data placement, but not at the same time to improve the performance of the whole system. Also, classical evolutionary algorithms frequently have trouble converging quickly in healthcare IoT networks, which are usually high-dimensional and discrete problem spaces. In medical situations where time is of the essence, like finding brain tumors, these delays are intolerable13. This study was inspired by the urgent need for a framework for detecting tumors that is quick, energy-efficient, and accurate in a fog-based smart healthcare setting. The suggested HETS-IP technique fills this gap by using a modified Particle Swarm Optimization (PSO) algorithm along with direct binary encoding to optimize the placement of data and the allocation of virtual machines at the same time. This dual optimization enables more effective handling of the exponentially generated IoT data and their resource constraints within the fog infrastructure, addressing the unique complexities of IoT-fog systems. Distributing the data from IoT nodes to the fog infrastructure is essential, considering the unique computation and communication attributes. We are compelled to tackle these challenges by the application of an evolutionary approach. Algorithms based on PSO require fewer parameters for calibration. By considering the interaction of particles, it can achieve the ideal solution. However, when faced with a multidimensional search space, PSO typically converges on the global optimum. This way, it makes sure that latency is lower, energy use is better, and real-time diagnosis is possible, which is a big gap in current IoT-fog medical frameworks.

The subsequent sections of the paper are structured as follows: The “Related work” section examines the current research on data placement and IoT-based healthcare systems. The “Problem formulation” section delineates the system model and the formulation of the problem. The suggested methodology is detailed in the “Proposed algorithm” section. The “Implementation” section delineates the findings of experimentation and comparison. The “Conclusions” section summarizes the paper and delineates future research directions.

The medical field has evolved into a highly competitive domain. Patient satisfaction can be improved by considering patient demands during service design and delivery, particularly by reviewing patients’ medical histories14. Moreover, accessing medical facilities has become challenging due to the prevalence of infectious and epidemic diseases like COVID-19, along with high costs, long distances, and quarantine factors that disproportionately affect elderly and disabled individuals15,16. In this regard, the architecture of the smart hospital system (SHS) enables rapid access to patient data and expedites the treatment process17. This design facilitates automatic tracking and monitoring of hospital practitioners, biomedical equipment, and patients18. It also continuously records variations in patients’ vital physiological signs and environmental conditions. Using a configurable web service based on representational state transfer, the collected parameters are sent to a control center, allowing both local and remote access19,20. The proposed SHS can be validated through two primary scenarios: one involving patient monitoring and the other addressing emergency management in the event of real-time patient falls21. The use of healthcare information technology (HIT) in hospitals and related institutions is deeply influenced by safety concerns and healthcare quality, which encompasses medical care, general care, ambulatory services, and specialized disease care. Thousands of sensors may be installed in hospitals and smart buildings to record operational metrics such as patient condition, blood pressure, ECG, blood glucose levels, humidity, and temperature. These sensors periodically transmit data to local storage servers, and after processing, relevant insights about a patient’s condition are sent to physicians or caregivers22. In emergencies such as heart attacks or sudden spikes in blood pressure, real-time data analysis and communication are critical, and cloud architectures are often insufficient to meet these time-sensitive needs1. In such cases, fog computing bridges the gap between the sensing layer and healthcare centers. Fog nodes provide a scalable, mobile, and reliable computational and storage infrastructure that supports various devices, including desktop PCs, smartphones, and tablets23.

A study in24proposed a comprehensive multi-layer architecture for IoT-enabled e-health systems integrating cloud computing, fog computing, and edge devices. This architecture is designed to address challenges related to latency, data heterogeneity, and real-time processing. The study also identified issues including scalability, data management, interoperability, privacy, and security, and emphasized the need for networks with high bandwidth, low latency, and user-friendly interfaces. However, recent publications were not reviewed, and the methodology for selecting sources was not transparent. Another study25emphasized the role of nanocomposites in healthcare sectors, such as food processing, biomedical applications, and nano-carrier technologies. The study focused on polymer nanocomposites used in diagnostics and treatment, along with their various processing techniques. However, the selection criteria for reviewed papers lacked clarity, and the publication dates were not disclosed. The investigation in26analyzed industry sources, including websites, white papers, and podcasts, to provide a structured overview of fog computing in healthcare. It highlighted technological standards, current challenges, and system functionalities such as health monitoring and specialized care for defensive applications. In27, a comprehensive survey was conducted on the role of IoT and cloud computing in healthcare systems. The concept of Cloud IoT-Health was introduced, integrating these technologies for applications such as medication tracking, telemedicine, and smart hospitals. An evaluation of open-source projects was included. While the study provided useful insights, it did not cover fog computing, and the paper selection process was not clearly explained. A separate review28discussed the Cloud of Things (CoT) in healthcare, examining its architecture and platforms. The findings showed that most studies neglected energy efficiency considerations, especially in scenarios requiring the balancing of delay, quality of service, and energy consumption. This study, too, excluded fog-related research, lacked recent literature, and was limited in scope. Additional surveys, such as29, examined cybersecurity concerns to propose a comprehensive security model for electronic health records. Another study30reviewed recent research trends in assisted healthcare environments, particularly those supporting cardiovascular disease monitoring and diagnostics. A review in31focused on IoT-based cardiovascular health systems, identifying critical challenges that must be overcome to innovate in heart care technologies. In32, data management strategies for healthcare systems using IoT and cloud infrastructures were analyzed. A review in33addressed data privacy challenges in constrained healthcare settings. The study presented a deep analysis of privacy-preserving mechanisms and concluded that a balanced approach is required to protect patient data. However, it did not discuss fog computing. Finally, a study in34categorized fog computing applications in healthcare, outlined relevant use cases, and described the associated devices and networks. While it demonstrated the potential of fog computing, it lacked comparative analysis and did not identify unresolved issues or future research directions. In35, a model called UCFNNet is proposed in order to improve the accuracy of ulcerative colitis assessment. The system includes an extensive lesion learner, which is focused on identifying specific patterns of inflammation in endoscopic images, and a noise suppression gating process, which reduces irrelevant noise. These components work together to improve the segmentation and evaluation of inflamed regions in the colon. The tumor identification and visualization of the small arteries requires rapid and precise imaging. In13, a method based on a deep learning algorithm is proposed to speed up the denoising process. The traditional methods may deliver noisy images and require long processing times. The new model is efficient in image improvement without losing important information, which makes the technology more relevant in real-time medical applications. Detection of hypoxic cancer cells is important in cancer studies. In36, the authors describe a highly efficient fluorescent probe that uses a nitroaryl group to bind to nitro reductase. On contact with nitro reductase, the probe fluoresces, which enables the use of imaging techniques to assess hypoxic cells in tumors. This could help to understand how tumors act and create better diagnostic tools.

Authors in37presented a thorough review of how to find Down syndrome using new technology, concentrating on how fetal screening methods and smart diagnostic models have improved. Their work shows how important it is to find problems early and how image processing and machine learning may help make diagnoses better. Also, another work in38has suggested EURI, a deep ensemble design for segmenting and detecting oral lesions. This shows how successful ensemble tactics can be in making difficult medical imaging jobs easier to find and classify. Even though these studies are about different areas of medicine, they all show how important it is to have strong, real-time detection frameworks for making clinical decisions. They also highlight the need to build high-performance models like the HETS-IP that was suggested. A work in39proposed a modified convolutional neural network for medical image classification within a fog-cloud architecture. The model was evaluated on pandemic-related X-ray datasets and achieved nearly 99.9 percent accuracy. The study emphasized reducing model complexity to fit the limitations of edge devices. It showed that lighter CNNs could match or exceed the performance of more complex models like VGG16. These results support the choice of CNNs for real-time medical processing in fog environments. Authors in40presented a federated learning-based system named FedHealthFog for predicting heart disease in fog networks. The model allows collaborative learning across medical sites while protecting patient privacy. Although promising, it introduces high communication overhead and synchronization challenges that can affect response time. These limitations highlight the advantages of HETS-IP, which avoids such delays by operating locally on fog nodes. This makes it better suited for emergency diagnosis and continuous monitoring. another work in41reviewed the current state of federated learning in healthcare and identified significant implementation challenges. It discussed problems such as data imbalance across clients, hardware variability, and the high cost of secure communications. The review concluded that while federated learning is valuable, it remains difficult to scale efficiently in clinical settings. In contrast, HETS-IP reduces complexity by processing data locally, which lowers delay and system dependence on network stability. For predicting cardiovascular events in fog environments, the authors in42proposed a hybrid architecture combining CNN, BiLSTM, and reinforcement learning. The system used digital twins to simulate and adapt to device conditions in real time. It achieved strong accuracy and operated well under fluctuating load conditions. Authors in43introduced a transformer-based model applied within a federated learning framework for handling multi-modal healthcare data. It addressed client diversity and cross-device inconsistencies that challenge global training. The approach improves generalization but requires large computational resources not typically available on fog nodes.

Isolated OptimizationMost studies conducted so far have focused on resource allocation or data placement in isolation, without considering how they interact with each other. In real-time healthcare scenarios, this segmentation results in a less efficient and less responsive system overall.

Delay ConstraintsA lot of traditional models struggle to scale when applied to large, mixed datasets generated in modern healthcare settings. Inefficient scheduling and poor resource utilization exacerbate latency-sensitive apps, such as brain tumor detection, which are particularly affected.

Not Handling Discrete Search Spaces Well EnoughTraditional evolutionary algorithms like GA and ACO often fail to perform optimally on high-dimensional, discrete decision issues like allocating data tasks in foggy settings. They tend to converge prematurely and lack the adaptability required in changing IoT settings.

Not Being Able to Work in Real TimeCloud-based solutions are robust, but they generally cannot provide the low latency necessary for important diagnostic operations. These systems typically slow down data transmission and processing, making them unsuitable for emergency medical care.

Limited use of Learning-Based ClassificationSome studies employ machine learning or deep learning to detect tumors, but few combine these classifiers with the best fog node placement procedures. As a result, diagnosis workflows can’t be accelerated from start to finish.

To overcome these challenges, we developed the HETS-IP system, which integrates evolutionary optimization for data placement with deep learning-based tumor categorization directly at fog nodes. A survey of the existing works in fog/IoT-based healthcare systems is shown in Table1.

Comparative survey of related works in IoT and Fog-based healthcare systems.

The diverse data distribution throughout a healthcare system’s computing environment is depicted in Fig.2. The system consists of a set of IoT terminal devices T = {t1, t2, t3, …, ti, …, tn}, where each device tigenerates data tasks Ai. These tasks are placed on a set of fog nodes FN = {FN1, FN2,…,FNm}, each characterized by computational resources Rj, storage capacity Cj, and network bandwidth Nj. The placement decision is denoted by a binary variable rij(t) ∈ {0,1}, where rij(t) indicates that task Ai is placed to fog node FNjat time slot t.

Each task A is defined by attributes PA= {Si, βi, Mi,max, Mi,exp, Pi} where.

Each terminal device tihas attributes Qt= {ptr, pte, bi, wtd, wed, Er}, where ptrindicates the transmission power of ti, pterepresents the idle power of ti, and biis a binary variable indicating the mobility status of ti. The parameters wtdand wedsignify the delayed weight and energy consumption weight of ti, respectively. If bi= 1, it denotes that tiis a mobile device with limited energy. Considering the energy consumption aspect during data placement, we set wtd= 0.8 and wed= 0.2. In contrast, if bi= 0, it signifies that tiis a static device with infinite energy, leading to the configuration of wtd= 1 and wed= 0. Erdenotes the remaining energy of ti.

The total latency for processing task Ai on fog node FNjat time slot t, denoted Lij(t), comprises three components: transmission latency (Ltr,ij(t)), queuing latency (Lq,ij(t)), and computation latency (Lcomp,ij(t)). The model accounts for dynamic network conditions and task prioritization. The transmission latency is calculated as Eq. (1).

where kij(t) is the data transmission rate between device tiand fog node FNj, modeled using the Shannon-Hartley theorem in Eq. (2).

where gij(t) is the channel gain, ϵ(t) is the noise power, and Nj(t) is the available bandwidth. The queuing latency accounts for the waiting time due to other tasks at fog node FNjis as Eq. (3).

where Pkis the priority weight of task Ak, ensuring that high-priority tasks (e.g., critical health-care data) experience reduced queuing delays. The computation latency is given by Eq. (4).

where Rj(t) is the computational resource of fog node FNj.

Therefore, the total latency is calculated as Eq. (5).

To ensure real-time processing, the model enforces a constraint as Eq. (6).

The total energy consumed by terminal device ti is categorized into the transmission energy Etr,i(t), idle energy Eidle,i(t), and placement energy Eoff,i(t). The model is designed such that it adapts the power control mechanism and takes into consideration the energy used to placement tasks to fog nodes.

The transmission of energy is calculated as Eq. (7).

The idle energy, incurred when the device is not transmitting, is calculated as Eq. (8).

The placement energy, which accounts for the energy consumed by the fog node to process the task, is calculated by Eq. (9).

where κjis the energy coefficient of fog node FNj, and fjis the processing frequency of the fog node.

The total energy consumption for device tiis calculated by Eq. (10).

An energy constraintensures that the device does not deplete its battery.

There exists a lack of interdependence across data, with each data functioning autonomously from the others.

Data are not multi-assignable, and each data can only be allocated to a single fog node.

Furthermore, the calculation approach does not account for the mobility of terminal equipment.

Each fog node remains stationary, and once a data commences execution, it does not undergo any interruptions.

The objective is to minimize a weighted combination of total latency and energy consumption across.

Data placement constitutes a complicated, multidimensional, nonlinear combinatorial optimization challenge with multiple objectives. The incorporation of several variables and constraints into the objective function complicates the identification of the optimal solution by polynomial methods. To address these issues, we employ the modified PSO (MPSO) algorithm. Our solution, grounded in the PSO approach, optimally positions data within their appropriate nodes. The PSO algorithm offers an efficient and rapid solution for intricate optimization problems; nevertheless, its usefulness is constrained to the distinct features of the fog computing task placement problem44. The intrinsic granularity of the parameters necessitates the application of discretization techniques in the standard PSO algorithm. To overcome this limitation and redefine the parameters of particle positions and velocities, an enhanced discrete particle MPSO algorithm is employed to address the data placement issue efficiently. Figure3illustrates the flowchart of the MPSO algorithm.

Our study utilizes a direct binary encoding method, a representation strategy in which the particle’s position is defined as a binary string composed of 0s and 1s. Each bit in this string corresponds to a specific decision variable. For example, whether a given data task is assigned (1) or not assigned (0) to a particular fog node. This technique is especially well-suited for discrete optimization problems like task placement, where solutions must satisfy binary constraints45. The velocity values, although continuous in [0,1], are interpreted as probabilities and mapped to binary positions via a sigmoid transformation. This mapping ensures that the PSO algorithm remains effective even in non-continuous solution spaces. The positional representation in binary encoding is illustrated by.

where xi​ represents the binary position of the particle i in the solution space, and d is the dimension of the data placement problem (number of data).

In standard PSO, positions are updated based on velocity values. For binary PSO, velocities are treated as probabilities, and the position update follows a sigmoid transformation to probabilistically flip the position values from 0 to 1 (or vice versa)40. The velocity update is shown in Eq. (12).

Here, vij(t) is the velocity of particle i at iteration t for data j, w is the inertia weight, c1and c2​ are acceleration coefficients, and r1​, r2​ are random values between 0 and 1.andrepresent the personal and global best positions, respectively. Also Eq. (13) demonstrate the utilization of the sigmoid function S to represent the position vector of the particles, where the rijvalues are treated as binary variables46.

where k is the number of iterations; pij and gij are the optimal and ideal positions of the j-th and j-th particles, respectively; c1 and c2 are acceleration coefficients; νijis the particle’s current velocity in the k-th iteration; and r1, r2, and r3 are random variables in the interval [0, 1].

The inertia parameter, denoted as η, plays a crucial role in shaping the exploration of particles within both global and local search regions. By increasing the contraction factor, the algorithm’s global search capability is improved while simultaneously enhancing its local search ability. Equation (15) presents the computational formula for determining the contraction factor.

Also, to enhance the PSO’s convergence and global optimization performance, adaptive coefficients for inertia weight w and acceleration coefficients c1and c2are introduced47. These parameters dynamically change during the optimization process to balance exploration and exploitation according to Eq. (16).

where wmaxand wminrepresent the maximum and smallest values of inertia weight, t denotes the current iteration, and Tmaxsignifies the total number of iterations. The adaptive modification of inertia weight facilitates increased exploration during initial phases and encourages convergence in subsequent phases, thus averting premature convergence and enhancing global optimization efficacy.

As indicated in Algorithm (1), Lines 1–3 present the algorithm’s inputs, outputs, and initiation phases. Line 4 evaluates the fitness of every particle. Lines five and six are meant to establish both local and global locations as well as particle fitness. Usingand fitness function, each particle iteratively runs from 1 to the maximum number of iterations in lines 7–11 to compute location. d location. These processes change the velocity of every particle in line with its existing velocity, local ideal location, and global optimum position. The inertia of the particle, cognitive component (personal experience), and social component (collective experience) determine the new velocity. Lines 11–16 evaluate if the fitness of the new location exceeds the particle’s current local maximum fitness and update the local best position to the new one should the new position be better (line 13); should the new position be inferior, keep the local best position (line 15). Line 16 confirms whether the fitness of the new position exceeds the global best fitness; if so, the global best position is changed to the new position. On the other hand, should the new position be less than the present global best position, the former line 20 preserves it.

The next phase defines every particle in the population’s starting position and speed. Every iteration uses Eqs. (12) and (13) to change the velocity and position of every particle. Also, every particle evaluates the. Update both pbestiand gbest.

After optimal data placement is determined by MPSO algorithm, the next critical step involves leveraging this strategically placed data to perform accurate tumor detection and classification directly at the fog nodes. Efficient placement of medical data to suitable fog nodes significantly reduces data processing latency, making real-time medical diagnostics feasible and improving patient outcomes in urgent clinical scenarios.

This sequential processing ensures the accuracy and timely availability of diagnostic outcomes, fully leveraging the benefits of strategically optimized data placement.

The method is structured into several units, as illustrated in Fig.4. Initially, a Gaussian filter is employed during the preprocessing phase to enhance image quality and eliminate noise. Subsequently, the Gray Level Co-occurrence Matrix (GLCM) is utilized to extract texture information, a method commonly applied in healthcare processing and analysis systems due to its statistical nature. Moreover, significant features are selected and classified to predict the tumor with high accuracy. In the final phase, a deep CNN is implemented for classification.

In tumor detection applications, the quality and clarity of edges are very critical. Tumors typically manifest as distinct regions with varying intensities compared to brain tissue, and it is crucial to preserve the form and boundary integrity of these regions for feature extraction48. This method employs a bilateral filter to address these difficulties. The bilateral filter enhances it by eliminating high-frequency noise while preserving the fundamental edges of tumor formation and retaining the precise texture features for classification. Medical image processing, particularly in brain tumor identification, significantly benefits from bilateral filters that provide nonlinear, edge-preserving noise reduction capabilities. Conventional filtering methods uniformly smooth images, diminishing significant edges, but the bilateral filter employs a dual criterion to selectively apply smoothing to areas that are geographically and intensity similar49. The filter’s remarkable attribute is particularly beneficial in brain MRI analysis, as it aids in identifying and differentiating diagnostic tissue borders between normal brain tissue and malignancies.

For grayscale image If, the bilateral filter calculates a weighted average of neighboring pixels to get a new filtered image If. The primary principle is to average not only adjacent pixels but also those with similar intensity values. Thus, it is expressed as Eq. (17).

i is the index (coordinates) of the center pixel being filtered.

j is the index of a neighboring pixel within a window Ω around i.

|i – j| is the Euclidean distance between pixels iii and j.

∣I(i) − I(j)∣ is the Absolute intensity difference between pixel i and j.

Gσs​​is the Spatial Gaussian kernel (weights pixels based on distance).

Gσris the Range Gaussian kernel (weights pixels based on similarity in intensity).

Also, Wtis the normalization term to ensure all weights sum to 1. It is formulated as Eq. (18).

The key parameter in the bilateral filter is shown in Table2.

The primary goal of feature selection is to discern traits that enhance outcomes and exclude those that produce errors. To enhance the model’s accuracy and computational effectiveness, characteristics exhibiting a significant association with the target attribute are selected. We utilize the two-dimensional (2D) GLCM for feature selection50. The occurrence frequency of event x with y is denoted by the (x,y) element of the two-dimensional histogram referred to as GLCM. The angle θ (0 degrees horizontal, 90 degrees vertical, 45 degrees along the positive diameter, and 135 degrees along the negative diameter) and the relative distance (d) between two pixels are utilized to ascertain the presence of a pixel with intensity x in relation to another pixel j at a defined distance (d) and direction (θ). Grayscale and y-axes are also utilized. The predominant method for texture feature extraction is the GLCM. This method calculates the GLCM, from which the statistical texture features of energy, contrast, homogeneity, and correlation are derived. The hierarchically converted image is subsequently enhanced. The pseudocode of the GLCM method is shown in Algorithm 2.

Figure5depicts the methodology for computing the G matrix at a distance of 1 unit in the 0-degree direction. The GLCM matrix is constructed using a 4 × 5 matrix of the image depicted in this figure. This matrix comprises eight distinct color levels. The range (i,j) in the GLCM matrix denotes the frequency of pixels with color i in the image matrix, whereas the adjacent pixel color is j. The GLCM matrix is employed to obtain features or define specific regions of an image for testing and learning applications.

illustrates the procedure for processing the input image (a) into the GLCM image (b) using GLCM processing. The diagram (c) depicts the spatial arrangement of pixels in the array, specifically showing the row offsets and column offsets. In this context, D denotes the distance between each pixel of interest. The presence of red circles in the image indicates the frequency at which distinct combinations of gray levels occur. The arrows, on the other hand, represent the number of occurrences of these combinations in the GLCM picture.

Three primary characteristics, namely "contrast, homogeneity, and energy," are utilized in accordance with Eqs. (19), (20) and (21) pertaining to the GLCM matrix. Equation (19) delineates the method for calculating contrast. The spatial frequency contrast of a picture is assessed. Contrast delineates the disparity between the maximum and minimum values inside a sequential array of pixels.

Also, image homogeneity is calculated through Eq. (20). In the pair of elements whose gray color has a small difference, its value will be large. Also, when all the elements in the image are the same, it will have the highest value.

The energy computation is presented in Eq. (21). The technique computes the energy of the anomalies within the image textures, with a maximum attainable value of one. The maximum energy is observed when the gray-level distribution in the image is uniform or has a periodic pattern.

This research employs a Convolutional Neural Network (CNN) with a sequential architecture, comprising an input layer, four convolutional layers, two pooling layers, and two fully connected layers, to classify brain tumors. The sequential CNN was chosen for its ability to effectively capture hierarchical spatial features from MRI images, which is critical for distinguishing complex tumor patterns in medical imaging51. Its layered architecture allows for progressive feature extraction, from low-level edges to high-level tumor-specific patterns, making it well-suited for the BraTS 2023 dataset’s multi-modal MRI scans. The sequential design also balances computational efficiency and model complexity, enabling deployment on resource-constrained fog nodes while maintaining high accuracy. Compared to alternative algorithms, such as Support Vector Machines (SVM), Random Forest, and other neural network architectures like DenseNet or PSP-Net, the sequential CNN offers superior performance for this task. SVM, while effective for smaller datasets, struggles with the high-dimensional, volumetric MRI data due to computational complexity and memory requirements, achieving lower accuracy52. Random Forest excels in tabular data but lacks the spatial feature extraction capabilities needed for image-based tumor detection, resulting in reduced accuracy53. DenseNet, although powerful for feature reuse, increases computational overhead, making it less efficient for real-time fog computing applications. PSP-Net, optimized for segmentation rather than classification, is less suitable for the final classification step, though it could complement the framework in segmentation tasks. As will be shown later, the sequential CNN achieves better accuracy and precision, outperforming these alternatives while maintaining computational efficiency suitable for fog nodes. Figure6illustrates its fundamental architecture, comprising an input layer, two pooling layers, two fully connected (FC) layers, and four convolutional layers. The convolution layer is the fundamental component of a CNN, tasked with processing data from a receiving cell.

where the variables P, Wi, S, and M stand for the stride, input volume size, kernel size of the convolutional layer neurons, and amount of zero padding, respectively.

The function of the convolutional layer is to extract features from the data. It consists of many convolutional kernel layers, each linked to a weight and a bias coefficient. The weight coefficient, bi, and the input of convolutional layer I should correspond to Xj−1, weighti, and bi, respectively, during the operation of the convolution kernel (i). A method to articulate the convolution process is equal to Eq. (23).

where f(x) is the activation function, ⊗ denotes the convolution operation, and Xj is the output result of convolution kernel I.

The convolution kernel extracts details from the incoming data by periodic sweeping. Additionally, ReLU serves as the activation function for the convolutional layer. The ReLU activation function is more straightforward to derive than the sigmoid, tanh, and other activation functions. This facilitates the prevention of gradient vanishing and accelerates model training. The ReLU function can be expressed as Eq. (24).

where Maximum is the max pooling operation, p is the element (n) of the pooling area r, andis the output result of the pooling region j.

Subsequent to the convolutional layers, the ultimate representation of the extracted features is achieved by one or more fully connected layers. A fully connected layer resembles a convolutional layer; however, it is characterized by a comprehensive link to its preceding layer, in contrast to the sparse interactions typical of conventional neural networks. The last layer generates a one-dimensional vector, with its length corresponding to the total number of categorization classes. This layer is tasked with executing classification. The network’s output is employed to calculate its error rate, which is subsequently used to determine the network’s parameters and aid in its training. During this procedure, the network’s output is evaluated against the correct answer with an error function, and the resultant error rate is calculated. The procedure for determining experimental error is delineated by Eq. (26).

The cost function L(C,) measures the penalty associated with inaccurately forecastingin place of C. The post-propagation phase commences according to the calculated error rate in the following stage. In this phase, the gradient of each parameter is calculated via the chain rule, and all parameters are modified according to their influence on the error produced in the network, employing Eq. (27).

Upon adjusting the parameters, the subsequent stage of feedforward begins. Upon the completion of a designated sequence of these stages, the network training process concludes. The fully connected layers function as the primary classifiers of the convolutional neural network. Their main responsibility is to reconfigure the characteristics of the pooling and convolutional layers from the hidden-layer domain to the sample-marker domain through weighting. To prevent overfitting, a dropout mechanism is implemented in the fully connected layer to randomly eliminate neurons. The model incorporates a sequential architecture with one-dimensional (1D) CNN techniques. Figure7illustrates the flow diagram of the proposed paradigm.

In the constructed sequential model, the input layer is positioned first. This layer delineates the dataset’s size. Figure7demonstrates the application of two CNN algorithms. The parameters of the CNNs were adjusted to achieve optimal results. Table3presents these parameters in a tabular format. The pooling layer was applied subsequent to the convolution process. The major goal of the pooling layer is to reduce the dimensionality of the retrieved feature matrix. Essential information is preserved in the pooling layer, thus reducing the processing strain55. Max pooling was employed during the first and second CNNs, while average pooling was utilized after the second CNN.

In both CNN algorithms, the activation function was the ReLu Eq. (28). Because it removes negative numbers and simplifies computation, the ReLu function was used.

For the development and validation of the models, we use BraTS 2023 dataset56. The BRATS dataset represents a standard benchmark that medical imaging specialists use to build and test their methods for brain tumor detection from MRI data. The MICCAI BRATS challenges introduced this dataset, which contains multi-institutional, multi-modality MRI scans of patients with glioma with high-grade (HGG) and low-grade (LGG). Researchers can analyze different tumor sub-regions, such as the enhancing tumor along with peritumoral edema and necrotic core, through T1 and T1ce and T2 and FLAIR MRI modalities in each scan. Neuroradiology experts segment patients’ brain tumors by hand to yield high-quality ground truth annotation data. The BraTS parameters are shown in Table4.

In the BraTS 2023 dataset used for model training, LGG includes both WHO grade 2 and grade 3 tumors as a unified class. This categorization reflects a widely adopted practice in neuro-oncology research, where grade 2 and 3 gliomas are often grouped due to their similar imaging characteristics and overlapping clinical management pathways. The proposed model, therefore, focuses on distinguishing major tumor categories that are clinically actionable, such as LGG versus HGG, rather than performing fine-grained grading. This aligns with practical diagnostic needs where early detection and broad-grade classification are critical for initiating timely treatment. However, we recognize that sub-classification of LGG into grades 2 and 3 could further support treatment planning. Future works of the model may explore integration with histopathological or genomic data to enable more detailed stratification without sacrificing the system’s real-time performance at the fog layer.

In this section, we give a detailed account of the simulation configuration and performance analysis of the proposed method, according to the results obtained from the performed experiments. The computed results illustrate the efficiency and applicability of the algorithm for different cases, which demonstrates the suitability of the algorithm in different parameters and conditions. We will also address the consequences of these results on further studies and applications. These results not only improve our understanding of the algorithm, but also potentially lead to novel applications.

Tables6and Table7also list the characteristics of the fog nodes and the respective gateways, respectively.

The selected methods for comparison are Ant Colony Optimization (ACO), Shortest Job First (SJF), Round Robin (R.R), Genetic and Simulated Annealing (GASA), Harris Hawks Optimization (HHO), Enhanced African Vultures Optimization algorithm (EAVOA), Deep Deterministic Policy Gradient (DDPG), Genetic Algorithm (GA)60–62, and the Proposed Method (HETS-IP). Metaheuristic parameter values are presented in Table8.

In Test 1, the data quantity is consistently adjusted within a range of [150–800], in increments of 150. A consistent number of 60 virtual machines is upheld during the experiment. Figures8,9and10present bar charts that depict the comparative performance of algorithms regarding cumulative execution time, makespan, degree of imbalance, and energy consumption. Reducing makespan improves throughput and service quality by allowing the system to process more data in resource-limited fog conditions. Minimizing energy consumption mitigates node overloads, hence assuring cost-efficiency, system dependability, and sustainability. Reducing execution time is essential for satisfying low-latency demands in time-sensitive applications. Efficient load balancing enhances scalability, fault tolerance, and resource consumption, guaranteeing optimal data processing throughout the system.

The examination of makespan performance, depicted in Fig.8, reveals a distinct trend: the makespan escalates with the increasing volume of data across all algorithms. This is anticipated, as handling increased data typically results in extended processing durations. Nonetheless, HETS-IP distinguishes itself by continuously attaining the minimal makespan across all data sets. The dual-phase strategy, which optimizes virtual machine deployment and subsequently data placement using an advanced PSO algorithm, guarantees more efficient data management, leading to reduced overall completion times. Conversely, the SJF algorithm consistently demonstrates the largest makespan values in all cases. This inefficiency arises from SJF’s emphasis on prioritizing shorter data while neglecting the total system load and the potential delays affecting longer data. It is far more efficient than the two recent methods, specifically the DDPG and EAVOA.

HETS-IP attains a reduced makespan relative to DDPG and EAVOA by optimizing data and VM placement, hence directly diminishing execution time. DDPG prioritizes dynamic load balancing, EAVOA underscores overall optimization, whereas HETS-IP utilizes an advanced PSO algorithm with direct binary encoding to accurately allocate data to VMs and reduce idle times. Furthermore, its virtual machine placement technique, which classifies actual computers according to resource availability, guarantees optimal allocation and prevents bottlenecks. These customized capabilities allow HETS-IP to manage data more effectively, leading to a continually reduced makespan. Also, Compared to traditional evolutionary methods such as ACO, GA, and GASA, HETS-IP exhibits superior performance due to several key enhancements. First, while GA and ACO rely on iterative random search and pheromone trails, respectively, they tend to converge slowly and can become trapped in local optima, especially in high-dimensional or discrete task placement scenarios. GASA, though improved with simulated annealing for better convergence control, still suffers from high computational overhead due to its hybrid nature. Essentially, the shorter makespan that HETS-IP achieves allows for faster processing and analysis of vital medical data, like brain scans or sensor readings, which facilitates quicker diagnosis and shorter decision cycles in healthcare systems. This efficiency can result in better patient outcomes, faster clinician response times, and increased system scalability as data loads rise in high-throughput and emergency settings like remote diagnosis units or intensive care unit monitoring. Additionally, shorter processing times result in lower energy requirements, which is essential for Internet of Things-based medical devices that operate in environments with limited power, like mobile diagnostic units or rural clinics.

As a result, the makespan increases significantly, particularly as the number of data grows. This comparison underscores HETS-IP’s superior ability to handle varying workloads effectively, minimizing the makespan and enhancing overall system performance.

The comparison experiments illustrated in Fig.9demonstrate that HETS-IP and GA are the most efficient algorithms for reducing total execution time across different data volumes in fog computing environments. HETS-IP consistently attains the minimal execution times at 150, 300, 600, and 800 data points, demonstrating its durability and efficiency. GA excels at 450 data and EAVOA at 150; however, this advantage is not sustained across all scenarios, suggesting that while competitive, HETS-IP exhibits superior consistency. The findings underscore HETS-IP’s enhanced scalability and adaptability, efficiently handling bigger workloads and maximizing resources used more consistently than GA, ACO and EAVOA. In contrast, the SJF algorithm consistently exhibits the longest overall execution durations across all data volumes, indicating its inefficiency in managing bigger workloads due to its emphasis on shorter tasks without regard for system load. Likewise, the Round Robin and HHO algorithms are positioned slightly above SJF, exhibiting comparatively prolonged execution times owing to their rudimentary data allocation methodologies. The findings indicate that sophisticated algorithms such as HETS-IP, GASA, GA, and EAVOA are favored for enhancing performance in fog computing, whereas simpler methods like SJF and R.R. may be less successful for intricate, large-scale data.

Also, Reduced execution time directly supports the provision of real-time diagnostic services in healthcare systems. Rapid data analysis is essential for medical situations like tumor detection, patient monitoring, and emergency triage in order to support decisions that could save lives. HETS-IP enables a quicker transition from sensor input to diagnostic insight by guaranteeing low-latency execution across large, heterogeneous workloads, enhancing the clinical responsiveness of intelligent healthcare systems.

Figure10illustrates an analysis of energy consumption among several algorithms, indicating that energy usage escalates with the rise in data volume. Among the assessed algorithms, HETS-IP consistently demonstrates the lowest consumption of energy across all data volumes, underscoring its resource management efficiency. The minimal energy use is a direct consequence of HETS-IP’s optimized data and VM placement procedures, which reduce superfluous energy expenditure. Conversely, the SJF, HHO, and LSTM algorithms consistently exhibit the highest energy consumption in all cases. This pattern corresponds with their performance in other metrics, where they similarly show inefficiencies. The elevated energy consumption of SJF and LSTM underscores their suboptimal management of data and resources, resulting in heightened operating expenses and diminished overall efficiency. HETS-IP is the most efficient in terms of energy algorithm, rendering it very beneficial in fog computing scenarios where conserving energy is paramount. The persistent underachievement of SJF, HHO, and LSTM across several measures, including energy usage, highlights their deficiencies in resource optimization, hence reinforcing HETS-IP as the preeminent option for effective data placement and managing resources. Specifically, as illustrated in Fig.10, HETS-IP exhibits enhanced energy efficiency relative to DDPG and EAVOA by integrating efficient data placement with an energy-conscious VM placement strategy. HETS-IP classifies real machines according to resource availability, ensuring data is assigned to the most appropriate VMs, hence minimizing resource waste and avoiding the unnecessary deployment of additional machines. Conversely, DDPG and EAVOA exhibit an absence of comprehensive data allocation strategies, resulting in increased energy consumption. Consequently, HETS-IP constantly attains reduced energy consumption while sustaining superior performance. HETS-IP also outperforms conventional metaheuristics like GA, GASA, and ACO. These approaches either need more iterations to converge or do not explicitly incorporate energy constraints into their objective functions, which raises the overall energy consumption. On the other hand, HETS-IP prioritizes low-energy configurations in its solution space and enables faster convergence through the use of adaptive inertia control and direct binary encoding within its PSO core.

These energy savings have important ramifications for healthcare applications. Numerous medical Internet of Things devices, like wearable monitors or portable diagnostic sensors, function in energy-constrained settings where it is impractical to replace or recharge batteries frequently, particularly in disaster areas, mobile health units, and rural clinics. HETS-IP contributes to increased system reliability and operational sustainability by extending device uptime and reducing infrastructure strain through the reduction of energy expenditure. Additionally, it lowers long-term energy expenses for medical facilities that implement fog-based systems on a large scale.

Tables9,10and11display the numerical outcomes corresponding to each illustration.

This section describes standard evaluation metrics for deep learning models. Confusion matrix can be used to describe the performance of the classifier in a 2D tabular form (stored in the form of a table). It comprises the TP, FN, FP and TN values. These are then the subsequent foundation of a number of metrics. These metrics based on the values in the confusion matrix provide a measurement of model performance with regard to accuracy, precision, recall and F1-score. These metrics, derived from the values in the confusion matrix, help quantify the model’s performance. By analyzing these metrics, practitioners can gain insights into how well their deep learning model is performing and where improvements may be needed.

The algorithms for image pre-processing, feature extraction, and CNN based classification have been implemented using MATLAB R2022 environment. Tables12and13display the outcomes of CNN classification for the 3 distinct categories of brain tumor based on BRATS dataset.

Performance of CNN classification based on level 1 GLCM.

Performance of CNN classification based on level 2 GLCM.

We have employed a method to differentiate various categories based on their grades: Low-Grade Glioma (Grade 1), Glioblastoma (Grade 2), and No tumor (grade 0). LGG is a combined category of WHO grade 2 and grade 3 gliomas and this type of LGG tumors is less aggressive, malignant brain tumors, and High-Grade Glioma (HGG) refers only to WHO grade 4 gliomas (Glioblastoma), which are well-known Grade 2 gliomas grow more slowly as compared to Grade 3 or anaplastic gliomas which are more aggressive but not as much as Grade 463. As the imaging features of Grades 2 and 3 are usually very similar BraTS combines them to LGG, thus making the classification tasks to LGG vs HGG without clearly discriminating Grade 2 from Grade 3. We have extracted six texture features (dissimilarity, contrast, entropy, energy, homogeneity, and correlation) from sampling images. A thorough analysis of this research reveals that image texture encompasses significant diagnostic information essential for the processing and differentiation of benign and malignant tumors in medical imaging. However, the proposed features are insufficient for distinguishing between benign and malignant tumors; it is imperative to identify and differentiate them using more reliable indicators.

Furthermore, Fig.11and Table14are dedicated to comparing HETS-IP with alternative approaches regarding Accuracy, Precision, Recall, F1 Score, and AUC. The stated outcomes encompass the average values compiled from various executions of each program. The proposed approach has been compared with analogous methods, the majority of which pertain to tumor detection. Accuracy quantifies the model’s overall correctness by computing the ratio of properly predicted occurrences to the total number of instances. Precision is the proportion of accurately predicted positive observations to the total expected positives, indicating the model’s capacity to minimize false positives. Recall is the ratio of accurately predicted positive observations to all actual positives, reflecting the model’s capacity to identify all pertinent cases (true positives). The F1 score represents the harmonic means of precision and recall, offering a balanced metric that accounts for both false positives and false negatives.

The results reported in Table5demonstrate that the HETS-IP method surpasses its counterparts, which were also examined in this experiment. This better performance is related to the effectiveness of the feature extraction techniques applied, particularly the GLCM method. These strategies were essential in obtaining important textural information from tumor tissue images, considerably boosting the classifier’s capacity to reliably identify between histological grades of tumor. The influence of these advanced feature extraction approaches on the classifier’s success is both clear and large, as indicated by the remarkable accuracy and precision metrics achieved.

To evaluate the significance of the reported enhancements, the results obtained from several iterations of each method were analyzed statistically using IBM SPSS V.26. Additionally, in conjunction with the typical calculation of descriptive statistics (mean ± SD), MANOVA and Tukey’s test were performed for each evaluation metric to ascertain any significant differences in the comparisons conducted. The selected significance level was 0.05. The MANOVA test aims to determine whether there is a significant difference among the outcomes. Tukey’s HSD test facilitates the comparison of each pair of means, allowing us to ascertain whether pairs have a significant difference. The results obtained from the MANOVA analysis are displayed in Table15.

The MANOVA test results, presented in Table15, indicate statistically significant differences among the means of the analyzed algorithms across all metrics: Accuracy, Precision, Recall, F1, and AUC. To conduct a pairwise analysis for further research, it is necessary to run the Tukey’s HSD test. The importance of the disparity between each algorithm pair presented in Table16was analyzed utilizing the HSD test. For this part of the statistical analysis, we concentrate exclusively on assessing the difference between the strategy that has yielded the most favorable results and its alternative methods. Table8presents the outcomes of the HSD test for three pairs, encompassing the proposed method and its nearest rivals. The table emphasizes the principal metric Q, commonly referred to as the studentized range statistic. Q is determined by using the pair of mean values under examination.

The results of Tukey’s HSD test reveal a significant difference between the superior outcomes achieved by HETS-IP and those of its equivalents in most instances. However, there were a few instances where the difference was not deemed substantial. These exceptions arose for the PSP-Net + and CNN algorithms. The Tukey’s HSD test indicated that the enhancement in precision attained by the suggested method relative to the PSP-Net + method is not statistically significant. A comparable scenario is noted regarding the discrepancies between the outcomes of the HETS-IP and CNN algorithms, specifically with Precision and F1, which were statistically insignificant.

The convergence of IoT and fog computing not only transformed the smart healthcare but also enabled real-time and low-latency diagnosis which are very critical in the case of time critical medical conditions like detection of brain tumor. This study suggested HETS-IP, a hybrid evolutionary framework improved with a modified PSO algorithm and direct binary encoding, to address the urgent issues of data placement, computational efficiency, and responsiveness in healthcare systems. The technique dramatically lowers latency, energy consumption, and processing time by strategically placing medical IoT data at the best fog nodes. According to simulation results, HETS-IP performs better than conventional algorithms in terms of makespan, execution time, and energy efficiency. These benefits are essential in real-world clinical settings where prompt and accurate processing of sensor and imaging data is necessary for ongoing monitoring, early diagnosis, and prompt medical intervention.

The implications of this approach go far beyond the detection of brain tumors and encompass a broad range of fog-based healthcare services. For example, it supports emergency response systems, remote chronic disease management, wearable health monitoring for high-risk or elderly patients, and real-time ECG or EEG analysis. In rural or resource-constrained environments, HETS-IP helps create scalable, resilient, and energy-conscious medical infrastructures by decreasing reliance on cloud servers and increasing local processing efficiency. Furthermore, the framework’s flexibility enables it to manage the high-frequency, context-sensitive, and heterogeneous data that characterizes contemporary healthcare IoT environments. Because of this, HETS-IP is positioned to play a key role in the creation of autonomous, decentralized healthcare ecosystems in the future. These ecosystems will be able to support AI-driven clinical decision support, anomaly detection, and predictive analytics. Although the suggested model performs well, there are some limitations that call for more research. First, the system has not yet been implemented or tested in real-world clinical settings, where hardware malfunctions and environmental fluctuations could affect performance. Instead, it is currently validated in a simulated environment. Second, the tumor detection framework may not be as generalizable to other imaging modalities or institutions because it was trained and validated on the BraTS dataset. Third, the current model ignores data security and privacy issues, which are crucial in clinical settings where sensitive patient data is handled. Future research will concentrate on incorporating mobility-aware and adaptive resource allocation strategies, extending the system to accommodate cross-institutional and multimodal datasets, integrating privacy-preserving measures like secure data access protocols and encryption, and investigating hybrid deep learning models for more comprehensive, context-aware health analytics in order to address these issues. The practicality, robustness, and clinical relevance of fog-based healthcare systems driven by HETS-IP are intended to be further improved by these developments. Also, future iterations of the proposed framework may incorporate explainable AI (XAI) techniques to highlight tumor-relevant regions within the MRI images. Methods such as Grad-CAM, saliency maps, or attention-based visualization can help radiologists and clinicians better understand the rationale behind the model’s predictions. Integrating these tools into the fog-based processing pipeline would not only improve transparency but also assist in validating segmentation boundaries and localizing tumor sub-regions. This aligns with the increasing demand for interpretable AI solutions in healthcare and complements the system’s diagnostic utility in real-time environments.