Entrustable professional activity (EPA) observations can be used to develop a holistic picture of trainee competency in professional tasks.
Narrative feedback is an essential component of EPAs, but there is a lack of published literature capturing undergraduate student perceptions.
Students who completed Year 3 of the MD programme in 2022–2023 at one institution in Canada were invited to participate in a survey designed to elicit their perceptions of narrative feedback in EPAs.
Survey methods included closed‐ended questions (analysed descriptively) and open‐ended questions (analysed through thematic analysis).
The response rate to the survey was 23%.
Over 60% of students reported that narrative comments in EPAs were specific and aligned with EPA scores, and 86% reported that the narrative feedback was individualised at least some of the time.
However, 57% reported that they never or rarely received actionable feedback for growth.
Students demonstrated mixed feelings as to whether EPAs help support their clinical development.
Some noted that they can help students identify gaps and reinforce positives.
Others reported preferring verbal feedback to written feedback and that EPAs resulted in an administrative burden.
Over 90% of students reported barriers to obtaining EPAs, and almost 90% expressed the need for changes to EPAs.
A predominant theme from students was the desire for a reduction in EPA requirements.
Students perceived the narrative feedback to be individualized and specific but reported that the feedback rarely contained feedback for growth.
Students identified several barriers to EPA completion and provided recommended changes.
Competency‐based medical education (CBME) is the foundation of postgraduate and undergraduate medical training in Canada [1].
Entrustable professional activities (EPAs) are a central concept within CBME and play an important role in providing clear, observable and assessable milestones in a trainee's progression.
EPAs refer to specific tasks or responsibilities that trainees can be entrusted to do independently once they have demonstrated competency [2].
The focus on entrustment aligns with real‐world professional responsibilities, ensuring that trainees are prepared to meet the demands of independent practice [2].
The Association of Faculties of Medicine of Canada created 12 core EPAs in which all graduating medical students in Canada are expected to demonstrate competence (AppendixS1) [3].
These 12 EPAs were implemented at this institution in 2021.
Faculty development on EPAs included synchronous and asynchronous opportunities, including information handouts, prerecorded videos and grand round sessions.
Strategies to engage students included peer learning through student representatives, asynchronous videos and information handouts.
The narrative feedback captured in EPA observations serves multiple purposes.
It is intended to justify the score given by faculty whilst also providing formative feedback to students via low‐stakes assessments [4].
It can advance learning outcomes by helping students reflect on their strengths and areas for growth [5].
Hattie and Timperley [6] conceptualised feedback as ‘information provided by an agent regarding aspect's of one's performance’.
This model outlines three questions that need to be addressed in order to provide effective feedback: ‘where am I going?’ (feed up), ‘how am I going’ (feedback) and ‘where to next’ (feed forward) [6].
Feedback should include information on the student's goals, their progress in relation to these goals and feedback on actions that could reduce the gap between the student's current performance and their goals.
Unfortunately, narrative feedback in EPAs rarely contains all three types of feedback [7].
Student views of feedback support the Hattie and Timperley feedback model.
They would like specific feedback on what to improve on and, equally important, specific feedback onhowto improve [8].
The quality of feedback may influence student perceptions where lower quality feedback is perceived as less valuable.
There is a paucity of evidence about undergraduate student perceptions of narrative feedback in EPA observations.
Trainees in postgraduate medical education report that EPAs have increased the quantity of feedback whilst failing to improve the quality [9].
There is the possibility that student perceptions are similar; however, students generally have less autonomy than postgraduate trainees, influencing their perceptions of feedback [10].
It has also been demonstrated that junior learners value positive feedback whereas more senior learners value specific and constructive feedback [11].
The purpose of this study was to explore undergraduate medical student perceptions of narrative feedback in EPA observations.
Specifically, this study aimed to explore whether narrative feedback in EPA observations is perceived by students as helpful for their clinical development, whether students perceive any barriers to obtaining EPA observations and how students feel that use of EPA observations could be improved to support learning.
The purpose of this study was to explore undergraduate medical student perceptions of narrative feedback in EPA observations.
Students who completed Year 3 of the MD programme in 2022–2023 at one institution in Canada were invited via email to participate in an online survey designed to elicit their perceptions of narrative feedback in EPA observations.
Through the online survey format, information about the study was provided, followed by an implied consent statement.
Participants could withdraw consent at any time during the survey by exiting the online form prior to submission.
Once submitted, survey responses could not be withdrawn, as data were collected anonymously.
The survey included a combination of closed and open‐ended questions and was distributed via REDCap, a secure web platform for building and managing surveys, which has been shown to be easy to use and reliable [12].
The survey contained 5‐point Likert questions and free‐text response questions (AppendixS2).
Different question types were chosen because student perceptions are multifaceted and therefore are best captured using varied data collection.
All data were encrypted and stored on the institution's secure network.
Only the study team had access to the data.
Ethical approval was obtained from the institutional Research Ethics Board and Trainee Research Access Committee.
Descriptive statistics were used to analyse closed ended questions, such as the median and mode [13].
Excel was used to manage the quantitative data.
Thematic analysis, a process where themes from narrative data are identified and analysed, was used to synthesise narrative comments in the survey [14].
An inductive approach was utilised to allow the survey data to determine the themes [15].
Microsoft Word was used to store and manage the qualitative data.
Two members of the research team (ND and RL) performed the thematic analysis.
All of the narrative comments provided by students to the free‐text questions in the survey were reviewed to familiarise the investigators with the data.
Initial codes were then generated.
Themes were then identified and reviewed to ensure they were complete and meaningful and to ensure that they were sufficiently different to warrant separate themes.
Operational saturation was achieved when no new codes or themes were able to be generated.
The themes were then named and the key components of each theme elaborated on.
Disagreements were resolved through consensus discussion.
In total, 35 out of 153 students (23%) completed the survey.
The results of the closed ended questions are shown below in Figures1and2.
The survey results were allocated a numerical value, withneverbeing represented as ‘1’ andall of the timebeing represented as ‘5’.
The median was 3, and the mode was 3 for all questions with the exception of the question on specific actions included in narrative comments, which had a median of 2 and a mode of 2, and the question on narrative comment alignment with numerical scores, which had a median of 4 and a mode of 4.
The majority of students, 86%, reported that the narrative feedback in EPA observations was individualised some or most of the time.
Over 60% reported that narrative comments were aligned with EPA scores most or all of the time.
Sixty per cent reported that it helped them to identify areas of strength some of the time.
However, 20% reported that faculty rarely or never complete EPA observations in a timely manner, and 57% reported that they rarely or never receive feedback for growth.
The majority of students, 86%, reported that the narrative feedback in EPA observations was individualised.
The overall theme for this domain was that students have varying perceptions about whether EPA observations are helpful for their clinical development.
Students who found the narrative feedback helpful noted that gaps in their learning were made clear.
Students who did not find EPAs helpful noted that immediate verbal feedback obtained in daily clinical activities was more helpful and that narrative feedback was redundant following this.
Finally, some felt that EPAs were being used to force the programme to make improvements to supervision levels.
Representative quotes for this theme are shown in Table1.
Students have varying perceptions about whether EPA observations are helpful for their clinical development.
The overall theme in this domain was that there were barriers to students obtaining EPA observations.
Students identified that it could be challenging to get staff to complete EPA observations, with many staff refusing or agreeing, but subsequently failing to complete them.
Students reported feeling uncomfortable when requesting EPA observations due to awareness that faculty are busy.
A final subtheme identified was that there can be challenges obtaining specific EPAs, particularly EPAs, which require direct observation.
Representative quotes for this theme are shown in Table2.
The overall theme in this domain was that students felt that there should be changes made to EPA observations.
The predominant subtheme was that students felt that there should be a reduction in EPA requirements with an increased emphasis on more global evaluations of performance to permit students to focus on clinical learning and to reduce EPA burden.
A further subtheme that emerged was moving the onus for completing the EPA observations from students to preceptors so that students are not penalised for staff not completing them.
A final subtheme was changing the wording on EPA observation forms to make the criteria more explicit as students felt that staff often misunderstood the scoring scale.
Representative quotes for this theme are shown in Table3.
The majority (66%) of students report that EPAs may not contribute to their clinical development as intended, yet report positive impressions in several subcomponents.
For example, the majority (77%) of students report that the narrative feedback was reflective of the clerkship learning objectives, which suggests that feedback is provided on the goals that students are working towards.
The majority (86%) report that the feedback received helped to identify areas of strength and that the feedback was specific, individualised and relevant, at least some of the time.
This suggests that they are receiving narrative feedback on their current performance.
However, 57% of students report rarely or never receiving narrative comments on how to improve or work towards a goal.
Using Hattie and Timperley's feedback model, these results suggest that students are being provided with feedback on ‘where am I going’ and ‘how am I going’ most of the time but rarely on ‘where to next’.
This is in keeping with published literature, which suggests that ‘feed forward’ feedback is the least frequently delivered despite the fact that this type of feedback has the greatest impact [6].
Students do not feel that faculty complete EPA observations in a timely fashion, with 20% reporting that faculty rarely or never complete them in a timely manner.
This is concerning because evidence suggests that delayed feedback results in poorer quality feedback and is perceived as less credible [8].
Credibility of the qualitative data was established by ensuring investigator triangulation after independent and dual review of the data [16].
Year 4 students were intentionally selected as the study population to ensure that participants had experienced EPA observations [17].
The surveys were anonymous to eliminate potential confidentiality concerns, which could negatively impact on the trustworthiness of the data [17].
The findings from this study are similar to research involving residents, which indicate that these results are derived from data [9,16].
Clear records of the research path ensured that the research is dependable and confirmable [16].
This study provides a detailed description of the participants and the research process to enable readers to assess for transferability to their settings [16].
This study demonstrates that students have mixed feelings as to whether EPAs help support their clinical development.
Some students report that EPA observations encourage them to reflect on their performance more often than they would without the inclusion of EPA observations in their learning.
Self‐reflection is felt to be a key element of professional development, and it has been shown that increased reflection is associated with an improved learning experience [18].
Others felt that EPA observations forced faculty to provide supervision and feedback.
This has been reported in postgraduate medical education literature but, to our knowledge, has not previously been reported in undergraduate medical education [19].
This raises the question of whether the required documentation of the EPA observations may be prompting and/or promoting the verbal feedback that learners report that they value and prefer.
Over 90% of students report barriers to obtaining EPA observations, which likely adds to the perception that EPA observations are an administrative burden.
Students report that faculty often refuse to complete them or initially agree but then fail to complete them.
It is possible that students who perceive that EPA observations are an administrative burden are less likely to meaningfully engage with the feedback received in EPA observations, thus reducing learning opportunities.
Over 90% of students report barriers to obtaining EPA observations.
Faculty recognise that time to complete EPA observations is a barrier to completing them and that competing demands often mean that EPA observations are overlooked [20].
Students report being aware of faculty workload and, as a result, feel uncomfortable requesting EPA observations, limiting their ability to acquire them.
If undergraduate medical education wishes to continue using EPAs to drive learning and assess competencies, it is imperative that steps are taken to address faculty engagement and accountability.
Lack of faculty engagement with EPA observations can negatively impact the quality of feedback provided to students and can increase learner anxiety around requesting EPA observations [20].
Difficulties obtaining EPA observations in certain clerkship settings provide an opportunity for programmes to reassess which EPA observations are required in each clerkship.
Students in this survey suggested that some EPAs should only be assessed in certain clerkship settings due to lack of specific learning opportunities.
Another interpretation is that difficulties obtaining EPA observations can be used to identify clerkships, which require development to create richer learning opportunities [21].
Almost 90% of students expressed the need for changes to be made to EPA observations.
The predominant subtheme was a desire for a reduction in EPA observation requirements.
Students felt that the volume of EPA observations hampered, rather than supported, learning and that EPA observations took away from other valuable learning opportunities.
This raises the discussion of how many EPA observations should be required.
As the number of observations decrease, they may be perceived as more high stakes events.
To date, there are no clear data to suggest what the minimum number of EPA observations a student should obtain to be viewed as having achieved competence for a graduating medical student [22].
The predominant subtheme was a desire for a reduction in EPA observation requirements.
Some students felt that global performance assessments should be used instead of EPA observations.
Some felt that global performance assessments provide more overall feedback and a more holistic assessment of student's strengths and weaknesses.
Interestingly, one publication found that EPA observations discriminated clinical performance better than global clinical performance assessments [23].
However, another study reported that EPA observations and global assessment tools may capture different aspects of clinical performance, indicating that there may be a role for both [24].
Another subtheme that emerged was to move the onus of EPA observation completion from students to faculty.
Some suggested financial compensation to incentivise completion.
From a practical perspective, this is unlikely to be a realistic solution due to cost.
In this institution, the number of EPA observations completed is reported and recognised in faculty's annual report.
However, given the ongoing challenges that students report, it does not appear that this is having a significant impact on EPA observation completion.
The third subtheme identified involved the specific wording of EPA observation forms.
Multiple students felt that the language on the scoring scale was poorly understood by staff.
This aligns with other research findings, which report that the interpretation of the scale is dependent on the exact phrasing that the scale uses [25].
Students suggested that the EPA observation forms include specific expectations for each score to help reduce subjective variability.
Only 23% of eligible students completed the survey; however, it has been found that surveys with smaller sample sizes only require a 20%–25% response rate to provide fairly confident estimates [26].
The response rate also aligns with response rates from other surveys of healthcare professionals, with reported average response rates for online surveys ranging from 13% to 46% [27,28].
One concern with low response rates is whether the survey data are generalizable to nonrespondents.
Most nonresponse to surveys is passive in nature where potential participants forget about the survey.
Passive nonrespondents have been found to be attitudinally similar to survey respondents [29].
Conversely, active nonrespondents make a deliberate decision to not complete the survey.
Active nonrespondents tend to differ attitudinally from respondents but only represent approximately 15% of nonrespondents [29].
As a result, we feel that the survey results are generalizable to nonrespondents.
Response rate is only one component of survey validity.
By inviting all Year 4 students, we ensured that potential participants were representative of the population, which we hoped to study [17].
It also ensured that all participants were qualified to participate because they had all engaged with EPAs.
Another limitation of this study was this it was a single centre study; however, the transparency of the methods and study processes foster the ability for this work to be transferable to other institutions.
This study provides a valuable contribution to understanding student perceptions of narrative comments in EPA observations.
Future research can build on these results to further develop understanding of this important issue.
Students reported the view that verbal feedback was more helpful than narrative comments in EPA observations.
Future research could capture verbal feedback through audio recordings and assess the objective quality and student perceptions of this feedback.
Future research could also study whether narrative feedback in EPA observations could be improved by using artificial intelligence to summarise verbal feedback provided by preceptors.
Faculty development has been suggested to improve faculty engagement with EPAs, but it is unclear what format this should take [30].
Future research should evaluate the impact of different formats of faculty development on both faculty engagement and on the quality of narrative comments provided to students.
Students expressed a wish for changes to be made to EPAs.
Future research should focus on whether interventions such as changes to EPA requirements result in improved feedback quality and student satisfaction.
The results from this study suggest that students perceived the narrative feedback they received via EPA observations to be individualised, and specific.
They perceived that the feedback they received never or rarely included feedback for growth.
Students identified several barriers to EPA observation completion and recommended several changes to be made, most prominently reducing the number of EPA observations required.
Finding balance between the number of EPA observations that students are required to complete whilst maintaining meaningful feedback and other benefits of EPA observations must be a key focus in the future for undergraduate medical programmes.
