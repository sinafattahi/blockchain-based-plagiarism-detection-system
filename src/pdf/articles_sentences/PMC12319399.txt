Clinical documentation is necessary for effective and safe healthcare practice.
This paper systematically reviewed educational interventions aimed at developing entry‐level health professional students' clinical documentation skills.
A systematic search of electronic databases (PubMed, CINAHL, Embase and Cochrane) from January 2000 to May 2023 was performed, with additional forward and backward citation searching.
Inclusion was limited to original manuscripts published in English from January 2000, reporting an experimental or quasi‐experimental design and using objective performance‐based measures.
Quality appraisal was conducted using the Medical Education Research Study Quality Instrument (MERSQI), with narrative synthesis of results due to the heterogeneity of outcome measures.
Of 5313 records identified, 29 studies were eligible for inclusion.
The health professions represented were medicine, dentistry, nursing, pharmacy, psychology and veterinary science.
Teaching methodologies included the following: didactic instruction; provision of templates, guidelines and/or examples; instructor‐led group discussion; individual or group instructor feedback; near‐peer or peer feedback; self‐evaluation; writing practice activities; worked examples; and response‐to‐stimulus writing activities (written, video or live standardised patient cases).
Research quality was low in MERSQI domains of ‘study design’ and ‘validity of evaluation instruments’.
Several training methods appear valuable in developing student skills in clinical documentation; however, high‐quality evaluation of documentation training interventions is lacking.
Future research is recommended to compare existing methods of documentation training and to evaluate training in underexplored healthcare disciplines.
Clinical documentation is integral to effective and safe healthcare practice, and engaging with clinical documentation represents a significant portion of a health professional's workload [1].
Despite the central and ubiquitous role of clinical documentation, challenges with ensuring safe, effective and legal documentation practices are reported across health professions worldwide.
These challenges include concerns related to accuracy, structure and content of clinical documents, in addition to difficulties navigating clinical documentation workloads, workflows and systems [2,3,4].
Educational institutions delivering entry‐level health professional programs are responsible for preparing health professional graduates to practice clinical documentation at a minimum standard, typically in accordance with professional requirements and regulations.
However, research indicates that perceived inadequacies and challenges may exist in entry‐level clinical documentation training [5,6,7] and that health professional graduates may consequently be insufficiently prepared to practice effective clinical documentation upon workforce entry [4,8].
Additional challenges have been identified with prioritising clinical documentation training, secondary to perceptions among healthcare professionals and educators that this skill has reduced educational value or relevance within tightly packed entry‐level curricula [7,9].
The importance of ensuring that health professionals are equipped with effective clinical documentation skills is especially relevant in the context of the rapidly evolving global health information environment, which necessitates that health professionals adapt alongside changing health information systems, requirements and expectations [10].
Training approaches for developing entry‐level clinical documentation skills should accordingly be based on best‐practice pedagogical theories and methods.
A systematic review methodology has consequently been implemented to identify experimental or quasi‐experimental research studies that have evaluated the impact of educational interventions on clinical documentation performance in populations of entry‐level health professional students.
The aim of this systematic review was to identify and critically discuss pedagogical methodologies that may inform the design and optimisation of entry‐level health professional curricula in the domain of clinical documentation.
Ensuring that health professionals are equipped with effective clinical documentation skills is especially relevant in the rapidly evolving global health information environment.
The review protocol was developed in accordance with the Preferred Reporting Items for Systematic reviews and Meta‐Analyses (PRISMA) guidelines [11].
No ethical approval was required for this project.
This review was not registered, and a protocol has not previously been published.
An electronic search of four databases (PubMed, CINAHL, Embase and Cochrane Central) from January 2000 to May 2023 was performed.
Search terms were developed based on four main concepts linked by the ‘AND’ operator: problem (clinical documentation), population (health professional students), intervention (education) and outcome (skill development).
A combination of medical subject headings (MeSH terms), title terms, abstract terms and/or free keywords was used, with synonyms combined using the ‘OR’ operator.
Filters were applied, limiting results to publications in English, human studies and those dated from the year 2000 onwards.
Search strings were trialled in the PubMed database and refined to increase the search sensitivity and specificity.
Final search terms and strings for each database can be found inAppendices 1and2.
Forward and backward searching methods were also employed to identify further records, including checking reference lists of eligible studies and relevant literature or systematic reviews identified by the searches.
The review included studies that reported experimental or quasi‐experimental designs.
Eligibility was determined based on the actual study design reported, not the design labels attributed by the authors, as these have historically been inconsistently or incorrectly applied in education research [12].
Studies needed to be original manuscripts published in English, from January 2000 onwards, to maintain focus on contemporary education methods and professional needs.
Papers were also excluded if they had no full‐text available (e.g., conference abstracts or abstracts only).
No eligibility criteria were placed upon methodological quality.
Studies were included if they examined populations of healthcare professional students from one or more healthcare disciplines.
Healthcare professionals were defined as individuals who provide diagnostic, preventative and/or treatment services in the interest of human or animal health, with discipline lists from the International Standard Classification of Occupations used as guidance [13].
Students were required to be enrolled in an entry‐level, preregistration or prelicensure program.
No restrictions were placed on year of study, age, gender, country of origin or country of residence.
To be included, studies must have reported an aim to evaluate an educational intervention designed specifically to enhance clinical documentation or educational interventions targeting a range of clinical skills where clinical documentation was a primary focus.
Studies were excluded if they reported interventions aimed solely at improving scholarly, academic or scientific writing (e.g., essays, case reports and research articles), digital literacy skills (e.g., computer skills and competency using electronic health records) or other foundational skills independent of a clinical documentation context (e.g., English literacy or penmanship).
No restrictions were placed on the intervention delivery mode, duration or timing.
Studies must have measured clinical documentation as a primary outcome of interest.
If the study considered other clinical skills, separate data must have been reported for clinical documentation.
Studies measuring outcomes solely via student self‐report (e.g., self‐report surveys, self‐assessment, focus groups or interviews) were excluded, as research has shown learners have limited capacity to accurately self‐evaluate the outcomes of their learning [14].
No restrictions were placed on data collection timing.
Search results were imported to reference manager EndNote for duplicate removal and subsequently uploaded to Covidence, a web‐based software platform that coordinates the screening process.
One reviewer (S.W.)
initially screened all titles and abstracts to eliminate irrelevant studies.
Two authors (S.W.
independently screened remaining titles and abstracts for eligibility in accordance with defined criteria.
Additional records identified by forward and backward citation searching (S.W.)
were added to the screening pool.
Full‐text copies were retrieved in instances where the title and abstract contained insufficient detail to assess eligibility.
Conflicts during the screening process were resolved by discussion until there was consensus within the team.
Reasons for exclusion following full‐text screening were recorded (Figure1).
The included study data were extracted by two authors (S.W.
in the following key domains: study design, population characteristics, intervention characteristics, outcome measures and results.
Data were also extracted on the model or theoretical framework guiding study design [12].
Where data were unavailable or unclear, this was indicated using the phrase ‘not reported’.
Data synthesis was conducted narratively, according to key aspects relating to the educational interventions [15].
Statistical pooling and meta‐analysis was inappropriate due to methodological heterogeneity and incomplete reporting of outcomes.
The Medical Education Research Study Quality Instrument (MERSQI) was used to evaluate the quality of included studies, which was completed by the lead author (S.W.).
The MERSQI is a validated, widely used instrument for evaluating the methodological quality of medical education research [16].
The MERSQI consists of 10 items that evaluate six domains: study design, sampling, type of data, validity of evaluation instrument, data analysis and outcomes.
Each domain is scored on an ordinal scale from 0 to 3, with a maximum summed total of 18 points.
Higher scores indicate greater methodological quality, and interpretation should focus on item‐specific scores rather than the summed total [17].
Results are presented for individual domains (Table4) and discussed accordingly.
The database search strategies identified 5254 records after duplicates were removed.
Backwards citation searching of relevant reviews (n= 35) and forward citation searching (n= 24) resulted in a total of 5313 records.
The screening process yielded 135 articles for full‐text review.
Studies that did not have full text available (n= 7) or studies that did not satisfy inclusion criteria (n= 99) were excluded, with reasons recorded (Figure1).
The most common exclusion criteria were irrelevant aims (n= 40), ineligible outcome measures (n= 26) and ineligible study design (n= 25).
Notable ‘near misses’ reported either nonexperimental methodologies or insufficient outcome measures [18,19,20].
A total of 29 studies were included in the final review [21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49].
The two reviewers had moderate agreement during screening (Cohen's kappa coefficient, 0.51).
Figure1contains a detailed overview of the study selection process.
Reviewed studies included a mix of experimental (n= 5) and quasi‐experimental (n= 24) research methodologies.
Experimental study designs included randomised control trials (n= 2), a randomised 2 × 2 factorial trial (n= 1) and a randomised parallel intervention trial (n= 1).
Quasi‐experimental study designs included single‐group pre–post comparison (n= 7), single group repeated measure (n= 10), single group with historical control (n= 6) and nonrandomised control trials (n= 2).
The majority of studies were published between the years 2010–2019 (n= 16), with several studies published from 2020 onwards (n= 8) and relatively few prior to 2010 (n= 3).
Interventions were conducted in educational institutions predominantly located in North America (n= 24), Europe (n= 2) and Australia (n= 1).
The Middle East (n= 1) and Asia (n= 1) were also represented.
Detailed study characteristics are provided in Table1.
The health professions represented were medicine (n= 12), pharmacy (n= 12), dentistry (n= 2), nursing (n= 1), psychology (n= 1) and veterinary science (n= 1).
There were no interprofessional or cross‐institutional student cohorts.
Sample sizes were highly variable, with groups ranging from 19 to 365 participants.
The majority of studies (n= 15) had total sample sizes over 100 participants.
Educational interventions were conducted at different time points across the first to final years of training programs, with the largest number of studies featuring students in their third year (n= 14) compared with first year (n= 6), second year (n= 7) and fourth year or beyond (n= 5).
No studies evaluated clinical documentation training across the duration of an entry‐level program.
The majority of included studies (n= 21) did not report any age or gender data for the participant populations.
For the studies with available age data (n= 7), participants were typically in their twenties (range of 18–46 years), and six studies reported a majority of female participants.
Data on student ethnicity or language of origin were not available.
Detailed participant characteristics are provided in Table1.
The educational interventions were highly heterogeneous across the included studies.
Table2contains a summary of educational intervention design, with further details regarding educational intervention content provided in Appendix3.
Table3also provides results relating to intervention efficacy.
Summary of educational interventions to improve clinical documentation.
Abbreviations: SOAP = subjective, objective, assessment and plan; SOAPE = subjective, objective, assessment, plan and education.
Summary of included study outcome measures and results.
The predominant type of clinical documentation addressed by the educational intervention was ‘clinical records’, including: SOAP note (n= 8), SOAPE note (n= 1), patient note (n= 3), clinical note (n= 1), write‐up (n= 2), consult note (n= 2), case note (n= 2) and history of presenting illness (n= 2).
Other clinical documentation types represented included: discharge letters or summaries (n= 2), clinical reports (n= 1), email (n= 2), written patient communications (n= 2) and dental prosthetic prescriptions (n= 1).
Educational interventions were either embedded within an existing curriculum or coursework (n= 18) or implemented on clinical practice placements, rotations, and clerkships (n= 6).
Some interventions were standalone courses addressing learning objectives solely related to clinical documentation, and other interventions were situated within a broader course or activity with additional learning objectives.
Learning environments ranged from lecture halls, classrooms, and clinical spaces (simulated or real).
Class sizes and ratios of instructors to students were highly heterogeneous, and dependent upon the type of instructional activity being implemented; for example, simulation activities often provided learners with more direct and individual instructor contact versus didactic instruction where a single instructor delivered material to a large cohort of learners.
Several pedagogical methodologies were reported, including: didactic instruction, guided discussion (either instructor led or near‐peer led), self‐evaluation, peer review, individualized or group feedback, provision of templates and guidelines, worked examples (e.g., audit activities or structured review of examples) and practice writing activities (e.g., simulated standardised patients and response‐to‐stimulus activities).
Many studies combined several teaching methods within their educational intervention, often in the following sequence: (i) didactic instruction; (ii) provision of templates, examples and/or guidelines; (iii) writing activity or task; and (iv) feedback and/or reflection.
Content was highly heterogenous across the sample of studies.
Didactic introductory sessions tended to focus on concepts of documentation accuracy, readability and medicolegal adherence.
Templates, examples and guidelines provided to learners typically included explicit examples of the documentation type, checklists of key features relevant to the document and/or rubrics indicating quality measures for different document criteria.
Response‐to‐stimulus writing activities and simulated documentation practice often involved case‐based patient scenarios—either simulated in real time or prompted through pre‐recorded audiovisual or written material.
Further detail on intervention content is available in Appendix3.
Outcome measures were highly heterogeneous and are summarised in Table3.
Most studies implemented criteria‐based or scaled rubrics (n= 17), dichotomous checklists (n= 5), a global rating scale (n= 1) or a combination of outcomes (n= 6) to assess clinical documentation performance.
Outcome measures typically focus on writing elements such as content, length, formatting, style of writing and other literacy features (e.g., spelling and grammar).
The majority of these outcome measures were designed for an instructor or third‐party evaluator to complete; however, some interventions involved releasing the outcome measures to students as part of peer‐review feedback, self‐evaluation or instructor feedback.
Studies either generated outcome measures specifically for the project, used existing outcomes embedded within the curriculum or used modified versions of previously published outcome measures.
Access to full outcome measures was provided in the majority of studies (n= 16).
No studies measured outcomes related to the impact on patient outcomes.
The total MERSQI scores of the 29 studies ranged between 9.5 and 13.5 points (from a possible maximum of 18 points).
Research quality was low in the MERSQI domains of ‘study design’and ‘validity of evaluation instruments’.
Table4provides the MERSQI scores for individual domains.
The aim of this systematic review was to explore the effectiveness of educational interventions to improve clinical documentation skills in health professional students.
A total of 5313 records were identified from the search strategy, and subsequent independent screening yielded a total of 29 studies for inclusion.
These featured entry‐level student populations from the disciplines of medicine, dentistry, nursing, pharmacy, psychology and veterinary science.
Teaching methodologies evaluated in those populations included the following: didactic instruction; provision of templates, guidelines and/or examples; instructor‐led group discussion; individual or group instructor feedback; near‐peer or peer feedback; self‐evaluation; writing practice activities; worked examples; and response‐to‐stimulus writing activities (written, video or live standardised patient cases).
The quality of the 29 included studies was assessed using the MERSQI, which indicated low methodological quality in the domains of ‘study design’ and ‘validity of evaluation instruments’.
The majority of studies (n= 23) reported on highly heterogeneous interventions that featured a combination of different teaching methods.
These results suggest that educators prefer to employ several different educational methods to develop documentation skills in entry‐level learners.
Although the majority of studies reported favorable outcomes from the featured educational interventions, caution should be taken when drawing conclusions about the efficacy of these results due to the overall low quality of methodological design for many included studies (e.g., the absence of control or comparison groups or the absence of pretest measures).
Results suggest that educators prefer to employ several different educational methods to develop documentation skills in entry‐level learners.
Didactic instruction was featured frequently within the described educational interventions, often in combination with other teaching methods.
Health education literature has increasingly emphasised educational methods that promote active, learner‐driven knowledge construction over traditional didactic instruction [55].
However, according to schema theory [56], learners are best positioned to learn novel information or skills by first constructing ‘schemas’ or frameworks to organise their understanding and attribute meaning.
Active learning methods, such as problem solving (e.g., case‐based learning and response‐to‐stimulus), may be poorly suited to developing this foundational understanding in the form of schemas for a highly procedural skill such as clinical documentation [57].
Therefore, it is important to appreciate that didactic instruction likely has a key role in the early stages of teaching documentation in entry‐level healthcare programs.
Multiple studies featured learning interventions under the category of ‘problem solving’, such as response‐to‐stimulus activities (n= 16).
According to the expertise reversal effect, the use of didactic and prescriptive teaching methods typically becomes less effective as the expertise of a learner increases and may eventually become redundant or counterproductive to skill development [58].
Therefore, it is important to consider the gradual incorporation of independent problem‐solving tasks as learners gain expertise in this domain.
Many studies featured in the review reflected this by including problem‐solving in the educational interventions delivered to students in the later years of their entry‐level program.
Simulation‐based documentation learning activities featured strongly across the included studies, including live standardised patient encounters (n= 4), video recordings of simulated patient encounters (n= 4) and written case material (n= 8).
Engaging in authentic learning experiences that reflect the complexities of practice is strongly advocated within health education literature [59] and also represents the evolution of increasingly complex problem‐solving activities for learners progressing along a continuum of skill development.
Students may therefore benefit from developing documentation skills through simulation‐based learning, which supports learners to master clinical documentation skills in a controlled learning environment [60].
Several studies provided opportunities for learners to receive feedback on their performance.
Studies employed instructor feedback (n= 12), peer feedback (n= 6) or self‐evaluation (n= 3), typically alongside use of a guideline (such as a rubric or checklist) to facilitate reflection.
Provision of worked answers and documentation guidelines to students may be a more realistic and practical use of educational resources in higher education contexts, given that individual feedback on written documents can be resource‐consuming.
Encouraging self‐reflection and peer feedback may also facilitate students to further develop professional reflective practice skills, which have been reported as crucial for ongoing clinical documentation skill development during clinical placements and upon entry to the workforce [8].
Most studies in this systematic review describe a single module or standalone course delivered to students later in their entry‐level program, demonstrating a trend towards teaching clinical documentation skills to students with an existing level of clinical knowledge.
This aligns with literature indicating a connection between clinical reasoning processes and clinical documentation skills [61,62] and suggests that educators may seek to ensure students have a sufficient foundation of clinical knowledge upon which to successfully develop their clinical documentation skills.
Previous research has identified a preference for clinical documentation to be taught throughout an entry‐level curriculum in a distributed fashion, evolving in complexity alongside the acquisition of clinical knowledge and development of clinical reasoning [7].
Thus, implementing documentation training across the duration of entry‐level programs may also provide valuable opportunities to enhance reflective practice and reinforce clinical knowledge.
Relatively few studies (n= 5) explicitly described the use of a conceptual or theoretical framework to guide educational intervention design.
This is consistent with Cook et al. [12] who found this area to be lacking in medical education research.
Conceptual frameworks consist of theories, models and best practices to explain how a concept operates or functions and are essential to the quality design of health education research [63].
The absence of a guiding theory limits the ability to meaningfully interpret outcomes and adapt educational interventions for implementation in curricula or future research [64].
Outcome measures were highly heterogeneous across the included studies, with the majority of studies using criteria‐based rubrics to assess documentation performance.
Few papers reported information regarding criterion validity of the outcome measures employed (n= 2), face validity was deemed appropriate in a furthern= 14.
Many papers also did not report information regarding the interrater reliability of outcome measures (n= 19).
This has implications for the validity of reported findings and whether outcome measures were sufficiently specific, sensitive or reliable to evaluate changes in clinical documentation performance.
The educational institutions providing these educational interventions were predominantly located within developed, Westernised countries in the continents of North America, Europe and Australia.
This is likely a consequence of the inclusion criteria requiring papers to be published in English but may also be indicative of the unequal representation of developing countries and non‐Western cultures in health professions education literature [65].
Given the increasing globalisation of healthcare education and the drive for the World Health Organisation to create globally standardised clinical documentation guidelines [10], seeking evidence from non‐Western and developing nations may provide unexplored insights on aspects of health professions education such as clinical documentation.
Despite the amount of existing literature dedicated to teaching documentation in nursing students [5], only one study investigating a population of nursing students met the inclusion criteria for this systematic review.
Likewise, there was a distinct lack of studies investigating populations of allied health professional students; a single paper was included examining psychology students.
This systematic review has identified a gap in published studies on documentation training for health professional students outside the disciplines of medicine, nursing, pharmacy and dentistry.
Writing skills are widely acknowledged to be domain‐ and context‐dependent and often do not easily transfer across genres [66], indicating the need for research investigating the efficacy of clinical documentation training in under‐researched healthcare disciplines (e.g., nursing, allied health).
The systematic review also highlights the opportunity for improvements in the study design of future projects evaluating the efficacy of clinical documentation education, including ensuring robust sampling methods, selecting appropriate and validated outcome measures and reporting a guiding theoretical framework.
Key strengths of this review were: use of a comprehensive search protocol; independent screening, data extraction and quality assessment; use of a robust methodological quality assessment tool (MERSQI); and narrative synthesis and discussion of results by a research team of registered health professionals who collectively possess extensive experience in healthcare practice, healthcare quality and safety processes, healthcare curriculum design and delivery and implementation of teaching and learning research.
However, it is important to note several limitations of this review.
Only papers with full‐text available in English were considered eligible, resulting in selection bias that possibly excluded research from regions or institutions that offer entry‐level healthcare training in languages other than English.
A third‐party was not used to mediate conflicts in the screening process—these were resolved with discussion between the two independent reviewers—possibly introducing some selection bias.
This review did not place any specific requirements on outcome measures, given that no ‘gold standard’ performance‐based measures exist for measuring clinical documentation skills across different health professions, resulting in a high level of heterogeneity of results.
A research quality threshold was not incorporated in the inclusion criteria, which has the potential to lower the overall quality of research studies included for review and consequently impact the strength and quality of outcomes and recommendations resulting from this review.
This systematic review aimed to examine the efficacy of teaching interventions for developing clinical documentation skills and, to the best knowledge available, is the first to report on clinical documentation training interventions for students across all health professions disciplines and all documentation types.
Understanding the efficacy and usefulness of various teaching methodologies is key to developing an entry‐level curriculum for health professional programs in the domain of clinical documentation.
This review determined that several methodologies are effective in improving student performance in clinical documentation, including didactic instruction, peer review, explicit feedback, worked examples and simulation (e.g., documentation in response to a stimulus).
This review highlights the need for additional research examining clinical documentation training approaches and further examination of the impact of entry‐level training on student transition to the workforce.