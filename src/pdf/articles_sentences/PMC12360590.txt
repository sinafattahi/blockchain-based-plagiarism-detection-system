The rise of social media has revolutionized information dissemination, creating new opportunities but also significant challenges.
One such challenge is the proliferation of fake news, which undermines the credibility of journalism and contributes to societal unrest.
Manually identifying fake news is impractical due to the vast volume of content, prompting the development of automated systems for fake news detection.
This challenge has motivated numerous research efforts aimed at developing automated systems for fake news detection.
However, most of these approaches rely on supervised learning, which requires significant time and effort to construct labeled datasets.
While there have been a few attempts to develop unsupervised methods for fake news detection, their reported accuracy results thereof remain unsatisfactory.
This research proposes an unsupervised approach using clustering algorithms, including Gaussian Mixture Model (GMM), K-means, and K-medoids, to eliminate the need for manual labeling in detecting fake news.
In particular, it also proposes a novel hybrid method that leverages the Gaussian Mixture Model (GMM) in conjunction with the Group Counseling Optimizer (GCO), a metaheuristic optimization algorithm, to identify the optimal number of clusters for the detection of fake news.
The comparative analysis of the evaluation results on real-world data demonstrated that the proposed hybrid GMM outperforms the state-of-the-art techniques, with a silhouette score of 0.77, ARI of 0.83, and a purity score of 0.88, indicating a significantly improved quality of clustering results.
The emergence of the Internet and the increasing popularity of social media platforms, such as Facebook and Twitter, have revolutionized the way information is disseminated in today‚Äôs world.
This revolution has led news providers to transition from traditional media formats, such as newspapers and magazines, towards digital formats including online news platforms, blogs, and social media feeds, enabling them to deliver up-to-date news in near real-time to their subscribers [3,4].
As the internet usage has expanded, an increasing number of consumers turned away from traditional media channels, preferring to access information through online platforms [5,6].
This shift has made it incredibly convenient for consumers to access the latest news instantly [7,8].
Consequently, web portals have become a popular tool for delivering and receiving news, especially for users who are always on the move [9].
Notably, Facebook alone drives 70% of the traffic on news websites [10].
Social media platforms have become powerful tools for discussion, idea-sharing, and debate on topics like democracy, education, and health [11‚Äì14], enabling decentralized content creation and consumption [15].
However, alongside these benefits come significant challenges [2,11,16‚Äì19].
The overwhelming volume of information can make it difficult to distinguish credible sources [19,20].
Moreover, these platforms are often misused for financial gain, spreading biased views, manipulating opinions, and disseminating misinformation or fake news [21‚Äì24].
The phenomenon of fake news is not a recent development.
It existed even long before the advent of the Internet [25].
Allcott and Gentzkow [26] initially defined the concept of fake news as news that is intentionally fabricated and verifiably false and designed to mislead readers [27].
Another widely used definition of fake news is that fake news itself is not actual but is made real for a specific purpose [28].
Additionally, the dissemination of fake news on social media platforms is exacerbated by its user-friendly and rapid sharing capabilities, which provide remarkable convenience in the creation and distribution of misinformation, further imploring the audience to disseminate the content at an exponential pace on the Internet.
Unlike social media, mainstream media follow editorial standards and guidelines, and are operated by professional journalists [29].
On the other hand, social media primarily relies on user-generated content with limited editorial controls or gatekeeping procedures [30].
Instead of encountering reliable and unbiased information, users are inundated with a multitude of fake news on social media platforms [31,32].
Therefore, social media has also become a breeding ground for misinformation [33].
There have been numerous examples of fake news throughout history.
A recent example is the proliferation of information about COVID-19.
In late 2019, massive amounts of informative data related to COVID-19 pandemic were generated and disseminated on social media networks.
Although, these networks make it possible to disseminate information to a large audience [34].
Unfortunately, not all disseminated information about the outbreak is accurate and trustworthy.
During this era, some of the information disseminated around these networks was identified as fake news or even exacerbated the crisis by promoting unproven treatments and fueling vaccine hesitancy leading to detrimental health, social, and cultural consequences [35].
Thus, WHO called it an ‚Äúinformation epidemic‚Äù.
Fake news can also incite violence, panic, or harm to public safety in various contexts [36,37].
Unfortunately, the epidemic spread of fake news is a byproduct of the popularity of the social media platforms.
According to a report, only 26% of the participants expressed a high level of confidence in discriminating between real and fake [28,38].
This ratio is comparatively low and indicates insufficient capacity to discriminate between genuine and fake news.
Consequently, it has become increasingly challenging for authoritative institutions to effectively combat the rapid dissemination of fake news.
Although significant efforts have been made to address this issue, researchers are increasingly turning to artificial intelligence (AI) solutions [39].
Machine learning, particularly natural language processing techniques, have shown promising contribution in fake news detection [40‚Äì43].
Other researchers [44‚Äì46] explored to utilize a combination of different deep learning models.
Some have even demonstrated that deep learning techniques outperform traditional machine-learning techniques [46].
However, the inherent ‚Äúblack-box‚Äù nature of advanced deep learning models presents significant challenges, particularly in fields requiring transparency and interpretability [47].
According to the existing literature, considerable attention has been given to machine learning-based supervised methods for detecting fake news [41‚Äì43].
These studies developed classification models using different sets of features including user profiles, news content [29], message propagation [48] and social contexts [23,49‚Äì51].
While these models have shown promising results to some extent, these supervised methods require labeled data to build a classification model [52,53].
However, obtaining a large number of annotations is time consuming and labor intensive, as the process requires careful examination of news content, as well as other additional evidence such as authoritative reports [54,55].
Leveraging a crowdsourcing approach to obtain annotations could alleviate the burden of expert checking; however, the quality of annotations may be compromised [56].
As fake news is intentionally written to mislead readers [57], individual human workers alone may not possess the domain expertise to differentiate between real and fake news [58].
Besides this, fake news detection often involves identifying and tracking patterns in data that change dynamically over time, such as shifts in the spread of misinformation, the emergence of new narratives, or changes in the behavior of online communities.
These evolving patterns require models that can adapt to and account for changes in the data distribution, similar to how dynamic systems evolve based on state variables and external inputs.
In the context of Gaussian Mixture Models (GMMs) [59], these models have been widely applied for modeling dynamic systems to capture the underlying probabilistic structure of time-varying processes [60‚Äì67].
Similarly, in fake news detection, GMMs can be used to model the distribution of news articles or social media posts that change over time, grouping them into clusters that reflect the underlying patterns of authenticity versus misinformation [58].
By leveraging GMMs, we can identify clusters of fake and real news articles that evolve as new information is disseminated, without requiring labeled data for training [68].
Although, GMM is a potentially powerful technique for fake news detection, it has several limitations when applied to textual data [68].
As, it relies on the assumption that data is generated from a mixture of Gaussian distributions, which may not hold for textual data, as these features often exhibit complex, non-Gaussian patterns [69].
Additionally, textual data is typically high-dimensional, and GMM can struggle with the curse of dimensionality [64,70‚Äì73].
The Expectation-Maximization (EM) algorithm used in GMM is also sensitive to choice of initial parameters selection, potentially leading to convergence on local optima rather than the global optimum, leading to erroneous results [74‚Äì76].
Overall, these limitations highlight the need for additional techniques or enhancements to improve the model‚Äôs effectiveness in fake news detection.
Unlike EM algorithm, GCO, a meta-heuristic optimization technique, explores the parameter space more effectively through iterative improvements based on group counseling behavior, helping find the global optimum and avoiding local optima [77,78].
GCO handles large datasets well, making it suitable for real-world news classification [79].
To address these challenges, this paper presents a novel hybrid Gaussian Mixture model, GCOGMM, a framework to identify fake news in a fully unsupervised manner.
The primary objective of this study is to optimize parameters selection and avoid local optima, thereby enhancing the model‚Äôs ability to effectively handle complex, high-dimensional textual data to effectively identify fake information.
The proposed research integrates Group Counseling Optimization (GCO) with the Gaussian Mixture Model (GMM) to optimize parameters selection and avoid local optima, thereby enhancing the model‚Äôs ability to effectively handle complex, high-dimensional textual data.
To explore the automatic fake news detection based on the analysis of surface-level linguistic patterns.
A comprehensive evaluation analysis of the proposed approach is proformed on benchmark dataset.
Particularly, for evaluation, the proposed research developed three baseline unsupervised methods for detecting fake news, namely k-means, K-mediods and Gaussian Mixture Models (GMM).
Additionally, to the best of available knowledge, the utility of the proposed approach when combined with standard GMM has not been investigated to address the issue of fake news identification on social media, which has become a daunting societal and individual issue.
Furthermore, evaluation results depicted that the proposed method outperforms over all the baseline models.
It also demonstrated that this hybrid approach can improve a text-only Gaussian model.
The dataset provides a detailed classification of news articles into six subcategories: True, Mostly True, Half True, Barely True, False, and Pants on Fire.
These subcategories offer a nuanced perspective on the varying degrees of truthfulness within the dataset.
These subcategories are represented by varying frequencies within the dataset: Half True (2,114 instances), False (1,994 instances), Mostly True (1,962 instances), True (1,676 instances), Barely True (1,654 instances), and Pants on Fire (839 instances).
Each data point in the dataset have a unique ID.
This distribution reveals an imbalance in the dataset, with certain categories like Half True and False being more prevalent, while others such as Pants on Fire are significantly less represented.
This imbalance highlights the complexities of truthfulness identification and may pose significant challenges during the analysis process.
To address this problem and ensure an equitable representation across all the categories, we balanced the dataset by randomly selecting 839 instances from each category without replacement.
This approach helps to create a more uniform dataset.
However, to ensure that the clusters formed reflect the actual structure of the data, rather than being influenced by human-imposed labels that may not fully capture the complexity or nuances of the dataset, all pre-existing labels are removed before conducting the clustering analysis.
This approach aligns with the principles of unsupervised learning and is essential for achieving accurate, data-driven clustering outcomes in the context of this study.
As a written document is composed of sentences, and sentences can comprise a combination of uppercase and lowercase letters, the initial preprocessing step involves converting all text data to a single text case.
We have chosen lowercase for this purpose.
This is crucial to ensure consistency in the text analysis procedure [80,81].
By converting everything to lowercase, we eliminate potential inconsistencies that may arise from various cases of the same word.
It is a crucial step in NLP where a stream of text is divided into smaller units, which typically referred to as tokens [82].
These tokens can be words, phrases, symbols, or individual characters, depending on the level of granularity needed for the analysis.
This process is essential for many NLP tasks as it facilitates the understanding and manipulation of text by converting it into a format that computational models can process.
Thus tokenization is often the first step in a pipeline that includes other tasks, which collectively enhance the performance of NLP applications [82].
Therefore, tokenization is performed on each sentence in the dataset.
Following tokenization, it is quite common to remove non-alphanumeric characters from the text.
This particular step includes eliminating symbols, punctuation marks, and special characters that do not hold substantial semantic meaning in the text [83].
Examples of non-alphanumeric characters include commas, periods, exclamation marks, and other symbols that are not part of words or numbers.
Stop words are indeed terms that are commonly found in documents and are considered to have little semantic meaning in the context of text analysis.
Their removal is a standard preprocessing step in NLP and text mining applications to improve the performance and efficiency of these systems [84].
Therefore removed from the text during the analysis process.
Examples of stop words include ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúand,‚Äù ‚Äúof,‚Äù and so on.
The removal of stop words helps to reduce noise in the text data and concentrate on more significant terms.
Moreover, the elimination of punctuation marks further refines the text and prepares it for analysis.
NLTK library in Python contains a list of stop words for the English language.
Stemming is a process widely used to reduce words to their derived words known as stem, base, or root form.
This process typically entails the elimination of extraneous characters to normalize words, which involves removing prefixes and suffixes [85,86].
For instance, the words ‚Äúwrite‚Äù, ‚Äúwrote,‚Äù, ‚Äúwritten‚Äù and ‚Äúwriting‚Äù would all be stemmed to the base form ‚Äúwrite‚Äù By stemming words, the vocabulary size is reduced, and similar words are consolidated, which can enhance the effectiveness of text analysis algorithms [87].
Porter Stemmer is one of the stemming model which is used in this study to convert the words into their root form.
After applying the aforementioned preprocessing steps, a high-dimensional feature space of 184,412 unique tokens were obtained and are reunited to form a processed text dataset that is prepared for further analysis.
This consolidated text preserves the pertinent semantic information while eradicating extraneous details and noise, making it appropriate for tasks such as text classification, sentiment analysis, or information retrieval.Fig 1showcases a more detailed representation of the dataset in the form of word cloud.
The conspicuous presence of political news and the substantial inclusion of global events indicate particular thematic priorities within the media.
This observation invites a deeper analysis of the content, aiming to uncover potential underlying influences that may be shaping the portrayal of political and global events in the dataset.
The choice of metrics for encoding textual data is pivotal in determining the performance and suitability of a particular model.
While advanced encoding methods like contextual embeddings or topic modeling techniques (e.g., BERT, LDA) offer deeper semantic understanding, their high computational cost and preprocessing requirements [88] may not align with the study‚Äôs objectives.
As the primary research objective is to enhance the efficacy of detecting fake news using unsupervised techniques.
Therefore, to achieve this, TF-IDF (Term Frequency-Inverse Document Frequency) [89] was chosen based on its balance between simplicity, interpretability, and its effectiveness in identifying informative features within textual data.
It excels in distinguishing relevant terms by penalizing high-frequency words that are common across documents, making it suitable for clustering tasks where capturing term-specific relevance is crucial.
Thus, the TF-IDF is instrumental in transforming unstructured textual information into a format that is compatible with machine learning techniques [91].
It represents text as numerical vectors enables algorithms to operate on the data efficiently to extract meaningful patterns, and make informed decisions.
After carrying out above mentioned preprocessing tasks, it is necessary to decrease the number of features to lower computational complexity and improve performance [92‚Äì94].
The optimal selection of feature extraction algorithm is crucial to the quality of the results [95].
According to existing literature to deal with the high dimensional data Information Gain (IG) [96], Mutual Information [97,98], Gini Coefficient (GI), Term Frequency-Inverse Document Frequency (TF-IDF), Principal Component Analysis (PCA) [27,99,100] and Chi-Square Statistics (CHI) [101] are frequently used for feature extraction.
However, to improve the scalability of the obtained results, PCA is widely used as the dimensionality reduction approach in fake news detection applications [102‚Äì106].
PCA applies a linear transformation to reduce the dimensionality of a feature set, simplifying the dataset while preserving the essential characteristics of the original data [102].
The transformed dataset may have the same or fewer features compared to the original.
Principal components are derived by calculating the covariance matrix [107].
These components are arranged in decreasing order of importance.
Q is the matrix of eigenvectors of Œ£ derived from the text vectors, where each column represents an eigenvector.
Where Œõ is a diagonal matrix containing the corresponding eigenvalues.
This reduction in dimensionality facilitated faster processing and aided in uncovering underlying patterns and structures within the textual data.
The transformed data, now represented in a lower-dimensional feature space, served as the input for our clustering algorithm as depicted inFig 2.
Where x is a D-dimensional data point Œº is the D-dimensional mean vector, Œ£ is a D √ó D variance matrix, |Œ£| determinant of the variance matrix andŒ£‚àí1is the inverse of the variance matrix, allowing the model to capture complex, multidimensional relationships within the data..
Here P represents the probability of component k such that‚àëk=1KœÄk=1.
WhereasŒºkis the D-dimensional mean vector of component k,Œ£kis the variance matrix of component k andùí©(x|Œºk,Œ£k) is the multivariate Gaussian PDF of component k. Subsequently, Expectation-Maximization (EM) Algorithm for Multivariate GMM consists of two steps.
The first step, known as the expectation step to calculate the probability for each data pointxito eachCkwherexi‚ààXgiven the model parameters |Œºk,Œ£kandœÄk.
Œ≥(ùíµnk)is the probability that data pointxkbelongs to component k. whereas the second step known as Maximization in this step, the parametersœÄk,ŒºkandŒ£kare updated based on theŒ≥(ùíµnk)computed in the expectation step.
Thus,Œ≥(ùíµnk)=p(Ck|xi,Œº,Œ£,œÄwhere.
Herexiis the data point,Œº,Œ£,œÄare the parameters of the GMM andN(xi|Œºk,Œ£k)is the probability density function of component k evaluated at a data pointxi.This process continues iteratively until convergence criteria are met, such as changes in log-likelihood or parameter stability.
Incorporating Group Counseling Optimization (GCO) with a Gaussian Mixture Model (GMM) automates and improves the parameter tuning process addressing the limitation of traditional GMM.
As discussed above the performance of traditional GMM is sensitive to key hyper-parameters, such as the number of components and the convergence threshold of the Expectation-Maximization (EM) algorithm.
Furthermore, manual tuning methods often fail to ensure optimal performance, especially with complex datasets.
Group Counseling Optimization (GCO) is a a population-based metaheuristic optimization algorithm inspired by the idea of collaborative learning and behavioral improvement in group counseling dynamics.
It avoids local optima primarily through population diversity, iterative feedback mechanisms, and dynamic exploration-exploitation balancing.
Unlike traditional optimization algorithms that follow a single trajectory (e.g., gradient descent), GCO maintains a diverse population of candidate solutions.
It maintains a diverse population of candidate solutions, allowing it to explore multiple regions of the solution space simultaneously and avoid being trapped in local optima.
Individuals learn from better-performing peers through guided interactions, helping weaker candidates move toward more promising areas.
Furthermore, during each iteration, individuals undergo small perturbations or adjustments that introduce randomness in their behavior (solution state).
This allows GCO to explore new areas in the solution space rather than converging prematurely to a suboptimal point.
The randomness ensures that even if the population converges temporarily, there‚Äôs always a chance of discovering better global optima.
It also balances exploitation of strong solutions with exploration of new possibilities, ensuring a robust search process.
Over successive iterations, the population improves collectively, enhancing convergence toward globally optimal solutions.
It efficiently explores the parameter space, preventing local optima and ensuring globally optimized solutions.
This integration not only addresses the limitations of traditional GMM but also provides a scalable and purely unsupervised framework suitable for real-world applications where labeled data is limited or unavailable.The pseudocode of the proposed Hybrid GMM is depicted inFig 3.
Thus, to optimize a Gaussian Mixture Model (GMM) for fake news detection using Group Counseling Optimization (GCO), we start by initializing the population of agents.
An agent represent an individual candidate solution in the search space.
Each agent in the population is represented by a parameter vectorxi=[xi1,xi2,xi3,‚Ä¶‚Ä¶,xid], where each parameterxij, is randomly generated within its respective bounds [li,ui}.
Here, bounds variable particularly defines the ranges of the parameters that the Group Counseling Optimization (GCO) algorithm will optimize.
Each element of the bounds list is a tuple that specifies the lower and upper limits for a particular parameter.
Specifically, the bounds cover key GMM hyperparameters, including the number of components, Regularization parameter for covariance matrices, Maximum number of iterations for the Expectation-Maximization (EM) algorithm, and tolerance (Convergence threshold.
Mathematically, each parameterxij, for agentiis drawn from a uniform distribution within its boundsxij~u(li,ui).
This transformation is used because optimization algorithms, including GCO, are typically designed to minimize an objective function.
The optimization process involves iterating for a predefined number of iterations.
In each iteration, the mean vector of the population is calculated, and a random vector with values in the range [‚àí1] is generated.
Each agent‚Äôs parameters are updated based on the difference between the agent‚Äôs parameters and the mean vector, scaled by the random vectorr.
The updated parameters are adjusted to ensure they range within the predefined bounds.
The fitness of the updated agentf(xnew)is then evaluated.
If the new fitness is better (lower) than the current fitness, the agent‚Äôs parameters are updated to the new values.
This process is repeated for all agents in the population and continues until convergence or maximum iterations are reached.
After completing the iterations, the agent with the best (lowest) fitness in the population is identified as the best solutionx*=argminx_if(xi).
This optimization process using GCO ensures that the GMM parameters are tuned to achieve the highest accuracy possible to effectively distinguish between real and fake news.
K-means clustering is a fundamental clustering algorithm within the realms of unsupervised techniques.
It discover concealed patterns within the data while effectively grouping data points that share similarities together [109].
Its primary objective is to iteratively partition the data points into k non overlapping clusters.
During this partitioning process, each cluster finds its representation through a centroid point based on a distance metric [110].
This process aims to minimize intra-cluster variance and maximize inter-cluster variance, leading to well-defined clusters [111].
Let D is a D-dimensional dataset of n observations represented asX={x1,x2‚Ä¶‚Ä¶xn}where each data point belongs to a cluster C = {C1, C2,‚Ä¶.., Ck} based on some proximity measure.
In this research to find out the similarity between data points we incorporated Euclidean distance as a proximity measure or partitioning method Dist(i, j) that assures how similar or distant observation i from j are; consequently build k distinct partitioning from a D dataset of n observations.
Where each k corresponds to a distinct cluster consist of at least one object and K ‚â§ n. Euclidean proximity measure can be determined as follows.
In this research, we also incorporated K-means clustering to group TF-IDF vectors of news data with similar characteristics, aiding in the identification of patterns and clusters within the dataset.
However, it requires specifying the number of clusters (K) in advance, which is a challenging task.
In this research we incorporated elbow and Bayesian Information Criterion (BIC) techniques to address this challenge.
K-medoids clustering [112] is similar to K-Means, but instead of using the mean of the data points to determine the cluster centre point, it uses actual data points as the cluster centers (medoids).
For each data pointxifind the medoidmjthat best represents the center of each cluster by minimizing the sum of distances within each cluster, enhancing the robustness and interpretability of the clustering results.
Where k is the number of clusters, x is the data point in the ith cluster withŒºicentre point.
In this study, we employed the Bayesian Information Criterion (BIC) [116] to determine the optimal number of components for a Gaussian Mixture Model (GMM) applied to our dataset.
The BIC is a statistical criterion used for model selection, balancing model complexity and goodness of fit [117].
The model with the lowest BIC was considered the most suitable in terms of both model complexity and ability to capture the underlying structure of the data [88].
The Elbow method and BIC are incorporated as baseline techniques for cluster estimation for classsical K-means, K-medoids, and GMM techniques to facilitate comparative evaluation.While commonly used, the Elbow Method, especially when relying on first-order derivatives (Œî) often lacks robustness and precision.
This highlights the need for a more adaptive approach.
The proposed GCO-based optimization addresses this by effectively exploring the parameter space without relying on heuristic or visual methods.
In this study, the proposed method is implemented in Python.
While the K-means [112], K-medoids [113] and GMM [59] based clustering algorithms were implemented using python scikit-learn package, a widely adopted library for machine learning in Python.
The computational evaluation were conducted on a Laptop equipped with an Intel Core i7 10th Generation processor, 16 GB of RAM, Windows 10 operating system and a solid-state drive (SSD) connected via the Serial Advanced Technology Attachment (SATA) interface.
To evaluate the performance of the clustering results, three evaluation measures are commonly used in existing literature including silhouette score, adjusted rand index and purity score.
These evaluation metrics help in assessing the effectiveness of the unsupervised techniques and provide quantitative measures of its performance.
Hence, we also incorporated these measures to evaluate the performance of our method.
The silhouette score offers an intuitive way to evaluate the quality of a clustering result by determining the mean silhouette coefficient across all samples [118,119].
This quantifies how well each data point fits within its assigned cluster compared to other clusters.
Its value ranges from ‚àí1‚Äì1, where a higher silhouette score suggests better-defined, well-separated clustering results [121].
The silhouette score for a data pointiis defined as: a metric that quantifies how well each data point fits within its assigned cluster compared to other clusters.
Wherea(i)represents the average distance between the pointiand all other points within the same cluster whileb(i)is the minimum average distance between the pointiand all points in the next nearest cluster [122].
However, the overall silhouette score for the entire dataset is calculated by the average of the silhouette scores for all the individual points.
WhereRIdenotes the Rand Index that measures the agreement between two clustering results by counting pairs of data points that are either grouped together or assigned to different clusters in both the predicted and true clustering.
WhereE[RI]represents the expected Rand Index, accounting for random chance.
The ARI has been generalized to compare a set of clustering solutions with a consensus matrix (ARImp) and to evaluate the consistency between two similarity matrices (ARImm), preserving desirable properties of the original ARI and introducing new ones, such as the ability to detect negative correlation and to compute in a distributed environment.
WhereCirepresents the set of points in ith cluster, andLjdenotes the set of points in the ground-truth classj.Whereas themax(|Ci‚à©Lj|)represents the largest number of points in clusteribelonging to a single ground-truth classLj.
The computational time complexity of hybrid GM to identifying the clustering results isO(t*KiniD2).
Where t is the number of iterations, n is the number of data points, K is the number of components, and D is the dimensionality of the data.
It can be observed that the dimensionality dominates the computation particularly for high-dimensional data sets.
In E-step, computing the posterior probabilityŒ≥(ùíµnk)requiresO(KiniD)operations.
In M-step, computing the parametersœÄk,ŒºkandŒ£kinvolveO(Kini)operations.
The estimation of D-dimensional mean vector of component k andŒ£kvariance matrix of component k required additional operations.
Therefore, the overall time complexity is a combination of the complexities of both the Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM) and the Group Counseling Optimization (GCO) algorithm.
In this research, our primary objective is to identify fake news on social media platforms based on unsupervised method, specifically utilizing a Gaussian Mixture Model (GMM).
While it is well-known that the Expectation-Maximization (EM) algorithm, commonly used for fitting GMMs, is highly sensitive to initial values and requires the number of components to be specified a priori [128].
To address these challenges, we introduce a novel hybrid Gaussian Mixture model for detecting fake news in a fully unsupervised manner.
Unlike previous works that separately address the issues of initialization [129] and the estimation of the number of components [130,131] we tackle these problems simultaneously.
The proposed approach also introduced a novel objective function for the GMM based on mixture distributions, and then leveraged a meta-heuristic optimization algorithm, Group Counseling Optimization (GCO) to find the best set of parameters to fit the Gaussian Mixture Model on news data.
It iteratively adjusts the GMM parameters, ensuring that the model is both robust to initial values and capable of automatically determining the optimal number of clusters.
This method significantly enhances the performance and reliability of the EM algorithm, thus, addressing the limitations of traditional GMM particularly sensitivity toward initial parameters values and number of cluster estimations, leading to more stable and optimal clustering.
Finally, we also demonstrated the effectiveness of our innovative approach by applying it on a real-world news dataset.
In the initial step of the proposed Hybrid GMM, we initialized all the parameters of bounds vector as follows: the number of components (n_components) was adjusted within a range of 2‚Äì10, allowing the algorithm to identify the optimal number of clusters.
This range was selected because a minimum of two components is necessary for meaningful clustering, and ten components represent a practical upper limit for this dataset.
As there is a trade-off in the selection of an appropriate number of clusters.
A higher number of clusters can offer a more detailed and fine-grained representation of the data, potentially enhancing clustering accuracy.
However, a large number of clusters also leads to greater computational complexity and adds to the workload of post-processing [132].
The regularization on covariance (reg_covar) was adjusted between 10‚àí6and 10‚àí2ensuring that the covariance matrices remained stable and invertible while preventing numerical issues.
This range provided a balance between too little and too much regularization, both of which could negatively impact the GMM‚Äôs performance.
Furthermore, the maximum number of iterations (max_iter) was set between 100 and 500, providing the Expectation-Maximization (EM) algorithm with ample opportunity to converge while minimizing unnecessary computational overhead.
The tolerance parameter (tol), ranging from 10‚àí4and 10‚àí2, defined the convergence threshold for the EM algorithm, balancing the need for precision with the desire for computational efficiency.
These carefully selected bounds allowed the GCO algorithm to effectively search for the optimal GMM parameters, leading to enhanced clustering performance and demonstrating the robustness of the approach in finding an optimal solution.
Following the optimization process, the GCO algorithm identified the optimal parameters for hybrid GMM, resulting in a model comprising of 3 components, with a regularization on covariance of 0.00617, a maximum of 224 iterations, and a tolerance of 0.00632.
These optimized parameters were subsequently utilized to fit the hybrid GMM, superseding the initial values set within the bounds vector.
Subsequently, the proposed hybrid GMM, configured with the identified 3 mixture components and the aforementioned optimized parameters is employed to cluster the two-dimensional unlabeled data, where each observation corresponds to a news statement.
Upon fitting the hybrid GMM, the observations belonging to the same mixture component were considered as in the same cluster, facilitating the identification of underlying patterns in the news data.Fig 4illustrated the clustering results produced by hybrid GMM over scaled data obtained from the preprocessing steps.
It demonstrated that the proposed hybrid GMM partitioned the news data into specified number of clusters based on the mixture components as identified by the GCO optimizer and discussed above.
A Superficial comparative analysis ofFigs 2and4might demonstrated that hybrid GMM primarily provides cluster delineation to the PCA representation, however, the integration of GMM with PCA offers substantial advantages that extend beyond mere visualization.
It enables the modeling of data as a mixture of multiple Gaussian distributions, facilitating the identification of underlying data structures and probabilistic cluster assignments.
Nevertheless, the clustering results yielded by the hybrid Gaussian Mixture Model (GMM) are unlabeled meaning that while the data points are grouped into distinct clusters based on their features, the specific nature or identity of these clusters is not immediately discernible [64], as depicted inFig 4.
Thus, it does not provide any explicit indication of which cluster corresponds to which type of news.
Furthermore, without labels, we are unable to ascertain the characteristics or thematic content of the clusters, nor can we directly identify the nature of the news articles within each cluster.
Labeling these clusters would be a crucial step toward understanding the underlying structure of the data [133], as it would allow us to interpret the clusters in a meaningful way, such as identifying whether a cluster predominantly contains real or fake news.
Thus, the process of labeling or annotating these clusters is essential for translating the unsupervised clustering results into actionable insights and for making informed decisions based on the classification of the news articles.
To address this challenge and to enhance the interpretability of the clusters generated by the hybrid GMM, a method leveraging the majority heuristic to label the clusters based on the ground truth was employed.
This approach is pivotal in aligning the initially unlabeled clusters with the predefined categories within the dataset, thereby ensuring that the clustering outcomes are not only meaningful but also actionable.
Thus, the analysis of the ground truth labels associated with each data point, uniquely identified by an individual identifier, is conducted within each cluster.
This process involves systematically evaluating the distribution of ground truth labels among the data points in each cluster.
By determining the most prevalent or dominant ground truth label within a cluster, that label is designated as the representative identity for the entire cluster.
For example, if the majority of data points within a cluster were labeled as ‚ÄúTrue,‚Äù this label is assigned to the entire cluster, thus defining its identity.
The supplementary file presents a subset of data points (news) randomly sampled from each cluster, providing a representative overview of the clustering results.
According to supplementary file, cluster 1, as demonstrated in the given Table, predominantly includes news statements labeled as ‚ÄúMostly True‚Äù.
In contrast, the cluster 2 is characterized by a majority of news items labeled as either ‚ÄúFalse‚Äù or ‚ÄúBarely True.‚Äù Furthermore, the cluster 3 is predominantly composed of news items labeled as ‚ÄúPants on Fire,‚Äù a designation reserved for statements that are not only false but are also egregiously incorrect or exaggerated.
The consistent presence of these labels within a particular cluster highlights the effectiveness of the hybrid GMM algorithm in grouping news items that share a similar information.
A more thorough examination of the clustering results revealed that the proposed hybrid Gaussian Mixture Model (GMM) effectively consolidated the six subcategories of the news dataset into three primary categories.
As previously discussed, Cluster 1 (green colored) primarily comprises news items labeled as ‚ÄúMostly True,‚Äù this cluster can be labeled as ‚ÄúMostly Accurate‚Äù cluster.
On the other hand, Cluster 2 (purple colored) is characterized by a majority of news items labeled as either ‚ÄúFalse‚Äù or ‚ÄúBarely True.‚Äù Consequently, this cluster can be categorized under the ‚ÄúPartially Accurate‚Äù label, reflecting the presence of news items that, while containing some elements of truth, are either misleading or significantly distorted.
Additionally, Cluster 3 (yellow colored), is predominantly composed of news items labeled as ‚ÄúPants on Fire‚Äù.
As a result, this cluster can be labeled as ‚ÄúInaccurate‚Äù.
Furthermore, it also simplifies the data, facilitating clearer distinctions between authentic and misleading information, which is crucial for the detection and analysis of fake information.
These categorizations illustrate the capability of the clustering algorithm to effectively distinguish among varying levels of truthfulness or misinformation in the news items and accurately reflecting the underlying structure of the dataset.
Furthermore, by aligning clusters with these major categories, the analysis reinforces the robustness and reliability of the clustering approach, providing a clear understanding of the distribution of authentic and misleading information across the dataset.
This structured categorization also enhances the potential for practical applications, such as misinformation detection and content validation, by enabling targeted analysis and intervention based on the identified cluster identities.
We also conducted a comprehensive set of evaluation to compare the performance of our proposed approach against traditional clustering algorithms, including Gaussian Mixture Model (GMM), K-means and K-medoids using the same news dataset.
This comparative analysis was essential to evaluate the effectiveness and robustness of our method in contrast to classical clustering techniques.
During the evaluation each algorithm was meticulously applied under standardized conditions.
The Bayesian Information Criterion (BIC) was utilized to ascertain the optimal number of components for traditional GMM [134].
For K-means and K-medoids clustering techniques, the Elbow method was employed to determine the optimal number of clusters, providing a visual assessment of the elbow point where adding more clusters fails to significantly improve the model [135].
The elbow point serves as a visual indicator for selecting the optimal number of clusters by analyzing the Within-Cluster Sum of Squares (WCSS) [133,134].
When applied to the news dataset, the Elbow method identified three clusters for K-means and five clusters for K-medoids, as shown inFig 5.
For K-means, it can be observed that increasing the number of clusters up to three as depicted inFig 5(A)resulted in a significant reduction in WCSS (Elbow point), beyond which the improvement becomes marginal.
At this point, the trade-off between reducing WCSS and avoiding the unnecessary complexity of adding more clusters is balanced, ensuring that the selected model is efficient and well-fitted to the data and avoiding over-fitting.
Similarly, for K-medoids, adding up to five clusters led to a substantial decrease in WCSS, after which further increases did not lead to noticeable improvement as depicted inFig 5(B).
Whereas BIC also suggested five clusters (components) for the classical GMM as can be seen inFig 6.
It also demonstrated that lower BIC scores indicating better model fit.
The asterisk (*) highlights the configuration with the lowest BIC score, representing the optimal number of clusters and covariance structure for the GMM on the given news dataset.
This divergence in the optimal number of clusters across the algorithms highlights the sensitivity of each clustering technique to the underlying data.
The justification related to the selection of these parameter selection techniques is detailed in the Methodology section.
Furthermore in the context of fake news detection,Figs 7and8illustrates the clustering results obtained through traditional GMM and K-medoid clustering respectively, on the same dataset.
These visual representations suggest a notable contrast in the distribution of data points between fake and true labels.
Both traditional GMM and K-medoid identified five clusters, closely aligning with the five subcategories present in the news dataset.
On the other hand, K-means discovered only three clusters as depicted inFig 9.
The evaluation results demonstrated that K-means algorithm tends to generalize the data, potentially merging subcategories that share similar characteristics, unlike GMM, which assigns data points based on probability distributions.
Furthermore, K-means algorithm minimize intra-cluster variance and effectively consolidates subcategories with shared characteristics into unified clusters.
This integration is particularly beneficial as it reduces the complexity of the dataset and highlights broader patterns across related subcategories that exhibit surface-level similarities.
It also provide a more comprehensive analysis of general trends and themes within the data.
Such clustering can provide valuable insights into the structural relationships between different types of news content, enhancing the overall understanding of the dataset without compromising interpretability.
Therefore, to provide insight into the internal structure of clusters and their alignment with ground truth, we employed three performance measures including the Silhouette Score, Adjusted Rand Index (ARI), and Purity Score.
Furthermore, the performance of these varied techniques in selecting the most suitable cluster count can be assessed through their respective Silhouette Score, Adjusted Rand Index (ARI), and Purity Score.Table 1demonstrated a comparative analysis of the performance for each clustering algorithm.
The evaluation results demonstrated the superiority of our proposed Hybrid GMM, achieving a silhouette score of 0.77, which demonstrated more compact and well-separated clustering results, leading to better-defined cluster boundaries.
The K-means algorithm also performed relatively well, with a silhouette score of 0.65, indicating that while its clusters are reasonably distinct, they are less cohesive than those produced by the Hybrid GMM.
The Classical GMM yielded a silhouette score of 0.61, showing moderate cluste r separation and cohesion.
Although it performed better than K-medoids, its lower score compared to the Hybrid GMM reflects its inability to define well-separated clusters.
On the other hand, the K-medoid algorithm demonstrated the lowest performance, with a silhouette score of 0.52.
This low score suggests that the clusters produced by K-medoid are less distinct and more prone to overlap, potentially leading to ambiguous clustering assignments.
It can also be inferred that K-medoid may not be as effective in capturing the complex structure of the data.
Furthermore, comparative analysis of evaluation results also depicted that hyprid GMM depicted an adjusted rand score of 0.83 and a purity score of 0.88.
The purity score of hybrid GMM demonstrated a strong ability to form clusters with or homogeneous data points.
On the other hand, K-means clustering algorithm yields 0.72 and 0.79 an adjusted rand score and purity score respectively.
Classical GMM and K- medoid clustering slightly performed poorly with an adjusted rand score and purity score of 0.68, of 0.75, 0.62 and 0.73 respectively.
In the context of clustering Fake News, the emphasis is often on achieving high Purity to ensure that most articles within a cluster are from the same category.
Hybrid GMM, with the superior silhouette, adjusted rand score and purity appears to be the more effective model for this specific task.
The present findings highlight substantial differences in clustering performance as measured by ARI on news data among the clustering techniques incorporated in this research.
Therefore, based on the evaluation measures and the specific focus on clustering fake news, the proposed hybrid GMM emerges as significantly promising model for this analysis.
Its promising performance across all the performance measures particularly purity score highlighted the Hybrid GMM‚Äôs ability to effectively handle complex data, providing more reliable and well-defined clustering outcomes compared to classical GMM, k-means and k-medoid cluster methods incorporated in this research.
Furthermore, the proposed method is also aligned with real-world practical applications, especially in the realm of social media monitoring and content regulation.
While the results are encouraging, the proposed technique has several limitation.
such as it does not incorporate community detection algorithms, which could aid in addressing the spread of misinformation and enable network-wide countermeasures.
Moreover, the inclusion of hybrid models utilizing deep learning techniques and transformer-based architectures could improve detection precision by capturing intricate contextual subtleties within the text.
This research proposed a hybrid Gaussian Mixture Model (GMM) for fake news detection.
By leveraging Group Counseling Optimization (GCO), a meta-heuristic optimization algorithm, the proposed approach ensured robust parameter optimization for hybrid GMM and automatically determining the optimal number of clusters, significantly enhancing the reliability and performance of the Expectation-Maximization (EM) algorithm.
The evaluation results also demonstrated that the proposed hybrid GMM outperformed traditional clustering methods, including classical GMM, K-means, and K-medoids.
It can be concluded that the Hybrid GMM is a robust and promising solution for fake news detection, with practical potential for combating misinformation on social media platforms.