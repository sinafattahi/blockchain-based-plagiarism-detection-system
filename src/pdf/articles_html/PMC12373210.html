
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            TomoGRAF: An X-ray physics-driven generative radiance field framework for extremely sparse view CT reconstruction - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4A5778AF2305305A5770009379A23.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373210/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="TomoGRAF: An X-ray physics-driven generative radiance field framework for extremely sparse view CT reconstruction">
<meta name="citation_author" content="Di Xu">
<meta name="citation_author_institution" content="Radiation Oncology, University of California, San Francisco, California, United States of America">
<meta name="citation_author" content="Yang Yang">
<meta name="citation_author_institution" content="Radiology, University of California, San Francisco, California, United States of America">
<meta name="citation_author" content="Hengjie Liu">
<meta name="citation_author_institution" content="Radiation Oncology, University of California, Los Angeles, California, United States of America">
<meta name="citation_author" content="Qihui Lyu">
<meta name="citation_author_institution" content="Radiation Oncology, University of California, San Francisco, California, United States of America">
<meta name="citation_author" content="Martina Descovich">
<meta name="citation_author_institution" content="Radiation Oncology, University of California, San Francisco, California, United States of America">
<meta name="citation_author" content="Dan Ruan">
<meta name="citation_author_institution" content="Radiation Oncology, University of California, Los Angeles, California, United States of America">
<meta name="citation_author" content="Ke Sheng">
<meta name="citation_author_institution" content="Radiation Oncology, University of California, San Francisco, California, United States of America">
<meta name="citation_publication_date" content="2025 Aug 22">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0330463">
<meta name="citation_doi" content="10.1371/journal.pone.0330463">
<meta name="citation_pmid" content="40845061">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373210/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373210/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373210/pdf/pone.0330463.pdf">
<meta name="description" content="Computed tomography (CT) provides high spatial-resolution visualization of 3D structures for various applications. Traditional analytical/iterative CT reconstruction algorithms require hundreds of angular samplings, a condition may not be met ...">
<meta name="og:title" content="TomoGRAF: An X-ray physics-driven generative radiance field framework for extremely sparse view CT reconstruction">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Computed tomography (CT) provides high spatial-resolution visualization of 3D structures for various applications. Traditional analytical/iterative CT reconstruction algorithms require hundreds of angular samplings, a condition may not be met ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373210/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12373210">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0330463"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0330463.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373210%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12373210/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12373210/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373210/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0330463" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 22;20(8):e0330463. doi: <a href="https://doi.org/10.1371/journal.pone.0330463" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330463</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>TomoGRAF: An X-ray physics-driven generative radiance field framework for extremely sparse view CT reconstruction</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Xu%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Di Xu</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Di Xu</span></h3>
<div class="p">
<sup>1</sup>Radiation Oncology, University of California, San Francisco, California, United States of America</div>
<div>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Validation, Visualization, Writing – original draft</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Xu%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">Di Xu</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Yang%20Y%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Yang Yang</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Yang Yang</span></h3>
<div class="p">
<sup>2</sup>Radiology, University of California, San Francisco, California, United States of America</div>
<div>Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Yang%20Y%22%5BAuthor%5D" class="usa-link"><span class="name western">Yang Yang</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20H%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Hengjie Liu</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Hengjie Liu</span></h3>
<div class="p">
<sup>3</sup>Radiation Oncology, University of California, Los Angeles, California, United States of America</div>
<div>Resources, Writing – original draft, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20H%22%5BAuthor%5D" class="usa-link"><span class="name western">Hengjie Liu</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Lyu%20Q%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Qihui Lyu</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Qihui Lyu</span></h3>
<div class="p">
<sup>1</sup>Radiation Oncology, University of California, San Francisco, California, United States of America</div>
<div>Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Lyu%20Q%22%5BAuthor%5D" class="usa-link"><span class="name western">Qihui Lyu</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Descovich%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Martina Descovich</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Martina Descovich</span></h3>
<div class="p">
<sup>1</sup>Radiation Oncology, University of California, San Francisco, California, United States of America</div>
<div>Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Descovich%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Martina Descovich</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ruan%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Dan Ruan</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Dan Ruan</span></h3>
<div class="p">
<sup>3</sup>Radiation Oncology, University of California, Los Angeles, California, United States of America</div>
<div>Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ruan%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">Dan Ruan</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sheng%20K%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Ke Sheng</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Ke Sheng</span></h3>
<div class="p">
<sup>1</sup>Radiation Oncology, University of California, San Francisco, California, United States of America</div>
<div>Conceptualization, Funding acquisition, Project administration, Resources, Supervision, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sheng%20K%22%5BAuthor%5D" class="usa-link"><span class="name western">Ke Sheng</span></a>
</div>
</div>
<sup>1,</sup><sup>*</sup>
</div>
<div class="cg p">Editor: <span class="name western">Zhentian Wang</span><sup>4</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>Radiation Oncology, University of California, San Francisco, California, United States of America</div>
<div id="aff002">
<sup>2</sup>Radiology, University of California, San Francisco, California, United States of America</div>
<div id="aff003">
<sup>3</sup>Radiation Oncology, University of California, Los Angeles, California, United States of America</div>
<div id="edit1">
<sup>4</sup>Tsinghua University, CHINA</div>
<div class="author-notes p">
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>NO authors have competing interests.</p></div>
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>ke.sheng@ucsf.edu</span></p>
</div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Di Xu</span></strong>: <span class="role">Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Validation, Visualization, Writing – original draft</span>
</div>
<div>
<strong class="contrib"><span class="name western">Yang Yang</span></strong>: <span class="role">Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Hengjie Liu</span></strong>: <span class="role">Resources, Writing – original draft, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Qihui Lyu</span></strong>: <span class="role">Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Martina Descovich</span></strong>: <span class="role">Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Dan Ruan</span></strong>: <span class="role">Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Ke Sheng</span></strong>: <span class="role">Conceptualization, Funding acquisition, Project administration, Resources, Supervision, Writing – review &amp; editing</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Zhentian Wang</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Apr 11; Accepted 2025 Jul 31; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Xu et al</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12373210  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40845061/" class="usa-link">40845061</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<section id="sec001"><h3 class="pmc_sec_title">Objectives</h3>
<p>Computed tomography (CT) provides high spatial-resolution visualization of 3D structures for various applications. Traditional analytical/iterative CT reconstruction algorithms require hundreds of angular samplings, a condition may not be met practically for physical and mechanical limitations. Sparse view CT reconstruction has been proposed using constrained optimization and machine learning methods with varying success, less so for ultra-sparse view reconstruction. Neural radiance field (NeRF) is a powerful tool for reconstructing and rendering 3D natural scenes from sparse views, but its direct application to 3D medical image reconstruction has been minimally successful due to the differences in photon transportation and available prior information between optic and X-ray.</p></section><section id="sec002"><h3 class="pmc_sec_title">Methods</h3>
<p>We develop TomoGRAF to reconstruct high-quality 3D CT volumes using ultra-sparse projections. TomoGRAF has two main novelties pertinent to X-ray physics and CT imaging. First, TomoGRAF’s volume rendering module accumulates x-ray material attenuation passing through an object with CT geometry rather than visible light material color and opacity from surface interaction in NeRF. Second, TomoGRAF penalizes the difference between the simulated and ground truth volume during training besides the 2D views, thus significantly improving the prior fidelity.</p></section><section id="sec003"><h3 class="pmc_sec_title">Results</h3>
<p>TomoGRAF is trained on LIDC-IDRI dataset (1011 scans) and evaluated on an unseen in-house dataset (100 scans) of distinct imaging characteristics from training and demonstrates a vast leap in performance compared with state-of-the-art deep learning and NeRF methods.</p></section><section id="sec004"><h3 class="pmc_sec_title">Conclusion</h3>
<p>TomoGRAF provides the first generalizable solution for image-guided radiotherapy and interventional radiology applications, where only one/a few X-ray views are available, but 3D volumetric information is desired.</p></section></section><section id="sec005"><h2 class="pmc_sec_title">1. Introduction</h2>
<p>Computed tomography (CT) acquires x-ray projections around the subject to generate 3D cross-sectional images. Compared to 2D radiographs, where the depth information along the ray direction is lost and structures superimposed, CT enables the 3D representation of rich internal information for quantitative structure characterization. Analytically, Tuy’s data sufficiency condition covering a sufficient sampling trajectory is required for mathematical rigid reconstruction [<a href="#pone.0330463.ref001" class="usa-link" aria-describedby="pone.0330463.ref001">1</a>]. Violating Tuy’s condition leads to geometry and intensity distortion in the reconstructed images (referred to as limited-angle artifacts in the following). Besides the sampling trajectory, a minimal sampling density is required to avoid streak artifacts that can severely corrupt the image with sparse, e.g., &lt; 100 views.</p>
<p>On the other hand, data-sufficient conditions may not always be met due to practical limitations, including imaging dose considerations, limited gantry freedom, and the need for continuous image guidance for radiotherapy and interventional radiology [<a href="#pone.0330463.ref002" class="usa-link" aria-describedby="pone.0330463.ref002">2</a>,<a href="#pone.0330463.ref003" class="usa-link" aria-describedby="pone.0330463.ref003">3</a>]. For instance, the total ionizing radiation exposure in mammograms is kept low to protect the sensitive tissue [<a href="#pone.0330463.ref004" class="usa-link" aria-describedby="pone.0330463.ref004">4</a>]. However, 2D mammograms without depth differentiation can be inadequate with dense breast tissues. Digital breast tomosynthesis (DBT), a limited-angle tomographic breast imaging technique, was introduced to overcome the problem of tissue superposition in 2D mammography while maintaining a low dose level. In DBT, limited projection views are acquired while the X-ray source traverses along a predefined trajectory, typically an arc spanning an angular range of 60° or less. The acquired limited angle samplings are then reconstructed as the volumetric representation with improved depth differentiation but still inferior quality to CT [<a href="#pone.0330463.ref005" class="usa-link" aria-describedby="pone.0330463.ref005">5</a>,<a href="#pone.0330463.ref006" class="usa-link" aria-describedby="pone.0330463.ref006">6</a>]. In different applications, the acquisition angles are not restricted, but the density of projection is reduced to lower the imaging dose [<a href="#pone.0330463.ref007" class="usa-link" aria-describedby="pone.0330463.ref007">7</a>,<a href="#pone.0330463.ref008" class="usa-link" aria-describedby="pone.0330463.ref008">8</a>] or to capture the dynamic information in retrospectively sorted 4DCT [<a href="#pone.0330463.ref009" class="usa-link" aria-describedby="pone.0330463.ref009">9</a>,<a href="#pone.0330463.ref010" class="usa-link" aria-describedby="pone.0330463.ref010">10</a>], resulting in sparse views in each sorting bin.</p>
<p>Image reconstruction from sparse-view and limited-angle samplings is an ill-posed inverse problem. Due to insufficient projection angles, the conventional filtered back-projection (FBP) [<a href="#pone.0330463.ref011" class="usa-link" aria-describedby="pone.0330463.ref011">11</a>] algorithm, algebraic reconstruction technique (ART) [<a href="#pone.0330463.ref012" class="usa-link" aria-describedby="pone.0330463.ref012">12</a>], and simultaneous algebraic reconstruction technique (SART) [<a href="#pone.0330463.ref013" class="usa-link" aria-describedby="pone.0330463.ref013">13</a>] suffer from limited-angle artifacts that worsen with sparser projections. Over the past few decades, substantial effort has been made to advance the development of sparse-view CT from two general avenues. One line of research lies in developing regularized iterative methods based on the compressed sensing (CS) [<a href="#pone.0330463.ref014" class="usa-link" aria-describedby="pone.0330463.ref014">14</a>] theory. For instance, Sidky et al. proposed the adaptive steepest descent projection onto convex sets (ASD-POCS) method by minimizing the total variation (TV) of the expected CT volume from sparsely sampled views [<a href="#pone.0330463.ref015" class="usa-link" aria-describedby="pone.0330463.ref015">15</a>]. Following that, the adaptive-weighted TV (awTV) model was introduced by Liu et al. for improved edge preservation with local information constraints [<a href="#pone.0330463.ref016" class="usa-link" aria-describedby="pone.0330463.ref016">16</a>], while an improved TV-based algorithm named TV-stokes-projection onto Convex sets (TVS-POCS) was proposed immediately after to eliminate the patchy artifacts and preserving more subtle structures [<a href="#pone.0330463.ref017" class="usa-link" aria-describedby="pone.0330463.ref017">17</a>]. Apart from the TV-based methods, the prior image-constrained CS (PICCS) [<a href="#pone.0330463.ref018" class="usa-link" aria-describedby="pone.0330463.ref018">18</a>], patch-based nonlocal means (NLM) [<a href="#pone.0330463.ref019" class="usa-link" aria-describedby="pone.0330463.ref019">19</a>], tight wavelet frames [<a href="#pone.0330463.ref020" class="usa-link" aria-describedby="pone.0330463.ref020">20</a>], and feature dictionary learning [<a href="#pone.0330463.ref021" class="usa-link" aria-describedby="pone.0330463.ref021">21</a>] algorithms were introduced to further improve the reconstruction performance in representing patch-wise structure features. More recently, deep learning (DL) techniques were explored for improved CT reconstruction quality in the image or sinogram domain. The image domain methods learn the mapping from the noisy sparse-view reconstructed CT to the corresponding high-quality CT using diverse network structures such as feed-forward network [<a href="#pone.0330463.ref022" class="usa-link" aria-describedby="pone.0330463.ref022">22</a>,<a href="#pone.0330463.ref023" class="usa-link" aria-describedby="pone.0330463.ref023">23</a>], U-Net [<a href="#pone.0330463.ref024" class="usa-link" aria-describedby="pone.0330463.ref024">24</a>], and ResNet [<a href="#pone.0330463.ref025" class="usa-link" aria-describedby="pone.0330463.ref025">25</a>]. The sinogram domain methods work on improving/mapping the FBP algorithm [<a href="#pone.0330463.ref026" class="usa-link" aria-describedby="pone.0330463.ref026">26</a>–<a href="#pone.0330463.ref029" class="usa-link" aria-describedby="pone.0330463.ref029">29</a>] or interpolating the missing information in the sparse-sampled sinograms [<a href="#pone.0330463.ref030" class="usa-link" aria-describedby="pone.0330463.ref030">30</a>–<a href="#pone.0330463.ref033" class="usa-link" aria-describedby="pone.0330463.ref033">33</a>] with DL techniques. These and other deep learning-based sparse view CT reconstruction studies are comprehensively reviewed in Podgorsak et al. and Sun et al. [<a href="#pone.0330463.ref034" class="usa-link" aria-describedby="pone.0330463.ref034">34</a>,<a href="#pone.0330463.ref035" class="usa-link" aria-describedby="pone.0330463.ref035">35</a>].</p>
<p>Yet, with the exception where the same patient’s different CT was used as the prior [<a href="#pone.0330463.ref036" class="usa-link" aria-describedby="pone.0330463.ref036">36</a>], little progress was made in reconstructing high-quality tomographic imaging using less than ten projections, a practical problem in real-time radiotherapy or interventional procedures. For the former, an onboard X-ray imager orthogonal to the mega-voltage (MV) therapeutic X-ray provides the most common modality of image-guided radiotherapy (IGRT). However, a trade-off must be made between slow 3D cone beam CT (CBCT) and fast 2D X-rays [<a href="#pone.0330463.ref037" class="usa-link" aria-describedby="pone.0330463.ref037">37</a>,<a href="#pone.0330463.ref038" class="usa-link" aria-describedby="pone.0330463.ref038">38</a>]. A similar trade-off exists in interventional radiology [<a href="#pone.0330463.ref039" class="usa-link" aria-describedby="pone.0330463.ref039">39</a>]. When real-time interventional decisions need to be made in the time frame affording one to two 2D X-rays, yet 3D visualization of the anatomy is desired, a unique class of ultra-sparse view CT reconstruction problems combining extremely limited projection angles <em>and</em> sparsity is created.</p>
<p>With the advancement of deep learning and more powerful computation hardware, several recent studies proposed harnessing inversion priors through training data-driven networks for single/dual-view(s) image reconstruction. Specifically, Shen et al. designed a three-stage convolutional neural network (CNN) trained on patient-specific 4DCT to infer CT of a different respiratory phase using a single or two orthogonal view(s) [<a href="#pone.0330463.ref040" class="usa-link" aria-describedby="pone.0330463.ref040">40</a>]. Ying et al. built a generative adversarial network framework (X2CT-GAN) with a 2D to 3D CNN generator to predict tomographic volume from two orthogonal projections [<a href="#pone.0330463.ref041" class="usa-link" aria-describedby="pone.0330463.ref041">41</a>]. Though promising, their generalization and robustness to external datasets have not been demonstrated and may be fundamentally limited by two factors: First, they generate volumetric predictions purely from 2D manifold learning. As a result, these networks are incapable of comprehending the 3D world and the projection view formation process [<a href="#pone.0330463.ref042" class="usa-link" aria-describedby="pone.0330463.ref042">42</a>]. Second, a prerequisite for deep networks with complex enough parameters to implicitly represent 2D to 3D manifold mapping is large and diverse training data, a condition difficult to meet for medical imaging [<a href="#pone.0330463.ref043" class="usa-link" aria-describedby="pone.0330463.ref043">43</a>].</p>
<p>An effective perspective to mitigate those problems is to leverage intermediate voxel-based representation in combination with differentiable volume rendering for a 3D-aware model, which requires smaller data to generalize. The Neural Radiance Fields (NeRF) [<a href="#pone.0330463.ref044" class="usa-link" aria-describedby="pone.0330463.ref044">44</a>] model successfully implemented this principle for volumetric scene rendering. NeRF proposed synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. NeRF achieved this by representing a scene with a fully connected deep network with the input of 5D coordinates representing the spatial location, view direction, and the output of the volume density and view-dependent emitted radiance at the spatial location. The novel view was synthesized by querying 5D coordinates along the camera rays and using volume rendering techniques to project the object surface color and densities onto an image [<a href="#pone.0330463.ref044" class="usa-link" aria-describedby="pone.0330463.ref044">44</a>]. NeRF was designed to generate unseen views from the same object and typically required fixed camera positions as supervision. As an improvement, GRAF, a 3D-aware generative model 2D-supervised by unposed image patches, introduced a conditional radiance field generator trained within the Generative Adversarial Network (GAN) framework [<a href="#pone.0330463.ref045" class="usa-link" aria-describedby="pone.0330463.ref045">45</a>] that is capable of rendering views of novel objects from given sparse projection views [<a href="#pone.0330463.ref042" class="usa-link" aria-describedby="pone.0330463.ref042">42</a>].</p>
<p>The success of NeRF and GRAF motivated their applications to solve the 3D tomography problems. MedNeRF [<a href="#pone.0330463.ref046" class="usa-link" aria-describedby="pone.0330463.ref046">46</a>] was proposed by Corona-Figueroa et al. for novel view rendering from a few or single X-ray projections. MedNeRF inherits the general GRAF framework, remains 2D-supervised, and assumes visible-light photon transportation configuration in the generator with an addition of self-supervised loss to the discriminator. However, there are distinct differences between CT volume reconstruction and “natural object” 3D representation rendering in terms of the available choices of training supervision, imaging setup, and properties of the rays. Specifically, 3D training supervision (object mesh with information on surface color) is often hard to acquire for “natural objects.” In contrast, existing CT scans are an ideal volumetric training ground truth (GT) for fitting a 3D tomographic representation learning model. Moreover, optical raytracing works by computing a path from an imaginary camera (eye) through each pixel in a virtual screen and calculating the surface color and opacity of the object visible through the virtual screen via simulating ray reflection, shading, or refraction on the object surface (<a href="#pone.0330463.g001" class="usa-link">Fig 1(b)</a>). The solving target of optical ray tracing is the object surface color <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e001"><math id="M1" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><mi>r</mi><mo>,</mo><mi>g</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></math></span> and density <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e002"><math id="M2" display="inline" overflow="linebreak"><mrow><mi>σ</mi></mrow></math></span> in a 3D location <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e003"><math id="M3" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></math></span> Meanwhile, x-rays are transported from the focal spot and pass through an object to the detector plane, accounting for scattering and attenuation (<a href="#pone.0330463.g001" class="usa-link">Fig 1(c)</a>). The goal of CT reconstruction is voxel-wise material density <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e004"><math id="M4" display="inline" overflow="linebreak"><mrow><mi>δ</mi></mrow></math></span> at a 3D location <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e005"><math id="M5" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></math></span>. Because of these major differences between natural scenes and 3D medical images, the direct application of NeRF has not resulted in usable CT with ultra-sparse views.</p>
<figure class="fig xbox font-sm" id="pone.0330463.g001"><h3 class="obj_head">Fig 1. Architecture of the proposed TomoGRAF framework.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8873/12373210/92eb376a9156/pone.0330463.g001.jpg" loading="lazy" height="1609" width="800" alt="Fig 1"></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330463.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>a, The illustration of energy/wavelength difference between visible lights and X-rays. <strong>b,</strong> The visualization of object imaging with visible lights in 3D. <strong>c,</strong> The visualization of object imaging with X-ray in 3D. <strong>d,</strong> The diagram visualization of the TomoGRAF pipline in the training and testing stages. <strong>e,</strong> The visualization of the training and inference stage of TomoGRAF. TomoGRAF references multi-view projections and the corresponding CT volume during training using data collected from multiple institutes (inst.) and will only require sparse-view projections referencing at the inference stage to render the predicted CT volume from a new institute. <strong>f,</strong> The pipeline of the TomoGRAF network. TomoGRAF works on projection patches and CT sub-volume in training. The input to TomoGRAF consists of source setup matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e006"><math id="M6" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">K</mi></mrow></mrow></mrow></math></span>, view direction (pose) <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e007"><math id="M7" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></math></span> and 2D sampling pattern <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e008"><math id="M8" display="inline" overflow="linebreak"><mrow><mi>v</mi></mrow></math></span>. <strong>g,</strong> The ray sampling mechanism in a patch of the projection input to TomoGRAF. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e009"><math id="M9" display="inline" overflow="linebreak"><mrow><mi>u</mi></mrow></math></span> represents the center of the sampled patch, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e010"><math id="M10" display="inline" overflow="linebreak"><mrow><mi>s</mi></mrow></math></span> refers to the distance between two sampled patches. <strong>h,</strong> The design of conditional radiance field in TomoGRAF with a fully connected coordinate network <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e011"><math id="M11" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><msub><mrow><mi>g</mi></mrow><mrow><mi>ϑ</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math></span> which consists of shape encoding <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e012"><math id="M12" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>h</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> and density head <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e013"><math id="M13" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>d</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span>.</p></figcaption></figure><p>To overcome the challenges in ultra-sparse view CT reconstruction while maintaining the superior NeRF 3D structure representation efficiency, we introduced an x-ray-aware tomographic volume generator, termed TomoGRAF, to simulate CT imaging setup and use CT and its projections for 3D- and 2D-supervised training. TomoGRAF is further enhanced with a GAN framework and computationally scaled with sub-volume and image patch GTs training. To the best of our knowledge, this is the first pipeline that informs the NeRF simulator with X-ray physics to achieve generalizable high-performing CT volume reconstruction with ultra-sparse projection representation.</p></section><section id="sec006"><h2 class="pmc_sec_title">2. Materials and methods</h2>
<section id="sec007"><h3 class="pmc_sec_title">2.1. Problem formulation</h3>
<p>As illustrated in <a href="#pone.0330463.g001" class="usa-link">Fig 1(d)</a>, we formulated the problem of 3D image reconstruction from 2D projection(s) into a GAN-based DL framework, including modules of generator G and discriminator equation, given a series of 2D projections <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e014"><math id="M14" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">X</mi></mrow></mrow></mrow></math></span> denoted as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e015"><math id="M15" display="inline" overflow="linebreak"><mrow><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msub><mrow><mi>X</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>,</mo><msub><mrow><mi>X</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><msub><mrow><mi>X</mi></mrow><mrow><mi>N</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">}</mo></mrow></mrow></math></span> where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e016"><math id="M16" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>X</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>2</mn></mrow></msup><mo>=</mo><msup><mi>ℝ</mi><mrow><mi>H</mi><mi>×</mi><mi>W</mi></mrow></msup></mrow></math></span> for <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e017"><math id="M17" display="inline" overflow="linebreak"><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mi>N</mi><mo stretchy="false">]</mo></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e018"><math id="M18" display="inline" overflow="linebreak"><mrow><mi>N</mi></mrow></math></span> is the number of available 2D projections, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e019"><math id="M19" display="inline" overflow="linebreak"><mrow><mi>H</mi></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e020"><math id="M20" display="inline" overflow="linebreak"><mrow><mi>W</mi></mrow></math></span> is the projection height and width. Our modeling target was to form a <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e021"><math id="M21" display="inline" overflow="linebreak"><mrow><mi>G</mi></mrow></math></span> that can predict the 3D volume <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e022"><math id="M22" display="inline" overflow="linebreak"><mrow><mover><mrow><mi>Y</mi></mrow><mo stretchy="false">^</mo></mover><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>3</mn></mrow></msup><mo>=</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mi>×</mi><mi>H</mi><mi>×</mi><mi>W</mi></mrow></msup></mrow></math></span> (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e023"><math id="M23" display="inline" overflow="linebreak"><mrow><mi>D</mi></mrow></math></span> represents volume depth) where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e024"><math id="M24" display="inline" overflow="linebreak"><mrow><mi>G</mi></mrow></math></span> is supervised by <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e025"><math id="M25" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">X</mi></mrow></mrow></mrow></math></span> and 3D volume GT <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e026"><math id="M26" display="inline" overflow="linebreak"><mrow><mi>Y</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>3</mn></mrow></msup><mo>=</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mi>×</mi><mi>H</mi><mi>×</mi><mi>W</mi></mrow></msup></mrow></math></span>, and is penalized by <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e027"><math id="M27" display="inline" overflow="linebreak"><mrow><mi>D</mi></mrow></math></span> to encourage optimal convergence.</p></section><section id="sec008"><h3 class="pmc_sec_title">2.2. TomoGRAF framework</h3>
<p>TomoGRAF does not aim to optimize densely posed projections for rendering a single patient volume. Instead, it targets fitting a network for synthesizing new patient volume by learning on various unposed projections. Note that the generator and discriminator work on image patches and sub-volumes during training for better efficiency, whereas a complete patient volume is rendered at inference time. The detailed components in TomoGRAF are shown in <a href="#pone.0330463.g001" class="usa-link">Fig 1(f</a>–<a href="#pone.0330463.g001" class="usa-link">h</a>). In what follows, we elaborate on the model architecture.</p>
<section id="sec009"><h4 class="pmc_sec_title">2.2.1. Generator.</h4>
<p>Adapted from GRAF [<a href="#pone.0330463.ref042" class="usa-link" aria-describedby="pone.0330463.ref042">42</a>], Our generator consists of three main components: ray sampling, conditional generative radiance field, and projection rendering. Ray sampling modules render the x-ray paths in 3D that are associated with truth patch/sub-volume, and the conditional generative radiance field module predicts the material density from a 3D location along the rendered x-ray paths. Lastly, the projection rendering module obtains the 2D composition from the predicted volumetric material densities. Overall, the generator <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e028"><math id="M28" display="inline" overflow="linebreak"><mrow><mi>G</mi></mrow></math></span> takes x-ray source setup matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e029"><math id="M29" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">K</mi></mrow></mrow></mrow></math></span>, view direction (pose) <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e030"><math id="M30" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow><mo>=</mo><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>ϕ</mi><mo stretchy="false">)</mo></mrow></math></span>, 2D sampling pattern <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e031"><math id="M31" display="inline" overflow="linebreak"><mrow><mi>v</mi></mrow></math></span>, shape code <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e032"><math id="M32" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">s</mi></mrow></mrow></mrow></msub></mrow></msup></mrow></math></span> and appearance code <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e033"><math id="M33" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>a</mi></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">a</mi></mrow></mrow></mrow></msub></mrow></msup></mrow></math></span> as input, and predicts a size <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e034"><math id="M34" display="inline" overflow="linebreak"><mrow><mi>M</mi><mi>×</mi><mi>M</mi></mrow></math></span> CT projection patch <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e035"><math id="M35" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>P</mi></mrow><mrow><mi>′</mi></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>2</mn></mrow></msup><mo>=</mo><msup><mi>ℝ</mi><mrow><mi>M</mi><mi>×</mi><mi>M</mi></mrow></msup></mrow></math></span> (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e036"><math id="M36" display="inline" overflow="linebreak"><mrow><mi>M</mi></mrow></math></span> is a hyper-parameter defined by user; <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e037"><math id="M37" display="inline" overflow="linebreak"><mrow><mi>M</mi><mo>=</mo><mn>32</mn></mrow></math></span> is used in our experiments) and the associated CT sub-volume <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e038"><math id="M38" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>V</mi></mrow><mrow><mi>′</mi></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>3</mn></mrow></msup></mrow></math></span> corrsponding to <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e039"><math id="M39" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>P</mi></mrow><mrow><mi>′</mi></mrow></msup></mrow></math></span> at <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e040"><math id="M40" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></math></span> (In orthogonal viewing, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e041"><math id="M41" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>V</mi></mrow><mrow><mi>′</mi></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>3</mn></mrow></msup><msup><mrow><mo mathvariant="double-struck">=</mo><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>M</mi><mi>×</mi><mi>M</mi><mi>×</mi><mi>D</mi></mrow></msup></mrow></math></span>; In non-orthognal views, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e042"><math id="M42" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>V</mi></mrow><mrow><mi>′</mi></mrow></msup></mrow></math></span> has varied dimension and is essentially a collection of points where converging rays intersecting the 3D grid). <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e043"><math id="M43" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">K</mi></mrow></mrow></mrow></math></span> consists of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e044"><math id="M44" display="inline" overflow="linebreak"><mrow><mi>d</mi><mo>=</mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msub><mrow><mi>d</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>,</mo><msub><mrow><mi>d</mi></mrow><mrow><mn>2</mn></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><msub><mrow><mi>d</mi></mrow><mrow><mn>1</mn></mrow></msub></mrow></math></span> is distance of source to patient (SAD), <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e045"><math id="M45" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>d</mi></mrow><mrow><mn>2</mn></mrow></msub></mrow></math></span> is distance of source to detector (SID). The detector resolution <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e046"><math id="M46" display="inline" overflow="linebreak"><mrow><mi>S</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>2</mn></mrow></msup></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e047"><math id="M47" display="inline" overflow="linebreak"><mrow><mi>M</mi></mrow></math></span> is bounded by<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e048"><math id="M48" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow></math></span>.</p>
<p><strong>Ray Sampling</strong>: The rendered rays <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e049"><math id="M49" display="inline" overflow="linebreak"><mrow><mi>r</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>3</mn></mrow></msup></mrow></math></span> are constrained by <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e050"><math id="M50" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e051"><math id="M51" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">K</mi></mrow></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e052"><math id="M52" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>P</mi></mrow><mrow><mi>′</mi></mrow></msup><mo>.</mo></mrow></math></span> The x-ray pose <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e053"><math id="M53" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></math></span> is sampled from a predefined pose distribution <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e054"><math id="M54" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>p</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></msub></mrow></math></span> collected from projection angles in training data with the x-ray source facing towards the origin of the coordinate system all the time. As shown in <a href="#pone.0330463.g001" class="usa-link">Fig 1(g)</a>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e055"><math id="M55" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">v</mi></mrow></mrow><mo>=</mo><mo stretchy="false">(</mo><mrow><mrow><mi mathvariant="bold-italic">u</mi></mrow></mrow><mo>,</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></math></span> determines the center <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e056"><math id="M56" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">u</mi></mrow><mo>=</mo></mrow><mo stretchy="false">(</mo><msup><mrow><mi>x</mi></mrow><mrow><mi>′</mi></mrow></msup><mo>,</mo><mi>y</mi><mrow><mi>′</mi></mrow><mo stretchy="false">)</mo><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>2</mn></mrow></msup></mrow></math></span> and scales <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e057"><math id="M57" display="inline" overflow="linebreak"><mrow><mi>s</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mo>+</mo></mrow></msup></mrow></math></span> of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e058"><math id="M58" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>P</mi></mrow><mrow><mi>′</mi></mrow></msup></mrow></math></span>that we target to predict, where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e059"><math id="M59" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>V</mi></mrow><mrow><mi>′</mi></mrow></msup></mrow></math></span> is formed with all the corresponding 3D coordinates that form <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e060"><math id="M60" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>P</mi></mrow><mrow><mi>′</mi></mrow></msup><mo>.</mo></mrow></math></span> At the training stage, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e061"><math id="M61" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">u</mi></mrow></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e062"><math id="M62" display="inline" overflow="linebreak"><mrow><mi>s</mi></mrow></math></span> are uniformly drawn from <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e063"><math id="M63" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">u</mi></mrow></mrow><mi>~</mi><mi>U</mi><mo stretchy="false">(</mo><mi>Ω</mi><mo stretchy="false">)</mo></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e064"><math id="M64" display="inline" overflow="linebreak"><mrow><mi>s</mi><mi>~</mi><mi>U</mi><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mi>S</mi><mo stretchy="false">]</mo></mrow></math></span> where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e065"><math id="M65" display="inline" overflow="linebreak"><mrow><mi>Ω</mi></mrow></math></span> defined the 2D projection domain and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e066"><math id="M66" display="inline" overflow="linebreak"><mrow><mi>S</mi><mo>=</mo><mrow><mtext>min</mtext></mrow><mo stretchy="false">(</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo><mo>/</mo><mi>M</mi></mrow></math></span>. Noteworthily, the coordinates in <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e067"><math id="M67" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>P</mi></mrow><mrow><mi>′</mi></mrow></msup></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e068"><math id="M68" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>V</mi></mrow><mrow><mi>′</mi></mrow></msup></mrow></math></span> are real numbers for the purpose of continuous radiance field evaluation. Following the stratified sampling approach in Mildenhall et al. [<a href="#pone.0330463.ref044" class="usa-link" aria-describedby="pone.0330463.ref044">44</a>], <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e069"><math id="M69" display="inline" overflow="linebreak"><mrow><mi>Q</mi></mrow></math></span> number of points are sampled along each <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e070"><math id="M70" display="inline" overflow="linebreak"><mrow><mi>r</mi></mrow></math></span>. The number of rays <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e071"><math id="M71" display="inline" overflow="linebreak"><mrow><mi>R</mi><mo>=</mo><mi>M</mi><mi>×</mi><mi>M</mi></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e072"><math id="M72" display="inline" overflow="linebreak"><mrow><mi>R</mi><mo>=</mo><mi>H</mi><mi>×</mi><mi>W</mi></mrow></math></span> per training and inference, respectively.</p>
<p><strong>Conditional Generative Radiance Field</strong>: Adapted from GRAF [<a href="#pone.0330463.ref042" class="usa-link" aria-describedby="pone.0330463.ref042">42</a>], the radiance field is represented by a deep fully connected coordinate network <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e073"><math id="M73" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>g</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> with the input of the positional encoding of a 7D vector <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e074"><math id="M74" display="inline" overflow="linebreak"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo>,</mo><mi>θ</mi><mo>,</mo><mi>ϕ</mi><mo>,</mo><msub><mrow><mi>z</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub><mo>,</mo><msub><mrow><mi>z</mi></mrow><mrow><mi>a</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math></span> consisting of 3D location <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e075"><math id="M75" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">x</mi></mrow><mo>=</mo></mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></math></span> and projection pose <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e076"><math id="M76" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></math></span>. The output is the material density <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e077"><math id="M77" display="inline" overflow="linebreak"><mrow><mi>δ</mi></mrow></math></span> in the corresponding <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e078"><math id="M78" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">x</mi></mrow></mrow></mrow></math></span>, where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e079"><math id="M79" display="inline" overflow="linebreak"><mrow><mi>ϑ</mi></mrow></math></span> represents the network parameters and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e080"><math id="M80" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub><mi>~</mi><msub><mrow><mi>p</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e081"><math id="M81" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>a</mi></mrow></msub><mi>~</mi><msub><mrow><mi>p</mi></mrow><mrow><mi>a</mi></mrow></msub></mrow></math></span> with <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e082"><math id="M82" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>p</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e083"><math id="M83" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>p</mi></mrow><mrow><mi>a</mi></mrow></msub></mrow></math></span> drawn from standard Gaussian distribution.</p>
<table class="disp-formula p" id="pone.0330463.e084"><tr>
<td class="formula"><math id="M84" display="block" overflow="linebreak"><mrow><msub><mrow><mi>g</mi></mrow><mrow><mi>ϑ</mi></mrow></msub><mi>:</mi><msup><mi>ℝ</mi><mrow><msub><mi>ℒ</mi><mrow><mrow><mrow><mi mathvariant="bold-italic">x</mi></mrow></mrow></mrow></msub></mrow></msup><mi>×</mi><msup><mi>ℝ</mi><mrow><msub><mi>ℒ</mi><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></msub></mrow></msup><mi>×</mi><msup><mi>ℝ</mi><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">s</mi></mrow><mrow><mi mathvariant="bold-italic">h</mi></mrow></mrow></mrow></msub></mrow></msup><mi>×</mi><msup><mi>ℝ</mi><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">a</mi></mrow></mrow></mrow></msub></mrow></msup><mo>→</mo><msup><mi>ℝ</mi><mrow><mn>3</mn></mrow></msup><mi>×</mi><msup><mi>ℝ</mi><mrow><mo>+</mo></mrow></msup></mrow></math></td>
<td class="label">(1)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330463.e085"><tr>
<td class="formula"><math id="M85" display="block" overflow="linebreak"><mrow><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>γ</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mrow><mi mathvariant="bold-italic">x</mi></mrow></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><mi>γ</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><msub><mrow><mi>z</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub><mo>,</mo><msub><mrow><mi>z</mi></mrow><mrow><mi>a</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>→</mo><mi>δ</mi></mrow></math></td>
<td class="label">(2)</td>
</tr></table>
<p>Where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e086"><math id="M86" display="inline" overflow="linebreak"><mrow><msub><mi>ℒ</mi><mrow><mi>x</mi></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e087"><math id="M87" display="inline" overflow="linebreak"><mrow><msub><mi>ℒ</mi><mrow><mi>ξ</mi></mrow></msub></mrow></math></span> represent the latent codes of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e088"><math id="M88" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">x</mi></mrow></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e089"><math id="M89" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e090"><math id="M90" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">s</mi></mrow><mrow><mi mathvariant="bold-italic">h</mi></mrow></mrow></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e091"><math id="M91" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">a</mi></mrow></mrow></mrow></msub></mrow></math></span> define the shape and appearance codes with <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e092"><math id="M92" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">s</mi></mrow><mrow><mi mathvariant="bold-italic">h</mi></mrow></mrow></mrow></msub></mrow></msup></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e093"><math id="M93" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>a</mi></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">a</mi></mrow></mrow></mrow></msub></mrow></msup></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e094"><math id="M94" display="inline" overflow="linebreak"><mrow><mi>γ</mi><mo stretchy="false">(</mo><mi>·</mi><mo stretchy="false">)</mo></mrow></math></span> represents positional encoding.</p>
<p>The architecture of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e095"><math id="M95" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>g</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> is visualized in <a href="#pone.0330463.g001" class="usa-link">Fig 1(h)</a> with Equations (3–6). First, shape encoding <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e096"><math id="M96" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>h</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> is conducted with the input of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e097"><math id="M97" display="inline" overflow="linebreak"><mrow><mi>γ</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mrow><mi mathvariant="bold-italic">x</mi></mrow></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e098"><math id="M98" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub></mrow></math></span>. Second, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e099"><math id="M99" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>h</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> is concatenated with <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e100"><math id="M100" display="inline" overflow="linebreak"><mrow><mi>γ</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e101"><math id="M101" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>a</mi></mrow></msub></mrow></math></span> and then sent to the density head <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e102"><math id="M102" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>d</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> to predict <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e103"><math id="M103" display="inline" overflow="linebreak"><mrow><mi>δ</mi></mrow></math></span>.</p>
<table class="disp-formula p" id="pone.0330463.e104"><tr>
<td class="formula"><math id="M104" display="block" overflow="linebreak"><mrow><msub><mrow><mi>h</mi></mrow><mrow><mi>ϑ</mi></mrow></msub><mi>:</mi><msup><mi>ℝ</mi><mrow><msub><mi>ℒ</mi><mrow><mrow><mrow><mi mathvariant="bold-italic">x</mi></mrow></mrow></mrow></msub></mrow></msup><mi>×</mi><msup><mi>ℝ</mi><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">s</mi></mrow><mrow><mi mathvariant="bold-italic">h</mi></mrow></mrow></mrow></msub></mrow></msup><mo>→</mo><msup><mi>ℝ</mi><mrow><mi>H</mi></mrow></msup></mrow></math></td>
<td class="label">(3)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330463.e105"><tr>
<td class="formula"><math id="M105" display="block" overflow="linebreak"><mrow><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>γ</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mrow><mi mathvariant="bold-italic">x</mi></mrow></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><msub><mrow><mi>z</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>→</mo><mrow><mrow><mi mathvariant="bold-italic">h</mi></mrow></mrow></mrow></math></td>
<td class="label">(4)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330463.e106"><tr>
<td class="formula"><math id="M106" display="block" overflow="linebreak"><mrow><msub><mrow><mi>d</mi></mrow><mrow><mi>ϑ</mi></mrow></msub><mi>:</mi><msup><mrow><mi>R</mi></mrow><mrow><mi>H</mi></mrow></msup><mi>×</mi><msup><mi>ℝ</mi><mrow><msub><mi>ℒ</mi><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></msub></mrow></msup><mi>×</mi><msup><mi>ℝ</mi><mrow><msub><mrow><mi>M</mi></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">a</mi></mrow></mrow></mrow></msub></mrow></msup><mo>→</mo><msup><mi>ℝ</mi><mrow><mn>1</mn></mrow></msup><mtext>\ \ \ \ \ \ \ \ \ \ \ </mtext></mrow></math></td>
<td class="label">(5)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330463.e107"><tr>
<td class="formula"><math id="M107" display="block" overflow="linebreak"><mrow><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mrow><mi mathvariant="bold-italic">h</mi></mrow></mrow><mo>,</mo><mi>γ</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>,</mo><msub><mrow><mi>z</mi></mrow><mrow><mi>a</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>→</mo><mtext>\ </mtext><mi>δ</mi><mtext>\ \ \ \ \ \ \ </mtext></mrow></math></td>
<td class="label">(6)</td>
</tr></table>
<p>where the encoding was implemented with a fully connected network with ReLU activation.</p>
<p><strong>X-ray Physics Informed Projection Rendering</strong>: Lastly, given the material density <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e108"><math id="M108" display="inline" overflow="linebreak"><mrow><msubsup><mrow><mo stretchy="false">{</mo><mi>δ</mi></mrow><mrow><mi>i</mi></mrow><mrow><mi>r</mi></mrow></msubsup><mo stretchy="false">}</mo><mo>=</mo><mi>V</mi><mrow><mi>′</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>3</mn></mrow></msup></mrow></math></span> where 1<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e109"><math id="M109" display="inline" overflow="linebreak"><mrow><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>Q</mi></mrow></math></span> of all points along the rays <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e110"><math id="M110" display="inline" overflow="linebreak"><mrow><mo stretchy="false">{</mo><msub><mrow><mi>r</mi></mrow><mrow><mi>j</mi></mrow></msub><mo stretchy="false">}</mo></mrow></math></span> where 1<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e111"><math id="M111" display="inline" overflow="linebreak"><mrow><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>M</mi><mi>×</mi><mi>M</mi></mrow></math></span> in training, we used a CT projection algorithm, Siddon Ray Tracing algorithm [<a href="#pone.0330463.ref047" class="usa-link" aria-describedby="pone.0330463.ref047">47</a>], to synthetic 2D radiograph patch <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e112"><math id="M112" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>P</mi></mrow><mrow><mi>′</mi></mrow></msup></mrow></math></span>given preset <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e113"><math id="M113" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">K</mi></mrow></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e114"><math id="M114" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></math></span>. In specific, Siddon algirhtm computes the path lenghs of the ray <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e115"><math id="M115" display="inline" overflow="linebreak"><mrow><mi>r</mi></mrow></math></span> through each interescted voxel <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e116"><math id="M116" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">x</mi></mrow></mrow></mrow></math></span> within the 3D grid to enable efficient ray path (line) intergral over the grid. The pseudo code for the applied Siddon algorithm is listed in <a href="#pone.0330463.s001" class="usa-link">S1 Appendix</a>.</p></section><section id="sec010"><h4 class="pmc_sec_title">2.2.2. Discriminator.</h4>
<p>Following the discriminator architecture defined in MedNeRF [<a href="#pone.0330463.ref046" class="usa-link" aria-describedby="pone.0330463.ref046">46</a>], two self-supervised auto-encoded CNN discriminators, [<a href="#pone.0330463.ref048" class="usa-link" aria-describedby="pone.0330463.ref048">48</a>] <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e117"><math id="M117" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>1</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e118"><math id="M118" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>2</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> compare predicted sub-volume <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e119"><math id="M119" display="inline" overflow="linebreak"><mrow><mi>V</mi><mrow><mi>′</mi></mrow></mrow></math></span> to real sub-volume <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e120"><math id="M120" display="inline" overflow="linebreak"><mrow><mi>V</mi></mrow></math></span> extracted from real volume <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e121"><math id="M121" display="inline" overflow="linebreak"><mrow><mi>Y</mi></mrow></math></span> and predict projection patch <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e122"><math id="M122" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>P</mi></mrow><mrow><mi>′</mi></mrow></msup></mrow></math></span> to real projection patch <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e123"><math id="M123" display="inline" overflow="linebreak"><mrow><mi>P</mi></mrow></math></span> extracted from real projection <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e124"><math id="M124" display="inline" overflow="linebreak"><mrow><mi>I</mi></mrow></math></span>, respectively. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e125"><math id="M125" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>1</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> is defined to convolve in 3D while <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e126"><math id="M126" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>2</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> is defined to convolve in 2D to align with the dimension of its discrimination targets. For extracting <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e127"><math id="M127" display="inline" overflow="linebreak"><mrow><mi>V</mi></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e128"><math id="M128" display="inline" overflow="linebreak"><mrow><mi>P</mi></mrow></math></span>, we first extracted <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e129"><math id="M129" display="inline" overflow="linebreak"><mrow><mi>P</mi></mrow></math></span> from a real projection <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e130"><math id="M130" display="inline" overflow="linebreak"><mrow><mi>I</mi></mrow></math></span> given <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e131"><math id="M131" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">v</mi></mrow></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e132"><math id="M132" display="inline" overflow="linebreak"><mrow><mi>s</mi></mrow></math></span> randomly drawn from their corresponding distributions, and then located the coordinates of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e133"><math id="M133" display="inline" overflow="linebreak"><mrow><mi>V</mi></mrow></math></span> from <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e134"><math id="M134" display="inline" overflow="linebreak"><mrow><mi>Y</mi></mrow></math></span> based on <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e135"><math id="M135" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">ξ</mi></mrow></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e136"><math id="M136" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">K</mi></mrow></mrow></mrow></math></span>. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e137"><math id="M137" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>1</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e138"><math id="M138" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>2</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> are backpropagated separately with their respective weights, while we defined them to share weights while discriminating different sub-volume/patch locations.</p></section><section id="sec011"><h4 class="pmc_sec_title">2.2.3. Supervision.</h4>
<p>A distinctive supervision strategy is designed for the training and testing phases to better adapt to the imaging setup and available information in CT reconstructions.</p>
<p>During training, the model is guided using hybrid (2D and 3D) supervision. Specifically, multiple 2D subpatches <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e139"><math id="M139" display="inline" overflow="linebreak"><mrow><mi>P</mi></mrow></math></span> from different view angles and their paired subvolmes <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e140"><math id="M140" display="inline" overflow="linebreak"><mrow><mi>V</mi></mrow></math></span> are used as GTs to converge the prior. This approach enables the model to efficiently and effectively establish a trained prior that represents the universal features shared across the training cohort. The use of 3D supervision is particularly advantageous in CT imaging, as it enables the model to learn not only the external surface but also the complex internal anatomy of the object. Unlike visible light-based reconstruction methods, which are typically limited to surface information from 2D views and are incapable of exploiting the full 3D volume, TomoGRAF leverages the availability of 3D ground truth data to guide the reconstruction of deeper anatomical structures.</p>
<p>The inference stage can be further divided into two sub-steps, including patient-specific fine tuning and CT volume prediction. For patient-specific finetuning, the trained prior is further optimized to adapt to the incoming patient’s morphologies with supervision of the patient’s 2D sparse-view projection. At this stage, we use the complete 2D projections instead of 2D subpatches for supervision. This approach prioritizes strict maintenance of global structures and anatomical consistency during fine-tuning, albeit with a higher demand for GPU memory. Following the fine-tuning process, where the trained prior is tailored to the specific morphologies of the patient using their sparse-view 2D projections, the algorithm reconstructs the complete 3D CT volume. This reconstructed volume incorporates patient-specific anatomical details, ensuring that the final output accurately represents the unique structural characteristics of the individual.</p></section><section id="sec012"><h4 class="pmc_sec_title">2.2.4. Loss function.</h4>
<p><strong>Training stage</strong>: Our loss objective consists of discrimination towards patch as well as sub-volume predictions. First, the global structures in intermediate decoded patches of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e141"><math id="M141" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>1</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e142"><math id="M142" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>2</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> were separately assessed by Learned Perceptual Image Patch Similarity (LPIPS) [<a href="#pone.0330463.ref049" class="usa-link" aria-describedby="pone.0330463.ref049">49</a>] (denoted in Equations (7–8)).</p>
<table class="disp-formula p" id="pone.0330463.e143"><tr>
<td class="formula"><math id="M143" display="block" overflow="linebreak"><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>r</mi><mo>,</mo><mi>V</mi></mrow></msub><mo>=</mo><msub><mrow><mi>E</mi></mrow><mrow><msub><mrow><mi>f</mi></mrow><mrow><mi>v</mi></mrow></msub><mi>~</mi><msub><mrow><mi>D</mi></mrow><mrow><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo>,</mo><mi>v</mi><mi>~</mi><mi>V</mi></mrow></msub><mo stretchy="false">[</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>w</mi><mi>h</mi><mi>d</mi></mrow></mfrac><msub><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msub><mo>∅</mo><mrow><mi>i</mi></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>𝒢</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msub><mrow><mrow><mi mathvariant="bold-italic">f</mi></mrow></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">v</mi></mrow></mrow></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>−</mo><msub><mo>∅</mo><mrow><mi>i</mi></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>𝒯</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mrow><mn>2</mn></mrow></msub></mrow></math></td>
<td class="label">(7)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330463.e144"><tr>
<td class="formula"><math id="M144" display="block" overflow="linebreak"><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>r</mi><mo>,</mo><mi>P</mi></mrow></msub><mo>=</mo><msub><mrow><mi>E</mi></mrow><mrow><msub><mrow><mi>f</mi></mrow><mrow><mi>p</mi></mrow></msub><mi>~</mi><msub><mrow><mi>D</mi></mrow><mrow><mn>2</mn></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>,</mo><mi>p</mi><mi>~</mi><mi>P</mi></mrow></msub><mo stretchy="false">[</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>w</mi><mi>h</mi><mi>d</mi></mrow></mfrac><msub><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msub><mo>∅</mo><mrow><mi>i</mi></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>𝒢</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msub><mrow><mrow><mi mathvariant="bold-italic">f</mi></mrow></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">p</mi></mrow></mrow></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>−</mo><msub><mo>∅</mo><mrow><mi>i</mi></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>𝒯</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mrow><mn>2</mn></mrow></msub></mrow></math></td>
<td class="label">(8)</td>
</tr></table>
<p>Where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e145"><math id="M145" display="inline" overflow="linebreak"><mrow><msub><mo>∅</mo><mrow><mi>i</mi></mrow></msub><mo stretchy="false">(</mo><mi>·</mi><mo stretchy="false">)</mo></mrow></math></span> denotes the output from the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e146"><math id="M146" display="inline" overflow="linebreak"><mrow><mi>i</mi></mrow></math></span>th layer of the pretrained VGG16 [<a href="#pone.0330463.ref050" class="usa-link" aria-describedby="pone.0330463.ref050">50</a>] network, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e147"><math id="M147" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">f</mi></mrow></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">v</mi></mrow></mrow></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e148"><math id="M148" display="inline" overflow="linebreak"><mrow><msub><mrow><mrow><mi mathvariant="bold-italic">f</mi></mrow></mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">p</mi></mrow></mrow></mrow></msub><mrow><mrow></mrow></mrow></mrow></math></span>represent the feature maps from <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e149"><math id="M149" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>1</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e150"><math id="M150" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>2</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e151"><math id="M151" display="inline" overflow="linebreak"><mrow><mi>w</mi><mo>,</mo><mi>h</mi></mrow></math></span>and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e152"><math id="M152" display="inline" overflow="linebreak"><mrow><mi>d</mi></mrow></math></span> stands for the width, height and depth of the corresponding feature space, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e153"><math id="M153" display="inline" overflow="linebreak"><mrow><mi>𝒢</mi></mrow></math></span> is the pre-processing on <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e154"><math id="M154" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="bold-italic">f</mi></mrow></mrow></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e155"><math id="M155" display="inline" overflow="linebreak"><mrow><mi>𝒯</mi></mrow></math></span> is the processing on truth sub-volumes/patches.</p>
<p>Second, hinge loss was selected to classify <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e156"><math id="M156" display="inline" overflow="linebreak"><mrow><msup><mrow><mi>P</mi></mrow><mrow><mi>′</mi></mrow></msup></mrow></math></span> from <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e157"><math id="M157" display="inline" overflow="linebreak"><mrow><mi>P</mi></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e158"><math id="M158" display="inline" overflow="linebreak"><mrow><mi>V</mi><mrow><mi>′</mi></mrow></mrow></math></span> from <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e159"><math id="M159" display="inline" overflow="linebreak"><mrow><mi>V</mi></mrow></math></span> with formulas listed in Equations (9–10).</p>
<table class="disp-formula p" id="pone.0330463.e160"><tr>
<td class="formula"><math id="M160" display="block" overflow="linebreak"><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>h</mi><mo>,</mo><mi>V</mi></mrow></msub><mo>=</mo><msub><mrow><mi>E</mi></mrow><mrow><mi>v</mi><mrow><mi>′</mi></mrow><mi>~</mi><mi>V</mi><mrow><mi>′</mi></mrow></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mrow><mi>D</mi></mrow><mrow><mn>1</mn><mo>,</mo><mo>∅</mo></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msup><mrow><mi>v</mi></mrow><mrow><mi>′</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">]</mo></mrow><mo>+</mo><msub><mrow><mi>E</mi></mrow><mrow><mi>v</mi><mi>~</mi><mi>V</mi></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mi>f</mi><msub><mrow><mo stretchy="false">(</mo><mo>−</mo><mi>D</mi></mrow><mrow><mn>1</mn><mo>,</mo><mo>∅</mo></mrow></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">]</mo></mrow><mtext>\ \ \ \ \ \ </mtext></mrow></math></td>
<td class="label">(9)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330463.e161"><tr>
<td class="formula"><math id="M161" display="block" overflow="linebreak"><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>h</mi><mo>,</mo><mi>P</mi></mrow></msub><mo>=</mo><msub><mrow><mi>E</mi></mrow><mrow><mi>p</mi><mrow><mi>′</mi></mrow><mi>~</mi><mi>P</mi><mrow><mi>′</mi></mrow></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mrow><mi>D</mi></mrow><mrow><mn>2</mn><mo>,</mo><mo>∅</mo></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msup><mrow><mi>p</mi></mrow><mrow><mi>′</mi></mrow></msup><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">]</mo></mrow><mo>+</mo><msub><mrow><mi>E</mi></mrow><mrow><mi>p</mi><mi>~</mi><mi>P</mi></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mi>f</mi><msub><mrow><mo stretchy="false">(</mo><mo>−</mo><mi>D</mi></mrow><mrow><mn>2</mn><mo>,</mo><mo>∅</mo></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo fence="true" form="postfix" stretchy="true">]</mo></mrow><mtext>\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ </mtext></mrow></math></td>
<td class="label">(10)</td>
</tr></table>
<p>Where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e162"><math id="M162" display="inline" overflow="linebreak"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>max</mtext><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>+</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></math></span>.</p>
<p>Lastly, data augmentation, including random flipping and rotation, was implemented to V’ and P’ prior to sending into <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e163"><math id="M163" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>1</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e164"><math id="M164" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>2</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span>, following the theory proposed by Data Augmentation Optimized for GAN (DAG) framework [<a href="#pone.0330463.ref051" class="usa-link" aria-describedby="pone.0330463.ref051">51</a>]. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e165"><math id="M165" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>1</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span>/<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e166"><math id="M166" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>D</mi></mrow><mrow><mn>2</mn><mo>,</mo><mo>∅</mo></mrow></msub></mrow></math></span> share weights while discriminating multiple augmented sub-volumes/patches. Therefore, we have the overall loss objective formulated as Equations (11–12).</p>
<table class="disp-formula p" id="pone.0330463.e167"><tr>
<td class="formula"><math id="M167" display="block" overflow="linebreak"><mrow><mi>L</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>ϑ</mi><mo>,</mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msub><mo>∅</mo><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">}</mo></mrow><mo>,</mo><mrow><mo fence="true" form="prefix" stretchy="true">{</mo><msub><mo>∅</mo><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">}</mo></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>=</mo><mi>L</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>ϑ</mi><mo>,</mo><msub><mo>∅</mo><mrow><mn>1</mn><mo>,</mo><mn>0</mn></mrow></msub><mo>,</mo><msub><mo>∅</mo><mrow><mn>2</mn><mo>,</mo><mn>0</mn></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>+</mo><mfrac><mrow><msub><mrow><mi>λ</mi></mrow><mrow><mn>1</mn></mrow></msub></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><mrow><mi>L</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>ϑ</mi><mo>,</mo><msub><mo>∅</mo><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msub><mo>∅</mo><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow></mrow></mrow></math></td>
<td class="label">(11)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330463.e168"><tr>
<td class="formula"><math id="M168" display="block" overflow="linebreak"><mrow><mi>L</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mi>ϑ</mi><mo>,</mo><msub><mo>∅</mo><mrow><mn>1</mn><mo>,</mo><mi>k</mi></mrow></msub><mo>,</mo><msub><mo>∅</mo><mrow><mn>2</mn><mo>,</mo><mi>k</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>=</mo><msub><mrow><mi>L</mi></mrow><mrow><mi>r</mi><mo>,</mo><mi>V</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>+</mo><msub><mrow><mi>L</mi></mrow><mrow><mi>h</mi><mo>,</mo><mi>V</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>+</mo><msub><mrow><mi>λ</mi></mrow><mrow><mn>2</mn></mrow></msub><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msub><mrow><mi>L</mi></mrow><mrow><mi>r</mi><mo>,</mo><mi>P</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>+</mo><msub><mrow><mi>L</mi></mrow><mrow><mi>h</mi><mo>,</mo><mi>P</mi><mo>,</mo><mi>k</mi></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mtext>\ \ \ \ \ \ \ \ </mtext></mrow></math></td>
<td class="label">(12)</td>
</tr></table>
<p>Where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e169"><math id="M169" display="inline" overflow="linebreak"><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e170"><math id="M170" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>λ</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>=</mo><mn>0.2</mn></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e171"><math id="M171" display="inline" overflow="linebreak"><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow></math></span> corresponding to the identity transformation follows the definition in Trans et al [<a href="#pone.0330463.ref051" class="usa-link" aria-describedby="pone.0330463.ref051">51</a>]. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e172"><math id="M172" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>λ</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>=</mo><mn>0.5</mn></mrow></math></span> to give the model more attention on conformal <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e173"><math id="M173" display="inline" overflow="linebreak"><mrow><mi>V</mi><mrow><mi>′</mi></mrow></mrow></math></span> rendering.</p>
<p><strong>Inference Stage</strong>: A fully trained <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e174"><math id="M174" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> was further fine-tuned with <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e175"><math id="M175" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>s</mi><mi>h</mi></mrow></msub></mrow></math></span>and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e176"><math id="M176" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>z</mi></mrow><mrow><mi>a</mi></mrow></msub></mrow></math></span> using the trained prior and incoming patients’ sparse view projection(s) to render the final patient-specific volumetric prediction <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e177"><math id="M177" display="inline" overflow="linebreak"><mrow><mover><mrow><mi>Y</mi></mrow><mo stretchy="false">^</mo></mover></mrow></math></span>. Since we conducted moderate optimization with limited iterations, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e178"><math id="M178" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> was tuned with fully size <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e179"><math id="M179" display="inline" overflow="linebreak"><mrow><mi>I</mi></mrow></math></span> instead of patches. Depending on the available views, a referenced projection was randomly drawn for each iteration until <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e180"><math id="M180" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> reached the convergence criteria. In our experiments, peak signal-to-noise ratio (PSNR)=25 was set as the stopping threshold. The inference loss objective is defined in Equation (13) with a combination of LPIPS, PSNR, and the negative log-likelihood loss (NLL).</p>
<table class="disp-formula p" id="pone.0330463.e181"><tr>
<td class="formula"><math id="M181" display="block" overflow="linebreak"><mrow><msub><mrow><mi>L</mi></mrow><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></msub><mo>=</mo><msub><mrow><mi>λ</mi></mrow><mrow><mn>1</mn></mrow></msub><msub><mrow><mi>L</mi></mrow><mrow><mi>r</mi><mo>,</mo><mi>I</mi></mrow></msub><mo>+</mo><msub><mrow><mi>λ</mi></mrow><mrow><mn>2</mn></mrow></msub><msub><mrow><mi>L</mi></mrow><mrow><mi>P</mi><mi>S</mi><mi>N</mi><mi>R</mi><mo>,</mo><mi>I</mi></mrow></msub><mo>+</mo><msub><mrow><mi>λ</mi></mrow><mrow><mn>3</mn></mrow></msub><msub><mrow><mi>L</mi></mrow><mrow><mi>N</mi><mi>L</mi><mi>L</mi><mo>,</mo><mi>I</mi></mrow></msub><mtext>\ \ \ \ \ \ \ \ \ \ \ \ </mtext></mrow></math></td>
<td class="label">(13)</td>
</tr></table>
<p>Where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e182"><math id="M182" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>λ</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>=</mo><msub><mrow><mi>λ</mi></mrow><mrow><mn>3</mn></mrow></msub><mo>=</mo><mn>0.3</mn></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e183"><math id="M183" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>λ</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>=</mo><mn>0.1</mn></mrow></math></span> were set in our experiments.</p></section></section><section id="sec013"><h3 class="pmc_sec_title">2.3. Implementation details</h3>
<p>During training, the RMSprop optimizer [<a href="#pone.0330463.ref052" class="usa-link" aria-describedby="pone.0330463.ref052">52</a>] with a batch size of 4 (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e184"><math id="M184" display="inline" overflow="linebreak"><mrow><mn>4</mn><mi>×</mi><mn>1</mn></mrow></math></span>), learning rate of 0.0005 for the generator, learning rate of 0.0001 for the discriminator, and 40000 iterations were performed. Per inference fine-tuning, RMSprop [<a href="#pone.0330463.ref052" class="usa-link" aria-describedby="pone.0330463.ref052">52</a>] optimizer with a batch size of 1, learning rate of 0.0005, stopping threshold of PNSR = 25 (mostly under 1000 iterations) was implemented towards the generator. All the experiments were carried out on a NVDIA RTX 4 × A6000 cluster.</p></section><section id="sec014"><h3 class="pmc_sec_title">2.4. Evaluation metrics</h3>
<p>We evaluate the predicted CT volume <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e185"><math id="M185" display="inline" overflow="linebreak"><mrow><mover><mrow><mi>Y</mi></mrow><mo stretchy="false">^</mo></mover></mrow></math></span> and projection <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e186"><math id="M186" display="inline" overflow="linebreak"><mrow><mover><mrow><mi>I</mi></mrow><mo stretchy="false">^</mo></mover></mrow></math></span> corresponding to the reference view of our TomoGRAF generator using PSNR, structure similarity index measurement (SSIM) and rooted mean squared error (RMSE) as Equations (14–16).</p>
<table class="disp-formula p" id="pone.0330463.e187"><tr>
<td class="formula"><math id="M187" display="block" overflow="linebreak"><mrow><mi>P</mi><mi>S</mi><mi>N</mi><mi>R</mi><mo>=</mo><mn>20</mn><mi>·</mi><msub><mrow><mtext>log</mtext></mrow><mrow><mn>10</mn></mrow></msub><mfrac><mrow><msub><mrow><mi>M</mi><mi>A</mi><mi>X</mi></mrow><mrow><mi>I</mi></mrow></msub></mrow><mrow><mi>R</mi><mi>M</mi><mi>S</mi><mi>E</mi></mrow></mfrac></mrow></math></td>
<td class="label">(14)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330463.e188"><tr>
<td class="formula"><math id="M188" display="block" overflow="linebreak"><mrow><mi>S</mi><mi>S</mi><mi>I</mi><mi>M</mi><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mn>2</mn><msub><mrow><mi>μ</mi></mrow><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></msub><msub><mrow><mi>μ</mi></mrow><mrow><mi>y</mi></mrow></msub><mo>+</mo><msub><mrow><mi>c</mi></mrow><mrow><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>2</mn><msub><mrow><mi>σ</mi></mrow><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub><mi>y</mi></mrow></msub><mo>+</mo><msub><mrow><mi>c</mi></mrow><mrow><mn>2</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><msubsup><mrow><mo stretchy="false">(</mo><mi>μ</mi></mrow><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow><mrow><mn>2</mn></mrow></msubsup><mo>+</mo><msubsup><mrow><mi>μ</mi></mrow><mrow><mi>y</mi></mrow><mrow><mn>2</mn></mrow></msubsup><mo>+</mo><msub><mrow><mi>c</mi></mrow><mrow><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><msubsup><mrow><mo stretchy="false">(</mo><mi>σ</mi></mrow><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow><mrow><mn>2</mn></mrow></msubsup><mo>+</mo><msubsup><mrow><mi>σ</mi></mrow><mrow><mi>y</mi></mrow><mrow><mn>2</mn></mrow></msubsup><mo>+</mo><msub><mrow><mi>c</mi></mrow><mrow><mn>2</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></td>
<td class="label">(15)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330463.e189"><tr>
<td class="formula"><math id="M189" display="block" overflow="linebreak"><mrow><mi>R</mi><mi>M</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></msubsup><msup><mrow><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>−</mo><mover><mrow><mi>y</mi></mrow><mo stretchy="false">^</mo></mover><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo stretchy="false">|</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow><mrow><mi>N</mi></mrow></mfrac></mrow></msqrt></mrow></math></td>
<td class="label">(16)</td>
</tr></table>
<p>Where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e190"><math id="M190" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>M</mi><mi>A</mi><mi>X</mi></mrow><mrow><mi>I</mi></mrow></msub></mrow></math></span> is the max possible pixel value in a tensor, RMSE stands for rooted mean squared error, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e191"><math id="M191" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>μ</mi></mrow><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e192"><math id="M192" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>μ</mi></mrow><mrow><mi>y</mi></mrow></msub></mrow></math></span> is the pixel mean of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e193"><math id="M193" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e194"><math id="M194" display="inline" overflow="linebreak"><mrow><mi>y</mi></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e195"><math id="M195" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>σ</mi></mrow><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub><mi>y</mi></mrow></msub></mrow></math></span> is the covariance between <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e196"><math id="M196" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e197"><math id="M197" display="inline" overflow="linebreak"><mrow><mi>y</mi></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e198"><math id="M198" display="inline" overflow="linebreak"><mrow><msubsup><mrow><mi>σ</mi></mrow><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow><mrow><mn>2</mn></mrow></msubsup></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e199"><math id="M199" display="inline" overflow="linebreak"><mrow><msubsup><mrow><mi>σ</mi></mrow><mrow><mi>y</mi></mrow><mrow><mn>2</mn></mrow></msubsup></mrow></math></span> is the variance of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e200"><math id="M200" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>G</mi></mrow><mrow><mi>ϑ</mi></mrow></msub></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e201"><math id="M201" display="inline" overflow="linebreak"><mrow><mi>y</mi><mo>.</mo></mrow></math></span> Lastly, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e202"><math id="M202" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>c</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><msub><mrow><mi>k</mi></mrow><mrow><mn>1</mn></mrow></msub><mi>L</mi><mo stretchy="false">)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e203"><math id="M203" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>c</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><msub><mrow><mi>k</mi></mrow><mrow><mn>2</mn></mrow></msub><mi>L</mi><mo stretchy="false">)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></math></span>, where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e204"><math id="M204" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>k</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>=</mo><mn>0.01</mn></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e205"><math id="M205" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>k</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>=</mo><mn>0.03</mn></mrow></math></span> in the current work and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e206"><math id="M206" display="inline" overflow="linebreak"><mrow><mi>L</mi></mrow></math></span> is the dynamic range of the pixel values (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e207"><math id="M207" display="inline" overflow="linebreak"><mrow><msup><mrow><mn>2</mn></mrow><mrow><mi>#</mi><mi>b</mi><mi>i</mi><mi>t</mi><mi>s</mi><mi>p</mi><mi>e</mi><mi>r</mi><mi>p</mi><mi>i</mi><mi>x</mi><mi>e</mi><mi>l</mi></mrow></msup><mo>−</mo><mn>1</mn></mrow></math></span>).</p></section><section id="sec015"><h3 class="pmc_sec_title">2.5. Baseline algorithms</h3>
<p>A CNN-based method X2CT-GAN [<a href="#pone.0330463.ref041" class="usa-link" aria-describedby="pone.0330463.ref041">41</a>], and a NeRF-based method MedNeRF [<a href="#pone.0330463.ref046" class="usa-link" aria-describedby="pone.0330463.ref046">46</a>] were included as our benchmarks with both the performance in projection inference and CT volume rendering compared. X2CT-GAN and MedNeRF are evaluated using the open-sourced codes and network weights released by authors, with the input of our in-house test set arranged following their data organization guidelines.</p></section><section id="sec016"><h3 class="pmc_sec_title">2.6. Data cohorts</h3>
<p>1011 CT scans were selected from LIDC-IDRI [<a href="#pone.0330463.ref053" class="usa-link" aria-describedby="pone.0330463.ref053">53</a>] thoracic CT database for organizing the training set. Digital reconstructed radiographs (DRRs) were generated as projections for training supervision. Several scanner manufacturers and models were included (GE Medical Systems LightSpeed scanner models, Philips Brilliance scanner models, Siemens Definition, Siemens Emotion, Siemens Sensation, and Toshiba Aquilion). The tube peak potential energies for scan acquisition include 120 kV, 130 kV, 135 kV, and 140 kV. The tube current is in the range of 40–627 mA. Slice thickness includes 0.6 mm, 0.75 mm, 0.9 mm, 1.0 mm, 1.25 mm, 1.5 mm, 2.0 mm, 2.5 mm, 3.0 mm, 4.0 mm and 5.0 mm. SAD includes 595 mm, 541 mm, 570 mm and 535 mm with corresponding SID of 1085.6 mm, 949.1 mm, 1040 mm and 940 mm, respectively. The in-plane pixel size ranges from 0.461 to 0.977 mm [<a href="#pone.0330463.ref053" class="usa-link" aria-describedby="pone.0330463.ref053">53</a>]. 72 DRRs that cover a full 360° (generated each of 5° rotations) vertical rotations were generated for each scan. All the DRRs and CT volumes were black-border cropped out. DRRs were resized with a resolution of 128 times 128, and CT volumes were interpolated with a resolution of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e208"><math id="M208" display="inline" overflow="linebreak"><mrow><mi>×</mi><mi>H</mi><mi>×</mi><mi>W</mi><mo>=</mo><mn>128</mn><mi>×</mi><mn>128</mn><mi>×</mi><mn>128</mn></mrow></math></span> for model learning preparation. All the data were patient-wise normalized to [0, 1].</p>
<p>The test data was organized under IRB approval (IRB # 20–32527), entitled “Image-guided radiation therapy”, which include 100 de-identified CT scans from lung cancer patients who underwent robotic radiation therapy. Informeed consent was not required for the retrospective imaging study. All patients were scanned by Siemens Sensation with tube peak potential energy of 120 kV, tube current of 120 mA, slice thickness of 1.5 mm, and in-plane pixel size of 0.977 mm. SAD and SID are of 570 mm and 1040 mm. The anterior-posterior (AP) and lateral views were generated for inference reference, with 1-view-based inference solely referencing the AP view projection. All the DRRs and CT volumes had the black border cropped out. Additionally, DRRs were resized with a resolution of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e209"><math id="M209" display="inline" overflow="linebreak"><mrow><mn>128</mn><mi>×</mi><mn>128</mn></mrow></math></span>, and CT volumes were interpolated with a volume size of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e210"><math id="M210" display="inline" overflow="linebreak"><mrow><mn>128</mn><mi>×</mi><mn>128</mn><mi>×</mi><mn>128</mn></mrow></math></span>. All the data were patient-wise normalized to [0, 1] prior to being fed into models for inference.</p></section><section id="sec017"><h3 class="pmc_sec_title">2.7. Model performance as a function of the number of views and 3D supervision</h3>
<p>The model baseline performance was established using a single AP view. 1, 2, 5, and 10 view reconstructions were also performed to determine the model performance. The view angles are specified as follows: for 1-view-based reconstruction, the AP view is used for referencing. For 2-view-based reconstruction, the AP and lateral views are used for inferencing. For 5-view-based reconstruction, a full 360° is covered with rotation every 72°, starting from the AP view. For 10-view-based reconstruction, a full 360° is covered with rotation of every 36°, starting from the AP view.</p></section></section><section id="sec018"><h2 class="pmc_sec_title">3. Results</h2>
<p>The results of TomoGRAF, MedNeRF, and X2CT-GAN with 1 or 2 views for 2D projection and volume rendering are visually demonstrated in <a href="#pone.0330463.g002" class="usa-link">Figs 2</a> and <a href="#pone.0330463.g003" class="usa-link">3</a>, with accompanying statistics reported in <a href="#pone.0330463.t001" class="usa-link">Table 1</a> and <a href="#pone.0330463.g004" class="usa-link">Fig 4</a>. TomoGRAF consistently outperforms MedNeRF and X2CT-GAN in both tasks with the most evident advantage in 1-view volume reconstruction.</p>
<figure class="fig xbox font-sm" id="pone.0330463.g002"><h3 class="obj_head">Fig 2. Lung projection rendering in four 360°-clockwise-rotated (visualized every 90° of rotation) views from a patient in a test set.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373210_pone.0330463.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8873/12373210/ce5d42a224db/pone.0330463.g002.jpg" loading="lazy" height="427" width="777" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330463.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Projections from X2CT-GAN are generated by applying the DRR synthesis algorithm on predicted CT volumes since the original X2CT-GAN was only designed to predict the CT volume. TomoGRAF and MedNeRF directly predicted the 2D projections. Results rendered by referencing 1-view and 2-views are both visualized. All the images are shown with a normalized window of [0, 1].</p></figcaption></figure><figure class="fig xbox font-sm" id="pone.0330463.g003"><h3 class="obj_head">Fig 3. CT Reconstruction results for two representative patients in the test set.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373210_pone.0330463.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8873/12373210/e0e9a3e2bd61/pone.0330463.g003.jpg" loading="lazy" height="214" width="758" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330463.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Only the residual maps of the TomoGRAF are shown, as the two comparison methods result in a large mismatch with GT. The images are visualized in a lung window with (a Hounsfield Unit width and level) of (1500 −600). The red boxes denote the lung tumors, while the arrows point to the pacemaker in patient 1.</p></figcaption></figure><section class="tw xbox font-sm" id="pone.0330463.t001"><h3 class="obj_head">Table 1. Statistical results evaluated on the test set. The best results from each metric are underscored. ↑ indicates the higher the statistical value, the better, and vice versa for ↓. SSIM and PSNR are calculated with images normalized to [0,1] scales and RMSE are calculated based on Hounsfield Units (HUs). 0.31 ± 0.12.</h3>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2" colspan="1"></th>
<th align="left" rowspan="2" colspan="1">Modality</th>
<th align="left" colspan="4" rowspan="1">1-View</th>
<th align="left" colspan="4" rowspan="1">2-Views</th>
</tr>
<tr>
<th align="left" rowspan="1" colspan="1">SSIM↑</th>
<th align="left" rowspan="1" colspan="1">PSNR(dB)↑</th>
<th align="left" rowspan="1" colspan="1">RMSE(HU)↓</th>
<th align="left" rowspan="1" colspan="1">Inference Time (s)↓</th>
<th align="left" rowspan="1" colspan="1">SSIM↑</th>
<th align="left" rowspan="1" colspan="1">PSNR(dB)↑</th>
<th align="left" rowspan="1" colspan="1">RMSE↓</th>
<th align="left" rowspan="1" colspan="1">Inference Time (s)↓</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="3" colspan="1">CT Volume</td>
<td align="left" rowspan="1" colspan="1">
<strong>X2CT-GAN</strong>
</td>
<td align="left" rowspan="1" colspan="1">0.31 ± 0.12</td>
<td align="left" rowspan="1" colspan="1">14.39 ± 0.19</td>
<td align="left" rowspan="1" colspan="1">386.69 ± 27.43</td>
<td align="left" rowspan="1" colspan="1">0.27</td>
<td align="left" rowspan="1" colspan="1">0.48 ± 0.06</td>
<td align="left" rowspan="1" colspan="1">17.35 ± 0.21</td>
<td align="left" rowspan="1" colspan="1">347.76 ± 25.46</td>
<td align="left" rowspan="1" colspan="1">1.31</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>MedNeRF</strong>
</td>
<td align="left" rowspan="1" colspan="1">0.37 ± 0.08</td>
<td align="left" rowspan="1" colspan="1">7.68 ± 0.10</td>
<td align="left" rowspan="1" colspan="1">321.87 ± 22.87</td>
<td align="left" rowspan="1" colspan="1">527.78 ± 15.48</td>
<td align="left" rowspan="1" colspan="1">0.50 ± 0.09</td>
<td align="left" rowspan="1" colspan="1">18.21 ± 0.09</td>
<td align="left" rowspan="1" colspan="1">299.49 ± 21.58</td>
<td align="left" rowspan="1" colspan="1">865.46 ± 30.81</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>TomoGRAF</strong>
</td>
<td align="left" rowspan="1" colspan="1">0.79 ± 0.03</td>
<td align="left" rowspan="1" colspan="1">33.45 ± 0.13</td>
<td align="left" rowspan="1" colspan="1">175.48 ± 10.47</td>
<td align="left" rowspan="1" colspan="1">344.25 ± 10.32</td>
<td align="left" rowspan="1" colspan="1">0.85 ± 0.04</td>
<td align="left" rowspan="1" colspan="1">35.89 ± 0.13</td>
<td align="left" rowspan="1" colspan="1">146.73 ± 9.63</td>
<td align="left" rowspan="1" colspan="1">719.46 ± 26.78</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">
<strong>Projection</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>X2CT-GAN</strong>
</td>
<td align="left" rowspan="1" colspan="1">0.34 ± 0.11</td>
<td align="left" rowspan="1" colspan="1">11.88 ± 0.19</td>
<td align="left" rowspan="1" colspan="1">51.96 ± 7.98</td>
<td align="left" rowspan="3" colspan="1">–</td>
<td align="left" rowspan="1" colspan="1">0.51 ± 0.09</td>
<td align="left" rowspan="1" colspan="1">18.23 ± 0.26</td>
<td align="left" rowspan="1" colspan="1">47.64 ± 7.32</td>
<td align="left" rowspan="3" colspan="1">–</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>MedNeRF</strong>
</td>
<td align="left" rowspan="1" colspan="1">0.67 ± 0.07</td>
<td align="left" rowspan="1" colspan="1">25.02 ± 0.15</td>
<td align="left" rowspan="1" colspan="1">36.48 ± 4.36</td>
<td align="left" rowspan="1" colspan="1">0.69 ± 0.08</td>
<td align="left" rowspan="1" colspan="1">27.31 ± 0.14</td>
<td align="left" rowspan="1" colspan="1">33.42 ± 4.01</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>TomoGRAF</strong>
</td>
<td align="left" rowspan="1" colspan="1">0.69 ± 0.03</td>
<td align="left" rowspan="1" colspan="1">25.43 ± 0.14</td>
<td align="left" rowspan="1" colspan="1">34.37 ± 4.58</td>
<td align="left" rowspan="1" colspan="1">0.71 ± 0.04</td>
<td align="left" rowspan="1" colspan="1">27.99 ± 0.13</td>
<td align="left" rowspan="1" colspan="1">31.22 ± 4.12</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330463.t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="pone.0330463.g004"><h3 class="obj_head">Fig 4. Patient-wise evaluated SSIM distribution (visualized using histograms with smoothed trend curves) for a, 1-view-based volume rendering results of TomoGRAF, X2CT-GAN, and MedNeRF in the test set.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373210_pone.0330463.g004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8873/12373210/20ed6c45037c/pone.0330463.g004.jpg" loading="lazy" height="389" width="777" alt="Fig 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330463.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>b,</strong> 2-view-based volume rendering results of TomoGRAF, X2CT-GAN, and MedNeRF in the test set.</p></figcaption></figure><p>For 2D projection rendering, TomoGRAF achieves marginally better results than MedNeRF. Both models maintain the overall critical body shapes of GT, and TomoGRAF visualizes more detailed morphology, such as heart, spine, and vascular structures. In comparison, the projection results of X2CT-GAN show visible distortion and significantly worse quantitative performance.</p>
<p>For 3D volume reconstruction, as shown in <a href="#pone.0330463.g003" class="usa-link">Fig 3</a>, with 1-view, TomoGRAF depicts rich and correct anatomical details with visible tumors and a pacemaker consistent with GT. The results are further refined with the second orthogonal X-ray view, improving fine details’ recovery. In comparison, MedNeRF and X2CT-GAN fail to render patient-relevant 3D volumes with 1 or 2 views. MedNeRF loses most anatomical details; X2CT-GAN deforms 3D anatomies that do not reflect patient-specific characteristics, such as lung tumors and the pacemaker.</p>
<p>As shown in <a href="#pone.0330463.t001" class="usa-link">Table 1</a>, TomoGRAF is vastly superior in quantitative imaging metrics, achieving SSIM and PSNR of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e211"><math id="M211" display="inline" overflow="linebreak"><mrow><mn>0.79</mn><mi>±</mi><mn>0.03</mn></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e212"><math id="M212" display="inline" overflow="linebreak"><mrow><mn>33.45</mn><mi>±</mi><mn>0.13</mn></mrow></math></span>, respectively, vs. MedNeRF (SSIM at <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e213"><math id="M213" display="inline" overflow="linebreak"><mrow><mn>0.37</mn><mi>±</mi><mn>0.05</mn></mrow></math></span> and PSNR at <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e214"><math id="M214" display="inline" overflow="linebreak"><mrow><mn>7.68</mn><mi>±</mi><mn>0.10</mn></mrow></math></span>) and X2CT-GAN (SSIM at <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e215"><math id="M215" display="inline" overflow="linebreak"><mrow><mn>0.31</mn><mi>±</mi><mn>0.012</mn></mrow></math></span> and PSNR at <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330463.e216"><math id="M216" display="inline" overflow="linebreak"><mrow><mn>14.39</mn><mi>±</mi><mn>0.19</mn></mrow></math></span>). There is a similar reduction in RMSE. Additionally, we can observe from <a href="#pone.0330463.g004" class="usa-link">Fig 4</a> that the SSIM distribution of TomoGRAF is highly left skewed and leptokurtic in both 1 and 2-view-based volume rendering, with the majority clustering tightly towards the higher end, while that of MedNeRF and X2CT-GAN tends to be normal and moderately right-skewed (values lean towards the lower end).</p>
<p><a href="#pone.0330463.t002" class="usa-link">Table 2</a> shows the 3D reconstruction performance with or without 3D supervision using 1, 2, 5, and 10 views as input. Using more views improved both volume and projection inference performance. 3D GT training markedly boosted the model performance in 3D volume rendering only. <a href="#pone.0330463.g005" class="usa-link">Fig 5</a> shows line profile comparisons for varying view inputs. 1 view TomoGRAF recovered major structures but missed fine details, which were better preserved with more X-ray views.</p>
<section class="tw xbox font-sm" id="pone.0330463.t002"><h3 class="obj_head">Table 2. Statistical results of TomoGRAF ablation study evaluated on test set. 1/2/5/10-Views represent the number of views used for reconstruction reference. Training with and without 3D CT supervision is also compared in the current table. ↑ indicates the higher the statistical value, the better, and vice versa for ↓. SSIM and PSNR are calculated with images normalized to [0,1] scales and RMSE are calculated based on Hounsfield units (HUs).</h3>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1"></th>
<th align="left" rowspan="1" colspan="1">Reference View</th>
<th align="left" rowspan="1" colspan="1">3D Supervised Training</th>
<th align="left" rowspan="1" colspan="1">SSIM↑</th>
<th align="left" rowspan="1" colspan="1">PSNR(dB)↑</th>
<th align="left" rowspan="1" colspan="1">RMSE(HU) ↓</th>
<th align="left" rowspan="1" colspan="1">Inference Time (s) ↓</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="8" colspan="1">
<strong>CT Volume</strong>
</td>
<td align="left" rowspan="2" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">Y</td>
<td align="left" rowspan="1" colspan="1">0.79 ± 0.03</td>
<td align="left" rowspan="1" colspan="1">33.45 ± 0.13</td>
<td align="left" rowspan="1" colspan="1">175.48 ± 10.47</td>
<td align="left" rowspan="2" colspan="1">344.25 ± 10.32</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">N</td>
<td align="left" rowspan="1" colspan="1">0.66 ± 0.05</td>
<td align="left" rowspan="1" colspan="1">26.76 ± 0.16</td>
<td align="left" rowspan="1" colspan="1">197.47 ± 11.24</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">Y</td>
<td align="left" rowspan="1" colspan="1">0.85 ± 0.04</td>
<td align="left" rowspan="1" colspan="1">35.89 ± 0.13</td>
<td align="left" rowspan="1" colspan="1">146.73 ± 9.63</td>
<td align="left" rowspan="2" colspan="1">719.46 ± 26.78</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">N</td>
<td align="left" rowspan="1" colspan="1">0.69 ± 0.06</td>
<td align="left" rowspan="1" colspan="1">29.87 ± 0.19</td>
<td align="left" rowspan="1" colspan="1">168.35 ± 10.21</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">Y</td>
<td align="left" rowspan="1" colspan="1">0.88 ± 0.03</td>
<td align="left" rowspan="1" colspan="1">37.23 ± 0.13</td>
<td align="left" rowspan="1" colspan="1">138.45 ± 9.12</td>
<td align="left" rowspan="2" colspan="1">987.35 ± 37.89</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">N</td>
<td align="left" rowspan="1" colspan="1">0.72 ± 0.04</td>
<td align="left" rowspan="1" colspan="1">30.15 ± 0.18</td>
<td align="left" rowspan="1" colspan="1">147.56 ± 9.79</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">Y</td>
<td align="left" rowspan="1" colspan="1">0.93 ± 0.01</td>
<td align="left" rowspan="1" colspan="1">39.98 ± 0.11</td>
<td align="left" rowspan="1" colspan="1">127.68 ± 8.78</td>
<td align="left" rowspan="2" colspan="1">1238.81 ± 46.72</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">N</td>
<td align="left" rowspan="1" colspan="1">0.75 ± 0.01</td>
<td align="left" rowspan="1" colspan="1">31.86 ± 0.18</td>
<td align="left" rowspan="1" colspan="1">138.98 ± 9.54</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330463.t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="pone.0330463.g005"><h3 class="obj_head">Fig 5. Line profile comparison of the two patients shown in <a href="#pone.0330463.g003" class="usa-link">Fig 3</a>.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373210_pone.0330463.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8873/12373210/179f1053edc9/pone.0330463.g005.jpg" loading="lazy" height="696" width="734" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330463.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The images visualized in a and <strong>c</strong> are GT slices downsampled to 128 × 128 to align with the prediction resolution. <strong>a,</strong> Indication of the location of the profile in patient 1 in coronal view across the lung tumor. <strong>b,</strong> Line profiles of 3D images rendered by TomoGRAF with 1, 2, 5, and 10-view inputs for patient 1. <strong>c,</strong> Indication of the location of the profile in patient 2 across the lung tumor in sagittal view. <strong>d,</strong> Line profiles of 3D images rendered by TomoGRAF with 1, 2, 5, and 10-view inputs for patient 2.</p></figcaption></figure></section><section id="sec019"><h2 class="pmc_sec_title">4. Discussion</h2>
<p>This paper presents a GAN-embedded NeRF generator (TomoGRAF) for volumetric CT rendering from ultra-sparse X-ray views. TomoGRAF extends radiance fields into medical imaging reconstruction with a CT imaging-informed ray casting/tracing simulator. Also, TomoGRAF leverages the availability of 3D volumetric information at the training stage to enable an effective generator trained with full volumetric supervision. The robustness of TomoGRAF is demonstrated on an external dataset independent of the training set. TomoGRAF vastly improves 1-view 3D reconstruction performance yet scales well with additional views to accommodate practical balances in image acquisitions and quality requirements.</p>
<p>Reconstruction of 3D CT volume from ultra-sparse angular sampling is an ill-posed inverse problem that is extremely underconditioned to solve. On the other hand, such 3D reconstruction is practically desirable and widely applicable when a full gantry rotation is prevented by mechanical limitations or the dynamic process of interest is significantly faster than CT acquisition speed [<a href="#pone.0330463.ref005" class="usa-link" aria-describedby="pone.0330463.ref005">5</a>,<a href="#pone.0330463.ref006" class="usa-link" aria-describedby="pone.0330463.ref006">6</a>,<a href="#pone.0330463.ref009" class="usa-link" aria-describedby="pone.0330463.ref009">9</a>]. Therefore, there has been a consistent effort to reconstruct 3D images with extremely sparse views that circumvent these mechanical and temporal restrictions. Although CS-powered iterative methods and some earlier DL methods were able to reconstruct 3D images with as few as 20 views [<a href="#pone.0330463.ref018" class="usa-link" aria-describedby="pone.0330463.ref018">18</a>], the resultant image quality noticeably degraded. Still, they were unable to meet the challenges of many aforementioned practical scenarios where only one or two views were available for a given anatomical instance. Reconstruction with even more sparse views cannot be achieved without stronger priors and statistical learning. DL methods marched further in realizing ultra-sparse sampling (1 view) reconstruction using state-of-the-art (SOTA) networks, two of which are compared in this study.</p>
<p>TomoGRAF is distinctly superior to two SOTA methods for ultra-sparse view CT reconstruction in the following aspects. 1) In comparison to CNN-based networks with an extremely large number of parameters, such as X2CT-GAN [<a href="#pone.0330463.ref041" class="usa-link" aria-describedby="pone.0330463.ref041">41</a>], the radiance field-based generators train a lighter model with significantly fewer parameters (X2CT-GAN: ~ 4.28G FLOPS vs. TomoGRAF ~ 0.9G FLOPS, FLOPS: floating point operation per second) to represent the interaction process between photons and objects, which formalizes a better-defined goal for the network to reach and can effectively reduce the amount of training data required for achieving global robustness. Plus, CNN frameworks generally lack flexibility in view referencing. The number and angle of views at the inference stage must align with the input at the training stage. In addition to single-view referencing, TomoGRAF is capable of incorporating multiple X-ray views from diverse directions, as demonstrated in this study using uniformly distributed acquisition angles. It is worth noting that X2CT-GAN performance in the current study is markedly worse than the original report [<a href="#pone.0330463.ref041" class="usa-link" aria-describedby="pone.0330463.ref041">41</a>]. To determine the correctness of our implementation, we tested the X2CT-GAN code on the LIDC-IDRI data with the same data split and arrived at a similar performance. We believe the sharp decline in performance from internal LIDC-IDRI data testing to external in-house organized data testing is due to the differences in the training and testing data. CT images in LIDC-IDRI are cropped to keep only the thoracic organs, while our in-house test data are intact CT with the complete patient’s chest wall, arms, and neck. Such variation or domain shift is common and expected in practice: the patients can vary in size and be set up with different immobilization devices or arm positions. In stark contrast to X2CT-GAN, TomoGRAF is robust to such variation. 2) Compared to MedNeRF, we adapt NeRF with a physically realistic volume rendering mechanism based on the x-ray transportation properties, where photons pass through the body, and the volumetric photon attenuation along the ray path within the body is the focus of reconstruction. In other words, TomoGRAF learns 3D X-ray image formation physics, whereas MedNeRF assumes visible light transportation physics and is intended for 2D manifold learning of object surfaces. The limitation is evident in both low quantitative imaging metrics and orthogonal cuts of MedNeRF reconstructed patients: there is better retention of outer patient contour than internal anatomical details, which are largely lost in MedNeRF images. Additionally, TomoGRAF employs paired 3D CT supervision at the training stage to maximize the prior knowledge exposed to the network, which contributed to the model robustness in volume rendering at the inference stage. As a result, TomoGRAF successfully leverages the efficient object representation capacity of NeRF while overcoming the intrinsic limitations due to its lack of X-ray transportation physics and 3D volume comprehension. To our knowledge, TomoGRAF is the first truly generalizable single-view 3D X-ray reconstruction pipeline robust to substantial domain shifts.</p>
<p>At the practical level, TomoGRAF provides a unique solution for applications where only one or a few X-ray views are available, but 3D volumetric information is desired. The applications include image-guided radiotherapy, interventional radiology, and angiography. For the former, 2D kV X-rays can be interlaced with MV therapeutic X-ray beams to provide a real-time view of the patient during treatment [<a href="#pone.0330463.ref054" class="usa-link" aria-describedby="pone.0330463.ref054">54</a>]. However, the 2D projection images do not describe the full 3D anatomy, which is critical for adapting radiotherapy to the real-time patient target and surrounding tissue geometry. Similarly, 4D CT digitally subtracted angiography (DSA) better describes dynamics of the contrast for enhanced diagnosis than single-phase CT DSA [<a href="#pone.0330463.ref055" class="usa-link" aria-describedby="pone.0330463.ref055">55</a>], but fast helical and flat panel-based 4D-DSA requires repeated scans of the subject, increasing the imaging dose and leading to compromised temporal resolution for intricate vascular structures [<a href="#pone.0330463.ref056" class="usa-link" aria-describedby="pone.0330463.ref056">56</a>]. TomoGRAF can be potentially used to infer real-time time-resolved 3D DSA with significantly reduced imaging dose. Our results show that TomoGRAF is flexible in incorporating more views for further improved inference performance. Dual views with fixed X-ray systems are widely used in radiotherapy for stereotactic localization [<a href="#pone.0330463.ref038" class="usa-link" aria-describedby="pone.0330463.ref038">38</a>,<a href="#pone.0330463.ref057" class="usa-link" aria-describedby="pone.0330463.ref057">57</a>,<a href="#pone.0330463.ref058" class="usa-link" aria-describedby="pone.0330463.ref058">58</a>], but the modality is limited to triangulating bony anatomies or implanted fiducials. TomoGRAF can utilize the same 2D stereotactic views to provide rich 3D anatomies for soft tissue-based registration and localization. Besides mechanical and imaging dose constraints, inexpensive portable 2D X-rays are more readily available for point-of-care and low-resource settings where a CT is impractical. The ability to reconstruct 3D volumes using a single 2D view would markedly increase the imaging information available for clinical decisions. Our study also shows the feasibility of using more views in TomoGRAF for further improved performance and broader applications, including 4D CBCT and tomosynthesis with sparse or limited angle views.</p>
<p>At the theoretical level, TomoGRAF validates the extremely high data efficiency of neural field representation of 3D voxelized medical images. TomoGRAF, for the first time, materializes high data efficiency, achieving good quality (SSIM = 0.79–0.93) 3D reconstruction of CT images with 1–10 views, which is a major stride in comparison to existing research using NeRF or GRAF. The work thus has significant implications in 3D image acquisition, storage, and processing, which are currently voxel-based. Voxelized 3D representation does not provide intrinsic structural information regarding the relationships among voxels and thus can be expensive to acquire and reconstruct. Previous compressed sensing research explored some of the explicit structural correlations, such as piece-wise smoothness, for reduced data requirements. TomoGRAF indicates a new form of data representation that exploits implicit structural information with higher efficiency than conventional methods or neural networks without encoded physics.</p>
<p>Nevertheless, the current study leaves several areas for future improvement. First, TomoGRAF requires further fine-tuning at the inference stage, which increases the reconstruction time (1-view at 344.25 ± 10.32 s and 2-view at 719.46 ± 26.78 s). The time further increases with inference using more views. Significant acceleration is desired for online procedures such as motion adaptive radiotherapy [<a href="#pone.0330463.ref059" class="usa-link" aria-describedby="pone.0330463.ref059">59</a>]. Model compression techniques such as network pruning [<a href="#pone.0330463.ref060" class="usa-link" aria-describedby="pone.0330463.ref060">60</a>] and quantization [<a href="#pone.0330463.ref061" class="usa-link" aria-describedby="pone.0330463.ref061">61</a>] can decrease computational complexity while maintaining accuracy. Additionally, hardware acceleration via TensorRT [<a href="#pone.0330463.ref062" class="usa-link" aria-describedby="pone.0330463.ref062">62</a>] optimization or specialized processors (e.g., FPGAs [<a href="#pone.0330463.ref063" class="usa-link" aria-describedby="pone.0330463.ref063">63</a>], TPUs [<a href="#pone.0330463.ref064" class="usa-link" aria-describedby="pone.0330463.ref064">64</a>]) could also potentially improve the inference speed. Architecturally, incorporating efficient neural representations (e.g., lightweight MLPs [<a href="#pone.0330463.ref065" class="usa-link" aria-describedby="pone.0330463.ref065">65</a>] or hash-based encoding [<a href="#pone.0330463.ref066" class="usa-link" aria-describedby="pone.0330463.ref066">66</a>]) and adaptive sampling [<a href="#pone.0330463.ref067" class="usa-link" aria-describedby="pone.0330463.ref067">67</a>] methods could reduce computational overhead by prioritizing critical regions. Future work will explore these optimizations to improve TomoGRAF’s feasibility for real-time clinical applications, which would be essential for interventional procedures. Second, TomoGRAF is developed and tested on CT-synthesized DRR, which differs from kV X-rays obtained using an actual detector in image characteristics due to simplification of the physical projection model, detector dynamic ranges, noise, pre and postprocessing [<a href="#pone.0330463.ref039" class="usa-link" aria-describedby="pone.0330463.ref039">39</a>]. The current model may need to be adapted based on actual X-ray projections. Third, TomoGRAF reconstruction results with 1-view are geometrically correct but lose fine details and CT number accuracy, which is partially mitigated with increasing views up to 10. Therefore, in its current form, TomoGRAF is suited for object detection and localization tasks, but its appropriateness for quantitative tasks such as radiation dose calculation needs to be further studied. Moreover, the recovery of detail should also improve with reconstruction resolution, which is currently limited in rendering a maximum of 128 times 128 times 128 resolution due to GPU memory constraints. This limitation, however, is expected to be overcome soon with rapidly-increasing GPU memory capacity. Additionally, TomoGRAF exhibits moderate interpretability, as its foundation in generative radiance fields aligns with ray-based CT physics, ensuring a degree of physical consistency. The use of 2D DRRs and paired 3D ground truth during training enhances structured learning, while the subpatch-based approach improves generalizability. However, the implicit representation of NeRF structure poses challenges in direct voxel interpretation. While GAN-based training further introduces a black-box component, inference remains L2-based, reducing the risk of unrealistic features. Sparse-view adaptation further complicates interpretability, as the model’s implicit prior may influence reconstructions in ways distinct from traditional model-based iterative reconstruction methods. Future improvements could include feature sensitivity analyses and latent space visualization [<a href="#pone.0330463.ref068" class="usa-link" aria-describedby="pone.0330463.ref068">68</a>] to better delineate learned structures from data-driven priors. Lastly, while the current study primarily focuses on demonstrating the technical feasibility of ultra-sparse view reconstruction of the proposed TomoGRAF framework, we recognize that conventional quantitative image quality metrics may not fully capture the clinical utility of reconstructed images. As a future direction, incorporating clinical evaluation, such as qualitative scoring by radiologists or task-based diagnostic assessment, will further inform the real-world applicability and reliability of TomoGRAF in clinical practice.</p></section><section id="sec020"><h2 class="pmc_sec_title">5. Conclusion</h2>
<p>TomoGRAF, a novel GAN-based NeRF generator, is presented in the current work. TomoGRAF is trained on a public dataset and evaluated on 100 in-house lung CTs. TomoGRAF reconstructed good quality 3D images with correct internal anatomies using 1–2 X-ray views, which state-of-the-art DL methods fail to accomplish. TomoGRAF performance further improves with more views. The superior TomoGRAF performance is attributed to novel x-ray physics encoding in the radiance field training and paired 3D CT supervision.</p></section><section id="sec021"><h2 class="pmc_sec_title">Supporting information</h2>
<section class="sm xbox font-sm" id="pone.0330463.s001"><div class="caption p">
<span>S1 Appendix. Siddon’s Ray Tracing algorithm pseudo code applied in TomoGRAF projection rendering module.</span><p>(DOCX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373210/bin/pone.0330463.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330463.s001.docx</a><sup> (14.5KB, docx) </sup>
</div></div></section></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>Data cannot be shared publicly because of institutional restriction. Data are available from the UCSF Institutional Data Access / Ethics Committee for researchers who meet the criteria for access to confidential data. Interested researchers should follow the instructions outlined on <a href="https://icd.ucsf.edu/materialdata-transfer-agreements" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://icd.ucsf.edu/materialdata-transfer-agreements</a>. Please contact <span>Industrycontracts@ucsf.edu</span> | (415) 350-5408 for additional assistance.</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>The research is supported by NIH R01CA259008, R44CA183390 and R01EB031577.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0330463.ref001">
<span class="label">1.</span><cite>Tuy HK. An inversion formula for cone-beam reconstruction. SIAM J Appl Math. 1983;43(3):546–52. doi: 10.1137/0143035</cite> [<a href="https://doi.org/10.1137/0143035" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=SIAM%20J%20Appl%20Math&amp;title=An%20inversion%20formula%20for%20cone-beam%20reconstruction&amp;author=HK%20Tuy&amp;volume=43&amp;issue=3&amp;publication_year=1983&amp;pages=546-52&amp;doi=10.1137/0143035&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref002">
<span class="label">2.</span><cite>Ma C-MC, Paskalev K. In-room CT techniques for image-guided radiation therapy. Med Dosim. 2006;31(1):30–9. doi: 10.1016/j.meddos.2005.12.010

</cite> [<a href="https://doi.org/10.1016/j.meddos.2005.12.010" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16551527/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Dosim&amp;title=In-room%20CT%20techniques%20for%20image-guided%20radiation%20therapy&amp;author=C-MC%20Ma&amp;author=K%20Paskalev&amp;volume=31&amp;issue=1&amp;publication_year=2006&amp;pages=30-9&amp;pmid=16551527&amp;doi=10.1016/j.meddos.2005.12.010&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref003">
<span class="label">3.</span><cite>Gupta R, Walsh C, Wang IS, Kachelrieß M, Kuntz J, Bartling S.
CT-Guided Interventions: Current Practice and Future Directions. In: Jolesz FA, editor. Intraoperative Imaging and Image-Guided Therapy [Internet]. New York, NY: Springer New York; 2014. pp. 173–91. [cited 2024 Feb 19]. Available from: <a href="https://link.springer.com/10.1007/978-1-4614-7657-3_12" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://link.springer.com/10.1007/978-1-4614-7657-3_12</a></cite> [<a href="https://scholar.google.com/scholar_lookup?title=Intraoperative%20Imaging%20and%20Image-Guided%20Therapy%20%5BInternet%5D&amp;author=R%20Gupta&amp;author=C%20Walsh&amp;author=IS%20Wang&amp;author=M%20Kachelrie%C3%9F&amp;author=J%20Kuntz&amp;publication_year=2014&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref004">
<span class="label">4.</span><cite>Eschrich SA, Fulp WJ, Pawitan Y, Foekens JA, Smid M, Martens JWM, et al. Validation of a radiosensitivity molecular signature in breast cancer. Clin Cancer Res. 2012;18(18):5134–43. doi: 10.1158/1078-0432.CCR-12-0891

</cite> [<a href="https://doi.org/10.1158/1078-0432.CCR-12-0891" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3993974/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22832933/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Clin%20Cancer%20Res&amp;title=Validation%20of%20a%20radiosensitivity%20molecular%20signature%20in%20breast%20cancer&amp;author=SA%20Eschrich&amp;author=WJ%20Fulp&amp;author=Y%20Pawitan&amp;author=JA%20Foekens&amp;author=M%20Smid&amp;volume=18&amp;issue=18&amp;publication_year=2012&amp;pages=5134-43&amp;pmid=22832933&amp;doi=10.1158/1078-0432.CCR-12-0891&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref005">
<span class="label">5.</span><cite>Vedantham S, Karellas A, Vijayaraghavan GR, Kopans DB. Digital Breast Tomosynthesis: State of the Art. Radiology. 2015;277(3):663–84. doi: 10.1148/radiol.2015141303

</cite> [<a href="https://doi.org/10.1148/radiol.2015141303" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4666121/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26599926/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Radiology&amp;title=Digital%20Breast%20Tomosynthesis:%20State%20of%20the%20Art&amp;author=S%20Vedantham&amp;author=A%20Karellas&amp;author=GR%20Vijayaraghavan&amp;author=DB%20Kopans&amp;volume=277&amp;issue=3&amp;publication_year=2015&amp;pages=663-84&amp;pmid=26599926&amp;doi=10.1148/radiol.2015141303&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref006">
<span class="label">6.</span><cite>Sechopoulos I. A review of breast tomosynthesis. Part II. Image reconstruction, processing and analysis, and advanced applications. Med Phys. 2013;40(1):014302. doi: 10.1118/1.4770281

</cite> [<a href="https://doi.org/10.1118/1.4770281" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3548896/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23298127/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Phys&amp;title=A%20review%20of%20breast%20tomosynthesis.%20Part%20II.%20Image%20reconstruction,%20processing%20and%20analysis,%20and%20advanced%20applications&amp;author=I%20Sechopoulos&amp;volume=40&amp;issue=1&amp;publication_year=2013&amp;pages=014302&amp;pmid=23298127&amp;doi=10.1118/1.4770281&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref007">
<span class="label">7.</span><cite>Kim K, Ye JC, Worstell W, Ouyang J, Rakvongthai Y, El Fakhri G, et al. Sparse-view spectral CT reconstruction using spectral patch-based low-rank penalty. IEEE Trans Med Imaging. 2015;34(3):748–60. doi: 10.1109/TMI.2014.2380993

</cite> [<a href="https://doi.org/10.1109/TMI.2014.2380993" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25532170/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Med%20Imaging&amp;title=Sparse-view%20spectral%20CT%20reconstruction%20using%20spectral%20patch-based%20low-rank%20penalty&amp;author=K%20Kim&amp;author=JC%20Ye&amp;author=W%20Worstell&amp;author=J%20Ouyang&amp;author=Y%20Rakvongthai&amp;volume=34&amp;issue=3&amp;publication_year=2015&amp;pages=748-60&amp;pmid=25532170&amp;doi=10.1109/TMI.2014.2380993&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref008">
<span class="label">8.</span><cite>Rui X, Cheng L, Long Y, Fu L, Alessio AM, Asma E. Ultra-low dose CT attenuation correction for PET/CT: analysis of sparse view data acquisition and reconstruction algorithms. Phys Med Biol. 2015;60(19):7437–60.
</cite> [<a href="https://doi.org/10.1088/0031-9155/60/19/7437" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5260824/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26352168/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Phys%20Med%20Biol&amp;title=Ultra-low%20dose%20CT%20attenuation%20correction%20for%20PET/CT:%20analysis%20of%20sparse%20view%20data%20acquisition%20and%20reconstruction%20algorithms&amp;author=X%20Rui&amp;author=L%20Cheng&amp;author=Y%20Long&amp;author=L%20Fu&amp;author=AM%20Alessio&amp;volume=60&amp;issue=19&amp;publication_year=2015&amp;pages=7437-60&amp;pmid=26352168&amp;doi=10.1088/0031-9155/60/19/7437&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref009">
<span class="label">9.</span><cite>Meinel FG, Nikolaou K, Weidenhagen R, Hellbach K, Helck A, Bamberg F, et al. Time-resolved CT angiography in aortic dissection. Eur J Radiol. 2012;81(11):3254–61. doi: 10.1016/j.ejrad.2012.03.006

</cite> [<a href="https://doi.org/10.1016/j.ejrad.2012.03.006" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22459348/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eur%20J%20Radiol&amp;title=Time-resolved%20CT%20angiography%20in%20aortic%20dissection&amp;author=FG%20Meinel&amp;author=K%20Nikolaou&amp;author=R%20Weidenhagen&amp;author=K%20Hellbach&amp;author=A%20Helck&amp;volume=81&amp;issue=11&amp;publication_year=2012&amp;pages=3254-61&amp;pmid=22459348&amp;doi=10.1016/j.ejrad.2012.03.006&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref010">
<span class="label">10.</span><cite>Shieh C-C, Kipritidis J, O’Brien RT, Kuncic Z, Keall PJ. Image quality in thoracic 4D cone-beam CT: a sensitivity analysis of respiratory signal, binning method, reconstruction algorithm, and projection angular spacing. Med Phys. 2014;41(4):041912. doi: 10.1118/1.4868510

</cite> [<a href="https://doi.org/10.1118/1.4868510" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3978414/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24694143/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Phys&amp;title=Image%20quality%20in%20thoracic%204D%20cone-beam%20CT:%20a%20sensitivity%20analysis%20of%20respiratory%20signal,%20binning%20method,%20reconstruction%20algorithm,%20and%20projection%20angular%20spacing&amp;author=C-C%20Shieh&amp;author=J%20Kipritidis&amp;author=RT%20O%E2%80%99Brien&amp;author=Z%20Kuncic&amp;author=PJ%20Keall&amp;volume=41&amp;issue=4&amp;publication_year=2014&amp;pages=041912&amp;pmid=24694143&amp;doi=10.1118/1.4868510&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref011">
<span class="label">11.</span><cite>Feldkamp LA, Davis LC, Kress JW. Practical cone-beam algorithm. J Opt Soc Am A. 1984;1(6):612.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Opt%20Soc%20Am%20A&amp;title=Practical%20cone-beam%20algorithm&amp;author=LA%20Feldkamp&amp;author=LC%20Davis&amp;author=JW%20Kress&amp;volume=1&amp;issue=6&amp;publication_year=1984&amp;pages=612&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref012">
<span class="label">12.</span><cite>Gordon R, Bender R, Herman GT. Algebraic reconstruction techniques (ART) for three-dimensional electron microscopy and x-ray photography. J Theor Biol. 1970;29(3):471–81. doi: 10.1016/0022-5193(70)90109-8

</cite> [<a href="https://doi.org/10.1016/0022-5193(70)90109-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/5492997/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Theor%20Biol&amp;title=Algebraic%20reconstruction%20techniques%20(ART)%20for%20three-dimensional%20electron%20microscopy%20and%20x-ray%20photography&amp;author=R%20Gordon&amp;author=R%20Bender&amp;author=GT%20Herman&amp;volume=29&amp;issue=3&amp;publication_year=1970&amp;pages=471-81&amp;pmid=5492997&amp;doi=10.1016/0022-5193(70)90109-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref013">
<span class="label">13.</span><cite>Andersen AH, Kak AC. Simultaneous algebraic reconstruction technique (SART): a superior implementation of the art algorithm. Ultrason Imaging. 1984;6(1):81–94. doi: 10.1177/016173468400600107

</cite> [<a href="https://doi.org/10.1177/016173468400600107" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/6548059/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ultrason%20Imaging&amp;title=Simultaneous%20algebraic%20reconstruction%20technique%20(SART):%20a%20superior%20implementation%20of%20the%20art%20algorithm&amp;author=AH%20Andersen&amp;author=AC%20Kak&amp;volume=6&amp;issue=1&amp;publication_year=1984&amp;pages=81-94&amp;pmid=6548059&amp;doi=10.1177/016173468400600107&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref014">
<span class="label">14.</span><cite>Donoho DL. Compressed sensing. IEEE Trans Inform Theory. 2006;52(4):1289–306. doi: 10.1109/tit.2006.871582</cite> [<a href="https://doi.org/10.1109/tit.2006.871582" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Inform%20Theory&amp;title=Compressed%20sensing&amp;author=DL%20Donoho&amp;volume=52&amp;issue=4&amp;publication_year=2006&amp;pages=1289-306&amp;doi=10.1109/tit.2006.871582&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref015">
<span class="label">15.</span><cite>Sidky EY, Pan X. Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization. Phys Med Biol. 2008;53(17):4777–807.
</cite> [<a href="https://doi.org/10.1088/0031-9155/53/17/021" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC2630711/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/18701771/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Phys%20Med%20Biol&amp;title=Image%20reconstruction%20in%20circular%20cone-beam%20computed%20tomography%20by%20constrained,%20total-variation%20minimization&amp;author=EY%20Sidky&amp;author=X%20Pan&amp;volume=53&amp;issue=17&amp;publication_year=2008&amp;pages=4777-807&amp;pmid=18701771&amp;doi=10.1088/0031-9155/53/17/021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref016">
<span class="label">16.</span><cite>Liu Y, Ma J, Fan Y, Liang Z. Adaptive-weighted total variation minimization for sparse data toward low-dose x-ray computed tomography image reconstruction. Phys Med Biol. 2012;57(23):7923–56. doi: 10.1088/0031-9155/57/23/7923

</cite> [<a href="https://doi.org/10.1088/0031-9155/57/23/7923" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3502686/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23154621/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Phys%20Med%20Biol&amp;title=Adaptive-weighted%20total%20variation%20minimization%20for%20sparse%20data%20toward%20low-dose%20x-ray%20computed%20tomography%20image%20reconstruction&amp;author=Y%20Liu&amp;author=J%20Ma&amp;author=Y%20Fan&amp;author=Z%20Liang&amp;volume=57&amp;issue=23&amp;publication_year=2012&amp;pages=7923-56&amp;pmid=23154621&amp;doi=10.1088/0031-9155/57/23/7923&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref017">
<span class="label">17.</span><cite>Liu Y, Liang Z, Ma J, Lu H, Wang K, Zhang H, et al. Total variation-stokes strategy for sparse-view X-ray CT image reconstruction. IEEE Trans Med Imaging. 2014;33(3):749–63. doi: 10.1109/TMI.2013.2295738

</cite> [<a href="https://doi.org/10.1109/TMI.2013.2295738" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3950963/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24595347/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Med%20Imaging&amp;title=Total%20variation-stokes%20strategy%20for%20sparse-view%20X-ray%20CT%20image%20reconstruction&amp;author=Y%20Liu&amp;author=Z%20Liang&amp;author=J%20Ma&amp;author=H%20Lu&amp;author=K%20Wang&amp;volume=33&amp;issue=3&amp;publication_year=2014&amp;pages=749-63&amp;pmid=24595347&amp;doi=10.1109/TMI.2013.2295738&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref018">
<span class="label">18.</span><cite>Chen GH, Tang J, Leng S. Prior image constrained compressed sensing (PICCS). In: Oraevsky AA, Wang LV, editors. San Jose, CA; 2008. pp. 685618. [cited 2024 Feb 26]. Available from: <a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.770532" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.770532</a></cite> [<a href="https://doi.org/10.1117/12.770532" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC2735051/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19724658/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Prior%20image%20constrained%20compressed%20sensing%20(PICCS)&amp;author=GH%20Chen&amp;author=J%20Tang&amp;author=S%20Leng&amp;author=AA%20Oraevsky&amp;author=LV%20Wang&amp;publication_year=2008&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref019">
<span class="label">19.</span><cite>Lu K, He N, Li L. Nonlocal means-based denoising for medical images. Comput Math Methods Med. 2012;2012:438617. doi: 10.1155/2012/438617

</cite> [<a href="https://doi.org/10.1155/2012/438617" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3291081/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22454694/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Math%20Methods%20Med&amp;title=Nonlocal%20means-based%20denoising%20for%20medical%20images&amp;author=K%20Lu&amp;author=N%20He&amp;author=L%20Li&amp;volume=2012&amp;publication_year=2012&amp;pages=438617&amp;pmid=22454694&amp;doi=10.1155/2012/438617&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref020">
<span class="label">20.</span><cite>Chen Y, Shi L, Feng Q, Yang J, Shu H, Luo L, et al. Artifact suppressed dictionary learning for low-dose CT image processing. IEEE Trans Med Imaging. 2014;33(12):2271–92. doi: 10.1109/TMI.2014.2336860

</cite> [<a href="https://doi.org/10.1109/TMI.2014.2336860" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25029378/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Med%20Imaging&amp;title=Artifact%20suppressed%20dictionary%20learning%20for%20low-dose%20CT%20image%20processing&amp;author=Y%20Chen&amp;author=L%20Shi&amp;author=Q%20Feng&amp;author=J%20Yang&amp;author=H%20Shu&amp;volume=33&amp;issue=12&amp;publication_year=2014&amp;pages=2271-92&amp;pmid=25029378&amp;doi=10.1109/TMI.2014.2336860&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref021">
<span class="label">21.</span><cite>Chen Y, Yang Z, Hu Y, Yang G, Zhu Y, Li Y, et al. Thoracic low-dose CT image processing using an artifact suppressed large-scale nonlocal means. Phys Med Biol. 2012;57(9):2667–88. doi: 10.1088/0031-9155/57/9/2667

</cite> [<a href="https://doi.org/10.1088/0031-9155/57/9/2667" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22504130/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Phys%20Med%20Biol&amp;title=Thoracic%20low-dose%20CT%20image%20processing%20using%20an%20artifact%20suppressed%20large-scale%20nonlocal%20means&amp;author=Y%20Chen&amp;author=Z%20Yang&amp;author=Y%20Hu&amp;author=G%20Yang&amp;author=Y%20Zhu&amp;volume=57&amp;issue=9&amp;publication_year=2012&amp;pages=2667-88&amp;pmid=22504130&amp;doi=10.1088/0031-9155/57/9/2667&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref022">
<span class="label">22.</span><cite>Burger HC, Schuler CJ, Harmeling S. Image denoising: Can plain neural networks compete with BM3D? In: 2012 IEEE Conference on Computer Vision and Pattern Recognition [Internet]. Providence, RI: IEEE; 2012. pp. 2392–9. [cited 2024 Feb 26]. Available from: <a href="http://ieeexplore.ieee.org/document/6247952/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://ieeexplore.ieee.org/document/6247952/</a></cite> [<a href="https://scholar.google.com/scholar_lookup?title=2012%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20%5BInternet%5D&amp;author=HC%20Burger&amp;author=CJ%20Schuler&amp;author=S%20Harmeling&amp;publication_year=2012&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref023">
<span class="label">23.</span><cite>Burger HC, Schuler CJ, Harmeling S. Image denoising with multi-layer perceptrons, part 1: comparison with existing algorithms and with bounds [Internet]. arXiv; 2012.  [cited 2024 Feb 26]. Available from: <a href="http://arxiv.org/abs/1211.1544" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1211.1544</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Image%20denoising%20with%20multi-layer%20perceptrons,%20part%201:%20comparison%20with%20existing%20algorithms%20and%20with%20bounds%20%5BInternet%5D&amp;author=HC%20Burger&amp;author=CJ%20Schuler&amp;author=S%20Harmeling&amp;publication_year=2012&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref024">
<span class="label">24.</span><cite>Prakash P, Kalra MK, Kambadakone AK, Pien H, Hsieh J, Blake MA, et al. Reducing Abdominal CT Radiation Dose With Adaptive Statistical Iterative Reconstruction Technique. Investig Radiol. 2010;45(4):202–10.
</cite> [<a href="https://doi.org/10.1097/RLI.ob013e3181dzfeec" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20177389/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Investig%20Radiol&amp;title=Reducing%20Abdominal%20CT%20Radiation%20Dose%20With%20Adaptive%20Statistical%20Iterative%20Reconstruction%20Technique&amp;author=P%20Prakash&amp;author=MK%20Kalra&amp;author=AK%20Kambadakone&amp;author=H%20Pien&amp;author=J%20Hsieh&amp;volume=45&amp;issue=4&amp;publication_year=2010&amp;pages=202-10&amp;pmid=20177389&amp;doi=10.1097/RLI.ob013e3181dzfeec&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref025">
<span class="label">25.</span><cite>Kang E, Min J, Ye JC. A deep convolutional neural network using directional wavelets for low-dose X-ray CT reconstruction. Med Phys. 2017;44(10):e360-75.</cite> [<a href="https://doi.org/10.1002/mp.12344" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29027238/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Phys&amp;title=A%20deep%20convolutional%20neural%20network%20using%20directional%20wavelets%20for%20low-dose%20X-ray%20CT%20reconstruction&amp;author=E%20Kang&amp;author=J%20Min&amp;author=JC%20Ye&amp;volume=44&amp;issue=10&amp;publication_year=2017&amp;pmid=29027238&amp;doi=10.1002/mp.12344&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref026">
<span class="label">26.</span><cite>Jin KH, McCann MT, Froustey E, Unser M. Deep Convolutional Neural Network for Inverse Problems in Imaging. IEEE Trans Image Process. 2017;26(9):4509–22.
</cite> [<a href="https://doi.org/10.1109/TIP.2017.2713099" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28641250/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Image%20Process&amp;title=Deep%20Convolutional%20Neural%20Network%20for%20Inverse%20Problems%20in%20Imaging&amp;author=KH%20Jin&amp;author=MT%20McCann&amp;author=E%20Froustey&amp;author=M%20Unser&amp;volume=26&amp;issue=9&amp;publication_year=2017&amp;pages=4509-22&amp;pmid=28641250&amp;doi=10.1109/TIP.2017.2713099&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref027">
<span class="label">27.</span><cite>Pelt DM, Batenburg KJ. Improving filtered backprojection reconstruction by data-dependent filtering. IEEE Trans Image Process. 2014;23(11):4750–62. doi: 10.1109/TIP.2014.2341971

</cite> [<a href="https://doi.org/10.1109/TIP.2014.2341971" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25069117/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Image%20Process&amp;title=Improving%20filtered%20backprojection%20reconstruction%20by%20data-dependent%20filtering&amp;author=DM%20Pelt&amp;author=KJ%20Batenburg&amp;volume=23&amp;issue=11&amp;publication_year=2014&amp;pages=4750-62&amp;pmid=25069117&amp;doi=10.1109/TIP.2014.2341971&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref028">
<span class="label">28.</span><cite>Würfl T, Ghesu FC, Christlein V, Maier A.
Deep Learning Computed Tomography. In: Ourselin S, Joskowicz L, Sabuncu MR, Unal G, Wells W, editors. Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016 [Internet]. Lecture Notes in Computer Science. vol. 9902. Cham: Springer International Publishing; 2016. pp. 432–40. [cited 2024 Feb 26]. Available from: <a href="https://link.springer.com/10.1007/978-3-319-46726-9_50" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://link.springer.com/10.1007/978-3-319-46726-9_50</a></cite> [<a href="https://scholar.google.com/scholar_lookup?title=Medical%20Image%20Computing%20and%20Computer-Assisted%20Intervention%20-%20MICCAI%202016%20%5BInternet%5D.%20Lecture%20Notes%20in%20Computer%20Science&amp;author=T%20W%C3%BCrfl&amp;author=FC%20Ghesu&amp;author=V%20Christlein&amp;author=A.%20Maier&amp;author=S%20Ourselin&amp;publication_year=2016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref029">
<span class="label">29.</span><cite>Ma XF, Fukuhara M, Takeda T. Neural network CT image reconstruction method for small amount of projection data. Nucl Instrum Methods Phys Res Section A: Accelerators, Spectrometers Detectors Associated Equipment. 2000;449(1–2):366–77. doi: 10.1016/s0168-9002(99)01453-9</cite> [<a href="https://doi.org/10.1016/s0168-9002(99)01453-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nucl%20Instrum%20Methods%20Phys%20Res%20Section%20A:%20Accelerators,%20Spectrometers%20Detectors%20Associated%20Equipment&amp;title=Neural%20network%20CT%20image%20reconstruction%20method%20for%20small%20amount%20of%20projection%20data&amp;author=XF%20Ma&amp;author=M%20Fukuhara&amp;author=T%20Takeda&amp;volume=449&amp;publication_year=2000&amp;pages=366-77&amp;doi=10.1016/s0168-9002(99)01453-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref030">
<span class="label">30.</span><cite>Li S, Cao Q, Chen Y, Hu Y, Luo L, Toumoulin C. Dictionary learning based sinogram inpainting for CT sparse reconstruction. Optik. 2014;125(12):2862–7. doi: 10.1016/j.ijleo.2014.01.003</cite> [<a href="https://doi.org/10.1016/j.ijleo.2014.01.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Optik&amp;title=Dictionary%20learning%20based%20sinogram%20inpainting%20for%20CT%20sparse%20reconstruction&amp;author=S%20Li&amp;author=Q%20Cao&amp;author=Y%20Chen&amp;author=Y%20Hu&amp;author=L%20Luo&amp;volume=125&amp;issue=12&amp;publication_year=2014&amp;pages=2862-7&amp;doi=10.1016/j.ijleo.2014.01.003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref031">
<span class="label">31.</span><cite>Chen Y, Zhang Y, Shu H, Yang J, Luo L, Coatrieux J-L, et al. Structure-Adaptive Fuzzy Estimation for Random-Valued Impulse Noise Suppression. IEEE Trans Circuits Syst Video Technol. 2018;28(2):414–27. doi: 10.1109/tcsvt.2016.2615444</cite> [<a href="https://doi.org/10.1109/tcsvt.2016.2615444" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Circuits%20Syst%20Video%20Technol&amp;title=Structure-Adaptive%20Fuzzy%20Estimation%20for%20Random-Valued%20Impulse%20Noise%20Suppression&amp;author=Y%20Chen&amp;author=Y%20Zhang&amp;author=H%20Shu&amp;author=J%20Yang&amp;author=L%20Luo&amp;volume=28&amp;issue=2&amp;publication_year=2018&amp;pages=414-27&amp;doi=10.1109/tcsvt.2016.2615444&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref032">
<span class="label">32.</span><cite>Liu J, Ma J, Zhang Y, Chen Y, Yang J, Shu H, et al. Discriminative Feature Representation to Improve Projection Data Inconsistency for Low Dose CT Imaging. IEEE Trans Med Imaging. 2017;36(12):2499–509. doi: 10.1109/TMI.2017.2739841

</cite> [<a href="https://doi.org/10.1109/TMI.2017.2739841" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28816658/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Med%20Imaging&amp;title=Discriminative%20Feature%20Representation%20to%20Improve%20Projection%20Data%20Inconsistency%20for%20Low%20Dose%20CT%20Imaging&amp;author=J%20Liu&amp;author=J%20Ma&amp;author=Y%20Zhang&amp;author=Y%20Chen&amp;author=J%20Yang&amp;volume=36&amp;issue=12&amp;publication_year=2017&amp;pages=2499-509&amp;pmid=28816658&amp;doi=10.1109/TMI.2017.2739841&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref033">
<span class="label">33.</span><cite>Lee H, Lee J, Cho S.
View-interpolation of sparsely sampled sinogram using convolutional neural network. In: Styner MA, Angelini ED, editors. Orlando, Florida, United States; 2017. pp. 1013328. [cited 2024 Feb 26]. Available from: <a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2254244" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2254244</a></cite> [<a href="https://scholar.google.com/scholar_lookup?title=View-interpolation%20of%20sparsely%20sampled%20sinogram%20using%20convolutional%20neural%20network&amp;author=H%20Lee&amp;author=J%20Lee&amp;author=S.%20Cho&amp;author=MA%20Styner&amp;author=ED%20Angelini&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref034">
<span class="label">34.</span><cite>Podgorsak AR, Shiraz Bhurwani MM, Ionita CN. CT artifact correction for sparse and truncated projection data using generative adversarial networks. Med Phys. 2021;48(2):615–26. doi: 10.1002/mp.14504

</cite> [<a href="https://doi.org/10.1002/mp.14504" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32996149/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Phys&amp;title=CT%20artifact%20correction%20for%20sparse%20and%20truncated%20projection%20data%20using%20generative%20adversarial%20networks&amp;author=AR%20Podgorsak&amp;author=MM%20Shiraz%20Bhurwani&amp;author=CN%20Ionita&amp;volume=48&amp;issue=2&amp;publication_year=2021&amp;pages=615-26&amp;pmid=32996149&amp;doi=10.1002/mp.14504&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref035">
<span class="label">35.</span><cite>Sun J, Li H, Xu Z. Deep ADMM-Net for compressive sensing MRI. Adv Neu Inf Process Syst. 2016;29.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neu%20Inf%20Process%20Syst&amp;title=Deep%20ADMM-Net%20for%20compressive%20sensing%20MRI&amp;author=J%20Sun&amp;author=H%20Li&amp;author=Z%20Xu&amp;volume=29&amp;publication_year=2016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref036">
<span class="label">36.</span><cite>Xu Y, Yan H, Ouyang L, Wang J, Zhou L, Cervino L, et al. A method for volumetric imaging in radiotherapy using single x-ray projection. Med Phys. 2015;42(5):2498–509. doi: 10.1118/1.4918577

</cite> [<a href="https://doi.org/10.1118/1.4918577" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4409629/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25979043/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Phys&amp;title=A%20method%20for%20volumetric%20imaging%20in%20radiotherapy%20using%20single%20x-ray%20projection&amp;author=Y%20Xu&amp;author=H%20Yan&amp;author=L%20Ouyang&amp;author=J%20Wang&amp;author=L%20Zhou&amp;volume=42&amp;issue=5&amp;publication_year=2015&amp;pages=2498-509&amp;pmid=25979043&amp;doi=10.1118/1.4918577&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref037">
<span class="label">37.</span><cite>Hrinivich WT, Chernavsky NE, Morcos M, Li T, Wu P, Wong J, et al. Effect of subject motion and gantry rotation speed on image quality and dose delivery in CT-guided radiotherapy. Med Phys. 2022;49(11):6840–55. doi: 10.1002/mp.15877

</cite> [<a href="https://doi.org/10.1002/mp.15877" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35880711/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Phys&amp;title=Effect%20of%20subject%20motion%20and%20gantry%20rotation%20speed%20on%20image%20quality%20and%20dose%20delivery%20in%20CT-guided%20radiotherapy&amp;author=WT%20Hrinivich&amp;author=NE%20Chernavsky&amp;author=M%20Morcos&amp;author=T%20Li&amp;author=P%20Wu&amp;volume=49&amp;issue=11&amp;publication_year=2022&amp;pages=6840-55&amp;pmid=35880711&amp;doi=10.1002/mp.15877&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref038">
<span class="label">38.</span><cite>Xu D, Descovich M, Liu H, Lao Y, Gottschalk AR, Sheng K. Deep match: A zero-shot framework for improved fiducial-free respiratory motion tracking. Radiother Oncol. 2024;194:110179. doi: 10.1016/j.radonc.2024.110179

</cite> [<a href="https://doi.org/10.1016/j.radonc.2024.110179" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38403025/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Radiother%20Oncol&amp;title=Deep%20match:%20A%20zero-shot%20framework%20for%20improved%20fiducial-free%20respiratory%20motion%20tracking&amp;author=D%20Xu&amp;author=M%20Descovich&amp;author=H%20Liu&amp;author=Y%20Lao&amp;author=AR%20Gottschalk&amp;volume=194&amp;publication_year=2024&amp;pages=110179&amp;pmid=38403025&amp;doi=10.1016/j.radonc.2024.110179&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref039">
<span class="label">39.</span><cite>Schafer S, Siewerdsen JH. Technology and applications in interventional imaging: 2D X-ray radiography/fluoroscopy and 3D cone-beam CT. Handbook of Medical Image Computing and Computer Assisted Intervention. Elsevier; 2020. pp. 625–71.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Handbook%20of%20Medical%20Image%20Computing%20and%20Computer%20Assisted%20Intervention&amp;author=S%20Schafer&amp;author=JH%20Siewerdsen&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref040">
<span class="label">40.</span><cite>Shen L, Zhao W, Xing L. Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning. Nat Biomed Eng. 2019;3(11):880–8.
</cite> [<a href="https://doi.org/10.1038/s41551-019-0466-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6858583/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31659306/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat%20Biomed%20Eng&amp;title=Patient-specific%20reconstruction%20of%20volumetric%20computed%20tomography%20images%20from%20a%20single%20projection%20view%20via%20deep%20learning&amp;author=L%20Shen&amp;author=W%20Zhao&amp;author=L%20Xing&amp;volume=3&amp;issue=11&amp;publication_year=2019&amp;pages=880-8&amp;pmid=31659306&amp;doi=10.1038/s41551-019-0466-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref041">
<span class="label">41.</span><cite>Ying X, Guo H, Ma K, Wu J, Weng Z, Zheng Y. X2CT-GAN: Reconstructing CT from Biplanar X-Rays with Generative Adversarial Networks. arXiv. 2019. [cited 2024 Feb 15]. <a href="http://arxiv.org/abs/1905.06902" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1905.06902</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=X2CT-GAN:%20Reconstructing%20CT%20from%20Biplanar%20X-Rays%20with%20Generative%20Adversarial%20Networks&amp;author=X%20Ying&amp;author=H%20Guo&amp;author=K%20Ma&amp;author=J%20Wu&amp;author=Z%20Weng&amp;publication_year=2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref042">
<span class="label">42.</span><cite>Schwarz K, Liao Y, Niemeyer M, Geiger A. GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis. arXiv. 2021. [cited 2024 Feb 21]. <a href="http://arxiv.org/abs/2007.02442" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2007.02442</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=GRAF:%20Generative%20Radiance%20Fields%20for%203D-Aware%20Image%20Synthesis&amp;author=K%20Schwarz&amp;author=Y%20Liao&amp;author=M%20Niemeyer&amp;author=A%20Geiger&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref043">
<span class="label">43.</span><cite>Shen D, Wu G, Suk H-I. Deep Learning in Medical Image Analysis. Annu Rev Biomed Eng. 2017;19:221–48. doi: 10.1146/annurev-bioeng-071516-044442

</cite> [<a href="https://doi.org/10.1146/annurev-bioeng-071516-044442" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5479722/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28301734/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Annu%20Rev%20Biomed%20Eng&amp;title=Deep%20Learning%20in%20Medical%20Image%20Analysis&amp;author=D%20Shen&amp;author=G%20Wu&amp;author=H-I%20Suk&amp;volume=19&amp;publication_year=2017&amp;pages=221-48&amp;pmid=28301734&amp;doi=10.1146/annurev-bioeng-071516-044442&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref044">
<span class="label">44.</span><cite>Mildenhall B, Srinivasan PP, Tancik M, Barron JT, Ramamoorthi R, Ng R. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. arXiv. 2020. [cited 2024 Feb 20]. <a href="http://arxiv.org/abs/2003.08934" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2003.08934</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=NeRF:%20Representing%20Scenes%20as%20Neural%20Radiance%20Fields%20for%20View%20Synthesis&amp;author=B%20Mildenhall&amp;author=PP%20Srinivasan&amp;author=M%20Tancik&amp;author=JT%20Barron&amp;author=R%20Ramamoorthi&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref045">
<span class="label">45.</span><cite>Isola P, Zhu JY, Zhou T, Efros AA. Image-to-Image Translation with Conditional Adversarial Networks. 2016.  [cited 2023 Sep 4]; Available from: <a href="https://arxiv.org/abs/1611.07004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1611.07004</a></cite> [<a href="https://scholar.google.com/scholar_lookup?Isola%20P,%20Zhu%20JY,%20Zhou%20T,%20Efros%20AA.%20Image-to-Image%20Translation%20with%20Conditional%20Adversarial%20Networks.%202016.%20%5Bcited%202023%20Sep%204%5D;%20Available%20from:%20https://arxiv.org/abs/1611.07004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref046">
<span class="label">46.</span><cite>Corona-Figueroa A, Frawley J, Taylor SB, Bethapudi S, Shum HPH, Willcocks CG. MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray. In: 2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC) [Internet]. Glasgow, Scotland, United Kingdom: IEEE; 2022. pp. 3843–8. [cited 2024 Feb 15]. Available from: <a href="https://ieeexplore.ieee.org/document/9871757/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/document/9871757/</a></cite> [<a href="https://doi.org/10.1109/EMBC48229.2022.9871757" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36085823/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?title=2022%2044th%20Annual%20International%20Conference%20of%20the%20IEEE%20Engineering%20in%20Medicine%20&amp;%20Biology%20Society%20(EMBC)%20%5BInternet%5D&amp;author=A%20Corona-Figueroa&amp;author=J%20Frawley&amp;author=SB%20Taylor&amp;author=S%20Bethapudi&amp;author=HPH%20Shum&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref047">
<span class="label">47.</span><cite>Siddon RL. Calculation of the radiological depth: Technical Reports: Calculation of the radiological depth. Med Phys. 1985;12(1):84–7.
</cite> [<a href="https://doi.org/10.1118/1.595739" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/3974530/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Phys&amp;title=Calculation%20of%20the%20radiological%20depth:%20Technical%20Reports:%20Calculation%20of%20the%20radiological%20depth&amp;author=RL%20Siddon&amp;volume=12&amp;issue=1&amp;publication_year=1985&amp;pages=84-7&amp;pmid=3974530&amp;doi=10.1118/1.595739&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref048">
<span class="label">48.</span><cite>Liu B, Zhu Y, Song K, Elgammal A. Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis [Internet]. arXiv; 2021.  [cited 2024 Feb 21]. Available from: <a href="http://arxiv.org/abs/2101.04775" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2101.04775</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Towards%20Faster%20and%20Stabilized%20GAN%20Training%20for%20High-fidelity%20Few-shot%20Image%20Synthesis%20%5BInternet%5D&amp;author=B%20Liu&amp;author=Y%20Zhu&amp;author=K%20Song&amp;author=A%20Elgammal&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref049">
<span class="label">49.</span><cite>Zhang R, Isola P, Efros AA, Shechtman E, Wang O. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric [Internet]. arXiv; 2018. [cited 2024 Feb 21]. Available from: <a href="http://arxiv.org/abs/1801.03924" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1801.03924</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=The%20Unreasonable%20Effectiveness%20of%20Deep%20Features%20as%20a%20Perceptual%20Metric%20%5BInternet%5D&amp;author=R%20Zhang&amp;author=P%20Isola&amp;author=AA%20Efros&amp;author=E%20Shechtman&amp;author=O%20Wang&amp;publication_year=2018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref050">
<span class="label">50.</span><cite>Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv. 2015. doi: arXiv:1409.1556</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Very%20Deep%20Convolutional%20Networks%20for%20Large-Scale%20Image%20Recognition&amp;author=K%20Simonyan&amp;author=A%20Zisserman&amp;publication_year=2015&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref051">
<span class="label">51.</span><cite>Tran N-T, Tran V-H, Nguyen N-B, Nguyen T-K, Cheung N-M. On Data Augmentation for GAN Training. IEEE Trans Image Process. 2021;30:1882–97. doi: 10.1109/TIP.2021.3049346

</cite> [<a href="https://doi.org/10.1109/TIP.2021.3049346" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33428571/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Image%20Process&amp;title=On%20Data%20Augmentation%20for%20GAN%20Training&amp;author=N-T%20Tran&amp;author=V-H%20Tran&amp;author=N-B%20Nguyen&amp;author=T-K%20Nguyen&amp;author=N-M%20Cheung&amp;volume=30&amp;publication_year=2021&amp;pages=1882-97&amp;pmid=33428571&amp;doi=10.1109/TIP.2021.3049346&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref052">
<span class="label">52.</span><cite>Kingma DP, Welling M. Auto-Encoding Variational Bayes [Internet]. arXiv; 2022.  [cited 2024 Feb 22]. Available from: <a href="http://arxiv.org/abs/1312.6114" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1312.6114</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Auto-Encoding%20Variational%20Bayes%20%5BInternet%5D&amp;author=DP%20Kingma&amp;author=M%20Welling&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref053">
<span class="label">53.</span><cite>Armato SG 3rd, McLennan G, Bidaut L, McNitt-Gray MF, Meyer CR, Reeves AP, et al. The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans. Med Phys. 2011;38(2):915–31. doi: 10.1118/1.3528204

</cite> [<a href="https://doi.org/10.1118/1.3528204" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3041807/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21452728/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Phys&amp;title=The%20Lung%20Image%20Database%20Consortium%20(LIDC)%20and%20Image%20Database%20Resource%20Initiative%20(IDRI):%20a%20completed%20reference%20database%20of%20lung%20nodules%20on%20CT%20scans&amp;author=SG%203rd%20Armato&amp;author=G%20McLennan&amp;author=L%20Bidaut&amp;author=MF%20McNitt-Gray&amp;author=CR%20Meyer&amp;volume=38&amp;issue=2&amp;publication_year=2011&amp;pages=915-31&amp;pmid=21452728&amp;doi=10.1118/1.3528204&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref054">
<span class="label">54.</span><cite>Li R, Mok E, Chang DT, Daly M, Loo BW, Diehn M. Intrafraction verification of gated RapidArc by using beam-level kilovoltage X-ray images. Int J Radiat Oncol Biol Phys. 2012;83(5):e709–715.</cite> [<a href="https://doi.org/10.1016/j.ijrobp.2012.03.006" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4476315/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22554582/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Radiat%20Oncol%20Biol%20Phys&amp;title=Intrafraction%20verification%20of%20gated%20RapidArc%20by%20using%20beam-level%20kilovoltage%20X-ray%20images&amp;author=R%20Li&amp;author=E%20Mok&amp;author=DT%20Chang&amp;author=M%20Daly&amp;author=BW%20Loo&amp;volume=83&amp;issue=5&amp;publication_year=2012&amp;pmid=22554582&amp;doi=10.1016/j.ijrobp.2012.03.006&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref055">
<span class="label">55.</span><cite>Haubenreisser H, Bigdeli A, Meyer M, Kremer T, Riester T, Kneser U, et al. From 3D to 4D: Integration of temporal information into CT angiography studies. Eur J Radiol. 2015;84(12):2421–4. doi: 10.1016/j.ejrad.2015.06.014

</cite> [<a href="https://doi.org/10.1016/j.ejrad.2015.06.014" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26152869/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eur%20J%20Radiol&amp;title=From%203D%20to%204D:%20Integration%20of%20temporal%20information%20into%20CT%20angiography%20studies&amp;author=H%20Haubenreisser&amp;author=A%20Bigdeli&amp;author=M%20Meyer&amp;author=T%20Kremer&amp;author=T%20Riester&amp;volume=84&amp;issue=12&amp;publication_year=2015&amp;pages=2421-4&amp;pmid=26152869&amp;doi=10.1016/j.ejrad.2015.06.014&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref056">
<span class="label">56.</span><cite>Keil F, Bergkemper A, Birkhold A, Kowarschik M, Tritt S, Berkefeld J. 4D Flat Panel Conebeam CTA for Analysis of the Angioarchitecture of Cerebral AVMs with a Novel Software Prototype. AJNR Am J Neuroradiol. 2022;43(1):102–9. doi: 10.3174/ajnr.A7382

</cite> [<a href="https://doi.org/10.3174/ajnr.A7382" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8757557/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35027345/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=AJNR%20Am%20J%20Neuroradiol&amp;title=4D%20Flat%20Panel%20Conebeam%20CTA%20for%20Analysis%20of%20the%20Angioarchitecture%20of%20Cerebral%20AVMs%20with%20a%20Novel%20Software%20Prototype&amp;author=F%20Keil&amp;author=A%20Bergkemper&amp;author=A%20Birkhold&amp;author=M%20Kowarschik&amp;author=S%20Tritt&amp;volume=43&amp;issue=1&amp;publication_year=2022&amp;pages=102-9&amp;pmid=35027345&amp;doi=10.3174/ajnr.A7382&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref057">
<span class="label">57.</span><cite>Lewis BC, Snyder WJ, Kim S, Kim T. Monitoring frequency of intra-fraction patient motion using the ExacTrac system for LINAC-based SRS treatments. J Appl Clin Med Phys. 2018;19(3):58–63. doi: 10.1002/acm2.12279

</cite> [<a href="https://doi.org/10.1002/acm2.12279" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5978384/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29577592/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Appl%20Clin%20Med%20Phys&amp;title=Monitoring%20frequency%20of%20intra-fraction%20patient%20motion%20using%20the%20ExacTrac%20system%20for%20LINAC-based%20SRS%20treatments&amp;author=BC%20Lewis&amp;author=WJ%20Snyder&amp;author=S%20Kim&amp;author=T%20Kim&amp;volume=19&amp;issue=3&amp;publication_year=2018&amp;pages=58-63&amp;pmid=29577592&amp;doi=10.1002/acm2.12279&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref058">
<span class="label">58.</span><cite>Kilby W, Dooley JR, Kuduvalli G, Sayeh S, Maurer CR. The CyberKnife Robotic Radiosurgery System in 2010. Technol Cancer Res Treat. 2010;9(5):433–52.
</cite> [<a href="https://doi.org/10.1177/153303461000900502" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20815415/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Technol%20Cancer%20Res%20Treat&amp;title=The%20CyberKnife%20Robotic%20Radiosurgery%20System%20in%202010&amp;author=W%20Kilby&amp;author=JR%20Dooley&amp;author=G%20Kuduvalli&amp;author=S%20Sayeh&amp;author=CR%20Maurer&amp;volume=9&amp;issue=5&amp;publication_year=2010&amp;pages=433-52&amp;pmid=20815415&amp;doi=10.1177/153303461000900502&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref059">
<span class="label">59.</span><cite>Keall PJ, Sawant A, Berbeco RI, Booth JT, Cho B, Cerviño LI, et al. AAPM Task Group 264: The safe clinical implementation of MLC tracking in radiotherapy. Med Phys. 2021;48(5):e44–64. doi: 10.1002/mp.14625

</cite> [<a href="https://doi.org/10.1002/mp.14625" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33260251/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med%20Phys&amp;title=AAPM%20Task%20Group%20264:%20The%20safe%20clinical%20implementation%20of%20MLC%20tracking%20in%20radiotherapy&amp;author=PJ%20Keall&amp;author=A%20Sawant&amp;author=RI%20Berbeco&amp;author=JT%20Booth&amp;author=B%20Cho&amp;volume=48&amp;issue=5&amp;publication_year=2021&amp;pmid=33260251&amp;doi=10.1002/mp.14625&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref060">
<span class="label">60.</span><cite>Han S, Pool J, Tran J, Dally WJ. Learning both Weights and Connections for Efficient Neural Networks [Internet]. arXiv; 2015.  [cited 2025 Mar 17]. Available from: <a href="http://arxiv.org/abs/1506.02626" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1506.02626</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Learning%20both%20Weights%20and%20Connections%20for%20Efficient%20Neural%20Networks%20%5BInternet%5D&amp;author=S%20Han&amp;author=J%20Pool&amp;author=J%20Tran&amp;author=WJ%20Dally&amp;publication_year=2015&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref061">
<span class="label">61.</span><cite>Jacob B, Kligys S, Chen B, Zhu M, Tang M, Howard A, et al. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [Internet]. arXiv; 2017.  [cited 2025 Mar 17]. Available from: <a href="http://arxiv.org/abs/1712.05877" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1712.05877</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Quantization%20and%20Training%20of%20Neural%20Networks%20for%20Efficient%20Integer-Arithmetic-Only%20Inference%20%5BInternet%5D&amp;author=B%20Jacob&amp;author=S%20Kligys&amp;author=B%20Chen&amp;author=M%20Zhu&amp;author=M%20Tang&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref062">
<span class="label">62.</span><cite>Shafi O, Rai C, Sen R, Ananthanarayanan G. Demystifying TensorRT: Characterizing Neural Network Inference Engine on Nvidia Edge Devices. In: 2021 IEEE International Symposium on Workload Characterization (IISWC) [Internet]. Storrs, CT, USA: IEEE; 2021. pp. 226–37. [cited 2025 Mar 17]. Available from: <a href="https://ieeexplore.ieee.org/document/9668285/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/document/9668285/</a></cite> [<a href="https://scholar.google.com/scholar_lookup?title=Demystifying%20TensorRT:%20Characterizing%20Neural%20Network%20Inference%20Engine%20on%20Nvidia%20Edge%20Devices.%20In:%202021%20IEEE%20International%20Symposium%20on%20Workload%20Characterization%20(IISWC)%20%5BInternet%5D&amp;author=O%20Shafi&amp;author=C%20Rai&amp;author=R%20Sen&amp;author=G%20Ananthanarayanan&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref063">
<span class="label">63.</span><cite>Nurvitadhi E, Venkatesh G, Sim J, Marr D, Huang R, Ong Gee Hock J, et al. Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Neural Networks? In: Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. Monterey California USA: ACM; 2017. pp. 5–14. doi: 10.1145/3020078.3021740</cite> [<a href="https://doi.org/10.1145/3020078.3021740" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%202017%20ACM/SIGDA%20International%20Symposium%20on%20Field-Programmable%20Gate%20Arrays.%20Monterey%20California%20USA:%20ACM&amp;title=Can%20FPGAs%20Beat%20GPUs%20in%20Accelerating%20Next-Generation%20Deep%20Neural%20Networks?&amp;author=E%20Nurvitadhi&amp;author=G%20Venkatesh&amp;author=J%20Sim&amp;author=D%20Marr&amp;author=R%20Huang&amp;publication_year=2017&amp;pages=5-14&amp;doi=10.1145/3020078.3021740&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref064">
<span class="label">64.</span><cite>Jouppi NP, Young C, Patil N, Patterson D, Agrawal G, Bajwa R, et al. In-Datacenter Performance Analysis of a Tensor Processing Unit. In: Proceedings of the 44th Annual International Symposium on Computer Architecture. Toronto ON Canada: ACM; 2017. pp. 1–12. [cited 2025 Mar 17]. doi: 10.1145/3079856.3080246</cite> [<a href="https://doi.org/10.1145/3079856.3080246" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%2044th%20Annual%20International%20Symposium%20on%20Computer%20Architecture.%20Toronto%20ON%20Canada:%20ACM&amp;title=In-Datacenter%20Performance%20Analysis%20of%20a%20Tensor%20Processing%20Unit.&amp;author=NP%20Jouppi&amp;author=C%20Young&amp;author=N%20Patil&amp;author=D%20Patterson&amp;author=G%20Agrawal&amp;publication_year=2017&amp;pages=1-12&amp;doi=10.1145/3079856.3080246&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref065">
<span class="label">65.</span><cite>Sitzmann V, Martel JNP, Bergman AW, Lindell DB, Wetzstein G. Implicit Neural Representations with Periodic Activation Functions. arXiv. 2020. [cited 2024 Oct 3]. <a href="http://arxiv.org/abs/2006.09661" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2006.09661</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Implicit%20Neural%20Representations%20with%20Periodic%20Activation%20Functions&amp;author=V%20Sitzmann&amp;author=JNP%20Martel&amp;author=AW%20Bergman&amp;author=DB%20Lindell&amp;author=G%20Wetzstein&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref066">
<span class="label">66.</span><cite>Müller T, Evans A, Schied C, Keller A. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans Graph. 2022;41(4):1–15. doi: 10.1145/3528223.3530127</cite> [<a href="https://doi.org/10.1145/3528223.3530127" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Trans%20Graph&amp;title=Instant%20neural%20graphics%20primitives%20with%20a%20multiresolution%20hash%20encoding&amp;author=T%20M%C3%BCller&amp;author=A%20Evans&amp;author=C%20Schied&amp;author=A%20Keller&amp;volume=41&amp;issue=4&amp;publication_year=2022&amp;pages=1-15&amp;doi=10.1145/3528223.3530127&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref067">
<span class="label">67.</span><cite>Hedman P, Srinivasan PP, Mildenhall B, Barron JT, Debevec P. Baking Neural Radiance Fields for Real-Time View Synthesis [Internet]. arXiv; 2021.  [cited 2025 Mar 17]. Available from: <a href="http://arxiv.org/abs/2103.14645" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2103.14645</a></cite> [<a href="https://doi.org/10.1109/TPAMI.2024.3381001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38526902/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Hedman%20P,%20Srinivasan%20PP,%20Mildenhall%20B,%20Barron%20JT,%20Debevec%20P.%20Baking%20Neural%20Radiance%20Fields%20for%20Real-Time%20View%20Synthesis%20%5BInternet%5D.%20arXiv;%202021.%20%5Bcited%202025%20Mar%2017%5D.%20Available%20from:%20http://arxiv.org/abs/2103.14645" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330463.ref068">
<span class="label">68.</span><cite>Patrício C, Neves JC, Teixeira LF. Explainable deep learning methods in medical image classification: a survey. ACM Comput Surv. 2024;56(4):1–41.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Comput%20Surv&amp;title=Explainable%20deep%20learning%20methods%20in%20medical%20image%20classification:%20a%20survey&amp;author=C%20Patr%C3%ADcio&amp;author=JC%20Neves&amp;author=LF%20Teixeira&amp;volume=56&amp;issue=4&amp;publication_year=2024&amp;pages=1-41&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section></section><article class="sub-article" id="pone.0330463.r001"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330463.r001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330463.r001</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 0</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Zhentian Wang</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Zhentian Wang</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zhentian Wang</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.c" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.c" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.c" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Zhentian Wang</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.c" class="d-panel p" style="display: none">
<div>© 2025 Zhentian Wang</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>13 Jun 2025</em>
</p>
<p>PONE-D-25-15335TomoGRAF: An X-Ray Physics-Driven Generative Radiance Field Framework for Extremely Sparse View CT ReconstructionPLOS ONE</p>
<p>Dear Dr. Sheng,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful evaluation, the reviewers raised some relevant concerns that need to be addressed first. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
<p>Please submit your revised manuscript by Jul 28 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span> . When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<p>Please include the following items when submitting your revised manuscript:</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a> .</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Zhentian Wang, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>
<strong>Journal Requirements:</strong>
</p>
<p>1. When submitting your revision, we need you to address these additional requirements. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at <a href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</a> and <a href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</a> 2. Thank you for stating in your Funding Statement: The research is supported by NIH R01CA259008, R44CA183390 and R01EB031577.  Please provide an amended statement that declares *all* the funding or sources of support (whether external or internal to your organization) received during this study, as detailed online in our guide for authors at <a href="http://journals.plos.org/plosone/s/submit-now" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://journals.plos.org/plosone/s/submit-now. </a> Please also include the statement “There was no additional external funding received for this study.” in your updated Funding Statement. Please include your amended Funding Statement within your cover letter. We will change the online submission form on your behalf. 3. Thank you for stating the following in the Acknowledgments Section of your manuscript: The research is supported by NIH R01CA259008, R44CA183390 and R01EB031577. We note that you have provided funding information that is not currently declared in your Funding Statement. However, funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form. Please remove any funding-related text from the manuscript and let us know how you would like to update your Funding Statement. Currently, your Funding Statement reads as follows: The research is supported by NIH R01CA259008, R44CA183390 and R01EB031577.  Please include your amended statements within your cover letter; we will change the online submission form on your behalf. 4. We note that you have indicated that there are restrictions to data sharing for this study. For studies involving human research participant data or other sensitive data, we encourage authors to share de-identified or anonymized data. However, when data cannot be publicly shared for ethical reasons, we allow authors to make their data sets available upon request. For information on unacceptable data access restrictions, please see <a href="http://journals.plos.org/plosone/s/data-availability#loc-unacceptable-data-access-restrictions.%C2%A0%C2%A0Before" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://journals.plos.org/plosone/s/data-availability#loc-unacceptable-data-access-restrictions.  Before</a> we proceed with your manuscript, please address the following prompts: a) If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially identifying or sensitive patient information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., a Research Ethics Committee or Institutional Review Board, etc.). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent. b) If there are no restrictions, please upload the minimal anonymized data set necessary to replicate your study findings to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. Please see <a href="http://www.bmj.com/content/340/bmj.c181.long" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.bmj.com/content/340/bmj.c181.long</a> for guidelines on how to de-identify and prepare clinical data for publication. For a list of recommended repositories, please see <a href="https://journals.plos.org/plosone/s/recommended-repositories" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/recommended-repositories</a>. You also have the option of uploading the data as Supporting Information files, but we would recommend depositing data directly to a data repository if possible. Please update your Data Availability statement in the submission form accordingly. 5. Please include your full ethics statement in the ‘Methods’ section of your manuscript file. In your statement, please include the full name of the IRB or ethics committee who approved or waived your study, as well as whether or not you obtained informed written or verbal consent. If consent was waived for your study, please include this information in your statement as well.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>
<strong>Comments to the Author</strong>
</p>
<p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>2. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: N/A</p>
<p>**********</p>
<p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>5. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p><strong>Reviewer #1:</strong>  This article describes a machine learning framework to allow for 3D medical image reconstruction from limited angle viewing data. While my background includes experience with NeRF and related techniques, I do not have a medical background. Nonetheless, I enjoyed reading this article, understood its significance, and am pleased to recommend it for publication. I have a few comments that, if properly addressed, will substantially improve the paper:</p>
<p>1) My understanding of the method, as described in Sec. II, relied heavily on reading the background literature related to GRAF, MedNeRF and X2CT-GAN. I highly recommend that the authors consider creating some additional illustrations to complement the architecture shown in Figure 1. In particular, the details of the Generator and Discriminator and the physical meaning of the varying inputs was difficult to follow without directly referencing other papers. I also suspect that Figure 1 has some typos, such as the input to the Generator.</p>
<p>2) Overall, the writing was good and generally easy to read; however, the manuscript would benefit from another revision. There are still several typos and grammatical errors that would benefit from proofreading.</p>
<p>3) Should Eq 16 be RMSE, not SSIM?</p>
<p><strong>Reviewer #2: </strong> The authors present TomoGRAF, an innovative approach leveraging ultra-sparse projections to achieve high-quality 3D CT volume reconstruction, marking a significant advancement in X-ray physics and CT imaging. This work introduces the first universal framework for image-guided radiotherapy and interventional radiology, offering substantial clinical utility. While the manuscript demonstrates considerable promise for publication in PLOS ONE, the authors should address the following points prior to final acceptance.</p>
<p>1. Most NeRF methods rely on 2D supervision, while this paper mentions using 3D CT as the supervision signal, but does not explicitly explain how to ensure the effectiveness of 3D supervision during training. How can 3D supervision improve fidelity? It is suggested to add relevant ablation experiments or analysis to demonstrate the specific impact of 3D supervision on the final reconstruction quality.</p>
<p>2. The paper claims to model X-ray attenuation, but it is unclear whether scattering, beam hardening, or noise were considered. These factors are crucial for real CT simulations.</p>
<p>3. The paper only mentions two data augmentation techniques - random flipping and rotation. Were other data augmentation methods employed, such as noise injection or varying SID/SAD, to improve generalization capability?</p>
<p>4. For the loss functions in Equation (11) and Equation (12), how were the parameters α and β determined? Were ablation experiments conducted to ensure they are optimal values? For the loss function in Equation (13), how were the parameters γ, δ, and ε determined? And how were θ and ϕ determined in the evaluation metrics?</p>
<p>5. In the experiments, why was the peak signal-to-noise ratio (PSNR) = 25 set as the stopping threshold? Why not choose a higher PSNR value?</p>
<p>6. During the experimental process, your model's baseline performance was established using a single AP view. To determine model performance, reconstructions were additionally performed with 1, 2, 5, and 10 views. The view angles were specified as follows: for 1-view reconstruction, the AP view was used as reference; for 2-view reconstruction, AP and lateral views were used for inference; for 5-view reconstruction, starting from the AP view, every 72° rotation was applied to cover the full 360°; for 10-view reconstruction, starting from the AP view, every 36° rotation was applied to cover the full 360°. However, in the Discussion section, the statement "Meanwhile, TomoGRAF, besides 1-view referencing, can leverage additional X-ray views at arbitrary angles" appears to lack strong experimental support, since the experimental procedure in this study clearly defined view angles with uniform angular intervals, which seems inconsistent with "arbitrary angles". We recommend either: (1) adding experiments to demonstrate the value of arbitrary angles (e.g., using only the AP view and its adjacent angles) for TomoGRAF during inference, or (2) rephrasing this statement to better align with the actual experimental conditions.</p>
<p>7. TomoGRAF requires fine-tuning during the inference phase. For specific fine-tuning, the trained prior model is optimized under the supervision of the patient's 2D sparse view projections to adapt to new patient anatomies. Does this mean that during the inference phase, fine-tuning is required for projection images from every angle used? If so, this would lead to increased inference time, and when more views are used for inference, the time would increase accordingly. The paper mentions that single-view reconstruction takes approximately 344 seconds, while each additional view roughly doubles the reconstruction time, which appears relatively slow for practical applications. Can the network be improved to enhance its computational efficiency?</p>
<p>8. In the quantitative analysis, this paper employed SSIM, PSNR, and RMSE metrics, but it did not thoroughly discuss in the qualitative analysis whether these metrics can reflect clinical diagnostic requirements. In clinical applications, the accuracy of CT images is crucial. Have you considered inviting clinicians to evaluate the generated predicted CT images and assess their feasibility?</p>
<p>9. The pseudocode of the Siddon algorithm (Appendix 1) serves as an important methodological supplement. Please further clarify its specific implementation details in TomoGRAF, such as how it integrates with the fully connected network - does the network directly learn attenuation coefficients, or is this achieved through post-processing?</p>
<p>**********</p>
<p>6. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a> ). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a> .</p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span> . Please note that Supporting Information files do not need this step.</p>
<section class="sm xbox font-sm" id="pone.0330463.s002"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Comments.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373210/bin/pone.0330463.s002.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330463.s002.docx</a><sup> (14.9KB, docx) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0330463.r002"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 22;20(8):e0330463. doi: <a href="https://doi.org/10.1371/journal.pone.0330463.r002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330463.r002</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 1</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.d" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.d" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.d" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub2" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.d" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>3 Jul 2025</em>
</p>
<p>Q: Most NeRF methods rely on 2D supervision, while this paper mentions using 3D CT as the supervision signal but does not explicitly explain how to ensure the effectiveness of 3D supervision during training. How can 3D supervision improve fidelity? It is suggested to add relevant ablation experiments or analysis to demonstrate the specific impact of 3D supervision on the final reconstruction quality.</p>
<p>A: We thank the reviewer for highlighting the importance of analyzing the effect of 3D supervision. Compared to prior NeRF-based approaches such as MedNeRF, our TomoGRAF framework introduces two major enhancements to improve reconstruction fidelity:</p>
<p>a more physically accurate forward model using Siddon’s ray tracing, and</p>
<p>the incorporation of 3D supervision from paired CT volumes during training.</p>
<p>These two design choices work in tandem to enhance anatomical consistency, especially under ultra-sparse-view constraints. Since our model ultimately generates 3D volumes at inference time, incorporating 3D supervision during training is both intuitive and effective—it aligns the learning objective with the test-time task and enables the model to learn more robust and structured anatomical priors.</p>
<p>We agree that many existing NeRF-based medical imaging frameworks rely on 2D supervision primarily due to the unavailability of paired 3D data for objects viewed under natural lights, rather than by design choice. In contrast, TomoGRAF is trained on paired 2D–3D data, giving the unique advantage of X-ray imaging in visualizing complex 3D anatomical structures.</p>
<p>While we agree that an ablation study isolating the impact of 3D supervision would be informative, the current manuscript, which is already lengthy, includes extensive evaluations across various sparse-view configurations. To maintain focus and clarity, we limited the scope to demonstrating the feasibility and effectiveness of the proposed framework. A deeper exploration of supervision strategies and architectural contributions should belong to a separate future study.</p>
<p>Q: The paper claims to model X-ray attenuation, but it is unclear whether scattering, beam hardening, or noise were considered. These factors are crucial for real CT simulations.</p>
<p>A: We thank the reviewer for raising this important point. Our current implementation of the forward model uses Siddon’s ray tracing algorithm, which simulates X-ray attenuation as line integrals through a voxelized volume. This approach assumes a monochromatic source and neglects scattering, beam hardening, and noise. While this simplified model is commonly used in simulation studies due to its computational efficiency and clarity, we agree that these effects are relevant to actual image quality. Future work will incorporate more realistic forward models that simulate polychromatic spectra, scatter, and noise characteristics in sparse reconstruction using actual projection X-ray images. That said, the focus of this study is to evaluate theoretical potential of TomoGRAF under ideal conditions.</p>
<p>Q: The paper only mentions two data augmentation techniques - random flipping and rotation. Were other data augmentation methods employed, such as noise injection or varying SID/SAD, to improve generalization capability?</p>
<p>A: We thank the reviewer for this thoughtful question. In this work, we employed random flipping and rotation as basic data augmentation strategies. We did not apply noise injection or vary geometric parameters such as SID/SAD during training, for the following reasons:</p>
<p>SID/SAD Parameters: These values were extracted directly from each patient’s DICOM metadata and therefore naturally vary across the cohort, reflecting realistic acquisition conditions without the need for synthetic augmentation.</p>
<p>Noise Injection: Our training cohort includes over 1,000 patients, each contributing multiple view angles and patches for supervision, offering substantial variability in anatomy, pose, and appearance. Since our method is designed to learn a universal prior during training—rather than rely on patient-specific fine-tuning or enhance the robustness of a CNN inference model—we determined that additional noise injection was unnecessary.</p>
<p>Purpose of the Prior: Since the prior is trained to capture general anatomical structure across patients and is later fine-tuned to sparse-view test data, our focus was on extracting shared structural features rather than simulating acquisition-specific degradations (which are better handled during test-time optimization).</p>
<p>That said, we agree that investigating the effect of more aggressive augmentations (including noise) could be a direction for further robustifying the prior, especially when adapting TomoGRAF to real clinic which requires institution-specific fine tuning on the prior. The additional augmentation is unnecessary for the current study which is based on relatively large and diverse datasets.</p>
<p>Q: For the loss functions in Equation (11) and Equation (12), how were the parameters α and β determined? Were ablation experiments conducted to ensure they are optimal values? For the loss function in Equation (13), how were the parameters γ, δ, and ε determined? And how were θ and φ determined in the evaluation metrics?</p>
<p>A: The parameters α, ϕ, γ, δ, and ξ were not used in Equations (11-12). The parameters ϕ, α and ξ were mentioned and defined in Page 12 as “Overall, the generator G takes x-ray source setup matrix K, view direction (pose) ξ=(θ,ϕ) , 2D sampling pattern v, shape code z_sh∈R^(M_s ) and appearance code z_a∈R^(M_a ) as input” as highlighted in green in the manuscript, where ϕ is the elevation angel, and θ is the azimuthal angel of view position pose ξ, and appearance code z_a is a latent vector that encodes view-dependent features and is optimized by the model during training. The parameter δ were mentioned in Page 12 as “The output is the material density δ in the corresponding x, where ϑ represents the network parameters and z_sh~p_sh and z_a~p_a with p_sh and p_a drawn from standard Gaussian distribution.” (highlighted in green in the manuscript), where δ represents the material density within the 3D CT volume which needs to be learnt by the trained model. The parameter γ was defined in page 14 as “Where L_x and L_ξ represent the latent codes of x and ξ, M_sh and M_a define the shape and appearance codes with z_sh∈R^(M_sh ) and z_a∈R^(M_a ), and γ(∙) represents positional encoding.” (highlighted in green in the manuscript), where γ(∙) represents positional encoding of the model. We did not use β in Equation (11-13) and throughout the manuscript.</p>
<p>Additionally, in Equations (11-13), hyperparameters λ_1, λ_2 〖,λ〗_3,〖 λ〗_4 and λ_5 are used to control the relative contributions of different loss terms (Equations 11–13) and metric components. These values were empirically chosen based on standard practices in the literature and preliminary experiments to ensure training stability and reasonable convergence.</p>
<p>Q: In the experiments, why was the peak signal-to-noise ratio (PSNR) = 25 set as the stopping threshold? Why not choose a higher PSNR value?</p>
<p>A: We appreciate the reviewer’s question. The PSNR (a hyperparameter) = 25 stopping threshold was selected based on empirical observations during test-time optimization. In our ultra-sparse view setup (e.g., 1–5 projection views), we found that PSNR values above 25 already corresponded to visually and structurally meaningful reconstructions. Setting a higher threshold (e.g., PSNR ≥ 30) offered marginal improvements while significantly increasing the computational burden and risk of overfitting to noise or limited view information. Moreover, our goal was not to achieve the maximum PSNR of the model inference to the referenced sparse views, but rather to perform efficient fine-tuning sufficient for realistic anatomical rendering. We found that PSNR ≈ 25 served as a practical and consistent early stopping criterion across different test cases. Nonetheless, we agree that adaptive or dynamic stopping strategies based on perceptual metrics can be explored in future work for deploying TomoGRAF into clinics.</p>
<p>Q: During the experimental process, your model's baseline performance was established using a single AP view. To determine model performance, reconstructions were additionally performed with 1, 2, 5, and 10 views. The view angles were specified as follows: for 1-view reconstruction, the AP view was used as reference; for 2-view reconstruction, AP and lateral views were used for inference; for 5-view reconstruction, starting from the AP view, every 72° rotation was applied to cover the full 360°; for 10-view reconstruction, starting from the AP view, every 36° rotation was applied to cover the full 360°. However, in the Discussion section, the statement "Meanwhile, TomoGRAF, besides 1-view referencing, can leverage additional X-ray views at arbitrary angles" appears to lack strong experimental support, since the experimental procedure in this study clearly defined view angles with uniform angular intervals, which seems inconsistent with "arbitrary angles". We recommend either: (1) adding experiments to demonstrate the value of arbitrary angles (e.g., using only the AP view and its adjacent angles) for TomoGRAF during inference, or (2) rephrasing this statement to better align with the actual experimental conditions.</p>
<p>A: We thank reviewer for pointing this out. We have revised our statement in Discussion to “In addition to single-view referencing, TomoGRAF is capable of incorporating multiple X-ray views from diverse directions, as demonstrated in this study using uniformly distributed acquisition angles.”</p>
<p>Q: TomoGRAF requires fine-tuning during the inference phase. For specific fine-tuning, the trained prior model is optimized under the supervision of the patient's 2D sparse view projections to adapt to new patient anatomies. Does this mean that during the inference phase, fine-tuning is required for projection images from every angle used? If so, this would lead to increased inference time, and when more views are used for inference, the time would increase accordingly. The paper mentions that single-view reconstruction takes approximately 344 seconds, while each additional view roughly doubles the reconstruction time, which appears relatively slow for practical applications. Can the network be improved to enhance its computational efficiency?</p>
<p>A: We appreciate the reviewer’s concern regarding inference efficiency. To clarify, fine-tuning is not performed independently for each projection view. Instead, during test-time optimization, random image patches from all available views are sampled and fed into the model in a unified optimization loop. As the number of input views increases, the model benefits from more diverse supervision, which may slightly increase the number of iterations needed to converge, but this increase is sublinear (as demonstrated in Figure 1 in this response letter) rather than linear. In our experiments (Table 2), we observed that while reconstruction time increases with additional views, the marginal cost per view decreases, given improved convergence behavior.</p>
<p>We agree that inference speed is a crucial aspect for practical deployment. Therefore, strategies to improve computational efficiency have been thoroughly discussed in the Limitations section of the original manuscript as “First, TomoGRAF requires further fine-tuning at the inference stage, which increases the reconstruction time (1-view at 344.25±10.32 s and 2-view at 719.46±26.78 s). The time further increases with inference using more views. Significant acceleration is desired for online procedures such as motion adaptive radiotherapy (59). Model compression techniques such as network pruning (60) and quantization (61) can decrease computational complexity while maintaining accuracy. Additionally, hardware acceleration via TensorRT (62) optimization or specialized processors (e.g., FPGAs (63), TPUs (64)) could also potentially improve the inference speed. Architecturally, incorporating efficient neural representations (e.g., lightweight MLPs (65) or hash-based encoding (66)) and adaptive sampling (67) methods could reduce computational overhead by prioritizing critical regions. Future work will explore these optimizations to improve TomoGRAF’s feasibility for real-time clinical applications, which would be essential for interventional procedures.” and the content has been highlighted in green in the manuscript.</p>
<p>Figure 1: Relationship between TomoGRAF inference time and number of referenced sparse views.</p>
<p>Q: In the quantitative analysis, this paper employed SSIM, PSNR, and RMSE metrics, but it did not thoroughly discuss in the qualitative analysis whether these metrics can reflect clinical diagnostic requirements. In clinical applications, the accuracy of CT images is crucial. Have you considered inviting clinicians to evaluate the generated predicted CT images and assess their feasibility?</p>
<p>A: We thank the reviewer for highlighting this important consideration. We agree that conventional quantitative metrics such as SSIM, PSNR, and RMSE, while commonly used in the literature, do not fully capture the diagnostic relevance of reconstructed CT images. In this study, our focus was on establishing a technical proof-of-concept for the TomoGRAF framework, and as such, we did not incorporate clinical reader evaluations. That said, we acknowledge the value of involving clinical experts in future evaluations, especially as we move toward applying this framework to real patient data. We added a note in the Discussion section to reflect this important point and outline plans for future clinical validation as “Lastly, while the current study primarily focuses on demonstrating the technical feasibility of ultra-sparse view reconstruction of the proposed TomoGRAF framework, we recognize that conventional quantitative image quality metrics may not fully capture the clinical utility of reconstructed images. As a future direction, incorporating clinical evaluation, such as qualitative scoring by radiologists or task-based diagnostic assessment, will be informative to assess the real-world applicability and reliability of TomoGRAF in clinical practice.”</p>
<p>Q: The pseudocode of the Siddon algorithm (Appendix 1) serves as an important methodological supplement. Please further clarify its specific implementation details in TomoGRAF, such as how it integrates with the fully connected network - does the network directly learn attenuation coefficients, or is this achieved through post-processing?</p>
<p>A: We thank the reviewer for this insightful question. Siddon’s ray tracing algorithm serves as the forward projection operator within the TomoGRAF framework, replacing the original ray tracing in NeRF method for natural lights to compute line integrals through the reconstructed volume.</p>
<p>Specifically, the fully connected network directly outputs voxel-wise attenuation coefficients that represent the 3D volume. Siddon’s algorithm then integrates these coefficients along rays corresponding to the given projection views to produce synthetic 2D projections. This forward projection step is fully differentiable and embedded within the network training loop, allowing end-to-end optimization. There is no separate post-processing step for attenuation coefficients; the network learns to represent the volume implicitly, and Siddon’s algorithm models the physics of X-ray projection during</p>
<section class="sm xbox font-sm" id="pone.0330463.s004"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers_Final.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373210/bin/pone.0330463.s004.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330463.s004.docx</a><sup> (75.3KB, docx) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0330463.r003"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330463.r003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330463.r003</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 1</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id9"><span class="name western">Zhentian Wang</span></a><div hidden="hidden" id="id9">
<h3><span class="name western">Zhentian Wang</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zhentian Wang</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.e" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.e" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.e" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Zhentian Wang</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.e" class="d-panel p" style="display: none">
<div>© 2025 Zhentian Wang</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>1 Aug 2025</em>
</p>
<p>TomoGRAF: An X-Ray Physics-Driven Generative Radiance Field Framework for Extremely Sparse View CT Reconstruction</p>
<p>PONE-D-25-15335R1</p>
<p>Dear Dr. Sheng,</p>
<p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
<p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
<p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Editorial Manager®</a>  and clicking the ‘Update My Information' link at the top of the page. For questions related to billing, please contact <a href="https://plos.my.site.com/s/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">billing support</a> .</p>
<p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>Kind regards,</p>
<p>Zhentian Wang, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Additional Editor Comments (optional):</p>
<p>Both reviewers have confirmed that their comments have been addressed in the revision.</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>
<strong>Comments to the Author</strong>
</p>
<p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.</p>
<p>Reviewer #2: All comments have been addressed</p>
<p>**********</p>
<p>2. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>3. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #2: N/A</p>
<p>**********</p>
<p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>6. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p>Reviewer #2: (No Response)</p>
<p>**********</p>
<p>7. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a> ). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a> .</p>
<p>Reviewer #2: No</p>
<p>**********</p></section></article><article class="sub-article" id="pone.0330463.r004"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330463.r004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330463.r004</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Acceptance letter</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id10"><span class="name western">Zhentian Wang</span></a><div hidden="hidden" id="id10">
<h3><span class="name western">Zhentian Wang</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zhentian Wang</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.f" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.f" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.f" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Zhentian Wang</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.f" class="d-panel p" style="display: none">
<div>© 2025 Zhentian Wang</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>PONE-D-25-15335R1</p>
<p>PLOS ONE</p>
<p>Dear Dr. Sheng,</p>
<p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p>
<p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p>
<p>* All references, tables, and figures are properly cited</p>
<p>* All relevant supporting information is included in the manuscript submission,</p>
<p>* There are no issues that prevent the paper from being properly typeset</p>
<p>You will receive further instructions from the production team, including instructions on how to review your proof when it is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p>
<p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>You will receive an invoice from PLOS for your publication fee after your manuscript has reached the completed accept phase. If you receive an email requesting payment before acceptance or for any other service, this may be a phishing scheme. Learn how to identify phishing emails and protect your accounts at <a href="https://explore.plos.org/phishing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://explore.plos.org/phishing</a>.</p>
<p>If we can help with anything else, please email us at customercare@plos.org.</p>
<p>Thank you for submitting your work to PLOS ONE and supporting open access.</p>
<p>Kind regards,</p>
<p>PLOS ONE Editorial Office Staff</p>
<p>on behalf of</p>
<p>Prof. Zhentian Wang</p>
<p>Academic Editor</p>
<p>PLOS ONE</p></section></article><article class="sub-article" id="_ad93_"><section class="pmc-layout__citation font-secondary font-xs"><div></div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Associated Data</h1></hgroup><ul class="d-buttons inline-list"></ul>
<div class="d-panels font-secondary-light"></div>
<div></div>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
</div></section><section class="body sub-article-body"><section id="_adsm93_" lang="en" class="supplementary-materials"><h2 class="pmc_sec_title">Supplementary Materials</h2>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="caption p">
<span>S1 Appendix. Siddon’s Ray Tracing algorithm pseudo code applied in TomoGRAF projection rendering module.</span><p>(DOCX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373210/bin/pone.0330463.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330463.s001.docx</a><sup> (14.5KB, docx) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Comments.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373210/bin/pone.0330463.s002.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330463.s002.docx</a><sup> (14.9KB, docx) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material3_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers_Final.docx</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373210/bin/pone.0330463.s004.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330463.s004.docx</a><sup> (75.3KB, docx) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h2 class="pmc_sec_title">Data Availability Statement</h2>
<p>Data cannot be shared publicly because of institutional restriction. Data are available from the UCSF Institutional Data Access / Ethics Committee for researchers who meet the criteria for access to confidential data. Interested researchers should follow the instructions outlined on <a href="https://icd.ucsf.edu/materialdata-transfer-agreements" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://icd.ucsf.edu/materialdata-transfer-agreements</a>. Please contact <span>Industrycontracts@ucsf.edu</span> | (415) 350-5408 for additional assistance.</p></section></section></article><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0330463"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0330463.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (2.7 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12373210/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12373210/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373210%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373210/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12373210/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12373210/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40845061/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12373210/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40845061/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12373210/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12373210/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="774nfJHHkiNo8S21IWybunhLUcqV2SY1LO7w7ceMq9kNQ1XVkOhsheHnwKvAJsP2">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-574fdcc6.js"></script>
    
    

    </body>
</html>
