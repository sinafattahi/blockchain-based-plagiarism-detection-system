
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Clinical Applications of Artificial Intelligence in Corneal Diseases - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="AE5345E28AF438430545E2002936B7AC.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="vision">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12372148/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Vision">
<meta name="citation_title" content="Clinical Applications of Artificial Intelligence in Corneal Diseases">
<meta name="citation_author" content="Omar Nusair">
<meta name="citation_author_institution" content="Kittner Eye Center, Department of Ophthalmology, University of North Carolina, Chapel Hill, NC 27517, USA">
<meta name="citation_author" content="Hassan Asadigandomani">
<meta name="citation_author_institution" content="Department of Ophthalmology, University of California, San Francisco, CA 94143, USA">
<meta name="citation_author" content="Hossein Farrokhpour">
<meta name="citation_author_institution" content="Translational Ophthalmology Research Center, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran 1336616351, Iran">
<meta name="citation_author" content="Fatemeh Moosaie">
<meta name="citation_author_institution" content="Universal Scientific Education and Research Network (USERN), Tehran University of Medical Sciences, Tehran 1416634793, Iran">
<meta name="citation_author" content="Zahra Bibak-Bejandi">
<meta name="citation_author_institution" content="Department of Ophthalmology and Visual Sciences, University of Illinois at Chicago, Chicago, IL 60612, USA">
<meta name="citation_author" content="Alireza Razavi">
<meta name="citation_author_institution" content="Eye Research Center, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran 1336616351, Iran">
<meta name="citation_author" content="Kimia Daneshvar">
<meta name="citation_author_institution" content="Eye Research Center, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran 1336616351, Iran">
<meta name="citation_author" content="Mohammad Soleimani">
<meta name="citation_author_institution" content="Kittner Eye Center, Department of Ophthalmology, University of North Carolina, Chapel Hill, NC 27517, USA">
<meta name="citation_publication_date" content="2025 Aug 18">
<meta name="citation_volume" content="9">
<meta name="citation_issue" content="3">
<meta name="citation_firstpage" content="71">
<meta name="citation_doi" content="10.3390/vision9030071">
<meta name="citation_pmid" content="40843795">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12372148/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12372148/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12372148/pdf/vision-09-00071.pdf">
<meta name="description" content="We evaluated the clinical applications of artificial intelligence models in diagnosing corneal diseases, highlighting their performance metrics and clinical potential. A systematic search was conducted for several disease categories: keratoconus ...">
<meta name="og:title" content="Clinical Applications of Artificial Intelligence in Corneal Diseases">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="We evaluated the clinical applications of artificial intelligence models in diagnosing corneal diseases, highlighting their performance metrics and clinical potential. A systematic search was conducted for several disease categories: keratoconus ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12372148/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12372148">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.3390/vision9030071"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/vision-09-00071.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12372148%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12372148/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12372148/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12372148/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-vision.png" alt="Vision logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Vision" title="Link to Vision" shape="default" href="https://www.mdpi.com/journal/vision" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Vision (Basel)</button></div>. 2025 Aug 18;9(3):71. doi: <a href="https://doi.org/10.3390/vision9030071" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.3390/vision9030071</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Vision%20(Basel)%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Vision%20(Basel)%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Vision%20(Basel)%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Vision%20(Basel)%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Clinical Applications of Artificial Intelligence in Corneal Diseases</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Nusair%20O%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Omar Nusair</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Omar Nusair</span></h3>
<div class="p">
<sup>1</sup>Kittner Eye Center, Department of Ophthalmology, University of North Carolina, Chapel Hill, NC 27517, USA</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Nusair%20O%22%5BAuthor%5D" class="usa-link"><span class="name western">Omar Nusair</span></a>
</div>
</div>
<sup>1,</sup><sup>†</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Asadigandomani%20H%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Hassan Asadigandomani</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Hassan Asadigandomani</span></h3>
<div class="p">
<sup>2</sup>Department of Ophthalmology, University of California, San Francisco, CA 94143, USA</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Asadigandomani%20H%22%5BAuthor%5D" class="usa-link"><span class="name western">Hassan Asadigandomani</span></a>
</div>
</div>
<sup>2,</sup><sup>†</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Farrokhpour%20H%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Hossein Farrokhpour</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Hossein Farrokhpour</span></h3>
<div class="p">
<sup>3</sup>Translational Ophthalmology Research Center, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran 1336616351, Iran</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Farrokhpour%20H%22%5BAuthor%5D" class="usa-link"><span class="name western">Hossein Farrokhpour</span></a>
</div>
</div>
<sup>3,</sup><sup>†</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Moosaie%20F%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Fatemeh Moosaie</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Fatemeh Moosaie</span></h3>
<div class="p">
<sup>4</sup>Universal Scientific Education and Research Network (USERN), Tehran University of Medical Sciences, Tehran 1416634793, Iran</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Moosaie%20F%22%5BAuthor%5D" class="usa-link"><span class="name western">Fatemeh Moosaie</span></a>
</div>
</div>
<sup>4</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Bibak-Bejandi%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Zahra Bibak-Bejandi</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Zahra Bibak-Bejandi</span></h3>
<div class="p">
<sup>5</sup>Department of Ophthalmology and Visual Sciences, University of Illinois at Chicago, Chicago, IL 60612, USA</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Bibak-Bejandi%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zahra Bibak-Bejandi</span></a>
</div>
</div>
<sup>5</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Razavi%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Alireza Razavi</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Alireza Razavi</span></h3>
<div class="p">
<sup>6</sup>Eye Research Center, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran 1336616351, Iran</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Razavi%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Alireza Razavi</span></a>
</div>
</div>
<sup>6</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Daneshvar%20K%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Kimia Daneshvar</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Kimia Daneshvar</span></h3>
<div class="p">
<sup>6</sup>Eye Research Center, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran 1336616351, Iran</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Daneshvar%20K%22%5BAuthor%5D" class="usa-link"><span class="name western">Kimia Daneshvar</span></a>
</div>
</div>
<sup>6</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Soleimani%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Mohammad Soleimani</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Mohammad Soleimani</span></h3>
<div class="p">
<sup>1</sup>Kittner Eye Center, Department of Ophthalmology, University of North Carolina, Chapel Hill, NC 27517, USA</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Soleimani%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Mohammad Soleimani</span></a>
</div>
</div>
<sup>1,</sup><sup>*</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="af1-vision-09-00071">
<sup>1</sup>Kittner Eye Center, Department of Ophthalmology, University of North Carolina, Chapel Hill, NC 27517, USA</div>
<div id="af2-vision-09-00071">
<sup>2</sup>Department of Ophthalmology, University of California, San Francisco, CA 94143, USA</div>
<div id="af3-vision-09-00071">
<sup>3</sup>Translational Ophthalmology Research Center, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran 1336616351, Iran</div>
<div id="af4-vision-09-00071">
<sup>4</sup>Universal Scientific Education and Research Network (USERN), Tehran University of Medical Sciences, Tehran 1416634793, Iran</div>
<div id="af5-vision-09-00071">
<sup>5</sup>Department of Ophthalmology and Visual Sciences, University of Illinois at Chicago, Chicago, IL 60612, USA</div>
<div id="af6-vision-09-00071">
<sup>6</sup>Eye Research Center, Farabi Eye Hospital, Tehran University of Medical Sciences, Tehran 1336616351, Iran</div>
<div class="author-notes p">
<div class="fn" id="c1-vision-09-00071">
<sup>*</sup><p class="display-inline">Correspondence: <span>msolei@unc.edu</span>; Tel.: +1-984-974-2020</p>
</div>
<div class="fn" id="fn1-vision-09-00071">
<sup>†</sup><p class="display-inline">These authors contributed equally to this work.</p>
</div>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Jun 10; Revised 2025 Aug 12; Accepted 2025 Aug 13; Collection date 2025 Sep.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 by the authors.</div>
<p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://creativecommons.org/licenses/by/4.0/</a>).</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12372148  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40843795/" class="usa-link">40843795</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>We evaluated the clinical applications of artificial intelligence models in diagnosing corneal diseases, highlighting their performance metrics and clinical potential. A systematic search was conducted for several disease categories: keratoconus (KC), Fuch’s endothelial corneal dystrophy (FECD), infectious keratitis (IK), corneal neuropathy, dry eye disease (DED), and conjunctival diseases. Metrics such as sensitivity, specificity, accuracy, and area under the curve (AUC) were extracted. Across the diseases, convolutional neural networks and other deep learning models frequently achieved or exceeded established diagnostic benchmarks (AUC &gt; 0.90; sensitivity/specificity &gt; 0.85–0.90), with a particularly strong performance for KC and FECD when trained on consistent imaging modalities such as anterior segment optical coherence tomography (AS-OCT). Models for IK and conjunctival diseases showed promise but faced challenges in heterogeneous image quality and limited objective training criteria. DED and tear film models benefited from multimodal data yet lacked direct comparisons with expert clinicians. Despite high diagnostic precision, challenges from heterogeneous data, a lack of standardization in disease definitions, imaging acquisition, and model training remain. The broad implementation of artificial intelligence must address these limitations to improve eye care equity.</p>
<section id="kwd-group1" class="kwd-group"><p><strong>Keywords:</strong> artificial intelligence, deep learning, machine learning, corneal diseases, keratoconus, infectious keratitis, dry eye disease, tear film, corneal neuropathy, conjunctiva</p></section></section><section id="sec1-vision-09-00071"><h2 class="pmc_sec_title">1. Introduction</h2>
<p>Ocular diseases impose a substantial burden of blindness and visual impairment worldwide [<a href="#B1-vision-09-00071" class="usa-link" aria-describedby="B1-vision-09-00071">1</a>]. Despite the progress in lowering age-adjusted blindness since 1990, population growth, aging, and evolving lifestyles have caused a notable increase in the number of blindness and vision impairment cases. The overwhelming of ophthalmic services has been noted as a major factor behind this rise, marking the need for more efficient methods [<a href="#B2-vision-09-00071" class="usa-link" aria-describedby="B2-vision-09-00071">2</a>].</p>
<p>Corneal diseases, in particular, are a major contributor to the global burden of eye diseases [<a href="#B3-vision-09-00071" class="usa-link" aria-describedby="B3-vision-09-00071">3</a>]. Additionally, disorders including dry eye disease (DED), corneal neuropathy, keratoconus (KC), and infectious keratitis (IK) have substantial effects on patients’ quality of life and drive substantial economic costs [<a href="#B2-vision-09-00071" class="usa-link" aria-describedby="B2-vision-09-00071">2</a>,<a href="#B4-vision-09-00071" class="usa-link" aria-describedby="B4-vision-09-00071">4</a>,<a href="#B5-vision-09-00071" class="usa-link" aria-describedby="B5-vision-09-00071">5</a>,<a href="#B6-vision-09-00071" class="usa-link" aria-describedby="B6-vision-09-00071">6</a>,<a href="#B7-vision-09-00071" class="usa-link" aria-describedby="B7-vision-09-00071">7</a>,<a href="#B8-vision-09-00071" class="usa-link" aria-describedby="B8-vision-09-00071">8</a>,<a href="#B9-vision-09-00071" class="usa-link" aria-describedby="B9-vision-09-00071">9</a>,<a href="#B10-vision-09-00071" class="usa-link" aria-describedby="B10-vision-09-00071">10</a>,<a href="#B11-vision-09-00071" class="usa-link" aria-describedby="B11-vision-09-00071">11</a>].</p>
<p>There are a variety of diagnostic methods available for each of these disorders, including slit lamp examinations, various imaging devices such as anterior segment optical coherence tomography (AS-OCT), tear film assessments, in vivo corneal confocal microscopy (IVCM), and microbial cultures [<a href="#B12-vision-09-00071" class="usa-link" aria-describedby="B12-vision-09-00071">12</a>,<a href="#B13-vision-09-00071" class="usa-link" aria-describedby="B13-vision-09-00071">13</a>,<a href="#B14-vision-09-00071" class="usa-link" aria-describedby="B14-vision-09-00071">14</a>,<a href="#B15-vision-09-00071" class="usa-link" aria-describedby="B15-vision-09-00071">15</a>,<a href="#B16-vision-09-00071" class="usa-link" aria-describedby="B16-vision-09-00071">16</a>]. However, they are often time-consuming and labor-intensive, requiring precise manual analysis and limiting clinical workflow. Moreover, these approaches also demand trained specialists that are in limited supply, especially in low-resource regions.</p>
<p>The applications and performance of artificial intelligence (AI) have emerged as a promising tool for improving diagnostic dilemmas and addressing the current limitations. Machine learning (ML), a subset of AI, and deep learning (DL), a subset of ML, offer novel and efficient insights. They have been used for enhancing the diagnosis of multiple eye diseases [<a href="#B17-vision-09-00071" class="usa-link" aria-describedby="B17-vision-09-00071">17</a>,<a href="#B18-vision-09-00071" class="usa-link" aria-describedby="B18-vision-09-00071">18</a>]. Thus far, various algorithms and models have been developed using existing data, enabling automated segmentation, feature extraction, and predictive analytics on par with highly trained specialists [<a href="#B19-vision-09-00071" class="usa-link" aria-describedby="B19-vision-09-00071">19</a>,<a href="#B20-vision-09-00071" class="usa-link" aria-describedby="B20-vision-09-00071">20</a>,<a href="#B21-vision-09-00071" class="usa-link" aria-describedby="B21-vision-09-00071">21</a>,<a href="#B22-vision-09-00071" class="usa-link" aria-describedby="B22-vision-09-00071">22</a>,<a href="#B23-vision-09-00071" class="usa-link" aria-describedby="B23-vision-09-00071">23</a>,<a href="#B24-vision-09-00071" class="usa-link" aria-describedby="B24-vision-09-00071">24</a>,<a href="#B25-vision-09-00071" class="usa-link" aria-describedby="B25-vision-09-00071">25</a>,<a href="#B26-vision-09-00071" class="usa-link" aria-describedby="B26-vision-09-00071">26</a>].</p>
<p>With the continuous advancements in AI technologies, including large language models (LLMs) such as ChatGPT-4.0, becoming more accessible, their integration into clinical practice necessitates ongoing evaluation. While previous studies have explored the applications of AI in corneal disease, many focus on only one or a few corneal conditions. Our study offers a comprehensive overview of several corneal diseases and AI models, allowing for the recognition of patterns between models and diseases. Additionally, we identify the strengths and areas of improvement for this rapidly evolving field.</p></section><section id="sec2-vision-09-00071"><h2 class="pmc_sec_title">2. Methods</h2>
<p>In this narrative study, a systematic literature search across three databases, using PubMed, Web of Science, and SCOPUS, was performed. All articles published until April 2025 were included in the search. The search strategy utilized a combination of keywords and controlled vocabulary (e.g., MeSH terms) such as “artificial intelligence,” “machine learning,” “deep learning,” “corneal diseases,” “keratoconus,” “infectious keratitis,” “corneal neuropathy,” “dry eye disease,” “corneal dystrophy,” “corneal degeneration,” “ocular surface tumors,” “pterygium,” and “conjunctival diseases,” tailored to each database’s syntax. Abstracts meeting the criteria were selected for full-text review. Two independent reviewers then assessed the full texts for eligibility. Discrepancies between the reviewers were resolved through discussion to reach a consensus on inclusion. Studies were included if they reported LLM-, ML-, or DL-based approaches (e.g., neural networks, support vector machines) applied to corneal disease diagnosis or monitoring. No language or publication date restrictions were applied initially, though primarily English-language articles were ultimately reviewed. Data extraction focused on performance metrics (sensitivity, specificity, accuracy, area under the curve [AUC]), imaging techniques, AI model types, and study limitations. Qualitative synthesis was employed to summarize the findings, with an emphasis on model performance, challenges, and clinical implications. Study quality was not formally assessed, aligning with the narrative review framework, but key trends and discrepancies were critically evaluated based on the extracted data. The study adhered to the tenets of the Declaration of Helsinki.</p></section><section id="sec3-vision-09-00071"><h2 class="pmc_sec_title">3. Results</h2>
<section id="sec3dot1-vision-09-00071"><h3 class="pmc_sec_title">3.1. Corneal Structural Diseases</h3>
<section id="sec3dot1dot1-vision-09-00071"><h4 class="pmc_sec_title">3.1.1. Keratoconus</h4>
<p>Keratoconus is a progressive condition where the cornea bulges into a cone shape due to stromal thinning [<a href="#B27-vision-09-00071" class="usa-link" aria-describedby="B27-vision-09-00071">27</a>,<a href="#B28-vision-09-00071" class="usa-link" aria-describedby="B28-vision-09-00071">28</a>]. The early detection of KC is crucial for preventing unpredictable outcomes in refractive surgery and contact lens fitting [<a href="#B29-vision-09-00071" class="usa-link" aria-describedby="B29-vision-09-00071">29</a>,<a href="#B30-vision-09-00071" class="usa-link" aria-describedby="B30-vision-09-00071">30</a>,<a href="#B31-vision-09-00071" class="usa-link" aria-describedby="B31-vision-09-00071">31</a>,<a href="#B32-vision-09-00071" class="usa-link" aria-describedby="B32-vision-09-00071">32</a>]. However, identifying subclinical KC (SKC) remains a significant challenge before refractive surgery due to the overlap in topography and tomography imaging parameters between SKC and normal eyes [<a href="#B33-vision-09-00071" class="usa-link" aria-describedby="B33-vision-09-00071">33</a>,<a href="#B34-vision-09-00071" class="usa-link" aria-describedby="B34-vision-09-00071">34</a>,<a href="#B35-vision-09-00071" class="usa-link" aria-describedby="B35-vision-09-00071">35</a>,<a href="#B36-vision-09-00071" class="usa-link" aria-describedby="B36-vision-09-00071">36</a>]. The condition’s cause is multifactorial, involving genetic predisposition, eye rubbing, and biomechanical factors. A cascade of biomechanical decompensation triggered by a focal change in corneal elasticity is thought to drive KC progression [<a href="#B37-vision-09-00071" class="usa-link" aria-describedby="B37-vision-09-00071">37</a>,<a href="#B38-vision-09-00071" class="usa-link" aria-describedby="B38-vision-09-00071">38</a>,<a href="#B39-vision-09-00071" class="usa-link" aria-describedby="B39-vision-09-00071">39</a>,<a href="#B40-vision-09-00071" class="usa-link" aria-describedby="B40-vision-09-00071">40</a>,<a href="#B41-vision-09-00071" class="usa-link" aria-describedby="B41-vision-09-00071">41</a>]. ML algorithms and neural networks have the potential to determine disease progression in an unbiased manner [<a href="#B37-vision-09-00071" class="usa-link" aria-describedby="B37-vision-09-00071">37</a>,<a href="#B42-vision-09-00071" class="usa-link" aria-describedby="B42-vision-09-00071">42</a>,<a href="#B43-vision-09-00071" class="usa-link" aria-describedby="B43-vision-09-00071">43</a>,<a href="#B44-vision-09-00071" class="usa-link" aria-describedby="B44-vision-09-00071">44</a>,<a href="#B45-vision-09-00071" class="usa-link" aria-describedby="B45-vision-09-00071">45</a>,<a href="#B46-vision-09-00071" class="usa-link" aria-describedby="B46-vision-09-00071">46</a>,<a href="#B47-vision-09-00071" class="usa-link" aria-describedby="B47-vision-09-00071">47</a>,<a href="#B48-vision-09-00071" class="usa-link" aria-describedby="B48-vision-09-00071">48</a>,<a href="#B49-vision-09-00071" class="usa-link" aria-describedby="B49-vision-09-00071">49</a>,<a href="#B50-vision-09-00071" class="usa-link" aria-describedby="B50-vision-09-00071">50</a>,<a href="#B51-vision-09-00071" class="usa-link" aria-describedby="B51-vision-09-00071">51</a>,<a href="#B52-vision-09-00071" class="usa-link" aria-describedby="B52-vision-09-00071">52</a>,<a href="#B53-vision-09-00071" class="usa-link" aria-describedby="B53-vision-09-00071">53</a>,<a href="#B54-vision-09-00071" class="usa-link" aria-describedby="B54-vision-09-00071">54</a>,<a href="#B55-vision-09-00071" class="usa-link" aria-describedby="B55-vision-09-00071">55</a>,<a href="#B56-vision-09-00071" class="usa-link" aria-describedby="B56-vision-09-00071">56</a>,<a href="#B57-vision-09-00071" class="usa-link" aria-describedby="B57-vision-09-00071">57</a>,<a href="#B58-vision-09-00071" class="usa-link" aria-describedby="B58-vision-09-00071">58</a>,<a href="#B59-vision-09-00071" class="usa-link" aria-describedby="B59-vision-09-00071">59</a>,<a href="#B60-vision-09-00071" class="usa-link" aria-describedby="B60-vision-09-00071">60</a>]. For instance, DL of optical coherence tomography (OCT) color-coded maps have shown strong potential in identifying progressive from non-progressive KC with an 85% accuracy using the adjusted age algorithm [<a href="#B61-vision-09-00071" class="usa-link" aria-describedby="B61-vision-09-00071">61</a>].</p>
<p>Numerous studies have tested a variety of AI models in the context of KC diagnosis, screening, and prediction [<a href="#B61-vision-09-00071" class="usa-link" aria-describedby="B61-vision-09-00071">61</a>,<a href="#B62-vision-09-00071" class="usa-link" aria-describedby="B62-vision-09-00071">62</a>,<a href="#B63-vision-09-00071" class="usa-link" aria-describedby="B63-vision-09-00071">63</a>,<a href="#B64-vision-09-00071" class="usa-link" aria-describedby="B64-vision-09-00071">64</a>,<a href="#B65-vision-09-00071" class="usa-link" aria-describedby="B65-vision-09-00071">65</a>,<a href="#B66-vision-09-00071" class="usa-link" aria-describedby="B66-vision-09-00071">66</a>,<a href="#B67-vision-09-00071" class="usa-link" aria-describedby="B67-vision-09-00071">67</a>,<a href="#B68-vision-09-00071" class="usa-link" aria-describedby="B68-vision-09-00071">68</a>,<a href="#B69-vision-09-00071" class="usa-link" aria-describedby="B69-vision-09-00071">69</a>,<a href="#B70-vision-09-00071" class="usa-link" aria-describedby="B70-vision-09-00071">70</a>,<a href="#B71-vision-09-00071" class="usa-link" aria-describedby="B71-vision-09-00071">71</a>,<a href="#B72-vision-09-00071" class="usa-link" aria-describedby="B72-vision-09-00071">72</a>,<a href="#B73-vision-09-00071" class="usa-link" aria-describedby="B73-vision-09-00071">73</a>,<a href="#B74-vision-09-00071" class="usa-link" aria-describedby="B74-vision-09-00071">74</a>,<a href="#B75-vision-09-00071" class="usa-link" aria-describedby="B75-vision-09-00071">75</a>,<a href="#B76-vision-09-00071" class="usa-link" aria-describedby="B76-vision-09-00071">76</a>,<a href="#B77-vision-09-00071" class="usa-link" aria-describedby="B77-vision-09-00071">77</a>,<a href="#B78-vision-09-00071" class="usa-link" aria-describedby="B78-vision-09-00071">78</a>,<a href="#B79-vision-09-00071" class="usa-link" aria-describedby="B79-vision-09-00071">79</a>,<a href="#B80-vision-09-00071" class="usa-link" aria-describedby="B80-vision-09-00071">80</a>,<a href="#B81-vision-09-00071" class="usa-link" aria-describedby="B81-vision-09-00071">81</a>,<a href="#B82-vision-09-00071" class="usa-link" aria-describedby="B82-vision-09-00071">82</a>,<a href="#B83-vision-09-00071" class="usa-link" aria-describedby="B83-vision-09-00071">83</a>,<a href="#B84-vision-09-00071" class="usa-link" aria-describedby="B84-vision-09-00071">84</a>,<a href="#B85-vision-09-00071" class="usa-link" aria-describedby="B85-vision-09-00071">85</a>,<a href="#B86-vision-09-00071" class="usa-link" aria-describedby="B86-vision-09-00071">86</a>,<a href="#B87-vision-09-00071" class="usa-link" aria-describedby="B87-vision-09-00071">87</a>,<a href="#B88-vision-09-00071" class="usa-link" aria-describedby="B88-vision-09-00071">88</a>,<a href="#B89-vision-09-00071" class="usa-link" aria-describedby="B89-vision-09-00071">89</a>,<a href="#B90-vision-09-00071" class="usa-link" aria-describedby="B90-vision-09-00071">90</a>,<a href="#B91-vision-09-00071" class="usa-link" aria-describedby="B91-vision-09-00071">91</a>,<a href="#B92-vision-09-00071" class="usa-link" aria-describedby="B92-vision-09-00071">92</a>,<a href="#B93-vision-09-00071" class="usa-link" aria-describedby="B93-vision-09-00071">93</a>,<a href="#B94-vision-09-00071" class="usa-link" aria-describedby="B94-vision-09-00071">94</a>,<a href="#B95-vision-09-00071" class="usa-link" aria-describedby="B95-vision-09-00071">95</a>,<a href="#B96-vision-09-00071" class="usa-link" aria-describedby="B96-vision-09-00071">96</a>,<a href="#B97-vision-09-00071" class="usa-link" aria-describedby="B97-vision-09-00071">97</a>,<a href="#B98-vision-09-00071" class="usa-link" aria-describedby="B98-vision-09-00071">98</a>,<a href="#B99-vision-09-00071" class="usa-link" aria-describedby="B99-vision-09-00071">99</a>,<a href="#B100-vision-09-00071" class="usa-link" aria-describedby="B100-vision-09-00071">100</a>,<a href="#B101-vision-09-00071" class="usa-link" aria-describedby="B101-vision-09-00071">101</a>,<a href="#B102-vision-09-00071" class="usa-link" aria-describedby="B102-vision-09-00071">102</a>,<a href="#B103-vision-09-00071" class="usa-link" aria-describedby="B103-vision-09-00071">103</a>,<a href="#B104-vision-09-00071" class="usa-link" aria-describedby="B104-vision-09-00071">104</a>,<a href="#B105-vision-09-00071" class="usa-link" aria-describedby="B105-vision-09-00071">105</a>,<a href="#B106-vision-09-00071" class="usa-link" aria-describedby="B106-vision-09-00071">106</a>,<a href="#B107-vision-09-00071" class="usa-link" aria-describedby="B107-vision-09-00071">107</a>,<a href="#B108-vision-09-00071" class="usa-link" aria-describedby="B108-vision-09-00071">108</a>,<a href="#B109-vision-09-00071" class="usa-link" aria-describedby="B109-vision-09-00071">109</a>,<a href="#B110-vision-09-00071" class="usa-link" aria-describedby="B110-vision-09-00071">110</a>,<a href="#B111-vision-09-00071" class="usa-link" aria-describedby="B111-vision-09-00071">111</a>,<a href="#B112-vision-09-00071" class="usa-link" aria-describedby="B112-vision-09-00071">112</a>,<a href="#B113-vision-09-00071" class="usa-link" aria-describedby="B113-vision-09-00071">113</a>,<a href="#B114-vision-09-00071" class="usa-link" aria-describedby="B114-vision-09-00071">114</a>,<a href="#B115-vision-09-00071" class="usa-link" aria-describedby="B115-vision-09-00071">115</a>,<a href="#B116-vision-09-00071" class="usa-link" aria-describedby="B116-vision-09-00071">116</a>,<a href="#B117-vision-09-00071" class="usa-link" aria-describedby="B117-vision-09-00071">117</a>,<a href="#B118-vision-09-00071" class="usa-link" aria-describedby="B118-vision-09-00071">118</a>,<a href="#B119-vision-09-00071" class="usa-link" aria-describedby="B119-vision-09-00071">119</a>,<a href="#B120-vision-09-00071" class="usa-link" aria-describedby="B120-vision-09-00071">120</a>,<a href="#B121-vision-09-00071" class="usa-link" aria-describedby="B121-vision-09-00071">121</a>,<a href="#B122-vision-09-00071" class="usa-link" aria-describedby="B122-vision-09-00071">122</a>,<a href="#B123-vision-09-00071" class="usa-link" aria-describedby="B123-vision-09-00071">123</a>,<a href="#B124-vision-09-00071" class="usa-link" aria-describedby="B124-vision-09-00071">124</a>,<a href="#B125-vision-09-00071" class="usa-link" aria-describedby="B125-vision-09-00071">125</a>,<a href="#B126-vision-09-00071" class="usa-link" aria-describedby="B126-vision-09-00071">126</a>,<a href="#B127-vision-09-00071" class="usa-link" aria-describedby="B127-vision-09-00071">127</a>,<a href="#B128-vision-09-00071" class="usa-link" aria-describedby="B128-vision-09-00071">128</a>,<a href="#B129-vision-09-00071" class="usa-link" aria-describedby="B129-vision-09-00071">129</a>,<a href="#B130-vision-09-00071" class="usa-link" aria-describedby="B130-vision-09-00071">130</a>,<a href="#B131-vision-09-00071" class="usa-link" aria-describedby="B131-vision-09-00071">131</a>,<a href="#B132-vision-09-00071" class="usa-link" aria-describedby="B132-vision-09-00071">132</a>,<a href="#B133-vision-09-00071" class="usa-link" aria-describedby="B133-vision-09-00071">133</a>,<a href="#B134-vision-09-00071" class="usa-link" aria-describedby="B134-vision-09-00071">134</a>,<a href="#B135-vision-09-00071" class="usa-link" aria-describedby="B135-vision-09-00071">135</a>]. Most commonly, traditional ML models are utilized such as neural networks (NNs), decision trees, random forest (RF) models, support vector machines (SVMs) and more. However, DL models such as convolutional neural networks (CNNs) and generative adversarial networks (GAN) are also used. Models are trained with extensive data from corneal imaging including Placido topography, Scheimpflug tomography, slit scanning systems, OCT, and Pentacam topography. Pentacam topography, especially, has made the most significant contribution to enhancing sensitivity and specificity in ML algorithms (27 studies) [<a href="#B31-vision-09-00071" class="usa-link" aria-describedby="B31-vision-09-00071">31</a>,<a href="#B32-vision-09-00071" class="usa-link" aria-describedby="B32-vision-09-00071">32</a>,<a href="#B43-vision-09-00071" class="usa-link" aria-describedby="B43-vision-09-00071">43</a>,<a href="#B45-vision-09-00071" class="usa-link" aria-describedby="B45-vision-09-00071">45</a>,<a href="#B46-vision-09-00071" class="usa-link" aria-describedby="B46-vision-09-00071">46</a>,<a href="#B62-vision-09-00071" class="usa-link" aria-describedby="B62-vision-09-00071">62</a>,<a href="#B63-vision-09-00071" class="usa-link" aria-describedby="B63-vision-09-00071">63</a>,<a href="#B64-vision-09-00071" class="usa-link" aria-describedby="B64-vision-09-00071">64</a>,<a href="#B65-vision-09-00071" class="usa-link" aria-describedby="B65-vision-09-00071">65</a>,<a href="#B66-vision-09-00071" class="usa-link" aria-describedby="B66-vision-09-00071">66</a>,<a href="#B67-vision-09-00071" class="usa-link" aria-describedby="B67-vision-09-00071">67</a>,<a href="#B68-vision-09-00071" class="usa-link" aria-describedby="B68-vision-09-00071">68</a>,<a href="#B69-vision-09-00071" class="usa-link" aria-describedby="B69-vision-09-00071">69</a>,<a href="#B70-vision-09-00071" class="usa-link" aria-describedby="B70-vision-09-00071">70</a>,<a href="#B71-vision-09-00071" class="usa-link" aria-describedby="B71-vision-09-00071">71</a>,<a href="#B72-vision-09-00071" class="usa-link" aria-describedby="B72-vision-09-00071">72</a>,<a href="#B73-vision-09-00071" class="usa-link" aria-describedby="B73-vision-09-00071">73</a>,<a href="#B74-vision-09-00071" class="usa-link" aria-describedby="B74-vision-09-00071">74</a>,<a href="#B75-vision-09-00071" class="usa-link" aria-describedby="B75-vision-09-00071">75</a>,<a href="#B76-vision-09-00071" class="usa-link" aria-describedby="B76-vision-09-00071">76</a>,<a href="#B77-vision-09-00071" class="usa-link" aria-describedby="B77-vision-09-00071">77</a>,<a href="#B78-vision-09-00071" class="usa-link" aria-describedby="B78-vision-09-00071">78</a>,<a href="#B79-vision-09-00071" class="usa-link" aria-describedby="B79-vision-09-00071">79</a>,<a href="#B80-vision-09-00071" class="usa-link" aria-describedby="B80-vision-09-00071">80</a>,<a href="#B81-vision-09-00071" class="usa-link" aria-describedby="B81-vision-09-00071">81</a>,<a href="#B82-vision-09-00071" class="usa-link" aria-describedby="B82-vision-09-00071">82</a>,<a href="#B83-vision-09-00071" class="usa-link" aria-describedby="B83-vision-09-00071">83</a>].</p>
<p>Neural networks, including multi-layer perceptrons (MLPs), have demonstrated high diagnostic accuracy, with some analyses reporting a sensitivity up to 100% [<a href="#B84-vision-09-00071" class="usa-link" aria-describedby="B84-vision-09-00071">84</a>]. Our review showed that 30 studies used neural networks for KC detection [<a href="#B27-vision-09-00071" class="usa-link" aria-describedby="B27-vision-09-00071">27</a>,<a href="#B28-vision-09-00071" class="usa-link" aria-describedby="B28-vision-09-00071">28</a>,<a href="#B29-vision-09-00071" class="usa-link" aria-describedby="B29-vision-09-00071">29</a>,<a href="#B51-vision-09-00071" class="usa-link" aria-describedby="B51-vision-09-00071">51</a>,<a href="#B85-vision-09-00071" class="usa-link" aria-describedby="B85-vision-09-00071">85</a>,<a href="#B86-vision-09-00071" class="usa-link" aria-describedby="B86-vision-09-00071">86</a>,<a href="#B87-vision-09-00071" class="usa-link" aria-describedby="B87-vision-09-00071">87</a>,<a href="#B88-vision-09-00071" class="usa-link" aria-describedby="B88-vision-09-00071">88</a>,<a href="#B89-vision-09-00071" class="usa-link" aria-describedby="B89-vision-09-00071">89</a>,<a href="#B90-vision-09-00071" class="usa-link" aria-describedby="B90-vision-09-00071">90</a>,<a href="#B91-vision-09-00071" class="usa-link" aria-describedby="B91-vision-09-00071">91</a>,<a href="#B92-vision-09-00071" class="usa-link" aria-describedby="B92-vision-09-00071">92</a>,<a href="#B93-vision-09-00071" class="usa-link" aria-describedby="B93-vision-09-00071">93</a>,<a href="#B94-vision-09-00071" class="usa-link" aria-describedby="B94-vision-09-00071">94</a>,<a href="#B95-vision-09-00071" class="usa-link" aria-describedby="B95-vision-09-00071">95</a>,<a href="#B96-vision-09-00071" class="usa-link" aria-describedby="B96-vision-09-00071">96</a>,<a href="#B97-vision-09-00071" class="usa-link" aria-describedby="B97-vision-09-00071">97</a>,<a href="#B98-vision-09-00071" class="usa-link" aria-describedby="B98-vision-09-00071">98</a>,<a href="#B99-vision-09-00071" class="usa-link" aria-describedby="B99-vision-09-00071">99</a>,<a href="#B100-vision-09-00071" class="usa-link" aria-describedby="B100-vision-09-00071">100</a>,<a href="#B101-vision-09-00071" class="usa-link" aria-describedby="B101-vision-09-00071">101</a>,<a href="#B102-vision-09-00071" class="usa-link" aria-describedby="B102-vision-09-00071">102</a>,<a href="#B103-vision-09-00071" class="usa-link" aria-describedby="B103-vision-09-00071">103</a>,<a href="#B104-vision-09-00071" class="usa-link" aria-describedby="B104-vision-09-00071">104</a>,<a href="#B105-vision-09-00071" class="usa-link" aria-describedby="B105-vision-09-00071">105</a>,<a href="#B106-vision-09-00071" class="usa-link" aria-describedby="B106-vision-09-00071">106</a>,<a href="#B107-vision-09-00071" class="usa-link" aria-describedby="B107-vision-09-00071">107</a>,<a href="#B108-vision-09-00071" class="usa-link" aria-describedby="B108-vision-09-00071">108</a>,<a href="#B109-vision-09-00071" class="usa-link" aria-describedby="B109-vision-09-00071">109</a>], with most relying on corneal topography. The highest sensitivity and specificity (both 100%) were achieved in the study by Fisher et al. [<a href="#B94-vision-09-00071" class="usa-link" aria-describedby="B94-vision-09-00071">94</a>]. The FPA-K-means unsupervised algorithm, free from pre-labeling bias, also efficiently identified KC [<a href="#B110-vision-09-00071" class="usa-link" aria-describedby="B110-vision-09-00071">110</a>]. Additionally, an SVM model using eight key corneal parameters from OCT-based topography achieved 94% accuracy [<a href="#B111-vision-09-00071" class="usa-link" aria-describedby="B111-vision-09-00071">111</a>].</p>
<p>Random forest classifiers achieved 98% and 95% accuracy in identifying class 2 and class 4 keratoconus, respectively, using the Harvard Dataverse KC dataset with sequential forward selection [<a href="#B112-vision-09-00071" class="usa-link" aria-describedby="B112-vision-09-00071">112</a>]. Logistic regression and SVM models helped develop indices such as the Fourier-based keratoconus detection index (FKI), which demonstrated strong subclinical KC diagnostic performance [<a href="#B113-vision-09-00071" class="usa-link" aria-describedby="B113-vision-09-00071">113</a>,<a href="#B114-vision-09-00071" class="usa-link" aria-describedby="B114-vision-09-00071">114</a>,<a href="#B115-vision-09-00071" class="usa-link" aria-describedby="B115-vision-09-00071">115</a>,<a href="#B116-vision-09-00071" class="usa-link" aria-describedby="B116-vision-09-00071">116</a>,<a href="#B117-vision-09-00071" class="usa-link" aria-describedby="B117-vision-09-00071">117</a>]. SVMs using a cubic kernel and elevation parameters achieved 96.6% accuracy in distinguishing KC from normal cases [<a href="#B66-vision-09-00071" class="usa-link" aria-describedby="B66-vision-09-00071">66</a>] and reported over 93% prediction accuracy and 95% overall accuracy in other studies [<a href="#B32-vision-09-00071" class="usa-link" aria-describedby="B32-vision-09-00071">32</a>].</p>
<p>Naïve Bayes (NB) classifiers and clustering methods like hierarchical clustering and k-nearest neighbor were also used to stratify patient subgroups and enhance diagnostic precision [<a href="#B114-vision-09-00071" class="usa-link" aria-describedby="B114-vision-09-00071">114</a>,<a href="#B118-vision-09-00071" class="usa-link" aria-describedby="B118-vision-09-00071">118</a>]. Among various AI models proposed for KC screening, an ensemble model using soft voting achieved the highest sensitivity—90.5% on internal and 96.4% on external validation data [<a href="#B119-vision-09-00071" class="usa-link" aria-describedby="B119-vision-09-00071">119</a>]. Newer AI-based indices such as BESTi showed improved sensitivity and specificity (84.97%) for detecting SKC compared with the Pentacam Random Forest Index (PRFI) and Belin–Ambrósio Deviation Index (BAD-D) [<a href="#B120-vision-09-00071" class="usa-link" aria-describedby="B120-vision-09-00071">120</a>].</p>
<p>In a diagnostic study evaluating AI’s power in distinguishing non-KC, SKC, and KC cases, the global diagnostic accuracy reached 95.53%; the aforementioned model exceeded ophthalmology residents’ accuracy of 93.55%, with a validation AUC of 0.99 [<a href="#B121-vision-09-00071" class="usa-link" aria-describedby="B121-vision-09-00071">121</a>]. Automated tree-based ML classifiers reached 100% sensitivity and 99.5% specificity in differentiating KC from normal corneas [<a href="#B82-vision-09-00071" class="usa-link" aria-describedby="B82-vision-09-00071">82</a>]. Deep learning models, particularly CNNs, were trained on Scheimpflug tomography and OCT images. These DL models demonstrated excellent performance in differentiating non-KC, SKC, and manifest KC [<a href="#B84-vision-09-00071" class="usa-link" aria-describedby="B84-vision-09-00071">84</a>,<a href="#B115-vision-09-00071" class="usa-link" aria-describedby="B115-vision-09-00071">115</a>,<a href="#B122-vision-09-00071" class="usa-link" aria-describedby="B122-vision-09-00071">122</a>]. Summary sensitivity and specificity for manifest KC diagnosis reached 98.6% and 98.3%, respectively [<a href="#B113-vision-09-00071" class="usa-link" aria-describedby="B113-vision-09-00071">113</a>], while SKC detection reported a slightly lower sensitivity (90.0%) [<a href="#B113-vision-09-00071" class="usa-link" aria-describedby="B113-vision-09-00071">113</a>]. Combining CNNs with color-coded Scheimpflug images showed that the model was dependent on specific spatial regions to differentiate between KC, SKC and non-KC eyes (accuracies ranging from 0.98 to 0.99) [<a href="#B123-vision-09-00071" class="usa-link" aria-describedby="B123-vision-09-00071">123</a>]. One study cited that a combination of corneal elevation data and tomography improved SKC discrimination from normal eyes with 92.5% sensitivity and 92% specificity [<a href="#B124-vision-09-00071" class="usa-link" aria-describedby="B124-vision-09-00071">124</a>].</p>
<p>Other DL approaches included GANs used for data augmentation and representation learning [<a href="#B122-vision-09-00071" class="usa-link" aria-describedby="B122-vision-09-00071">122</a>]. Deep learning on OCT color-coded maps demonstrated 85% accuracy in distinguishing progressive from non-progressive KC using an adjusted age algorithm [<a href="#B61-vision-09-00071" class="usa-link" aria-describedby="B61-vision-09-00071">61</a>]. The Ectasia Status Index (ESI), developed using unsupervised ML from OCT data, effectively assessed KC severity [<a href="#B35-vision-09-00071" class="usa-link" aria-describedby="B35-vision-09-00071">35</a>]. Posterior corneal surface characteristics and thickness were also predictive of disease progression. Neural networks achieved the highest prediction accuracy (98.29%), followed by SVMs (97.72%), though SVMs performed better in terms of training and testing time [<a href="#B107-vision-09-00071" class="usa-link" aria-describedby="B107-vision-09-00071">107</a>].</p>
<p>In a novel approach, researchers assessed KC severity using the RETICS scale. The model achieved 85% sensitivity on validation and 95% on training data [<a href="#B60-vision-09-00071" class="usa-link" aria-describedby="B60-vision-09-00071">60</a>]. Influential risk factors included gender, coma-like aberrations, central thickness, higher-order aberrations, and temporal corneal thinning. This RETICS-based web application enables rapid, objective, and quantitative KC evaluation, aiding early diagnosis and disease grading.</p>
<p>Across 25 ML models that utilized OCT-based topography and multiple corneal parameters (topography, elevation, pachymetry), the most accurate model was an SVM algorithm using eight corneal features, which achieved 94% accuracy [<a href="#B110-vision-09-00071" class="usa-link" aria-describedby="B110-vision-09-00071">110</a>]. These models show strong potential for integration into clinical imaging devices or as diagnostic software for early KC detection and assessment. The keratoconus AI models are summarized in <a href="#vision-09-00071-t001" class="usa-link">Table 1</a>.</p>
<section class="tw xbox font-sm" id="vision-09-00071-t001"><h5 class="obj_head">Table 1.</h5>
<div class="caption p"><p>Summary of keratoconus models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AI Model</th>
<th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Imaging Modality</th>
<th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance Metrics</th>
<th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Notes</th>
</tr></thead>
<tbody>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Random Forest [<a href="#B112-vision-09-00071" class="usa-link" aria-describedby="B112-vision-09-00071">112</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pentacam, Harvard Dataverse (topography); 434 eyes</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 98%, 95%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Used sequential forward selection (SFS) for feature selection</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Support Vector Machine [<a href="#B32-vision-09-00071" class="usa-link" aria-describedby="B32-vision-09-00071">32</a>,<a href="#B66-vision-09-00071" class="usa-link" aria-describedby="B66-vision-09-00071">66</a>,<a href="#B111-vision-09-00071" class="usa-link" aria-describedby="B111-vision-09-00071">111</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3151 OCT images [<a href="#B113-vision-09-00071" class="usa-link" aria-describedby="B113-vision-09-00071">113</a>], Pentacam parameters from 5881 eyes [<a href="#B67-vision-09-00071" class="usa-link" aria-describedby="B67-vision-09-00071">67</a>] and 3502 eyes [<a href="#B32-vision-09-00071" class="usa-link" aria-describedby="B32-vision-09-00071">32</a>] (elevation and topography)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 96.6%, 95%; prediction: &gt;93%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Used cubic kernel and 8 key parameters</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural Networks [<a href="#B27-vision-09-00071" class="usa-link" aria-describedby="B27-vision-09-00071">27</a>,<a href="#B28-vision-09-00071" class="usa-link" aria-describedby="B28-vision-09-00071">28</a>,<a href="#B29-vision-09-00071" class="usa-link" aria-describedby="B29-vision-09-00071">29</a>,<a href="#B51-vision-09-00071" class="usa-link" aria-describedby="B51-vision-09-00071">51</a>,<a href="#B84-vision-09-00071" class="usa-link" aria-describedby="B84-vision-09-00071">84</a>,<a href="#B85-vision-09-00071" class="usa-link" aria-describedby="B85-vision-09-00071">85</a>,<a href="#B86-vision-09-00071" class="usa-link" aria-describedby="B86-vision-09-00071">86</a>,<a href="#B87-vision-09-00071" class="usa-link" aria-describedby="B87-vision-09-00071">87</a>,<a href="#B88-vision-09-00071" class="usa-link" aria-describedby="B88-vision-09-00071">88</a>,<a href="#B89-vision-09-00071" class="usa-link" aria-describedby="B89-vision-09-00071">89</a>,<a href="#B90-vision-09-00071" class="usa-link" aria-describedby="B90-vision-09-00071">90</a>,<a href="#B91-vision-09-00071" class="usa-link" aria-describedby="B91-vision-09-00071">91</a>,<a href="#B92-vision-09-00071" class="usa-link" aria-describedby="B92-vision-09-00071">92</a>,<a href="#B93-vision-09-00071" class="usa-link" aria-describedby="B93-vision-09-00071">93</a>,<a href="#B94-vision-09-00071" class="usa-link" aria-describedby="B94-vision-09-00071">94</a>,<a href="#B95-vision-09-00071" class="usa-link" aria-describedby="B95-vision-09-00071">95</a>,<a href="#B96-vision-09-00071" class="usa-link" aria-describedby="B96-vision-09-00071">96</a>,<a href="#B97-vision-09-00071" class="usa-link" aria-describedby="B97-vision-09-00071">97</a>,<a href="#B98-vision-09-00071" class="usa-link" aria-describedby="B98-vision-09-00071">98</a>,<a href="#B99-vision-09-00071" class="usa-link" aria-describedby="B99-vision-09-00071">99</a>,<a href="#B100-vision-09-00071" class="usa-link" aria-describedby="B100-vision-09-00071">100</a>,<a href="#B101-vision-09-00071" class="usa-link" aria-describedby="B101-vision-09-00071">101</a>,<a href="#B102-vision-09-00071" class="usa-link" aria-describedby="B102-vision-09-00071">102</a>,<a href="#B103-vision-09-00071" class="usa-link" aria-describedby="B103-vision-09-00071">103</a>,<a href="#B104-vision-09-00071" class="usa-link" aria-describedby="B104-vision-09-00071">104</a>,<a href="#B105-vision-09-00071" class="usa-link" aria-describedby="B105-vision-09-00071">105</a>,<a href="#B106-vision-09-00071" class="usa-link" aria-describedby="B106-vision-09-00071">106</a>,<a href="#B107-vision-09-00071" class="usa-link" aria-describedby="B107-vision-09-00071">107</a>,<a href="#B108-vision-09-00071" class="usa-link" aria-describedby="B108-vision-09-00071">108</a>,<a href="#B109-vision-09-00071" class="usa-link" aria-describedby="B109-vision-09-00071">109</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Corneal topography images (quantity varies by study)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 94–100%, specificity: 97.6–100%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High performance across multiple studies</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FPA-K-means (Unsupervised) [<a href="#B110-vision-09-00071" class="usa-link" aria-describedby="B110-vision-09-00071">110</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6961 OCT-based topography</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 96.03%, precision: 96.29%, recall: 96.06%, F-score: 96.17%, purity: 96.03%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Unsupervised, pre-labeling bias-free</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Automated Tree-Based Classifier [<a href="#B81-vision-09-00071" class="usa-link" aria-describedby="B81-vision-09-00071">81</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Topography parameters from 372 eyes</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 100%, specificity: 99.5%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High performance differentiating KC from non-KC</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN + Scheimpflug [<a href="#B123-vision-09-00071" class="usa-link" aria-describedby="B123-vision-09-00071">123</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pentacam four-map color-coded display of 3218 eyes</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracies: 0.98 to 0.99 for tested classes/sets</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Excellent KC, SKC, vs. non-KC eye discrimination</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Soft Voting Ensemble Model [<a href="#B119-vision-09-00071" class="usa-link" aria-describedby="B119-vision-09-00071">119</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None (used clinical data to recommend topography)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 90.5% (internal), 96.4% (external)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Used for triage and screening</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BESTi [<a href="#B120-vision-09-00071" class="usa-link" aria-describedby="B120-vision-09-00071">120</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pentacam (topography) of 2893 eyes</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity and specificity: 84.97%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outperformed PRFI and BAD-D indices for SKC detection</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RETICS-based AI Model [<a href="#B60-vision-09-00071" class="usa-link" aria-describedby="B60-vision-09-00071">60</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None (used morpho-geometric parameters)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 85% (validation), 95% (training)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Categorized severity; considered gender, HOAs, coma-like aberrations</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ectasia Status Index (ESI) [<a href="#B35-vision-09-00071" class="usa-link" aria-describedby="B35-vision-09-00071">35</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3156 eyes selected from OCT</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 97.7%, specificity: 94.1%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Based on posterior corneal surface and thickness; grades KC severity</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gradient Boosting Decision Tree (GBDT) algorithm compared with residents [<a href="#B121-vision-09-00071" class="usa-link" aria-describedby="B121-vision-09-00071">121</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OCT-based topography of 2018 cases</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 95.53% vs. 93.55% (residents), AUC: 0.99</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outperformed ophthalmology residents</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep Learning (adjusted age algorithm) [<a href="#B61-vision-09-00071" class="usa-link" aria-describedby="B61-vision-09-00071">61</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">274 OCT color-coded maps</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 85%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Differentiates progressive from non-progressive KC</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Naïve Bayes [<a href="#B114-vision-09-00071" class="usa-link" aria-describedby="B114-vision-09-00071">114</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not quantified</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Used for patient subgroup stratification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hierarchical Clustering [<a href="#B114-vision-09-00071" class="usa-link" aria-describedby="B114-vision-09-00071">114</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not quantified</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Unsupervised clustering method</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">K-Nearest Neighbor [<a href="#B118-vision-09-00071" class="usa-link" aria-describedby="B118-vision-09-00071">118</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3886 eyes using Scheimpflug-based corneal tomography + biomechanical assessments</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.8% sensitivity, 99.8% specificity, and 99.8% accuracy (left eye); 99.9% sensitivity, 99.4% specificity, and 99.8% accuracy (right eye)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Used for classification and subgrouping</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GAN [<a href="#B122-vision-09-00071" class="usa-link" aria-describedby="B122-vision-09-00071">122</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not specified</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not quantified</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Used for data augmentation and feature representation</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM (progression prediction) [<a href="#B107-vision-09-00071" class="usa-link" aria-describedby="B107-vision-09-00071">107</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">695 eyes using corneal topography</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 97.72%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Second highest accuracy with faster training/testing time</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural Networks (progression prediction) [<a href="#B107-vision-09-00071" class="usa-link" aria-describedby="B107-vision-09-00071">107</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">695 eyes using corneal topography</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 98.29%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Highest accuracy in predicting KC progression</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/vision-09-00071-t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec3dot1dot2-vision-09-00071"><h4 class="pmc_sec_title">3.1.2. Fuch’s Endothelial Corneal Dystrophy</h4>
<p>Fuch’s endothelial corneal dystrophy (FECD) is a progressive disease characterized by corneal endothelial loss [<a href="#B136-vision-09-00071" class="usa-link" aria-describedby="B136-vision-09-00071">136</a>]. As a consequence, this leads to stromal edema and guttata formation [<a href="#B136-vision-09-00071" class="usa-link" aria-describedby="B136-vision-09-00071">136</a>]. This sequence of events can lead to significant visual impairment. Artificial intelligence has shown significant promise in the diagnosis and management of FECD, particularly through imaging modalities such as AS-OCT, specular microscopy, and slit lamp photography.</p>
<p>Several DL models have been developed to differentiate between early- and late-stage FECD, as well as to distinguish FECD from non-FECD corneas. Eleiwa et al., using high-definition OCT images, developed a model capable of differentiating non-FECD corneas from early and late FECD with a sensitivity of 99% and specificity of 98% [<a href="#B137-vision-09-00071" class="usa-link" aria-describedby="B137-vision-09-00071">137</a>]. The same authors using AS-OCT achieved a high accuracy in distinguishing between early-FECD, late-FECD, and non-FECD eyes. Early-stage FECD reached an AUC of 0.997 (sensitivity 91%, specificity 97%), late-stage FECD had an AUC of 0.974 (sensitivity up to 100%, specificity 92%), and distinguishing all FECD cases from non-FECD eyes achieved an AUC of 0.998 (sensitivity 99%, specificity 98%) [<a href="#B137-vision-09-00071" class="usa-link" aria-describedby="B137-vision-09-00071">137</a>]. Another AS-OCT-based DL model diagnosing DED, KC, and FECD demonstrated excellent performance, achieving AUCs of 1.0 (F1 score 100%) for FECD, 0.99 for KC (F1 98%), and 0.99 for DED (F1 90%) [<a href="#B138-vision-09-00071" class="usa-link" aria-describedby="B138-vision-09-00071">138</a>].</p>
<p>In studies using slit lamp photographs (SLP), Gu et al. reported an AUC of 0.939 in detecting corneal dystrophy and degeneration, including FECD, alongside other ocular surface conditions [<a href="#B139-vision-09-00071" class="usa-link" aria-describedby="B139-vision-09-00071">139</a>]. The hierarchical DL model achieved AUCs ranging from 0.903 to 0.951 in retrospective testing and &gt;0.91 in a prospective cohort of 510 cases [<a href="#B139-vision-09-00071" class="usa-link" aria-describedby="B139-vision-09-00071">139</a>]. Another study applying semantic segmentation to SLP images reported a high diagnostic accuracy across ten anterior segment pathologies, with accuracy ranging from 79 to 99%, sensitivity from 53 to 99%, and specificity from 85 to 99% [<a href="#B140-vision-09-00071" class="usa-link" aria-describedby="B140-vision-09-00071">140</a>].</p>
<p>AI has also been used for the quantitative assessment of the corneal endothelium using specular microscopy [<a href="#B141-vision-09-00071" class="usa-link" aria-describedby="B141-vision-09-00071">141</a>]. DL models based on CNN U-net architectures have enhanced the speed and accuracy of morphologic analyses compared with manual assessments and conventional software [<a href="#B141-vision-09-00071" class="usa-link" aria-describedby="B141-vision-09-00071">141</a>]. In one comparative analysis, CNN-based models could estimate endothelial parameters in 98% of images with a percentage error of 2.5–5.7%, while conventional specular microscopy could only analyze 31–72% of images with higher error margins of 7.5–18.3% [<a href="#B142-vision-09-00071" class="usa-link" aria-describedby="B142-vision-09-00071">142</a>]. Despite these advancements, specular microscopy is limited in advanced FECD due to corneal edema that obscures imaging [<a href="#B143-vision-09-00071" class="usa-link" aria-describedby="B143-vision-09-00071">143</a>,<a href="#B144-vision-09-00071" class="usa-link" aria-describedby="B144-vision-09-00071">144</a>].</p>
<p>To overcome these limitations, salience map (SM) imaging has been explored. One study using 775 SM images reported a strong performance in internal validation (AUC 0.92, sensitivity 0.86, specificity 0.86) and a moderate performance in external validation (AUC 0.82, sensitivity 0.74, specificity 0.74) for detecting abnormal images [<a href="#B145-vision-09-00071" class="usa-link" aria-describedby="B145-vision-09-00071">145</a>]. For distinguishing FECD from other diagnoses, internal validation showed an AUC of 0.96 (sensitivity 0.91, specificity 0.91), whereas external validation showed a lower performance (AUC 0.77, sensitivity 0.69, specificity 0.68) [<a href="#B145-vision-09-00071" class="usa-link" aria-describedby="B145-vision-09-00071">145</a>]. A separate DL model identified widefield SM images with an endothelial cell density (ECD) &gt; 1000 cells/mm<sup>2</sup>, diagnosing FECD eyes with a sensitivity of 0.79, specificity of 0.78, and an AUC of 0.88 [<a href="#B145-vision-09-00071" class="usa-link" aria-describedby="B145-vision-09-00071">145</a>].</p>
<p>A support vector machine (SVM) model was used to differentiate non-pathological and pathological corneas based on pupil size and principal components (PCs) [<a href="#B146-vision-09-00071" class="usa-link" aria-describedby="B146-vision-09-00071">146</a>]. For a 3 mm pupil analysis with three PCs, the model achieved an accuracy of 92.8%, a sensitivity of 96.9%, and a precision of 94.8%; with five PCs, the accuracy remained at 92.8%, the sensitivity was 96.1%, and the precision was 95.2% [<a href="#B146-vision-09-00071" class="usa-link" aria-describedby="B146-vision-09-00071">146</a>]. At 5 mm pupil size, performance slightly decreased but remained strong, with an accuracy around 90.2–90.7%, sensitivity of 94.8%, and precision of 92.1–92.7% [<a href="#B146-vision-09-00071" class="usa-link" aria-describedby="B146-vision-09-00071">146</a>].</p>
<p>Another innovation involved the use of edema fraction (EF), the ratio of pixels labeled as edema to the total corneal pixels, as a marker for early disease detection [<a href="#B147-vision-09-00071" class="usa-link" aria-describedby="B147-vision-09-00071">147</a>]. For detecting a 20 µm change in differential central corneal thickness (DCCT), EF achieved an AUC of 0.97 overall, 0.96 in FECD and normal eyes, and 0.99 in non-FECD and normal eyes [<a href="#B147-vision-09-00071" class="usa-link" aria-describedby="B147-vision-09-00071">147</a>].</p>
<p>Additionally, one study highlighted the importance of guttae area ratio (GAR%) as a quantitative imaging biomarker that aligns well with the m-Krachmer grading scale. However, this study was focused on the central corneal region [<a href="#B148-vision-09-00071" class="usa-link" aria-describedby="B148-vision-09-00071">148</a>].</p>
<p>Despite all these advances, a systemic review by Liu et al. found no studies utilizing Scheimpflug imaging (e.g., from Pentacam) for AI training in FECD [<a href="#B149-vision-09-00071" class="usa-link" aria-describedby="B149-vision-09-00071">149</a>]. They hypothesized that this was due to limitations in data accessibility from proprietary formats [<a href="#B149-vision-09-00071" class="usa-link" aria-describedby="B149-vision-09-00071">149</a>]. AS-OCT remains the preferred modality due to its superior resolution and capacity to differentiate between epithelial and stromal edema—critical for evaluating disease severity and treatment response [<a href="#B149-vision-09-00071" class="usa-link" aria-describedby="B149-vision-09-00071">149</a>].</p>
<p>Nevertheless, AI applications in FECD diagnosis have demonstrated excellent diagnostic accuracy, sensitivity, and specificity across imaging modalities, highlighting its potential to augment clinical evaluation, especially in early or ambiguous cases. A summary of the models discussed in this section is found in <a href="#vision-09-00071-t002" class="usa-link">Table 2</a>.</p>
<section class="tw xbox font-sm" id="vision-09-00071-t002"><h5 class="obj_head">Table 2.</h5>
<div class="caption p"><p>Summary of Fuch’s endothelial dystrophy models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Type</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Imaging Modality</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance Metrics</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Notes</th>
</tr></thead>
<tbody>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL model [<a href="#B137-vision-09-00071" class="usa-link" aria-describedby="B137-vision-09-00071">137</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18,720 AS-OCT</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity = 99%, Specificity = 98%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Differentiated normal, early-stage, and late-stage FECD</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL model [<a href="#B137-vision-09-00071" class="usa-link" aria-describedby="B137-vision-09-00071">137</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18,720 AS-OCT</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Early-FECD: AUC = 0.997, sensitivity = 91%, specificity = 97%<br>Late-FECD: AUC = 0.974, sensitivity up to 100%, specificity = 92%<br>FECD vs. non-FECD: AUC = 0.998, sensitivity 99%, specificity 98%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High accuracy in staging and diagnosis of FECD</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL model [<a href="#B138-vision-09-00071" class="usa-link" aria-describedby="B138-vision-09-00071">138</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">158,220 AS-OCT</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FECD: AUC = 1.0, F1 = 100%<br>Keratoconus: AUC = 0.99, F1 = 98%<br>DED: AUC = 0.99, F1 = 90%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-disease diagnosis, including FECD</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hierarchical DL model [<a href="#B139-vision-09-00071" class="usa-link" aria-describedby="B139-vision-09-00071">139</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5325 Slit lamp photography</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC = 0.939 (FECD + other dystrophies), AUC range = 0.90 = 0.95 (retrospective), &gt;0.91 (prospective)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Included FECD as part of broader corneal dystrophy classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Semantic segmentation DL model [<a href="#B140-vision-09-00071" class="usa-link" aria-describedby="B140-vision-09-00071">140</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1772 Slit lamp photography</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy = 79–99%, sensitivity = 53–99%, specificity = 85–99%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Detected 10 anterior segment pathologies including FECD</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-based DL model [<a href="#B142-vision-09-00071" class="usa-link" aria-describedby="B142-vision-09-00071">142</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">383 Specular microscopy</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Success = 98% of images, error = 2.5–5.7% vs. 7.5–18.3% (manual); manual only worked in 31–72% of images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outperformed traditional methods</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL model [<a href="#B145-vision-09-00071" class="usa-link" aria-describedby="B145-vision-09-00071">145</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">775 Salience map images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Internal validation: AUC = 0.92, sensitivity = 0.86, specificity = 0.86<br>External validation: AUC = 0.82, sensitivity = 0.74, specificity = 0.74</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Detected abnormal images across FECD</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL model [<a href="#B145-vision-09-00071" class="usa-link" aria-describedby="B145-vision-09-00071">145</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">775 Salience map images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Internal validation: AUC = 0.96, sensitivity = 0.91, specificity = 0.91<br>External validation: AUC = 0.77, sensitivity = 0.69, specificity = 0.68</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Distinguished FECD from other diagnoses</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL model [<a href="#B145-vision-09-00071" class="usa-link" aria-describedby="B145-vision-09-00071">145</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">775 Widefield salience map images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC = 0.88, Sensitivity = 0.79, Specificity = 0.78</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Detected ECD &gt; 1000 cells/mm<sup>2</sup> to diagnose FECD</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Support Vector Machine [<a href="#B146-vision-09-00071" class="usa-link" aria-describedby="B146-vision-09-00071">146</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not specified (247 eyes in 3 mm group and 149 in 5 mm group)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3 mm pupil: accuracy = 92.8%, sensitivity = 96.9%, precision = 94.8–95.2%<br>5 mm pupil: accuracy = 90.2–90.7%, sensitivity = 94.8%, precision = 92.1–92.7%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Differentiated healthy vs. pathological corneas using statistical features</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL model using edema fraction [<a href="#B147-vision-09-00071" class="usa-link" aria-describedby="B147-vision-09-00071">147</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1992 AS-OCT</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC = 0.97 overall; 0.96 (FECD vs. normal); 0.99 (non-FECD vs. normal)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EF used as early biomarker based on DCCT change</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL model using GAR% [<a href="#B147-vision-09-00071" class="usa-link" aria-describedby="B147-vision-09-00071">147</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">104 eyes from AS-OCT (central cornea)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Correlation = 0.60, <em>p</em> &lt; 0.001</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GAR% correlated with m-Krachmer grading scale</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/vision-09-00071-t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section><section id="sec3dot2-vision-09-00071"><h3 class="pmc_sec_title">3.2. Dry Eye Disease</h3>
<p>Dry eye disease is a prevalent eye condition worldwide, affecting between 5 and 50% of the population, depending on the diagnostic criteria and study population. DED is a multifactorial condition classified into aqueous-deficient and evaporative DED, associated with insufficient tear production and dysfunctional meibomian glands, respectively [<a href="#B150-vision-09-00071" class="usa-link" aria-describedby="B150-vision-09-00071">150</a>].</p>
<p>The subjective nature of diagnostic tools has made DED diagnosis challenging [<a href="#B151-vision-09-00071" class="usa-link" aria-describedby="B151-vision-09-00071">151</a>]. Key clinical signs include decreased tear volume, rapid tear film break-up, and ocular surface microwounds. However, diagnostic tests may not always correlate with symptom severity. A comprehensive DED evaluation utilizes a range of tests to assess tear film parameters, including tear film break-up time (TBUT), Schirmer’s test, tear osmolarity, and tear meniscus height [<a href="#B152-vision-09-00071" class="usa-link" aria-describedby="B152-vision-09-00071">152</a>]. Additional diagnostic tools include ocular surface staining, corneal sensibility, interblink frequency, corneal topography, interferometry, aberrometry, and advanced imaging techniques like meibography and corneal confocal microscopy (CCM) [<a href="#B152-vision-09-00071" class="usa-link" aria-describedby="B152-vision-09-00071">152</a>].</p>
<p>Applying AI to the analysis of DED testing and protocol development for diagnosis and disease monitoring potentially enhances the accuracy and consistency of DED diagnosis. Newer studies have integrated AI with bioinformatics tools in the analysis of biofluid markers for DED. Artificial Neural Networks (ANNs), hierarchical clustering, and RF models have been utilized alongside the protein and metabolite profiling of tears for various functions: classifying disease subgroups, differentiating DED from other ocular surface diseases (OSDs), identifying risk factors, and predicting prognosis [<a href="#B153-vision-09-00071" class="usa-link" aria-describedby="B153-vision-09-00071">153</a>]. One study utilizing a nonlinear iterative partial least squares (NIPALS) algorithm followed by a multi-layer perceptron neural network (MLP NN) achieved 89.3% accuracy in classifying tear proteome profiles of non-dry eye, dry eye, and MGD-associated dry eye individuals [<a href="#B153-vision-09-00071" class="usa-link" aria-describedby="B153-vision-09-00071">153</a>]. Another study employed a multi-layer feed-forward network trained on a seven-biomarker tear panel reported an AUC of 0.93 [<a href="#B153-vision-09-00071" class="usa-link" aria-describedby="B153-vision-09-00071">153</a>]. Despite their strong performance, these models were limited in interpretability and generalizability due to their “black-box” approach [<a href="#B153-vision-09-00071" class="usa-link" aria-describedby="B153-vision-09-00071">153</a>].</p>
<p>Additionally, AI-driven tools have been used to analyze temperature profiles from thermal images of the cornea to distinguish between non-ADDE eyes and those with Aqueous Deficient Dry Eye (ADDE) [<a href="#B154-vision-09-00071" class="usa-link" aria-describedby="B154-vision-09-00071">154</a>]. One study using an infrared thermography (IRT) model reported 84% sensitivity, 83% specificity, and AUC of 0.87 [<a href="#B141-vision-09-00071" class="usa-link" aria-describedby="B141-vision-09-00071">141</a>]. Other IRT studies using ML classifiers like PNN, KNN, and SVM reported near-perfect metrics, with one achieving 99.8% sensitivity, specificity, and accuracy (left eye), and another reaching 99.9% sensitivity, 99.4% specificity, and 99.8% accuracy (right eye) [<a href="#B154-vision-09-00071" class="usa-link" aria-describedby="B154-vision-09-00071">154</a>]. A KNN-based model with ten-fold cross-validation reported 99.88% accuracy, 99.7% sensitivity, and 100% specificity [<a href="#B154-vision-09-00071" class="usa-link" aria-describedby="B154-vision-09-00071">154</a>].</p>
<p>Cartes et al. found that the tear film osmolarity variability was higher in individuals with dry eye syndrome compared with non-dry eye controls. They created a classification model with an initial accuracy of around 85%, emphasizing the potential diagnostic value of tear osmolarity in distinguishing between non-DED individuals and those with DED [<a href="#B155-vision-09-00071" class="usa-link" aria-describedby="B155-vision-09-00071">155</a>].</p>
<p>The analysis of tear proteins’ electrophoretic patterns using an ML-based ANN achieved an accuracy rate of 89% in diagnosing DED. The ANN demonstrated the ability to detect signs of DED from electrophoretic patterns [<a href="#B156-vision-09-00071" class="usa-link" aria-describedby="B156-vision-09-00071">156</a>].</p>
<p>Protein chip array profiling of tear samples, combined with AI analysis, achieved 90% sensitivity and specificity in distinguishing DED patients from non-DED controls [<a href="#B157-vision-09-00071" class="usa-link" aria-describedby="B157-vision-09-00071">157</a>].</p>
<p>The creation of innovative diagnostic algorithms, based on TBUT, is a significant advancement in the assessment of DED as well. Combining video-based TBUT analysis with advanced computational techniques demonstrates the value of using new technologies to tackle challenges linked to diagnosing and grading DED. In 2007, Yedidya et al. developed an algorithm using the EyeScan system to automatically detect DED by analyzing slit lamp videos for the first time. Their initial method achieved 91% accuracy compared with optometrist assessment and was later expanded to include additional tear film metrics [<a href="#B158-vision-09-00071" class="usa-link" aria-describedby="B158-vision-09-00071">158</a>]. Su et al. conducted a study to explore the potential of CNN technology in evaluating tear film stability by analyzing fluorescein TBUT (FTBUT). They developed a CNN-based method to segment fluorescent images and detect areas of broken tear film using a 5 s cutoff value after blinking. The method achieved a high accuracy rate of 98.3% in detecting the break-up area, highlighting the effectiveness of CNN-based FTBUT assessment as a reliable tool for evaluating tear film stability and screening for DED [<a href="#B159-vision-09-00071" class="usa-link" aria-describedby="B159-vision-09-00071">159</a>].</p>
<p>Vyas et al. proposed a new algorithm that uses TBUT data from video recordings to diagnose and grade the severity of DED. Their approach achieved 83% accuracy in detecting TBUT frames and classifying the severity of DED as normal, moderate, or severe [<a href="#B160-vision-09-00071" class="usa-link" aria-describedby="B160-vision-09-00071">160</a>].</p>
<p>Su et al. investigated the effectiveness of CNN classifiers in detecting superficial punctate keratitis (SPK), an important clinical feature of DED. The trained CNN classifiers achieved an accuracy of 97% in detecting punctate dots observed through fluorescein staining. Additionally, the proposed CNN-SPK grading system effectively estimated the coverage of punctate dots and demonstrated a strong correlation (r = 0.81, <em>p</em> &lt; 0.05) with clinical gradings [<a href="#B161-vision-09-00071" class="usa-link" aria-describedby="B161-vision-09-00071">161</a>].</p>
<p>The utilization of tear film lipid interferometer imaging has shown promise in aiding the classification of DED [<a href="#B149-vision-09-00071" class="usa-link" aria-describedby="B149-vision-09-00071">149</a>]. Recent studies have used ML techniques to develop predictive models that analyze tear film lipid interferometer images. These models have shown a strong positive correlation with Schirmer test values and a high level of agreement (0.82) in accurately classifying the images into healthy, aqueous-deficient DED, or evaporative DED categories [<a href="#B162-vision-09-00071" class="usa-link" aria-describedby="B162-vision-09-00071">162</a>,<a href="#B163-vision-09-00071" class="usa-link" aria-describedby="B163-vision-09-00071">163</a>].</p>
<p>Moreover, the use of DL algorithms to study blink patterns in video recordings has shown promising results in assessing DED. Recent studies have shown the frequency of incomplete blinking, as measured by the DL-based algorithm, is closely correlated with the clinical presentation of DED [<a href="#B164-vision-09-00071" class="usa-link" aria-describedby="B164-vision-09-00071">164</a>,<a href="#B165-vision-09-00071" class="usa-link" aria-describedby="B165-vision-09-00071">165</a>].</p>
<p>In a more recent study, researchers used blink videos to study spontaneous blink patterns in patients with DED. They used a U-Net image segmentation algorithm to identify complete and partial blink patterns and developed a ResNet-based DL model to classify the blink videos [<a href="#B166-vision-09-00071" class="usa-link" aria-describedby="B166-vision-09-00071">166</a>]. The study found that DED patients exhibit a higher incidence of partial blinks, shorter closure time, and reduced blink amplitude compared with non-DED controls [<a href="#B166-vision-09-00071" class="usa-link" aria-describedby="B166-vision-09-00071">166</a>]. The U-Net segmentation model achieved 96.3% accuracy, and the classification model achieved 96.0% accuracy [<a href="#B166-vision-09-00071" class="usa-link" aria-describedby="B166-vision-09-00071">166</a>].</p>
<p>By using AS-OCT corneal epithelial mapping data, researchers created a diagnostic AI algorithm using random forest regression. This approach yielded outstanding performance results, with a sensitivity of 86.4% and a specificity of 91.7% [<a href="#B167-vision-09-00071" class="usa-link" aria-describedby="B167-vision-09-00071">167</a>]. Additionally, superior intermediate epithelial thickness was identified as a potential AS-OCT marker for diagnosing DED (AUC: 0.87) [<a href="#B167-vision-09-00071" class="usa-link" aria-describedby="B167-vision-09-00071">167</a>]. The difference between inferior and superior peripheral epithelial zones was identified as the best marker for grading DED [<a href="#B167-vision-09-00071" class="usa-link" aria-describedby="B167-vision-09-00071">167</a>].</p>
<p>A recent study used a rapid, non-invasive DED screening algorithm combining the Symptom Assessment in Dry Eye (SANDE) questionnaire and NIBUT, using optimal cutoffs of SANDE ≥ 30 and NIBUT &lt; 10 s. The results exhibited 86% sensitivity and 94% specificity for detecting DED according to Tear Film and Ocular Surface Society Dry Eye Workshops (TFOS DEWS II) criteria, indicating its potential to facilitate the efficient clinical identification of the condition [<a href="#B168-vision-09-00071" class="usa-link" aria-describedby="B168-vision-09-00071">168</a>].</p>
<p>According to a meta-analysis of AI usage in the diagnosing of DED, the overall accuracy of AI models was found to be 91.91% (95% confidence interval: 87.46–95.49) with a sensitivity of 89.58 (±6.13) and a specificity of 92.62 (±6.61) [<a href="#B169-vision-09-00071" class="usa-link" aria-describedby="B169-vision-09-00071">169</a>]. These data indicate that the combination of bioinformatics and AI-driven technologies in the diagnosis, management, and mass screening of DED potentially improves the accuracy, consistency, and objectivity of DED assessment. The DED AI models are summarized in <a href="#vision-09-00071-t003" class="usa-link">Table 3</a>.</p>
<section class="tw xbox font-sm" id="vision-09-00071-t003"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Summary of dry eye disease models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AI Model</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Imaging Modality</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance Metrics</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Notes</th>
</tr></thead>
<tbody>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NIPALS+ MLP Neural Network (MLP NN) [<a href="#B153-vision-09-00071" class="usa-link" aria-describedby="B153-vision-09-00071">153</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None; used tear proteomics</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 89.3%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classified non-DED, DED, and MGD-associated DED using proteomics</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-layer Feed-forward NN [<a href="#B153-vision-09-00071" class="usa-link" aria-describedby="B153-vision-09-00071">153</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None; used seven-biomarker tear panel</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC: 0.93</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Strong performance but limited interpretability</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hierarchical Clustering + RF [<a href="#B153-vision-09-00071" class="usa-link" aria-describedby="B153-vision-09-00071">153</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None; used tear proteins and metabolites</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not specified</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Differentiated DED from other OSDs; identified subgroups and risk factors</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrared Thermography [<a href="#B154-vision-09-00071" class="usa-link" aria-describedby="B154-vision-09-00071">154</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thermal images (quantity not specified)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 84–99.9%, specificity: 83–99.4%, accuracy: 84–99.8%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IRT + ML (KNN, PNN, SVM); high accuracy for ADDE classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">K-Nearest Neighbor [<a href="#B154-vision-09-00071" class="usa-link" aria-describedby="B154-vision-09-00071">154</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thermography images (quantity not specified)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 99.88%, sensitivity: 99.7%, specificity: 100%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Evaluated corneal temperature profiles</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artificial Neural Network [<a href="#B156-vision-09-00071" class="usa-link" aria-describedby="B156-vision-09-00071">156</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None; tear proteomics (electrophoresis)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 89%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Automatically detected DED from protein electrophoretic patterns</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Protein Chip Array + AI [<a href="#B157-vision-09-00071" class="usa-link" aria-describedby="B157-vision-09-00071">157</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None; tear protein microarrays</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity and specificity: 90%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Differentiated DED patients from non-DED controls</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EyeScan Algorithm [<a href="#B158-vision-09-00071" class="usa-link" aria-describedby="B158-vision-09-00071">158</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 slit lamp videos (TBUT)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 91%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">First automated video-based DED tool (2007), expanded with additional tear metrics</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN (FTBUT) [<a href="#B159-vision-09-00071" class="usa-link" aria-describedby="B159-vision-09-00071">159</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60 fluorescein TBUT videos</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 98.3%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Segmented TBUT break-up areas automatically</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TBUT Video Algorithm [<a href="#B160-vision-09-00071" class="usa-link" aria-describedby="B160-vision-09-00071">160</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30 TBUT video recordings</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 83%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Diagnosed and graded DED severity based on TBUT</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN (SPK detection) [<a href="#B161-vision-09-00071" class="usa-link" aria-describedby="B161-vision-09-00071">161</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5160 fluorescein-stained images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 97%, r = 0.81 (with clinical grading)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Graded punctate dots and correlated with clinical severity</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lipid Interferometer + ML [<a href="#B162-vision-09-00071" class="usa-link" aria-describedby="B162-vision-09-00071">162</a>,<a href="#B163-vision-09-00071" class="usa-link" aria-describedby="B163-vision-09-00071">163</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">414 [<a href="#B163-vision-09-00071" class="usa-link" aria-describedby="B163-vision-09-00071">163</a>] and 138 [<a href="#B164-vision-09-00071" class="usa-link" aria-describedby="B164-vision-09-00071">164</a>], tear film lipid interferometer images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Agreement: 0.82</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classified healthy, ADDE, or evaporative DED</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL-based Blink Analysis [<a href="#B164-vision-09-00071" class="usa-link" aria-describedby="B164-vision-09-00071">164</a>,<a href="#B165-vision-09-00071" class="usa-link" aria-describedby="B165-vision-09-00071">165</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Video recordings from 100 eyes [<a href="#B165-vision-09-00071" class="usa-link" aria-describedby="B165-vision-09-00071">165</a>] and 1196 frame images from a keratograph [<a href="#B166-vision-09-00071" class="usa-link" aria-describedby="B166-vision-09-00071">166</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not quantified</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Linked blink frequency and incomplete blinking to DED</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">U-Net + ResNet [<a href="#B166-vision-09-00071" class="usa-link" aria-describedby="B166-vision-09-00071">166</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Blink video recordings (quantity not specified)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Segmentation accuracy: 96.3%, classification accuracy: 96.0%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Differentiated partial/complete blinks; identified blink dynamics in DED</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Random Forest Regression [<a href="#B167-vision-09-00071" class="usa-link" aria-describedby="B167-vision-09-00071">167</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AS-OCT epithelial mapping of 114 patients</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 86.4%, specificity: 91.7%, AUC: 0.87</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Identified epithelial thickness differences as DED markers</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SANDE + NIBUT Rule-Based Algorithm [<a href="#B168-vision-09-00071" class="usa-link" aria-describedby="B168-vision-09-00071">168</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Clinical survey + NIBUT (non-invasive TBUT) of 235 patients using infrared meibography</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 86%, specificity: 94%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Combined symptom and tear stability data for fast DED screening</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">General AI Models (Meta-analysis) [<a href="#B169-vision-09-00071" class="usa-link" aria-describedby="B169-vision-09-00071">169</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Various</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 91.91%, sensitivity: 89.58%, specificity: 92.62%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Based on pooled results across studies</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/vision-09-00071-t003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec3dot3-vision-09-00071"><h3 class="pmc_sec_title">3.3. Tear Film</h3>
<p>Just as a combination of biofluid markers and AI-driven tools were utilized for DED, similar methodologies have been implemented for tear film analysis. These models are guiding biomarker discovery, differentiation between various OSDs, and making prognostications in clinical settings.</p>
<p>Santos et al., for instance, developed a fully automated model to quantify in vivo tear film thickness using high-resolution OCT. The model demonstrated a relative accuracy of 65% and excellent reproducibility, which can aid in DED diagnosis and treatment evaluation [<a href="#B170-vision-09-00071" class="usa-link" aria-describedby="B170-vision-09-00071">170</a>].</p>
<p>Tear meniscus height (TMH) is a valuable diagnostic tool in the assessment of aqueous-deficient DED. Researchers have developed an automated method using a CNN to segment the tear meniscus area and calculate TMH [<a href="#B171-vision-09-00071" class="usa-link" aria-describedby="B171-vision-09-00071">171</a>]. This system achieved an average Intersection of Union (IoU) of 82.5% and demonstrated a higher correlation (0.965) with ground truth data compared with manual measurements (0.898) [<a href="#B171-vision-09-00071" class="usa-link" aria-describedby="B171-vision-09-00071">171</a>]. The improved accuracy and consistency of this approach suggests the potential to enhance the diagnosis and monitoring of aqueous-deficient DED [<a href="#B171-vision-09-00071" class="usa-link" aria-describedby="B171-vision-09-00071">171</a>].</p>
<p>Stegmann et al. utilized ultrahigh-resolution OCT to assess various tear film parameters, including TMH, tear meniscus area, tear meniscus depth, and tear meniscus radius. Using conventional image processing algorithms, the researchers accurately segmented the tear meniscus and quantified these crucial metrics, revealing significant correlations among them (all <em>p</em> &lt; 0.001, all r ≥ 0.657) [<a href="#B172-vision-09-00071" class="usa-link" aria-describedby="B172-vision-09-00071">172</a>].</p>
<p>Moreover, they developed an ML-based approach to segment the lower tear meniscus using AS-OCT images. Their thresholding-based algorithm demonstrated exceptional performance, achieving a sensitivity of 96% and a specificity of 100% [<a href="#B173-vision-09-00071" class="usa-link" aria-describedby="B173-vision-09-00071">173</a>]. This automated segmentation technique represents a significant advancement in the objective quantification of tear meniscus parameters, which is crucial for the assessment and management of DED [<a href="#B173-vision-09-00071" class="usa-link" aria-describedby="B173-vision-09-00071">173</a>]. A summary of tear film models is shown in <a href="#vision-09-00071-t004" class="usa-link">Table 4</a>.</p>
<section class="tw xbox font-sm" id="vision-09-00071-t004"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>Summary of tear film models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AI Model</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Imaging Modalities</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance Metrics</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Notes</th>
</tr></thead>
<tbody>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Automated Tear Film Thickness Model [<a href="#B170-vision-09-00071" class="usa-link" aria-describedby="B170-vision-09-00071">170</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High-Resolution OCT (quantity not specified)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Relative accuracy: 65%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fully automated; high reproducibility for DED diagnosis and treatment monitoring</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-Based TMH Segmentation [<a href="#B171-vision-09-00071" class="usa-link" aria-describedby="B171-vision-09-00071">171</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">485 Tear Meniscus Images (OCT)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IoU: 82.5%, correlation with ground truth: 0.965 vs. manual 0.898</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Automated measurement of TMH; improved consistency and reliability in aqueous-deficient DED</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Conventional Image Processing [<a href="#B172-vision-09-00071" class="usa-link" aria-describedby="B172-vision-09-00071">172</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">384 Ultrahigh-Resolution OCT images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Significant correlation (all r ≥ 0.657)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accurately segmented TMH, tear meniscus area, depth, radius using traditional methods</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Thresholding-Based ML Algorithm [<a href="#B173-vision-09-00071" class="usa-link" aria-describedby="B173-vision-09-00071">173</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6658 AS-OCT</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 96%, specificity: 100%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Robust segmentation of lower tear meniscus; enhances objective quantification for DED evaluation</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/vision-09-00071-t004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec3dot4-vision-09-00071"><h3 class="pmc_sec_title">3.4. Infectious Keratitis</h3>
<p>Infectious keratitis is a sight-threatening disease that requires early diagnosis due to its significant morbidity and potential for poor outcomes [<a href="#B174-vision-09-00071" class="usa-link" aria-describedby="B174-vision-09-00071">174</a>,<a href="#B175-vision-09-00071" class="usa-link" aria-describedby="B175-vision-09-00071">175</a>,<a href="#B176-vision-09-00071" class="usa-link" aria-describedby="B176-vision-09-00071">176</a>]. Traditional diagnostic tools, such as slit lamp-based evaluations, remain the cornerstone of IK diagnosis [<a href="#B177-vision-09-00071" class="usa-link" aria-describedby="B177-vision-09-00071">177</a>]. However, the visual interpretation of microbial keratitis (MK) patterns is highly subjective and requires extensive clinical experience [<a href="#B178-vision-09-00071" class="usa-link" aria-describedby="B178-vision-09-00071">178</a>]. Standard diagnostic methods like corneal scrapings and cultures are time-consuming, expertise-dependent, and often yield false-negative results due to their low sensitivity [<a href="#B178-vision-09-00071" class="usa-link" aria-describedby="B178-vision-09-00071">178</a>,<a href="#B179-vision-09-00071" class="usa-link" aria-describedby="B179-vision-09-00071">179</a>,<a href="#B180-vision-09-00071" class="usa-link" aria-describedby="B180-vision-09-00071">180</a>,<a href="#B181-vision-09-00071" class="usa-link" aria-describedby="B181-vision-09-00071">181</a>]. These limitations have driven the development and adoption of AI, particularly DL and ML, as more efficient and accurate alternatives [<a href="#B182-vision-09-00071" class="usa-link" aria-describedby="B182-vision-09-00071">182</a>,<a href="#B183-vision-09-00071" class="usa-link" aria-describedby="B183-vision-09-00071">183</a>,<a href="#B184-vision-09-00071" class="usa-link" aria-describedby="B184-vision-09-00071">184</a>,<a href="#B185-vision-09-00071" class="usa-link" aria-describedby="B185-vision-09-00071">185</a>,<a href="#B186-vision-09-00071" class="usa-link" aria-describedby="B186-vision-09-00071">186</a>].</p>
<p>AI models for IK classification commonly utilize images from various modalities, including slit lamp, digital anterior segment photographs (ASP), and IVCM [<a href="#B182-vision-09-00071" class="usa-link" aria-describedby="B182-vision-09-00071">182</a>,<a href="#B183-vision-09-00071" class="usa-link" aria-describedby="B183-vision-09-00071">183</a>,<a href="#B184-vision-09-00071" class="usa-link" aria-describedby="B184-vision-09-00071">184</a>,<a href="#B185-vision-09-00071" class="usa-link" aria-describedby="B185-vision-09-00071">185</a>,<a href="#B186-vision-09-00071" class="usa-link" aria-describedby="B186-vision-09-00071">186</a>,<a href="#B187-vision-09-00071" class="usa-link" aria-describedby="B187-vision-09-00071">187</a>]. These models serve several diagnostic functions: distinguishing infectious from noninfectious keratitis, differentiating among bacterial, fungal, viral, and Acanthamoeba keratitis, and even identifying fungal subtypes like yeast versus filamentous species [<a href="#B182-vision-09-00071" class="usa-link" aria-describedby="B182-vision-09-00071">182</a>,<a href="#B183-vision-09-00071" class="usa-link" aria-describedby="B183-vision-09-00071">183</a>,<a href="#B184-vision-09-00071" class="usa-link" aria-describedby="B184-vision-09-00071">184</a>,<a href="#B185-vision-09-00071" class="usa-link" aria-describedby="B185-vision-09-00071">185</a>,<a href="#B186-vision-09-00071" class="usa-link" aria-describedby="B186-vision-09-00071">186</a>,<a href="#B187-vision-09-00071" class="usa-link" aria-describedby="B187-vision-09-00071">187</a>].</p>
<p>As early as 2003, Jagjit S. Saini et al. introduced neural networks for identifying bacterial keratitis (BK) and fungal keratitis (FK), achieving an accuracy of 90.7% and outperforming clinician diagnoses [<a href="#B188-vision-09-00071" class="usa-link" aria-describedby="B188-vision-09-00071">188</a>]. Since then, AI applications have expanded significantly; CNN-based DL models developed for slit lamp photographs demonstrated a high diagnostic accuracy in multiple studies. One study developed three models: one for diagnosing IK (accuracy 99.3%), another for distinguishing BK from FK (accuracy ~84%), and a third for classifying yeast versus filamentous fungi (accuracy 77.5%) [<a href="#B183-vision-09-00071" class="usa-link" aria-describedby="B183-vision-09-00071">183</a>].</p>
<p>More complex DL systems, like DeepIK, mimicked expert diagnostic processes using a two-stage classifier: one to distinguish infectious from noninfectious keratitis and another to classify specific etiologies. DeepIK outperformed other DL models like DenseNet121, InceptionResNetV2, and Swin-Transformer, in line with reference standards (Cohen’s Kappa 0.70–0.77) and rapid image processing (0.034 s/image) [<a href="#B185-vision-09-00071" class="usa-link" aria-describedby="B185-vision-09-00071">185</a>]. VGG19, ResNet50, and DenseNet121 were evaluated on 2167 mixed BK and FK images; VGG19 outperformed the others with an F1 score of 0.78 and AUPRC of 0.86 [<a href="#B189-vision-09-00071" class="usa-link" aria-describedby="B189-vision-09-00071">189</a>].</p>
<p>Large-scale DL evaluations have further validated these models. For instance, an ensemble model analyzing 2167 slit lamp photos achieved a high performance in differentiating BK from FK, with sensitivity, F1 scores, and AUPRC of 0.77, 0.83, and 0.904, respectively [<a href="#B190-vision-09-00071" class="usa-link" aria-describedby="B190-vision-09-00071">190</a>,<a href="#B191-vision-09-00071" class="usa-link" aria-describedby="B191-vision-09-00071">191</a>]. Other models trained on 1330 cropped images reached a diagnostic accuracy of 80.0% for BK and FK [<a href="#B190-vision-09-00071" class="usa-link" aria-describedby="B190-vision-09-00071">190</a>]. MobileNet achieved the highest performance among five CNNs, with an AUC of 0.86 in single-center and 0.83 in multi-center tests [<a href="#B191-vision-09-00071" class="usa-link" aria-describedby="B191-vision-09-00071">191</a>]. ResNet-50 CNN models showed a specificity and sensitivity of 80% and 70% for BK, and 70% and 80% for FK, respectively [<a href="#B192-vision-09-00071" class="usa-link" aria-describedby="B192-vision-09-00071">192</a>].</p>
<p>Advanced AI methods also analyze lesion characteristics. Natural language processing (NLP) algorithms quantified microbial keratitis (MK) centrality, thinning, and depth with sensitivities ranging from 50% to 100% [<a href="#B193-vision-09-00071" class="usa-link" aria-describedby="B193-vision-09-00071">193</a>]. EfficientNet B3 achieved a sensitivity and specificity of 74 and 64, respectively, for BK diagnosis, comparable to ophthalmologists [<a href="#B194-vision-09-00071" class="usa-link" aria-describedby="B194-vision-09-00071">194</a>].</p>
<p>For lesion segmentation, models like SLIT-Net and multi-scale CNNs with ResNeXt achieved up to 88.96% diagnostic accuracy [<a href="#B195-vision-09-00071" class="usa-link" aria-describedby="B195-vision-09-00071">195</a>,<a href="#B196-vision-09-00071" class="usa-link" aria-describedby="B196-vision-09-00071">196</a>]. Region-based CNNs reached Dice similarity coefficients between 0.74 and 0.76 for segmenting stromal infiltrates, hypopyons, and other features [<a href="#B197-vision-09-00071" class="usa-link" aria-describedby="B197-vision-09-00071">197</a>]. ResNet50-based multi-attribute networks identified keratitis characteristics with 89.51% accuracy [<a href="#B198-vision-09-00071" class="usa-link" aria-describedby="B198-vision-09-00071">198</a>]. Ming-Tse Kuo’s DenseNet model used corneal photos for FK diagnosis, and semi-automated algorithms measuring epithelial defects (ED) showed high reliability (ICC 0.96–0.98) compared with ophthalmologists (ICC 0.84–0.88) [<a href="#B199-vision-09-00071" class="usa-link" aria-describedby="B199-vision-09-00071">199</a>,<a href="#B200-vision-09-00071" class="usa-link" aria-describedby="B200-vision-09-00071">200</a>,<a href="#B201-vision-09-00071" class="usa-link" aria-describedby="B201-vision-09-00071">201</a>].</p>
<p>Image enhancement techniques like histogram matching fusion (HMF) have been applied to AlexNet and VGGNet models, yielding accuracies as high as 99.95% [<a href="#B202-vision-09-00071" class="usa-link" aria-describedby="B202-vision-09-00071">202</a>]. Confocal texture analysis using an adaptive robust binary pattern (ARBP) reached 99.74% accuracy, with a TPR, TNR, and AUC near 1 [<a href="#B203-vision-09-00071" class="usa-link" aria-describedby="B203-vision-09-00071">203</a>]. A ResNet-based DL model for FK via IVCM demonstrated a high AUC (0.987), accuracy (0.962), sensitivity (0.918), and specificity (0.9834) [<a href="#B204-vision-09-00071" class="usa-link" aria-describedby="B204-vision-09-00071">204</a>].</p>
<p>Web-based models using confocal microscopy and networks like AlexNet, ZFNet, and VGG16 showed excellent performance, with VGG16 achieving a high accuracy (0.992), sensitivity (0.993), specificity (0.992), and AUC (0.999) [<a href="#B205-vision-09-00071" class="usa-link" aria-describedby="B205-vision-09-00071">205</a>]. Slit lamp photo datasets used by Li et al. and others covered thousands of cases. One DL model processed 1772 slit lamp photos to classify corneal disorders, while another processed 5325 images to distinguish between IK subtypes, neoplasms, and dystrophies, with AUCs over 0.910 [<a href="#B140-vision-09-00071" class="usa-link" aria-describedby="B140-vision-09-00071">140</a>].</p>
<p>In another study, a hybrid DL model on 4306 images yielded the following accuracy and AUC: 90.7% and 0.963 for bacteria, 95.0% and 0.975 for fungi, 97.9% and 0.995 for Acanthamoeba, and 92.3% and 0.946 for HSV, respectively [<a href="#B206-vision-09-00071" class="usa-link" aria-describedby="B206-vision-09-00071">206</a>]. A separate CNN study using 10,739 images achieved accuracies of 91.91%, 79.77%, and 81.27% for BK, FK, and Acanthamoeba, respectively [<a href="#B207-vision-09-00071" class="usa-link" aria-describedby="B207-vision-09-00071">207</a>].</p>
<p>Models using 928 slit photos achieved near-perfect training and testing accuracies of 100%, 99.1%, and 99.6% across AlexNet, VGG-16, and VGG-19 networks [<a href="#B208-vision-09-00071" class="usa-link" aria-describedby="B208-vision-09-00071">208</a>]. In sequential-level DL using 362 photos, diagnostic accuracies for BK, FK, and HSK surpassed 121 ophthalmologists at 78.7%, 74.23%, and 75.1%, respectively [<a href="#B209-vision-09-00071" class="usa-link" aria-describedby="B209-vision-09-00071">209</a>]. Visual Concept Mining (VCM) improved classification through pixel-level saliency analysis, with DenseNet121 yielding the best F1 scores (0.431 for BK, 0.872 for FK, 0.651 for HSK) [<a href="#B210-vision-09-00071" class="usa-link" aria-describedby="B210-vision-09-00071">210</a>]. DenseNet121 also outperformed ResNet50 and InceptionV3 in classifying healthy eyes versus BK, FK, and HSK with a 72% accuracy [<a href="#B211-vision-09-00071" class="usa-link" aria-describedby="B211-vision-09-00071">211</a>]. Additionally, HSV necrotizing stromal keratitis classification using DenseNet on 307 images showed 72% accuracy, 0.73 AUC, 69.6% sensitivity, and 76.5% specificity [<a href="#B212-vision-09-00071" class="usa-link" aria-describedby="B212-vision-09-00071">212</a>]. Other CNN models distinguished between active and scarred keratitis with 78.2% sensitivity and 91.3% specificity [<a href="#B213-vision-09-00071" class="usa-link" aria-describedby="B213-vision-09-00071">213</a>]. SVM classifier models were able to classify ulcer severity better than type, however [<a href="#B214-vision-09-00071" class="usa-link" aria-describedby="B214-vision-09-00071">214</a>].</p>
<p>For AK diagnosis, a CNN based on IVCM images (HRT3) differentiated AK from nonspecific findings with 76% accuracy, sensitivity, and specificity [<a href="#B185-vision-09-00071" class="usa-link" aria-describedby="B185-vision-09-00071">185</a>]. Meanwhile, ML models including logistic regression, random forest, and decision trees have also been explored for FK diagnosis, alongside lasso regression for assessing clinical signs [<a href="#B183-vision-09-00071" class="usa-link" aria-describedby="B183-vision-09-00071">183</a>].</p>
<p>Systematic reviews report a varied performance: a pooled accuracy of 64.38% for BK vs. FK classification and 96.6% for infectious vs. noninfectious keratitis classification [<a href="#B180-vision-09-00071" class="usa-link" aria-describedby="B180-vision-09-00071">180</a>]. DL was found to outperform human experts in all comparative studies [<a href="#B180-vision-09-00071" class="usa-link" aria-describedby="B180-vision-09-00071">180</a>]. IVCM-based DL models were more effective than ASP-based ones, with a higher sensitivity (91.8% vs. 86.2%) and specificity (94.0% vs. 83.6%), likely due to better image consistency and patient selection [<a href="#B181-vision-09-00071" class="usa-link" aria-describedby="B181-vision-09-00071">181</a>]. A summary of keratitis models can be seen in <a href="#vision-09-00071-t005" class="usa-link">Table 5</a>.</p>
<section class="tw xbox font-sm" id="vision-09-00071-t005"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>Summary of infectious keratitis models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Type</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Imaging Modality</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance Metrics</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Notes</th>
</tr></thead>
<tbody>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN (3 models) [<a href="#B183-vision-09-00071" class="usa-link" aria-describedby="B183-vision-09-00071">183</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9329 slit lamp photos</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracies: 99.3% (IK), ~84% (BK vs. FK), 77.5% (yeast vs. filamentous)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-task model for IK diagnosis and etiology differentiation</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DeepIK (custom DL) [<a href="#B185-vision-09-00071" class="usa-link" aria-describedby="B185-vision-09-00071">185</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23,055 slit lamp photos</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cohen’s Kappa: 0.70–0.77; time/image: 0.034 sec</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Two-stage classification mimicking expert workflow; outperformed other DL models</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural networks [<a href="#B188-vision-09-00071" class="usa-link" aria-describedby="B188-vision-09-00071">188</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None (input clinical variables)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 90.7%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Early AI model distinguishing BK vs. FK; outperformed clinicians</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ensemble CNN [<a href="#B190-vision-09-00071" class="usa-link" aria-describedby="B190-vision-09-00071">190</a>,<a href="#B191-vision-09-00071" class="usa-link" aria-describedby="B191-vision-09-00071">191</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2167 slit lamp photos</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 0.77; F1: 0.83; AUPRC: 0.904</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BK vs. FK classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN [<a href="#B190-vision-09-00071" class="usa-link" aria-describedby="B190-vision-09-00071">190</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1330 cropped images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 80%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BK and FK classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MobileNet [<a href="#B191-vision-09-00071" class="usa-link" aria-describedby="B191-vision-09-00071">191</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">980 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC: 0.86 (single-center), 0.83 (multi-center)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN comparison for BK/FK detection</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet-50 [<a href="#B192-vision-09-00071" class="usa-link" aria-describedby="B192-vision-09-00071">192</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Slit lamp images (quantity not specified)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Specificity/sensitivity (BK): 80%/70%; (FK): 70%/80%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Differentiating BK vs. FK</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NLP algorithm [<a href="#B193-vision-09-00071" class="usa-link" aria-describedby="B193-vision-09-00071">193</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">None (qualitative phrases from notes)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 50–100%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Quantified MK features: centrality, thinning, depth</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EfficientNet B3 [<a href="#B194-vision-09-00071" class="usa-link" aria-describedby="B194-vision-09-00071">194</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1512 slit lamp images </td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity: 74%; specificity: 64%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BK diagnosis; performance comparable to ophthalmologists</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG19, ResNet50, DenseNet121 [<a href="#B189-vision-09-00071" class="usa-link" aria-describedby="B189-vision-09-00071">189</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2167 mixed BK/FK images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Best: VGG19 F1 score: 0.78; AUPRC: 0.861</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Comparative study for BK/FK classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SLIT-Net, ResNeXt [<a href="#B195-vision-09-00071" class="usa-link" aria-describedby="B195-vision-09-00071">195</a>,<a href="#B196-vision-09-00071" class="usa-link" aria-describedby="B196-vision-09-00071">196</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">195 white light and 148 blue light slit lamp images [<a href="#B196-vision-09-00071" class="usa-link" aria-describedby="B196-vision-09-00071">196</a>]; 133 clinically suspected slit lamp images and 540 collated public-domain images [<a href="#B197-vision-09-00071" class="usa-link" aria-describedby="B197-vision-09-00071">197</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: up to 88.96%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lesion segmentation models</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R-CNN [<a href="#B197-vision-09-00071" class="usa-link" aria-describedby="B197-vision-09-00071">197</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80 Slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dice coefficient: 0.74–0.76</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Segmentation of IK features (e.g., infiltrates, hypopyons)</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50 (multi-attribute) [<a href="#B198-vision-09-00071" class="usa-link" aria-describedby="B198-vision-09-00071">198</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Slit lamp images (quantity not specified)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 89.51%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-attribute IK feature detection</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DenseNet + semi-automated algorithm [<a href="#B199-vision-09-00071" class="usa-link" aria-describedby="B199-vision-09-00071">199</a>,<a href="#B200-vision-09-00071" class="usa-link" aria-describedby="B200-vision-09-00071">200</a>,<a href="#B201-vision-09-00071" class="usa-link" aria-describedby="B201-vision-09-00071">201</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">288 Slit lamp photos [<a href="#B200-vision-09-00071" class="usa-link" aria-describedby="B200-vision-09-00071">200</a>]; 92 images [<a href="#B201-vision-09-00071" class="usa-link" aria-describedby="B201-vision-09-00071">201</a>]; not specified [<a href="#B202-vision-09-00071" class="usa-link" aria-describedby="B202-vision-09-00071">202</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ICC: 0.96–0.98 (automated), 0.84–0.88 (human)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FK diagnosis; epithelial defect measurement</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AlexNet, VGGNet + HMF [<a href="#B202-vision-09-00071" class="usa-link" aria-describedby="B202-vision-09-00071">202</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">977 abnormal and 876 normal slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 99.95%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Used histogram matching fusion for enhancement</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARBP texture analysis [<a href="#B203-vision-09-00071" class="usa-link" aria-describedby="B203-vision-09-00071">203</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">183 normal and 195 abnormal confocal microscopy images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 99.74%; TPR, TNR, AUC ≈ 1</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Texture-based classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet [<a href="#B204-vision-09-00071" class="usa-link" aria-describedby="B204-vision-09-00071">204</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2088 in vivo confocal microscopy</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC: 0.987; accuracy: 0.962; sensitivity: 0.918; specificity: 0.9834</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FK classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AlexNet, ZFNet, VGG16 [<a href="#B205-vision-09-00071" class="usa-link" aria-describedby="B205-vision-09-00071">205</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Confocal microscopy (quantity not specified)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG16: accuracy: 0.992; sensitivity: 0.993; specificity: 0.992; AUC: 0.999</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High performance in IK classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL models [<a href="#B140-vision-09-00071" class="usa-link" aria-describedby="B140-vision-09-00071">140</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1772 and 5325 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUCs &gt; 0.91</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classification of IK subtypes and other corneal pathologies</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hybrid DL [<a href="#B206-vision-09-00071" class="usa-link" aria-describedby="B206-vision-09-00071">206</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4306 images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bacteria: 90.7%/0.963; fungi: 95.0%/0.975; Acanthamoeba: 97.9%/0.995; HSV: 92.3%/0.946</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-pathogen classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN [<a href="#B207-vision-09-00071" class="usa-link" aria-describedby="B207-vision-09-00071">207</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10,739 images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BK: 91.91%; FK: 79.77%; Acanthamoeba: 81.27%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-class classification study</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AlexNet, VGG-16, VGG-19 [<a href="#B208-vision-09-00071" class="usa-link" aria-describedby="B208-vision-09-00071">208</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">928 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Training: ~100%; testing: 99.1–100%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IK classification across three networks</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sequential CNN [<a href="#B209-vision-09-00071" class="usa-link" aria-describedby="B209-vision-09-00071">209</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">362 images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BK: 78.7%; FK: 74.23%; HSK: 75.1%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outperformed 121 ophthalmologists</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DenseNet121 + VCM [<a href="#B210-vision-09-00071" class="usa-link" aria-describedby="B210-vision-09-00071">210</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3319 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1: BK 0.431; FK 0.872; HSK 0.651</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pixel-level saliency-enhanced classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">InceptionV3 [<a href="#B211-vision-09-00071" class="usa-link" aria-describedby="B211-vision-09-00071">211</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5673 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DenseNet121 outperformed ResNet50 and InceptionV3</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal vs. BK, FK, HSK classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DenseNet [<a href="#B212-vision-09-00071" class="usa-link" aria-describedby="B212-vision-09-00071">212</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">307 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 72%; AUC: 0.73; sensitivity: 69.6%; specificity: 76.5%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HSV necrotizing stromal keratitis classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN [<a href="#B186-vision-09-00071" class="usa-link" aria-describedby="B186-vision-09-00071">186</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3312 in vivo confocal microscopy (HRT3)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy, sensitivity, specificity: 76%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acanthamoeba keratitis diagnosis</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Logistic regression, RF, DT, Lasso [<a href="#B184-vision-09-00071" class="usa-link" aria-describedby="B184-vision-09-00071">184</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1047 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Internal validation: mean AUC of 0.916, 0.920, and 0.859, respectively</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Explored for FK diagnosis and clinical sign prediction</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Systematic review [<a href="#B181-vision-09-00071" class="usa-link" aria-describedby="B181-vision-09-00071">181</a>,<a href="#B182-vision-09-00071" class="usa-link" aria-describedby="B182-vision-09-00071">182</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34,070 slit lamp photos; 136,401 anterior segment photos, AS-OCT, IVCM, and corneal topography</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 64.38% (BK vs. FK); 96.6% (infectious vs. noninfectious)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL models outperformed clinicians; IVCM superior to ASP</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/vision-09-00071-t005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec3dot5-vision-09-00071"><h3 class="pmc_sec_title">3.5. Corneal Neuropathy</h3>
<p>Diabetes adversely impacts corneal nerves through reduced sensitivity and epithelial regeneration, thereby potentially making the corneal nerve plexus a surrogate for diabetic peripheral neuropathy [<a href="#B215-vision-09-00071" class="usa-link" aria-describedby="B215-vision-09-00071">215</a>]. Manifestations range from decreased sensitivity to vision-threatening ulcers. Changes include decreased branching, reduced sub-basal density, and increased tortuosity, potentially representing regeneration [<a href="#B215-vision-09-00071" class="usa-link" aria-describedby="B215-vision-09-00071">215</a>]. Moreover, the cornea’s integrity relies on its dense innervation, enabling the protection of its tear film through reflex arcs. The disruption of the tear film in DED leads to epithelial and nerve damage, resulting in nerve dysfunction that drives DED progression, ocular pain, and neuropathic symptoms [<a href="#B216-vision-09-00071" class="usa-link" aria-describedby="B216-vision-09-00071">216</a>]. The sub-basal plexus is more accessible for in vivo confocal evaluation than stromal nerves. The IVCM evaluation of corneal nerve fiber length (CNFL) is a valuable diagnostic tool, but current manual methods lack efficiency and objectivity. Rapid and automated segmentation techniques for evaluating the morphology of sub-basal corneal nerves are essential for the accurate diagnosis and effective management of corneal neuropathies [<a href="#B217-vision-09-00071" class="usa-link" aria-describedby="B217-vision-09-00071">217</a>].</p>
<p>Preston et al. designed an AI-based algorithm that could classify CCM images of patients without the need for prior image segmentation [<a href="#B218-vision-09-00071" class="usa-link" aria-describedby="B218-vision-09-00071">218</a>]. The algorithm demonstrated high levels of sensitivity, correctly identifying healthy control subjects with a sensitivity of 1.0 [<a href="#B218-vision-09-00071" class="usa-link" aria-describedby="B218-vision-09-00071">218</a>]. Furthermore, it achieved sensitivities of 0.85 in detecting patients with diabetic peripheral neuropathy (DPN) and 0.83 for patients without DPN [<a href="#B218-vision-09-00071" class="usa-link" aria-describedby="B218-vision-09-00071">218</a>].</p>
<p>Salahoudin et al. developed a novel, automated AI-based algorithm that utilized CCM images to rapidly quantify corneal nerve fiber length and classify DPN patients [<a href="#B219-vision-09-00071" class="usa-link" aria-describedby="B219-vision-09-00071">219</a>]. The algorithm, built upon a U-Net network and adaptive neuro-fuzzy inference system, achieved an AUC of 0.95 (92% sensitivity, 80% specificity) for detecting patients with and without DPN [<a href="#B219-vision-09-00071" class="usa-link" aria-describedby="B219-vision-09-00071">219</a>]. Furthermore, the algorithm demonstrated an AUC of 1.0 (100% sensitivity, 95% specificity) in discriminating healthy subjects from DPN patients, surpassing the ACCMetrics method [<a href="#B219-vision-09-00071" class="usa-link" aria-describedby="B219-vision-09-00071">219</a>].</p>
<p>Scarpa et al. developed a CNN model that utilized three non-overlapping CCM images from each eye to discriminate between healthy controls and individuals with DPN [<a href="#B220-vision-09-00071" class="usa-link" aria-describedby="B220-vision-09-00071">220</a>]. The CNN model demonstrated excellent performance, achieving a sensitivity of 0.98, a specificity of 0.96, and an overall accuracy of 0.97 in this classification task [<a href="#B220-vision-09-00071" class="usa-link" aria-describedby="B220-vision-09-00071">220</a>].</p>
<p>William et al. developed an innovative model that can automatically segment sub-basal nerve plexus fibers in CCM images, demonstrating promising diagnostic capabilities with a sensitivity of 0.68, a specificity of 0.87, and an AUC of 0.83 in detecting DPN [<a href="#B221-vision-09-00071" class="usa-link" aria-describedby="B221-vision-09-00071">221</a>]. Importantly, the model outperformed ACCMetrics in quantifying and evaluating corneal nerve fiber morphology [<a href="#B221-vision-09-00071" class="usa-link" aria-describedby="B221-vision-09-00071">221</a>].</p>
<p>Mou et al. developed a CNN-based deep grading algorithm that demonstrated a superior performance in segmenting and quantifying corneal nerve tortuosity, achieving 85.64% accuracy in four-level classification [<a href="#B222-vision-09-00071" class="usa-link" aria-describedby="B222-vision-09-00071">222</a>]. This automated DL algorithm identified significant differences in nerve tortuosity between diabetic patients and non-diabetic controls [<a href="#B222-vision-09-00071" class="usa-link" aria-describedby="B222-vision-09-00071">222</a>]. The results of these models are summarized in <a href="#vision-09-00071-t006" class="usa-link">Table 6</a>.</p>
<section class="tw xbox font-sm" id="vision-09-00071-t006"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>Summary of corneal neuropathy models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AI Model</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Image Modality</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance Metrics</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Diagnostic Target</th>
</tr></thead>
<tbody>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Modified ResNet-50 [<a href="#B218-vision-09-00071" class="usa-link" aria-describedby="B218-vision-09-00071">218</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">369 confocal microscopy images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity 1.0 for healthy controls, 0.85 for DPN+, 0.83 for DPN-</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Diabetic peripheral neuropathy (DPN)</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">U-Net + adaptive neuro-fuzzy inference system [<a href="#B219-vision-09-00071" class="usa-link" aria-describedby="B219-vision-09-00071">219</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trained on 174 confocal microscopy images, validated with 534</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC 0.95 (92% sensitivity, 80% specificity) for DPN detection</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Diabetic neuropathy (DPN)</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN model [<a href="#B220-vision-09-00071" class="usa-link" aria-describedby="B220-vision-09-00071">220</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">600 confocal microscopy images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity 0.98, specificity 0.96, accuracy 0.97</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DPN detection</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Automatic segmentation model [<a href="#B221-vision-09-00071" class="usa-link" aria-describedby="B221-vision-09-00071">221</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1698 confocal microscopy images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity 0.68, specificity 0.87, AUC 0.83</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DPN detection</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep Grading CNN [<a href="#B222-vision-09-00071" class="usa-link" aria-describedby="B222-vision-09-00071">222</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">354 confocal microscopy images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.64% accuracy</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Nerve tortuosity quantification</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/vision-09-00071-t006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec3dot6-vision-09-00071"><h3 class="pmc_sec_title">3.6. Conjunctiva</h3>
<p>Ocular diseases affecting the conjunctiva, such as conjunctivitis and DED, significantly impact vision and quality of life. A wide range of AI models have been explored for the diagnosis and classification of conjunctival and ocular surface diseases, demonstrating promising results across multiple modalities and disease categories. Traditionally, conjunctival assessment relied on subjective clinician evaluations. However, the introduction of AI-powered diagnostic tools will potentially revolutionize the objectivity of conjunctival examination [<a href="#B223-vision-09-00071" class="usa-link" aria-describedby="B223-vision-09-00071">223</a>].</p>
<section id="sec3dot6dot1-vision-09-00071"><h4 class="pmc_sec_title">3.6.1. Vascular Assessments</h4>
<p>AI tools have been applied to various types of vascular assessments and disease diagnostics. For instance, Delgado-Rivera et al. applied CNN to segmented conjunctival images and achieved a 77.58% sensitivity in detecting anemia compared with laboratory test results [<a href="#B224-vision-09-00071" class="usa-link" aria-describedby="B224-vision-09-00071">224</a>].</p>
<p>Derakhshani et al. explored two approaches to assess conjunctival vascularity from color digital images, with the best method achieving a 0.89 correlation between predicted and actual values using an ANN [<a href="#B225-vision-09-00071" class="usa-link" aria-describedby="B225-vision-09-00071">225</a>]. Conjunctival blood velocity is known to decrease in various ocular diseases such as diabetic retinopathy and DED; hence, it serves as a valuable indicator of disease progression. However, eye movements during image acquisition and motion artifacts can hinder the accuracy of conjunctival blood flow segmentation and velocity measurement. To address this challenge, Jo et al. developed a motion correction algorithm and a segmentation approach to blurred images based on the Attention U-Net architecture [<a href="#B226-vision-09-00071" class="usa-link" aria-describedby="B226-vision-09-00071">226</a>]. Owen et al. developed an automated algorithm for measuring conjunctival vessel width, which demonstrated high session reliability and strong agreement with manual assessment methods on digital photographs [<a href="#B227-vision-09-00071" class="usa-link" aria-describedby="B227-vision-09-00071">227</a>]. Given the clinical value of monitoring ocular interventions and diseases, such as diabetes, this automated approach can facilitate the accurate and rapid assessment of conjunctival vessels [<a href="#B227-vision-09-00071" class="usa-link" aria-describedby="B227-vision-09-00071">227</a>].</p>
<p>Researchers developed a neural network system trained on the Japan Ocular Allergy Society (JOAS) criteria to accurately grade the severity of conjunctival hyperemia, successfully determining the vessel coverage area in 71.8% of images, with a strong correlation (r = 0.737, <em>p</em> &lt; 0.01) between the model’s predictions and expert assessments [<a href="#B228-vision-09-00071" class="usa-link" aria-describedby="B228-vision-09-00071">228</a>].</p>
<p>The study by Li et al. showed that diabetes could be detected from conjunctival images with 75.1% accuracy [<a href="#B229-vision-09-00071" class="usa-link" aria-describedby="B229-vision-09-00071">229</a>]. Changes in the conjunctival microcirculation reflect diabetic vasculopathy, with 78.7% sensitivity and 69.0% specificity for type 2 diabetes diagnosis [<a href="#B229-vision-09-00071" class="usa-link" aria-describedby="B229-vision-09-00071">229</a>].</p>
<p>For ocular surface neovascularization, a fine-tuned U-Net model on 120 annotated slit lamp images showed excellent segmentation capabilities as well. The IoU scores for detecting total corneal area ranged from 90.0% to 95.5%, and for non-vascularized regions from 76.6% to 82.2%, with specificity values above 96% for both [<a href="#B230-vision-09-00071" class="usa-link" aria-describedby="B230-vision-09-00071">230</a>].</p></section><section id="sec3dot6dot2-vision-09-00071"><h4 class="pmc_sec_title">3.6.2. Ocular Surface Tumors</h4>
<p>A recent systemic review discussed the usage of deep learning algorithms in combination with anterior segment swept-source OCT (AS SS-OCT). Studies showed success in the detection and classification of angle closure glaucoma, with near-perfect sensitivity and specificity [<a href="#B231-vision-09-00071" class="usa-link" aria-describedby="B231-vision-09-00071">231</a>]. These models were also able to strongly estimate visual acuity in patients with senile cataracts [<a href="#B231-vision-09-00071" class="usa-link" aria-describedby="B231-vision-09-00071">231</a>]. The usage of AS SS-OCT as an imaging modality for AI models offers potential in extending its use in diagnosing conjunctival diseases such as conjunctival tumors and structural lesions for future studies.</p>
<p>Yoo et al. developed a CNN model to diagnose various conjunctival conditions, including rare diseases such as conjunctival melanoma, which has an incidence as low as 0.3 per 1,000,000 [<a href="#B232-vision-09-00071" class="usa-link" aria-describedby="B232-vision-09-00071">232</a>]. Given the limited image data available for training, the researchers employed data augmentation techniques to enhance the size and diversity of the training dataset. With this data augmentation approach, the CNN model achieved an accuracy of 97% in the detection of conjunctival melanoma using smartphone-captured images [<a href="#B232-vision-09-00071" class="usa-link" aria-describedby="B232-vision-09-00071">232</a>].</p>
<p>Other studies have demonstrated a high diagnostic accuracy for detecting ocular surface tumors using AI as well. A YOLOv5-based DL model applied to slit lamp images captured via smartphones achieved an AUC of 0.997 for ocular surface tumors, and a similarly high accuracy for corneal scars and corneal deposits [<a href="#B233-vision-09-00071" class="usa-link" aria-describedby="B233-vision-09-00071">233</a>]. The CorneAI platform, also using smartphone and slit lamp images, significantly improved ophthalmologists’ diagnostic accuracy, from 79.2% to 88.8% overall (<em>p</em> &lt; 0.001), with specialists improving from 82.8% to 90.0% and residents from 75.6% to 86.2% [<a href="#B234-vision-09-00071" class="usa-link" aria-describedby="B234-vision-09-00071">234</a>]. CorneAI’s own accuracy was 86%, and its use enhanced physicians’ performance beyond its standalone output [<a href="#B234-vision-09-00071" class="usa-link" aria-describedby="B234-vision-09-00071">234</a>]. However, individual task performance varied: CorneAI achieved an AUC of 0.62 for tumor detection and 0.71 for deposits [<a href="#B235-vision-09-00071" class="usa-link" aria-describedby="B235-vision-09-00071">235</a>].</p>
<p>Other DL models such as ResNet50V2, YOLOv8x, and VGG19 trained on 2774 IVCM images, including of ocular surface squamous neoplasias (OSSNs), achieved binary classification accuracies above 97%, with precision ≥ 98%, recall ≥ 85%, and F1 scores ≥ 92% [<a href="#B236-vision-09-00071" class="usa-link" aria-describedby="B236-vision-09-00071">236</a>]. Furthermore, DL has shown potential for OSSN subtype stratification, aiding in potentially personalizing treatment plans [<a href="#B237-vision-09-00071" class="usa-link" aria-describedby="B237-vision-09-00071">237</a>].</p>
<p>The ocular surface pretrained model (OSPM)-enhanced classification model (OECM), trained on 1455 histologically confirmed ocular surface tumor (OST) images, demonstrated AUCs ranging from 0.891 to 0.993 across internal, external, and prospective datasets [<a href="#B238-vision-09-00071" class="usa-link" aria-describedby="B238-vision-09-00071">238</a>]. OECM significantly outperformed conventional CNNs and matched senior ophthalmologists in performance while enhancing the diagnostic capabilities of junior ophthalmologists [<a href="#B238-vision-09-00071" class="usa-link" aria-describedby="B238-vision-09-00071">238</a>].</p>
<p>Laslty, another study using 398 publicly available ocular surface images and CNN models such as MobileNetV2, NASNet, GoogleNet, ResNet50, and InceptionV3 were tested. MobileNetV2 performed best for conjunctival melanoma detection (AUC: 0.976, accuracy: 96.5%) [<a href="#B239-vision-09-00071" class="usa-link" aria-describedby="B239-vision-09-00071">239</a>]. When synthetic images generated via GANs were added, model performance improved further—MobileNetV2 reached an AUC of 0.983 and an accuracy of 97.2% [<a href="#B239-vision-09-00071" class="usa-link" aria-describedby="B239-vision-09-00071">239</a>].</p></section><section id="sec3dot6dot3-vision-09-00071"><h4 class="pmc_sec_title">3.6.3. Pterygium</h4>
<p>The pterygium is a fleshy conjunctival growth that extends onto the cornea and can obscure vision [<a href="#B240-vision-09-00071" class="usa-link" aria-describedby="B240-vision-09-00071">240</a>]. AI applications for pterygia span from segmentation to classification. A comprehensive review of 33 studies highlighted the evolution from manual image segmentation to end-to-end DL-based diagnosis [<a href="#B241-vision-09-00071" class="usa-link" aria-describedby="B241-vision-09-00071">241</a>]. Though DL models match ophthalmologists in accuracy, variability in clinical manifestations and the absence of a standardized grading system remain challenges [<a href="#B241-vision-09-00071" class="usa-link" aria-describedby="B241-vision-09-00071">241</a>].</p>
<p>Several studies underscore DL’s high performance in pterygium detection. One ensemble model trained on 172 anterior segment images achieved an accuracy of 94.12% and an AUC of 0.980 [<a href="#B242-vision-09-00071" class="usa-link" aria-describedby="B242-vision-09-00071">242</a>]. In a binary classification task involving 367 normal and 367 pterygium images, the VGG16 model achieved 99% accuracy, 98% sensitivity, 99.33% specificity, a Kappa of 0.98, and an F1 score of 99% [<a href="#B243-vision-09-00071" class="usa-link" aria-describedby="B243-vision-09-00071">243</a>].</p>
<p>Segmentation models also performed well. One CNN-based model using 489 slit lamp images achieved Dice coefficients of 0.9620 for cornea and 0.9020 for pterygium segmentation. The Kappa agreement with clinical visual inspection was 0.918 [<a href="#B244-vision-09-00071" class="usa-link" aria-describedby="B244-vision-09-00071">244</a>]. Another DL model distinguishing among primary, recurrent, and no pterygium in 258 eyes achieved 91.7% for sensitivity, specificity, accuracy, and an F1 score of 0.846 [<a href="#B245-vision-09-00071" class="usa-link" aria-describedby="B245-vision-09-00071">245</a>].</p>
<p>The RFRC (Faster RCNN + ResNet101) detection model and SRU-Net (SE-ResNeXt50-based U-Net) segmentation model, trained on 20,987 slit lamp and 1094 smartphone images, achieved 95.24% detection accuracy [<a href="#B246-vision-09-00071" class="usa-link" aria-describedby="B246-vision-09-00071">246</a>]. The fusion segmentation model achieved a microaverage F1 score of 0.8981, a sensitivity of 0.8709, a specificity of 0.9668, and an AUC of 0.9295, demonstrating robust performance across smartphone brands and matching experienced clinicians [<a href="#B246-vision-09-00071" class="usa-link" aria-describedby="B246-vision-09-00071">246</a>].</p>
<p>Two DL algorithms were developed to detect any pterygium and referable pterygium using anterior segment photographs. For any pterygium, the AUROCs were 99.5% (internal set, sensitivity = 98.6%, specificity = 99.0%), 99.1% (external set 1), and 99.7% (external set 2) [<a href="#B247-vision-09-00071" class="usa-link" aria-describedby="B247-vision-09-00071">247</a>]. For referable pterygium, AUROCs were 98.5% (internal), 99.7% (external set 1), and 99.0% (external set 2), confirming high sensitivity and specificity across settings [<a href="#B247-vision-09-00071" class="usa-link" aria-describedby="B247-vision-09-00071">247</a>].</p>
<p>In another study using 436 anterior segment images, MobileNetV2 again showed strong results with a sensitivity of 0.8370, a specificity of 0.9048, and an F1 score of 0.8250 [<a href="#B248-vision-09-00071" class="usa-link" aria-describedby="B248-vision-09-00071">248</a>].</p>
<p>Finally, the MOSAIC system—a multimodal assessment tool using gpt-4-turbo, claude-3-opus, and gemini-1.5-pro—demonstrated 86.96% accuracy in detecting ocular surface diseases and 66.67% accuracy in grading pterygium using 375 smartphone-acquired images [<a href="#B249-vision-09-00071" class="usa-link" aria-describedby="B249-vision-09-00071">249</a>]. This highlights the growing role of large language models in multimodal diagnostic pipelines.</p>
<p>Collectively, these studies showcase the diverse and high-performing AI approaches in diagnosing and managing conjunctival and ocular surface disorders, particularly emphasizing their potential as clinical decision support tools and screening instruments.</p></section><section id="sec3dot6dot4-vision-09-00071"><h4 class="pmc_sec_title">3.6.4. Infectious Conjunctivitis</h4>
<p>Similarly to applications of AI in DED and tear film, the use of molecular biomarkers in combination with AI is an area of critical interest. A recent study used ML algorithms to analyze RNA sequencing data from conjunctival samples of patients with presumed infectious conjunctivitis [<a href="#B250-vision-09-00071" class="usa-link" aria-describedby="B250-vision-09-00071">250</a>]. The primary goal was to predict corneal involvement, a marker of disease severity, based on gene expression profiles. SHAP (Shapley Additive Explanations) values identified apolipoprotein E (APOE) as a key gene associated with corneal involvement [<a href="#B250-vision-09-00071" class="usa-link" aria-describedby="B250-vision-09-00071">250</a>]. The model’s performance dropped significantly when APOE was excluded, underscoring its potential clinical value [<a href="#B250-vision-09-00071" class="usa-link" aria-describedby="B250-vision-09-00071">250</a>]. By identifying biomarkers that predict disease progression or severity, the models could eventually guide treatment decisions and prognostication in clinical settings. A summary of all conjunctival model results is in <a href="#vision-09-00071-t007" class="usa-link">Table 7</a>.</p>
<section class="tw xbox font-sm" id="vision-09-00071-t007"><h5 class="obj_head">Table 7.</h5>
<div class="caption p"><p>Summary of conjunctiva models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Type</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Imaging Modality</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Performance Metrics</th>
<th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Notes</th>
</tr></thead>
<tbody>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN [<a href="#B224-vision-09-00071" class="usa-link" aria-describedby="B224-vision-09-00071">224</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">115 segmented conjunctiva images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.58% sensitivity (anemia detection)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Compared to lab test results for anemia detection</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ANN [<a href="#B225-vision-09-00071" class="usa-link" aria-describedby="B225-vision-09-00071">225</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">271 color digital images of conjunctiva</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.89 correlation with ground truth</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Evaluated conjunctival vascularity; best method out of two</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Attention U-Net [<a href="#B226-vision-09-00071" class="usa-link" aria-describedby="B226-vision-09-00071">226</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15 conjunctival images with motion artifacts</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High segmentation performance</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Developed for motion correction and segmentation in blurred images</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Custom Algorithm [<a href="#B227-vision-09-00071" class="usa-link" aria-describedby="B227-vision-09-00071">227</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">101 vessels from 12 digital photographs</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High intra-session repeatability; strong agreement with manual assessment</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Measured conjunctival vessel width with minor estimation errors</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural Network (JOAS criteria) [<a href="#B228-vision-09-00071" class="usa-link" aria-describedby="B228-vision-09-00071">228</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5008 conjunctival images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.8% vessel area detection; r = 0.737, <em>p</em> &lt; 0.01</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Graded conjunctival hyperemia severity vs. expert evaluation</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural Network [<a href="#B229-vision-09-00071" class="usa-link" aria-describedby="B229-vision-09-00071">229</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">611 conjunctival images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.1% accuracy; 78.7% sensitivity; 69.0% specificity</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Detected diabetes via conjunctival microcirculation analysis</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL algorithms [<a href="#B231-vision-09-00071" class="usa-link" aria-describedby="B231-vision-09-00071">231</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AS SS-OCT images (quantity not specified)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Near-perfect sensitivity and specificity</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Applied to angle closure glaucoma detection and VA prediction; future potential in conjunctival tumor assessment</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN with data augmentation [<a href="#B232-vision-09-00071" class="usa-link" aria-describedby="B232-vision-09-00071">232</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">398 smartphone-captured conjunctival images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97% accuracy</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Detected conjunctival melanoma using enhanced training dataset</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv5-based DL model [<a href="#B233-vision-09-00071" class="usa-link" aria-describedby="B233-vision-09-00071">233</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6442 smartphone slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC: 0.997 (ocular surface tumors)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High diagnostic accuracy for multiple ocular conditions</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CorneAI platform [<a href="#B234-vision-09-00071" class="usa-link" aria-describedby="B234-vision-09-00071">234</a>,<a href="#B235-vision-09-00071" class="usa-link" aria-describedby="B235-vision-09-00071">235</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5270 smartphone + slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy: 86%; improved accuracy from 79.2% to 88.8% overall; AUC: 0.62 (tumor), 0.71 (deposits)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improved attending and resident performance</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50V2, YOLOv8x, VGG19 [<a href="#B236-vision-09-00071" class="usa-link" aria-describedby="B236-vision-09-00071">236</a>,<a href="#B237-vision-09-00071" class="usa-link" aria-describedby="B237-vision-09-00071">237</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2774 IVCM images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy &gt; 97%, precision ≥ 98%, recall ≥ 85%, F1 score ≥ 92%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Applied to OSSN; enabled subtype stratification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OSPM-enhanced classification model [<a href="#B238-vision-09-00071" class="usa-link" aria-describedby="B238-vision-09-00071">238</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1455 tumor images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC range: 0.89–0.99 across datasets</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outperformed standard CNNs; matched senior ophthalmologists; improved junior diagnostic performance</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MobileNetV2, NASNet, GoogleNet, ResNet50, InceptionV3 [<a href="#B239-vision-09-00071" class="usa-link" aria-describedby="B239-vision-09-00071">239</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">398 public ocular surface images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Best: MobileNetV2<br>AUC = 0.976, accuracy = 96.5%; with GANs: AUC = 0.983, accuracy = 97.2%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GAN-enhanced training improved results</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ensemble DL model [<a href="#B242-vision-09-00071" class="usa-link" aria-describedby="B242-vision-09-00071">242</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">172 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy = 94.12%, AUC = 0.980</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pterygium detection</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG16 [<a href="#B243-vision-09-00071" class="usa-link" aria-describedby="B243-vision-09-00071">243</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">734 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy = 99%, sensitivity = 98%, specificity = 99.33%, Kappa = 0.98, F1 = 99%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High performance in binary classification</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-based segmentation model [<a href="#B244-vision-09-00071" class="usa-link" aria-describedby="B244-vision-09-00071">244</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">489 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dice = 0.9620 (cornea), 0.9020 (pterygium); Kappa = 0.918 </td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Effective in anatomical segmentation tasks</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DL model [<a href="#B245-vision-09-00071" class="usa-link" aria-describedby="B245-vision-09-00071">245</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">258 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity, specificity, and accuracy = 91.7%; F1 = 0.846</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classified primary, recurrent, and no pterygium</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RFRC and SRU-Net [<a href="#B246-vision-09-00071" class="usa-link" aria-describedby="B246-vision-09-00071">246</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20,987 slit lamp + 1094 smartphone images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy = 95.24%; fusion segmentation: F1 = 0.8981, sensitivity = 0.8709, specificity = 0.9668, AUC = 0.9295</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Performed well across devices; comparable to clinicians</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Two custom DL algorithms [<a href="#B247-vision-09-00071" class="usa-link" aria-describedby="B247-vision-09-00071">247</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2503 slit lamp photographs</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Any pterygium AUC = 99.5% (internal), 99.1% (ext1), 99.7% (ext2); referable AUC = 98.5%, 99.7%, 99.0%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High generalizability across datasets</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MobileNetV2 [<a href="#B248-vision-09-00071" class="usa-link" aria-describedby="B248-vision-09-00071">248</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">436 slit lamp images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity = 0.8370, specificity = 0.9048, F1 = 0.8250</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Consistent high performance in multiple studies</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MOSAIC (GPT-4 Turbo, Claude-3 Opus, Gemini-1.5 Pro) [<a href="#B249-vision-09-00071" class="usa-link" aria-describedby="B249-vision-09-00071">249</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">375 smartphone-acquired images</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy = 86.96% (detection), 66.67% (grading pterygium)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Integrates LLMs in diagnostics</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ML (with SHAP analysis) [<a href="#B250-vision-09-00071" class="usa-link" aria-describedby="B250-vision-09-00071">250</a>]</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58 RNA sequencing samples</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Performance dropped without APOE gene</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Predicted corneal involvement in infectious conjunctivitis; identified APOE as key biomarker</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/vision-09-00071-t007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section><section id="sec3dot7-vision-09-00071"><h3 class="pmc_sec_title">3.7. Large Language Models in Corneal Disease</h3>
<p>LLMs are text-based AI models that have gained massive popularity over the past few years, with several new studies being published using LLMs in the diagnosis of corneal diseases. Some examples of LLMs include ChatGPT-4.0, DeepSeek V3, Qwen 2.5 MAX, Claude3.5 Sonnet, Grok3, Gemini 1.5 Flash, or other variations of these models. A few studies compared the performance of LLMs on text-based clinical case reports and vignettes against each other and expert ophthalmologists [<a href="#B251-vision-09-00071" class="usa-link" aria-describedby="B251-vision-09-00071">251</a>,<a href="#B252-vision-09-00071" class="usa-link" aria-describedby="B252-vision-09-00071">252</a>,<a href="#B253-vision-09-00071" class="usa-link" aria-describedby="B253-vision-09-00071">253</a>]. The results of the studies are summarized in <a href="#vision-09-00071-t008" class="usa-link">Table 8</a>. Generally, the models that performed the best were ChatGPT-4.0 and DeepSeek V3 [<a href="#B252-vision-09-00071" class="usa-link" aria-describedby="B252-vision-09-00071">252</a>,<a href="#B253-vision-09-00071" class="usa-link" aria-describedby="B253-vision-09-00071">253</a>]. Human cornea specialists, however, generally outperformed LLMs [<a href="#B254-vision-09-00071" class="usa-link" aria-describedby="B254-vision-09-00071">254</a>]. Earlier models, especially, performed the worst among all models tested, regardless of LLM type [<a href="#B251-vision-09-00071" class="usa-link" aria-describedby="B251-vision-09-00071">251</a>,<a href="#B252-vision-09-00071" class="usa-link" aria-describedby="B252-vision-09-00071">252</a>,<a href="#B253-vision-09-00071" class="usa-link" aria-describedby="B253-vision-09-00071">253</a>].</p>
<section class="tw xbox font-sm" id="vision-09-00071-t008"><h4 class="obj_head">Table 8.</h4>
<div class="caption p"><p>Summary of LLM diagnostic performance.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th>
<th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Diagnostic Accuracy</th>
<th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Accuracy by Disease Type</th>
<th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AI–Physician Agreement</th>
<th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Physician–Physician Agreement</th>
</tr>
<tr>
<th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pairwise Agreement with Experts [<a href="#B253-vision-09-00071" class="usa-link" aria-describedby="B253-vision-09-00071">253</a>]</th>
<th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interobserver Agreement [<a href="#B251-vision-09-00071" class="usa-link" aria-describedby="B251-vision-09-00071">251</a>,<a href="#B252-vision-09-00071" class="usa-link" aria-describedby="B252-vision-09-00071">252</a>]</th>
<th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kappa (<em>p</em>-Value) [<a href="#B254-vision-09-00071" class="usa-link" aria-describedby="B254-vision-09-00071">254</a>]</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ChatGPT-4.0 (GPT-4.0)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85%, 85%, 80%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Degenerative: 100%, Inflammatory: 40%, Congenital: 83.3%, Infectious: 100%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80%, 75%, 85%, 80%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85%, 80%, 75%, 65% (vs. GPT-3.5)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.348 (<em>p</em> = 0.040)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.3%, 90.5%</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ChatGPT-01</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.3% less than physician average</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.3%</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ChatGPT-3.5 (GPT-3.5)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60%, 60%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Degenerative: 66.7%, Inflammatory: 60%, Congenital: 50%, Infectious: 66.7%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60%, 60%, 60%, 60%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60%, 65% (vs. GPT-4.0)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.146 (<em>p</em> = 0.209)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.3%</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GPT-4.0 Mini</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Degenerative: 83.3%, Inflammatory: 40%, Congenital: 33.3%, Infectious: 66.7%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55%, 50%, 60%, 65%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.121 (<em>p</em> = 0.257)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.5%</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DeepSeek (V3 and R1)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90% (V3), 65% (R1)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Degenerative: 100%, Inflammatory: 40%, Congenital: 50%, Infectious: 66.7%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65%, 75%, 70%, 70%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.3% less than physician average (V3)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.178 (<em>p</em> = 0.162) (R1)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.3%</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Claude 3.5 Sonnet</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Degenerative: 100%, Inflammatory: 60%, Congenital: 50%, Infectious: 66.7%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60%, 60%, 60%, 65%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.219 (<em>p</em> = 0.117)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.5%</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Grok3</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Degenerative: 83.3%, Inflammatory: 40%, Congenital: 66.7%, Infectious: 100%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80%, 75%, 85%, 80%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.219 (<em>p</em> = 0.117)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.5%</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Qwen 2.5 MAX</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38.3% less than physician average</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.3%</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gemini 1.5 Flash</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Degenerative: 33.3%, Inflammatory: 0%, Congenital: 33.3%, Infectious: 66.7%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30%, 30%, 30%, 30%</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">—</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.044 (<em>p</em> = 0.502)</td>
<td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.5%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/vision-09-00071-t008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section><section id="sec4-vision-09-00071"><h2 class="pmc_sec_title">4. Discussion</h2>
<p>This review provides an overview of different AI models used in diagnosing corneal diseases. We noticed a diverse range of models and imaging modalities utilized in the various studies cited. However, many of these studies did share the same performance metrics extracted during our literature search: accuracy, AUC, sensitivity, and specificity.</p>
<p>According to many published reporting guidelines, an AUC greater than 0.90 or sensitivity/specificity exceeding 0.85–0.90 is considered “excellent” or “clinically meaningful” for AI diagnostic standards [<a href="#B255-vision-09-00071" class="usa-link" aria-describedby="B255-vision-09-00071">255</a>,<a href="#B256-vision-09-00071" class="usa-link" aria-describedby="B256-vision-09-00071">256</a>,<a href="#B257-vision-09-00071" class="usa-link" aria-describedby="B257-vision-09-00071">257</a>]. Across all types of corneal diseases examined in this paper, AI has proven itself to meet and at times even exceed these standards. For example, a deep learning model trained on OCT topography exceeded the diagnostic accuracy of keratoconus compared with resident ophthalmologists [<a href="#B121-vision-09-00071" class="usa-link" aria-describedby="B121-vision-09-00071">121</a>]. Their ability to differentiate between subclinical KC, KC, and non-KC corneas is excellent [<a href="#B81-vision-09-00071" class="usa-link" aria-describedby="B81-vision-09-00071">81</a>,<a href="#B113-vision-09-00071" class="usa-link" aria-describedby="B113-vision-09-00071">113</a>,<a href="#B115-vision-09-00071" class="usa-link" aria-describedby="B115-vision-09-00071">115</a>,<a href="#B121-vision-09-00071" class="usa-link" aria-describedby="B121-vision-09-00071">121</a>,<a href="#B122-vision-09-00071" class="usa-link" aria-describedby="B122-vision-09-00071">122</a>,<a href="#B123-vision-09-00071" class="usa-link" aria-describedby="B123-vision-09-00071">123</a>], showing promise in future clinical applications. However, these metrics cannot be taken simply at face-value. It is important to note that these studies are retrospective [<a href="#B81-vision-09-00071" class="usa-link" aria-describedby="B81-vision-09-00071">81</a>,<a href="#B113-vision-09-00071" class="usa-link" aria-describedby="B113-vision-09-00071">113</a>,<a href="#B121-vision-09-00071" class="usa-link" aria-describedby="B121-vision-09-00071">121</a>], which possess inherent biases especially in patient selection for case–controlled studies. Additionally, there was a high degree of variability in the variables and imaging modalities the models were trained on.</p>
<p>Studies evaluating the performance of FECD demonstrated high performance metrics, often meeting or exceeding reporting guidelines [<a href="#B137-vision-09-00071" class="usa-link" aria-describedby="B137-vision-09-00071">137</a>,<a href="#B138-vision-09-00071" class="usa-link" aria-describedby="B138-vision-09-00071">138</a>,<a href="#B139-vision-09-00071" class="usa-link" aria-describedby="B139-vision-09-00071">139</a>,<a href="#B140-vision-09-00071" class="usa-link" aria-describedby="B140-vision-09-00071">140</a>,<a href="#B142-vision-09-00071" class="usa-link" aria-describedby="B142-vision-09-00071">142</a>,<a href="#B145-vision-09-00071" class="usa-link" aria-describedby="B145-vision-09-00071">145</a>,<a href="#B146-vision-09-00071" class="usa-link" aria-describedby="B146-vision-09-00071">146</a>,<a href="#B147-vision-09-00071" class="usa-link" aria-describedby="B147-vision-09-00071">147</a>]. Models trained for FECD evaluation used a variety of different imaging sources for training. Most commonly, AS-OCT, which is also deemed the most preferred form of imaging [<a href="#B137-vision-09-00071" class="usa-link" aria-describedby="B137-vision-09-00071">137</a>,<a href="#B138-vision-09-00071" class="usa-link" aria-describedby="B138-vision-09-00071">138</a>,<a href="#B149-vision-09-00071" class="usa-link" aria-describedby="B149-vision-09-00071">149</a>]. Other modalities such as SLP and specular microscopy were used. Some studies creatively implemented salience maps, edema fraction, and guttae area ratio as model variables as well [<a href="#B139-vision-09-00071" class="usa-link" aria-describedby="B139-vision-09-00071">139</a>,<a href="#B140-vision-09-00071" class="usa-link" aria-describedby="B140-vision-09-00071">140</a>,<a href="#B141-vision-09-00071" class="usa-link" aria-describedby="B141-vision-09-00071">141</a>,<a href="#B142-vision-09-00071" class="usa-link" aria-describedby="B142-vision-09-00071">142</a>,<a href="#B145-vision-09-00071" class="usa-link" aria-describedby="B145-vision-09-00071">145</a>,<a href="#B147-vision-09-00071" class="usa-link" aria-describedby="B147-vision-09-00071">147</a>,<a href="#B148-vision-09-00071" class="usa-link" aria-describedby="B148-vision-09-00071">148</a>]. While some models had objective variables such as the Krachmer scale or edema fraction to compare between eyes [<a href="#B145-vision-09-00071" class="usa-link" aria-describedby="B145-vision-09-00071">145</a>,<a href="#B146-vision-09-00071" class="usa-link" aria-describedby="B146-vision-09-00071">146</a>,<a href="#B147-vision-09-00071" class="usa-link" aria-describedby="B147-vision-09-00071">147</a>], others did not [<a href="#B138-vision-09-00071" class="usa-link" aria-describedby="B138-vision-09-00071">138</a>,<a href="#B139-vision-09-00071" class="usa-link" aria-describedby="B139-vision-09-00071">139</a>]. Feeding the models images for training purposes without objective metrics for analysis contributes to the “black box” approach in their decision-making capabilities.</p>
<p>Several models for dry eye disease and tear film assessment achieved high diagnostic metrics as well [<a href="#B153-vision-09-00071" class="usa-link" aria-describedby="B153-vision-09-00071">153</a>,<a href="#B154-vision-09-00071" class="usa-link" aria-describedby="B154-vision-09-00071">154</a>,<a href="#B155-vision-09-00071" class="usa-link" aria-describedby="B155-vision-09-00071">155</a>,<a href="#B156-vision-09-00071" class="usa-link" aria-describedby="B156-vision-09-00071">156</a>,<a href="#B157-vision-09-00071" class="usa-link" aria-describedby="B157-vision-09-00071">157</a>,<a href="#B158-vision-09-00071" class="usa-link" aria-describedby="B158-vision-09-00071">158</a>,<a href="#B159-vision-09-00071" class="usa-link" aria-describedby="B159-vision-09-00071">159</a>,<a href="#B160-vision-09-00071" class="usa-link" aria-describedby="B160-vision-09-00071">160</a>,<a href="#B161-vision-09-00071" class="usa-link" aria-describedby="B161-vision-09-00071">161</a>,<a href="#B162-vision-09-00071" class="usa-link" aria-describedby="B162-vision-09-00071">162</a>,<a href="#B163-vision-09-00071" class="usa-link" aria-describedby="B163-vision-09-00071">163</a>,<a href="#B164-vision-09-00071" class="usa-link" aria-describedby="B164-vision-09-00071">164</a>,<a href="#B165-vision-09-00071" class="usa-link" aria-describedby="B165-vision-09-00071">165</a>,<a href="#B166-vision-09-00071" class="usa-link" aria-describedby="B166-vision-09-00071">166</a>,<a href="#B169-vision-09-00071" class="usa-link" aria-describedby="B169-vision-09-00071">169</a>,<a href="#B171-vision-09-00071" class="usa-link" aria-describedby="B171-vision-09-00071">171</a>,<a href="#B172-vision-09-00071" class="usa-link" aria-describedby="B172-vision-09-00071">172</a>,<a href="#B173-vision-09-00071" class="usa-link" aria-describedby="B173-vision-09-00071">173</a>]. Many of these models integrated clinical signs, biomarkers, and imaging to improve objectivity and accuracy. Unfortunately, due to the fact that data providing direct comparisons to expert clinicians was limited, it affects their clinical adoption. Additionally, there was great variability in the number of images used to train these models, ranging from double digits to thousands. Some studies tried to mitigate the limited sample size by implementing multiple training cycles [<a href="#B155-vision-09-00071" class="usa-link" aria-describedby="B155-vision-09-00071">155</a>]. Others did not specify the quantity of images used, which should warrant caution in interpreting their results for generalizability and reproducibility [<a href="#B154-vision-09-00071" class="usa-link" aria-describedby="B154-vision-09-00071">154</a>,<a href="#B166-vision-09-00071" class="usa-link" aria-describedby="B166-vision-09-00071">166</a>,<a href="#B170-vision-09-00071" class="usa-link" aria-describedby="B170-vision-09-00071">170</a>].</p>
<p>In the diagnosis of infectious keratitis, several AI models have been used to differentiate between types of keratitis, even as specific as yeast versus filamentous fungi [<a href="#B182-vision-09-00071" class="usa-link" aria-describedby="B182-vision-09-00071">182</a>,<a href="#B183-vision-09-00071" class="usa-link" aria-describedby="B183-vision-09-00071">183</a>,<a href="#B184-vision-09-00071" class="usa-link" aria-describedby="B184-vision-09-00071">184</a>,<a href="#B185-vision-09-00071" class="usa-link" aria-describedby="B185-vision-09-00071">185</a>,<a href="#B186-vision-09-00071" class="usa-link" aria-describedby="B186-vision-09-00071">186</a>]. DeepIK matched Cohen’s Kappa reference standards, indicating agreement with expert-level diagnosis [<a href="#B181-vision-09-00071" class="usa-link" aria-describedby="B181-vision-09-00071">181</a>]. Similarly, ensemble CNNs reached F1 and AUPRC metric values that indicate deep learning models can achieve robust results even in imbalanced classification settings [<a href="#B189-vision-09-00071" class="usa-link" aria-describedby="B189-vision-09-00071">189</a>,<a href="#B190-vision-09-00071" class="usa-link" aria-describedby="B190-vision-09-00071">190</a>]. Just as there were many models without objective criteria to screen for in FECD training images, the same applies for IK models [<a href="#B183-vision-09-00071" class="usa-link" aria-describedby="B183-vision-09-00071">183</a>,<a href="#B185-vision-09-00071" class="usa-link" aria-describedby="B185-vision-09-00071">185</a>,<a href="#B190-vision-09-00071" class="usa-link" aria-describedby="B190-vision-09-00071">190</a>,<a href="#B191-vision-09-00071" class="usa-link" aria-describedby="B191-vision-09-00071">191</a>,<a href="#B192-vision-09-00071" class="usa-link" aria-describedby="B192-vision-09-00071">192</a>,<a href="#B194-vision-09-00071" class="usa-link" aria-describedby="B194-vision-09-00071">194</a>,<a href="#B199-vision-09-00071" class="usa-link" aria-describedby="B199-vision-09-00071">199</a>]. Nevertheless, some models did use specific variables and annotation methods to train their models [<a href="#B188-vision-09-00071" class="usa-link" aria-describedby="B188-vision-09-00071">188</a>,<a href="#B189-vision-09-00071" class="usa-link" aria-describedby="B189-vision-09-00071">189</a>,<a href="#B193-vision-09-00071" class="usa-link" aria-describedby="B193-vision-09-00071">193</a>,<a href="#B200-vision-09-00071" class="usa-link" aria-describedby="B200-vision-09-00071">200</a>,<a href="#B201-vision-09-00071" class="usa-link" aria-describedby="B201-vision-09-00071">201</a>,<a href="#B202-vision-09-00071" class="usa-link" aria-describedby="B202-vision-09-00071">202</a>,<a href="#B203-vision-09-00071" class="usa-link" aria-describedby="B203-vision-09-00071">203</a>,<a href="#B204-vision-09-00071" class="usa-link" aria-describedby="B204-vision-09-00071">204</a>,<a href="#B205-vision-09-00071" class="usa-link" aria-describedby="B205-vision-09-00071">205</a>]. However, AI models for IK still face challenges such as image quality variability and risk of overfitting [<a href="#B180-vision-09-00071" class="usa-link" aria-describedby="B180-vision-09-00071">180</a>]. To improve generalizability, future research should incorporate multivariate models that combine clinical history, physical exam findings, and imaging data [<a href="#B184-vision-09-00071" class="usa-link" aria-describedby="B184-vision-09-00071">184</a>].</p>
<p>For conjunctival disease, a neural network trained on JOAS criteria exhibited strong agreement with expert grading (r = 0.737, <em>p</em> &lt; 0.01), suggesting that AI can match clinician assessments in more subjective domains as well [<a href="#B228-vision-09-00071" class="usa-link" aria-describedby="B228-vision-09-00071">228</a>]. Even more impressive is the ability to diagnose ocular surface tumors, such as conjunctival melanoma, with simple smartphone images to such a high degree of accuracy [<a href="#B232-vision-09-00071" class="usa-link" aria-describedby="B232-vision-09-00071">232</a>,<a href="#B233-vision-09-00071" class="usa-link" aria-describedby="B233-vision-09-00071">233</a>,<a href="#B234-vision-09-00071" class="usa-link" aria-describedby="B234-vision-09-00071">234</a>,<a href="#B235-vision-09-00071" class="usa-link" aria-describedby="B235-vision-09-00071">235</a>]. The same can be seen with models trained to classify pterygia [<a href="#B242-vision-09-00071" class="usa-link" aria-describedby="B242-vision-09-00071">242</a>,<a href="#B243-vision-09-00071" class="usa-link" aria-describedby="B243-vision-09-00071">243</a>,<a href="#B244-vision-09-00071" class="usa-link" aria-describedby="B244-vision-09-00071">244</a>,<a href="#B245-vision-09-00071" class="usa-link" aria-describedby="B245-vision-09-00071">245</a>,<a href="#B246-vision-09-00071" class="usa-link" aria-describedby="B246-vision-09-00071">246</a>,<a href="#B247-vision-09-00071" class="usa-link" aria-describedby="B247-vision-09-00071">247</a>] and nerve morphologies for corneal neuropathy [<a href="#B219-vision-09-00071" class="usa-link" aria-describedby="B219-vision-09-00071">219</a>,<a href="#B220-vision-09-00071" class="usa-link" aria-describedby="B220-vision-09-00071">220</a>]. The main limitation for studies in this category of disease were the high variability in image quality and quantity [<a href="#B224-vision-09-00071" class="usa-link" aria-describedby="B224-vision-09-00071">224</a>,<a href="#B225-vision-09-00071" class="usa-link" aria-describedby="B225-vision-09-00071">225</a>,<a href="#B226-vision-09-00071" class="usa-link" aria-describedby="B226-vision-09-00071">226</a>,<a href="#B227-vision-09-00071" class="usa-link" aria-describedby="B227-vision-09-00071">227</a>,<a href="#B228-vision-09-00071" class="usa-link" aria-describedby="B228-vision-09-00071">228</a>,<a href="#B229-vision-09-00071" class="usa-link" aria-describedby="B229-vision-09-00071">229</a>,<a href="#B232-vision-09-00071" class="usa-link" aria-describedby="B232-vision-09-00071">232</a>,<a href="#B233-vision-09-00071" class="usa-link" aria-describedby="B233-vision-09-00071">233</a>,<a href="#B234-vision-09-00071" class="usa-link" aria-describedby="B234-vision-09-00071">234</a>,<a href="#B235-vision-09-00071" class="usa-link" aria-describedby="B235-vision-09-00071">235</a>,<a href="#B236-vision-09-00071" class="usa-link" aria-describedby="B236-vision-09-00071">236</a>,<a href="#B237-vision-09-00071" class="usa-link" aria-describedby="B237-vision-09-00071">237</a>,<a href="#B238-vision-09-00071" class="usa-link" aria-describedby="B238-vision-09-00071">238</a>,<a href="#B239-vision-09-00071" class="usa-link" aria-describedby="B239-vision-09-00071">239</a>,<a href="#B240-vision-09-00071" class="usa-link" aria-describedby="B240-vision-09-00071">240</a>,<a href="#B241-vision-09-00071" class="usa-link" aria-describedby="B241-vision-09-00071">241</a>,<a href="#B242-vision-09-00071" class="usa-link" aria-describedby="B242-vision-09-00071">242</a>,<a href="#B243-vision-09-00071" class="usa-link" aria-describedby="B243-vision-09-00071">243</a>,<a href="#B244-vision-09-00071" class="usa-link" aria-describedby="B244-vision-09-00071">244</a>,<a href="#B245-vision-09-00071" class="usa-link" aria-describedby="B245-vision-09-00071">245</a>,<a href="#B246-vision-09-00071" class="usa-link" aria-describedby="B246-vision-09-00071">246</a>,<a href="#B247-vision-09-00071" class="usa-link" aria-describedby="B247-vision-09-00071">247</a>,<a href="#B248-vision-09-00071" class="usa-link" aria-describedby="B248-vision-09-00071">248</a>,<a href="#B249-vision-09-00071" class="usa-link" aria-describedby="B249-vision-09-00071">249</a>,<a href="#B250-vision-09-00071" class="usa-link" aria-describedby="B250-vision-09-00071">250</a>]. Some studies with smaller sample sizes utilized a multi-fold cross-validation technique to account for this limitation as well [<a href="#B232-vision-09-00071" class="usa-link" aria-describedby="B232-vision-09-00071">232</a>,<a href="#B242-vision-09-00071" class="usa-link" aria-describedby="B242-vision-09-00071">242</a>].</p>
<p>Large language models have become popular in recent years, with significant improvements in diagnostic capabilities over the past few generations of models. These models have been tested with diverse sets of corneal cases and vignettes. The best performing LLMs were the newer generation ChatGPT and DeepSeek models [<a href="#B251-vision-09-00071" class="usa-link" aria-describedby="B251-vision-09-00071">251</a>,<a href="#B252-vision-09-00071" class="usa-link" aria-describedby="B252-vision-09-00071">252</a>,<a href="#B253-vision-09-00071" class="usa-link" aria-describedby="B253-vision-09-00071">253</a>]. Despite these advancements, they do not quite measure up to the “excellent” standards set by AI diagnostic guidelines. They still do perform well overall, however, with studies citing their potential utility in augmenting clinical decision-making [<a href="#B251-vision-09-00071" class="usa-link" aria-describedby="B251-vision-09-00071">251</a>,<a href="#B252-vision-09-00071" class="usa-link" aria-describedby="B252-vision-09-00071">252</a>,<a href="#B253-vision-09-00071" class="usa-link" aria-describedby="B253-vision-09-00071">253</a>,<a href="#B254-vision-09-00071" class="usa-link" aria-describedby="B254-vision-09-00071">254</a>]. More studies using LLMs are still emerging and warrant further exploration. There are currently a limited number of studies on the use of LLMs in corneal disease, far too limited to make any broad generalizations at this time.</p>
<p>Collectively, these findings across all types of ocular surface diseases show that AI models are able to achieve a high diagnostic accuracy aligning closely with expert-level performance, or at least enough to consider augmenting clinical practice. Despite the fact that many models performed highly in this review, certain models outshined others in comparison. Across all types of corneal disease, neural networks performed well, specifically, CNNs consistently performed the best compared with other types of models [<a href="#B107-vision-09-00071" class="usa-link" aria-describedby="B107-vision-09-00071">107</a>,<a href="#B115-vision-09-00071" class="usa-link" aria-describedby="B115-vision-09-00071">115</a>,<a href="#B122-vision-09-00071" class="usa-link" aria-describedby="B122-vision-09-00071">122</a>,<a href="#B123-vision-09-00071" class="usa-link" aria-describedby="B123-vision-09-00071">123</a>,<a href="#B159-vision-09-00071" class="usa-link" aria-describedby="B159-vision-09-00071">159</a>,<a href="#B161-vision-09-00071" class="usa-link" aria-describedby="B161-vision-09-00071">161</a>,<a href="#B166-vision-09-00071" class="usa-link" aria-describedby="B166-vision-09-00071">166</a>,<a href="#B181-vision-09-00071" class="usa-link" aria-describedby="B181-vision-09-00071">181</a>,<a href="#B184-vision-09-00071" class="usa-link" aria-describedby="B184-vision-09-00071">184</a>,<a href="#B205-vision-09-00071" class="usa-link" aria-describedby="B205-vision-09-00071">205</a>,<a href="#B206-vision-09-00071" class="usa-link" aria-describedby="B206-vision-09-00071">206</a>,<a href="#B209-vision-09-00071" class="usa-link" aria-describedby="B209-vision-09-00071">209</a>,<a href="#B220-vision-09-00071" class="usa-link" aria-describedby="B220-vision-09-00071">220</a>,<a href="#B222-vision-09-00071" class="usa-link" aria-describedby="B222-vision-09-00071">222</a>,<a href="#B232-vision-09-00071" class="usa-link" aria-describedby="B232-vision-09-00071">232</a>,<a href="#B239-vision-09-00071" class="usa-link" aria-describedby="B239-vision-09-00071">239</a>,<a href="#B244-vision-09-00071" class="usa-link" aria-describedby="B244-vision-09-00071">244</a>,<a href="#B246-vision-09-00071" class="usa-link" aria-describedby="B246-vision-09-00071">246</a>]. Deep learning models typically outperformed traditional machine learning, large language, and algorithm-based models. Deep learning models especially outperformed traditional machine learning and algorithm-based models in image analysis and expert-level classification tasks [<a href="#B107-vision-09-00071" class="usa-link" aria-describedby="B107-vision-09-00071">107</a>,<a href="#B115-vision-09-00071" class="usa-link" aria-describedby="B115-vision-09-00071">115</a>,<a href="#B121-vision-09-00071" class="usa-link" aria-describedby="B121-vision-09-00071">121</a>,<a href="#B122-vision-09-00071" class="usa-link" aria-describedby="B122-vision-09-00071">122</a>,<a href="#B123-vision-09-00071" class="usa-link" aria-describedby="B123-vision-09-00071">123</a>,<a href="#B137-vision-09-00071" class="usa-link" aria-describedby="B137-vision-09-00071">137</a>,<a href="#B138-vision-09-00071" class="usa-link" aria-describedby="B138-vision-09-00071">138</a>,<a href="#B139-vision-09-00071" class="usa-link" aria-describedby="B139-vision-09-00071">139</a>,<a href="#B140-vision-09-00071" class="usa-link" aria-describedby="B140-vision-09-00071">140</a>,<a href="#B141-vision-09-00071" class="usa-link" aria-describedby="B141-vision-09-00071">141</a>,<a href="#B146-vision-09-00071" class="usa-link" aria-describedby="B146-vision-09-00071">146</a>,<a href="#B159-vision-09-00071" class="usa-link" aria-describedby="B159-vision-09-00071">159</a>,<a href="#B161-vision-09-00071" class="usa-link" aria-describedby="B161-vision-09-00071">161</a>,<a href="#B166-vision-09-00071" class="usa-link" aria-describedby="B166-vision-09-00071">166</a>,<a href="#B181-vision-09-00071" class="usa-link" aria-describedby="B181-vision-09-00071">181</a>,<a href="#B184-vision-09-00071" class="usa-link" aria-describedby="B184-vision-09-00071">184</a>,<a href="#B205-vision-09-00071" class="usa-link" aria-describedby="B205-vision-09-00071">205</a>,<a href="#B206-vision-09-00071" class="usa-link" aria-describedby="B206-vision-09-00071">206</a>,<a href="#B209-vision-09-00071" class="usa-link" aria-describedby="B209-vision-09-00071">209</a>,<a href="#B220-vision-09-00071" class="usa-link" aria-describedby="B220-vision-09-00071">220</a>,<a href="#B222-vision-09-00071" class="usa-link" aria-describedby="B222-vision-09-00071">222</a>,<a href="#B236-vision-09-00071" class="usa-link" aria-describedby="B236-vision-09-00071">236</a>,<a href="#B237-vision-09-00071" class="usa-link" aria-describedby="B237-vision-09-00071">237</a>,<a href="#B238-vision-09-00071" class="usa-link" aria-describedby="B238-vision-09-00071">238</a>,<a href="#B239-vision-09-00071" class="usa-link" aria-describedby="B239-vision-09-00071">239</a>]. Several ML models performed excellently in dry eye disease and tear film analysis [<a href="#B152-vision-09-00071" class="usa-link" aria-describedby="B152-vision-09-00071">152</a>,<a href="#B154-vision-09-00071" class="usa-link" aria-describedby="B154-vision-09-00071">154</a>,<a href="#B156-vision-09-00071" class="usa-link" aria-describedby="B156-vision-09-00071">156</a>,<a href="#B157-vision-09-00071" class="usa-link" aria-describedby="B157-vision-09-00071">157</a>,<a href="#B167-vision-09-00071" class="usa-link" aria-describedby="B167-vision-09-00071">167</a>,<a href="#B172-vision-09-00071" class="usa-link" aria-describedby="B172-vision-09-00071">172</a>,<a href="#B173-vision-09-00071" class="usa-link" aria-describedby="B173-vision-09-00071">173</a>]. Meanwhile, algorithm-based models excelled in screening tasks, such as the thresholding segmentation of tear meniscus and fixed-cutoff screening tools (SANDE + NIBUT) [<a href="#B168-vision-09-00071" class="usa-link" aria-describedby="B168-vision-09-00071">168</a>,<a href="#B173-vision-09-00071" class="usa-link" aria-describedby="B173-vision-09-00071">173</a>]. Lastly, LLMs performed well in text-based clinical cases, offering potential for augmenting clinical practice, though not comparable enough to experienced ophthalmologists [<a href="#B251-vision-09-00071" class="usa-link" aria-describedby="B251-vision-09-00071">251</a>,<a href="#B252-vision-09-00071" class="usa-link" aria-describedby="B252-vision-09-00071">252</a>,<a href="#B253-vision-09-00071" class="usa-link" aria-describedby="B253-vision-09-00071">253</a>,<a href="#B254-vision-09-00071" class="usa-link" aria-describedby="B254-vision-09-00071">254</a>].</p>
<p>While different AI models were created or trained differently by the investigators, we noticed a lack of standardization in model development and reporting. Even with similar models, there are inconsistencies in the cohort size, imaging modality used, methods for result validation, and ratio for dataset splitting to train the models [<a href="#B113-vision-09-00071" class="usa-link" aria-describedby="B113-vision-09-00071">113</a>,<a href="#B117-vision-09-00071" class="usa-link" aria-describedby="B117-vision-09-00071">117</a>,<a href="#B122-vision-09-00071" class="usa-link" aria-describedby="B122-vision-09-00071">122</a>,<a href="#B136-vision-09-00071" class="usa-link" aria-describedby="B136-vision-09-00071">136</a>,<a href="#B137-vision-09-00071" class="usa-link" aria-describedby="B137-vision-09-00071">137</a>,<a href="#B138-vision-09-00071" class="usa-link" aria-describedby="B138-vision-09-00071">138</a>,<a href="#B141-vision-09-00071" class="usa-link" aria-describedby="B141-vision-09-00071">141</a>,<a href="#B148-vision-09-00071" class="usa-link" aria-describedby="B148-vision-09-00071">148</a>,<a href="#B152-vision-09-00071" class="usa-link" aria-describedby="B152-vision-09-00071">152</a>,<a href="#B183-vision-09-00071" class="usa-link" aria-describedby="B183-vision-09-00071">183</a>,<a href="#B185-vision-09-00071" class="usa-link" aria-describedby="B185-vision-09-00071">185</a>,<a href="#B258-vision-09-00071" class="usa-link" aria-describedby="B258-vision-09-00071">258</a>]. While it may be difficult to standardize AI models due to a multitude of variables, such as what is being studied and institutional-, disease-, or patient-specific limitations, we recommend the standardization of reporting information in AI studies. In fact, standardization tools for AI studies already do exist, such as the STARD-AI [<a href="#B257-vision-09-00071" class="usa-link" aria-describedby="B257-vision-09-00071">257</a>] and QUADAS-AI [<a href="#B259-vision-09-00071" class="usa-link" aria-describedby="B259-vision-09-00071">259</a>] checklists. The STARD-AI checklist requires the clear reporting of study design, methodology, and evaluation to ensure transparency in AI diagnostic research. Meanwhile, QUADAS-AI assesses bias risk and methodology quality. These tools should be applied when evaluating these studies, especially in systemic reviews and meta-analyses, to allow fair comparisons between model types. Some systematic reviews in our review did apply the QUADAS-2 checklist [<a href="#B84-vision-09-00071" class="usa-link" aria-describedby="B84-vision-09-00071">84</a>,<a href="#B115-vision-09-00071" class="usa-link" aria-describedby="B115-vision-09-00071">115</a>,<a href="#B122-vision-09-00071" class="usa-link" aria-describedby="B122-vision-09-00071">122</a>,<a href="#B149-vision-09-00071" class="usa-link" aria-describedby="B149-vision-09-00071">149</a>,<a href="#B181-vision-09-00071" class="usa-link" aria-describedby="B181-vision-09-00071">181</a>,<a href="#B182-vision-09-00071" class="usa-link" aria-describedby="B182-vision-09-00071">182</a>]. Yet quite a few studies did not or utilized assessment tools not specific to AI studies [<a href="#B117-vision-09-00071" class="usa-link" aria-describedby="B117-vision-09-00071">117</a>,<a href="#B132-vision-09-00071" class="usa-link" aria-describedby="B132-vision-09-00071">132</a>,<a href="#B133-vision-09-00071" class="usa-link" aria-describedby="B133-vision-09-00071">133</a>,<a href="#B151-vision-09-00071" class="usa-link" aria-describedby="B151-vision-09-00071">151</a>,<a href="#B169-vision-09-00071" class="usa-link" aria-describedby="B169-vision-09-00071">169</a>,<a href="#B231-vision-09-00071" class="usa-link" aria-describedby="B231-vision-09-00071">231</a>,<a href="#B258-vision-09-00071" class="usa-link" aria-describedby="B258-vision-09-00071">258</a>].</p>
<p>Without the consistent application of these tools, it becomes challenging to evaluate AI models fairly or implement them in the clinical setting. Nevertheless, the successful integration of AI into healthcare systems could alleviate healthcare burden and reduce health disparities, especially in low-resource or -access settings [<a href="#B260-vision-09-00071" class="usa-link" aria-describedby="B260-vision-09-00071">260</a>]. For example, the Ophthalmologist Robot was developed to automate the collection of ocular surface and fundus images [<a href="#B261-vision-09-00071" class="usa-link" aria-describedby="B261-vision-09-00071">261</a>]. This robot supports the early detection of sight-threatening disease through eye screening and AI integration [<a href="#B261-vision-09-00071" class="usa-link" aria-describedby="B261-vision-09-00071">261</a>]. Innovations such as this one, along with the AI models evaluated in this review, could enable earlier interventions and reduce healthcare burden, though further studies are needed to confirm their impact on health equity.</p>
<p>Studies implementing smartphone images and smartphone-based AI tools offer a promising solution to eye care access in underserved areas as well [<a href="#B232-vision-09-00071" class="usa-link" aria-describedby="B232-vision-09-00071">232</a>,<a href="#B233-vision-09-00071" class="usa-link" aria-describedby="B233-vision-09-00071">233</a>,<a href="#B234-vision-09-00071" class="usa-link" aria-describedby="B234-vision-09-00071">234</a>,<a href="#B235-vision-09-00071" class="usa-link" aria-describedby="B235-vision-09-00071">235</a>,<a href="#B246-vision-09-00071" class="usa-link" aria-describedby="B246-vision-09-00071">246</a>,<a href="#B249-vision-09-00071" class="usa-link" aria-describedby="B249-vision-09-00071">249</a>]. These new technologies may advance the field of teleophthalmology. However, despite their auspicious results, the performance in real-world scenarios rather than a controlled environment may yield different results. Hence, future studies should take these tools a step further by focusing on user training, validation using different smartphone devices, and diversifying the datasets.</p>
<p>In the realm of healthcare access, ML models have also been effective in triaging patients with an elevated risk of developing ocular pathologies. For instance, one model achieved an AUC of 0.803 for ocular surface diseases requiring medication [<a href="#B262-vision-09-00071" class="usa-link" aria-describedby="B262-vision-09-00071">262</a>]. These algorithms could empower primary care providers to identify patients who would otherwise go unnoticed [<a href="#B262-vision-09-00071" class="usa-link" aria-describedby="B262-vision-09-00071">262</a>]. Therefore, AI integrations hold potential in allowing timely referrals that mitigate disease progression.</p>
<p>However, the integration of AI into ophthalmology necessitates an emphasis on ethical considerations. Transparency in model development, validation using diverse data, and performance monitoring can minimize the risk of bias and data misuse. Ethical oversight using tools such as the STARD-AI and QUADAS-AI could prevent the propagation of existing healthcare inequities, especially among socioeconomically disadvantaged populations.</p>
<p>Looking forward, the evolution of AI in ophthalmology must be guided by the technical expertise of AI developers, ophthalmologist clinical insights, and a universal ethical foundation. As AI advancements in ophthalmology continue, its success will be judged by its ability to equitably transform care delivery and reduce preventable blindness.</p></section><section id="sec5-vision-09-00071"><h2 class="pmc_sec_title">5. Conclusions</h2>
<p>Several AI models have been developed for diagnosing corneal diseases. Many of these models perform well according to published practice guidelines. Deep learning models specifically seem to outperform other models; however, other models also perform strongly in their defined tasks. AI will have a potentially profound impact on maximizing diagnostic efficiency and providing equitable care in under-resourced regions. However, with the rise in AI technologies, it is prudent to consider standardizing model comparison metrics, diagnostic definitions, and training methodologies. Standardization tools such as the STARD-AI and QUADAS-AI checklists already exist and would allow just comparison between these diverse model sets. Furthermore, these tools, along with consistent experimental methodologies, will ensure the ethical oversight of newly developed models.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author Contributions</h2>
<p>All authors made significant contributions to the creation of the current manuscript. They all meet the ICMJE criteria and approved the final version of this review, affirming its honesty and accuracy. All authors have read and agreed to the published version of the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Institutional Review Board Statement</h2>
<p>This manuscript does not involve any human subjects or animals.</p></section><section id="notes3"><h2 class="pmc_sec_title">Informed Consent Statement</h2>
<p>Not applicable.</p></section><section id="notes4"><h2 class="pmc_sec_title">Data Availability Statement</h2>
<p>No new data were created or analyzed in this study.</p></section><section id="notes5"><h2 class="pmc_sec_title">Conflicts of Interest</h2>
<p>The authors declare no conflicts of interest.</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>This research received no external funding.</p></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn2"><p><strong>Disclaimer/Publisher’s Note:</strong> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></div></div></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="B1-vision-09-00071">
<span class="label">1.</span><cite>Steinmetz J.D., GBD 2019 Blindness and Vision Impairment Collaborators. Vision Loss Expert Group of the Global Burden of Disease Study  Causes of blindness and vision impairment in 2020 and trends over 30 years, and prevalence of avoidable blindness in relation to VISION 2020: The Right to Sight: An analysis for the Global Burden of Disease Study. Lancet Glob. Health. 2021;9:e144–e160. doi: 10.1016/S2214-109X(20)30489-7.</cite> [<a href="https://doi.org/10.1016/S2214-109X(20)30489-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7820391/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33275949/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Lancet%20Glob.%20Health&amp;title=Causes%20of%20blindness%20and%20vision%20impairment%20in%202020%20and%20trends%20over%2030%20years,%20and%20prevalence%20of%20avoidable%20blindness%20in%20relation%20to%20VISION%202020:%20The%20Right%20to%20Sight:%20An%20analysis%20for%20the%20Global%20Burden%20of%20Disease%20Study&amp;author=J.D.%20Steinmetz&amp;volume=9&amp;publication_year=2021&amp;pages=e144-e160&amp;pmid=33275949&amp;doi=10.1016/S2214-109X(20)30489-7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B2-vision-09-00071">
<span class="label">2.</span><cite>Burton M.J., Ramke J., Marques A.P., Bourne R.R.A., Congdon A., Jones I., Tong B., Arunga S., Bachani D., Bascaran C., et al.  The Lancet Global Health Commission on Global Eye Health: Vision beyond 2020. Lancet Glob. Health. 2021;9:e489–e551. doi: 10.1016/S2214-109X(20)30488-5.</cite> [<a href="https://doi.org/10.1016/S2214-109X(20)30488-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7966694/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33607016/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Lancet%20Glob.%20Health&amp;title=The%20Lancet%20Global%20Health%20Commission%20on%20Global%20Eye%20Health:%20Vision%20beyond%202020&amp;author=M.J.%20Burton&amp;author=J.%20Ramke&amp;author=A.P.%20Marques&amp;author=R.R.A.%20Bourne&amp;author=A.%20Congdon&amp;volume=9&amp;publication_year=2021&amp;pages=e489-e551&amp;pmid=33607016&amp;doi=10.1016/S2214-109X(20)30488-5&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B3-vision-09-00071">
<span class="label">3.</span><cite>Whitcher J.P., Srinivasan M., Upadhyay M.P. Corneal blindness: A global perspective. Bull. World Health Organ. 2001;79:214–221.</cite> [<a href="/articles/PMC2566379/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11285665/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Bull.%20World%20Health%20Organ.&amp;title=Corneal%20blindness:%20A%20global%20perspective&amp;author=J.P.%20Whitcher&amp;author=M.%20Srinivasan&amp;author=M.P.%20Upadhyay&amp;volume=79&amp;publication_year=2001&amp;pages=214-221&amp;pmid=11285665&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B4-vision-09-00071">
<span class="label">4.</span><cite>Morthen M.K., Magno M.S., Utheim T.P., Snieder H., Hammond C.J., Vehof J. The physical and mental burden of dry eye disease: A large population-based study investigating the relationship with health-related quality of life and its determinants. Ocul. Surf. 2021;21:107–117. doi: 10.1016/j.jtos.2021.05.006.</cite> [<a href="https://doi.org/10.1016/j.jtos.2021.05.006" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34044135/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ocul.%20Surf.&amp;title=The%20physical%20and%20mental%20burden%20of%20dry%20eye%20disease:%20A%20large%20population-based%20study%20investigating%20the%20relationship%20with%20health-related%20quality%20of%20life%20and%20its%20determinants&amp;author=M.K.%20Morthen&amp;author=M.S.%20Magno&amp;author=T.P.%20Utheim&amp;author=H.%20Snieder&amp;author=C.J.%20Hammond&amp;volume=21&amp;publication_year=2021&amp;pages=107-117&amp;pmid=34044135&amp;doi=10.1016/j.jtos.2021.05.006&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B5-vision-09-00071">
<span class="label">5.</span><cite>Lanza M., Incagli F., Ceccato C., Reffo M.E., Mercuriali E., Parmeggiani F., Pagliano E., Saletti V., Leonardi M., Suppiej A., et al.  Quality of life, functioning and participation of children and adolescents with visual impairment: A scoping review. Res. Dev. Disabil. 2024;151:104772. doi: 10.1016/j.ridd.2024.104772.</cite> [<a href="https://doi.org/10.1016/j.ridd.2024.104772" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38870675/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Res.%20Dev.%20Disabil.&amp;title=Quality%20of%20life,%20functioning%20and%20participation%20of%20children%20and%20adolescents%20with%20visual%20impairment:%20A%20scoping%20review&amp;author=M.%20Lanza&amp;author=F.%20Incagli&amp;author=C.%20Ceccato&amp;author=M.E.%20Reffo&amp;author=E.%20Mercuriali&amp;volume=151&amp;publication_year=2024&amp;pages=104772&amp;pmid=38870675&amp;doi=10.1016/j.ridd.2024.104772&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B6-vision-09-00071">
<span class="label">6.</span><cite>Kandel H., Pesudovs K., Watson S.L. Measurement of Quality of Life in Keratoconus. Cornea. 2020;39:386–393. doi: 10.1097/ICO.0000000000002170.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000002170" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31599780/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Measurement%20of%20Quality%20of%20Life%20in%20Keratoconus&amp;author=H.%20Kandel&amp;author=K.%20Pesudovs&amp;author=S.L.%20Watson&amp;volume=39&amp;publication_year=2020&amp;pages=386-393&amp;pmid=31599780&amp;doi=10.1097/ICO.0000000000002170&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B7-vision-09-00071">
<span class="label">7.</span><cite>Yu J., Asche C.V., Fairchild C.J. The economic burden of dry eye disease in the United States: A decision tree analysis. Cornea. 2011;30:379–387. doi: 10.1097/ICO.0b013e3181f7f363.</cite> [<a href="https://doi.org/10.1097/ICO.0b013e3181f7f363" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21045640/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=The%20economic%20burden%20of%20dry%20eye%20disease%20in%20the%20United%20States:%20A%20decision%20tree%20analysis&amp;author=J.%20Yu&amp;author=C.V.%20Asche&amp;author=C.J.%20Fairchild&amp;volume=30&amp;publication_year=2011&amp;pages=379-387&amp;pmid=21045640&amp;doi=10.1097/ICO.0b013e3181f7f363&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B8-vision-09-00071">
<span class="label">8.</span><cite>Collier S.A., Gronostaj M.P., MacGurn A.K., Cope J.R., Awsumb K.L., Yoder J.S., Beach M.J. Estimated burden of keratitis—United States, 2010. MMWR Morb. Mortal. Wkly. Rep. 2014;63:1027–1030.</cite> [<a href="/articles/PMC5779494/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25393221/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=MMWR%20Morb.%20Mortal.%20Wkly.%20Rep.&amp;title=Estimated%20burden%20of%20keratitis%E2%80%94United%20States,%202010&amp;author=S.A.%20Collier&amp;author=M.P.%20Gronostaj&amp;author=A.K.%20MacGurn&amp;author=J.R.%20Cope&amp;author=K.L.%20Awsumb&amp;volume=63&amp;publication_year=2014&amp;pages=1027-1030&amp;pmid=25393221&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B9-vision-09-00071">
<span class="label">9.</span><cite>Vashist P., Gupta N., Tandon R., Gupta S.K., Dwivedi S., Mani K. Population-based assessment of vision-related quality of life in corneal disease: Results from the CORE study. Br. J. Ophthalmol. 2016;100:588–593. doi: 10.1136/bjophthalmol-2015-307619.</cite> [<a href="https://doi.org/10.1136/bjophthalmol-2015-307619" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26917676/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Br.%20J.%20Ophthalmol.&amp;title=Population-based%20assessment%20of%20vision-related%20quality%20of%20life%20in%20corneal%20disease:%20Results%20from%20the%20CORE%20study&amp;author=P.%20Vashist&amp;author=N.%20Gupta&amp;author=R.%20Tandon&amp;author=S.K.%20Gupta&amp;author=S.%20Dwivedi&amp;volume=100&amp;publication_year=2016&amp;pages=588-593&amp;pmid=26917676&amp;doi=10.1136/bjophthalmol-2015-307619&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B10-vision-09-00071">
<span class="label">10.</span><cite>Baral P., Kumaran S., Stapleton F., Pesudovs K. Quality of life impacts of ocular surface diseases: A qualitative exploration. Investig. Ophthalmol. Vis. Sci. 2024;65:1846.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Quality%20of%20life%20impacts%20of%20ocular%20surface%20diseases:%20A%20qualitative%20exploration&amp;author=P.%20Baral&amp;author=S.%20Kumaran&amp;author=F.%20Stapleton&amp;author=K.%20Pesudovs&amp;volume=65&amp;publication_year=2024&amp;pages=1846&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B11-vision-09-00071">
<span class="label">11.</span><cite>Cabrera-Aguas M., Khoo P., Watson S.L. Infectious keratitis: A review. Clin. Exp. Ophthalmol. 2022;50:543–562. doi: 10.1111/ceo.14113.</cite> [<a href="https://doi.org/10.1111/ceo.14113" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9542356/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35610943/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Clin.%20Exp.%20Ophthalmol.&amp;title=Infectious%20keratitis:%20A%20review&amp;author=M.%20Cabrera-Aguas&amp;author=P.%20Khoo&amp;author=S.L.%20Watson&amp;volume=50&amp;publication_year=2022&amp;pages=543-562&amp;pmid=35610943&amp;doi=10.1111/ceo.14113&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B12-vision-09-00071">
<span class="label">12.</span><cite>Han S.B., Liu Y.-C., Mohamed-Noriega K., Mehta J.S. Advances in Imaging Technology of Anterior Segment of the Eye. J. Ophthalmol. 2021;2021:9539765. doi: 10.1155/2021/9539765.</cite> [<a href="https://doi.org/10.1155/2021/9539765" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7925029/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33688432/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Ophthalmol.&amp;title=Advances%20in%20Imaging%20Technology%20of%20Anterior%20Segment%20of%20the%20Eye&amp;author=S.B.%20Han&amp;author=Y.-C.%20Liu&amp;author=K.%20Mohamed-Noriega&amp;author=J.S.%20Mehta&amp;volume=2021&amp;publication_year=2021&amp;pages=9539765&amp;pmid=33688432&amp;doi=10.1155/2021/9539765&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B13-vision-09-00071">
<span class="label">13.</span><cite>Ang M., Baskaran M., Werkmeister R.M., Chua J., Schmidl D., Aranha dos Santos V., Garhofer G., Mehta J.S., Schmetterer L. Anterior segment optical coherence tomography. Prog. Retin. Eye Res. 2018;66:132–156. doi: 10.1016/j.preteyeres.2018.04.002.</cite> [<a href="https://doi.org/10.1016/j.preteyeres.2018.04.002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29635068/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Prog.%20Retin.%20Eye%20Res.&amp;title=Anterior%20segment%20optical%20coherence%20tomography&amp;author=M.%20Ang&amp;author=M.%20Baskaran&amp;author=R.M.%20Werkmeister&amp;author=J.%20Chua&amp;author=D.%20Schmidl&amp;volume=66&amp;publication_year=2018&amp;pages=132-156&amp;pmid=29635068&amp;doi=10.1016/j.preteyeres.2018.04.002&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B14-vision-09-00071">
<span class="label">14.</span><cite>Kojima T., Dogru M., Kawashima M., Nakamura S., Tsubota K. Advances in the diagnosis and treatment of dry eye. Prog. Retin. Eye Res. 2020;78:100842. doi: 10.1016/j.preteyeres.2020.100842.</cite> [<a href="https://doi.org/10.1016/j.preteyeres.2020.100842" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32004729/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Prog.%20Retin.%20Eye%20Res.&amp;title=Advances%20in%20the%20diagnosis%20and%20treatment%20of%20dry%20eye&amp;author=T.%20Kojima&amp;author=M.%20Dogru&amp;author=M.%20Kawashima&amp;author=S.%20Nakamura&amp;author=K.%20Tsubota&amp;volume=78&amp;publication_year=2020&amp;pages=100842&amp;pmid=32004729&amp;doi=10.1016/j.preteyeres.2020.100842&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B15-vision-09-00071">
<span class="label">15.</span><cite>Cheng K.K.W., Fingerhut L., Duncan S., Prajna N.V., Rossi A.G., Mills B. In vitro and ex vivo models of microbial keratitis: Present and future. Prog. Retin. Eye Res. 2024;102:101287. doi: 10.1016/j.preteyeres.2024.101287.</cite> [<a href="https://doi.org/10.1016/j.preteyeres.2024.101287" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39004166/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Prog.%20Retin.%20Eye%20Res.&amp;title=In%20vitro%20and%20ex%20vivo%20models%20of%20microbial%20keratitis:%20Present%20and%20future&amp;author=K.K.W.%20Cheng&amp;author=L.%20Fingerhut&amp;author=S.%20Duncan&amp;author=N.V.%20Prajna&amp;author=A.G.%20Rossi&amp;volume=102&amp;publication_year=2024&amp;pages=101287&amp;pmid=39004166&amp;doi=10.1016/j.preteyeres.2024.101287&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B16-vision-09-00071">
<span class="label">16.</span><cite>Tuft S., Somerville T.B., Li J.-P.O., Neal T., De S., Horsburgh M.J., Fothergill J.L., Foulkes D., Kaye S. Bacterial keratitis: Identifying the areas of clinical uncertainty. Prog. Retin. Eye Res. 2022;89:101031. doi: 10.1016/j.preteyeres.2021.101031.</cite> [<a href="https://doi.org/10.1016/j.preteyeres.2021.101031" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34915112/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Prog.%20Retin.%20Eye%20Res.&amp;title=Bacterial%20keratitis:%20Identifying%20the%20areas%20of%20clinical%20uncertainty&amp;author=S.%20Tuft&amp;author=T.B.%20Somerville&amp;author=J.-P.O.%20Li&amp;author=T.%20Neal&amp;author=S.%20De&amp;volume=89&amp;publication_year=2022&amp;pages=101031&amp;pmid=34915112&amp;doi=10.1016/j.preteyeres.2021.101031&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B17-vision-09-00071">
<span class="label">17.</span><cite>Ting D.S.W., Peng L., Varadarajan A.V., Keane P.A., Burlina P.M., Chiang M.F., Schmetterer L., Pasquale L.R., Bressler N.M., Webster D.R. Deep learning in ophthalmology: The technical and clinical considerations. Prog. Retin. Eye Res. 2019;72:100759. doi: 10.1016/j.preteyeres.2019.04.003.</cite> [<a href="https://doi.org/10.1016/j.preteyeres.2019.04.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31048019/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Prog.%20Retin.%20Eye%20Res.&amp;title=Deep%20learning%20in%20ophthalmology:%20The%20technical%20and%20clinical%20considerations&amp;author=D.S.W.%20Ting&amp;author=L.%20Peng&amp;author=A.V.%20Varadarajan&amp;author=P.A.%20Keane&amp;author=P.M.%20Burlina&amp;volume=72&amp;publication_year=2019&amp;pages=100759&amp;pmid=31048019&amp;doi=10.1016/j.preteyeres.2019.04.003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B18-vision-09-00071">
<span class="label">18.</span><cite>Haug C.J., Drazen J.M. Artificial Intelligence and Machine Learning in Clinical Medicine, 2023. N. Engl. J. Med. 2023;388:1201–1208. doi: 10.1056/NEJMra2302038.</cite> [<a href="https://doi.org/10.1056/NEJMra2302038" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36988595/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=N.%20Engl.%20J.%20Med.&amp;title=Artificial%20Intelligence%20and%20Machine%20Learning%20in%20Clinical%20Medicine,%202023&amp;author=C.J.%20Haug&amp;author=J.M.%20Drazen&amp;volume=388&amp;publication_year=2023&amp;pages=1201-1208&amp;pmid=36988595&amp;doi=10.1056/NEJMra2302038&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B19-vision-09-00071">
<span class="label">19.</span><cite>Rampat R., Deshmukh R., Chen X., Ting D.S.W., Said D.G., Dua H.S., Ting D.S.J. Artificial Intelligence in Cornea, Refractive Surgery, and Cataract: Basic Principles, Clinical Applications, and Future Directions. Asia Pac. J. Ophthalmol. 2021;10:268–281. doi: 10.1097/APO.0000000000000394.</cite> [<a href="https://doi.org/10.1097/APO.0000000000000394" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7611495/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34224467/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Asia%20Pac.%20J.%20Ophthalmol.&amp;title=Artificial%20Intelligence%20in%20Cornea,%20Refractive%20Surgery,%20and%20Cataract:%20Basic%20Principles,%20Clinical%20Applications,%20and%20Future%20Directions&amp;author=R.%20Rampat&amp;author=R.%20Deshmukh&amp;author=X.%20Chen&amp;author=D.S.W.%20Ting&amp;author=D.G.%20Said&amp;volume=10&amp;publication_year=2021&amp;pages=268-281&amp;pmid=34224467&amp;doi=10.1097/APO.0000000000000394&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B20-vision-09-00071">
<span class="label">20.</span><cite>Nguyen T., Ong J., Masalkhi M., Waisberg E., Zaman N., Sarker P., Aman S., Lin H., Luo M., Ambrosio R., et al.  Artificial intelligence in corneal diseases: A narrative review. Cont. Lens Anterior Eye. 2024;47:102284. doi: 10.1016/j.clae.2024.102284.</cite> [<a href="https://doi.org/10.1016/j.clae.2024.102284" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11581915/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39198101/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cont.%20Lens%20Anterior%20Eye&amp;title=Artificial%20intelligence%20in%20corneal%20diseases:%20A%20narrative%20review&amp;author=T.%20Nguyen&amp;author=J.%20Ong&amp;author=M.%20Masalkhi&amp;author=E.%20Waisberg&amp;author=N.%20Zaman&amp;volume=47&amp;publication_year=2024&amp;pages=102284&amp;pmid=39198101&amp;doi=10.1016/j.clae.2024.102284&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B21-vision-09-00071">
<span class="label">21.</span><cite>Kang L.D., Ballouz D., Woodward M.A. Artificial intelligence and corneal diseases. Curr. Opin. Ophthalmol. 2022;33:407–417. doi: 10.1097/ICU.0000000000000885.</cite> [<a href="https://doi.org/10.1097/ICU.0000000000000885" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9357186/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35819899/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Curr.%20Opin.%20Ophthalmol.&amp;title=Artificial%20intelligence%20and%20corneal%20diseases&amp;author=L.D.%20Kang&amp;author=D.%20Ballouz&amp;author=M.A.%20Woodward&amp;volume=33&amp;publication_year=2022&amp;pages=407-417&amp;pmid=35819899&amp;doi=10.1097/ICU.0000000000000885&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B22-vision-09-00071">
<span class="label">22.</span><cite>Ting D.S.J., Foo V.H.X., Yang L.W.H., Sia J.T., Ang M., Lin H., Chodosh J., Mehta J.S., Ting D.S.W. Artificial intelligence for anterior segment diseases: Emerging applications in ophthalmology. Br. J. Ophthalmol. 2021;105:158–168. doi: 10.1136/bjophthalmol-2019-315651.</cite> [<a href="https://doi.org/10.1136/bjophthalmol-2019-315651" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32532762/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Br.%20J.%20Ophthalmol.&amp;title=Artificial%20intelligence%20for%20anterior%20segment%20diseases:%20Emerging%20applications%20in%20ophthalmology&amp;author=D.S.J.%20Ting&amp;author=V.H.X.%20Foo&amp;author=L.W.H.%20Yang&amp;author=J.T.%20Sia&amp;author=M.%20Ang&amp;volume=105&amp;publication_year=2021&amp;pages=158-168&amp;pmid=32532762&amp;doi=10.1136/bjophthalmol-2019-315651&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B23-vision-09-00071">
<span class="label">23.</span><cite>Linde G., Rodrigues de Souza W., Jr., Chalakkal R., Danesh-Meyer H.V., O’Keeffe B., Hong S.C. A comparative evaluation of deep learning approaches for ophthalmology. Sci. Rep. 2024;14:21829.  doi: 10.1038/s41598-024-72752-x.</cite> [<a href="https://doi.org/10.1038/s41598-024-72752-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11410932/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39294275/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=A%20comparative%20evaluation%20of%20deep%20learning%20approaches%20for%20ophthalmology&amp;author=G.%20Linde&amp;author=W.%20Rodrigues%20de%20Souza&amp;author=R.%20Chalakkal&amp;author=H.V.%20Danesh-Meyer&amp;author=B.%20O%E2%80%99Keeffe&amp;volume=14&amp;publication_year=2024&amp;pages=21829&amp;pmid=39294275&amp;doi=10.1038/s41598-024-72752-x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B24-vision-09-00071">
<span class="label">24.</span><cite>Li Z., Jiang J., Chen K., Chen Q., Zheng Q., Liu X., Weng H., Wu S., Chen W. Preventing corneal blindness caused by keratitis using artificial intelligence. Nat. Commun. 2021;12:3738. doi: 10.1038/s41467-021-24116-6.</cite> [<a href="https://doi.org/10.1038/s41467-021-24116-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8213803/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34145294/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Commun.&amp;title=Preventing%20corneal%20blindness%20caused%20by%20keratitis%20using%20artificial%20intelligence&amp;author=Z.%20Li&amp;author=J.%20Jiang&amp;author=K.%20Chen&amp;author=Q.%20Chen&amp;author=Q.%20Zheng&amp;volume=12&amp;publication_year=2021&amp;pages=3738&amp;pmid=34145294&amp;doi=10.1038/s41467-021-24116-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B25-vision-09-00071">
<span class="label">25.</span><cite>Biousse V., Newman N.J., Najjar R.P., Vasseneix C., Xu X., Ting D.S., Milea L.B., Hwang J.-M. Kim, D.H.; Yang, H.K.; et al. Optic Disc Classification by Deep Learning versus Expert Neuro-Ophthalmologists. Ann. Neurol. 2020;88:785–795. doi: 10.1002/ana.25839.</cite> [<a href="https://doi.org/10.1002/ana.25839" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32621348/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ann.%20Neurol.&amp;title=Kim,%20D.H.;%20Yang,%20H.K.;%20et%20al.%20Optic%20Disc%20Classification%20by%20Deep%20Learning%20versus%20Expert%20Neuro-Ophthalmologists&amp;author=V.%20Biousse&amp;author=N.J.%20Newman&amp;author=R.P.%20Najjar&amp;author=C.%20Vasseneix&amp;author=X.%20Xu&amp;volume=88&amp;publication_year=2020&amp;pages=785-795&amp;pmid=32621348&amp;doi=10.1002/ana.25839&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B26-vision-09-00071">
<span class="label">26.</span><cite>Yoo T.K., Ryu I.H., Lee G., Kim Y., Kim J.K., Lee I.S., Kim J.S., Rim T.H. Adopting machine learning to automatically identify candidate patients for corneal refractive surgery. NPJ Digit. Med. 2019;2:13. doi: 10.1038/s41746-019-0135-8.</cite> [<a href="https://doi.org/10.1038/s41746-019-0135-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6586803/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31304405/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=NPJ%20Digit.%20Med.&amp;title=Adopting%20machine%20learning%20to%20automatically%20identify%20candidate%20patients%20for%20corneal%20refractive%20surgery&amp;author=T.K.%20Yoo&amp;author=I.H.%20Ryu&amp;author=G.%20Lee&amp;author=Y.%20Kim&amp;author=J.K.%20Kim&amp;volume=2&amp;publication_year=2019&amp;pages=13&amp;pmid=31304405&amp;doi=10.1038/s41746-019-0135-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B27-vision-09-00071">
<span class="label">27.</span><cite>Accardo P.A., Pensiero S. Neural network-based system for early keratoconus detection from corneal topography. J. Biomed. Inform. 2002;35:151–159. doi: 10.1016/S1532-0464(02)00513-0.</cite> [<a href="https://doi.org/10.1016/S1532-0464(02)00513-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/12669978/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Biomed.%20Inform.&amp;title=Neural%20network-based%20system%20for%20early%20keratoconus%20detection%20from%20corneal%20topography&amp;author=P.A.%20Accardo&amp;author=S.%20Pensiero&amp;volume=35&amp;publication_year=2002&amp;pages=151-159&amp;pmid=12669978&amp;doi=10.1016/S1532-0464(02)00513-0&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B28-vision-09-00071">
<span class="label">28.</span><cite>Kamiya K., Ayatsuka Y., Kato Y., Fujimura F., Takahashi M., Shoji N., Mori Y., Miyata K. Keratoconus detection using deep learning of colour-coded maps with anterior segment optical coherence tomography: A diagnostic accuracy study. BMJ Open. 2019;9:e031313. doi: 10.1136/bmjopen-2019-031313.</cite> [<a href="https://doi.org/10.1136/bmjopen-2019-031313" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6773416/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31562158/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=BMJ%20Open&amp;title=Keratoconus%20detection%20using%20deep%20learning%20of%20colour-coded%20maps%20with%20anterior%20segment%20optical%20coherence%20tomography:%20A%20diagnostic%20accuracy%20study&amp;author=K.%20Kamiya&amp;author=Y.%20Ayatsuka&amp;author=Y.%20Kato&amp;author=F.%20Fujimura&amp;author=M.%20Takahashi&amp;volume=9&amp;publication_year=2019&amp;pages=e031313&amp;pmid=31562158&amp;doi=10.1136/bmjopen-2019-031313&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B29-vision-09-00071">
<span class="label">29.</span><cite>Souza M.B., de Medeiros F.W., Souza D.B., Alves M.R. Detection of keratoconus based on a neural network with orbscan. Arq. Bras. Oftalmol. 2008;71:65–68. doi: 10.1590/S0004-27492008000700013.</cite> [<a href="https://doi.org/10.1590/S0004-27492008000700013" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19274414/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Arq.%20Bras.%20Oftalmol.&amp;title=Detection%20of%20keratoconus%20based%20on%20a%20neural%20network%20with%20orbscan&amp;author=M.B.%20Souza&amp;author=F.W.%20de%20Medeiros&amp;author=D.B.%20Souza&amp;author=M.R.%20Alves&amp;volume=71&amp;publication_year=2008&amp;pages=65-68&amp;pmid=19274414&amp;doi=10.1590/S0004-27492008000700013&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B30-vision-09-00071">
<span class="label">30.</span><cite>Maeda N., Klyce S.D., Smolek M.K., Thompson H.W. Automated keratoconus screening with corneal topography analysis. Investig. Ophthalmol. Vis. Sci. 1994;35:2749–2757.</cite> [<a href="https://pubmed.ncbi.nlm.nih.gov/8188468/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Automated%20keratoconus%20screening%20with%20corneal%20topography%20analysis&amp;author=N.%20Maeda&amp;author=S.D.%20Klyce&amp;author=M.K.%20Smolek&amp;author=H.W.%20Thompson&amp;volume=35&amp;publication_year=1994&amp;pages=2749-2757&amp;pmid=8188468&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B31-vision-09-00071">
<span class="label">31.</span><cite>Malyugin B., Sakhnov S., Izmailova S., Boiko E., Pozdeyeva N., Axenova L., Axenov K., Titov A., Terentyeva A., Zakaraiia T., et al.  Keratoconus Diagnostic and Treatment Algorithms Based on Machine-Learning Methods. Diagnostics. 2021;11:1933.  doi: 10.3390/diagnostics11101933.</cite> [<a href="https://doi.org/10.3390/diagnostics11101933" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8535111/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34679631/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diagnostics&amp;title=Keratoconus%20Diagnostic%20and%20Treatment%20Algorithms%20Based%20on%20Machine-Learning%20Methods&amp;author=B.%20Malyugin&amp;author=S.%20Sakhnov&amp;author=S.%20Izmailova&amp;author=E.%20Boiko&amp;author=N.%20Pozdeyeva&amp;volume=11&amp;publication_year=2021&amp;pages=1933&amp;pmid=34679631&amp;doi=10.3390/diagnostics11101933&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B32-vision-09-00071">
<span class="label">32.</span><cite>Arbelaez M.C., Versaci F., Vestri G., Barboni P., Savini G. Use of a support vector machine for keratoconus and subclinical keratoconus detection by topographic and tomographic data. Ophthalmology. 2012;119:2231–2238. doi: 10.1016/j.ophtha.2012.06.005.</cite> [<a href="https://doi.org/10.1016/j.ophtha.2012.06.005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22892148/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ophthalmology&amp;title=Use%20of%20a%20support%20vector%20machine%20for%20keratoconus%20and%20subclinical%20keratoconus%20detection%20by%20topographic%20and%20tomographic%20data&amp;author=M.C.%20Arbelaez&amp;author=F.%20Versaci&amp;author=G.%20Vestri&amp;author=P.%20Barboni&amp;author=G.%20Savini&amp;volume=119&amp;publication_year=2012&amp;pages=2231-2238&amp;pmid=22892148&amp;doi=10.1016/j.ophtha.2012.06.005&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B33-vision-09-00071">
<span class="label">33.</span><cite>Toutounchian F., Shanbehzadeh J., Khanlari M. Detection of Keratoconus and Suspect Keratoconus by Machine Vision; Proceedings of the International Multiconference of Engineers and Computer Scientists, IMECS 2012; Hong Kong, China. 14–16 March 2012; pp. 89–91.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%20International%20Multiconference%20of%20Engineers%20and%20Computer%20Scientists,%20IMECS%202012&amp;title=Detection%20of%20Keratoconus%20and%20Suspect%20Keratoconus%20by%20Machine%20Vision&amp;author=F.%20Toutounchian&amp;author=J.%20Shanbehzadeh&amp;author=M.%20Khanlari&amp;volume=Volume%20I&amp;pages=89-91&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B34-vision-09-00071">
<span class="label">34.</span><cite>Song P., Ren S.W., Liu Y., Li P., Zeng Q.Y. Detection of subclinical keratoconus using a novel combined tomographic and biomechanical model based on an automated decision tree. Sci. Rep. 2022;12:5316.  doi: 10.1038/s41598-022-09160-6.</cite> [<a href="https://doi.org/10.1038/s41598-022-09160-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8964676/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35351951/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Detection%20of%20subclinical%20keratoconus%20using%20a%20novel%20combined%20tomographic%20and%20biomechanical%20model%20based%20on%20an%20automated%20decision%20tree&amp;author=P.%20Song&amp;author=S.W.%20Ren&amp;author=Y.%20Liu&amp;author=P.%20Li&amp;author=Q.Y.%20Zeng&amp;volume=12&amp;publication_year=2022&amp;pages=5316&amp;pmid=35351951&amp;doi=10.1038/s41598-022-09160-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B35-vision-09-00071">
<span class="label">35.</span><cite>Yousefi S., Yousefi E., Takahashi H., Hayashi T., Tampo H., Inoda S., Arai Y., Asbell P. Keratoconus severity identification using unsupervised machine learning. PLoS ONE. 2018;13:e0205998.  doi: 10.1371/journal.pone.0205998.</cite> [<a href="https://doi.org/10.1371/journal.pone.0205998" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6219768/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30399144/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20ONE&amp;title=Keratoconus%20severity%20identification%20using%20unsupervised%20machine%20learning&amp;author=S.%20Yousefi&amp;author=E.%20Yousefi&amp;author=H.%20Takahashi&amp;author=T.%20Hayashi&amp;author=H.%20Tampo&amp;volume=13&amp;publication_year=2018&amp;pages=e0205998&amp;pmid=30399144&amp;doi=10.1371/journal.pone.0205998&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B36-vision-09-00071">
<span class="label">36.</span><cite>Klyce S.D., Smolek M.K., Maeda N. Keratoconus detection with the KISA% method—Another view. J. Cataract Refract. Surg. 2000;26:472–474. doi: 10.1016/S0886-3350(00)00384-9.</cite> [<a href="https://doi.org/10.1016/S0886-3350(00)00384-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/10819626/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Cataract%20Refract.%20Surg.&amp;title=Keratoconus%20detection%20with%20the%20KISA%%20method%E2%80%94Another%20view&amp;author=S.D.%20Klyce&amp;author=M.K.%20Smolek&amp;author=N.%20Maeda&amp;volume=26&amp;publication_year=2000&amp;pages=472-474&amp;pmid=10819626&amp;doi=10.1016/S0886-3350(00)00384-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B37-vision-09-00071">
<span class="label">37.</span><cite>Jimenez-Garcia M., Issarti I., Kreps E.O., Dhubhghaill S.N., Koppen C., Varssano D., Rozema J.J. Keratoconus progression forecast by means of a time delay neural network. Investig. Ophthalmol. Vis. Sci. 2021;62:13.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Keratoconus%20progression%20forecast%20by%20means%20of%20a%20time%20delay%20neural%20network&amp;author=M.%20Jimenez-Garcia&amp;author=I.%20Issarti&amp;author=E.O.%20Kreps&amp;author=S.N.%20Dhubhghaill&amp;author=C.%20Koppen&amp;volume=62&amp;publication_year=2021&amp;pages=13&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B38-vision-09-00071">
<span class="label">38.</span><cite>Falahati Marvast F., Arabalibeik H., Alipour F., Sheikhtaheri A., Nouri L., Soozande M., Yarmahmoodi M.  Medicine Meets Virtual Reality 22. SAGE Publications Ltd; Thousand Oaks, CA, USA: 2016. Evaluation of RGP Contact Lens Fitting in Keratoconus Patients Using Hierarchical Fuzzy Model and Genetic Algorithms; pp. 124–129.</cite> [<a href="https://pubmed.ncbi.nlm.nih.gov/27046564/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Medicine%20Meets%20Virtual%20Reality%2022&amp;author=F.%20Falahati%20Marvast&amp;author=H.%20Arabalibeik&amp;author=F.%20Alipour&amp;author=A.%20Sheikhtaheri&amp;author=L.%20Nouri&amp;publication_year=2016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B39-vision-09-00071">
<span class="label">39.</span><cite>Hu D., Lin Z., Jiang J., Li P., Zhang Z., Yang C. Identification of Key Genes and Molecular Pathways in Keratoconus: Integrating Text Mining and Bioinformatics Analysis. BioMed Res. Int. 2022;2022:4740141.  doi: 10.1155/2022/4740141.</cite> [<a href="https://doi.org/10.1155/2022/4740141" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9427295/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36051483/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=BioMed%20Res.%20Int.&amp;title=Identification%20of%20Key%20Genes%20and%20Molecular%20Pathways%20in%20Keratoconus:%20Integrating%20Text%20Mining%20and%20Bioinformatics%20Analysis&amp;author=D.%20Hu&amp;author=Z.%20Lin&amp;author=J.%20Jiang&amp;author=P.%20Li&amp;author=Z.%20Zhang&amp;volume=2022&amp;publication_year=2022&amp;pages=4740141&amp;pmid=36051483&amp;doi=10.1155/2022/4740141&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B40-vision-09-00071">
<span class="label">40.</span><cite>Marsack J.D., Benoit J.S., Kollbaum P.S., Anderson H.A. Application of Topographical Keratoconus Detection Metrics to Eyes of Individuals with Down Syndrome. Optom. Vis. Sci. 2019;96:664–669. doi: 10.1097/OPX.0000000000001417.</cite> [<a href="https://doi.org/10.1097/OPX.0000000000001417" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6750806/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31479021/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Optom.%20Vis.%20Sci.&amp;title=Application%20of%20Topographical%20Keratoconus%20Detection%20Metrics%20to%20Eyes%20of%20Individuals%20with%20Down%20Syndrome&amp;author=J.D.%20Marsack&amp;author=J.S.%20Benoit&amp;author=P.S.%20Kollbaum&amp;author=H.A.%20Anderson&amp;volume=96&amp;publication_year=2019&amp;pages=664-669&amp;pmid=31479021&amp;doi=10.1097/OPX.0000000000001417&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B41-vision-09-00071">
<span class="label">41.</span><cite>Wang L., Wang Y., Liu J., Zhao W. Identification of Important Genes of Keratoconus and Construction of the Diagnostic Model. Genet. Res. 2022;2022:5878460. doi: 10.1155/2022/5878460.</cite> [<a href="https://doi.org/10.1155/2022/5878460" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9484959/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36160033/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Genet.%20Res.&amp;title=Identification%20of%20Important%20Genes%20of%20Keratoconus%20and%20Construction%20of%20the%20Diagnostic%20Model&amp;author=L.%20Wang&amp;author=Y.%20Wang&amp;author=J.%20Liu&amp;author=W.%20Zhao&amp;volume=2022&amp;publication_year=2022&amp;pages=5878460&amp;pmid=36160033&amp;doi=10.1155/2022/5878460&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B42-vision-09-00071">
<span class="label">42.</span><cite>Kamiya K., Ayatsuka Y., Kato Y., Shoji N., Miyai T., Ishii H., Mori Y., Miyata K. Prediction of keratoconus progression using deep learning of anterior segment optical coherence tomography maps. Ann. Transl. Med. 2021;9:1287. doi: 10.21037/atm-21-1772.</cite> [<a href="https://doi.org/10.21037/atm-21-1772" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8422102/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34532424/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ann.%20Transl.%20Med.&amp;title=Prediction%20of%20keratoconus%20progression%20using%20deep%20learning%20of%20anterior%20segment%20optical%20coherence%20tomography%20maps&amp;author=K.%20Kamiya&amp;author=Y.%20Ayatsuka&amp;author=Y.%20Kato&amp;author=N.%20Shoji&amp;author=T.%20Miyai&amp;volume=9&amp;publication_year=2021&amp;pages=1287&amp;pmid=34532424&amp;doi=10.21037/atm-21-1772&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B43-vision-09-00071">
<span class="label">43.</span><cite>Castro-Luna G., Jimenez-Rodriguez D., Castano-Fernandez A.B., Perez-Rueda A. Diagnosis of Subclinical Keratoconus Based on Machine Learning Techniques. J. Clin. Med. 2021;10:4281.  doi: 10.3390/jcm10184281.</cite> [<a href="https://doi.org/10.3390/jcm10184281" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8468312/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34575391/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Clin.%20Med.&amp;title=Diagnosis%20of%20Subclinical%20Keratoconus%20Based%20on%20Machine%20Learning%20Techniques&amp;author=G.%20Castro-Luna&amp;author=D.%20Jimenez-Rodriguez&amp;author=A.B.%20Castano-Fernandez&amp;author=A.%20Perez-Rueda&amp;volume=10&amp;publication_year=2021&amp;pages=4281&amp;pmid=34575391&amp;doi=10.3390/jcm10184281&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B44-vision-09-00071">
<span class="label">44.</span><cite>Wisse R.P.L., Godefrooij D.A., Nuijts R. Using Machine Learning to Monitor Keratoconus Progression—Reply. JAMA Ophthalmol. 2019;137:1468. doi: 10.1001/jamaophthalmol.2019.4201.</cite> [<a href="https://doi.org/10.1001/jamaophthalmol.2019.4201" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31670783/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=JAMA%20Ophthalmol.&amp;title=Using%20Machine%20Learning%20to%20Monitor%20Keratoconus%20Progression%E2%80%94Reply&amp;author=R.P.L.%20Wisse&amp;author=D.A.%20Godefrooij&amp;author=R.%20Nuijts&amp;volume=137&amp;publication_year=2019&amp;pages=1468&amp;pmid=31670783&amp;doi=10.1001/jamaophthalmol.2019.4201&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B45-vision-09-00071">
<span class="label">45.</span><cite>Yucekul B., Dick H.B., Taneri S. Systematic Detection of Keratoconus in Optical Coherence Tomography: Corneal and Epithelial Thickness Maps. J. Cataract Refract. Surg. 2022;48:1360–1365. doi: 10.1097/j.jcrs.0000000000000990.</cite> [<a href="https://doi.org/10.1097/j.jcrs.0000000000000990" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35714335/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Cataract%20Refract.%20Surg.&amp;title=Systematic%20Detection%20of%20Keratoconus%20in%20Optical%20Coherence%20Tomography:%20Corneal%20and%20Epithelial%20Thickness%20Maps&amp;author=B.%20Yucekul&amp;author=H.B.%20Dick&amp;author=S.%20Taneri&amp;volume=48&amp;publication_year=2022&amp;pages=1360-1365&amp;pmid=35714335&amp;doi=10.1097/j.jcrs.0000000000000990&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B46-vision-09-00071">
<span class="label">46.</span><cite>Reinstein D.Z., Archer T.J., Urs R., Gobbe M., RoyChoudhury A., Silverman R.H. Detection of Keratoconus in Clinically and Algorithmically Topographically Normal Fellow Eyes Using Epithelial Thickness Analysis. J. Refract. Surg. 2015;31:736–744. doi: 10.3928/1081597X-20151021-02.</cite> [<a href="https://doi.org/10.3928/1081597X-20151021-02" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5357464/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26544561/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Refract.%20Surg.&amp;title=Detection%20of%20Keratoconus%20in%20Clinically%20and%20Algorithmically%20Topographically%20Normal%20Fellow%20Eyes%20Using%20Epithelial%20Thickness%20Analysis&amp;author=D.Z.%20Reinstein&amp;author=T.J.%20Archer&amp;author=R.%20Urs&amp;author=M.%20Gobbe&amp;author=A.%20RoyChoudhury&amp;volume=31&amp;publication_year=2015&amp;pages=736-744&amp;pmid=26544561&amp;doi=10.3928/1081597X-20151021-02&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B47-vision-09-00071">
<span class="label">47.</span><cite>Ahuja A.S., Halperin L.S. Using Machine Learning to Monitor Keratoconus Progression. JAMA Ophthalmol. 2019;137:1467–1468. doi: 10.1001/jamaophthalmol.2019.4198.</cite> [<a href="https://doi.org/10.1001/jamaophthalmol.2019.4198" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31670786/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=JAMA%20Ophthalmol.&amp;title=Using%20Machine%20Learning%20to%20Monitor%20Keratoconus%20Progression&amp;author=A.S.%20Ahuja&amp;author=L.S.%20Halperin&amp;volume=137&amp;publication_year=2019&amp;pages=1467-1468&amp;pmid=31670786&amp;doi=10.1001/jamaophthalmol.2019.4198&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B48-vision-09-00071">
<span class="label">48.</span><cite>Kato N., Masumoto H., Tanabe M., Sakai C., Negishi K., Torii H., Tabuchi H., Tsubota K. Predicting Keratoconus Progression and Need for Corneal Crosslinking Using Deep Learning. J. Clin. Med. 2021;10:844.  doi: 10.3390/jcm10040844.</cite> [<a href="https://doi.org/10.3390/jcm10040844" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7923054/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33670732/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Clin.%20Med.&amp;title=Predicting%20Keratoconus%20Progression%20and%20Need%20for%20Corneal%20Crosslinking%20Using%20Deep%20Learning&amp;author=N.%20Kato&amp;author=H.%20Masumoto&amp;author=M.%20Tanabe&amp;author=C.%20Sakai&amp;author=K.%20Negishi&amp;volume=10&amp;publication_year=2021&amp;pages=844&amp;pmid=33670732&amp;doi=10.3390/jcm10040844&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B49-vision-09-00071">
<span class="label">49.</span><cite>Kalin N.S., Maeda N., Klyce S.D., Hargrave S., Wilson S.E. Automated Topographic Screening for Keratoconus in Refractive Surgery Candidates. CLAO J. 1996;22:164–167.</cite> [<a href="https://pubmed.ncbi.nlm.nih.gov/8828931/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=CLAO%20J.&amp;title=Automated%20Topographic%20Screening%20for%20Keratoconus%20in%20Refractive%20Surgery%20Candidates&amp;author=N.S.%20Kalin&amp;author=N.%20Maeda&amp;author=S.D.%20Klyce&amp;author=S.%20Hargrave&amp;author=S.E.%20Wilson&amp;volume=22&amp;publication_year=1996&amp;pages=164-167&amp;pmid=8828931&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B50-vision-09-00071">
<span class="label">50.</span><cite>Kheiri S., Meshkani M.R., Faghihzadeh S. A Correlated Frailty Model for Analysing Risk Factors in Bilateral Corneal Graft Rejection for Keratoconus: A Bayesian Approach. Stat. Med. 2005;24:2681–2693. doi: 10.1002/sim.2113.</cite> [<a href="https://doi.org/10.1002/sim.2113" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16118807/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Stat.%20Med.&amp;title=A%20Correlated%20Frailty%20Model%20for%20Analysing%20Risk%20Factors%20in%20Bilateral%20Corneal%20Graft%20Rejection%20for%20Keratoconus:%20A%20Bayesian%20Approach&amp;author=S.%20Kheiri&amp;author=M.R.%20Meshkani&amp;author=S.%20Faghihzadeh&amp;volume=24&amp;publication_year=2005&amp;pages=2681-2693&amp;pmid=16118807&amp;doi=10.1002/sim.2113&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B51-vision-09-00071">
<span class="label">51.</span><cite>Smolek M.K., Klyce S.D. Neural Network Pair for the Detection and Grading of Keratoconus and Keratoconus Suspects by Videokeratoscopy. Investig. Ophthalmol. Vis. Sci. 1996;37:4200.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Neural%20Network%20Pair%20for%20the%20Detection%20and%20Grading%20of%20Keratoconus%20and%20Keratoconus%20Suspects%20by%20Videokeratoscopy&amp;author=M.K.%20Smolek&amp;author=S.D.%20Klyce&amp;volume=37&amp;publication_year=1996&amp;pages=4200&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B52-vision-09-00071">
<span class="label">52.</span><cite>Dong Y., Li D., Guo Z., Liu Y., Lin P., Lv B., Lv C., Xie G., Xie L. Dissecting the Profile of Corneal Thickness with Keratoconus Progression Based on Anterior Segment Optical Coherence Tomography. Front. Neurosci. 2021;15:804273.  doi: 10.3389/fnins.2021.804273.</cite> [<a href="https://doi.org/10.3389/fnins.2021.804273" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8842478/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35173574/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Neurosci.&amp;title=Dissecting%20the%20Profile%20of%20Corneal%20Thickness%20with%20Keratoconus%20Progression%20Based%20on%20Anterior%20Segment%20Optical%20Coherence%20Tomography&amp;author=Y.%20Dong&amp;author=D.%20Li&amp;author=Z.%20Guo&amp;author=Y.%20Liu&amp;author=P.%20Lin&amp;volume=15&amp;publication_year=2021&amp;pages=804273&amp;pmid=35173574&amp;doi=10.3389/fnins.2021.804273&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B53-vision-09-00071">
<span class="label">53.</span><cite>Fariselli C., Vega-Estrada A., Arnalich-Montiel F., Alio J.L. Artificial Neural Network to Guide Intracorneal Ring Segments Implantation for Keratoconus Treatment: A Pilot Study. Eye Vis. 2020;7:20. doi: 10.1186/s40662-020-00184-5.</cite> [<a href="https://doi.org/10.1186/s40662-020-00184-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7144046/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32292796/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eye%20Vis.&amp;title=Artificial%20Neural%20Network%20to%20Guide%20Intracorneal%20Ring%20Segments%20Implantation%20for%20Keratoconus%20Treatment:%20A%20Pilot%20Study&amp;author=C.%20Fariselli&amp;author=A.%20Vega-Estrada&amp;author=F.%20Arnalich-Montiel&amp;author=J.L.%20Alio&amp;volume=7&amp;publication_year=2020&amp;pages=20&amp;pmid=32292796&amp;doi=10.1186/s40662-020-00184-5&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B54-vision-09-00071">
<span class="label">54.</span><cite>Hallett N., Yi K., Dick J., Hodge C., Sutton G., Wang Y.G., You J. Deep Learning Based Unsupervised and Semi-supervised Classification for Keratoconus; Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN); Glasgow, UK. 19–24 July 2020.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%202020%20International%20Joint%20Conference%20on%20Neural%20Networks%20(IJCNN)&amp;title=Deep%20Learning%20Based%20Unsupervised%20and%20Semi-supervised%20Classification%20for%20Keratoconus&amp;author=N.%20Hallett&amp;author=K.%20Yi&amp;author=J.%20Dick&amp;author=C.%20Hodge&amp;author=G.%20Sutton&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B55-vision-09-00071">
<span class="label">55.</span><cite>Hasan S.A., Singh M. An Algorithm to Differentiate Astigmatism from Keratoconus in Axial Topgraphic Images; Proceedings of the 2015 International Conference on Industrial Instrumentation and Control (ICIC); Pune, India. 28–30 May 2015; pp. 1134–1139.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%202015%20International%20Conference%20on%20Industrial%20Instrumentation%20and%20Control%20(ICIC)&amp;title=An%20Algorithm%20to%20Differentiate%20Astigmatism%20from%20Keratoconus%20in%20Axial%20Topgraphic%20Images&amp;author=S.A.%20Hasan&amp;author=M.%20Singh&amp;pages=1134-1139&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B56-vision-09-00071">
<span class="label">56.</span><cite>Silverman R.H., Urs R., RoyChaudhury A., Archer T.J., Gobbe M., Reinstein D.Z. Epithelial Remodeling as Basis for Machine-Based Identification of Keratoconus. Investig. Ophthalmol. Vis. Sci. 2014;55:1580–1587. doi: 10.1167/iovs.13-12578.</cite> [<a href="https://doi.org/10.1167/iovs.13-12578" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3954156/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24557351/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Epithelial%20Remodeling%20as%20Basis%20for%20Machine-Based%20Identification%20of%20Keratoconus&amp;author=R.H.%20Silverman&amp;author=R.%20Urs&amp;author=A.%20RoyChaudhury&amp;author=T.J.%20Archer&amp;author=M.%20Gobbe&amp;volume=55&amp;publication_year=2014&amp;pages=1580-1587&amp;pmid=24557351&amp;doi=10.1167/iovs.13-12578&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B57-vision-09-00071">
<span class="label">57.</span><cite>Souza M.B., Medeiros F.W., Souza D.B., Garcia R., Alves M.R. Evaluation of Machine Learning Classifiers in Keratoconus Detection from Orbscan II Examinations. Clinics. 2010;65:1223–1228. doi: 10.1590/S1807-59322010001200002.</cite> [<a href="https://doi.org/10.1590/S1807-59322010001200002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3020330/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21340208/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Clinics&amp;title=Evaluation%20of%20Machine%20Learning%20Classifiers%20in%20Keratoconus%20Detection%20from%20Orbscan%20II%20Examinations&amp;author=M.B.%20Souza&amp;author=F.W.%20Medeiros&amp;author=D.B.%20Souza&amp;author=R.%20Garcia&amp;author=M.R.%20Alves&amp;volume=65&amp;publication_year=2010&amp;pages=1223-1228&amp;pmid=21340208&amp;doi=10.1590/S1807-59322010001200002&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B58-vision-09-00071">
<span class="label">58.</span><cite>Subramanian P., Ramesh G.P., Parameshachari B.D.  Distributed Computing and Optimization Techniques, ICDCOT 2021. Springer; Berlin/Heidelberg, Germany: 2022. Comparative Analysis of Machine Learning Approaches for the Early Diagnosis of Keratoconus; pp. 241–250.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Distributed%20Computing%20and%20Optimization%20Techniques,%20ICDCOT%202021&amp;author=P.%20Subramanian&amp;author=G.P.%20Ramesh&amp;author=B.D.%20Parameshachari&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B59-vision-09-00071">
<span class="label">59.</span><cite>Valdes-Mas M.A., Martin-Guerrero J.D., Ruperez M.J., Pastor F., Dualde C., Monserrat C., Peris-Martinez C. A New Approach Based on Machine Learning for Predicting Corneal Curvature (K1) and Astigmatism in Patients with Keratoconus after Intracorneal Ring Implantation. Comput. Methods Programs Biomed. 2014;116:39–47. doi: 10.1016/j.cmpb.2014.04.003.</cite> [<a href="https://doi.org/10.1016/j.cmpb.2014.04.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24857632/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput.%20Methods%20Programs%20Biomed.&amp;title=A%20New%20Approach%20Based%20on%20Machine%20Learning%20for%20Predicting%20Corneal%20Curvature%20(K1)%20and%20Astigmatism%20in%20Patients%20with%20Keratoconus%20after%20Intracorneal%20Ring%20Implantation&amp;author=M.A.%20Valdes-Mas&amp;author=J.D.%20Martin-Guerrero&amp;author=M.J.%20Ruperez&amp;author=F.%20Pastor&amp;author=C.%20Dualde&amp;volume=116&amp;publication_year=2014&amp;pages=39-47&amp;pmid=24857632&amp;doi=10.1016/j.cmpb.2014.04.003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B60-vision-09-00071">
<span class="label">60.</span><cite>Bolarin J.M., Cavas E., Velazquez J.S., Alio J.L. A Machine-Learning Model Based on Morphogeometric Parameters for RETICS Disease Classification and GUI Development. Appl. Sci. 2020;10:1874.  doi: 10.3390/app10051874.</cite> [<a href="https://doi.org/10.3390/app10051874" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Appl.%20Sci.&amp;title=A%20Machine-Learning%20Model%20Based%20on%20Morphogeometric%20Parameters%20for%20RETICS%20Disease%20Classification%20and%20GUI%20Development&amp;author=J.M.%20Bolarin&amp;author=E.%20Cavas&amp;author=J.S.%20Velazquez&amp;author=J.L.%20Alio&amp;volume=10&amp;publication_year=2020&amp;pages=1874&amp;doi=10.3390/app10051874&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B61-vision-09-00071">
<span class="label">61.</span><cite>Kato N., Masumoto H., Tanabe M., Sakai C., Negishi K., Torii H., Tabuchi H., Tsubota K. Predicting keratoconus progression using deep learning. Investig. Ophthalmol. Vis. Sci. 2021;62:769. doi: 10.3390/jcm10040844.</cite> [<a href="https://doi.org/10.3390/jcm10040844" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7923054/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33670732/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Predicting%20keratoconus%20progression%20using%20deep%20learning&amp;author=N.%20Kato&amp;author=H.%20Masumoto&amp;author=M.%20Tanabe&amp;author=C.%20Sakai&amp;author=K.%20Negishi&amp;volume=62&amp;publication_year=2021&amp;pages=769&amp;pmid=33670732&amp;doi=10.3390/jcm10040844&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B62-vision-09-00071">
<span class="label">62.</span><cite>Cao K., Verspoor K., Chan E., Daniell M., Sahebjada S., Baird P.N. Machine learning with a reduced dimensionality representation of comprehensive Pentacam tomography parameters to identify subclinical keratoconus. Comput. Biol. Med. 2021;138:104884.  doi: 10.1016/j.compbiomed.2021.104884.</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2021.104884" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34607273/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput.%20Biol.%20Med.&amp;title=Machine%20learning%20with%20a%20reduced%20dimensionality%20representation%20of%20comprehensive%20Pentacam%20tomography%20parameters%20to%20identify%20subclinical%20keratoconus&amp;author=K.%20Cao&amp;author=K.%20Verspoor&amp;author=E.%20Chan&amp;author=M.%20Daniell&amp;author=S.%20Sahebjada&amp;volume=138&amp;publication_year=2021&amp;pages=104884&amp;pmid=34607273&amp;doi=10.1016/j.compbiomed.2021.104884&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B63-vision-09-00071">
<span class="label">63.</span><cite>Cao K., Verspoor K., Chan E., Daniell M., Sahebjada S., Baird P.N. Novel, high-performance machine learning model for detection of subclinical keratoconus. Investig. Ophthalmol. Vis. Sci. 2021;62:2157.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Novel,%20high-performance%20machine%20learning%20model%20for%20detection%20of%20subclinical%20keratoconus&amp;author=K.%20Cao&amp;author=K.%20Verspoor&amp;author=E.%20Chan&amp;author=M.%20Daniell&amp;author=S.%20Sahebjada&amp;volume=62&amp;publication_year=2021&amp;pages=2157&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B64-vision-09-00071">
<span class="label">64.</span><cite>Cao K., Verspoor K., Sahebjada S., Baird P.N. Evaluating the Performance of Various Machine Learning Algorithms to Detect Subclinical Keratoconus. Transl. Vis. Sci. Technol. 2020;9:24. doi: 10.1167/tvst.9.2.24.</cite> [<a href="https://doi.org/10.1167/tvst.9.2.24" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7396174/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32818085/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Transl.%20Vis.%20Sci.%20Technol.&amp;title=Evaluating%20the%20Performance%20of%20Various%20Machine%20Learning%20Algorithms%20to%20Detect%20Subclinical%20Keratoconus&amp;author=K.%20Cao&amp;author=K.%20Verspoor&amp;author=S.%20Sahebjada&amp;author=P.N.%20Baird&amp;volume=9&amp;publication_year=2020&amp;pages=24&amp;pmid=32818085&amp;doi=10.1167/tvst.9.2.24&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B65-vision-09-00071">
<span class="label">65.</span><cite>Gao Y.H., Wu Q., Li J., Sun J.D., Wan W.B. SVM-Based Automatic Diagnosis Method for Keratoconus; Proceedings of the Second International Workshop on Pattern Recognition; Singapore. 1–3 May 2017.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%20Second%20International%20Workshop%20on%20Pattern%20Recognition&amp;title=SVM-Based%20Automatic%20Diagnosis%20Method%20for%20Keratoconus&amp;author=Y.H.%20Gao&amp;author=Q.%20Wu&amp;author=J.%20Li&amp;author=J.D.%20Sun&amp;author=W.B.%20Wan&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B66-vision-09-00071">
<span class="label">66.</span><cite>Lavric A., Anchidin L., Popa V., Al-Timemy A.H., Alyasseri Z., Takahashi H., Yousefi S., Hazarbassanov R.M. Evaluation of keratoconus detection from elevation, topography and pachymetry raw data using machine learning. Investig. Ophthalmol. Vis. Sci. 2021;62:2154.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Evaluation%20of%20keratoconus%20detection%20from%20elevation,%20topography%20and%20pachymetry%20raw%20data%20using%20machine%20learning&amp;author=A.%20Lavric&amp;author=L.%20Anchidin&amp;author=V.%20Popa&amp;author=A.H.%20Al-Timemy&amp;author=Z.%20Alyasseri&amp;volume=62&amp;publication_year=2021&amp;pages=2154&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B67-vision-09-00071">
<span class="label">67.</span><cite>Herber R., Spoerl E., Pillunat L.E., Raiskup F. Classification of dynamic corneal response parameters concerning the topographical severity of keratoconus using the dynamic Scheimpflug imaging and machine-learning algorithms. Investig. Ophthalmol. Vis. Sci. 2020;61:5210.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Classification%20of%20dynamic%20corneal%20response%20parameters%20concerning%20the%20topographical%20severity%20of%20keratoconus%20using%20the%20dynamic%20Scheimpflug%20imaging%20and%20machine-learning%20algorithms&amp;author=R.%20Herber&amp;author=E.%20Spoerl&amp;author=L.E.%20Pillunat&amp;author=F.%20Raiskup&amp;volume=61&amp;publication_year=2020&amp;pages=5210&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B68-vision-09-00071">
<span class="label">68.</span><cite>Hernandez L.A., Sanchez-Huerta V., Ramirez-Fernandez M., Hernandez-Quintela E. Combinatorial approach to determine top performing keratometric features and machine learning algorithms for keratoconus detection. Investig. Ophthalmol. Vis. Sci. 2020;61:4750.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Combinatorial%20approach%20to%20determine%20top%20performing%20keratometric%20features%20and%20machine%20learning%20algorithms%20for%20keratoconus%20detection&amp;author=L.A.%20Hernandez&amp;author=V.%20Sanchez-Huerta&amp;author=M.%20Ramirez-Fernandez&amp;author=E.%20Hernandez-Quintela&amp;volume=61&amp;publication_year=2020&amp;pages=4750&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B69-vision-09-00071">
<span class="label">69.</span><cite>Hidalgo I.R., Perez P.R., Rozema J.J., Tassignon M. Comparison of Machine Learning Methods to Automatically Classify Keratoconus. Investig. Ophthalmol. Vis. Sci. 2014;55:4206.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Comparison%20of%20Machine%20Learning%20Methods%20to%20Automatically%20Classify%20Keratoconus&amp;author=I.R.%20Hidalgo&amp;author=P.R.%20Perez&amp;author=J.J.%20Rozema&amp;author=M.%20Tassignon&amp;volume=55&amp;publication_year=2014&amp;pages=4206&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B70-vision-09-00071">
<span class="label">70.</span><cite>Issarti I., Consejo A., Jimenez-Garcia M., Hershko S., Koppen C., Rozema J.J. Computer aided diagnosis for suspect keratoconus detection. Comput. Biol. Med. 2019;109:33–42. doi: 10.1016/j.compbiomed.2019.04.024.</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2019.04.024" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31035069/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput.%20Biol.%20Med.&amp;title=Computer%20aided%20diagnosis%20for%20suspect%20keratoconus%20detection&amp;author=I.%20Issarti&amp;author=A.%20Consejo&amp;author=M.%20Jimenez-Garcia&amp;author=S.%20Hershko&amp;author=C.%20Koppen&amp;volume=109&amp;publication_year=2019&amp;pages=33-42&amp;pmid=31035069&amp;doi=10.1016/j.compbiomed.2019.04.024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B71-vision-09-00071">
<span class="label">71.</span><cite>Issarti I., Consejo A., Jimenez-Garcia M., Kreps E.O., Koppen C., Rozema J.J. Logistic index for keratoconus detection and severity scoring (Logik) Comput. Biol. Med. 2020;122:103809.  doi: 10.1016/j.compbiomed.2020.103809.</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2020.103809" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32658727/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput.%20Biol.%20Med.&amp;title=Logistic%20index%20for%20keratoconus%20detection%20and%20severity%20scoring%20(Logik)&amp;author=I.%20Issarti&amp;author=A.%20Consejo&amp;author=M.%20Jimenez-Garcia&amp;author=E.O.%20Kreps&amp;author=C.%20Koppen&amp;volume=122&amp;publication_year=2020&amp;pages=103809&amp;pmid=32658727&amp;doi=10.1016/j.compbiomed.2020.103809&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B72-vision-09-00071">
<span class="label">72.</span><cite>Kovacs I., Mihaltz K., Kranitz K., Juhasz E., Takacs A., Dienes L., Gergely R., Nagy Z. Accuracy of machine learning classifiers using bilateral data from a Scheimpflug camera for identifying eyes with preclinical signs of keratoconus. J. Cataract Refract. Surg. 2016;42:275–283. doi: 10.1016/j.jcrs.2015.09.020.</cite> [<a href="https://doi.org/10.1016/j.jcrs.2015.09.020" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27026453/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Cataract%20Refract.%20Surg.&amp;title=Accuracy%20of%20machine%20learning%20classifiers%20using%20bilateral%20data%20from%20a%20Scheimpflug%20camera%20for%20identifying%20eyes%20with%20preclinical%20signs%20of%20keratoconus&amp;author=I.%20Kovacs&amp;author=K.%20Mihaltz&amp;author=K.%20Kranitz&amp;author=E.%20Juhasz&amp;author=A.%20Takacs&amp;volume=42&amp;publication_year=2016&amp;pages=275-283&amp;pmid=27026453&amp;doi=10.1016/j.jcrs.2015.09.020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B73-vision-09-00071">
<span class="label">73.</span><cite>Langenbucher A., Hafner L., Eppig T., Seitz B., Szentmary N., Flockerzi E. Keratoconus detection and classification from parameters of the Corvis (R) ST A study based on algorithms of machine learning. Ophthalmologe. 2021;118:697–706. doi: 10.1007/s00347-020-01231-1.</cite> [<a href="https://doi.org/10.1007/s00347-020-01231-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8260544/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32970190/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ophthalmologe&amp;title=Keratoconus%20detection%20and%20classification%20from%20parameters%20of%20the%20Corvis%20(R)%20ST%20A%20study%20based%20on%20algorithms%20of%20machine%20learning&amp;author=A.%20Langenbucher&amp;author=L.%20Hafner&amp;author=T.%20Eppig&amp;author=B.%20Seitz&amp;author=N.%20Szentmary&amp;volume=118&amp;publication_year=2021&amp;pages=697-706&amp;pmid=32970190&amp;doi=10.1007/s00347-020-01231-1&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B74-vision-09-00071">
<span class="label">74.</span><cite>Lavric A., Anchidin L., Popa V., Al-Timemy A.H., Alyasseri Z., Takahashi H. Keratoconus Severity Detection From Elevation, Topography and Pachymetry Raw Data Using a Machine Learning Approach. IEEE Access. 2021;9:84344–84355. doi: 10.1109/ACCESS.2021.3086021.</cite> [<a href="https://doi.org/10.1109/ACCESS.2021.3086021" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=Keratoconus%20Severity%20Detection%20From%20Elevation,%20Topography%20and%20Pachymetry%20Raw%20Data%20Using%20a%20Machine%20Learning%20Approach&amp;author=A.%20Lavric&amp;author=L.%20Anchidin&amp;author=V.%20Popa&amp;author=A.H.%20Al-Timemy&amp;author=Z.%20Alyasseri&amp;volume=9&amp;publication_year=2021&amp;pages=84344-84355&amp;doi=10.1109/ACCESS.2021.3086021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B75-vision-09-00071">
<span class="label">75.</span><cite>Lyra D., Ribeiro G., Torquetti L., Ferrara P., Machado A., Lyra J.M. Computational Models for Optimization of the Intrastromal Corneal Ring Choice in Patients With Keratoconus Using Corneal Tomography Data. J. Refract. Surg. 2018;34:547–550. doi: 10.3928/1081597X-20180615-01.</cite> [<a href="https://doi.org/10.3928/1081597X-20180615-01" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30089185/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Refract.%20Surg.&amp;title=Computational%20Models%20for%20Optimization%20of%20the%20Intrastromal%20Corneal%20Ring%20Choice%20in%20Patients%20With%20Keratoconus%20Using%20Corneal%20Tomography%20Data&amp;author=D.%20Lyra&amp;author=G.%20Ribeiro&amp;author=L.%20Torquetti&amp;author=P.%20Ferrara&amp;author=A.%20Machado&amp;volume=34&amp;publication_year=2018&amp;pages=547-550&amp;pmid=30089185&amp;doi=10.3928/1081597X-20180615-01&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B76-vision-09-00071">
<span class="label">76.</span><cite>Mehravaran S., Dehzangi I., Rahman M.M. Interocular Symmetry Analysis of Corneal Elevation Using the Fellow Eye as the Reference Surface and Machine Learning. Healthcare. 2021;9:1738.  doi: 10.3390/healthcare9121738.</cite> [<a href="https://doi.org/10.3390/healthcare9121738" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8702115/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34946464/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Healthcare&amp;title=Interocular%20Symmetry%20Analysis%20of%20Corneal%20Elevation%20Using%20the%20Fellow%20Eye%20as%20the%20Reference%20Surface%20and%20Machine%20Learning&amp;author=S.%20Mehravaran&amp;author=I.%20Dehzangi&amp;author=M.M.%20Rahman&amp;volume=9&amp;publication_year=2021&amp;pages=1738&amp;pmid=34946464&amp;doi=10.3390/healthcare9121738&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B77-vision-09-00071">
<span class="label">77.</span><cite>Ruiz Hidalgo I., Rodriguez P., Rozema J.J., Ni Dhubhghaill S., Zakaria N., Tassignon M.-J., Koppen C. Evaluation of a Machine-Learning Classifier for Keratoconus Detection Based on Scheimpflug Tomography. Cornea. 2016;35:827–832. doi: 10.1097/ICO.0000000000000834.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000000834" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27055215/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Evaluation%20of%20a%20Machine-Learning%20Classifier%20for%20Keratoconus%20Detection%20Based%20on%20Scheimpflug%20Tomography&amp;author=I.%20Ruiz%20Hidalgo&amp;author=P.%20Rodriguez&amp;author=J.J.%20Rozema&amp;author=S.%20Ni%20Dhubhghaill&amp;author=N.%20Zakaria&amp;volume=35&amp;publication_year=2016&amp;pages=827-832&amp;pmid=27055215&amp;doi=10.1097/ICO.0000000000000834&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B78-vision-09-00071">
<span class="label">78.</span><cite>Ruiz Hidalgo I., Rozema J.J., Saad A., Gatinel D., Rodriguez P., Zakaria N., Koppen C. Validation of an Objective Keratoconus Detection System Implemented in a Scheimpflug Tomographer and Comparison with Other Methods. Cornea. 2017;36:689–695. doi: 10.1097/ICO.0000000000001194.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000001194" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28368992/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Validation%20of%20an%20Objective%20Keratoconus%20Detection%20System%20Implemented%20in%20a%20Scheimpflug%20Tomographer%20and%20Comparison%20with%20Other%20Methods&amp;author=I.%20Ruiz%20Hidalgo&amp;author=J.J.%20Rozema&amp;author=A.%20Saad&amp;author=D.%20Gatinel&amp;author=P.%20Rodriguez&amp;volume=36&amp;publication_year=2017&amp;pages=689-695&amp;pmid=28368992&amp;doi=10.1097/ICO.0000000000001194&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B79-vision-09-00071">
<span class="label">79.</span><cite>Shi C., Wang M., Zhu T., Zhang Y., Ye Y., Jiang J., Chen S., Lu F., Shen M. Machine learning helps improve diagnostic ability of subclinical keratoconus using Scheimpflug and OCT imaging modalities. Eye Vis. 2020;7:48. doi: 10.1186/s40662-020-00213-3.</cite> [<a href="https://doi.org/10.1186/s40662-020-00213-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7507244/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32974414/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eye%20Vis.&amp;title=Machine%20learning%20helps%20improve%20diagnostic%20ability%20of%20subclinical%20keratoconus%20using%20Scheimpflug%20and%20OCT%20imaging%20modalities&amp;author=C.%20Shi&amp;author=M.%20Wang&amp;author=T.%20Zhu&amp;author=Y.%20Zhang&amp;author=Y.%20Ye&amp;volume=7&amp;publication_year=2020&amp;pages=48&amp;pmid=32974414&amp;doi=10.1186/s40662-020-00213-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B80-vision-09-00071">
<span class="label">80.</span><cite>Silverman R.H., Urs R., RoyChaudhury A., Archer T.J., Gobbe M., Reinstein D.Z. Combined tomography and epithelial thickness mapping for diagnosis of keratoconus. Eur. J. Ophthalmol. 2017;27:129–134. doi: 10.5301/ejo.5000850.</cite> [<a href="https://doi.org/10.5301/ejo.5000850" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5299092/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27515569/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eur.%20J.%20Ophthalmol.&amp;title=Combined%20tomography%20and%20epithelial%20thickness%20mapping%20for%20diagnosis%20of%20keratoconus&amp;author=R.H.%20Silverman&amp;author=R.%20Urs&amp;author=A.%20RoyChaudhury&amp;author=T.J.%20Archer&amp;author=M.%20Gobbe&amp;volume=27&amp;publication_year=2017&amp;pages=129-134&amp;pmid=27515569&amp;doi=10.5301/ejo.5000850&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B81-vision-09-00071">
<span class="label">81.</span><cite>Smadja D., Touboul D., Cohen A., Doveh E., Santhiago M.R., Mello G.R., Krueger R.R., Colin J. Detection of subclinical keratoconus using an automated decision tree classification. Am. J. Ophthalmol. 2013;156:237–246.e1. doi: 10.1016/j.ajo.2013.03.034.</cite> [<a href="https://doi.org/10.1016/j.ajo.2013.03.034" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23746611/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Am.%20J.%20Ophthalmol.&amp;title=Detection%20of%20subclinical%20keratoconus%20using%20an%20automated%20decision%20tree%20classification&amp;author=D.%20Smadja&amp;author=D.%20Touboul&amp;author=A.%20Cohen&amp;author=E.%20Doveh&amp;author=M.R.%20Santhiago&amp;volume=156&amp;publication_year=2013&amp;pages=237-246.e1&amp;pmid=23746611&amp;doi=10.1016/j.ajo.2013.03.034&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B82-vision-09-00071">
<span class="label">82.</span><cite>Srivatsa P.R., Shetty R., Matalia H., Roy A.S. A new Zernike algorithm to link asymmetric corneal thickness to corneal wavefront aberrations for diagnosis of keratoconus. Investig. Ophthalmol. Vis. Sci. 2015;56:1618.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=A%20new%20Zernike%20algorithm%20to%20link%20asymmetric%20corneal%20thickness%20to%20corneal%20wavefront%20aberrations%20for%20diagnosis%20of%20keratoconus&amp;author=P.R.%20Srivatsa&amp;author=R.%20Shetty&amp;author=H.%20Matalia&amp;author=A.S.%20Roy&amp;volume=56&amp;publication_year=2015&amp;pages=1618&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B83-vision-09-00071">
<span class="label">83.</span><cite>Takahashi H., Al-Timemy A.H., Mosa Z.M., Alyasseri Z., Lavric A., Filho J.A.P.M., Yuda K., Hazarbassonov R.M., Yousefi S. Detecting keratoconus severity from corneal data of different populations with machine learning. Investig. Ophthalmol. Vis. Sci. 2021;62:2145.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Detecting%20keratoconus%20severity%20from%20corneal%20data%20of%20different%20populations%20with%20machine%20learning&amp;author=H.%20Takahashi&amp;author=A.H.%20Al-Timemy&amp;author=Z.M.%20Mosa&amp;author=Z.%20Alyasseri&amp;author=A.%20Lavric&amp;volume=62&amp;publication_year=2021&amp;pages=2145&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B84-vision-09-00071">
<span class="label">84.</span><cite>Afifah A., Syafira F., Afladhanti P.M., Dharmawidiarini D. Artificial intelligence as diagnostic modality for keratoconus: A systematic review and meta-analysis. J. Taibah Univ. Med. Sci. 2024;19:296–303. doi: 10.1016/j.jtumed.2023.12.007.</cite> [<a href="https://doi.org/10.1016/j.jtumed.2023.12.007" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10821587/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38283379/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Taibah%20Univ.%20Med.%20Sci.&amp;title=Artificial%20intelligence%20as%20diagnostic%20modality%20for%20keratoconus:%20A%20systematic%20review%20and%20meta-analysis&amp;author=A.%20Afifah&amp;author=F.%20Syafira&amp;author=P.M.%20Afladhanti&amp;author=D.%20Dharmawidiarini&amp;volume=19&amp;publication_year=2024&amp;pages=296-303&amp;pmid=38283379&amp;doi=10.1016/j.jtumed.2023.12.007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B85-vision-09-00071">
<span class="label">85.</span><cite>Alshammari G., Hamad A.A., Abdullah Z.M., Alshareef A.M., Alhebaishi N., Alshammari A., Belay A. Applications of Deep Learning on Topographic Images to Improve the Diagnosis for Dynamic Systems and Unconstrained Optimization. Wirel. Commun. Mob. Comput. 2021;2021:4672688. doi: 10.1155/2021/4672688.</cite> [<a href="https://doi.org/10.1155/2021/4672688" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Wirel.%20Commun.%20Mob.%20Comput.&amp;title=Applications%20of%20Deep%20Learning%20on%20Topographic%20Images%20to%20Improve%20the%20Diagnosis%20for%20Dynamic%20Systems%20and%20Unconstrained%20Optimization&amp;author=G.%20Alshammari&amp;author=A.A.%20Hamad&amp;author=Z.M.%20Abdullah&amp;author=A.M.%20Alshareef&amp;author=N.%20Alhebaishi&amp;volume=2021&amp;publication_year=2021&amp;pages=4672688&amp;doi=10.1155/2021/4672688&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B86-vision-09-00071">
<span class="label">86.</span><cite>Al-Timemy A., Al-Zubaidi L., Ghaeb N., Takahashi H., Lavric A., Mosa Z., Hazarbassanov R.M., Alyasseri Z.A.A., Yousefi S. A device-agnostic deep learning model for detecting keratoconus based on anterior elevation corneal maps. Investig. Ophthalmol. Vis. Sci. 2022;63:2101-F0090.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=A%20device-agnostic%20deep%20learning%20model%20for%20detecting%20keratoconus%20based%20on%20anterior%20elevation%20corneal%20maps&amp;author=A.%20Al-Timemy&amp;author=L.%20Al-Zubaidi&amp;author=N.%20Ghaeb&amp;author=H.%20Takahashi&amp;author=A.%20Lavric&amp;volume=63&amp;publication_year=2022&amp;pages=2101-F0090&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B87-vision-09-00071">
<span class="label">87.</span><cite>Al-Timemy A.H., Ghaeb N.H., Mosa Z.M., Escudero J. Deep Transfer Learning for Improved Detection of Keratoconus using Corneal Topographic Maps. Cogn. Comput. 2022;14:1627–1642. doi: 10.1007/s12559-021-09880-3.</cite> [<a href="https://doi.org/10.1007/s12559-021-09880-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cogn.%20Comput.&amp;title=Deep%20Transfer%20Learning%20for%20Improved%20Detection%20of%20Keratoconus%20using%20Corneal%20Topographic%20Maps&amp;author=A.H.%20Al-Timemy&amp;author=N.H.%20Ghaeb&amp;author=Z.M.%20Mosa&amp;author=J.%20Escudero&amp;volume=14&amp;publication_year=2022&amp;pages=1627-1642&amp;doi=10.1007/s12559-021-09880-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B88-vision-09-00071">
<span class="label">88.</span><cite>Al-Timemy A.H., Hazarbassanov R.M., Mosa Z.M., Alyasseri M., Lavric A., Oliveira da Rosa C.A., Griz C.P., Takahashi H., Yousefi S. A hybrid deep learning framework for keratoconus detection based on anterior and posterior corneal maps. Investig. Ophthalmol. Vis. Sci. 2021;62:46.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=A%20hybrid%20deep%20learning%20framework%20for%20keratoconus%20detection%20based%20on%20anterior%20and%20posterior%20corneal%20maps&amp;author=A.H.%20Al-Timemy&amp;author=R.M.%20Hazarbassanov&amp;author=Z.M.%20Mosa&amp;author=M.%20Alyasseri&amp;author=A.%20Lavric&amp;volume=62&amp;publication_year=2021&amp;pages=46&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B89-vision-09-00071">
<span class="label">89.</span><cite>Al-Timemy A.H., Mosa Z.M., Alyasseri Z., Lavric A., Lui M.M., Hazarbassanov R.M., Yousefi S. A Hybrid Deep Learning Construct for Detecting Keratoconus from Corneal Maps. Transl. Vis. Sci. Technol. 2021;10:16. doi: 10.1167/tvst.10.14.16.</cite> [<a href="https://doi.org/10.1167/tvst.10.14.16" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8684312/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34913952/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Transl.%20Vis.%20Sci.%20Technol.&amp;title=A%20Hybrid%20Deep%20Learning%20Construct%20for%20Detecting%20Keratoconus%20from%20Corneal%20Maps&amp;author=A.H.%20Al-Timemy&amp;author=Z.M.%20Mosa&amp;author=Z.%20Alyasseri&amp;author=A.%20Lavric&amp;author=M.M.%20Lui&amp;volume=10&amp;publication_year=2021&amp;pages=16&amp;pmid=34913952&amp;doi=10.1167/tvst.10.14.16&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B90-vision-09-00071">
<span class="label">90.</span><cite>Dos Santos V.A., Schmetterer L., Stegmann H., Pfister M., Messner A., Schmidinger G., Garhofer G., Werkmeister R.M. CorneaNet: Fast segmentation of cornea OCT scans of healthy and keratoconic eyes using deep learning. Biomed. Opt. Express. 2019;10:622–641. doi: 10.1364/BOE.10.000622.</cite> [<a href="https://doi.org/10.1364/BOE.10.000622" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6377876/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30800504/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Biomed.%20Opt.%20Express&amp;title=CorneaNet:%20Fast%20segmentation%20of%20cornea%20OCT%20scans%20of%20healthy%20and%20keratoconic%20eyes%20using%20deep%20learning&amp;author=V.A.%20Dos%20Santos&amp;author=L.%20Schmetterer&amp;author=H.%20Stegmann&amp;author=M.%20Pfister&amp;author=A.%20Messner&amp;volume=10&amp;publication_year=2019&amp;pages=622-641&amp;pmid=30800504&amp;doi=10.1364/BOE.10.000622&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B91-vision-09-00071">
<span class="label">91.</span><cite>Feng R.W., Xu Z., Zheng X., Hu H., Jin X., Chen D.Z., Yao K., Wu J. KerNet: A Novel Deep Learning Approach for Keratoconus and Sub-Clinical Keratoconus Detection Based on Raw Data of the Pentacam HR System. IEEE J. Biomed. Health Inform. 2021;25:3898–3910. doi: 10.1109/JBHI.2021.3079430.</cite> [<a href="https://doi.org/10.1109/JBHI.2021.3079430" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33979295/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20J.%20Biomed.%20Health%20Inform.&amp;title=KerNet:%20A%20Novel%20Deep%20Learning%20Approach%20for%20Keratoconus%20and%20Sub-Clinical%20Keratoconus%20Detection%20Based%20on%20Raw%20Data%20of%20the%20Pentacam%20HR%20System&amp;author=R.W.%20Feng&amp;author=Z.%20Xu&amp;author=X.%20Zheng&amp;author=H.%20Hu&amp;author=X.%20Jin&amp;volume=25&amp;publication_year=2021&amp;pages=3898-3910&amp;pmid=33979295&amp;doi=10.1109/JBHI.2021.3079430&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B92-vision-09-00071">
<span class="label">92.</span><cite>Firat M., Cankaya C., Cinar A., Tuncer T. Automatic detection of keratoconus on Pentacam images using feature selection based on deep learning. Int. J. Imaging Syst. Technol. 2022;32:1548–1560. doi: 10.1002/ima.22717.</cite> [<a href="https://doi.org/10.1002/ima.22717" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int.%20J.%20Imaging%20Syst.%20Technol.&amp;title=Automatic%20detection%20of%20keratoconus%20on%20Pentacam%20images%20using%20feature%20selection%20based%20on%20deep%20learning&amp;author=M.%20Firat&amp;author=C.%20Cankaya&amp;author=A.%20Cinar&amp;author=T.%20Tuncer&amp;volume=32&amp;publication_year=2022&amp;pages=1548-1560&amp;doi=10.1002/ima.22717&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B93-vision-09-00071">
<span class="label">93.</span><cite>Fisher A.C., Taktak A.F.G., Iskander D.R., Naroo S.A., Shah S., Kaye S.B., Batterbury M. Optimisation by an Artificial Neural Network of general ellipsotoric, Fourier Series and Zernike polynomial decompositions of anterior and posterior Orbscan topography data in keratoconus. Investig. Ophthalmol. Vis. Sci. 2004;45:2875.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Optimisation%20by%20an%20Artificial%20Neural%20Network%20of%20general%20ellipsotoric,%20Fourier%20Series%20and%20Zernike%20polynomial%20decompositions%20of%20anterior%20and%20posterior%20Orbscan%20topography%20data%20in%20keratoconus&amp;author=A.C.%20Fisher&amp;author=A.F.G.%20Taktak&amp;author=D.R.%20Iskander&amp;author=S.A.%20Naroo&amp;author=S.%20Shah&amp;volume=45&amp;publication_year=2004&amp;pages=2875&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B94-vision-09-00071">
<span class="label">94.</span><cite>Gairola S., Joshi P., Balasubramaniam A., Murali K., Kwatra N., Jain M. Keratoconus Classifier for Smartphone-based Corneal Topographer; Proceedings of the Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society; Glasgow, UK. 11–15 July 2022; pp. 1875–1878.</cite> [<a href="https://doi.org/10.1109/EMBC48229.2022.9871744" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36086067/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%20Annual%20International%20Conference%20of%20the%20IEEE%20Engineering%20in%20Medicine%20&amp;%20Biology%20Society&amp;title=Keratoconus%20Classifier%20for%20Smartphone-based%20Corneal%20Topographer&amp;author=S.%20Gairola&amp;author=P.%20Joshi&amp;author=A.%20Balasubramaniam&amp;author=K.%20Murali&amp;author=N.%20Kwatra&amp;volume=Volume%202022&amp;pages=1875-1878&amp;pmid=36086067&amp;doi=10.1109/EMBC48229.2022.9871744&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B95-vision-09-00071">
<span class="label">95.</span><cite>Gao H.B., Pan Z.-G., Shen M.-X., Lu F., Li H., Zhang X.-Q. KeratoScreen: Early Keratoconus Classification With Zernike Polynomial Using Deep Learning. Cornea. 2022;41:1158–1165. doi: 10.1097/ICO.0000000000003038.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000003038" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35543584/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=KeratoScreen:%20Early%20Keratoconus%20Classification%20With%20Zernike%20Polynomial%20Using%20Deep%20Learning&amp;author=H.B.%20Gao&amp;author=Z.-G.%20Pan&amp;author=M.-X.%20Shen&amp;author=F.%20Lu&amp;author=H.%20Li&amp;volume=41&amp;publication_year=2022&amp;pages=1158-1165&amp;pmid=35543584&amp;doi=10.1097/ICO.0000000000003038&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B96-vision-09-00071">
<span class="label">96.</span><cite>Ghaderi M., Sharifi A., Jafarzadeh Pour E. Proposing an ensemble learning model based on neural network and fuzzy system for keratoconus diagnosis based on Pentacam measurements. Int. Ophthalmol. 2021;41:3935–3948. doi: 10.1007/s10792-021-01963-2.</cite> [<a href="https://doi.org/10.1007/s10792-021-01963-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34322847/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int.%20Ophthalmol.&amp;title=Proposing%20an%20ensemble%20learning%20model%20based%20on%20neural%20network%20and%20fuzzy%20system%20for%20keratoconus%20diagnosis%20based%20on%20Pentacam%20measurements&amp;author=M.%20Ghaderi&amp;author=A.%20Sharifi&amp;author=E.%20Jafarzadeh%20Pour&amp;volume=41&amp;publication_year=2021&amp;pages=3935-3948&amp;pmid=34322847&amp;doi=10.1007/s10792-021-01963-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B97-vision-09-00071">
<span class="label">97.</span><cite>Kamiya K., Ayatsuka Y., Kato Y., Shoji N., Mori Y., Miyata K. Diagnosability of Keratoconus Using Deep Learning with Placido Disk-Based Corneal Topography. Front. Med. 2021;8:724902.  doi: 10.3389/fmed.2021.724902.</cite> [<a href="https://doi.org/10.3389/fmed.2021.724902" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8520919/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34671618/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Med.&amp;title=Diagnosability%20of%20Keratoconus%20Using%20Deep%20Learning%20with%20Placido%20Disk-Based%20Corneal%20Topography&amp;author=K.%20Kamiya&amp;author=Y.%20Ayatsuka&amp;author=Y.%20Kato&amp;author=N.%20Shoji&amp;author=Y.%20Mori&amp;volume=8&amp;publication_year=2021&amp;pages=724902&amp;pmid=34671618&amp;doi=10.3389/fmed.2021.724902&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B98-vision-09-00071">
<span class="label">98.</span><cite>Karimi A., Meimani N., Razaghi R., Rahmati S.M., Jadidi K., Rostami M. Biomechanics of the Healthy and Keratoconic Corneas: A Combination of the Clinical Data, Finite Element Analysis, and Artificial Neural Network. Curr. Pharm. Des. 2018;24:4474–4483. doi: 10.2174/1381612825666181224123939.</cite> [<a href="https://doi.org/10.2174/1381612825666181224123939" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30582471/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Curr.%20Pharm.%20Des.&amp;title=Biomechanics%20of%20the%20Healthy%20and%20Keratoconic%20Corneas:%20A%20Combination%20of%20the%20Clinical%20Data,%20Finite%20Element%20Analysis,%20and%20Artificial%20Neural%20Network&amp;author=A.%20Karimi&amp;author=N.%20Meimani&amp;author=R.%20Razaghi&amp;author=S.M.%20Rahmati&amp;author=K.%20Jadidi&amp;volume=24&amp;publication_year=2018&amp;pages=4474-4483&amp;pmid=30582471&amp;doi=10.2174/1381612825666181224123939&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B99-vision-09-00071">
<span class="label">99.</span><cite>Kuo B.-I., Chang W.-Y., Liao T.-S., Liu F.-Y., Liu H.-Y., Chu H.-S., Chen W.-L., Hu F.-R., Yen J.-Y., Wang I.-J. Keratoconus Screening Based on Deep Learning Approach of Corneal Topography. Transl. Vis. Sci. Technol. 2020;9:53. doi: 10.1167/tvst.9.2.53.</cite> [<a href="https://doi.org/10.1167/tvst.9.2.53" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7533740/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33062398/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Transl.%20Vis.%20Sci.%20Technol.&amp;title=Keratoconus%20Screening%20Based%20on%20Deep%20Learning%20Approach%20of%20Corneal%20Topography&amp;author=B.-I.%20Kuo&amp;author=W.-Y.%20Chang&amp;author=T.-S.%20Liao&amp;author=F.-Y.%20Liu&amp;author=H.-Y.%20Liu&amp;volume=9&amp;publication_year=2020&amp;pages=53&amp;pmid=33062398&amp;doi=10.1167/tvst.9.2.53&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B100-vision-09-00071">
<span class="label">100.</span><cite>Lavric A., Valentin P. KeratoDetect: Keratoconus Detection Algorithm Using Convolutional Neural Networks. Comput. Intell. Neurosci. 2019;2019:8162567. doi: 10.1155/2019/8162567.</cite> [<a href="https://doi.org/10.1155/2019/8162567" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6364125/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30809255/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput.%20Intell.%20Neurosci.&amp;title=KeratoDetect:%20Keratoconus%20Detection%20Algorithm%20Using%20Convolutional%20Neural%20Networks&amp;author=A.%20Lavric&amp;author=P.%20Valentin&amp;volume=2019&amp;publication_year=2019&amp;pages=8162567&amp;pmid=30809255&amp;doi=10.1155/2019/8162567&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B101-vision-09-00071">
<span class="label">101.</span><cite>Dongfang L., Yanling D., Sen X., Zhen G., Suxia L., Yan G., Bin L., Lixin X. Deep learning based lesion detection from anterior segment optical coherence tomography images and its application in the diagnosis of keratoconus. Chin. J. Ophthalmol. 2021;57:447–453. doi: 10.3760/cma.j.cn112142-20200818-00540.</cite> [<a href="https://doi.org/10.3760/cma.j.cn112142-20200818-00540" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34098694/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Chin.%20J.%20Ophthalmol.&amp;title=Deep%20learning%20based%20lesion%20detection%20from%20anterior%20segment%20optical%20coherence%20tomography%20images%20and%20its%20application%20in%20the%20diagnosis%20of%20keratoconus&amp;author=L.%20Dongfang&amp;author=D.%20Yanling&amp;author=X.%20Sen&amp;author=G.%20Zhen&amp;author=L.%20Suxia&amp;volume=57&amp;publication_year=2021&amp;pages=447-453&amp;pmid=34098694&amp;doi=10.3760/cma.j.cn112142-20200818-00540&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B102-vision-09-00071">
<span class="label">102.</span><cite>Liu H., Anwar M., Koaik M., Taylor S., Karanjia R., Mintsioulis G., Ziai S., Baig K. Deep Learning for Detection of Keratoconus and Prediction of Crosslinking Efficacy. Investig. Ophthalmol. Vis. Sci. 2021;62:2044.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Deep%20Learning%20for%20Detection%20of%20Keratoconus%20and%20Prediction%20of%20Crosslinking%20Efficacy&amp;author=H.%20Liu&amp;author=M.%20Anwar&amp;author=M.%20Koaik&amp;author=S.%20Taylor&amp;author=R.%20Karanjia&amp;volume=62&amp;publication_year=2021&amp;pages=2044&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B103-vision-09-00071">
<span class="label">103.</span><cite>Pavlatos E., Huang D., Li Y. Combining OCT Corneal Topography and Thickness Maps to Diagnose Keratoconus Using a Convolutional Neural Network. Investig. Ophthalmol. Vis. Sci. 2022;63:2109-F0098.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Combining%20OCT%20Corneal%20Topography%20and%20Thickness%20Maps%20to%20Diagnose%20Keratoconus%20Using%20a%20Convolutional%20Neural%20Network&amp;author=E.%20Pavlatos&amp;author=D.%20Huang&amp;author=Y.%20Li&amp;volume=63&amp;publication_year=2022&amp;pages=2109-F0098&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B104-vision-09-00071">
<span class="label">104.</span><cite>Perissutti P., Accardo A.P., Pensiero S., Salvetat M.L. Automatic keratoconus detection by means of a neural network: Comparison between a monocular and a binocular approach; Proceedings of the 20th Annual International Conference of the IEEE Engineering in Medicine and Biology Society Biomedical Engineering Towards the Year 2000 and Beyond; Hong Kong, China. 1 November 1998; pp. 1397–1399.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%2020th%20Annual%20International%20Conference%20of%20the%20IEEE%20Engineering%20in%20Medicine%20and%20Biology%20Society%20Biomedical%20Engineering%20Towards%20the%20Year%202000%20and%20Beyond&amp;title=Automatic%20keratoconus%20detection%20by%20means%20of%20a%20neural%20network:%20Comparison%20between%20a%20monocular%20and%20a%20binocular%20approach&amp;author=P.%20Perissutti&amp;author=A.P.%20Accardo&amp;author=S.%20Pensiero&amp;author=M.L.%20Salvetat&amp;pages=1397-1399&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B105-vision-09-00071">
<span class="label">105.</span><cite>Quah X.M., Kerdraon Y. Construction and evaluation of an artificial neural network based screening tool for keratoconus using refractive error measurements. Clin. Exp. Ophthalmol. 2022;49:862–863.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Clin.%20Exp.%20Ophthalmol.&amp;title=Construction%20and%20evaluation%20of%20an%20artificial%20neural%20network%20based%20screening%20tool%20for%20keratoconus%20using%20refractive%20error%20measurements&amp;author=X.M.%20Quah&amp;author=Y.%20Kerdraon&amp;volume=49&amp;publication_year=2022&amp;pages=862-863&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B106-vision-09-00071">
<span class="label">106.</span><cite>Smolek M.K., Klyce S.D. Current keratoconus detection methods compared with a neural network approach. Investig. Ophthalmol. Vis. Sci. 1997;38:2290–2299.</cite> [<a href="https://pubmed.ncbi.nlm.nih.gov/9344352/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Current%20keratoconus%20detection%20methods%20compared%20with%20a%20neural%20network%20approach&amp;author=M.K.%20Smolek&amp;author=S.D.%20Klyce&amp;volume=38&amp;publication_year=1997&amp;pages=2290-2299&amp;pmid=9344352&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B107-vision-09-00071">
<span class="label">107.</span><cite>Ucar M., Sen B., Cakmak H.B., IEEE  A Novel Classification and Estimation Approach for Detecting Keratoconus Disease with Intelligent Systems; Proceedings of the 2013 8th International Conference on Electrical and Electronics Engineering (ELECO); Bursa, Turkey. 28–30 November 2013; pp. 521–525.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%202013%208th%20International%20Conference%20on%20Electrical%20and%20Electronics%20Engineering%20(ELECO)&amp;title=A%20Novel%20Classification%20and%20Estimation%20Approach%20for%20Detecting%20Keratoconus%20Disease%20with%20Intelligent%20Systems&amp;author=M.%20Ucar&amp;author=B.%20Sen&amp;author=H.B.%20Cakmak&amp;pages=521-525&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B108-vision-09-00071">
<span class="label">108.</span><cite>Zaki W., Daud M.M., Saad A.H., Hussain A., Mutalib H.A. A Mobile Solution for Lateral Segment Photographed Images Based Deep Keratoconus Screening Method. Int. J. Integr. Eng. 2021;13:18–27. doi: 10.30880/ijie.2021.13.05.003.</cite> [<a href="https://doi.org/10.30880/ijie.2021.13.05.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int.%20J.%20Integr.%20Eng.&amp;title=A%20Mobile%20Solution%20for%20Lateral%20Segment%20Photographed%20Images%20Based%20Deep%20Keratoconus%20Screening%20Method&amp;author=W.%20Zaki&amp;author=M.M.%20Daud&amp;author=A.H.%20Saad&amp;author=A.%20Hussain&amp;author=H.A.%20Mutalib&amp;volume=13&amp;publication_year=2021&amp;pages=18-27&amp;doi=10.30880/ijie.2021.13.05.003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B109-vision-09-00071">
<span class="label">109.</span><cite>Zaki W., Daud M.M., Saad A.H., Hussain A., Mutalib H.A. Towards Automated Keratoconus Screening Approach using Lateral Segment Photographed Images; Proceedings of the 2020 IEEE-EMBS Conference on Biomedical Engineering and Science (IECBES 2020): Leading Modern Healthcare Technology Enhancing Wellness; Langkawi Island, Malaysia. 1–3 March 2021; pp. 466–471.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%202020%20IEEE-EMBS%20Conference%20on%20Biomedical%20Engineering%20and%20Science%20(IECBES%202020):%20Leading%20Modern%20Healthcare%20Technology%20Enhancing%20Wellness&amp;title=Towards%20Automated%20Keratoconus%20Screening%20Approach%20using%20Lateral%20Segment%20Photographed%20Images&amp;author=W.%20Zaki&amp;author=M.M.%20Daud&amp;author=A.H.%20Saad&amp;author=A.%20Hussain&amp;author=H.A.%20Mutalib&amp;pages=466-471&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B110-vision-09-00071">
<span class="label">110.</span><cite>Hazarbassanov R.M., Alyasseri Z.A.A., Al-Timemy A., Lavric A., Abasid A.K., Takahashi H., Filho J.A.M., Campos M., Yousefi S. Detecting keratoconus on two different populations using an unsupervised hybrid artificial intelligence model. Investig. Ophthalmol. Vis. Sci. 2022;63:2088-F0077.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Detecting%20keratoconus%20on%20two%20different%20populations%20using%20an%20unsupervised%20hybrid%20artificial%20intelligence%20model&amp;author=R.M.%20Hazarbassanov&amp;author=Z.A.A.%20Alyasseri&amp;author=A.%20Al-Timemy&amp;author=A.%20Lavric&amp;author=A.K.%20Abasid&amp;volume=63&amp;publication_year=2022&amp;pages=2088-F0077&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B111-vision-09-00071">
<span class="label">111.</span><cite>Lavric A., Popa V., Takahashi H., Yousefi S. Detecting Keratoconus From Corneal Imaging Data Using Machine Learning. IEEE Access. 2020;8:149113–149121. doi: 10.1109/ACCESS.2020.3016060.</cite> [<a href="https://doi.org/10.1109/ACCESS.2020.3016060" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=Detecting%20Keratoconus%20From%20Corneal%20Imaging%20Data%20Using%20Machine%20Learning&amp;author=A.%20Lavric&amp;author=V.%20Popa&amp;author=H.%20Takahashi&amp;author=S.%20Yousefi&amp;volume=8&amp;publication_year=2020&amp;pages=149113-149121&amp;doi=10.1109/ACCESS.2020.3016060&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B112-vision-09-00071">
<span class="label">112.</span><cite>Herber R., Pillunat L.E., Raiskup F. Development of a classification system based on corneal biomechanical properties using artificial intelligence predicting keratoconus severity. Eye Vis. 2021;8:21. doi: 10.1186/s40662-021-00244-4.</cite> [<a href="https://doi.org/10.1186/s40662-021-00244-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8167942/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34059127/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eye%20Vis.&amp;title=Development%20of%20a%20classification%20system%20based%20on%20corneal%20biomechanical%20properties%20using%20artificial%20intelligence%20predicting%20keratoconus%20severity&amp;author=R.%20Herber&amp;author=L.E.%20Pillunat&amp;author=F.%20Raiskup&amp;volume=8&amp;publication_year=2021&amp;pages=21&amp;pmid=34059127&amp;doi=10.1186/s40662-021-00244-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B113-vision-09-00071">
<span class="label">113.</span><cite>Aatila M., Lachgar M., Hamid H., Kartit A. Keratoconus Severity Classification Using Features Selection and Machine Learning Algorithms. Comput. Math. Methods Med. 2021;2021:9979560. doi: 10.1155/2021/9979560.</cite> [<a href="https://doi.org/10.1155/2021/9979560" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8610665/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34824602/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput.%20Math.%20Methods%20Med.&amp;title=Keratoconus%20Severity%20Classification%20Using%20Features%20Selection%20and%20Machine%20Learning%20Algorithms&amp;author=M.%20Aatila&amp;author=M.%20Lachgar&amp;author=H.%20Hamid&amp;author=A.%20Kartit&amp;volume=2021&amp;publication_year=2021&amp;pages=9979560&amp;pmid=34824602&amp;doi=10.1155/2021/9979560&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B114-vision-09-00071">
<span class="label">114.</span><cite>Hosoda Y., Miyake M., Meguro A., Tabara Y., Iwai S., Ueda-Arakawa N., Nakano E., Mori Y., Yoshikawa M., Nakanishi H., et al.  Keratoconus-susceptibility gene identification by corneal thickness genome-wide association study and artificial intelligence IBM Watson. Commun. Biol. 2020;3:410.  doi: 10.1038/s42003-020-01137-3.</cite> [<a href="https://doi.org/10.1038/s42003-020-01137-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7395727/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32737415/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Commun.%20Biol.&amp;title=Keratoconus-susceptibility%20gene%20identification%20by%20corneal%20thickness%20genome-wide%20association%20study%20and%20artificial%20intelligence%20IBM%20Watson&amp;author=Y.%20Hosoda&amp;author=M.%20Miyake&amp;author=A.%20Meguro&amp;author=Y.%20Tabara&amp;author=S.%20Iwai&amp;volume=3&amp;publication_year=2020&amp;pages=410&amp;pmid=32737415&amp;doi=10.1038/s42003-020-01137-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B115-vision-09-00071">
<span class="label">115.</span><cite>Vandevenne M.M., Favuzza E., Veta M., Lucenteforte E., Berendschot T.T., Mencucci R., Nuijts R.M., Virgili G., Dickman M.M. Artificial intelligence for detecting keratoconus. Cochrane Database Syst. Rev. 2023;11:CD014911. doi: 10.1002/14651858.cd014911.pub2.</cite> [<a href="https://doi.org/10.1002/14651858.cd014911.pub2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10646985/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37965960/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cochrane%20Database%20Syst.%20Rev.&amp;title=Artificial%20intelligence%20for%20detecting%20keratoconus&amp;author=M.M.%20Vandevenne&amp;author=E.%20Favuzza&amp;author=M.%20Veta&amp;author=E.%20Lucenteforte&amp;author=T.T.%20Berendschot&amp;volume=11&amp;publication_year=2023&amp;pages=CD014911&amp;pmid=37965960&amp;doi=10.1002/14651858.cd014911.pub2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B116-vision-09-00071">
<span class="label">116.</span><cite>Niazi S., Jiménez-García M., Findl O., Gatzioufas Z., Doroodgar F., Shahriari M.H., Javadi M.A. Keratoconus Diagnosis: From Fundamentals to Artificial Intelligence: A Systematic Narrative Review. Diagnostics. 2023;13:2715.  doi: 10.3390/diagnostics13162715.</cite> [<a href="https://doi.org/10.3390/diagnostics13162715" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10453081/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37627975/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diagnostics&amp;title=Keratoconus%20Diagnosis:%20From%20Fundamentals%20to%20Artificial%20Intelligence:%20A%20Systematic%20Narrative%20Review&amp;author=S.%20Niazi&amp;author=M.%20Jim%C3%A9nez-Garc%C3%ADa&amp;author=O.%20Findl&amp;author=Z.%20Gatzioufas&amp;author=F.%20Doroodgar&amp;volume=13&amp;publication_year=2023&amp;pages=2715&amp;pmid=37627975&amp;doi=10.3390/diagnostics13162715&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B117-vision-09-00071">
<span class="label">117.</span><cite>Hashemi H., Doroodgar F., Niazi S., Khabazkhoob M., Heidari Z. Comparison of different corneal imaging modalities using artificial intelligence for diagnosis of keratoconus: A systematic review and meta-analysis. Graefes Arch. Clin. Exp. Ophthalmol. 2024;262:1017–1039. doi: 10.1007/s00417-023-06154-6.</cite> [<a href="https://doi.org/10.1007/s00417-023-06154-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37418053/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Graefes%20Arch.%20Clin.%20Exp.%20Ophthalmol.&amp;title=Comparison%20of%20different%20corneal%20imaging%20modalities%20using%20artificial%20intelligence%20for%20diagnosis%20of%20keratoconus:%20A%20systematic%20review%20and%20meta-analysis&amp;author=H.%20Hashemi&amp;author=F.%20Doroodgar&amp;author=S.%20Niazi&amp;author=M.%20Khabazkhoob&amp;author=Z.%20Heidari&amp;volume=262&amp;publication_year=2024&amp;pages=1017-1039&amp;pmid=37418053&amp;doi=10.1007/s00417-023-06154-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B118-vision-09-00071">
<span class="label">118.</span><cite>Rocha K.M., Van den Berg R., Van den Berg A., Ambrosio R. Optimized artificial intelligence for enhanced ectasia detection using Scheimpflug-based corneal tomography and biomechanical data. Am. J. Ophthalmol. 2022;240:115–123. doi: 10.1016/j.ajo.2022.12.016.</cite> [<a href="https://doi.org/10.1016/j.ajo.2022.12.016" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36549584/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Am.%20J.%20Ophthalmol.&amp;title=Optimized%20artificial%20intelligence%20for%20enhanced%20ectasia%20detection%20using%20Scheimpflug-based%20corneal%20tomography%20and%20biomechanical%20data&amp;author=K.M.%20Rocha&amp;author=R.%20Van%20den%20Berg&amp;author=A.%20Van%20den%20Berg&amp;author=R.%20Ambrosio&amp;volume=240&amp;publication_year=2022&amp;pages=115-123&amp;pmid=36549584&amp;doi=10.1016/j.ajo.2022.12.016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B119-vision-09-00071">
<span class="label">119.</span><cite>Ahn H., Kim N.E., Chung J.L., Kim Y.J., Jun I., Kim T.-I., Seo K.Y. Patient Selection for Corneal Topographic Evaluation of Keratoconus: A Screening Approach Using Artificial Intelligence. Front. Med. 2022;9:934865.  doi: 10.3389/fmed.2022.934865.</cite> [<a href="https://doi.org/10.3389/fmed.2022.934865" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9386450/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35991660/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Med.&amp;title=Patient%20Selection%20for%20Corneal%20Topographic%20Evaluation%20of%20Keratoconus:%20A%20Screening%20Approach%20Using%20Artificial%20Intelligence&amp;author=H.%20Ahn&amp;author=N.E.%20Kim&amp;author=J.L.%20Chung&amp;author=Y.J.%20Kim&amp;author=I.%20Jun&amp;volume=9&amp;publication_year=2022&amp;pages=934865&amp;pmid=35991660&amp;doi=10.3389/fmed.2022.934865&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B120-vision-09-00071">
<span class="label">120.</span><cite>Almeida G.C., Jr., Guido R.C., Balarin Silva H.M., Brandão C.C., de Mattos L.C., Lopes B.T., Machado A.P., Ambrósio R., Jr. New Artificial Intelligence Index Based on Scheimpflug Corneal Tomography to Distinguish Subclinical Keratoconus from Healthy Corneas. J. Cataract Refract. Surg. 2022;48:1168–1174. doi: 10.1097/j.jcrs.0000000000000946.</cite> [<a href="https://doi.org/10.1097/j.jcrs.0000000000000946" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35333829/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Cataract%20Refract.%20Surg.&amp;title=New%20Artificial%20Intelligence%20Index%20Based%20on%20Scheimpflug%20Corneal%20Tomography%20to%20Distinguish%20Subclinical%20Keratoconus%20from%20Healthy%20Corneas&amp;author=G.C.%20Almeida&amp;author=R.C.%20Guido&amp;author=H.M.%20Balarin%20Silva&amp;author=C.C.%20Brand%C3%A3o&amp;author=L.C.%20de%20Mattos&amp;volume=48&amp;publication_year=2022&amp;pages=1168-1174&amp;pmid=35333829&amp;doi=10.1097/j.jcrs.0000000000000946&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B121-vision-09-00071">
<span class="label">121.</span><cite>Zou H.H., Xu J.H., Zhang L., Ji S.F., Wang Y. Assistant Diagnose for Subclinical Keratoconus by Artificial Intelligence. Chin. J. Ophthalmol. 2019;55:911–915. doi: 10.3760/cma.j.issn.0412-4081.2019.12.008.</cite> [<a href="https://doi.org/10.3760/cma.j.issn.0412-4081.2019.12.008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31874504/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Chin.%20J.%20Ophthalmol.&amp;title=Assistant%20Diagnose%20for%20Subclinical%20Keratoconus%20by%20Artificial%20Intelligence&amp;author=H.H.%20Zou&amp;author=J.H.%20Xu&amp;author=L.%20Zhang&amp;author=S.F.%20Ji&amp;author=Y.%20Wang&amp;volume=55&amp;publication_year=2019&amp;pages=911-915&amp;pmid=31874504&amp;doi=10.3760/cma.j.issn.0412-4081.2019.12.008&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B122-vision-09-00071">
<span class="label">122.</span><cite>Goodman D., Zhu A.Y. Utility of Artificial Intelligence in the Diagnosis and Management of Keratoconus: A Systematic Review. Front. Ophthalmol. 2024;4:1380701.  doi: 10.3389/fopht.2024.1380701.</cite> [<a href="https://doi.org/10.3389/fopht.2024.1380701" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11182163/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38984114/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Ophthalmol.&amp;title=Utility%20of%20Artificial%20Intelligence%20in%20the%20Diagnosis%20and%20Management%20of%20Keratoconus:%20A%20Systematic%20Review&amp;author=D.%20Goodman&amp;author=A.Y.%20Zhu&amp;volume=4&amp;publication_year=2024&amp;pages=1380701&amp;pmid=38984114&amp;doi=10.3389/fopht.2024.1380701&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B123-vision-09-00071">
<span class="label">123.</span><cite>Abdelmotaal H., Mostafa M.M., Mostafa A.N.R., Mohamed A.A., Abdelazeem K. Classification of Color-Coded Scheimpflug Camera Corneal Tomography Images Using Deep Learning. Trans. Vis. Sci. Technol. 2020;9:30. doi: 10.1167/tvst.9.13.30.</cite> [<a href="https://doi.org/10.1167/tvst.9.13.30" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7757611/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33384884/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trans.%20Vis.%20Sci.%20Technol.&amp;title=Classification%20of%20Color-Coded%20Scheimpflug%20Camera%20Corneal%20Tomography%20Images%20Using%20Deep%20Learning&amp;author=H.%20Abdelmotaal&amp;author=M.M.%20Mostafa&amp;author=A.N.R.%20Mostafa&amp;author=A.A.%20Mohamed&amp;author=K.%20Abdelazeem&amp;volume=9&amp;publication_year=2020&amp;pages=30&amp;pmid=33384884&amp;doi=10.1167/tvst.9.13.30&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B124-vision-09-00071">
<span class="label">124.</span><cite>Gatinel D., Saad A. The Challenges of the Detection of Subclinical Keratoconus at Its Earliest Stage. Int. J. Keratoconus Ectatic Corneal Dis. 2012;1:36–43. doi: 10.5005/jp-journals-10025-1007.</cite> [<a href="https://doi.org/10.5005/jp-journals-10025-1007" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int.%20J.%20Keratoconus%20Ectatic%20Corneal%20Dis.&amp;title=The%20Challenges%20of%20the%20Detection%20of%20Subclinical%20Keratoconus%20at%20Its%20Earliest%20Stage&amp;author=D.%20Gatinel&amp;author=A.%20Saad&amp;volume=1&amp;publication_year=2012&amp;pages=36-43&amp;doi=10.5005/jp-journals-10025-1007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B125-vision-09-00071">
<span class="label">125.</span><cite>Kanellopoulos A.J. Keratoconus Management With Customized Photorefractive Keratectomy by Artificial Intelligence Ray-Tracing Optimization Combined with Higher Fluence Corneal Crosslinking: The Ray-Tracing Athens Protocol. Cornea. 2021;40:1181–1187. doi: 10.1097/ICO.0000000000002739.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000002739" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8330827/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34050067/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Keratoconus%20Management%20With%20Customized%20Photorefractive%20Keratectomy%20by%20Artificial%20Intelligence%20Ray-Tracing%20Optimization%20Combined%20with%20Higher%20Fluence%20Corneal%20Crosslinking:%20The%20Ray-Tracing%20Athens%20Protocol&amp;author=A.J.%20Kanellopoulos&amp;volume=40&amp;publication_year=2021&amp;pages=1181-1187&amp;pmid=34050067&amp;doi=10.1097/ICO.0000000000002739&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B126-vision-09-00071">
<span class="label">126.</span><cite>Kundu G., Shetty R., Khamar P., Mullick R., Gupta S., Nuijts R., Roy A.S. Universal Architecture of Corneal Segmental Tomography Biomarkers for Artificial Intelligence-Driven Diagnosis of Early Keratoconus. Br. J. Ophthalmol. 2021;107:635–643. doi: 10.1136/bjophthalmol-2021-319309.</cite> [<a href="https://doi.org/10.1136/bjophthalmol-2021-319309" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34916211/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Br.%20J.%20Ophthalmol.&amp;title=Universal%20Architecture%20of%20Corneal%20Segmental%20Tomography%20Biomarkers%20for%20Artificial%20Intelligence-Driven%20Diagnosis%20of%20Early%20Keratoconus&amp;author=G.%20Kundu&amp;author=R.%20Shetty&amp;author=P.%20Khamar&amp;author=R.%20Mullick&amp;author=S.%20Gupta&amp;volume=107&amp;publication_year=2021&amp;pages=635-643&amp;pmid=34916211&amp;doi=10.1136/bjophthalmol-2021-319309&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B127-vision-09-00071">
<span class="label">127.</span><cite>Lopes B.T., Ramos I.C., Salomao M.Q., Guerra F.P., Schallhorn S.C., Schallhorn J.M., Vinciguerra R., Vinciguerra P., Price F.W., Jr., Price M.O., et al.  Enhanced Tomographic Assessment to Detect Corneal Ectasia Based on Artificial Intelligence. Am. J. Ophthalmol. 2018;195:223–232. doi: 10.1016/j.ajo.2018.08.005.</cite> [<a href="https://doi.org/10.1016/j.ajo.2018.08.005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30098348/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Am.%20J.%20Ophthalmol.&amp;title=Enhanced%20Tomographic%20Assessment%20to%20Detect%20Corneal%20Ectasia%20Based%20on%20Artificial%20Intelligence&amp;author=B.T.%20Lopes&amp;author=I.C.%20Ramos&amp;author=M.Q.%20Salomao&amp;author=F.P.%20Guerra&amp;author=S.C.%20Schallhorn&amp;volume=195&amp;publication_year=2018&amp;pages=223-232&amp;pmid=30098348&amp;doi=10.1016/j.ajo.2018.08.005&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B128-vision-09-00071">
<span class="label">128.</span><cite>Mohammadpour M., Heidari Z., Hashemi H., Yaseri M., Fotouhi A. Comparison of Artificial Intelligence-Based Machine Learning Classifiers for Early Detection of Keratoconus. Eur. J. Ophthalmol. 2022;32:1352–1360. doi: 10.1177/11206721211073442.</cite> [<a href="https://doi.org/10.1177/11206721211073442" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35060771/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eur.%20J.%20Ophthalmol.&amp;title=Comparison%20of%20Artificial%20Intelligence-Based%20Machine%20Learning%20Classifiers%20for%20Early%20Detection%20of%20Keratoconus&amp;author=M.%20Mohammadpour&amp;author=Z.%20Heidari&amp;author=H.%20Hashemi&amp;author=M.%20Yaseri&amp;author=A.%20Fotouhi&amp;volume=32&amp;publication_year=2022&amp;pages=1352-1360&amp;pmid=35060771&amp;doi=10.1177/11206721211073442&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B129-vision-09-00071">
<span class="label">129.</span><cite>Shetty R., Kundu G., Narasimhan R., Khamar P., Gupta K., Singh N., Nuijits R.M.M.A., Roy A.S. Artificial Intelligence Efficiently Identifies Regional Differences in the Progression of Tomographic Parameters of Keratoconic Corneas. J. Refract. Surg. 2021;37:240–248. doi: 10.3928/1081597X-20210120-01.</cite> [<a href="https://doi.org/10.3928/1081597X-20210120-01" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34038661/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Refract.%20Surg.&amp;title=Artificial%20Intelligence%20Efficiently%20Identifies%20Regional%20Differences%20in%20the%20Progression%20of%20Tomographic%20Parameters%20of%20Keratoconic%20Corneas&amp;author=R.%20Shetty&amp;author=G.%20Kundu&amp;author=R.%20Narasimhan&amp;author=P.%20Khamar&amp;author=K.%20Gupta&amp;volume=37&amp;publication_year=2021&amp;pages=240-248&amp;pmid=34038661&amp;doi=10.3928/1081597X-20210120-01&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B130-vision-09-00071">
<span class="label">130.</span><cite>Tan Z., Chen X., Li K., Liu Y., Cao H., Li J., Jhanji V., Zou H., Liu F., Wang R., et al.  Artificial Intelligence-Based Diagnostic Model for Detecting Keratoconus Using Videos of Corneal Force Deformation. Transl. Vis. Sci. Technol. 2022;11:32. doi: 10.1167/tvst.11.9.32.</cite> [<a href="https://doi.org/10.1167/tvst.11.9.32" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9527334/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36178782/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Transl.%20Vis.%20Sci.%20Technol.&amp;title=Artificial%20Intelligence-Based%20Diagnostic%20Model%20for%20Detecting%20Keratoconus%20Using%20Videos%20of%20Corneal%20Force%20Deformation&amp;author=Z.%20Tan&amp;author=X.%20Chen&amp;author=K.%20Li&amp;author=Y.%20Liu&amp;author=H.%20Cao&amp;volume=11&amp;publication_year=2022&amp;pages=32&amp;pmid=36178782&amp;doi=10.1167/tvst.11.9.32&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B131-vision-09-00071">
<span class="label">131.</span><cite>Xu Z., Feng R., Jin X., Hu H., Ni S., Xu W., Zheng X., Wu J., Yao K. Evaluation of Artificial Intelligence Models for the Detection of Asymmetric Keratoconus Eyes Using Scheimpflug Tomography. Clin. Exp. Ophthalmol. 2022;50:714–723. doi: 10.1111/ceo.14126.</cite> [<a href="https://doi.org/10.1111/ceo.14126" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35704615/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Clin.%20Exp.%20Ophthalmol.&amp;title=Evaluation%20of%20Artificial%20Intelligence%20Models%20for%20the%20Detection%20of%20Asymmetric%20Keratoconus%20Eyes%20Using%20Scheimpflug%20Tomography&amp;author=Z.%20Xu&amp;author=R.%20Feng&amp;author=X.%20Jin&amp;author=H.%20Hu&amp;author=S.%20Ni&amp;volume=50&amp;publication_year=2022&amp;pages=714-723&amp;pmid=35704615&amp;doi=10.1111/ceo.14126&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B132-vision-09-00071">
<span class="label">132.</span><cite>Niazi S., Doroodgar F., Hashemi Nazari S., Rahimi Y., Alió Del Barrio J.L., Gatzioufas Z., Findl O., Vinciguerra P., Vinciguerra R., Moshirfar M., et al.  Refractive Surgical Approaches to Keratoconus: A Systematic Review and Network Meta-Analysis. Surv. Ophthalmol. 2024;69:779–788. doi: 10.1016/j.survophthal.2024.04.008.</cite> [<a href="https://doi.org/10.1016/j.survophthal.2024.04.008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38710236/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Surv.%20Ophthalmol.&amp;title=Refractive%20Surgical%20Approaches%20to%20Keratoconus:%20A%20Systematic%20Review%20and%20Network%20Meta-Analysis&amp;author=S.%20Niazi&amp;author=F.%20Doroodgar&amp;author=S.%20Hashemi%20Nazari&amp;author=Y.%20Rahimi&amp;author=J.L.%20Ali%C3%B3%20Del%20Barrio&amp;volume=69&amp;publication_year=2024&amp;pages=779-788&amp;pmid=38710236&amp;doi=10.1016/j.survophthal.2024.04.008&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B133-vision-09-00071">
<span class="label">133.</span><cite>Niazi S., Gatzioufas Z., Doroodgar F., Findl O., Baradaran-Rafii A., Liechty J., Moshirfar M. Keratoconus: Exploring Fundamentals and Future Perspectives—A Comprehensive Systematic Review. Ther. Adv. Ophthalmol. 2024;16:25158414241232258. doi: 10.1177/25158414241232258.</cite> [<a href="https://doi.org/10.1177/25158414241232258" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10956165/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38516169/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ther.%20Adv.%20Ophthalmol.&amp;title=Keratoconus:%20Exploring%20Fundamentals%20and%20Future%20Perspectives%E2%80%94A%20Comprehensive%20Systematic%20Review&amp;author=S.%20Niazi&amp;author=Z.%20Gatzioufas&amp;author=F.%20Doroodgar&amp;author=O.%20Findl&amp;author=A.%20Baradaran-Rafii&amp;volume=16&amp;publication_year=2024&amp;pages=25158414241232258&amp;pmid=38516169&amp;doi=10.1177/25158414241232258&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B134-vision-09-00071">
<span class="label">134.</span><cite>Lu N.J., Koppen C., Hafezi F., Ní Dhubhghaill S., Aslanides I.M., Wang Q.M., Cui L.L., Rozema J.J. Combinations of Scheimpflug Tomography, Ocular Coherence Tomography and Air-Puff Tonometry Improve the Detection of Keratoconus. Contact Lens Anterior Eye. 2023;46:101840. doi: 10.1016/j.clae.2023.101840.</cite> [<a href="https://doi.org/10.1016/j.clae.2023.101840" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37055334/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Contact%20Lens%20Anterior%20Eye&amp;title=Combinations%20of%20Scheimpflug%20Tomography,%20Ocular%20Coherence%20Tomography%20and%20Air-Puff%20Tonometry%20Improve%20the%20Detection%20of%20Keratoconus&amp;author=N.J.%20Lu&amp;author=C.%20Koppen&amp;author=F.%20Hafezi&amp;author=S.%20N%C3%AD%20Dhubhghaill&amp;author=I.M.%20Aslanides&amp;volume=46&amp;publication_year=2023&amp;pages=101840&amp;pmid=37055334&amp;doi=10.1016/j.clae.2023.101840&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B135-vision-09-00071">
<span class="label">135.</span><cite>Bodmer N.S., Christensen D.G., Bachmann L.M., Faes L., Sanak F., Iselin K., Kaufmann C., Thiel M.A., Baenninger P.B. Deep Learning Models Used in the Diagnostic Workup of Keratoconus: A Systematic Review and Exploratory Meta-Analysis. Cornea. 2024;43:916–931. doi: 10.1097/ICO.0000000000003467.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000003467" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11142647/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38300179/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Deep%20Learning%20Models%20Used%20in%20the%20Diagnostic%20Workup%20of%20Keratoconus:%20A%20Systematic%20Review%20and%20Exploratory%20Meta-Analysis&amp;author=N.S.%20Bodmer&amp;author=D.G.%20Christensen&amp;author=L.M.%20Bachmann&amp;author=L.%20Faes&amp;author=F.%20Sanak&amp;volume=43&amp;publication_year=2024&amp;pages=916-931&amp;pmid=38300179&amp;doi=10.1097/ICO.0000000000003467&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B136-vision-09-00071">
<span class="label">136.</span><cite>Gurnani B., Somani A.N., Moshirfar M., Patel B.C.  Fuchs Endothelial Dystrophy. StatPearls Publishing; Treasure Island, FL, USA: 2025.  [(accessed on 18 July 2025)].  Available online:  <a href="https://www.ncbi.nlm.nih.gov/books/NBK545248/" class="usa-link" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.ncbi.nlm.nih.gov/books/NBK545248/</a></cite> [<a href="https://pubmed.ncbi.nlm.nih.gov/31424832/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Fuchs%20Endothelial%20Dystrophy&amp;author=B.%20Gurnani&amp;author=A.N.%20Somani&amp;author=M.%20Moshirfar&amp;author=B.C.%20Patel&amp;publication_year=2025&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B137-vision-09-00071">
<span class="label">137.</span><cite>Eleiwa T., Elsawy A., Özcan E., Abou Shousha M. Automated diagnosis and staging of Fuchs’ endothelial cell corneal dystrophy using deep learning. Eye Vis. 2020;7:44. doi: 10.1186/s40662-020-00209-z.</cite> [<a href="https://doi.org/10.1186/s40662-020-00209-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7460770/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32884962/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eye%20Vis.&amp;title=Automated%20diagnosis%20and%20staging%20of%20Fuchs%E2%80%99%20endothelial%20cell%20corneal%20dystrophy%20using%20deep%20learning&amp;author=T.%20Eleiwa&amp;author=A.%20Elsawy&amp;author=E.%20%C3%96zcan&amp;author=M.%20Abou%20Shousha&amp;volume=7&amp;publication_year=2020&amp;pages=44&amp;pmid=32884962&amp;doi=10.1186/s40662-020-00209-z&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B138-vision-09-00071">
<span class="label">138.</span><cite>Elsawy A., Eleiwa T., Chase C., Ozcan E., Tolba M., Feuer W., Abdel-Mottaleb M., Abou Shousha M. Multidisease deep learning neural network for the diagnosis of corneal diseases. Am. J. Ophthalmol. 2021;226:252–261. doi: 10.1016/j.ajo.2021.01.018.</cite> [<a href="https://doi.org/10.1016/j.ajo.2021.01.018" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33529589/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Am.%20J.%20Ophthalmol.&amp;title=Multidisease%20deep%20learning%20neural%20network%20for%20the%20diagnosis%20of%20corneal%20diseases&amp;author=A.%20Elsawy&amp;author=T.%20Eleiwa&amp;author=C.%20Chase&amp;author=E.%20Ozcan&amp;author=M.%20Tolba&amp;volume=226&amp;publication_year=2021&amp;pages=252-261&amp;pmid=33529589&amp;doi=10.1016/j.ajo.2021.01.018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B139-vision-09-00071">
<span class="label">139.</span><cite>Gu H., Guo Y., Gu L., Wei A., Xie S., Ye Z., Xu J., Zhou X., Lu Y., Liu X., et al.  Deep Learning for Identifying Corneal Diseases from Ocular Surface Slit-Lamp Photographs. Sci. Rep. 2020;10:17851.  doi: 10.1038/s41598-020-75027-3.</cite> [<a href="https://doi.org/10.1038/s41598-020-75027-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7576153/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33082530/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Deep%20Learning%20for%20Identifying%20Corneal%20Diseases%20from%20Ocular%20Surface%20Slit-Lamp%20Photographs&amp;author=H.%20Gu&amp;author=Y.%20Guo&amp;author=L.%20Gu&amp;author=A.%20Wei&amp;author=S.%20Xie&amp;volume=10&amp;publication_year=2020&amp;pages=17851&amp;pmid=33082530&amp;doi=10.1038/s41598-020-75027-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B140-vision-09-00071">
<span class="label">140.</span><cite>Li W., Yang Y., Zhang K., Long E., He L., Zhang L., Zhu Y., Chen C., Liu Z., Wu X., et al.  Dense anatomical annotation of slit lamp images improves the performance of deep learning for the diagnosis of ophthalmic disorders. Nat. Biomed. Eng. 2020;4:767–777. doi: 10.1038/s41551-020-0577-y.</cite> [<a href="https://doi.org/10.1038/s41551-020-0577-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32572198/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Biomed.%20Eng.&amp;title=Dense%20anatomical%20annotation%20of%20slit%20lamp%20images%20improves%20the%20performance%20of%20deep%20learning%20for%20the%20diagnosis%20of%20ophthalmic%20disorders&amp;author=W.%20Li&amp;author=Y.%20Yang&amp;author=K.%20Zhang&amp;author=E.%20Long&amp;author=L.%20He&amp;volume=4&amp;publication_year=2020&amp;pages=767-777&amp;pmid=32572198&amp;doi=10.1038/s41551-020-0577-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B141-vision-09-00071">
<span class="label">141.</span><cite>Han S.B., Liu Y.C., Liu C., Mehta J.S. Applications of Imaging Technologies in Fuchs Endothelial Corneal Dystrophy: A Narrative Literature Review. Bioengineering. 2024;11:271.  doi: 10.3390/bioengineering11030271.</cite> [<a href="https://doi.org/10.3390/bioengineering11030271" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10968379/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38534545/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Bioengineering&amp;title=Applications%20of%20Imaging%20Technologies%20in%20Fuchs%20Endothelial%20Corneal%20Dystrophy:%20A%20Narrative%20Literature%20Review&amp;author=S.B.%20Han&amp;author=Y.C.%20Liu&amp;author=C.%20Liu&amp;author=J.S.%20Mehta&amp;volume=11&amp;publication_year=2024&amp;pages=271&amp;pmid=38534545&amp;doi=10.3390/bioengineering11030271&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B142-vision-09-00071">
<span class="label">142.</span><cite>Vigueras-Guillén J.P., van Rooij J., Engel A., Lemij H.G., van Vliet L.J., Vermeer K.A. Deep Learning for Assessing the Corneal Endothelium from Specular Microscopy Images up to 1 Year after Ultrathin-DSAEK Surgery. Transl. Vis. Sci. Technol. 2020;9:49. doi: 10.1167/tvst.9.2.49.</cite> [<a href="https://doi.org/10.1167/tvst.9.2.49" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7445361/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32884856/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Transl.%20Vis.%20Sci.%20Technol.&amp;title=Deep%20Learning%20for%20Assessing%20the%20Corneal%20Endothelium%20from%20Specular%20Microscopy%20Images%20up%20to%201%20Year%20after%20Ultrathin-DSAEK%20Surgery&amp;author=J.P.%20Vigueras-Guill%C3%A9n&amp;author=J.%20van%20Rooij&amp;author=A.%20Engel&amp;author=H.G.%20Lemij&amp;author=L.J.%20van%20Vliet&amp;volume=9&amp;publication_year=2020&amp;pages=49&amp;pmid=32884856&amp;doi=10.1167/tvst.9.2.49&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B143-vision-09-00071">
<span class="label">143.</span><cite>Laing R.A., Leibowitz H.M., Oak S.S., Chang R., Berrospi A.R., Theodore J. Endothelial mosaic in Fuchs’ dystrophy. A qualitative evaluation with the specular microscope. Arch. Ophthalmol. 1981;99:80–83. doi: 10.1001/archopht.1981.03930010082007.</cite> [<a href="https://doi.org/10.1001/archopht.1981.03930010082007" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/6970032/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Arch.%20Ophthalmol.&amp;title=Endothelial%20mosaic%20in%20Fuchs%E2%80%99%20dystrophy.%20A%20qualitative%20evaluation%20with%20the%20specular%20microscope&amp;author=R.A.%20Laing&amp;author=H.M.%20Leibowitz&amp;author=S.S.%20Oak&amp;author=R.%20Chang&amp;author=A.R.%20Berrospi&amp;volume=99&amp;publication_year=1981&amp;pages=80-83&amp;pmid=6970032&amp;doi=10.1001/archopht.1981.03930010082007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B144-vision-09-00071">
<span class="label">144.</span><cite>Ong Tone S., Jurkunas U. Imaging the Corneal Endothelium in Fuchs Corneal Endothelial Dystrophy. Semin. Ophthalmol. 2019;34:340–346. doi: 10.1080/08820538.2019.1632355.</cite> [<a href="https://doi.org/10.1080/08820538.2019.1632355" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6629500/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31215821/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Semin.%20Ophthalmol.&amp;title=Imaging%20the%20Corneal%20Endothelium%20in%20Fuchs%20Corneal%20Endothelial%20Dystrophy&amp;author=S.%20Ong%20Tone&amp;author=U.%20Jurkunas&amp;volume=34&amp;publication_year=2019&amp;pages=340-346&amp;pmid=31215821&amp;doi=10.1080/08820538.2019.1632355&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B145-vision-09-00071">
<span class="label">145.</span><cite>Foo V.H.X., Lim G.Y.S., Liu Y.C., Ong H.S., Wong E., Chan S., Wong J., Mehta J.S., Ting D.S.W., Ang M. Deep learning for detection of Fuchs endothelial dystrophy from widefield specular microscopy imaging: A pilot study. Eye Vis. 2024;11:11. doi: 10.1186/s40662-024-00378-1.</cite> [<a href="https://doi.org/10.1186/s40662-024-00378-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10946096/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38494521/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eye%20Vis.&amp;title=Deep%20learning%20for%20detection%20of%20Fuchs%20endothelial%20dystrophy%20from%20widefield%20specular%20microscopy%20imaging:%20A%20pilot%20study&amp;author=V.H.X.%20Foo&amp;author=G.Y.S.%20Lim&amp;author=Y.C.%20Liu&amp;author=H.S.%20Ong&amp;author=E.%20Wong&amp;volume=11&amp;publication_year=2024&amp;pages=11&amp;pmid=38494521&amp;doi=10.1186/s40662-024-00378-1&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B146-vision-09-00071">
<span class="label">146.</span><cite>Belda-Para C., Velarde-Rodríguez G., Marichal-Hernández J.G., Velasco-Ocaña M., Trujillo-Sevilla J.M., Alejandre-Alba N., Rodríguez-Ramos J.M. Fuchs’ Endothelial Corneal Dystrophy evaluation using a high-resolution wavefront sensor. Sci. Rep. 2024;14:20369.  doi: 10.1038/s41598-024-71480-6.</cite> [<a href="https://doi.org/10.1038/s41598-024-71480-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11368916/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39223223/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Fuchs%E2%80%99%20Endothelial%20Corneal%20Dystrophy%20evaluation%20using%20a%20high-resolution%20wavefront%20sensor&amp;author=C.%20Belda-Para&amp;author=G.%20Velarde-Rodr%C3%ADguez&amp;author=J.G.%20Marichal-Hern%C3%A1ndez&amp;author=M.%20Velasco-Oca%C3%B1a&amp;author=J.M.%20Trujillo-Sevilla&amp;volume=14&amp;publication_year=2024&amp;pages=20369&amp;pmid=39223223&amp;doi=10.1038/s41598-024-71480-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B147-vision-09-00071">
<span class="label">147.</span><cite>Bitton K., Zéboulon P., Ghazal W., Rizk M., Elahi S., Gatinel D. Deep Learning Model for the Detection of Corneal Edema Before Descemet Membrane Endothelial Keratoplasty on Optical Coherence Tomography Images. Transl. Vis. Sci. Technol. 2022;11:19. doi: 10.1167/tvst.11.12.19.</cite> [<a href="https://doi.org/10.1167/tvst.11.12.19" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9807180/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36583911/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Transl.%20Vis.%20Sci.%20Technol.&amp;title=Deep%20Learning%20Model%20for%20the%20Detection%20of%20Corneal%20Edema%20Before%20Descemet%20Membrane%20Endothelial%20Keratoplasty%20on%20Optical%20Coherence%20Tomography%20Images&amp;author=K.%20Bitton&amp;author=P.%20Z%C3%A9boulon&amp;author=W.%20Ghazal&amp;author=M.%20Rizk&amp;author=S.%20Elahi&amp;volume=11&amp;publication_year=2022&amp;pages=19&amp;pmid=36583911&amp;doi=10.1167/tvst.11.12.19&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B148-vision-09-00071">
<span class="label">148.</span><cite>Prada A.M., Quintero F., Mendoza K., Galvis V., Tello A., Romero L.A., Marrugo A.G. Assessing Fuchs Corneal Endothelial Dystrophy Using Artificial Intelligence-Derived Morphometric Parameters from Specular Microscopy Images. Cornea. 2024;43:1080–1087. doi: 10.1097/ICO.0000000000003460.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000003460" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11296282/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38334475/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Assessing%20Fuchs%20Corneal%20Endothelial%20Dystrophy%20Using%20Artificial%20Intelligence-Derived%20Morphometric%20Parameters%20from%20Specular%20Microscopy%20Images&amp;author=A.M.%20Prada&amp;author=F.%20Quintero&amp;author=K.%20Mendoza&amp;author=V.%20Galvis&amp;author=A.%20Tello&amp;volume=43&amp;publication_year=2024&amp;pages=1080-1087&amp;pmid=38334475&amp;doi=10.1097/ICO.0000000000003460&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B149-vision-09-00071">
<span class="label">149.</span><cite>Liu S., Kandakji L., Stupnicki A., Sumodhee D., Leucci M.T., Hau S., Balal S., Okonkwo A., Moghul I., Kanda S.P., et al.  Current Applications of Artificial Intelligence for Fuchs Endothelial Corneal Dystrophy: A Systematic Review. Transl. Vis. Sci. Technol. 2025;14:12. doi: 10.1167/tvst.14.6.12.</cite> [<a href="https://doi.org/10.1167/tvst.14.6.12" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC12155719/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40478592/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Transl.%20Vis.%20Sci.%20Technol.&amp;title=Current%20Applications%20of%20Artificial%20Intelligence%20for%20Fuchs%20Endothelial%20Corneal%20Dystrophy:%20A%20Systematic%20Review&amp;author=S.%20Liu&amp;author=L.%20Kandakji&amp;author=A.%20Stupnicki&amp;author=D.%20Sumodhee&amp;author=M.T.%20Leucci&amp;volume=14&amp;publication_year=2025&amp;pages=12&amp;pmid=40478592&amp;doi=10.1167/tvst.14.6.12&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B150-vision-09-00071">
<span class="label">150.</span><cite>Yang H.K., Che S.A., Hyon J.Y., Han S.B. Integration of Artificial Intelligence into the Approach for Diagnosis and Monitoring of Dry Eye Disease. Diagnostics. 2022;12:3167.  doi: 10.3390/diagnostics12123167.</cite> [<a href="https://doi.org/10.3390/diagnostics12123167" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9777416/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36553174/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diagnostics&amp;title=Integration%20of%20Artificial%20Intelligence%20into%20the%20Approach%20for%20Diagnosis%20and%20Monitoring%20of%20Dry%20Eye%20Disease&amp;author=H.K.%20Yang&amp;author=S.A.%20Che&amp;author=J.Y.%20Hyon&amp;author=S.B.%20Han&amp;volume=12&amp;publication_year=2022&amp;pages=3167&amp;pmid=36553174&amp;doi=10.3390/diagnostics12123167&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B151-vision-09-00071">
<span class="label">151.</span><cite>Pur D.R., Krance S.H., Pucchio A., Miranda R.N., Felfeli T. Current Uses of Artificial Intelligence in the Analysis of Biofluid Markers Involved in Corneal and Ocular Surface Diseases: A Systematic Review. Eye. 2023;37:2007–2019. doi: 10.1038/s41433-022-02307-9.</cite> [<a href="https://doi.org/10.1038/s41433-022-02307-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10333344/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36380089/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eye&amp;title=Current%20Uses%20of%20Artificial%20Intelligence%20in%20the%20Analysis%20of%20Biofluid%20Markers%20Involved%20in%20Corneal%20and%20Ocular%20Surface%20Diseases:%20A%20Systematic%20Review&amp;author=D.R.%20Pur&amp;author=S.H.%20Krance&amp;author=A.%20Pucchio&amp;author=R.N.%20Miranda&amp;author=T.%20Felfeli&amp;volume=37&amp;publication_year=2023&amp;pages=2007-2019&amp;pmid=36380089&amp;doi=10.1038/s41433-022-02307-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B152-vision-09-00071">
<span class="label">152.</span><cite>Mohammadi S.F., Farrokhpour H., Soltani G., Latifi G. Keratoneuropathy. Ocul. Surf. 2023;29:386–387. doi: 10.1016/j.jtos.2023.06.009.</cite> [<a href="https://doi.org/10.1016/j.jtos.2023.06.009" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37331694/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ocul.%20Surf.&amp;title=Keratoneuropathy&amp;author=S.F.%20Mohammadi&amp;author=H.%20Farrokhpour&amp;author=G.%20Soltani&amp;author=G.%20Latifi&amp;volume=29&amp;publication_year=2023&amp;pages=386-387&amp;pmid=37331694&amp;doi=10.1016/j.jtos.2023.06.009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B153-vision-09-00071">
<span class="label">153.</span><cite>Storås A.M., Strümke I., Riegler M.A., Grauslund J., Hammer H.L., Yazidi A., Halvorsen P., Gundersen K.G., Utheim T.P., Jackson C.J. Artificial Intelligence in Dry Eye Disease. Ocul. Surf. 2022;23:74–86. doi: 10.1016/j.jtos.2021.11.004.</cite> [<a href="https://doi.org/10.1016/j.jtos.2021.11.004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34843999/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ocul.%20Surf.&amp;title=Artificial%20Intelligence%20in%20Dry%20Eye%20Disease&amp;author=A.M.%20Stor%C3%A5s&amp;author=I.%20Str%C3%BCmke&amp;author=M.A.%20Riegler&amp;author=J.%20Grauslund&amp;author=H.L.%20Hammer&amp;volume=23&amp;publication_year=2022&amp;pages=74-86&amp;pmid=34843999&amp;doi=10.1016/j.jtos.2021.11.004&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B154-vision-09-00071">
<span class="label">154.</span><cite>Persiya J., Sasithradevi A. Thermal Mapping the Eye: A Critical Review of Advances in Infrared Imaging for Disease Detection. J. Therm. Biol. 2024;121:103867.  doi: 10.1016/j.jtherbio.2024.103867.</cite> [<a href="https://doi.org/10.1016/j.jtherbio.2024.103867" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38744026/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Therm.%20Biol.&amp;title=Thermal%20Mapping%20the%20Eye:%20A%20Critical%20Review%20of%20Advances%20in%20Infrared%20Imaging%20for%20Disease%20Detection&amp;author=J.%20Persiya&amp;author=A.%20Sasithradevi&amp;volume=121&amp;publication_year=2024&amp;pages=103867&amp;pmid=38744026&amp;doi=10.1016/j.jtherbio.2024.103867&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B155-vision-09-00071">
<span class="label">155.</span><cite>Cartes C., López D., Salinas D., Segovia C., Ahumada C., Pérez N., Valenzuela F., Lanza N., López Solís R.O., Perez V.L., et al.  Dry Eye Is Matched by Increased Intrasubject Variability in Tear Osmolarity as Confirmed by Machine Learning Approach. Arch. Soc. Esp. Oftalmol. 2019;94:337–342. doi: 10.1016/j.oftal.2019.03.007.</cite> [<a href="https://doi.org/10.1016/j.oftal.2019.03.007" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31122680/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Arch.%20Soc.%20Esp.%20Oftalmol.&amp;title=Dry%20Eye%20Is%20Matched%20by%20Increased%20Intrasubject%20Variability%20in%20Tear%20Osmolarity%20as%20Confirmed%20by%20Machine%20Learning%20Approach&amp;author=C.%20Cartes&amp;author=D.%20L%C3%B3pez&amp;author=D.%20Salinas&amp;author=C.%20Segovia&amp;author=C.%20Ahumada&amp;volume=94&amp;publication_year=2019&amp;pages=337-342&amp;pmid=31122680&amp;doi=10.1016/j.oftal.2019.03.007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B156-vision-09-00071">
<span class="label">156.</span><cite>Grus F.H., Augustin A.J. Analysis of Tear Protein Patterns by a Neural Network as a Diagnostical Tool for the Detection of Dry Eyes. Electrophoresis. 1999;20:875–880. doi: 10.1002/(SICI)1522-2683(19990101)20:4/5&amp;#x0003c;875::AID-ELPS875&amp;#x0003e;3.0.CO;2-V.</cite> [<a href="https://doi.org/10.1002/(SICI)1522-2683(19990101)20:4/5&amp;#x0003c;875::AID-ELPS875&amp;#x0003e;3.0.CO;2-V" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/10344262/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Electrophoresis&amp;title=Analysis%20of%20Tear%20Protein%20Patterns%20by%20a%20Neural%20Network%20as%20a%20Diagnostical%20Tool%20for%20the%20Detection%20of%20Dry%20Eyes&amp;author=F.H.%20Grus&amp;author=A.J.%20Augustin&amp;volume=20&amp;publication_year=1999&amp;pages=875-880&amp;pmid=10344262&amp;doi=10.1002/(SICI)1522-2683(19990101)20:4/5&amp;#x0003c;875::AID-ELPS875&amp;#x0003e;3.0.CO;2-V&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B157-vision-09-00071">
<span class="label">157.</span><cite>Grus F.H., Podust V.N., Bruns K., Lackner K., Fu S., Dalmasso E.A., Wirthlin A., Pfeiffer N. SELDI-TOF-MS ProteinChip Array Profiling of Tears from Patients with Dry Eye. Investig. Ophthalmol. Vis. Sci. 2005;46:863–876. doi: 10.1167/iovs.04-0448.</cite> [<a href="https://doi.org/10.1167/iovs.04-0448" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/15728542/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=SELDI-TOF-MS%20ProteinChip%20Array%20Profiling%20of%20Tears%20from%20Patients%20with%20Dry%20Eye&amp;author=F.H.%20Grus&amp;author=V.N.%20Podust&amp;author=K.%20Bruns&amp;author=K.%20Lackner&amp;author=S.%20Fu&amp;volume=46&amp;publication_year=2005&amp;pages=863-876&amp;pmid=15728542&amp;doi=10.1167/iovs.04-0448&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B158-vision-09-00071">
<span class="label">158.</span><cite>Yedidya T., Hartley R., Guillon J.-P., Kanagasingam Y., editors. Medical Image Computing and Computer-Assisted Intervention–MICCAI 2007. Volume 10. Springer; Berlin/Heidelberg, Germany: 2007. Automatic Dry Eye Detection.</cite> [<a href="https://doi.org/10.1007/978-3-540-75757-3_96" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/18051131/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Medical%20Image%20Computing%20and%20Computer-Assisted%20Intervention%E2%80%93MICCAI%202007&amp;publication_year=2007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B159-vision-09-00071">
<span class="label">159.</span><cite>Su T.-Y., Liu Z.-Y., Chen D.-Y. Tear Film Break-Up Time Measurement Using Deep Convolutional Neural Networks for Screening Dry Eye Disease. IEEE Sens. J. 2018;18:6857–6862. doi: 10.1109/JSEN.2018.2850940.</cite> [<a href="https://doi.org/10.1109/JSEN.2018.2850940" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Sens.%20J.&amp;title=Tear%20Film%20Break-Up%20Time%20Measurement%20Using%20Deep%20Convolutional%20Neural%20Networks%20for%20Screening%20Dry%20Eye%20Disease&amp;author=T.-Y.%20Su&amp;author=Z.-Y.%20Liu&amp;author=D.-Y.%20Chen&amp;volume=18&amp;publication_year=2018&amp;pages=6857-6862&amp;doi=10.1109/JSEN.2018.2850940&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B160-vision-09-00071">
<span class="label">160.</span><cite>Vyas A.H., Mehta M.A., Kotecha K., Pandya S., Alazab M., Gadekallu T.R. Tear Film Breakup Time-Based Dry Eye Disease Detection Using Convolutional Neural Network. Neural Comput. Appl. 2024;36:143–161. doi: 10.1007/s00521-022-07652-0.</cite> [<a href="https://doi.org/10.1007/s00521-022-07652-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neural%20Comput.%20Appl.&amp;title=Tear%20Film%20Breakup%20Time-Based%20Dry%20Eye%20Disease%20Detection%20Using%20Convolutional%20Neural%20Network&amp;author=A.H.%20Vyas&amp;author=M.A.%20Mehta&amp;author=K.%20Kotecha&amp;author=S.%20Pandya&amp;author=M.%20Alazab&amp;volume=36&amp;publication_year=2024&amp;pages=143-161&amp;doi=10.1007/s00521-022-07652-0&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B161-vision-09-00071">
<span class="label">161.</span><cite>Su T.-Y., Ting P.-J., Chang S.-W., Chen D.-Y. Superficial Punctate Keratitis Grading for Dry Eye Screening Using Deep Convolutional Neural Networks. IEEE Sens. J. 2019;20:1672–1678. doi: 10.1109/JSEN.2019.2948576.</cite> [<a href="https://doi.org/10.1109/JSEN.2019.2948576" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Sens.%20J.&amp;title=Superficial%20Punctate%20Keratitis%20Grading%20for%20Dry%20Eye%20Screening%20Using%20Deep%20Convolutional%20Neural%20Networks&amp;author=T.-Y.%20Su&amp;author=P.-J.%20Ting&amp;author=S.-W.%20Chang&amp;author=D.-Y.%20Chen&amp;volume=20&amp;publication_year=2019&amp;pages=1672-1678&amp;doi=10.1109/JSEN.2019.2948576&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B162-vision-09-00071">
<span class="label">162.</span><cite>Arita R., Yabusaki K., Yamauchi T., Ichihashi T., Morishige N. Diagnosis of Dry Eye Subtype by Artificial Intelligence Software Based on the Interferometric Fringe Pattern of the Tear Film Obtained with the Kowa DR-1α Instrument. Investig. Ophthalmol. Vis. Sci. 2018;59:1965.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Diagnosis%20of%20Dry%20Eye%20Subtype%20by%20Artificial%20Intelligence%20Software%20Based%20on%20the%20Interferometric%20Fringe%20Pattern%20of%20the%20Tear%20Film%20Obtained%20with%20the%20Kowa%20DR-1%CE%B1%20Instrument&amp;author=R.%20Arita&amp;author=K.%20Yabusaki&amp;author=T.%20Yamauchi&amp;author=T.%20Ichihashi&amp;author=N.%20Morishige&amp;volume=59&amp;publication_year=2018&amp;pages=1965&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B163-vision-09-00071">
<span class="label">163.</span><cite>Yabusaki K., Arita R., Yamauchi T. Automated Classification of Dry Eye Type Analyzing Interference Fringe Color Images of Tear Film Using Machine Learning Techniques. Model. Artif. Intell. Ophthalmol. 2019;2:28–35. doi: 10.35119/maio.v2i3.90.</cite> [<a href="https://doi.org/10.35119/maio.v2i3.90" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Model.%20Artif.%20Intell.%20Ophthalmol.&amp;title=Automated%20Classification%20of%20Dry%20Eye%20Type%20Analyzing%20Interference%20Fringe%20Color%20Images%20of%20Tear%20Film%20Using%20Machine%20Learning%20Techniques&amp;author=K.%20Yabusaki&amp;author=R.%20Arita&amp;author=T.%20Yamauchi&amp;volume=2&amp;publication_year=2019&amp;pages=28-35&amp;doi=10.35119/maio.v2i3.90&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B164-vision-09-00071">
<span class="label">164.</span><cite>Zheng Q., Wang L., Wen H., Ren Y., Huang S., Bai F., Li N., Craig J.P., Tong L., Chen W. Impact of Incomplete Blinking Analyzed Using a Deep Learning Model with the Keratograph 5M in Dry Eye Disease. Transl. Vis. Sci. Technol. 2022;11:38. doi: 10.1167/tvst.11.3.38.</cite> [<a href="https://doi.org/10.1167/tvst.11.3.38" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8976934/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35357395/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Transl.%20Vis.%20Sci.%20Technol.&amp;title=Impact%20of%20Incomplete%20Blinking%20Analyzed%20Using%20a%20Deep%20Learning%20Model%20with%20the%20Keratograph%205M%20in%20Dry%20Eye%20Disease&amp;author=Q.%20Zheng&amp;author=L.%20Wang&amp;author=H.%20Wen&amp;author=Y.%20Ren&amp;author=S.%20Huang&amp;volume=11&amp;publication_year=2022&amp;pages=38&amp;pmid=35357395&amp;doi=10.1167/tvst.11.3.38&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B165-vision-09-00071">
<span class="label">165.</span><cite>Zheng Q., Zhang X., Zhang J., Bai F., Huang S., Pu J., Chen W., Wang L. A Texture-Aware U-Net for Identifying Incomplete Blinking from Eye Videography. Biomed. Signal Process. Control. 2022;75:103630.  doi: 10.1016/j.bspc.2022.103630.</cite> [<a href="https://doi.org/10.1016/j.bspc.2022.103630" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9484405/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36127930/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Biomed.%20Signal%20Process.%20Control&amp;title=A%20Texture-Aware%20U-Net%20for%20Identifying%20Incomplete%20Blinking%20from%20Eye%20Videography&amp;author=Q.%20Zheng&amp;author=X.%20Zhang&amp;author=J.%20Zhang&amp;author=F.%20Bai&amp;author=S.%20Huang&amp;volume=75&amp;publication_year=2022&amp;pages=103630&amp;pmid=36127930&amp;doi=10.1016/j.bspc.2022.103630&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B166-vision-09-00071">
<span class="label">166.</span><cite>Zhang Z.Z., Kuang R.F., Wei Z.Y., Wang L.Y., Su G.Y., Ou Z.H., Liang Q.F. Detection of the Spontaneous Blinking Pattern of Dry Eye Patients Using the Machine Learning Method. Zhonghua Yan Ke Za Zhi. 2022;58:120–129. doi: 10.3760/cma.j.cn112142-20211110-00537.</cite> [<a href="https://doi.org/10.3760/cma.j.cn112142-20211110-00537" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35144352/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Zhonghua%20Yan%20Ke%20Za%20Zhi&amp;title=Detection%20of%20the%20Spontaneous%20Blinking%20Pattern%20of%20Dry%20Eye%20Patients%20Using%20the%20Machine%20Learning%20Method&amp;author=Z.Z.%20Zhang&amp;author=R.F.%20Kuang&amp;author=Z.Y.%20Wei&amp;author=L.Y.%20Wang&amp;author=G.Y.%20Su&amp;volume=58&amp;publication_year=2022&amp;pages=120-129&amp;pmid=35144352&amp;doi=10.3760/cma.j.cn112142-20211110-00537&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B167-vision-09-00071">
<span class="label">167.</span><cite>Edorh N.A., El Maftouhi A., Djerada Z., Arndt C., Denoyer A. New Model to Better Diagnose Dry Eye Disease Integrating OCT Corneal Epithelial Mapping. Br. J. Ophthalmol. 2022;106:1488–1495. doi: 10.1136/bjophthalmol-2021-318826.</cite> [<a href="https://doi.org/10.1136/bjophthalmol-2021-318826" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34031042/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Br.%20J.%20Ophthalmol.&amp;title=New%20Model%20to%20Better%20Diagnose%20Dry%20Eye%20Disease%20Integrating%20OCT%20Corneal%20Epithelial%20Mapping&amp;author=N.A.%20Edorh&amp;author=A.%20El%20Maftouhi&amp;author=Z.%20Djerada&amp;author=C.%20Arndt&amp;author=A.%20Denoyer&amp;volume=106&amp;publication_year=2022&amp;pages=1488-1495&amp;pmid=34031042&amp;doi=10.1136/bjophthalmol-2021-318826&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B168-vision-09-00071">
<span class="label">168.</span><cite>Wang M.T.M., Xue A.L., Craig J.P. Screening Utility of a Rapid Non-Invasive Dry Eye Assessment Algorithm. Cont. Lens Anterior Eye. 2019;42:497–501. doi: 10.1016/j.clae.2018.11.009.</cite> [<a href="https://doi.org/10.1016/j.clae.2018.11.009" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30473321/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cont.%20Lens%20Anterior%20Eye&amp;title=Screening%20Utility%20of%20a%20Rapid%20Non-Invasive%20Dry%20Eye%20Assessment%20Algorithm&amp;author=M.T.M.%20Wang&amp;author=A.L.%20Xue&amp;author=J.P.%20Craig&amp;volume=42&amp;publication_year=2019&amp;pages=497-501&amp;pmid=30473321&amp;doi=10.1016/j.clae.2018.11.009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B169-vision-09-00071">
<span class="label">169.</span><cite>Heidari Z., Hashemi H., Sotude D., Ebrahimi-Besheli K., Khabazkhoob M., Soleimani M., Djalilian A.R., Yousefi S. Applications of Artificial Intelligence in Diagnosis of Dry Eye Disease: A Systematic Review and Meta-Analysis. Cornea. 2024;43:1310–1318. doi: 10.1097/ICO.0000000000003626.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000003626" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38984532/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Applications%20of%20Artificial%20Intelligence%20in%20Diagnosis%20of%20Dry%20Eye%20Disease:%20A%20Systematic%20Review%20and%20Meta-Analysis&amp;author=Z.%20Heidari&amp;author=H.%20Hashemi&amp;author=D.%20Sotude&amp;author=K.%20Ebrahimi-Besheli&amp;author=M.%20Khabazkhoob&amp;volume=43&amp;publication_year=2024&amp;pages=1310-1318&amp;pmid=38984532&amp;doi=10.1097/ICO.0000000000003626&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B170-vision-09-00071">
<span class="label">170.</span><cite>Aranha dos Santos V., Schmetterer L., Gröschl M., Garhofer G., Schmidl D., Kucera M., Unterhuber A., Hermand J.-P., Werkmeister R.M. In Vivo Tear Film Thickness Measurement and Tear Film Dynamics Visualization Using Spectral Domain Optical Coherence Tomography. Opt. Express. 2015;23:21043–21063. doi: 10.1364/OE.23.021043.</cite> [<a href="https://doi.org/10.1364/OE.23.021043" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26367956/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Opt.%20Express&amp;title=In%20Vivo%20Tear%20Film%20Thickness%20Measurement%20and%20Tear%20Film%20Dynamics%20Visualization%20Using%20Spectral%20Domain%20Optical%20Coherence%20Tomography&amp;author=V.%20Aranha%20dos%20Santos&amp;author=L.%20Schmetterer&amp;author=M.%20Gr%C3%B6schl&amp;author=G.%20Garhofer&amp;author=D.%20Schmidl&amp;volume=23&amp;publication_year=2015&amp;pages=21043-21063&amp;pmid=26367956&amp;doi=10.1364/OE.23.021043&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B171-vision-09-00071">
<span class="label">171.</span><cite>Deng X., Tian L., Liu Z., Zhou Y., Jie Y. A Deep Learning Approach for the Quantification of Lower Tear Meniscus Height. Biomed. Signal Process. Control. 2021;68:102655.  doi: 10.1016/j.bspc.2021.102655.</cite> [<a href="https://doi.org/10.1016/j.bspc.2021.102655" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Biomed.%20Signal%20Process.%20Control&amp;title=A%20Deep%20Learning%20Approach%20for%20the%20Quantification%20of%20Lower%20Tear%20Meniscus%20Height&amp;author=X.%20Deng&amp;author=L.%20Tian&amp;author=Z.%20Liu&amp;author=Y.%20Zhou&amp;author=Y.%20Jie&amp;volume=68&amp;publication_year=2021&amp;pages=102655&amp;doi=10.1016/j.bspc.2021.102655&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B172-vision-09-00071">
<span class="label">172.</span><cite>Stegmann H., Aranha dos Santos V., Messner A., Unterhuber A., Schmidl D., Garhöfer G., Schmetterer L., Werkmeister R.M. Automatic Assessment of Tear Film and Tear Meniscus Parameters in Healthy Subjects Using Ultrahigh-Resolution Optical Coherence Tomography. Biomed. Opt. Express. 2019;10:2744–2756. doi: 10.1364/BOE.10.002744.</cite> [<a href="https://doi.org/10.1364/BOE.10.002744" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6583345/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31259048/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Biomed.%20Opt.%20Express&amp;title=Automatic%20Assessment%20of%20Tear%20Film%20and%20Tear%20Meniscus%20Parameters%20in%20Healthy%20Subjects%20Using%20Ultrahigh-Resolution%20Optical%20Coherence%20Tomography&amp;author=H.%20Stegmann&amp;author=V.%20Aranha%20dos%20Santos&amp;author=A.%20Messner&amp;author=A.%20Unterhuber&amp;author=D.%20Schmidl&amp;volume=10&amp;publication_year=2019&amp;pages=2744-2756&amp;pmid=31259048&amp;doi=10.1364/BOE.10.002744&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B173-vision-09-00071">
<span class="label">173.</span><cite>Stegmann H., Werkmeister R.M., Pfister M., Garhöfer G., Schmetterer L., dos Santos V.A. Deep Learning Segmentation for Optical Coherence Tomography Measurements of the Lower Tear Meniscus. Biomed. Opt. Express. 2020;11:1539–1554. doi: 10.1364/BOE.386228.</cite> [<a href="https://doi.org/10.1364/BOE.386228" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7075621/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32206427/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Biomed.%20Opt.%20Express&amp;title=Deep%20Learning%20Segmentation%20for%20Optical%20Coherence%20Tomography%20Measurements%20of%20the%20Lower%20Tear%20Meniscus&amp;author=H.%20Stegmann&amp;author=R.M.%20Werkmeister&amp;author=M.%20Pfister&amp;author=G.%20Garh%C3%B6fer&amp;author=L.%20Schmetterer&amp;volume=11&amp;publication_year=2020&amp;pages=1539-1554&amp;pmid=32206427&amp;doi=10.1364/BOE.386228&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B174-vision-09-00071">
<span class="label">174.</span><cite>Egrilmez S., Yildirim-Theveny Ş. Treatment-Resistant Bacterial Keratitis: Challenges and Solutions. Clin. Ophthalmol. 2020;14:287–297. doi: 10.2147/OPTH.S181997.</cite> [<a href="https://doi.org/10.2147/OPTH.S181997" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6996220/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32099313/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Clin.%20Ophthalmol.&amp;title=Treatment-Resistant%20Bacterial%20Keratitis:%20Challenges%20and%20Solutions&amp;author=S.%20Egrilmez&amp;author=%C5%9E.%20Yildirim-Theveny&amp;volume=14&amp;publication_year=2020&amp;pages=287-297&amp;pmid=32099313&amp;doi=10.2147/OPTH.S181997&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B175-vision-09-00071">
<span class="label">175.</span><cite>Farrokhpour H., Soleimani M., Cheraqpour K., Masoumi A., Tabatabaei S.A., Shahriari M., Hobaby S., Baharnoor S.M., Chaudhry A., Djalilian A.R. A Case Series of Infectious Keratitis After Corneal Cross-Linking. J. Refract. Surg. 2023;39:564–572. doi: 10.3928/1081597X-20230717-03.</cite> [<a href="https://doi.org/10.3928/1081597X-20230717-03" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37578174/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Refract.%20Surg.&amp;title=A%20Case%20Series%20of%20Infectious%20Keratitis%20After%20Corneal%20Cross-Linking&amp;author=H.%20Farrokhpour&amp;author=M.%20Soleimani&amp;author=K.%20Cheraqpour&amp;author=A.%20Masoumi&amp;author=S.A.%20Tabatabaei&amp;volume=39&amp;publication_year=2023&amp;pages=564-572&amp;pmid=37578174&amp;doi=10.3928/1081597X-20230717-03&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B176-vision-09-00071">
<span class="label">176.</span><cite>Soleimani M., Keykhaei M., Tabatabaei S.A., Shahriari M., Farrokhpour H., Ramezani B., Cheraqpour K. Post Photorefractive Keratectomy (PRK) Infectious Keratitis; Six-Year Experience of a Tertiary Eye Hospital. Eye. 2023;37:631–637. doi: 10.1038/s41433-022-02009-2.</cite> [<a href="https://doi.org/10.1038/s41433-022-02009-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9998852/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35273348/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eye&amp;title=Post%20Photorefractive%20Keratectomy%20(PRK)%20Infectious%20Keratitis;%20Six-Year%20Experience%20of%20a%20Tertiary%20Eye%20Hospital&amp;author=M.%20Soleimani&amp;author=M.%20Keykhaei&amp;author=S.A.%20Tabatabaei&amp;author=M.%20Shahriari&amp;author=H.%20Farrokhpour&amp;volume=37&amp;publication_year=2023&amp;pages=631-637&amp;pmid=35273348&amp;doi=10.1038/s41433-022-02009-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B177-vision-09-00071">
<span class="label">177.</span><cite>Parikh P.C., Valikodath N.G., Estopinal C.B., Shtein R.M., Sugar A., Niziol L.M., Woodward M.A. Precision of Epithelial Defect Measurements. Cornea. 2017;36:419–424. doi: 10.1097/ICO.0000000000001148.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000001148" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5517027/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28129296/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Precision%20of%20Epithelial%20Defect%20Measurements&amp;author=P.C.%20Parikh&amp;author=N.G.%20Valikodath&amp;author=C.B.%20Estopinal&amp;author=R.M.%20Shtein&amp;author=A.%20Sugar&amp;volume=36&amp;publication_year=2017&amp;pages=419-424&amp;pmid=28129296&amp;doi=10.1097/ICO.0000000000001148&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B178-vision-09-00071">
<span class="label">178.</span><cite>Upadhyay M.P., Srinivasan M., Whitcher J.P. Diagnosing and Managing Microbial Keratitis. Community Eye Health. 2015;28:3–6.</cite> [<a href="/articles/PMC4579990/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26435583/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Community%20Eye%20Health&amp;title=Diagnosing%20and%20Managing%20Microbial%20Keratitis&amp;author=M.P.%20Upadhyay&amp;author=M.%20Srinivasan&amp;author=J.P.%20Whitcher&amp;volume=28&amp;publication_year=2015&amp;pages=3-6&amp;pmid=26435583&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B179-vision-09-00071">
<span class="label">179.</span><cite>Avunduk A.M., Varnell E.D., Kaufman H.E. The Effect of Roscovitine on Herpetic Keratitis. Exp. Eye Res. 2003;76:679–683. doi: 10.1016/S0014-4835(03)00056-3.</cite> [<a href="https://doi.org/10.1016/S0014-4835(03)00056-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/12742350/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Exp.%20Eye%20Res.&amp;title=The%20Effect%20of%20Roscovitine%20on%20Herpetic%20Keratitis&amp;author=A.M.%20Avunduk&amp;author=E.D.%20Varnell&amp;author=H.E.%20Kaufman&amp;volume=76&amp;publication_year=2003&amp;pages=679-683&amp;pmid=12742350&amp;doi=10.1016/S0014-4835(03)00056-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B180-vision-09-00071">
<span class="label">180.</span><cite>Moshirfar M., Hopping G.C., Vaidyanathan U., Liu H., Somani A.N., Ronquillo Y.C., Hoopes P.C. Biological Staining and Culturing in Infectious Keratitis: Controversy in Clinical Utility. Med. Hypothesis Discov. Innov. Ophthalmol. 2019;8:145–151.</cite> [<a href="/articles/PMC6778464/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31598516/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med.%20Hypothesis%20Discov.%20Innov.%20Ophthalmol.&amp;title=Biological%20Staining%20and%20Culturing%20in%20Infectious%20Keratitis:%20Controversy%20in%20Clinical%20Utility&amp;author=M.%20Moshirfar&amp;author=G.C.%20Hopping&amp;author=U.%20Vaidyanathan&amp;author=H.%20Liu&amp;author=A.N.%20Somani&amp;volume=8&amp;publication_year=2019&amp;pages=145-151&amp;pmid=31598516&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B181-vision-09-00071">
<span class="label">181.</span><cite>Sarayar R., Lestari Y.D., Setio A.A.A., Sitompul R. Accuracy of Artificial Intelligence Model for Infectious Keratitis Classification: A Systematic Review and Meta-Analysis. Front. Public Health. 2023;11:1239231.  doi: 10.3389/fpubh.2023.1239231.</cite> [<a href="https://doi.org/10.3389/fpubh.2023.1239231" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10704127/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38074720/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Public%20Health&amp;title=Accuracy%20of%20Artificial%20Intelligence%20Model%20for%20Infectious%20Keratitis%20Classification:%20A%20Systematic%20Review%20and%20Meta-Analysis&amp;author=R.%20Sarayar&amp;author=Y.D.%20Lestari&amp;author=A.A.A.%20Setio&amp;author=R.%20Sitompul&amp;volume=11&amp;publication_year=2023&amp;pages=1239231&amp;pmid=38074720&amp;doi=10.3389/fpubh.2023.1239231&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B182-vision-09-00071">
<span class="label">182.</span><cite>Ong Z.Z., Sadek Y., Qureshi R., Liu S.-H., Li T., Liu X., Takwoingi Y., Sounderajah V., Ashrafian H., Ting D.S.W., et al.  Diagnostic Performance of Deep Learning for Infectious Keratitis: A Systematic Review and Meta-Analysis. EClinicalMedicine. 2024;77:102887. doi: 10.1016/j.eclinm.2024.102887.</cite> [<a href="https://doi.org/10.1016/j.eclinm.2024.102887" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11513659/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39469534/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=EClinicalMedicine&amp;title=Diagnostic%20Performance%20of%20Deep%20Learning%20for%20Infectious%20Keratitis:%20A%20Systematic%20Review%20and%20Meta-Analysis&amp;author=Z.Z.%20Ong&amp;author=Y.%20Sadek&amp;author=R.%20Qureshi&amp;author=S.-H.%20Liu&amp;author=T.%20Li&amp;volume=77&amp;publication_year=2024&amp;pages=102887&amp;pmid=39469534&amp;doi=10.1016/j.eclinm.2024.102887&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B183-vision-09-00071">
<span class="label">183.</span><cite>Soleimani M., Esmaili K., Rahdar A., Aminizadeh M., Cheraqpour K., Tabatabaei S.A., Mirshahi R., Bibak-Bejandi Z., Mohammadi S.F., Koganti R., et al.  From the Diagnosis of Infectious Keratitis to Discriminating Fungal Subtypes; A Deep Learning-Based Study. Sci. Rep. 2023;13:22200.  doi: 10.1038/s41598-023-49635-8.</cite> [<a href="https://doi.org/10.1038/s41598-023-49635-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10721811/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38097753/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=From%20the%20Diagnosis%20of%20Infectious%20Keratitis%20to%20Discriminating%20Fungal%20Subtypes;%20A%20Deep%20Learning-Based%20Study&amp;author=M.%20Soleimani&amp;author=K.%20Esmaili&amp;author=A.%20Rahdar&amp;author=M.%20Aminizadeh&amp;author=K.%20Cheraqpour&amp;volume=13&amp;publication_year=2023&amp;pages=22200&amp;pmid=38097753&amp;doi=10.1038/s41598-023-49635-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B184-vision-09-00071">
<span class="label">184.</span><cite>Wei Z., Wang S., Wang Z., Chen K., Gong L., Li G., Zheng Q., Zhang Q., He Y., Zhang Q., et al.  Development and Multi-Center Validation of Machine Learning Model for Early Detection of Fungal Keratitis. eBioMedicine. 2023;88:104438.  doi: 10.1016/j.ebiom.2023.104438.</cite> [<a href="https://doi.org/10.1016/j.ebiom.2023.104438" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9869416/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36681000/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=eBioMedicine&amp;title=Development%20and%20Multi-Center%20Validation%20of%20Machine%20Learning%20Model%20for%20Early%20Detection%20of%20Fungal%20Keratitis&amp;author=Z.%20Wei&amp;author=S.%20Wang&amp;author=Z.%20Wang&amp;author=K.%20Chen&amp;author=L.%20Gong&amp;volume=88&amp;publication_year=2023&amp;pages=104438&amp;pmid=36681000&amp;doi=10.1016/j.ebiom.2023.104438&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B185-vision-09-00071">
<span class="label">185.</span><cite>Li Z., Xie H., Wang Z., Li D., Chen K., Zong X., Qiang W., Wen F., Deng Z., Chen L., et al.  Deep Learning for Multi-Type Infectious Keratitis Diagnosis: A Nationwide, Cross-Sectional, Multicenter Study. npj Digit. Med. 2024;7:181. doi: 10.1038/s41746-024-01174-w.</cite> [<a href="https://doi.org/10.1038/s41746-024-01174-w" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11227533/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38971902/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=npj%20Digit.%20Med.&amp;title=Deep%20Learning%20for%20Multi-Type%20Infectious%20Keratitis%20Diagnosis:%20A%20Nationwide,%20Cross-Sectional,%20Multicenter%20Study&amp;author=Z.%20Li&amp;author=H.%20Xie&amp;author=Z.%20Wang&amp;author=D.%20Li&amp;author=K.%20Chen&amp;volume=7&amp;publication_year=2024&amp;pages=181&amp;pmid=38971902&amp;doi=10.1038/s41746-024-01174-w&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B186-vision-09-00071">
<span class="label">186.</span><cite>Shareef O., Soleimani M., Tu E., Jacobs D.S., Ciolino J.B., Rahdar A., Cheraqpour K., Ashraf M., Habib N.B., Greenfield J., et al.  A Novel Artificial Intelligence Model for Diagnosing Acanthamoeba Keratitis Through Confocal Microscopy. Ocul. Surf. 2024;34:159–164. doi: 10.1016/j.jtos.2024.07.010.</cite> [<a href="https://doi.org/10.1016/j.jtos.2024.07.010" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39084255/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ocul.%20Surf.&amp;title=A%20Novel%20Artificial%20Intelligence%20Model%20for%20Diagnosing%20Acanthamoeba%20Keratitis%20Through%20Confocal%20Microscopy&amp;author=O.%20Shareef&amp;author=M.%20Soleimani&amp;author=E.%20Tu&amp;author=D.S.%20Jacobs&amp;author=J.B.%20Ciolino&amp;volume=34&amp;publication_year=2024&amp;pages=159-164&amp;pmid=39084255&amp;doi=10.1016/j.jtos.2024.07.010&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B187-vision-09-00071">
<span class="label">187.</span><cite>Ting D.S., Gopal B.P., Deshmukh R., Seitzman G.D., Said D.G., Dua H.S. Diagnostic Armamentarium of Infectious Keratitis: A Comprehensive Review. Ocul. Surf. 2022;23:27–39. doi: 10.1016/j.jtos.2021.11.003.</cite> [<a href="https://doi.org/10.1016/j.jtos.2021.11.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8810150/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34781020/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ocul.%20Surf.&amp;title=Diagnostic%20Armamentarium%20of%20Infectious%20Keratitis:%20A%20Comprehensive%20Review&amp;author=D.S.%20Ting&amp;author=B.P.%20Gopal&amp;author=R.%20Deshmukh&amp;author=G.D.%20Seitzman&amp;author=D.G.%20Said&amp;volume=23&amp;publication_year=2022&amp;pages=27-39&amp;pmid=34781020&amp;doi=10.1016/j.jtos.2021.11.003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B188-vision-09-00071">
<span class="label">188.</span><cite>Saini J.S., Jain A.K., Kumar S., Vikal S., Pankaj S., Singh S. Neural Network Approach to Classify Infective Keratitis. Curr. Eye Res. 2003;27:111–116. doi: 10.1076/ceyr.27.2.111.15949.</cite> [<a href="https://doi.org/10.1076/ceyr.27.2.111.15949" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/14632163/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Curr.%20Eye%20Res.&amp;title=Neural%20Network%20Approach%20to%20Classify%20Infective%20Keratitis&amp;author=J.S.%20Saini&amp;author=A.K.%20Jain&amp;author=S.%20Kumar&amp;author=S.%20Vikal&amp;author=S.%20Pankaj&amp;volume=27&amp;publication_year=2003&amp;pages=111-116&amp;pmid=14632163&amp;doi=10.1076/ceyr.27.2.111.15949&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B189-vision-09-00071">
<span class="label">189.</span><cite>Ghosh A.K., Thammasudjarit R., Jongkhajornpong P., Attia J., Thakkinstian A. Deep Learning for Discrimination Between Fungal Keratitis and Bacterial Keratitis: DeepKeratitis. Cornea. 2022;41:616–622. doi: 10.1097/ICO.0000000000002830.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000002830" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8969839/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34581296/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Deep%20Learning%20for%20Discrimination%20Between%20Fungal%20Keratitis%20and%20Bacterial%20Keratitis:%20DeepKeratitis&amp;author=A.K.%20Ghosh&amp;author=R.%20Thammasudjarit&amp;author=P.%20Jongkhajornpong&amp;author=J.%20Attia&amp;author=A.%20Thakkinstian&amp;volume=41&amp;publication_year=2022&amp;pages=616-622&amp;pmid=34581296&amp;doi=10.1097/ICO.0000000000002830&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B190-vision-09-00071">
<span class="label">190.</span><cite>Hung N., Shih A.K.-Y., Lin C., Kuo M.-T., Hwang Y.-S., Wu W.-C., Kuo C.-F., Kang E.Y.-C., Hsiao C.-H. Using Slit-Lamp Images for Deep Learning-Based Identification of Bacterial and Fungal Keratitis: Model Development and Validation with Different Convolutional Neural Networks. Diagnostics. 2021;11:1246.  doi: 10.3390/diagnostics11071246.</cite> [<a href="https://doi.org/10.3390/diagnostics11071246" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8307675/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34359329/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diagnostics&amp;title=Using%20Slit-Lamp%20Images%20for%20Deep%20Learning-Based%20Identification%20of%20Bacterial%20and%20Fungal%20Keratitis:%20Model%20Development%20and%20Validation%20with%20Different%20Convolutional%20Neural%20Networks&amp;author=N.%20Hung&amp;author=A.K.-Y.%20Shih&amp;author=C.%20Lin&amp;author=M.-T.%20Kuo&amp;author=Y.-S.%20Hwang&amp;volume=11&amp;publication_year=2021&amp;pages=1246&amp;pmid=34359329&amp;doi=10.3390/diagnostics11071246&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B191-vision-09-00071">
<span class="label">191.</span><cite>Redd T.K., Prajna N.V., Srinivasan M., Lalitha P., Krishnan T., Rajaraman R., Venugopal A., Acharya N., Seitzman G.D., Lietman T.M., et al.  Image-Based Differentiation of Bacterial and Fungal Keratitis Using Deep Convolutional Neural Networks. Ophthalmol. Sci. 2022;2:100119. doi: 10.1016/j.xops.2022.100119.</cite> [<a href="https://doi.org/10.1016/j.xops.2022.100119" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9560557/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36249698/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ophthalmol.%20Sci.&amp;title=Image-Based%20Differentiation%20of%20Bacterial%20and%20Fungal%20Keratitis%20Using%20Deep%20Convolutional%20Neural%20Networks&amp;author=T.K.%20Redd&amp;author=N.V.%20Prajna&amp;author=M.%20Srinivasan&amp;author=P.%20Lalitha&amp;author=T.%20Krishnan&amp;volume=2&amp;publication_year=2022&amp;pages=100119&amp;pmid=36249698&amp;doi=10.1016/j.xops.2022.100119&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B192-vision-09-00071">
<span class="label">192.</span><cite>Redd T.K., Santina L.D., Prajna L.V., Lalitha P., Acharya N., Lietman T. Automated Differentiation of Bacterial from Fungal Keratitis Using Deep Learning. Investig. Ophthalmol. Vis. Sci. 2021;62:2161.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Automated%20Differentiation%20of%20Bacterial%20from%20Fungal%20Keratitis%20Using%20Deep%20Learning&amp;author=T.K.%20Redd&amp;author=L.D.%20Santina&amp;author=L.V.%20Prajna&amp;author=P.%20Lalitha&amp;author=N.%20Acharya&amp;volume=62&amp;publication_year=2021&amp;pages=2161&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B193-vision-09-00071">
<span class="label">193.</span><cite>Woodward M.A., Maganti N., Niziol L.M., Amin S., Hou A., Singh K. Development and Validation of a Natural Language Processing Algorithm to Extract Descriptors of Microbial Keratitis from the Electronic Health Record. Cornea. 2021;40:1548–1553. doi: 10.1097/ICO.0000000000002755.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000002755" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8578049/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34029244/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Development%20and%20Validation%20of%20a%20Natural%20Language%20Processing%20Algorithm%20to%20Extract%20Descriptors%20of%20Microbial%20Keratitis%20from%20the%20Electronic%20Health%20Record&amp;author=M.A.%20Woodward&amp;author=N.%20Maganti&amp;author=L.M.%20Niziol&amp;author=S.%20Amin&amp;author=A.%20Hou&amp;volume=40&amp;publication_year=2021&amp;pages=1548-1553&amp;pmid=34029244&amp;doi=10.1097/ICO.0000000000002755&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B194-vision-09-00071">
<span class="label">194.</span><cite>Kuo M.T., Hsu B.W., Lin Y.S., Fang P.C., Yu H.J., Chen A., Yu M.S., Tseng V.S. Comparisons of Deep Learning Algorithms for Diagnosing Bacterial Keratitis via External Eye Photographs. Sci. Rep. 2021;11:24227.  doi: 10.1038/s41598-021-03572-6.</cite> [<a href="https://doi.org/10.1038/s41598-021-03572-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8688438/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34930952/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Comparisons%20of%20Deep%20Learning%20Algorithms%20for%20Diagnosing%20Bacterial%20Keratitis%20via%20External%20Eye%20Photographs&amp;author=M.T.%20Kuo&amp;author=B.W.%20Hsu&amp;author=Y.S.%20Lin&amp;author=P.C.%20Fang&amp;author=H.J.%20Yu&amp;volume=11&amp;publication_year=2021&amp;pages=24227&amp;pmid=34930952&amp;doi=10.1038/s41598-021-03572-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B195-vision-09-00071">
<span class="label">195.</span><cite>Loo J., Woodward M.A., Prajna V., Kriegel M.F., Pawar M., Khan M., Niziol L.M., Farsiu S. Open-Source Automatic Biomarker Measurement on Slit-Lamp Photography to Estimate Visual Acuity in Microbial Keratitis. Transl. Vis. Sci. Technol. 2021;10:2. doi: 10.1167/tvst.10.12.2.</cite> [<a href="https://doi.org/10.1167/tvst.10.12.2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8496413/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34605877/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Transl.%20Vis.%20Sci.%20Technol.&amp;title=Open-Source%20Automatic%20Biomarker%20Measurement%20on%20Slit-Lamp%20Photography%20to%20Estimate%20Visual%20Acuity%20in%20Microbial%20Keratitis&amp;author=J.%20Loo&amp;author=M.A.%20Woodward&amp;author=V.%20Prajna&amp;author=M.F.%20Kriegel&amp;author=M.%20Pawar&amp;volume=10&amp;publication_year=2021&amp;pages=2&amp;pmid=34605877&amp;doi=10.1167/tvst.10.12.2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B196-vision-09-00071">
<span class="label">196.</span><cite>Mayya V., Kamath Shevgoor S., Kulkarni U., Hazarika M., Barua P.D., Acharya U.R. Multi-Scale Convolutional Neural Network for Accurate Corneal Segmentation in Early Detection of Fungal Keratitis. J. Fungi. 2021;7:850.  doi: 10.3390/jof7100850.</cite> [<a href="https://doi.org/10.3390/jof7100850" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8540278/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34682271/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Fungi&amp;title=Multi-Scale%20Convolutional%20Neural%20Network%20for%20Accurate%20Corneal%20Segmentation%20in%20Early%20Detection%20of%20Fungal%20Keratitis&amp;author=V.%20Mayya&amp;author=S.%20Kamath%20Shevgoor&amp;author=U.%20Kulkarni&amp;author=M.%20Hazarika&amp;author=P.D.%20Barua&amp;volume=7&amp;publication_year=2021&amp;pages=850&amp;pmid=34682271&amp;doi=10.3390/jof7100850&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B197-vision-09-00071">
<span class="label">197.</span><cite>Farsiu S., Loo J., Kreigel M.F., Tuohy M., Prajna V., Woodward M.A. Deep Learning-Based Automatic Segmentation of Stromal Infiltrates and Associated Biomarkers on Slit-Lamp Images of Microbial Keratitis. Investig. Ophthalmol. Vis. Sci. 2019;60:1480.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Deep%20Learning-Based%20Automatic%20Segmentation%20of%20Stromal%20Infiltrates%20and%20Associated%20Biomarkers%20on%20Slit-Lamp%20Images%20of%20Microbial%20Keratitis&amp;author=S.%20Farsiu&amp;author=J.%20Loo&amp;author=M.F.%20Kreigel&amp;author=M.%20Tuohy&amp;author=V.%20Prajna&amp;volume=60&amp;publication_year=2019&amp;pages=1480&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B198-vision-09-00071">
<span class="label">198.</span><cite>Ji Q., Jiang Y., Qu L., Yang Q., Zhang H. An Image Diagnosis Algorithm for Keratitis Based on Deep Learning. Neural Process. Lett. 2022;54:2007–2024. doi: 10.1007/s11063-021-10716-2.</cite> [<a href="https://doi.org/10.1007/s11063-021-10716-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neural%20Process.%20Lett.&amp;title=An%20Image%20Diagnosis%20Algorithm%20for%20Keratitis%20Based%20on%20Deep%20Learning&amp;author=Q.%20Ji&amp;author=Y.%20Jiang&amp;author=L.%20Qu&amp;author=Q.%20Yang&amp;author=H.%20Zhang&amp;volume=54&amp;publication_year=2022&amp;pages=2007-2024&amp;doi=10.1007/s11063-021-10716-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B199-vision-09-00071">
<span class="label">199.</span><cite>Kuo M.T., Hsu B.W., Yin Y.K., Fang P.C., Lai H.Y., Chen A., Yu M.S., Tseng V.S. A Deep Learning Approach in Diagnosing Fungal Keratitis Based on Corneal Photographs. Sci. Rep. 2020;10:14424.  doi: 10.1038/s41598-020-71425-9.</cite> [<a href="https://doi.org/10.1038/s41598-020-71425-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7468230/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32879364/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=A%20Deep%20Learning%20Approach%20in%20Diagnosing%20Fungal%20Keratitis%20Based%20on%20Corneal%20Photographs&amp;author=M.T.%20Kuo&amp;author=B.W.%20Hsu&amp;author=Y.K.%20Yin&amp;author=P.C.%20Fang&amp;author=H.Y.%20Lai&amp;volume=10&amp;publication_year=2020&amp;pages=14424&amp;pmid=32879364&amp;doi=10.1038/s41598-020-71425-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B200-vision-09-00071">
<span class="label">200.</span><cite>Kriegel M.F., Huang J., Ashfaq H.A., Niziol L.M., Preethi M., Tan H., Tuohy M.M., Patel T.P., Prajna V., Woodward M.A. Algorithm Variability in Quantification of Epithelial Defect Size in Microbial Keratitis Images. Cornea. 2020;39:628–633. doi: 10.1097/ICO.0000000000002258.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000002258" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7732187/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31977729/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Algorithm%20Variability%20in%20Quantification%20of%20Epithelial%20Defect%20Size%20in%20Microbial%20Keratitis%20Images&amp;author=M.F.%20Kriegel&amp;author=J.%20Huang&amp;author=H.A.%20Ashfaq&amp;author=L.M.%20Niziol&amp;author=M.%20Preethi&amp;volume=39&amp;publication_year=2020&amp;pages=628-633&amp;pmid=31977729&amp;doi=10.1097/ICO.0000000000002258&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B201-vision-09-00071">
<span class="label">201.</span><cite>Patel T.P., Prajna N.V., Farsiu S., Valikodath N.G., Niziol L.M., Dudeja L., Kim K.H., Woodward M.A. Novel Image-Based Analysis for Reduction of Clinician-Dependent Variability in Measurement of the Corneal Ulcer Size. Cornea. 2018;37:331–339. doi: 10.1097/ICO.0000000000001488.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000001488" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5799030/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29256985/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Novel%20Image-Based%20Analysis%20for%20Reduction%20of%20Clinician-Dependent%20Variability%20in%20Measurement%20of%20the%20Corneal%20Ulcer%20Size&amp;author=T.P.%20Patel&amp;author=N.V.%20Prajna&amp;author=S.%20Farsiu&amp;author=N.G.%20Valikodath&amp;author=L.M.%20Niziol&amp;volume=37&amp;publication_year=2018&amp;pages=331-339&amp;pmid=29256985&amp;doi=10.1097/ICO.0000000000001488&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B202-vision-09-00071">
<span class="label">202.</span><cite>Liu Z., Cao Y., Li Y., Xiao X., Qiu Q., Yang M., Zhao Y., Cui L. Automatic Diagnosis of Fungal Keratitis Using Data Augmentation and Image Fusion with Deep Convolutional Neural Network. Comput. Methods Programs Biomed. 2020;187:105019.  doi: 10.1016/j.cmpb.2019.105019.</cite> [<a href="https://doi.org/10.1016/j.cmpb.2019.105019" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31421868/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput.%20Methods%20Programs%20Biomed.&amp;title=Automatic%20Diagnosis%20of%20Fungal%20Keratitis%20Using%20Data%20Augmentation%20and%20Image%20Fusion%20with%20Deep%20Convolutional%20Neural%20Network&amp;author=Z.%20Liu&amp;author=Y.%20Cao&amp;author=Y.%20Li&amp;author=X.%20Xiao&amp;author=Q.%20Qiu&amp;volume=187&amp;publication_year=2020&amp;pages=105019&amp;pmid=31421868&amp;doi=10.1016/j.cmpb.2019.105019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B203-vision-09-00071">
<span class="label">203.</span><cite>Wu X., Qiu Q., Liu Z., Zhao Y., Zhang B., Zhang Y., Wu X., Ren J. Hyphae Detection in Fungal Keratitis Images with Adaptive Robust Binary Pattern. IEEE Access. 2018;6:13449–13460. doi: 10.1109/ACCESS.2018.2808941.</cite> [<a href="https://doi.org/10.1109/ACCESS.2018.2808941" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=Hyphae%20Detection%20in%20Fungal%20Keratitis%20Images%20with%20Adaptive%20Robust%20Binary%20Pattern&amp;author=X.%20Wu&amp;author=Q.%20Qiu&amp;author=Z.%20Liu&amp;author=Y.%20Zhao&amp;author=B.%20Zhang&amp;volume=6&amp;publication_year=2018&amp;pages=13449-13460&amp;doi=10.1109/ACCESS.2018.2808941&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B204-vision-09-00071">
<span class="label">204.</span><cite>Lv J., Zhang K., Chen Q., Chen Q., Huang W., Cui L., Li M., Li J., Chen L., Shen C., et al.  Deep Learning-Based Automated Diagnosis of Fungal Keratitis with In Vivo Confocal Microscopy Images. Ann. Transl. Med. 2020;8:706. doi: 10.21037/atm.2020.03.134.</cite> [<a href="https://doi.org/10.21037/atm.2020.03.134" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7327373/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32617326/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ann.%20Transl.%20Med.&amp;title=Deep%20Learning-Based%20Automated%20Diagnosis%20of%20Fungal%20Keratitis%20with%20In%20Vivo%20Confocal%20Microscopy%20Images&amp;author=J.%20Lv&amp;author=K.%20Zhang&amp;author=Q.%20Chen&amp;author=Q.%20Chen&amp;author=W.%20Huang&amp;volume=8&amp;publication_year=2020&amp;pages=706&amp;pmid=32617326&amp;doi=10.21037/atm.2020.03.134&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B205-vision-09-00071">
<span class="label">205.</span><cite>Hou H., Cao Y., Cui X., Liu Z., Xu H., Wang C., Zhang W., Zhang Y., Fang Y., Geng Y., et al.  Medical Image Management and Analysis System Based on Web for Fungal Keratitis Images. Math. Biosci. Eng. 2021;4:3667–3679. doi: 10.3934/mbe.2021183.</cite> [<a href="https://doi.org/10.3934/mbe.2021183" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34198405/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Math.%20Biosci.%20Eng.&amp;title=Medical%20Image%20Management%20and%20Analysis%20System%20Based%20on%20Web%20for%20Fungal%20Keratitis%20Images&amp;author=H.%20Hou&amp;author=Y.%20Cao&amp;author=X.%20Cui&amp;author=Z.%20Liu&amp;author=H.%20Xu&amp;volume=4&amp;publication_year=2021&amp;pages=3667-3679&amp;pmid=34198405&amp;doi=10.3934/mbe.2021183&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B206-vision-09-00071">
<span class="label">206.</span><cite>Koyama A., Miyazaki D., Nakagawa Y., Ayatsuka Y., Miyake H., Ehara F., Sasaki S.-I., Shimizu Y., Inoue Y. Determination of Probability of Causative Pathogen in Infectious Keratitis Using Deep Learning Algorithm of Slit-Lamp Images. Sci. Rep. 2021;11:22642.  doi: 10.1038/s41598-021-02138-w.</cite> [<a href="https://doi.org/10.1038/s41598-021-02138-w" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8608802/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34811468/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Determination%20of%20Probability%20of%20Causative%20Pathogen%20in%20Infectious%20Keratitis%20Using%20Deep%20Learning%20Algorithm%20of%20Slit-Lamp%20Images&amp;author=A.%20Koyama&amp;author=D.%20Miyazaki&amp;author=Y.%20Nakagawa&amp;author=Y.%20Ayatsuka&amp;author=H.%20Miyake&amp;volume=11&amp;publication_year=2021&amp;pages=22642&amp;pmid=34811468&amp;doi=10.1038/s41598-021-02138-w&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B207-vision-09-00071">
<span class="label">207.</span><cite>Soleimani M., Rahdar A., Esmaili K., Cheraqpour K., Baharnoori M., Kufta A., Mohammadi S.F., Cheung A., Yousefi S., Djalilian A.R. AI-Assisted Diagnosis and Subtype Differentiation of Microbial Keratitis: One Step Forward to Mitigate Health Disparities. Investig. Ophthalmol. Vis. Sci. 2024;65:1503.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=AI-Assisted%20Diagnosis%20and%20Subtype%20Differentiation%20of%20Microbial%20Keratitis:%20One%20Step%20Forward%20to%20Mitigate%20Health%20Disparities&amp;author=M.%20Soleimani&amp;author=A.%20Rahdar&amp;author=K.%20Esmaili&amp;author=K.%20Cheraqpour&amp;author=M.%20Baharnoori&amp;volume=65&amp;publication_year=2024&amp;pages=1503&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B208-vision-09-00071">
<span class="label">208.</span><cite>Vupparaboina K.K., Vedula S.N., Aithu S., Bashar S.B., Challa K., Loomba A., Taneja M., Channapayya S., Richhariya A. Artificial Intelligence Based Detection of Infectious Keratitis Using Slit-Lamp Images. Investig. Ophthalmol. Vis. Sci. 2019;60:4236.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Investig.%20Ophthalmol.%20Vis.%20Sci.&amp;title=Artificial%20Intelligence%20Based%20Detection%20of%20Infectious%20Keratitis%20Using%20Slit-Lamp%20Images&amp;author=K.K.%20Vupparaboina&amp;author=S.N.%20Vedula&amp;author=S.%20Aithu&amp;author=S.B.%20Bashar&amp;author=K.%20Challa&amp;volume=60&amp;publication_year=2019&amp;pages=4236&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B209-vision-09-00071">
<span class="label">209.</span><cite>Xu Y., Kong M., Xie W., Duan R., Fang Z., Lin Y., Zhu Q., Tang S., Wu F., Yao Y.-F. Deep Sequential Feature Learning in Clinical Image Classification of Infectious Keratitis. Engineering. 2021;7:1002–1010. doi: 10.1016/j.eng.2020.04.012.</cite> [<a href="https://doi.org/10.1016/j.eng.2020.04.012" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Engineering&amp;title=Deep%20Sequential%20Feature%20Learning%20in%20Clinical%20Image%20Classification%20of%20Infectious%20Keratitis&amp;author=Y.%20Xu&amp;author=M.%20Kong&amp;author=W.%20Xie&amp;author=R.%20Duan&amp;author=Z.%20Fang&amp;volume=7&amp;publication_year=2021&amp;pages=1002-1010&amp;doi=10.1016/j.eng.2020.04.012&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B210-vision-09-00071">
<span class="label">210.</span><cite>Fang Z., Kuang K., Lin Y., Wu F., Yao Y.-F., editors. Concept-Based Explanation for Fine-Grained Images and Its Application in Infectious Keratitis Classification; Proceedings of the 28th ACM International Conference on Multimedia; Seattle, WA, USA. 12–16 October 2020.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%2028th%20ACM%20International%20Conference%20on%20Multimedia&amp;title=Concept-Based%20Explanation%20for%20Fine-Grained%20Images%20and%20Its%20Application%20in%20Infectious%20Keratitis%20Classification&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B211-vision-09-00071">
<span class="label">211.</span><cite>Wang L., Chen K., Wen H., Zheng Q., Chen Y., Pu J., Chen W. Feasibility Assessment of Infectious Keratitis Depicted on Slit-Lamp and Smartphone Photographs Using Deep Learning. Int. J. Med. Inform. 2021;155:104583. doi: 10.1016/j.ijmedinf.2021.104583.</cite> [<a href="https://doi.org/10.1016/j.ijmedinf.2021.104583" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34560490/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int.%20J.%20Med.%20Inform.&amp;title=Feasibility%20Assessment%20of%20Infectious%20Keratitis%20Depicted%20on%20Slit-Lamp%20and%20Smartphone%20Photographs%20Using%20Deep%20Learning&amp;author=L.%20Wang&amp;author=K.%20Chen&amp;author=H.%20Wen&amp;author=Q.%20Zheng&amp;author=Y.%20Chen&amp;volume=155&amp;publication_year=2021&amp;pages=104583&amp;pmid=34560490&amp;doi=10.1016/j.ijmedinf.2021.104583&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B212-vision-09-00071">
<span class="label">212.</span><cite>Natarajan R., Matai H.D., Raman S., Kumar S., Ravichandran S., Swaminathan S., Alex John S.R. Advances in the Diagnosis of Herpes Simplex Stromal Necrotising Keratitis: A Feasibility Study on Deep Learning Approach. Indian J. Ophthalmol. 2022;70:3279–3283. doi: 10.4103/ijo.IJO_178_22.</cite> [<a href="https://doi.org/10.4103/ijo.IJO_178_22" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9675505/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36018103/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Indian%20J.%20Ophthalmol.&amp;title=Advances%20in%20the%20Diagnosis%20of%20Herpes%20Simplex%20Stromal%20Necrotising%20Keratitis:%20A%20Feasibility%20Study%20on%20Deep%20Learning%20Approach&amp;author=R.%20Natarajan&amp;author=H.D.%20Matai&amp;author=S.%20Raman&amp;author=S.%20Kumar&amp;author=S.%20Ravichandran&amp;volume=70&amp;publication_year=2022&amp;pages=3279-3283&amp;pmid=36018103&amp;doi=10.4103/ijo.IJO_178_22&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B213-vision-09-00071">
<span class="label">213.</span><cite>Tiwari M., Piech C., Baitemirova M., Prajna N.V., Srinivasan M., Lalitha P., Villegas N., Balachandar N., Chua J.T., Redd T., et al.  Differentiation of Active Corneal Infections from Healed Scars Using Deep Learning. Ophthalmology. 2022;129:139–146. doi: 10.1016/j.ophtha.2021.07.033.</cite> [<a href="https://doi.org/10.1016/j.ophtha.2021.07.033" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8792172/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34352302/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ophthalmology&amp;title=Differentiation%20of%20Active%20Corneal%20Infections%20from%20Healed%20Scars%20Using%20Deep%20Learning&amp;author=M.%20Tiwari&amp;author=C.%20Piech&amp;author=M.%20Baitemirova&amp;author=N.V.%20Prajna&amp;author=M.%20Srinivasan&amp;volume=129&amp;publication_year=2022&amp;pages=139-146&amp;pmid=34352302&amp;doi=10.1016/j.ophtha.2021.07.033&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B214-vision-09-00071">
<span class="label">214.</span><cite>Alquran H., Al-Issa Y., Alsalatie M., Mustafa W.A., Qasmieh I.A., Zyout A. Intelligent Diagnosis and Classification of Keratitis. Diagnostics. 2022;12:1344.  doi: 10.3390/diagnostics12061344.</cite> [<a href="https://doi.org/10.3390/diagnostics12061344" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9222010/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35741153/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diagnostics&amp;title=Intelligent%20Diagnosis%20and%20Classification%20of%20Keratitis&amp;author=H.%20Alquran&amp;author=Y.%20Al-Issa&amp;author=M.%20Alsalatie&amp;author=W.A.%20Mustafa&amp;author=I.A.%20Qasmieh&amp;volume=12&amp;publication_year=2022&amp;pages=1344&amp;pmid=35741153&amp;doi=10.3390/diagnostics12061344&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B215-vision-09-00071">
<span class="label">215.</span><cite>Mansoor H., Tan H.C., Lin M.T., Mehta J.S., Liu Y.C. Diabetic Corneal Neuropathy. J. Clin. Med. 2020;9:3956.  doi: 10.3390/jcm9123956.</cite> [<a href="https://doi.org/10.3390/jcm9123956" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7762152/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33291308/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Clin.%20Med.&amp;title=Diabetic%20Corneal%20Neuropathy&amp;author=H.%20Mansoor&amp;author=H.C.%20Tan&amp;author=M.T.%20Lin&amp;author=J.S.%20Mehta&amp;author=Y.C.%20Liu&amp;volume=9&amp;publication_year=2020&amp;pages=3956&amp;pmid=33291308&amp;doi=10.3390/jcm9123956&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B216-vision-09-00071">
<span class="label">216.</span><cite>Vereertbrugghen A., Galletti J.G. Corneal Nerves and Their Role in Dry Eye Pathophysiology. Exp. Eye Res. 2022;222:109191. doi: 10.1016/j.exer.2022.109191.</cite> [<a href="https://doi.org/10.1016/j.exer.2022.109191" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35850173/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Exp.%20Eye%20Res.&amp;title=Corneal%20Nerves%20and%20Their%20Role%20in%20Dry%20Eye%20Pathophysiology&amp;author=A.%20Vereertbrugghen&amp;author=J.G.%20Galletti&amp;volume=222&amp;publication_year=2022&amp;pages=109191&amp;pmid=35850173&amp;doi=10.1016/j.exer.2022.109191&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B217-vision-09-00071">
<span class="label">217.</span><cite>Liu Y.C., Lin M.T., Mehta J.S. Analysis of Corneal Nerve Plexus in Corneal Confocal Microscopy Images. Neural Regen. Res. 2021;16:690–691. doi: 10.4103/1673-5374.289435.</cite> [<a href="https://doi.org/10.4103/1673-5374.289435" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8067927/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33063728/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neural%20Regen.%20Res.&amp;title=Analysis%20of%20Corneal%20Nerve%20Plexus%20in%20Corneal%20Confocal%20Microscopy%20Images&amp;author=Y.C.%20Liu&amp;author=M.T.%20Lin&amp;author=J.S.%20Mehta&amp;volume=16&amp;publication_year=2021&amp;pages=690-691&amp;pmid=33063728&amp;doi=10.4103/1673-5374.289435&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B218-vision-09-00071">
<span class="label">218.</span><cite>Preston F.G., Meng Y., Burgess J., Ferdousi M., Azmi S., Petropoulos I.N., Kaye S., Malik R.A., Zheng Y., Alam U. Artificial Intelligence Utilising Corneal Confocal Microscopy for the Diagnosis of Peripheral Neuropathy in Diabetes Mellitus and Prediabetes. Diabetologia. 2022;65:457–466. doi: 10.1007/s00125-021-05617-x.</cite> [<a href="https://doi.org/10.1007/s00125-021-05617-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8803718/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34806115/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diabetologia&amp;title=Artificial%20Intelligence%20Utilising%20Corneal%20Confocal%20Microscopy%20for%20the%20Diagnosis%20of%20Peripheral%20Neuropathy%20in%20Diabetes%20Mellitus%20and%20Prediabetes&amp;author=F.G.%20Preston&amp;author=Y.%20Meng&amp;author=J.%20Burgess&amp;author=M.%20Ferdousi&amp;author=S.%20Azmi&amp;volume=65&amp;publication_year=2022&amp;pages=457-466&amp;pmid=34806115&amp;doi=10.1007/s00125-021-05617-x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B219-vision-09-00071">
<span class="label">219.</span><cite>Salahouddin T., Petropoulos I.N., Ferdousi M., Ponirakis G., Asghar O., Alam U., Kamran S., Mahfoud Z.R., Efron N., Malik R.A., et al.  Artificial Intelligence–Based Classification of Diabetic Peripheral Neuropathy from Corneal Confocal Microscopy Images. Diabetes Care. 2021;44:e151. doi: 10.2337/dc20-2012.</cite> [<a href="https://doi.org/10.2337/dc20-2012" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8323170/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34083322/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diabetes%20Care&amp;title=Artificial%20Intelligence%E2%80%93Based%20Classification%20of%20Diabetic%20Peripheral%20Neuropathy%20from%20Corneal%20Confocal%20Microscopy%20Images&amp;author=T.%20Salahouddin&amp;author=I.N.%20Petropoulos&amp;author=M.%20Ferdousi&amp;author=G.%20Ponirakis&amp;author=O.%20Asghar&amp;volume=44&amp;publication_year=2021&amp;pages=e151&amp;pmid=34083322&amp;doi=10.2337/dc20-2012&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B220-vision-09-00071">
<span class="label">220.</span><cite>Scarpa F., Colonna A., Ruggeri A. Multiple-Image Deep Learning Analysis for Neuropathy Detection in Corneal Nerve Images. Cornea. 2020;39:342–347. doi: 10.1097/ICO.0000000000002181.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000002181" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31658167/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Multiple-Image%20Deep%20Learning%20Analysis%20for%20Neuropathy%20Detection%20in%20Corneal%20Nerve%20Images&amp;author=F.%20Scarpa&amp;author=A.%20Colonna&amp;author=A.%20Ruggeri&amp;volume=39&amp;publication_year=2020&amp;pages=342-347&amp;pmid=31658167&amp;doi=10.1097/ICO.0000000000002181&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B221-vision-09-00071">
<span class="label">221.</span><cite>Williams B.M., Borroni D., Liu R., Zhao Y., Zhang J., Lim J., Ma B., Romano V., Qi H., Ferdousi M., et al.  An Artificial Intelligence-Based Deep Learning Algorithm for the Diagnosis of Diabetic Neuropathy Using Corneal Confocal Microscopy: A Development and Validation Study. Diabetologia. 2020;63:419–430. doi: 10.1007/s00125-019-05023-4.</cite> [<a href="https://doi.org/10.1007/s00125-019-05023-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6946763/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31720728/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diabetologia&amp;title=An%20Artificial%20Intelligence-Based%20Deep%20Learning%20Algorithm%20for%20the%20Diagnosis%20of%20Diabetic%20Neuropathy%20Using%20Corneal%20Confocal%20Microscopy:%20A%20Development%20and%20Validation%20Study&amp;author=B.M.%20Williams&amp;author=D.%20Borroni&amp;author=R.%20Liu&amp;author=Y.%20Zhao&amp;author=J.%20Zhang&amp;volume=63&amp;publication_year=2020&amp;pages=419-430&amp;pmid=31720728&amp;doi=10.1007/s00125-019-05023-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B222-vision-09-00071">
<span class="label">222.</span><cite>Mou L., Qi H., Liu Y., Zheng Y., Matthew P., Su P., Liu J., Zhang J., Zhao Y. DeepGrading: Deep Learning Grading of Corneal Nerve Tortuosity. IEEE Trans. Med. Imaging. 2022;41:2079–2091. doi: 10.1109/TMI.2022.3156906.</cite> [<a href="https://doi.org/10.1109/TMI.2022.3156906" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35245193/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans.%20Med.%20Imaging&amp;title=DeepGrading:%20Deep%20Learning%20Grading%20of%20Corneal%20Nerve%20Tortuosity&amp;author=L.%20Mou&amp;author=H.%20Qi&amp;author=Y.%20Liu&amp;author=Y.%20Zheng&amp;author=P.%20Matthew&amp;volume=41&amp;publication_year=2022&amp;pages=2079-2091&amp;pmid=35245193&amp;doi=10.1109/TMI.2022.3156906&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B223-vision-09-00071">
<span class="label">223.</span><cite>Wu X., Liu L., Zhao L., Guo C., Li R., Wang T., Yang X., Xie P., Liu Y., Lin H. Application of Artificial Intelligence in Anterior Segment Ophthalmic Diseases: Diversity and Standardization. Ann. Transl. Med. 2020;8:714. doi: 10.21037/atm-20-976.</cite> [<a href="https://doi.org/10.21037/atm-20-976" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7327317/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32617334/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ann.%20Transl.%20Med.&amp;title=Application%20of%20Artificial%20Intelligence%20in%20Anterior%20Segment%20Ophthalmic%20Diseases:%20Diversity%20and%20Standardization&amp;author=X.%20Wu&amp;author=L.%20Liu&amp;author=L.%20Zhao&amp;author=C.%20Guo&amp;author=R.%20Li&amp;volume=8&amp;publication_year=2020&amp;pages=714&amp;pmid=32617334&amp;doi=10.21037/atm-20-976&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B224-vision-09-00071">
<span class="label">224.</span><cite>Delgado-Rivera G., Roman-Gonzalez A., Alva-Mantari A., Saldivar-Espinoza B., Zimic M., Barrientos-Porras F., Salguedo-Bohorquez M. Method for the Automatic Segmentation of the Palpebral Conjunctiva Using Image Processing; Proceedings of the 2018 IEEE International Conference on Automation/XXIII Congress of the Chilean Association of Automatic Control (ICA-ACCA); Concepcion, Chile. 17–19 October 2018; Piscataway, NJ, USA: IEEE; 2018. </cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%202018%20IEEE%20International%20Conference%20on%20Automation/XXIII%20Congress%20of%20the%20Chilean%20Association%20of%20Automatic%20Control%20(ICA-ACCA)&amp;title=Method%20for%20the%20Automatic%20Segmentation%20of%20the%20Palpebral%20Conjunctiva%20Using%20Image%20Processing&amp;author=G.%20Delgado-Rivera&amp;author=A.%20Roman-Gonzalez&amp;author=A.%20Alva-Mantari&amp;author=B.%20Saldivar-Espinoza&amp;author=M.%20Zimic&amp;publication_year=2018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B225-vision-09-00071">
<span class="label">225.</span><cite>Derakhshani R., Saripalle S.K., Doynov P. Computational Methods for Objective Assessment of Conjunctival Vascularity; Proceedings of the 2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society; San Diego, CA, USA. 28 August–1 September 2012; Piscataway, NJ, USA: IEEE; 2012. </cite> [<a href="https://doi.org/10.1109/EMBC.2012.6346223" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23366184/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%202012%20Annual%20International%20Conference%20of%20the%20IEEE%20Engineering%20in%20Medicine%20and%20Biology%20Society&amp;title=Computational%20Methods%20for%20Objective%20Assessment%20of%20Conjunctival%20Vascularity&amp;author=R.%20Derakhshani&amp;author=S.K.%20Saripalle&amp;author=P.%20Doynov&amp;publication_year=2012&amp;pmid=23366184&amp;doi=10.1109/EMBC.2012.6346223&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B226-vision-09-00071">
<span class="label">226.</span><cite>Jo H.-C., Jeong H., Lee J., Na K.-S., Kim D.-Y. Quantification of Blood Flow Velocity in the Human Conjunctival Microvessels Using Deep Learning-Based Stabilization Algorithm. Sensors. 2021;21:3224.  doi: 10.3390/s21093224.</cite> [<a href="https://doi.org/10.3390/s21093224" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8124391/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34066590/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sensors&amp;title=Quantification%20of%20Blood%20Flow%20Velocity%20in%20the%20Human%20Conjunctival%20Microvessels%20Using%20Deep%20Learning-Based%20Stabilization%20Algorithm&amp;author=H.-C.%20Jo&amp;author=H.%20Jeong&amp;author=J.%20Lee&amp;author=K.-S.%20Na&amp;author=D.-Y.%20Kim&amp;volume=21&amp;publication_year=2021&amp;pages=3224&amp;pmid=34066590&amp;doi=10.3390/s21093224&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B227-vision-09-00071">
<span class="label">227.</span><cite>Owen C.G., Ellis T.J., Woodward E.G. A Comparison of Manual and Automated Methods of Measuring Conjunctival Vessel Widths from Photographic and Digital Images. Ophthalmic Physiol. Opt. 2004;24:74–81. doi: 10.1046/j.1475-1313.2003.00171.x.</cite> [<a href="https://doi.org/10.1046/j.1475-1313.2003.00171.x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/15005671/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ophthalmic%20Physiol.%20Opt.&amp;title=A%20Comparison%20of%20Manual%20and%20Automated%20Methods%20of%20Measuring%20Conjunctival%20Vessel%20Widths%20from%20Photographic%20and%20Digital%20Images&amp;author=C.G.%20Owen&amp;author=T.J.%20Ellis&amp;author=E.G.%20Woodward&amp;volume=24&amp;publication_year=2004&amp;pages=74-81&amp;pmid=15005671&amp;doi=10.1046/j.1475-1313.2003.00171.x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B228-vision-09-00071">
<span class="label">228.</span><cite>Masumoto H., Tabuchi H., Yoneda T., Nakakura S., Ohsugi H., Sumi T., Fukushima A. Severity Classification of Conjunctival Hyperaemia by Deep Neural Network Ensembles. J. Ophthalmol. 2019;2019:7820971. doi: 10.1155/2019/7820971.</cite> [<a href="https://doi.org/10.1155/2019/7820971" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6589312/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31275636/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Ophthalmol.&amp;title=Severity%20Classification%20of%20Conjunctival%20Hyperaemia%20by%20Deep%20Neural%20Network%20Ensembles&amp;author=H.%20Masumoto&amp;author=H.%20Tabuchi&amp;author=T.%20Yoneda&amp;author=S.%20Nakakura&amp;author=H.%20Ohsugi&amp;volume=2019&amp;publication_year=2019&amp;pages=7820971&amp;pmid=31275636&amp;doi=10.1155/2019/7820971&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B229-vision-09-00071">
<span class="label">229.</span><cite>Li X., Xia C., Li X., Wei S., Zhou S., Yu X., Gao J., Cao Y., Zhang H. Identifying Diabetes from Conjunctival Images Using a Novel Hierarchical Multi-Task Network. Sci. Rep. 2022;12:264.  doi: 10.1038/s41598-021-04006-z.</cite> [<a href="https://doi.org/10.1038/s41598-021-04006-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8742044/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34997031/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Identifying%20Diabetes%20from%20Conjunctival%20Images%20Using%20a%20Novel%20Hierarchical%20Multi-Task%20Network&amp;author=X.%20Li&amp;author=C.%20Xia&amp;author=X.%20Li&amp;author=S.%20Wei&amp;author=S.%20Zhou&amp;volume=12&amp;publication_year=2022&amp;pages=264&amp;pmid=34997031&amp;doi=10.1038/s41598-021-04006-z&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B230-vision-09-00071">
<span class="label">230.</span><cite>Mergen B., Safi T., Nadig M., Bhattrai G., Daas L., Alexandersson J., Seitz B. Detecting the corneal neovascularisation area using artificial intelligence. Br. J. Ophthalmol. 2024;108:667–672. doi: 10.1136/bjo-2023-323308.</cite> [<a href="https://doi.org/10.1136/bjo-2023-323308" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37339866/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Br.%20J.%20Ophthalmol.&amp;title=Detecting%20the%20corneal%20neovascularisation%20area%20using%20artificial%20intelligence&amp;author=B.%20Mergen&amp;author=T.%20Safi&amp;author=M.%20Nadig&amp;author=G.%20Bhattrai&amp;author=L.%20Daas&amp;volume=108&amp;publication_year=2024&amp;pages=667-672&amp;pmid=37339866&amp;doi=10.1136/bjo-2023-323308&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B231-vision-09-00071">
<span class="label">231.</span><cite>Mirzayev I., Gündüz A.K., Aydın Ellialtıoğlu P., Gündüz Ö.Ö. Clinical Applications of Anterior Segment Swept-Source Optical Coherence Tomography: A Systematic Review. Photodiagnosis Photodyn. Ther. 2023;42:103334. doi: 10.1016/j.pdpdt.2023.103334.</cite> [<a href="https://doi.org/10.1016/j.pdpdt.2023.103334" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36764640/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Photodiagnosis%20Photodyn.%20Ther.&amp;title=Clinical%20Applications%20of%20Anterior%20Segment%20Swept-Source%20Optical%20Coherence%20Tomography:%20A%20Systematic%20Review&amp;author=I.%20Mirzayev&amp;author=A.K.%20G%C3%BCnd%C3%BCz&amp;author=P.%20Ayd%C4%B1n%20Ellialt%C4%B1o%C4%9Flu&amp;author=%C3%96.%C3%96.%20G%C3%BCnd%C3%BCz&amp;volume=42&amp;publication_year=2023&amp;pages=103334&amp;pmid=36764640&amp;doi=10.1016/j.pdpdt.2023.103334&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B232-vision-09-00071">
<span class="label">232.</span><cite>Yoo T.K., Choi J.Y., Kim H.K., Ryu I.H., Kim J.K. Adopting Low-Shot Deep Learning for the Detection of Conjunctival Melanoma Using Ocular Surface Images. Comput. Methods Programs Biomed. 2021;205:106086.  doi: 10.1016/j.cmpb.2021.106086.</cite> [<a href="https://doi.org/10.1016/j.cmpb.2021.106086" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33862570/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput.%20Methods%20Programs%20Biomed.&amp;title=Adopting%20Low-Shot%20Deep%20Learning%20for%20the%20Detection%20of%20Conjunctival%20Melanoma%20Using%20Ocular%20Surface%20Images&amp;author=T.K.%20Yoo&amp;author=J.Y.%20Choi&amp;author=H.K.%20Kim&amp;author=I.H.%20Ryu&amp;author=J.K.%20Kim&amp;volume=205&amp;publication_year=2021&amp;pages=106086&amp;pmid=33862570&amp;doi=10.1016/j.cmpb.2021.106086&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B233-vision-09-00071">
<span class="label">233.</span><cite>Ueno Y., Oda M., Yamaguchi T., Fukuoka H., Nejima R., Kitaguchi Y., Miyake M., Akiyama M., Miyata K., Kashiwagi K., et al.  Deep learning model for extensive smartphone-based diagnosis and triage of cataracts and multiple corneal diseases. Br. J. Ophthalmol. 2024;108:1406–1413. doi: 10.1136/bjo-2023-324488.</cite> [<a href="https://doi.org/10.1136/bjo-2023-324488" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11503034/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38242700/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Br.%20J.%20Ophthalmol.&amp;title=Deep%20learning%20model%20for%20extensive%20smartphone-based%20diagnosis%20and%20triage%20of%20cataracts%20and%20multiple%20corneal%20diseases&amp;author=Y.%20Ueno&amp;author=M.%20Oda&amp;author=T.%20Yamaguchi&amp;author=H.%20Fukuoka&amp;author=R.%20Nejima&amp;volume=108&amp;publication_year=2024&amp;pages=1406-1413&amp;pmid=38242700&amp;doi=10.1136/bjo-2023-324488&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B234-vision-09-00071">
<span class="label">234.</span><cite>Maehara H., Ueno Y., Yamaguchi T., Kitaguchi Y., Miyazaki D., Nejima R., Inomata T., Kato N., Chikama T.I., Ominato J., et al.  Artificial intelligence support improves diagnosis accuracy in anterior segment eye diseases. Sci. Rep. 2025;15:5117.  doi: 10.1038/s41598-025-89768-6.</cite> [<a href="https://doi.org/10.1038/s41598-025-89768-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11814138/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39934383/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Artificial%20intelligence%20support%20improves%20diagnosis%20accuracy%20in%20anterior%20segment%20eye%20diseases&amp;author=H.%20Maehara&amp;author=Y.%20Ueno&amp;author=T.%20Yamaguchi&amp;author=Y.%20Kitaguchi&amp;author=D.%20Miyazaki&amp;volume=15&amp;publication_year=2025&amp;pages=5117&amp;pmid=39934383&amp;doi=10.1038/s41598-025-89768-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B235-vision-09-00071">
<span class="label">235.</span><cite>Taki Y., Ueno Y., Oda M., Kitaguchi Y., Ibrahim O.M.A., Aketa N., Yamaguchi T. Analysis of the performance of the CorneAI for iOS in the classification of corneal diseases and cataracts based on journal photographs. Sci. Rep. 2024;14:15517.  doi: 10.1038/s41598-024-66296-3.</cite> [<a href="https://doi.org/10.1038/s41598-024-66296-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11226423/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38969757/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Analysis%20of%20the%20performance%20of%20the%20CorneAI%20for%20iOS%20in%20the%20classification%20of%20corneal%20diseases%20and%20cataracts%20based%20on%20journal%20photographs&amp;author=Y.%20Taki&amp;author=Y.%20Ueno&amp;author=M.%20Oda&amp;author=Y.%20Kitaguchi&amp;author=O.M.A.%20Ibrahim&amp;volume=14&amp;publication_year=2024&amp;pages=15517&amp;pmid=38969757&amp;doi=10.1038/s41598-024-66296-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B236-vision-09-00071">
<span class="label">236.</span><cite>Kozma K., Jánki Z.R., Bilicki V., Csutak A., Szalai E. Artificial intelligence to enhance the diagnosis of ocular surface squamous neoplasia. Sci. Rep. 2025;15:9550.  doi: 10.1038/s41598-025-94876-4.</cite> [<a href="https://doi.org/10.1038/s41598-025-94876-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11923146/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40108432/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Artificial%20intelligence%20to%20enhance%20the%20diagnosis%20of%20ocular%20surface%20squamous%20neoplasia&amp;author=K.%20Kozma&amp;author=Z.R.%20J%C3%A1nki&amp;author=V.%20Bilicki&amp;author=A.%20Csutak&amp;author=E.%20Szalai&amp;volume=15&amp;publication_year=2025&amp;pages=9550&amp;pmid=40108432&amp;doi=10.1038/s41598-025-94876-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B237-vision-09-00071">
<span class="label">237.</span><cite>Nguena M.B., van den Tweel J.G., Makupa W., Hu V.H., Weiss H.A., Gichuhi S., Burton M.J. Diagnosing ocular surface squamous neoplasia in East Africa: Case-control study of clinical and in vivo confocal microscopy assessment. Ophthalmology. 2014;121:484–491. doi: 10.1016/j.ophtha.2013.09.027.</cite> [<a href="https://doi.org/10.1016/j.ophtha.2013.09.027" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3901930/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24321141/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ophthalmology&amp;title=Diagnosing%20ocular%20surface%20squamous%20neoplasia%20in%20East%20Africa:%20Case-control%20study%20of%20clinical%20and%20in%20vivo%20confocal%20microscopy%20assessment&amp;author=M.B.%20Nguena&amp;author=J.G.%20van%20den%20Tweel&amp;author=W.%20Makupa&amp;author=V.H.%20Hu&amp;author=H.A.%20Weiss&amp;volume=121&amp;publication_year=2014&amp;pages=484-491&amp;pmid=24321141&amp;doi=10.1016/j.ophtha.2013.09.027&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B238-vision-09-00071">
<span class="label">238.</span><cite>Li Z., Wang Y., Qiang W., Wu X., Zhang Y., Gu Y., Chen K., Qi D., Xiu L., Sun Y., et al.  A Domain-Specific Pretrained Model for Detecting Malignant and Premalignant Ocular Surface Tumors: A Multicenter Model Development and Evaluation Study. Research. 2025;8:0711. doi: 10.34133/research.0711.</cite> [<a href="https://doi.org/10.34133/research.0711" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC12104561/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40421109/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Research&amp;title=A%20Domain-Specific%20Pretrained%20Model%20for%20Detecting%20Malignant%20and%20Premalignant%20Ocular%20Surface%20Tumors:%20A%20Multicenter%20Model%20Development%20and%20Evaluation%20Study&amp;author=Z.%20Li&amp;author=Y.%20Wang&amp;author=W.%20Qiang&amp;author=X.%20Wu&amp;author=Y.%20Zhang&amp;volume=8&amp;publication_year=2025&amp;pages=0711&amp;pmid=40421109&amp;doi=10.34133/research.0711&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B239-vision-09-00071">
<span class="label">239.</span><cite>Wang Y., Yao Q., Kwok J.T., Ni L.M. Generalizing from a few examples: A survey on few-shot learning. ACM Comput. Surv. (CSUR) 2020;53:134. doi: 10.1145/3386252.</cite> [<a href="https://doi.org/10.1145/3386252" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Comput.%20Surv.%20(CSUR)&amp;title=Generalizing%20from%20a%20few%20examples:%20A%20survey%20on%20few-shot%20learning&amp;author=Y.%20Wang&amp;author=Q.%20Yao&amp;author=J.T.%20Kwok&amp;author=L.M.%20Ni&amp;volume=53&amp;publication_year=2020&amp;pages=134&amp;doi=10.1145/3386252&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B240-vision-09-00071">
<span class="label">240.</span><cite>Sarkar P., Tripathy K.  Pterygium. StatPearls Publishing; Treasure Island, FL, USA: 2025.  [(accessed on 19 July 2025)].  Available online:  <a href="https://www.ncbi.nlm.nih.gov/books/NBK558907/" class="usa-link" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.ncbi.nlm.nih.gov/books/NBK558907/</a></cite> [<a href="https://pubmed.ncbi.nlm.nih.gov/32644333/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Pterygium&amp;author=P.%20Sarkar&amp;author=K.%20Tripathy&amp;publication_year=2025&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B241-vision-09-00071">
<span class="label">241.</span><cite>Chen B., Fang X.W., Wu M.N., Zhu S.J., Zheng B., Liu B.Q., Wu T., Hong X.Q., Wang J.T., Yang W.H. Artificial intelligence assisted pterygium diagnosis: Current status and perspectives. Int. J. Ophthalmol. 2023;16:1386–1394. doi: 10.18240/ijo.2023.09.04.</cite> [<a href="https://doi.org/10.18240/ijo.2023.09.04" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10475638/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37724272/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int.%20J.%20Ophthalmol.&amp;title=Artificial%20intelligence%20assisted%20pterygium%20diagnosis:%20Current%20status%20and%20perspectives&amp;author=B.%20Chen&amp;author=X.W.%20Fang&amp;author=M.N.%20Wu&amp;author=S.J.%20Zhu&amp;author=B.%20Zheng&amp;volume=16&amp;publication_year=2023&amp;pages=1386-1394&amp;pmid=37724272&amp;doi=10.18240/ijo.2023.09.04&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B242-vision-09-00071">
<span class="label">242.</span><cite>Gan F., Chen W.Y., Liu H., Zhong Y.L. Application of artificial intelligence models for detecting the pterygium that requires surgical treatment based on anterior segment images. Front. Neurosci. 2022;16:1084118.  doi: 10.3389/fnins.2022.1084118.</cite> [<a href="https://doi.org/10.3389/fnins.2022.1084118" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9808075/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36605553/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Neurosci.&amp;title=Application%20of%20artificial%20intelligence%20models%20for%20detecting%20the%20pterygium%20that%20requires%20surgical%20treatment%20based%20on%20anterior%20segment%20images&amp;author=F.%20Gan&amp;author=W.Y.%20Chen&amp;author=H.%20Liu&amp;author=Y.L.%20Zhong&amp;volume=16&amp;publication_year=2022&amp;pages=1084118&amp;pmid=36605553&amp;doi=10.3389/fnins.2022.1084118&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B243-vision-09-00071">
<span class="label">243.</span><cite>Zhu S., Fang X., Qian Y., He K., Wu M., Zheng B., Song J. Pterygium Screening and Lesion Area Segmentation Based on Deep Learning. J. Healthc. Eng. 2022;2022:3942110. doi: 10.1155/2022/3942110.</cite> [<a href="https://doi.org/10.1155/2022/3942110" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9705081/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36451763/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Healthc.%20Eng.&amp;title=Pterygium%20Screening%20and%20Lesion%20Area%20Segmentation%20Based%20on%20Deep%20Learning&amp;author=S.%20Zhu&amp;author=X.%20Fang&amp;author=Y.%20Qian&amp;author=K.%20He&amp;author=M.%20Wu&amp;volume=2022&amp;publication_year=2022&amp;pages=3942110&amp;pmid=36451763&amp;doi=10.1155/2022/3942110&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B244-vision-09-00071">
<span class="label">244.</span><cite>Wan C., Shao Y., Wang C., Jing J., Yang W. A Novel System for Measuring Pterygium’s Progress Using Deep Learning. Front. Med. 2022;9:819971.  doi: 10.3389/fmed.2022.819971.</cite> [<a href="https://doi.org/10.3389/fmed.2022.819971" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8882585/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35237630/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Med.&amp;title=A%20Novel%20System%20for%20Measuring%20Pterygium%E2%80%99s%20Progress%20Using%20Deep%20Learning&amp;author=C.%20Wan&amp;author=Y.%20Shao&amp;author=C.%20Wang&amp;author=J.%20Jing&amp;author=W.%20Yang&amp;volume=9&amp;publication_year=2022&amp;pages=819971&amp;pmid=35237630&amp;doi=10.3389/fmed.2022.819971&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B245-vision-09-00071">
<span class="label">245.</span><cite>Hung K.H., Lin C., Roan J., Kuo C.F., Hsiao C.H., Tan H.Y., Chen H.C., Ma D.H., Yeh L.K., Lee O.K. Application of a Deep Learning System in Pterygium Grading and Further Prediction of Recurrence with Slit Lamp Photographs. Diagnostics. 2022;12:888.  doi: 10.3390/diagnostics12040888.</cite> [<a href="https://doi.org/10.3390/diagnostics12040888" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9029774/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35453936/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diagnostics&amp;title=Application%20of%20a%20Deep%20Learning%20System%20in%20Pterygium%20Grading%20and%20Further%20Prediction%20of%20Recurrence%20with%20Slit%20Lamp%20Photographs&amp;author=K.H.%20Hung&amp;author=C.%20Lin&amp;author=J.%20Roan&amp;author=C.F.%20Kuo&amp;author=C.H.%20Hsiao&amp;volume=12&amp;publication_year=2022&amp;pages=888&amp;pmid=35453936&amp;doi=10.3390/diagnostics12040888&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B246-vision-09-00071">
<span class="label">246.</span><cite>Liu Y., Xu C., Wang S., Chen Y., Lin X., Guo S., Liu Z., Wang Y., Zhang H., Guo Y., et al.  Accurate detection and grading of pterygium through smartphone by a fusion training model. Br. J. Ophthalmol. 2024;108:336–342. doi: 10.1136/bjo-2022-322552.</cite> [<a href="https://doi.org/10.1136/bjo-2022-322552" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10894821/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36858799/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Br.%20J.%20Ophthalmol.&amp;title=Accurate%20detection%20and%20grading%20of%20pterygium%20through%20smartphone%20by%20a%20fusion%20training%20model&amp;author=Y.%20Liu&amp;author=C.%20Xu&amp;author=S.%20Wang&amp;author=Y.%20Chen&amp;author=X.%20Lin&amp;volume=108&amp;publication_year=2024&amp;pages=336-342&amp;pmid=36858799&amp;doi=10.1136/bjo-2022-322552&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B247-vision-09-00071">
<span class="label">247.</span><cite>Fang X., Deshmukh M., Chee M.L., Soh Z.D., Teo Z.L., Thakur S., Goh J.H.L., Liu Y.C., Husain R., Mehta J.S., et al.  Deep learning algorithms for automatic detection of pterygium using anterior segment photographs from slit-lamp and hand-held cameras. Br. J. Ophthalmol. 2022;106:1642–1647. doi: 10.1136/bjophthalmol-2021-318866.</cite> [<a href="https://doi.org/10.1136/bjophthalmol-2021-318866" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9685734/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34244208/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Br.%20J.%20Ophthalmol.&amp;title=Deep%20learning%20algorithms%20for%20automatic%20detection%20of%20pterygium%20using%20anterior%20segment%20photographs%20from%20slit-lamp%20and%20hand-held%20cameras&amp;author=X.%20Fang&amp;author=M.%20Deshmukh&amp;author=M.L.%20Chee&amp;author=Z.D.%20Soh&amp;author=Z.L.%20Teo&amp;volume=106&amp;publication_year=2022&amp;pages=1642-1647&amp;pmid=34244208&amp;doi=10.1136/bjophthalmol-2021-318866&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B248-vision-09-00071">
<span class="label">248.</span><cite>Zheng B., Liu Y., He K., Wu M., Jin L., Jiang Q., Zhu S., Hao X., Wang C., Yang W. Research on an Intelligent Lightweight-Assisted Pterygium Diagnosis Model Based on Anterior Segment Images. Dis. Markers. 2021;2021:7651462. doi: 10.1155/2021/7651462.</cite> [<a href="https://doi.org/10.1155/2021/7651462" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8342163/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34367378/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Dis.%20Markers&amp;title=Research%20on%20an%20Intelligent%20Lightweight-Assisted%20Pterygium%20Diagnosis%20Model%20Based%20on%20Anterior%20Segment%20Images&amp;author=B.%20Zheng&amp;author=Y.%20Liu&amp;author=K.%20He&amp;author=M.%20Wu&amp;author=L.%20Jin&amp;volume=2021&amp;publication_year=2021&amp;pages=7651462&amp;pmid=34367378&amp;doi=10.1155/2021/7651462&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B249-vision-09-00071">
<span class="label">249.</span><cite>Li Z., Wang Z., Xiu L., Zhang P., Wang W., Wang Y., Chen G., Yang W., Chen W. Large language model-based multimodal system for detecting and grading ocular surface diseases from smartphone images. Front. Cell Dev. Biol. 2025;13:1600202.  doi: 10.3389/fcell.2025.1600202.</cite> [<a href="https://doi.org/10.3389/fcell.2025.1600202" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC12141289/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40486905/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Cell%20Dev.%20Biol.&amp;title=Large%20language%20model-based%20multimodal%20system%20for%20detecting%20and%20grading%20ocular%20surface%20diseases%20from%20smartphone%20images&amp;author=Z.%20Li&amp;author=Z.%20Wang&amp;author=L.%20Xiu&amp;author=P.%20Zhang&amp;author=W.%20Wang&amp;volume=13&amp;publication_year=2025&amp;pages=1600202&amp;pmid=40486905&amp;doi=10.3389/fcell.2025.1600202&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B250-vision-09-00071">
<span class="label">250.</span><cite>Seitzman G.D., Prajna L., Prajna N.V., Sansanayudh W., Satitpitakul V., Laovirojjanakul W., Chen C., Zhong L., Ouimette K., Redd T., et al.  Biomarker Detection and Validation for Corneal Involvement in Patients with Acute Infectious Conjunctivitis. JAMA Ophthalmol. 2024;142:865–871. doi: 10.1001/jamaophthalmol.2024.2891.</cite> [<a href="https://doi.org/10.1001/jamaophthalmol.2024.2891" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11327903/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39145969/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=JAMA%20Ophthalmol.&amp;title=Biomarker%20Detection%20and%20Validation%20for%20Corneal%20Involvement%20in%20Patients%20with%20Acute%20Infectious%20Conjunctivitis&amp;author=G.D.%20Seitzman&amp;author=L.%20Prajna&amp;author=N.V.%20Prajna&amp;author=W.%20Sansanayudh&amp;author=V.%20Satitpitakul&amp;volume=142&amp;publication_year=2024&amp;pages=865-871&amp;pmid=39145969&amp;doi=10.1001/jamaophthalmol.2024.2891&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B251-vision-09-00071">
<span class="label">251.</span><cite>Delsoz M., Madadi Y., Munir W.M., Tamm B., Mehravaran S., Soleimani M., Djalilian A., Yousefi S. Performance of ChatGPT in Diagnosis of Corneal Eye Diseases. Cornea. 2024;43:664–670. doi: 10.1097/ICO.0000000000003492.</cite> [<a href="https://doi.org/10.1097/ICO.0000000000003492" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38391243/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cornea&amp;title=Performance%20of%20ChatGPT%20in%20Diagnosis%20of%20Corneal%20Eye%20Diseases&amp;author=M.%20Delsoz&amp;author=Y.%20Madadi&amp;author=W.M.%20Munir&amp;author=B.%20Tamm&amp;author=S.%20Mehravaran&amp;volume=43&amp;publication_year=2024&amp;pages=664-670&amp;pmid=38391243&amp;doi=10.1097/ICO.0000000000003492&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B252-vision-09-00071">
<span class="label">252.</span><cite>Hussain Z.S., Delsoz M., Elahi M., Jerkins B., Kanner E., Wright C., Munir W.M., Soleimani M., Djalilian A., Lao P.A., et al.  Performance of DeepSeek, Qwen 2.5 MAX, and ChatGPT Assisting in Diagnosis of Corneal Eye Diseases, Glaucoma, and Neuro-Ophthalmology Diseases Based on Clinical Case Reports. medRxiv. 2025.  
preprint
.</cite> [<a href="https://doi.org/10.1101/2025.03.14.25323836" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B253-vision-09-00071">
<span class="label">253.</span><cite>Jiao C., Rosas E., Asadigandomani H., Delsoz M., Madadi Y., Raja H., Munir W.M., Tamm B., Mehravaran S., Djalilian A.R., et al.  Diagnostic Performance of Publicly Available Large Language Models in Corneal Diseases: A Comparison with Human Specialists. Diagnostics. 2025;15:1221.  doi: 10.3390/diagnostics15101221.</cite> [<a href="https://doi.org/10.3390/diagnostics15101221" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC12110359/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40428214/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Diagnostics&amp;title=Diagnostic%20Performance%20of%20Publicly%20Available%20Large%20Language%20Models%20in%20Corneal%20Diseases:%20A%20Comparison%20with%20Human%20Specialists&amp;author=C.%20Jiao&amp;author=E.%20Rosas&amp;author=H.%20Asadigandomani&amp;author=M.%20Delsoz&amp;author=Y.%20Madadi&amp;volume=15&amp;publication_year=2025&amp;pages=1221&amp;pmid=40428214&amp;doi=10.3390/diagnostics15101221&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B254-vision-09-00071">
<span class="label">254.</span><cite>Soleimani M., Cheraqpour K., Sadeghi R., Pezeshgi S., Koganti R., Djalilian A.R. Artificial Intelligence and Infectious Keratitis: Where Are We Now? Life. 2023;13:2117.  doi: 10.3390/life13112117.</cite> [<a href="https://doi.org/10.3390/life13112117" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10672455/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38004257/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Life&amp;title=Artificial%20Intelligence%20and%20Infectious%20Keratitis:%20Where%20Are%20We%20Now?&amp;author=M.%20Soleimani&amp;author=K.%20Cheraqpour&amp;author=R.%20Sadeghi&amp;author=S.%20Pezeshgi&amp;author=R.%20Koganti&amp;volume=13&amp;publication_year=2023&amp;pages=2117&amp;pmid=38004257&amp;doi=10.3390/life13112117&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B255-vision-09-00071">
<span class="label">255.</span><cite>Kim M., Sohn H., Choi S., Kim S. Requirements for Trustworthy Artifical Intelligence and its Application in Healthcare. Healthc. Inform. Res. 2021;27:105–113. doi: 10.4258/hir.2023.29.4.315.</cite> [<a href="https://doi.org/10.4258/hir.2023.29.4.315" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10651407/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37964453/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Healthc.%20Inform.%20Res.&amp;title=Requirements%20for%20Trustworthy%20Artifical%20Intelligence%20and%20its%20Application%20in%20Healthcare&amp;author=M.%20Kim&amp;author=H.%20Sohn&amp;author=S.%20Choi&amp;author=S.%20Kim&amp;volume=27&amp;publication_year=2021&amp;pages=105-113&amp;pmid=37964453&amp;doi=10.4258/hir.2023.29.4.315&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B256-vision-09-00071">
<span class="label">256.</span><cite>Liu X., Rivera S.C., Moher D., Calvert M.J., Denniston A.K., The SPIRIT-AI and CONSORT-AI Working Group  Reporting Guidelines for Clinical Trials Evaluating Artificial Intelligence Interventions: The CONSORT-AI Extension. npj Digit. Med. 2020;3:37. doi: 10.1038/s41591-020-1034-x.</cite> [<a href="https://doi.org/10.1038/s41591-020-1034-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=npj%20Digit.%20Med.&amp;title=Reporting%20Guidelines%20for%20Clinical%20Trials%20Evaluating%20Artificial%20Intelligence%20Interventions:%20The%20CONSORT-AI%20Extension&amp;author=X.%20Liu&amp;author=S.C.%20Rivera&amp;author=D.%20Moher&amp;author=M.J.%20Calvert&amp;author=A.K.%20Denniston&amp;volume=3&amp;publication_year=2020&amp;pages=37&amp;doi=10.1038/s41591-020-1034-x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B257-vision-09-00071">
<span class="label">257.</span><cite>Sounderajah V., Ashrafian H., Golub R.M., Shetty S., De Fauw J., Hooft L., Moons K., Collins G., Moher D., Bossuyt P.M., et al.  Developing a Reporting Guideline for Artificial Intelligence-Centred Diagnostic Test Accuracy Studies: The STARD-AI Protocol. BMJ Open. 2021;11:e047709. doi: 10.1136/bmjopen-2020-047709.</cite> [<a href="https://doi.org/10.1136/bmjopen-2020-047709" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8240576/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34183345/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=BMJ%20Open&amp;title=Developing%20a%20Reporting%20Guideline%20for%20Artificial%20Intelligence-Centred%20Diagnostic%20Test%20Accuracy%20Studies:%20The%20STARD-AI%20Protocol&amp;author=V.%20Sounderajah&amp;author=H.%20Ashrafian&amp;author=R.M.%20Golub&amp;author=S.%20Shetty&amp;author=J.%20De%20Fauw&amp;volume=11&amp;publication_year=2021&amp;pages=e047709&amp;pmid=34183345&amp;doi=10.1136/bmjopen-2020-047709&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B258-vision-09-00071">
<span class="label">258.</span><cite>Maile H., Li J.P.O., Gore D., Leucci M., Mulholland P., Hau S., Szabo A., Moghul I., Balaskas K., Fujinami K., et al.  Machine Learning Algorithms to Detect Subclinical Keratoconus: Systematic Review. JMIR Med. Inform. 2021;9:e27363. doi: 10.2196/27363.</cite> [<a href="https://doi.org/10.2196/27363" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8713097/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34898463/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=JMIR%20Med.%20Inform.&amp;title=Machine%20Learning%20Algorithms%20to%20Detect%20Subclinical%20Keratoconus:%20Systematic%20Review&amp;author=H.%20Maile&amp;author=J.P.O.%20Li&amp;author=D.%20Gore&amp;author=M.%20Leucci&amp;author=P.%20Mulholland&amp;volume=9&amp;publication_year=2021&amp;pages=e27363&amp;pmid=34898463&amp;doi=10.2196/27363&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B259-vision-09-00071">
<span class="label">259.</span><cite>Guni A., Sounderajah V., Whiting P., Bossuyt P., Darzi A., Ashrafian H. Revised Tool for the Quality Assessment of Diagnostic Accuracy Studies Using AI (QUADAS-AI): Protocol for a Qualitative Study. JMIR Res. Protoc. 2024;13:e58202. doi: 10.2196/58202.</cite> [<a href="https://doi.org/10.2196/58202" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11447435/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39293047/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=JMIR%20Res.%20Protoc.&amp;title=Revised%20Tool%20for%20the%20Quality%20Assessment%20of%20Diagnostic%20Accuracy%20Studies%20Using%20AI%20(QUADAS-AI):%20Protocol%20for%20a%20Qualitative%20Study&amp;author=A.%20Guni&amp;author=V.%20Sounderajah&amp;author=P.%20Whiting&amp;author=P.%20Bossuyt&amp;author=A.%20Darzi&amp;volume=13&amp;publication_year=2024&amp;pages=e58202&amp;pmid=39293047&amp;doi=10.2196/58202&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B260-vision-09-00071">
<span class="label">260.</span><cite>Kazemzadeh K. Artificial intelligence in ophthalmology: Opportunities, challenges, and ethical considerations. Med. Hypothesis Discov. Innov. Ophthalmol. J. 2025;14:255–272. doi: 10.51329/mehdiophthal1517.</cite> [<a href="https://doi.org/10.51329/mehdiophthal1517" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC12121673/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40453785/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Med.%20Hypothesis%20Discov.%20Innov.%20Ophthalmol.%20J.&amp;title=Artificial%20intelligence%20in%20ophthalmology:%20Opportunities,%20challenges,%20and%20ethical%20considerations&amp;author=K.%20Kazemzadeh&amp;volume=14&amp;publication_year=2025&amp;pages=255-272&amp;pmid=40453785&amp;doi=10.51329/mehdiophthal1517&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B261-vision-09-00071">
<span class="label">261.</span><cite>Li Q., Tan J., Xie H., Zhang X., Dai Q., Li Z., Yan L.L., Chen W. Evaluating the accuracy of the Ophthalmologist Robot for multiple blindness-causing eye diseases: A multicentre, prospective study protocol. BMJ Open. 2024;14:e077859. doi: 10.1136/bmjopen-2023-077859.</cite> [<a href="https://doi.org/10.1136/bmjopen-2023-077859" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10910653/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38431298/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=BMJ%20Open&amp;title=Evaluating%20the%20accuracy%20of%20the%20Ophthalmologist%20Robot%20for%20multiple%20blindness-causing%20eye%20diseases:%20A%20multicentre,%20prospective%20study%20protocol&amp;author=Q.%20Li&amp;author=J.%20Tan&amp;author=H.%20Xie&amp;author=X.%20Zhang&amp;author=Q.%20Dai&amp;volume=14&amp;publication_year=2024&amp;pages=e077859&amp;pmid=38431298&amp;doi=10.1136/bmjopen-2023-077859&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B262-vision-09-00071">
<span class="label">262.</span><cite>Young J.A., Chang C.W., Scales C.W., Menon S.V., Holy C.E., Blackie C.A. Machine Learning Methods Using Artificial Intelligence Deployed on Electronic Health Record Data for Identification and Referral of At-Risk Patients from Primary Care Physicians to Eye Care Specialists: Retrospective, Case-Controlled Study. JMIR AI. 2024;3:e48295. doi: 10.2196/48295.</cite> [<a href="https://doi.org/10.2196/48295" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11041486/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38875582/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=JMIR%20AI&amp;title=Machine%20Learning%20Methods%20Using%20Artificial%20Intelligence%20Deployed%20on%20Electronic%20Health%20Record%20Data%20for%20Identification%20and%20Referral%20of%20At-Risk%20Patients%20from%20Primary%20Care%20Physicians%20to%20Eye%20Care%20Specialists:%20Retrospective,%20Case-Controlled%20Study&amp;author=J.A.%20Young&amp;author=C.W.%20Chang&amp;author=C.W.%20Scales&amp;author=S.V.%20Menon&amp;author=C.E.%20Holy&amp;volume=3&amp;publication_year=2024&amp;pages=e48295&amp;pmid=38875582&amp;doi=10.2196/48295&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>No new data were created or analyzed in this study.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Vision are provided here courtesy of <strong>Multidisciplinary Digital Publishing Institute  (MDPI)</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.3390/vision9030071"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/vision-09-00071.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (525.4 KB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12372148/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12372148/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12372148%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12372148/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12372148/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12372148/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40843795/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12372148/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40843795/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12372148/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12372148/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="GYND3jI3CYoEzf0JMl1QuGFfDolxSVtgSZlcvOJTXbN4VinbOj6cVKFiCPaW8FpC">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
