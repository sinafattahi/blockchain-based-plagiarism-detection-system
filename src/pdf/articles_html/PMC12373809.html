
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Use of computer vision analysis for labeling inattention periods in EEG recordings with visual stimuli - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="15253A818AF20C13053A8100007F1747.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373809/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Use of computer vision analysis for labeling inattention periods in EEG recordings with visual stimuli">
<meta name="citation_author" content="Dmitry Yu Isaev">
<meta name="citation_author_institution" content="Department of Electrical and Computer Engineering, Duke University, Durham, NC USA">
<meta name="citation_author" content="Samantha Major">
<meta name="citation_author_institution" content="Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC USA">
<meta name="citation_author" content="Kimberly L H Carpenter">
<meta name="citation_author_institution" content="Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC USA">
<meta name="citation_author" content="Jordan Grapel">
<meta name="citation_author_institution" content="Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC USA">
<meta name="citation_author" content="Zhuoqing Chang">
<meta name="citation_author_institution" content="Department of Electrical and Computer Engineering, Duke University, Durham, NC USA">
<meta name="citation_author" content="Matias Di Martino">
<meta name="citation_author_institution" content="Universidad Católica del Uruguay, Montevideo, Uruguay">
<meta name="citation_author" content="David Carlson">
<meta name="citation_author_institution" content="Departments of Civil and Environmental Engineering, Biostatistics and Bioinformatics, Duke University, Durham, NC USA">
<meta name="citation_author" content="Geraldine Dawson">
<meta name="citation_author_institution" content="Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC USA">
<meta name="citation_author" content="Guillermo Sapiro">
<meta name="citation_author_institution" content="Department of Electrical and Computer Engineering, Duke University, Durham, NC USA">
<meta name="citation_author_institution" content="Departments of Biomedical Engineering, Computer Science, and Mathematics, Duke University, Durham, NC USA">
<meta name="citation_publication_date" content="2025 Aug 22">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30963">
<meta name="citation_doi" content="10.1038/s41598-025-10511-2">
<meta name="citation_pmid" content="40846872">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373809/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373809/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373809/pdf/41598_2025_Article_10511.pdf">
<meta name="description" content="Electroencephalography (EEG) recordings with visual stimuli require detailed coding to determine the periods of participant’s attention. Here we propose to use a supervised machine learning model and off-the-shelf video cameras only. We extract ...">
<meta name="og:title" content="Use of computer vision analysis for labeling inattention periods in EEG recordings with visual stimuli">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Electroencephalography (EEG) recordings with visual stimuli require detailed coding to determine the periods of participant’s attention. Here we propose to use a supervised machine learning model and off-the-shelf video cameras only. We extract ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373809/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12373809">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-10511-2"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_10511.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373809%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12373809/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12373809/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373809/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 22;15:30963. doi: <a href="https://doi.org/10.1038/s41598-025-10511-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-10511-2</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Use of computer vision analysis for labeling inattention periods in EEG recordings with visual stimuli</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Isaev%20DY%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Dmitry Yu Isaev</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Dmitry Yu Isaev</span></h3>
<div class="p">
<sup>1</sup>Department of Electrical and Computer Engineering, Duke University, Durham, NC USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Isaev%20DY%22%5BAuthor%5D" class="usa-link"><span class="name western">Dmitry Yu Isaev</span></a>
</div>
</div>
<sup>1,</sup><sup>#</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Major%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Samantha Major</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Samantha Major</span></h3>
<div class="p">
<sup>2</sup>Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Major%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Samantha Major</span></a>
</div>
</div>
<sup>2,</sup><sup>✉,</sup><sup>#</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Carpenter%20KLH%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Kimberly L H Carpenter</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Kimberly L H Carpenter</span></h3>
<div class="p">
<sup>2</sup>Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Carpenter%20KLH%22%5BAuthor%5D" class="usa-link"><span class="name western">Kimberly L H Carpenter</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Grapel%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Jordan Grapel</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Jordan Grapel</span></h3>
<div class="p">
<sup>2</sup>Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Grapel%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Jordan Grapel</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chang%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Zhuoqing Chang</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Zhuoqing Chang</span></h3>
<div class="p">
<sup>1</sup>Department of Electrical and Computer Engineering, Duke University, Durham, NC USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chang%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zhuoqing Chang</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Di%20Martino%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Matias Di Martino</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Matias Di Martino</span></h3>
<div class="p">
<sup>3</sup>Universidad Católica del Uruguay, Montevideo, Uruguay </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Di%20Martino%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Matias Di Martino</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Carlson%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">David Carlson</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">David Carlson</span></h3>
<div class="p">
<sup>4</sup>Departments of Civil and Environmental Engineering, Biostatistics and Bioinformatics, Duke University, Durham, NC USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Carlson%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">David Carlson</span></a>
</div>
</div>
<sup>4</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Dawson%20G%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Geraldine Dawson</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Geraldine Dawson</span></h3>
<div class="p">
<sup>2</sup>Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Dawson%20G%22%5BAuthor%5D" class="usa-link"><span class="name western">Geraldine Dawson</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sapiro%20G%22%5BAuthor%5D" class="usa-link" aria-describedby="id9"><span class="name western">Guillermo Sapiro</span></a><div hidden="hidden" id="id9">
<h3><span class="name western">Guillermo Sapiro</span></h3>
<div class="p">
<sup>1</sup>Department of Electrical and Computer Engineering, Duke University, Durham, NC USA </div>
<div class="p">
<sup>5</sup>Departments of Biomedical Engineering, Computer Science, and Mathematics, Duke University, Durham, NC USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sapiro%20G%22%5BAuthor%5D" class="usa-link"><span class="name western">Guillermo Sapiro</span></a>
</div>
</div>
<sup>1,</sup><sup>5</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Department of Electrical and Computer Engineering, Duke University, Durham, NC USA </div>
<div id="Aff2">
<sup>2</sup>Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC USA </div>
<div id="Aff3">
<sup>3</sup>Universidad Católica del Uruguay, Montevideo, Uruguay </div>
<div id="Aff4">
<sup>4</sup>Departments of Civil and Environmental Engineering, Biostatistics and Bioinformatics, Duke University, Durham, NC USA </div>
<div id="Aff5">
<sup>5</sup>Departments of Biomedical Engineering, Computer Science, and Mathematics, Duke University, Durham, NC USA </div>
<div class="author-notes p">
<div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div>
<div class="fn" id="_eqcntrb93pmc__">
<sup>#</sup><p class="display-inline">Contributed equally.</p>
</div>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Jun 25; Accepted 2025 Jul 3; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12373809  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40846872/" class="usa-link">40846872</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Electroencephalography (EEG) recordings with visual stimuli require detailed coding to determine the periods of participant’s attention. Here we propose to use a supervised machine learning model and off-the-shelf video cameras only. We extract computer vision-based features such as head pose, gaze, and face landmarks from the video of the participant, and train the machine learning model (multi-layer perceptron) on an initial dataset, then adapt it with a small subset of data from a new participant. Using a sample size of 23 autistic children with and without co-occurring ADHD (attention-deficit/hyperactivity disorder) aged 49–95 months, and training on additional 2560 labeled frames (equivalent to 85.3 s of the video) of a new participant, the median area under the receiver operating characteristic curve for inattention detection was 0.989 (IQR 0.984–0.993) and the median inter-rater reliability (Cohen’s kappa) with a trained human annotator was 0.888. Agreement with human annotations for nine participants was in the 0.616–0.944 range. Our results demonstrate the feasibility of automatic tools to detect inattention during EEG recordings, and its potential to reduce the subjectivity and time burden of human attention coding. The tool for model adaptation and visualization of the computer vision features is made publicly available to the research community.</p>
<section id="sec1"><h3 class="pmc_sec_title">Supplementary Information</h3>
<p>The online version contains supplementary material available at 10.1038/s41598-025-10511-2.</p></section><section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> EEG, Visual attention, Computer vision, Machine learning, Data processing automation</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Cognitive neuroscience, Attention</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par3">Electroencephalography (EEG) is a widely used method for studying brain-behavior relations. A typical EEG recording session includes visual and/or auditory tasks, which can be presented in an event-related potential (ERP) paradigm or during spontaneous EEG recording. Collecting data using visual tasks in children is significantly more challenging due to their reduced ability to sustain their attention to visual stimuli<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a>,<a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>. The ability to sustain attention during EEG tasks can be especially challenging for children with neurodevelopmental disorders, such as autism and ADHD (attention-deficit/hyperactivity disorder)<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a>,<a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>. A meta-analysis by Stets et al. (2012)<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup> reports that studies involving visual tasks in infants have significantly higher attrition rates than auditory or combined visual and auditory tasks. While reports of attrition rates in different studies vary<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a>,<a href="#CR5" class="usa-link" aria-describedby="CR5">5</a>,<a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>, a general recommendation is to design tasks that will be engaging for children, thereby facilitating the maintenance of visual attention<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>. To facilitate visual attention children may be asked to provide a behavioral response (e.g., press a button)<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a>,<a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup> or an experimenter may gently redirect a child to the screen when noticing signs of disengagement<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a>,<a href="#CR8" class="usa-link" aria-describedby="CR8">8</a>,<a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>.</p>
<p id="Par4">Removing segments of the data during which a participant did not look at the screen is often the first stage of EEG data processing in recordings with visual stimuli. Typically, researchers either code the participant’s attention on-line by pressing a button which sends a marker to the EEG recording when the participant was not attending to the stimulus<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a>,<a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>,<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup> or by recording the video of the participant’s behavior synchronously with the EEG recording and marking periods of inattention post-hoc upon reviewing the video offline<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a>,<a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>. This is a burdensome manual process requiring significant time and effort. It is also highly subjective; for example, the annotator might only see the participant’s face and must guess whether the participant’s gaze is directed to the area inside or outside of the screen. That is, the target of the child’s gaze is inferred based on the angle of the eyes because the recording camera is adjacent to the target screen, so the target screen is not actually captured in the video frame. For this reason, we expect human annotators coding inattention using EEG videos to vary more and have less reliability with one another than traditional video annotating. For example, a previous non-EEG study measuring inattention in young children and infants required human raters was able to obtain at least 90% frame-by-frame agreement before allowing them to work on the videos in the dataset, and their data had a range of agreement from 89.57 to 98.51% across multiple datasets<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>. </p>
<p id="Par5">Subjectivity during this first stage of data processing poses an obstacle for EEG studies, in particular for multi-center ones, since reproducibility and constancy of EEG data quality in multi-center studies are critical<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>,<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>.</p>
<p id="Par6">In addition to its value for data curation, information about inattention periods can be useful for creating clinical biomarkers. There is evidence of alterations in orienting, disengagement from, and sustaining attention to relevant stimuli in autistic children<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a>–<a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>, which undoubtedly influences the amount of inattention during the EEG study. Though a typical EEG study excludes from analysis time periods where the participant is not engaged with the visual stimulus<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a>,<a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>,<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup> inattentiveness during EEG in social/nonsocial stimuli can be a measure that distinguishes autistic and neurotypical children, used alone or in conjunction with EEG power features<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>.</p>
<p id="Par7">Inattention detection has been studied not just within the field of medicine, but also in other contexts such as autonomous driving safety and assisted driving tools. Within the field of autonomous driving, inattention has been operationalized as “insufficient or no attention, to activities critical for safe driving.” Inattention can be divided into five subtypes: restricted attention (due to physical obstructions or blinks), misprioritized attention (attending to a less important feature instead of a potential safety concern), neglected attention (not checking the blind spot), cursory attention (looking in the right direction but failing to process the information), and diverted attention (distraction by driving-related or non-driving-related tasks and events)<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>,<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. Computer vision can detect restricted attention, misprioritized attention, and diverted attention, but not cursory attention. Thus, more research is needed to accurately measure and predict all types of attention automatically.</p>
<p id="Par8">Modern computer vision techniques now enable accurate, real-time inattention detection—often by analyzing facial expressions, eye gaze, and head pose, eschewing use of expensive additional sensors<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>. To date, conventional eye-tracking technologies has been most commonly used for detecting inattention. Simultaneously presenting a stimulus on the eye-tracker screen while recording both eye-tracking and EEG signals enables the detection of a participant’s visual attention directed towards the screen<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>. For example, a study by Maguire et al. (2014)<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup> used an eye-tracker synchronized with EEG to present an “attention-getter” animation in an experiment with 6–8 year old children. They reported increased retention of EEG data compared to the condition where children were asked to provide a behavioral response (button pressing) to facilitate attention. However, eye-tracking equipment can be expensive and requires calibration. Advances in inattention detection solely based on computer vision technology have a promise of substantially reducing the cost and effort of data preprocessing in lab experiments.</p>
<p id="Par9">Here we propose a solution for monitoring attention during EEG acquisition based on computer vision analysis (CVA), which is scalable and less expensive than eye-tracking equipment, requiring only off-the-shelf cameras to objectively measure children’s movement behavior. This is largely enabled by the progress in face detection and estimation of facial landmarks, head pose, and gaze<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a>–<a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>. In non-EEG settings, these tools have been able to detect head turns in response to name<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>, and capture patterns of gaze in a low-cost setting without additional calibration<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a>,<a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup>. In the work of Qian et al. (2022)<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>, supervised machine learning in combination with CVA approaches were applied to detect blink and head movement artifacts detection in a minimally constrained portable EEG setting.</p>
<p id="Par10">A similar tool to the present CVA methodology, iCatcher<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup> is a publicly available supervised deep learning model trained to classify infants’ gaze into three categories (‘left’, ‘right’, and ‘away’) based on facial appearance. iCatcher offers a low-cost, automated alternative to traditional eye-tracking systems and is particularly useful for studies involving young children. iCatcher works best on short to moderately long videos with a single person in the frame. Clear visibility of the face is important, as obstructions like hats or coverings can affect accuracy. While development of toolboxes such as iCatcher have undoubtedly moved the field forward, there are some limitations that do not make it well-suited for videos recorded during EEG sessions. First, the EEG net/cap will obstruct part of the face, possibly below and above the eyes depending on what system is used. Secondly, when running EEG sessions with young children from neurodevelopmental populations, typically multiple research staff and/or parents are involved in helping the child sit still and complete the task, which would invalidate a tool like iCatcher which is optimized for videos with only one human face detectable.</p>
<p id="Par11">In this work, we developed a combination of CVA and a supervised machine learning model to detect inattention periods during the EEG recordings. This is computed from the videos of the child’s head and upper body captured synchronously with EEG and with simple off-the-shelf cameras. While these videos were recorded during EEG sessions, the EEG data itself is not utilized in the present analysis. We hypothesized that automatic CVA codes of eye gaze coordinates, head pose descriptors (pitch, yaw, and roll), and nose landmarks could reliably detect periods of visual distraction from the screen using a supervised machine learning model. The proposed method requires minimal involvement by human annotators to fine-tune the model to a new participant. In this process, a small number of frames from the new participant’s video are labeled by a human, followed by an additional round of model training. Minor human involvement is critical since head poses and facial expressions of children vary significantly in clinical populations, justifying the need and opportunity for tuning the pre-trained model to new participants. Recent work based on iCatcher provides evidence that the lowest agreement between human annotators and automatic models occurs on the label ‘looking away from the screen<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>. Thus, we developed a graphical user interface (GUI) allowing users to label data for fine-tuning, visualize video and corresponding time series of CVA features, and post-process the model results. The post-processing stage gives an opportunity for additional quality control of inattention periods proposed by the model.</p>
<p id="Par12">The proposed approach reduces subjectivity by providing the CVA features for human reference in the labeling process, thus standardizing the information an individual uses in their labeling. It also significantly reduces the coding time by decreasing the number of frames to be labeled manually. We therefore trained the model on an annotated dataset of children’s videos synchronized with EEG recordings, and then fine-tuned it to a new child by labeling a limited amount of randomly selected additional frames on the new video. To evaluate our approach, we trained and fine-tuned on a dataset of 23 videos in the leave-one-subject-out cross-validation setting, training on a dataset of 22 videos, and fine-tuning to a holdout video on each cross-validation iteration. Due to the length of the EEG sessions and the size of the videos created, a similar tool, iCatcher, was unable to handle the current data set. We have shared online the GUI for the video and CVA features inspection, model retraining, and predictions post-processing.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Methods</h2>
<section id="Sec3"><h3 class="pmc_sec_title">Participants</h3>
<p id="Par14">Participants were 23 children (16 males), ranging from 49 to 95 months of age who were part of a study funded by the National Institutes of Health (NICHD 2P50HD093074, Dawson, PI). The ethnic and racial composition of the sample was as follows: White, 17; Black, 0; Asian, 2; other and mixed race, 4; Hispanic, 4. All 23 children met DSM-5 criteria for autism spectrum disorder (ASD) based on the Autism Diagnostic Observation Schedule-2nd Edition<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup> by an experienced, research reliable psychologist. Eleven of the 23 children were diagnosed with co-occurring attention deficit/hyperactivity disorder (ADHD) based on a comprehensive clinical evaluation by a clinical psychologist with expertise in ADHD. Children had a mean Full-Scale IQ of 78.5 (SD = 25.5) based on the GCA Standard Score derived from Differential Ability Scales Second Edition<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>.</p>
<p id="Par15">All caregivers/legal guardians of participants gave written, informed consent and the study protocol was approved by the Duke University Health System Institutional Review Board (Protocol numbers Pro00085435 and Pro00085156). Informed consent was obtained from the subjects and/or their legal guardian(s) for publication of identifying information/images in an online open-access publication. Methods were carried out in accordance with institutional, State, and Federal guidelines and regulations. The procedures in these studies adhere to the tenants of the Declaration of Helsinki. Additionally, the caregiver of the participant whose video was used in the Supplementary Materials, as well as blurred in the Figures, provided consent to use the materials in publication. All other data presented have been anonymized.</p></section><section id="Sec4"><h3 class="pmc_sec_title">Recording synchronized video and EEG</h3>
<p id="Par17">Continuous EEG and event-related potentials (ERPs) were recorded as part of an EEG study while simultaneous video recording of the session was underway. EEG sessions were aborted early if the child could not comply with study procedures. Videos recorded during the EEG sessions were 00:04:15 to 00:31:09 in duration. One or two clinical research assistants were present in the room during the EEG recording to ensure the quality of the session and to gently redirect the participant’s attention back to the screen in case they were distracted. The child’s face was recorded from a Basler ACE acA1300-30uc camera below the screen synchronized with the EEG. The camera resolution was 1296 × 966 pixels and the frame rate was 30 fps. To synchronize the camera and EEG, an in-house software code was used, based on the Basler <em>pylon</em> library and Cedrus StimTracker hardware device used to set markers on the EEG recording. A diagram of the recording setup is shown in Fig. <a href="#Fig1" class="usa-link">1</a>.</p>
<figure class="fig xbox font-sm" id="Fig1"><h4 class="obj_head">Fig. 1.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373809_41598_2025_10511_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8046/12373809/a51c8a3f3346/41598_2025_10511_Fig1_HTML.jpg" loading="lazy" id="d33e478" height="475" width="669" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Recording setup. Video from the camera is recorded on Video Recording Computer, which sends a marker to the EEG Recording Computer via Cedrus Stimtracker every 100 frames. This allows for synchronization between the EEG and video recordings.</p></figcaption></figure></section><section id="Sec5"><h3 class="pmc_sec_title">Extracting CVA features</h3>
<p id="Par20">To extract the CVA features, we used in-house code involving three steps: (a) face detection and disambiguation, (b) extraction of landmarks and head pose angles, and (c) gaze estimation. The raw set of extracted features per frame included nose <em>x</em> (horizontal) and <em>y</em> (vertical) coordinates in the frame, gaze <em>x</em> and <em>y</em> coordinates in the presentation screen plane, and head pose angles (pitch, yaw, and roll).</p></section><section id="Sec6"><h3 class="pmc_sec_title">Face detection and disambiguation</h3>
<p id="Par22">Code for face detection and disambiguation used the <em>face_recognition</em> python library based on the <em>dlib</em> C + + library<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>. Every time the algorithm detected more than one face on the video (which happened either due to ambiguity of face detection – one face was detected twice, or when another person, e.g., research assistant(s) or parent(s) entered the frame), the algorithm showed the frame with a bounding box and prompted the user to select the correct participant’s face.</p></section><section id="Sec7"><h3 class="pmc_sec_title">Extraction of landmarks and head pose angles</h3>
<p id="Par24">After the faces were detected, an algorithm for facial landmark extraction based on the <em>intraface</em> software library<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> was applied to the detected faces. As a result, facial landmark pixel coordinates, as well as pitch, yaw, and roll head pose angles were obtained.</p>
<p id="Par25"><em>Gaze estimation.</em> The iTracker software<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup> was used for gaze estimation, providing gaze <em>x</em> and gaze <em>y</em> coordinates in the screen plane. Even though iTracker was trained to predict gaze coordinates on a mobile device screen using the mobile device frontal camera, we used the output of iTracker as a proxy for gaze coordinates in the presentation screen plane. The software package was modular and this component can be easily replaced by others as preferred by the user.</p>
<p id="Par26">Since the <em>intraface</em> library was not currently available to the general public, for the convenience of potential users we make publicly available an alternative processing pipeline which consists of our original face estimation and disambiguation code, and a code for landmarks, head pose and gaze extraction using the popular OpenFace software package<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>.</p></section><section id="Sec8"><h3 class="pmc_sec_title">Data attrition</h3>
<p id="Par28">Due to pauses between EEG/ERP recordings where the behavior of participants varied significantly, inattention detection was restricted only to the periods during the actual stimuli presentation, excluding breaks between stimuli, and the training set for the machine learning (ML) model included only data from frames inside those periods. Frames where the face could not be detected (hence there was no information on landmarks and head pose) were excluded from the analysis.</p></section><section id="Sec9"><h3 class="pmc_sec_title">Data pre-processing</h3>
<p id="Par30">Since inattention could happen in any direction (either when participants looked to the right or left, turned the head up or down, etc.), each feature for each participant was transformed into a positive (‘plus’; Eq. (<a href="#Equ1" class="usa-link">1</a>)) and negative (‘minus’; Eq. (<a href="#Equ2" class="usa-link">2</a>)) version,</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8046/12373809/34e77a75edcf/d33e560.gif" loading="lazy" id="d33e560" alt="graphic file with name d33e560.gif"></td>
<td class="label">1</td>
</tr></table>
<table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8046/12373809/6a6253716cd2/d33e567.gif" loading="lazy" id="d33e567" alt="graphic file with name d33e567.gif"></td>
<td class="label">2</td>
</tr></table>
<p id="Par32">The final set of features for the analysis are reported in Table <a href="#Tab1" class="usa-link">1</a>.</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>List of input features per frame for the machine learning model.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Feature name</th>
<th align="left" colspan="1" rowspan="1">Feature description</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">noseX<sub>plus</sub>
</td>
<td align="left" rowspan="4" colspan="1">Nose coordinates</td>
</tr>
<tr><td align="left" colspan="1" rowspan="1">noseX<sub>minus</sub>
</td></tr>
<tr><td align="left" colspan="1" rowspan="1">noseY<sub>plus</sub>
</td></tr>
<tr><td align="left" colspan="1" rowspan="1">noseY<sub>minus</sub>
</td></tr>
<tr>
<td align="left" colspan="1" rowspan="1">gazeX<sub>plus</sub>
</td>
<td align="left" rowspan="4" colspan="1">Gaze coordinates</td>
</tr>
<tr><td align="left" colspan="1" rowspan="1">gazeX<sub>minus</sub>
</td></tr>
<tr><td align="left" colspan="1" rowspan="1">gazeY<sub>plus</sub>
</td></tr>
<tr><td align="left" colspan="1" rowspan="1">gazeY<sub>minus</sub>
</td></tr>
<tr>
<td align="left" colspan="1" rowspan="1">yaw<sub>plus</sub>
</td>
<td align="left" rowspan="6" colspan="1">Head pose angles</td>
</tr>
<tr><td align="left" colspan="1" rowspan="1">yaw<sub>minus</sub>
</td></tr>
<tr><td align="left" colspan="1" rowspan="1">pitch<sub>plus</sub>
</td></tr>
<tr><td align="left" colspan="1" rowspan="1">pitch<sub>minus</sub>
</td></tr>
<tr><td align="left" colspan="1" rowspan="1">roll<sub>plus</sub>
</td></tr>
<tr><td align="left" colspan="1" rowspan="1">roll<sub>minus</sub>
</td></tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par34">After pre-processing the features, the participant identifier was one-hot encoded and added to the feature list. This allowed learning a separate bias term in the first layer of the trained neural network, resembling the design of mixed models. The number of categories for one-hot encoding was one more than the number of participants, with the assumption that the identifier of the participant whose data was used for model fine-tuning and prediction was encoded in the last category.</p></section><section id="Sec10"><h3 class="pmc_sec_title">Data labeling</h3>
<p id="Par36">Data for all 23 participants was labeled by one of the co-authors using the Elan v. 6.3 software. Nine (39%) participants were selected for independent annotation by another co-author. Neither annotator participated in data analysis. Annotators labeled data using the recorded video as ‘gaze off screen’ if the participant looked away from the screen, and/or as ‘head turn’ if the participant turned their head. For the purpose of inattention detection, a frame was labeled as ‘inattention’ if it either was labeled as a head turn or gaze off screen. Annotators included eye blinks within periods of inattention and ignored eye-blinks when the participant was visually attending; eye blinks did not interrupt or break-up inattention events. Agreement on inattention labels between independent annotators was assessed with Cohen’s kappa<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>.</p></section><section id="Sec11"><h3 class="pmc_sec_title">Training and evaluating machine learning model</h3>
<p id="Par38">Model inputs were features extracted from each video frame. We utilized a multi-layer perceptron (MLP) model with an input size of 37 features, two hidden layers (layer dimensions 512 and 14 were selected empirically following information bottleneck principle)<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>, and a temperature scaling layer for model calibration<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>. –<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup> The target variable for model training was each video frame’s inattention label, namely <em>inattention-present</em> or <em>inattention-absent</em>. The output was a one-hot encoding of binary inattention signal (dimension = 2). Cross-entropy loss was a cost function. Adam optimizer was used for model training<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>. We used weighted sampling for model training to allow each batch to have approximately equal amounts of positive and negative samples (inattention and attention respectively). Models were trained in the <em>pytorch</em> framework<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>. Evaluation was done using the leave-one-subject-out cross-validation (LOSO CV) method. To evaluate the model performance, we assessed average precision (AP, also known as area under precision-recall curve), area under the ROC curve (AUC), and maximal Cohen’s kappa (MK) between the human annotator and the machine learning predictions per participant across different thresholds. Additionally, we evaluated median Cohen’s kappa across the entire distribution at the range of thresholds between 0 and 1. This allowed us to assess the value of the threshold needed to achieve the best agreement between the model and the human coder over the entire distribution, without adjusting the threshold for each individual participant.</p></section><section id="Sec12"><h3 class="pmc_sec_title">Transfer learning: adjusting ML model to a new participant</h3>
<p id="Par40">Our adaptation approach involved selecting a batch of 128 frames (corresponding to 4.270 s) for labeling and training for 20 epochs (full cycles over the entire labeled dataset) on newly labeled data at each iteration of additional training. To evaluate the performance of this approach, we assessed the three metrics defined in the previous section, considering both sequential (where frame features and labels are sampled into the batch sequentially from the beginning of the video, which resembles how humans would look through the dataset and label it), and random frame sampling approaches. We additionally assessed the maximum of median Cohen’s kappa across distribution, and computed the respective prediction threshold at iterations 5, 10 and 20, which correspond to 21.3, 42.6, and 85.3 additionally labeled seconds of data per participant. The exact algorithm was as follows:</p>
<ol class="list" style="list-style-type:decimal">
<li><p id="Par42">Set <em>N</em> = 128 (the batch size).</p></li>
<li><p id="Par43">Create empty dataset for labeled data.</p></li>
<li><p id="Par44">Set <em>Iteration</em> = 0.</p></li>
<li><p id="Par45">Predict probabilities of sample being positive in each frame.</p></li>
<li><p id="Par46">If the approach is Random sampling, randomly sample N frames into the batch from the participant’s data.</p></li>
<li><p id="Par47">If the approach is Sequential sampling, sample next <em>N</em> frames from the beginning of the participant’s data into the batch.</p></li>
<li><p id="Par48">Remove frames included in the batch from the participant’s data.</p></li>
<li><p id="Par49">Add batch to the labeled dataset (for training in LOSO CV framework we used the labels from the dataset for the participant the algorithm was being trained on).</p></li>
<li><p id="Par50">Train for 20 epochs on the labeled dataset.</p></li>
<li><p id="Par51">Compute AP, AUC, and MK.</p></li>
<li><p id="Par52">Set <em>Iteration</em> + = 1.</p></li>
<li><p id="Par53">If <em>Iteration</em> = = 50: Stop.</p></li>
<li><p id="Par54">Go to 4.</p></li>
</ol></section><section id="Sec13"><h3 class="pmc_sec_title">Agreement measurements between model and human and between two humans</h3>
<p id="Par56">We used Cohen’s kappa as a metric of quality assessment for the human annotations. We selected nine participants and performed independent labeling by a second human annotator. We then computed Cohen’s kappa to measure agreement between both human annotators. We additionally computed Cohen’s kappa between the primary human annotator and the model prediction on a threshold level corresponding to maximal median kappa at iteration 20.</p></section><section id="Sec14"><h3 class="pmc_sec_title">Graphical user interface for visualizing and retraining the model</h3>
<p id="Par58">We created a web-based GUI which allows for visualizing the data, labeling the data frame-by-frame and re-training the model in the random sampling framework, and post-processing the data (see Fig. <a href="#Fig2" class="usa-link">2</a> for screenshot, and Supplementary Materials online for video (Supplementary Video S1) of how the tool works). The tool is based on open-source tools ‘plotly’ (<a href="https://plotly.com/python/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plotly.com/python/</a>) and ‘dash’ (<a href="https://dash.plotly.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://dash.plotly.com/</a>).</p>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373809_41598_2025_10511_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8046/12373809/8b3c3d40e009/41598_2025_10511_Fig2_HTML.jpg" loading="lazy" id="d33e835" height="190" width="669" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>A: Visualization of CVA features together with the video of the participant. B: Interface for labeling the frames. Image is included for educational purposes and the individual’s diagnostic cohort is not specified.</p></figcaption></figure></section></section><section id="Sec15"><h2 class="pmc_sec_title">Results</h2>
<section id="Sec16"><h3 class="pmc_sec_title">Dataset statistics</h3>
<p id="Par60">The full dataset consisted of 566,043 frames (videos were 00:04:15 to 00:31:09 in duration). After excluding frames where the face or gaze were not detected, 535,539 frames were retained (5.38% of frames were invalid), with an average of 23,284 and a standard deviation of 6,193 frames per participant. Of all the frames, 79,629 were labeled as inattention (14.86% of the dataset).</p></section><section id="Sec17"><h3 class="pmc_sec_title">Transfer learning results</h3>
<p id="Par61">The results of transfer learning can be seen in Table <a href="#Tab2" class="usa-link">2</a>; Fig. <a href="#Fig3" class="usa-link">3</a>. The sequential sampling approach performed substantially worse than the random sampling approach. Median AP, AUC and MK were 0.855, 0.965, 0.742 respectively at the start of the training (no adaptation to the participants yet). By iteration 20, median AP was 0.962, AUC 0.989, and MK 0.888 on random sampling approach as compared to median AP 0.640, AUC 0.862, and MK 0.548 in sequential sampling approach.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Average precision, AUC, and maximal cohen’s kappa percentiles at different iterations with two sampling/adaptation alternatives. The random sampling approach outperforms the sequential sampling one on all three metrics on each listed iteration.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" colspan="1" rowspan="1"></th>
<th align="left" colspan="1" rowspan="1"></th>
<th align="left" colspan="3" rowspan="1">Average precision<br>(percentile)</th>
<th align="left" colspan="3" rowspan="1">AUC<br>(percentile)</th>
<th align="left" colspan="3" rowspan="1">Maximal Cohen’s kappa<br>(percentile)</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">Sampling approach</th>
<th align="left" colspan="1" rowspan="1">Iteration</th>
<th align="left" colspan="1" rowspan="1">50%</th>
<th align="left" colspan="1" rowspan="1">25%</th>
<th align="left" colspan="1" rowspan="1">75%</th>
<th align="left" colspan="1" rowspan="1">50%</th>
<th align="left" colspan="1" rowspan="1">25%</th>
<th align="left" colspan="1" rowspan="1">75%</th>
<th align="left" colspan="1" rowspan="1">50%</th>
<th align="left" colspan="1" rowspan="1">25%</th>
<th align="left" colspan="1" rowspan="1">75%</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">No Fine Tuning</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">0.855</td>
<td align="left" colspan="1" rowspan="1">0.715</td>
<td align="left" colspan="1" rowspan="1">0.913</td>
<td align="left" colspan="1" rowspan="1">0.965</td>
<td align="left" colspan="1" rowspan="1">0.948</td>
<td align="left" colspan="1" rowspan="1">0.971</td>
<td align="left" colspan="1" rowspan="1">0.742</td>
<td align="left" colspan="1" rowspan="1">0.646</td>
<td align="left" colspan="1" rowspan="1">0.796</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">Random sampling</td>
<td align="left" colspan="1" rowspan="1">5</td>
<td align="left" colspan="1" rowspan="1">0.906</td>
<td align="left" colspan="1" rowspan="1">0.820</td>
<td align="left" colspan="1" rowspan="1">0.948</td>
<td align="left" colspan="1" rowspan="1">0.973</td>
<td align="left" colspan="1" rowspan="1">0.960</td>
<td align="left" colspan="1" rowspan="1">0.981</td>
<td align="left" colspan="1" rowspan="1">0.798</td>
<td align="left" colspan="1" rowspan="1">0.753</td>
<td align="left" colspan="1" rowspan="1">0.873</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">10</td>
<td align="left" colspan="1" rowspan="1">0.930</td>
<td align="left" colspan="1" rowspan="1">0.875</td>
<td align="left" colspan="1" rowspan="1">0.969</td>
<td align="left" colspan="1" rowspan="1">0.984</td>
<td align="left" colspan="1" rowspan="1">0.975</td>
<td align="left" colspan="1" rowspan="1">0.991</td>
<td align="left" colspan="1" rowspan="1">0.838</td>
<td align="left" colspan="1" rowspan="1">0.798</td>
<td align="left" colspan="1" rowspan="1">0.898</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">20</td>
<td align="left" colspan="1" rowspan="1">0.962</td>
<td align="left" colspan="1" rowspan="1">0.931</td>
<td align="left" colspan="1" rowspan="1">0.981</td>
<td align="left" colspan="1" rowspan="1">0.989</td>
<td align="left" colspan="1" rowspan="1">0.984</td>
<td align="left" colspan="1" rowspan="1">0.993</td>
<td align="left" colspan="1" rowspan="1">0.888</td>
<td align="left" colspan="1" rowspan="1">0.865</td>
<td align="left" colspan="1" rowspan="1">0.925</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">Sequential sampling</td>
<td align="left" colspan="1" rowspan="1">5</td>
<td align="left" colspan="1" rowspan="1">0.400</td>
<td align="left" colspan="1" rowspan="1">0.280</td>
<td align="left" colspan="1" rowspan="1">0.720</td>
<td align="left" colspan="1" rowspan="1">0.788</td>
<td align="left" colspan="1" rowspan="1">0.638</td>
<td align="left" colspan="1" rowspan="1">0.890</td>
<td align="left" colspan="1" rowspan="1">0.380</td>
<td align="left" colspan="1" rowspan="1">0.236</td>
<td align="left" colspan="1" rowspan="1">0.561</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">10</td>
<td align="left" colspan="1" rowspan="1">0.575</td>
<td align="left" colspan="1" rowspan="1">0.408</td>
<td align="left" colspan="1" rowspan="1">0.782</td>
<td align="left" colspan="1" rowspan="1">0.835</td>
<td align="left" colspan="1" rowspan="1">0.731</td>
<td align="left" colspan="1" rowspan="1">0.908</td>
<td align="left" colspan="1" rowspan="1">0.482</td>
<td align="left" colspan="1" rowspan="1">0.251</td>
<td align="left" colspan="1" rowspan="1">0.637</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">20</td>
<td align="left" colspan="1" rowspan="1">0.640</td>
<td align="left" colspan="1" rowspan="1">0.408</td>
<td align="left" colspan="1" rowspan="1">0.801</td>
<td align="left" colspan="1" rowspan="1">0.862</td>
<td align="left" colspan="1" rowspan="1">0.771</td>
<td align="left" colspan="1" rowspan="1">0.930</td>
<td align="left" colspan="1" rowspan="1">0.548</td>
<td align="left" colspan="1" rowspan="1">0.354</td>
<td align="left" colspan="1" rowspan="1">0.678</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373809_41598_2025_10511_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8046/12373809/b86a093db30b/41598_2025_10511_Fig3_HTML.jpg" loading="lazy" id="d33e1072" height="258" width="669" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Average precision, Maximal Cohen’s kappa and AUC per each iteration using different sampling/adaptation methods. Line color is median, and shaded area is interquartile range per each iteration.</p></figcaption></figure></section><section id="Sec18"><h3 class="pmc_sec_title">Cohen’s kappa analysis</h3>
<p id="Par64">Cohen’s kappa at different levels of prediction threshold for both sampling approaches (random and sequential) at iterations 5, 10, and 20 are shown in Fig. <a href="#Fig4" class="usa-link">4</a>. Thresholds at the highest median kappa and the corresponding median kappa values are shown in Tables <a href="#Tab2" class="usa-link">2</a> and <a href="#Tab3" class="usa-link">3</a>. The highest median kappa ranged between 0.792 and 0.888 in the random sampling approach, and between 0.223 and 0.426 in the sequential one. Figure <a href="#Fig4" class="usa-link">4</a> shows that the median Cohen’s kappa stayed relatively stable and high in the range of thresholds between 0.2 and 0.8, allowing for a general threshold for the model predictions to be set in this range.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373809_41598_2025_10511_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8046/12373809/7fd44b958c59/41598_2025_10511_Fig4_HTML.jpg" loading="lazy" id="d33e1098" height="267" width="669" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Median (thick line) and Interquartile Range (shaded area) of Cohen’s kappa at different threshold levels at iterations 5, 10, and 20.</p></figcaption></figure><section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Thresholds and cohen’s kappa levels at highest median value of kappa in the two sampling approaches at iterations 5,10,20.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Sampling approach</th>
<th align="left" colspan="1" rowspan="1">Iteration</th>
<th align="left" colspan="1" rowspan="1">Threshold</th>
<th align="left" colspan="1" rowspan="1">Median Cohen’s kappa</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="3" colspan="1">Random sampling</td>
<td align="center" colspan="1" rowspan="1">5</td>
<td align="center" colspan="1" rowspan="1">0.310</td>
<td align="center" colspan="1" rowspan="1">0.792</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1">10</td>
<td align="center" colspan="1" rowspan="1">0.484</td>
<td align="center" colspan="1" rowspan="1">0.838</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1">20</td>
<td align="center" colspan="1" rowspan="1">0.424</td>
<td align="center" colspan="1" rowspan="1">0.888</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">Sequential sampling</td>
<td align="center" colspan="1" rowspan="1">5</td>
<td align="center" colspan="1" rowspan="1">0.004</td>
<td align="center" colspan="1" rowspan="1">0.223</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1">10</td>
<td align="center" colspan="1" rowspan="1">0.008</td>
<td align="center" colspan="1" rowspan="1">0.296</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1">20</td>
<td align="center" colspan="1" rowspan="1">0.020</td>
<td align="center" colspan="1" rowspan="1">0.426</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par67">A second independent annotator labeled videos from nine participants, which in total accounted for 209,556 frames or 39% of the data set. It took the second annotator approximately 33 h to label nine videos, resulting in an average of 0.57 s spent per frame. Cohen’s kappa values between the two human annotators ranged between 0.548 and 0.859 (see Table <a href="#Tab4" class="usa-link">4</a>).</p>
<section class="tw xbox font-sm" id="Tab4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>Agreement level (Cohen’s kappa) between human annotators, and between the primary annotator and the models adapted by random sampling at iterations 5, 10 and 20.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Participant</th>
<th align="left" colspan="1" rowspan="1">Agreement between annotators</th>
<th align="left" colspan="1" rowspan="1">Agreement  <br>(model, annotator 1)<br>– iteration 5</th>
<th align="left" colspan="1" rowspan="1">Agreement<br>(model, annotator 1)<br>– iteration 10</th>
<th align="left" colspan="1" rowspan="1">Agreement <br>(model, annotator 1)<br>– iteration 20</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">PT01</td>
<td align="center" colspan="1" rowspan="1">0.584</td>
<td align="center" colspan="1" rowspan="1">0.498</td>
<td align="center" colspan="1" rowspan="1">0.541</td>
<td align="center" colspan="1" rowspan="1">0.616</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PT02</td>
<td align="center" colspan="1" rowspan="1">0.748</td>
<td align="center" colspan="1" rowspan="1">0.736</td>
<td align="center" colspan="1" rowspan="1">0.768</td>
<td align="center" colspan="1" rowspan="1">0.753</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PT03</td>
<td align="center" colspan="1" rowspan="1">0.600</td>
<td align="center" colspan="1" rowspan="1">0.868</td>
<td align="center" colspan="1" rowspan="1">0.862</td>
<td align="center" colspan="1" rowspan="1">0.895</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PT04</td>
<td align="center" colspan="1" rowspan="1">0.593</td>
<td align="center" colspan="1" rowspan="1">0.705</td>
<td align="center" colspan="1" rowspan="1">0.766</td>
<td align="center" colspan="1" rowspan="1">0.828</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PT09</td>
<td align="center" colspan="1" rowspan="1">0.727</td>
<td align="center" colspan="1" rowspan="1">0.856</td>
<td align="center" colspan="1" rowspan="1">0.888</td>
<td align="center" colspan="1" rowspan="1">0.943</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PT10</td>
<td align="center" colspan="1" rowspan="1">0.548</td>
<td align="center" colspan="1" rowspan="1">0.665</td>
<td align="center" colspan="1" rowspan="1">0.721</td>
<td align="center" colspan="1" rowspan="1">0.785</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PT16</td>
<td align="center" colspan="1" rowspan="1">0.844</td>
<td align="center" colspan="1" rowspan="1">0.954</td>
<td align="center" colspan="1" rowspan="1">0.953</td>
<td align="center" colspan="1" rowspan="1">0.963</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PT18</td>
<td align="center" colspan="1" rowspan="1">0.859</td>
<td align="center" colspan="1" rowspan="1">0.8</td>
<td align="center" colspan="1" rowspan="1">0.78</td>
<td align="center" colspan="1" rowspan="1">0.789</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PT20</td>
<td align="center" colspan="1" rowspan="1">0.793</td>
<td align="center" colspan="1" rowspan="1">0.937</td>
<td align="center" colspan="1" rowspan="1">0.942</td>
<td align="center" colspan="1" rowspan="1">0.944</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par69">Agreement between the primary annotator and the model adapted by random sampling increased with each iteration of additional training and was in the ranges [0.498–0.954] at iteration 5, [0.541–0.953] at iteration 10, and [0.616–0.963] at iteration 20 (Table <a href="#Tab4" class="usa-link">4</a>).</p></section><section id="Sec19"><h3 class="pmc_sec_title">GUI for visualizing and preprocessing pipeline</h3>
<p id="Par71">We developed a web-based GUI which may be used for reviewing the CVA features of the video, additional labeling of frames and retraining the model, and post-processing of the data, including setting the model decision threshold and rejection of falsely detected inattention events. We make publicly available a pipeline for data pre-processing based on in-house code for face detection and OpenFace framework for head pose and gaze estimation<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>.</p></section></section><section id="Sec20"><h2 class="pmc_sec_title">Discussion</h2>
<p id="Par73">In this work we proposed a method for detection of periods of inattention to visual stimuli during EEG recordings. The tool was based on the CVA of videos of participants’ movement behavior which were synchronously recorded with EEG. We outlined a data processing pipeline, including face and facial landmarks detection, head pose computation, and gaze estimation. We proposed a MLP model for predicting inattention from these CVA features, and random sampling as a means for fine-tuning the model for each participant. We made publicly available a GUI that allows for visualization of the CVA features, model fine-tuning, prediction thresholds adjustment, and results post-processing. While we utilized a sample of children with neurodevelopmental conditions to test the tool, we expect it will also work well on other research populations, including neurotypical children and adults. Features included gaze coordinates, pitch, yaw, roll, and nose coordinates; the nose landmark was particularly useful for detecting inattention that included a head turn to the side.</p>
<p id="Par74">The proposed random frame sampling approach for model adaptation to the participant outperformed the sequential sampling approach. For the non-fine-tuned model, maximal Cohen’s kappa was 0.742, placing the best potential agreement with the human rater in the ‘substantial’ range<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup>. Compared to the initial non-fine-tuned model prediction, the model trained on additional 2560 labeled frames (equivalent to labeling only about 85 s of the video) significantly improved performance, as indicated by all quality metrics. On the other hand, sequential frame sampling performance decreased in the initial five iterations (see Fig. <a href="#Fig3" class="usa-link">3</a>), then gradually improved, but did not reach the performance of the random sampling approach. The reasons behind this included the strong temporal correlation of the features, hence low variability in the new input data, and the rare occurrence of inattention (prevalence of inattention is 14.86%), causing the absence of positive labels in many batches.</p>
<p id="Par75">In line with a previous study<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>, we found that agreement on inattention labeling by human coders was in the ‘moderate’ to ‘substantial’ ranges in seven participants, and in the ‘perfect’ range for two participants<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup>. Labeling inattention is a challenging task for humans, likely because annotators need to make a subjective judgement regarding the boundaries of the stimulus presentation screen. The provided GUI tool allows for visualization the raw CVA features together with the participant’s video, also enabling coders to label frames for the fine-tuning or post-processing stage. When the annotator needs to make a decision on an ambiguous frame, they can play the video to compare the frame in question with neighboring frames, which may help to better evaluate whether the participant was attending to the screen.</p>
<p id="Par76">Our results show that the proposed approach makes annotation substantially more efficient. The existing human labeling system takes on average 220 min per video. Given that additional labeling takes about 0.57 s per frame, the need to label only about 2560 frames for a high quality labeling can significantly increase efficiency, reducing the effort to 24 min on average. Modularity of the tool we developed allows users to utilize any input/output compatible CVA pipeline and machine learning model, while keeping the same GUI. The initial model can be retrained as the amount of labeled data increase.</p>
<p id="Par77">Using the same prediction model and tool for discarding inattention periods may facilitate multi-center studies by unifying the data pre-processing pipelines. Another way to facilitate multi-center studies is to perform pre-processing and labeling of the data in each center separately, and then share only the CVA features and annotations for training of the model with larger amounts of data. Such an approach helps to preserve the privacy of the data in each center, allowing centers to share only specific de-identified CVA features.</p>
<p id="Par79">A limitation of the present study is the absence of a published model and our original full pre-processing pipeline. The reason for this is the removal of the <em>intraface</em> library<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> from public access. We provided the code for an alternative pre-processing pipeline predicting the same features based on the publicly available OpenFace library, and the model structure and interface needed for full integration into the GUI. Additionally, another limitation of the current study is that we calculated performance using annotations from a primary annotator who coded all the videos; while we were able to bolster confidence in the primary annotator’s performance by recruiting a second annotator to re-code 39% of the videos, full confidence could only be achieved by having multiple annotators code all available videos. However, in pediatric psychology research, it has been suggested that having 10–25% of videos re-coded by a second annotator is sufficient, depending on how variable the behavior being coded is<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a>,<a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup>. A potential future direction is to work with the missing data caused by an inability to detect a face in the video. CVA could not detect the face in 5.38% of the frames in our dataset, likely due to either extreme angles of the head with respect to the camera or because of face occlusions. Future studies may attempt to associate these periods with attention/inattention to the screen by using imputation/interpolation methods.</p>
<p id="Par80">We presented a low-cost scalable approach to inattention detection during EEG recordings using CVA, and made a publicly available tool for visualization, model fine-tuning, and post-processing of the system’s results. We also made publicly available an example of the computer vision analysis pipeline which can be used in future studies. We showed that fine-tuning the model on small amounts of new data by labeling the data on a per-frame basis substantially increased the model performance. Our work demonstrated that computer vision analysis was feasible for detecting inattention in EEG studies. We hope that by providing a scalable method for assessing inattention during EEG experiments, EEG studies will be more reproducible, and the feasibility of studying early brain development in populations in which sustained attention during EEG experiments can be challenging will increase. Such populations include infants and children with and without neurodevelopmental conditions, among others.</p></section><section id="Sec21"><h2 class="pmc_sec_title">Electronic supplementary material</h2>
<p>Below is the link to the electronic supplementary material.</p>
<section class="sm xbox font-sm" id="MOESM1"><div class="media p"><div class="caption">
<a href="/articles/instance/12373809/bin/41598_2025_10511_MOESM1_ESM.zip" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material 1</a><sup> (60.3MB, zip) </sup>
</div></div></section><section class="sm xbox font-sm" id="MOESM2"><div class="media p"><div class="caption">
<a href="/articles/instance/12373809/bin/41598_2025_10511_MOESM2_ESM.docx" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material 2</a><sup> (11.7KB, docx) </sup>
</div></div></section></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>This research was supported by a grant from the National Institutes of Health (NIH; NICHD 2P50HD093074, Dawson, PI). We thank the NIH and the children that participated in the research studies and their families.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>D.Yu.I., M.Di M., D.C., K.C., G.D. and G.S. contributed to the design of the work, data analysis and interpretation; S.M. and J.G. contributed to the data acquisition and labeling; D.Yu.I. and Z.C. contributed to the creation of the new software used in the work; D.Yu.I. and S.M. contributed to drafting the first version of the manuscript; all authors revised the final manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>Due to privacy concerns, participants’ videos cannot be shared. To enable the reproducibility of the results, the dataset with extracted CVA features that were used for model training, and code for initial model training and model fine-tuning, are made publicly available at https://github.com/dyisaev/eeg-cva-model-training. A pipeline based on OpenFace software for CVA feature extraction is made publicly available at https://github.com/dyisaev/eeg-cva-feature-extraction. A GUI interface for visualization, labeling, and post-processing, together with installation and usage instructions is available at https://github.com/dyisaev/eeg-cva-visualization-tool. Python 3.9.7 was used in the model training and data analysis. Versions of python packages are listed in the corresponding repositories.</p></section><section id="notes3"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par81">Dr. Dawson is on the Scientific Advisory Boards of the Nonverbal Learning Disability Project and Tris Pharma, Inc., and receives book royalties from Guilford Press and Springer Nature Press. Dr. Dawson has developed technology, data, and/or products that have been licensed to Cryocell, Inc., and Dawson and Duke University have benefited financially. Dr. Dawson has received funding from the Marcus Foundation, Cord Blood Association, the National Institutes of Health (NIH), and the Simons Foundation. Dr. Carpenter has had funding by the National Institutes of Health (NIH), the Department of Defense, and the Brain and Behavior Foundation. Dr. Carpenter is a standing member on the Programmatic Panel for the Department of Defense Congressionally Directed Medical Research Programs (CDMRP) Autism Research Program and has served as an ad hoc reviewer on NIH review panels; she has received reimbursement for her time on these panels. The remaining authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm">
<div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div>
<div class="fn p" id="fn2"><p>Dmitry Yu. Isaev and Samantha Major contributed equally to this work.</p></div>
<div class="fn p" id="fn3"><p>These authors jointly supervised this work: Geraldine Dawson and Guillermo Sapiro.</p></div>
</div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>DeBoer, T., Scott, L. &amp; Nelson, C. <em>Methods for Acquiring and Analyzing Infant Event-Related Potentials in Infant EEG and Event-Related Potentials</em>. 5–38 (Psychology, 2013).</cite>
</li>
<li id="CR2">
<span class="label">2.</span><cite>Thierry, G. The use of event-related potentials in the study of early cognitive development. <em>Infant Child. Dev.</em><strong>14</strong> (1), 85–94. 10.1002/icd.353 (2005).</cite> [<a href="https://scholar.google.com/scholar_lookup?Thierry,%20G.%20The%20use%20of%20event-related%20potentials%20in%20the%20study%20of%20early%20cognitive%20development.%20Infant%20Child.%20Dev.14%20(1),%2085%E2%80%9394.%2010.1002/icd.353%20(2005)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Isaev, D. Y. et al. Relative average look duration and its association with neurophysiological activity in young children with autism spectrum disorder. <em>Sci. Rep.</em><strong>10</strong> (1). 10.1038/s41598-020-57902-1 (2020).</cite> [<a href="https://doi.org/10.1038/s41598-020-57902-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7002421/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32024855/" class="usa-link">PubMed</a>]</li>
<li id="CR4">
<span class="label">4.</span><cite>Webb, S. J. et al. Guidelines and best practices for electrophysiological data collection, analysis and reporting in autism. <em>J. Autism Dev. Disord.</em><strong>45</strong> (2), 425–443. 10.1007/s10803-013-1916-6 (2015).
</cite> [<a href="https://doi.org/10.1007/s10803-013-1916-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4141903/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23975145/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Webb,%20S.%20J.%20et%20al.%20Guidelines%20and%20best%20practices%20for%20electrophysiological%20data%20collection,%20analysis%20and%20reporting%20in%20autism.%20J.%20Autism%20Dev.%20Disord.45%20(2),%20425%E2%80%93443.%2010.1007/s10803-013-1916-6%20(2015)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Stets, M., Stahl, D. &amp; Reid, V. M. A meta-analysis investigating factors underlying attrition rates in infant ERP studies. <em>Dev. Neuropsychol.</em><strong>37</strong> (3), 226–252. 10.1080/87565641.2012.654867 (2012).
</cite> [<a href="https://doi.org/10.1080/87565641.2012.654867" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22545660/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Stets,%20M.,%20Stahl,%20D.%20&amp;%20Reid,%20V.%20M.%20A%20meta-analysis%20investigating%20factors%20underlying%20attrition%20rates%20in%20infant%20ERP%20studies.%20Dev.%20Neuropsychol.37%20(3),%20226%E2%80%93252.%2010.1080/87565641.2012.654867%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR6">
<span class="label">6.</span><cite>Bell, M. A. &amp; Cuevas, K. Using EEG to study cognitive development: issues and practices. <em>J. Cogn. Dev.</em><strong>13</strong> (3), 281–294. 10.1080/15248372.2012.691143 (2012).
</cite> [<a href="https://doi.org/10.1080/15248372.2012.691143" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3491357/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23144592/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Bell,%20M.%20A.%20&amp;%20Cuevas,%20K.%20Using%20EEG%20to%20study%20cognitive%20development:%20issues%20and%20practices.%20J.%20Cogn.%20Dev.13%20(3),%20281%E2%80%93294.%2010.1080/15248372.2012.691143%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<span class="label">7.</span><cite>Ellis, A. E. &amp; Nelson, C. A. Category prototypicality judgments in adults and children: behavioral and electrophysiological correlates. <em>Dev. Neuropsychol.</em><strong>15</strong> (2), 193–211. 10.1080/87565649909540745 (1999).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ellis,%20A.%20E.%20&amp;%20Nelson,%20C.%20A.%20Category%20prototypicality%20judgments%20in%20adults%20and%20children:%20behavioral%20and%20electrophysiological%20correlates.%20Dev.%20Neuropsychol.15%20(2),%20193%E2%80%93211.%2010.1080/87565649909540745%20(1999)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>Todd, R. M., Lewis, M. D., Meusel, L. A. &amp; Zelazo, P. D. The time course of social-emotional processing in early childhood: ERP responses to facial affect and personal familiarity in a Go-Nogo task. <em>Neuropsychologia</em><strong>46</strong> (2), 595–613. 10.1016/j.neuropsychologia.2007.10.011 (2008).
</cite> [<a href="https://doi.org/10.1016/j.neuropsychologia.2007.10.011" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/18061633/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Todd,%20R.%20M.,%20Lewis,%20M.%20D.,%20Meusel,%20L.%20A.%20&amp;%20Zelazo,%20P.%20D.%20The%20time%20course%20of%20social-emotional%20processing%20in%20early%20childhood:%20ERP%20responses%20to%20facial%20affect%20and%20personal%20familiarity%20in%20a%20Go-Nogo%20task.%20Neuropsychologia46%20(2),%20595%E2%80%93613.%2010.1016/j.neuropsychologia.2007.10.011%20(2008)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>Murias, M. et al. Validation of eye-tracking measures of social attention as a potential biomarker for autism clinical trials. <em>Autism Res.</em><strong>11</strong> (1), 166–174. 10.1002/aur.1894 (2018).
</cite> [<a href="https://doi.org/10.1002/aur.1894" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29193826/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Murias,%20M.%20et%20al.%20Validation%20of%20eye-tracking%20measures%20of%20social%20attention%20as%20a%20potential%20biomarker%20for%20autism%20clinical%20trials.%20Autism%20Res.11%20(1),%20166%E2%80%93174.%2010.1002/aur.1894%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR10">
<span class="label">10.</span><cite>Dawson, G. et al. Early behavioral intervention is associated with normalized brain activity in young children with autism. <em>J. Am. Acad. Child Adolesc. Psychiatry</em>. <strong>51</strong> (11), 1150–1159. 10.1016/j.jaac.2012.08.018 (2012).
</cite> [<a href="https://doi.org/10.1016/j.jaac.2012.08.018" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3607427/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23101741/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Dawson,%20G.%20et%20al.%20Early%20behavioral%20intervention%20is%20associated%20with%20normalized%20brain%20activity%20in%20young%20children%20with%20autism.%20J.%20Am.%20Acad.%20Child%20Adolesc.%20Psychiatry.%2051%20(11),%201150%E2%80%931159.%2010.1016/j.jaac.2012.08.018%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>Orekhova, E. V., Stroganova, T. A., Posikera, I. N. &amp; Elam, M. EEG theta rhythm in infants and preschool children. <em>Clin. Neurophysiol.</em><strong>117</strong> (5), 1047–1062. 10.1016/j.clinph.2005.12.027 (2006).
</cite> [<a href="https://doi.org/10.1016/j.clinph.2005.12.027" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16515883/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Orekhova,%20E.%20V.,%20Stroganova,%20T.%20A.,%20Posikera,%20I.%20N.%20&amp;%20Elam,%20M.%20EEG%20theta%20rhythm%20in%20infants%20and%20preschool%20children.%20Clin.%20Neurophysiol.117%20(5),%201047%E2%80%931062.%2010.1016/j.clinph.2005.12.027%20(2006)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Murias, M. et al. Electrophysiological biomarkers predict clinical improvement in an open-label trial assessing efficacy of autologous umbilical cord blood for treatment of autism. <em>Stem Cells Transl. Med.</em> 783–791. 10.1002/sctm.18-0090 (2018).</cite> [<a href="https://doi.org/10.1002/sctm.18-0090" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6216432/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30070044/" class="usa-link">PubMed</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>Shannon, E. Y. et al. icatcher+: robust and automated annotation of infants’ and young children’s gaze behavior from videos collected in laboratory, field, and online studies. <em>Adv. Methods Practices Psychol. Sci.</em><strong>6</strong> (2), 25152459221147250 (2023).</cite> [<a href="https://doi.org/10.1177/25152459221147250" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10471135/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37655047/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Shannon,%20E.%20Y.%20et%20al.%20icatcher+:%20robust%20and%20automated%20annotation%20of%20infants%E2%80%99%20and%20young%20children%E2%80%99s%20gaze%20behavior%20from%20videos%20collected%20in%20laboratory,%20field,%20and%20online%20studies.%20Adv.%20Methods%20Practices%20Psychol.%20Sci.6%20(2),%2025152459221147250%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Kaiser, A. et al. EEG data quality: Determinants and impact in a multicenter study of children, adolescents, and adults with attention-deficit/hyperactivity disorder (ADHD). <em>Brain Sci.</em><strong>11</strong> (2). 10.3390/brainsci11020214 (2021).</cite> [<a href="https://doi.org/10.3390/brainsci11020214" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7916500/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33578741/" class="usa-link">PubMed</a>]</li>
<li id="CR15">
<span class="label">15.</span><cite>Webb, S. J. et al. Biomarker acquisition and quality control for multi-site studies: The autism biomarkers consortium for clinical trials [methods]. <em>Front. Integr. Nuerosci.</em><strong>13</strong>. 10.3389/fnint.2019.00071 (2020).</cite> [<a href="https://doi.org/10.3389/fnint.2019.00071" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7020808/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32116579/" class="usa-link">PubMed</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>Elsabbagh, M. et al. Disengagement of visual attention in infancy is associated with emerging autism in toddlerhood. <em>Biol. Psychiatry</em>. <strong>74</strong> (3), 189–194. 10.1016/j.biopsych.2012.11.030 (2013).
</cite> [<a href="https://doi.org/10.1016/j.biopsych.2012.11.030" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3715700/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23374640/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Elsabbagh,%20M.%20et%20al.%20Disengagement%20of%20visual%20attention%20in%20infancy%20is%20associated%20with%20emerging%20autism%20in%20toddlerhood.%20Biol.%20Psychiatry.%2074%20(3),%20189%E2%80%93194.%2010.1016/j.biopsych.2012.11.030%20(2013)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR17">
<span class="label">17.</span><cite>Keehn, B., Müller, R. A. &amp; Townsend, J. Atypical attentional networks and the emergence of autism. <em>Neurosci. Biobehav. Rev.</em><strong>37</strong> (2), 164–183. 10.1016/j.neubiorev.2012.11.014 (2013).
</cite> [<a href="https://doi.org/10.1016/j.neubiorev.2012.11.014" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3563720/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23206665/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Keehn,%20B.,%20M%C3%BCller,%20R.%20A.%20&amp;%20Townsend,%20J.%20Atypical%20attentional%20networks%20and%20the%20emergence%20of%20autism.%20Neurosci.%20Biobehav.%20Rev.37%20(2),%20164%E2%80%93183.%2010.1016/j.neubiorev.2012.11.014%20(2013)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR18">
<span class="label">18.</span><cite>McPartland, J. C., Webb, S. J., Keehn, B. &amp; Dawson, G. Patterns of visual attention to faces and objects in autism spectrum disorder. <em>J. Autism Dev. Disord.</em><strong>41</strong> (2), 148–157. 10.1007/s10803-010-1033-8 (2011).
</cite> [<a href="https://doi.org/10.1007/s10803-010-1033-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3074360/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20499148/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?McPartland,%20J.%20C.,%20Webb,%20S.%20J.,%20Keehn,%20B.%20&amp;%20Dawson,%20G.%20Patterns%20of%20visual%20attention%20to%20faces%20and%20objects%20in%20autism%20spectrum%20disorder.%20J.%20Autism%20Dev.%20Disord.41%20(2),%20148%E2%80%93157.%2010.1007/s10803-010-1033-8%20(2011)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19">
<span class="label">19.</span><cite>Werner, E., Dawson, G., Osterling, J. &amp; Dinno, N. Recognition of autism spectrum disorder before one year of age. <em>J. Autism Dev. Disord.</em><strong>30</strong> (2), 157–162 (2000).
</cite> [<a href="https://doi.org/10.1023/a:1005463707029" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/10832780/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Werner,%20E.,%20Dawson,%20G.,%20Osterling,%20J.%20&amp;%20Dinno,%20N.%20Recognition%20of%20autism%20spectrum%20disorder%20before%20one%20year%20of%20age.%20J.%20Autism%20Dev.%20Disord.30%20(2),%20157%E2%80%93162%20(2000)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Orekhova, E. V. et al. EEG hyper-connectivity in high-risk infants is associated with later autism. <em>J. Neurodevelopmental Disorders</em>. <strong>6</strong> (1), 1–11. 10.1186/1866-1955-6-40 (2014).</cite> [<a href="https://doi.org/10.1186/1866-1955-6-40" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4232695/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25400705/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Orekhova,%20E.%20V.%20et%20al.%20EEG%20hyper-connectivity%20in%20high-risk%20infants%20is%20associated%20with%20later%20autism.%20J.%20Neurodevelopmental%20Disorders.%206%20(1),%201%E2%80%9311.%2010.1186/1866-1955-6-40%20(2014)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR21">
<span class="label">21.</span><cite>Stroganova, T. A., Orekhova, V., Posikera, I. N. &amp; E., &amp; Externally and internally controlled attention in infants: an EEG study. <em>Int. J. Psychophysiol.</em><strong>30</strong> (3), 339–351. 10.1016/S0167-8760(98)00026-9 (1998).
</cite> [<a href="https://doi.org/10.1016/s0167-8760(98)00026-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/9834890/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Stroganova,%20T.%20A.,%20Orekhova,%20V.,%20Posikera,%20I.%20N.%20&amp;%20E.,%20&amp;%20Externally%20and%20internally%20controlled%20attention%20in%20infants:%20an%20EEG%20study.%20Int.%20J.%20Psychophysiol.30%20(3),%20339%E2%80%93351.%2010.1016/S0167-8760(98)00026-9%20(1998)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Kotseruba, I. &amp; Tsotsos, J. K. Attention for vision-based assistive and automated driving: A review of algorithms and datasets. <em>IEEE Trans. Intell. Transport. Syst.</em><strong>23</strong>(11), 19907–19928 10.1109/TITS.2022.3186613 (2022). </cite>
</li>
<li id="CR23">
<span class="label">23.</span><cite>Regan, M. A., Hallett, C. &amp; Gordon, C. P. Driver distraction and driver inattention: Definition, relationship and taxonomy. <em>Accid. Anal. Prevent</em>. <strong>43</strong>(5), 1771–1781 (2011).</cite> [<a href="https://doi.org/10.1016/j.aap.2011.04.008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21658505/" class="usa-link">PubMed</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Li, W. et al. A survey on vision-based driver distraction analysis. <em>J. Syst. Architect.</em><strong>121</strong>, 102319 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20W.%20et%20al.%20A%20survey%20on%20vision-based%20driver%20distraction%20analysis.%20J.%20Syst.%20Architect.121,%20102319%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR25">
<span class="label">25.</span><cite>Ahtola, E., Stjerna, S., Stevenson, N. &amp; Vanhatalo, S. Use of eye tracking improves the detection of evoked responses to complex visual stimuli during EEG in infants. <em>Clin. Neurophysiol. Pract.</em><strong>2</strong>, 81–90. 10.1016/j.cnp.2017.03.002 (2017).
</cite> [<a href="https://doi.org/10.1016/j.cnp.2017.03.002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6123848/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30214977/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ahtola,%20E.,%20Stjerna,%20S.,%20Stevenson,%20N.%20&amp;%20Vanhatalo,%20S.%20Use%20of%20eye%20tracking%20improves%20the%20detection%20of%20evoked%20responses%20to%20complex%20visual%20stimuli%20during%20EEG%20in%20infants.%20Clin.%20Neurophysiol.%20Pract.2,%2081%E2%80%9390.%2010.1016/j.cnp.2017.03.002%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<span class="label">26.</span><cite>Maguire, M. J., Magnon, G. &amp; Fitzhugh, A. E. Improving data retention in EEG research with children using child-centered eye tracking. <em>J. Neurosci. Methods</em>. <strong>238</strong>, 78–81. 10.1016/j.jneumeth.2014.09.014 (2014).
</cite> [<a href="https://doi.org/10.1016/j.jneumeth.2014.09.014" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4254274/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25251555/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Maguire,%20M.%20J.,%20Magnon,%20G.%20&amp;%20Fitzhugh,%20A.%20E.%20Improving%20data%20retention%20in%20EEG%20research%20with%20children%20using%20child-centered%20eye%20tracking.%20J.%20Neurosci.%20Methods.%20238,%2078%E2%80%9381.%2010.1016/j.jneumeth.2014.09.014%20(2014)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR27">
<span class="label">27.</span><cite>Baltrusaitis, T., Zadeh, A., Lim, Y. C. &amp; Morency, L. P. Openface 2.0: Facial behavior analysis toolkit. In <em>13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</em>, Xi’an, China, 2018. 59–66. 10.1109/FG.2018.00019 (2018). </cite>
</li>
<li id="CR28">
<span class="label">28.</span><cite>Krafka, K. Eye tracking for everyone. In <em>IEEE Conference on Computer Vision and Pattern Recognition</em><em>(CVPR)</em>, Las Vegas, NV, USA, 2016. 2176–2184. 10.1109/CVPR.2016.239 (2016). </cite>
</li>
<li id="CR29">
<span class="label">29.</span><cite>Lugaresi, C. et al. Mediapipe: A framework for Building perception pipelines. In <em>Third Workshop on Computer Vision for AR/VR at IEEE Computer Vision and Pattern Recognition (CVPR)</em>. 10.48550/arXiv.1906.08172 (2019). </cite>
</li>
<li id="CR30">
<span class="label">30.</span><cite>Torre, F. D. et al. IntraFace. In <em>11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</em>, Ljubljana, Slovenia, 2015. 1–8. 10.1109/FG.2015.7163082 (2015). </cite>
</li>
<li id="CR31">
<span class="label">31.</span><cite>Perochon, S. et al. A scalable computational approach to assessing response to name in toddlers with autism. <em>J. Child Psychol. Psychiatry</em>. <strong>62</strong> (9), 1120–1131. 10.1111/jcpp.13381 (2021).
</cite> [<a href="https://doi.org/10.1111/jcpp.13381" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8397798/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33641216/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Perochon,%20S.%20et%20al.%20A%20scalable%20computational%20approach%20to%20assessing%20response%20to%20name%20in%20toddlers%20with%20autism.%20J.%20Child%20Psychol.%20Psychiatry.%2062%20(9),%201120%E2%80%931131.%2010.1111/jcpp.13381%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Chang, Z. et al. Computational methods to measure patterns of gaze in toddlers with autism spectrum disorder. <em>JAMA Pediatr.</em><strong>175</strong> (8), 827–836. 10.1001/jamapediatrics.2021.0530 (2021).
</cite> [<a href="https://doi.org/10.1001/jamapediatrics.2021.0530" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8077044/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33900383/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Chang,%20Z.%20et%20al.%20Computational%20methods%20to%20measure%20patterns%20of%20gaze%20in%20toddlers%20with%20autism%20spectrum%20disorder.%20JAMA%20Pediatr.175%20(8),%20827%E2%80%93836.%2010.1001/jamapediatrics.2021.0530%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33">
<span class="label">33.</span><cite>Qian, X., Wang, M., Wang, X., Wang, Y. &amp; Dai, W. Intelligent method for real-time portable EEG artifact annotation in semiconstrained environment based on computer vision. <em>Comput. Intell. Neurosci.</em><strong>9590411</strong>. 10.1155/2022/9590411 (2022).</cite> [<a href="https://doi.org/10.1155/2022/9590411" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8858064/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35190736/" class="usa-link">PubMed</a>]</li>
<li id="CR34">
<span class="label">34.</span><cite>Gotham, K. et al. A replication of the autism diagnostic observation schedule (ADOS) revised algorithms. <em>J. Am. Acad. Child. Adolesc. Psychiatry</em>. <strong>47</strong> (6), 642–651. 10.1097/CHI.0b013e31816bffb7 (2008).
</cite> [<a href="https://doi.org/10.1097/CHI.0b013e31816bffb7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3057666/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/18434924/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Gotham,%20K.%20et%20al.%20A%20replication%20of%20the%20autism%20diagnostic%20observation%20schedule%20(ADOS)%20revised%20algorithms.%20J.%20Am.%20Acad.%20Child.%20Adolesc.%20Psychiatry.%2047%20(6),%20642%E2%80%93651.%2010.1097/CHI.0b013e31816bffb7%20(2008)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Elliott, C. D. <em>Differential Ability Scales</em>. 2nd Ed. (Harcourt Assessment, 2007).</cite>
</li>
<li id="CR36">
<span class="label">36.</span><cite>King, D. E. Dlib-ml: A machine learning toolkit. <em>J. Mach. Learn. Res.</em><strong>10</strong>, 1755–1758 (2009).</cite> [<a href="https://scholar.google.com/scholar_lookup?King,%20D.%20E.%20Dlib-ml:%20A%20machine%20learning%20toolkit.%20J.%20Mach.%20Learn.%20Res.10,%201755%E2%80%931758%20(2009)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR37">
<span class="label">37.</span><cite>Cohen, J. A coefficient of agreement for nominal scales. <em>Educ. Psychol. Meas.</em><strong>20</strong> (1), 37–46. 10.1177/001316446002000104 (1960).</cite> [<a href="https://scholar.google.com/scholar_lookup?Cohen,%20J.%20A%20coefficient%20of%20agreement%20for%20nominal%20scales.%20Educ.%20Psychol.%20Meas.20%20(1),%2037%E2%80%9346.%2010.1177/001316446002000104%20(1960)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR38">
<span class="label">38.</span><cite>Tishby, N. &amp; Noga Zaslavsky. and. Deep learning and the information bottleneck principle. In <em>2015 IEEE Information Theory Workshop (ITW)</em>. (IEEE, 2015).</cite>
</li>
<li id="CR39">
<span class="label">39.</span><cite>Guo, C., Pleiss, G., Sun, Y. &amp; Weinberger, K. Q. On calibration of modern neural networks. In <em>34th International Conference on Machine Learning, ICML 2017</em>. Vol. 3. 2130–2143 (2017).</cite>
</li>
<li id="CR40">
<span class="label">40.</span><cite>Hastie, T., Tibshirani, R. &amp; Friedman, J. <em>The Elements of Statistical Learning</em> (Springer, 2001).</cite>
</li>
<li id="CR41">
<span class="label">41.</span><cite>Kingma, D. P. &amp; Ba, J. <em>Adam: A Method for Stochastic Optimization</em>. <a href="http://arxiv.org/abs/1412.6980" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1412.6980</a> (2015). </cite>
</li>
<li id="CR42">
<span class="label">42.</span><cite>Paszke, A. et al. PyTorch: an imperative style, high-performance deep learning library. In <em>Proceedings of the 33rd International Conference on Neural Information Processing Systems</em>. Article 721. (Curran Associates Inc., 2019).</cite>
</li>
<li id="CR43">
<span class="label">43.</span><cite>McHugh, M. L. Interrater reliability: the kappa statistic. <em>Biochem. Med. (Zagreb)</em>. <strong>22</strong> (3), 276–282 (2012).
</cite> [<a href="/articles/PMC3900052/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23092060/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?McHugh,%20M.%20L.%20Interrater%20reliability:%20the%20kappa%20statistic.%20Biochem.%20Med.%20(Zagreb).%2022%20(3),%20276%E2%80%93282%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR44">
<span class="label">44.</span><cite>Bey, A. L. et al. Automated video tracking of autistic children’s movement during Caregiver-Child interaction: an exploratory study. <em>J. Autism Dev. Disord</em>. <strong>54</strong> (10), 3706–3718. 10.1007/s10803-023-06107-2 (2024). Epub 2023 Aug 29. PMID: 37642871.
</cite> [<a href="https://doi.org/10.1007/s10803-023-06107-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37642871/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Bey,%20A.%20L.%20et%20al.%20Automated%20video%20tracking%20of%20autistic%20children%E2%80%99s%20movement%20during%20Caregiver-Child%20interaction:%20an%20exploratory%20study.%20J.%20Autism%20Dev.%20Disord.%2054%20(10),%203706%E2%80%933718.%2010.1007/s10803-023-06107-2%20(2024).%20Epub%202023%20Aug%2029.%20PMID:%2037642871." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR45">
<span class="label">45.</span><cite>Chorney, J. M., McMurtry, C. M., Chambers, C. T. &amp; Bakeman, R. Developing and modifying behavioral coding schemes in pediatric psychology: A practical guide. <em>J. Pediatr. Psychol.</em><strong>40</strong> (1), 154–164. 10.1093/jpepsy/jsu099 (2015) (epub 2014 Nov 21). </cite> [<a href="https://doi.org/10.1093/jpepsy/jsu099" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4288308/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25416837/" class="usa-link">PubMed</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adsm93_" lang="en" class="supplementary-materials"><h3 class="pmc_sec_title">Supplementary Materials</h3>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373809/bin/41598_2025_10511_MOESM1_ESM.zip" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material 1</a><sup> (60.3MB, zip) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373809/bin/41598_2025_10511_MOESM2_ESM.docx" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material 2</a><sup> (11.7KB, docx) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>Due to privacy concerns, participants’ videos cannot be shared. To enable the reproducibility of the results, the dataset with extracted CVA features that were used for model training, and code for initial model training and model fine-tuning, are made publicly available at https://github.com/dyisaev/eeg-cva-model-training. A pipeline based on OpenFace software for CVA feature extraction is made publicly available at https://github.com/dyisaev/eeg-cva-feature-extraction. A GUI interface for visualization, labeling, and post-processing, together with installation and usage instructions is available at https://github.com/dyisaev/eeg-cva-visualization-tool. Python 3.9.7 was used in the model training and data analysis. Versions of python packages are listed in the corresponding repositories.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-10511-2"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_10511.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (1.5 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12373809/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12373809/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373809%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373809/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12373809/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12373809/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40846872/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12373809/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40846872/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12373809/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12373809/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="d852BCBZrZ2AyN5UuEnAH4xDxFoW18HvQBHmZtSvteHf90hZOlECzLjonEyDQ7uY">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
