
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            A comprehensive survey of deep face verification systems adversarial attacks and defense strategies - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E49FC08AF1B3A3059FC000486BB7ED.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373911/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="A comprehensive survey of deep face verification systems adversarial attacks and defense strategies">
<meta name="citation_author" content="Sohair Kilany">
<meta name="citation_author_institution" content="Computer Science Department, Faculty of Science, Minia University, Al Minya, Egypt">
<meta name="citation_author" content="Ahmed Mahfouz">
<meta name="citation_author_institution" content="Computer Science Department, Faculty of Science, Minia University, Al Minya, Egypt">
<meta name="citation_author_institution" content="Faculty of Computer Studies, Arab Open University, Muscat, Oman">
<meta name="citation_publication_date" content="2025 Aug 22">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30861">
<meta name="citation_doi" content="10.1038/s41598-025-15753-8">
<meta name="citation_pmid" content="40847113">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373911/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373911/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373911/pdf/41598_2025_Article_15753.pdf">
<meta name="description" content="Face Verification (FV) systems have exhibited remarkable performance in verification tasks and have consequently garnered extensive adoption across various applications, from identity duplication to authentication in mobile payments. However, the ...">
<meta name="og:title" content="A comprehensive survey of deep face verification systems adversarial attacks and defense strategies">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Face Verification (FV) systems have exhibited remarkable performance in verification tasks and have consequently garnered extensive adoption across various applications, from identity duplication to authentication in mobile payments. However, the ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373911/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12373911">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-15753-8"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_15753.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373911%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12373911/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12373911/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373911/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 22;15:30861. doi: <a href="https://doi.org/10.1038/s41598-025-15753-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-15753-8</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>A comprehensive survey of deep face verification systems adversarial attacks and defense strategies</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kilany%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Sohair Kilany</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Sohair Kilany</span></h3>
<div class="p">
<sup>1</sup>Computer Science Department, Faculty of Science, Minia University, Al Minya, Egypt </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kilany%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Sohair Kilany</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mahfouz%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Ahmed Mahfouz</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Ahmed Mahfouz</span></h3>
<div class="p">
<sup>1</sup>Computer Science Department, Faculty of Science, Minia University, Al Minya, Egypt </div>
<div class="p">
<sup>2</sup>Faculty of Computer Studies, Arab Open University, Muscat, Oman </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mahfouz%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Ahmed Mahfouz</span></a>
</div>
</div>
<sup>1,</sup><sup>2,</sup><sup>✉</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Computer Science Department, Faculty of Science, Minia University, Al Minya, Egypt </div>
<div id="Aff2">
<sup>2</sup>Faculty of Computer Studies, Arab Open University, Muscat, Oman </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Apr 17; Accepted 2025 Aug 11; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12373911  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40847113/" class="usa-link">40847113</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Face Verification (FV) systems have exhibited remarkable performance in verification tasks and have consequently garnered extensive adoption across various applications, from identity duplication to authentication in mobile payments. However, the surge in popularity of face verification has raised concerns about potential vulnerabilities in the face of adversarial attacks. These concerns originate from the fact that advanced FV systems, which rely on deep neural networks, have recently demonstrated susceptibility to crafted input samples known as adversarial examples. Although imperceptible to human observers, adversarial examples can deceive deep neural networks during the testing and deployment phases. These vulnerabilities raised significant concerns about the deployment of deep neural networks in safety-critical contexts, prompting extensive investigations into adversarial attacks and corresponding defense strategies. This comprehensive survey provides a comprehensive overview of recent advances in deep face verification, encompassing a broad spectrum of topics such as algorithmic designs, database utilization, protocols, and application scenarios. Furthermore, we conduct an in-depth examination of state-of-the-art algorithms to generate adversarial examples and the defense mechanisms devised to mitigate such adversarial threats.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Face verification, Deep neural network, Adversarial attacks, Adversarial perturbation, Defense techniques</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Computational science, Information technology, Scientific data</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Face verification (FV) is an active research topic in computer vision. It has been involved in various applications such as active authentication<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a>–<a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>, driving licenses, and airport security due to the growing size of face databases and the high accuracy of the FV system. Different organizations around the world have widely adopted FV.</p>
<p id="Par3">The key to face verification is extracting a discriminative set of features from face images using Deep Convolutional Neural Networks (DCNNs). With recent advances in DCNNs<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a>,<a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>, face verification reached impressive performances and highly accurate results over the past years<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a>–<a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>. For instance, the accuracy of Labeled Faces in the Wild (LFW) benchmark dataset<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup> has been boosted from 97% to above 99.8%, and Youtube Faces (YTF) dataset<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup> has been increased from 91.4% to above 98%. Moreover, DCNN frameworks enable end-to-end learning, i.e., learning a mapping from the input image space to the target label space.</p>
<p id="Par4">In the last few years, researchers have discovered that FV systems are susceptible to attacks that introduce data variations, which can deceive classifiers. These attacks can be accomplished either via (i) Spoof attacks: artifacts in the physical domain (i.e., 3D masks, eyeglasses, replaying videos)<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>, (ii) Adversarial perturbation attacks: imperceptible noises added to probes for evading FV systems, and (iii) Digital manipulation attacks: entirely or partially modified photo-realistic faces using generative models<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>.</p>
<p id="Par5">Among the various attacks, adversarial attacks are the most dangerous because they generally target Deep Neural Networks (DNNs) and focus on Convolutional Neural Networks (CNNs), which are based on the latest FV models. Figure <a href="#Fig1" class="usa-link">1</a> demonstrates the attractiveness of this type of attack and the explosive growth in the number of papers published each year in generating adversarial examples. Therefore, DCNNs are fragile and can be easily attacked by adversarial examples resulting from adding small perturbations<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a>–<a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. These amounts of perturbations are imperceptible to the human eye. The goal is to mislead the classifier to provide wrong prediction outputs because the synthesized images look almost the same as the original ones. Adversarial attacks on FV systems are generated in a manner that humans cannot notice the adversarial perturbations, but the perturbations cause the FV system to misclassify an image, as shown in Fig. <a href="#Fig2" class="usa-link">2</a>. Therefore , it is essential to gain a deeper understanding of how these models are susceptible to these attacks.</p>
<figure class="fig xbox font-sm" id="Fig1"><h3 class="obj_head">Fig. 1.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/0515b482bf8e/41598_2025_15753_Fig1_HTML.jpg" loading="lazy" id="MO1" height="446" width="634" alt="Fig. 1"></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The cumulative number of adversarial example papers published in the last years (Image Credit:<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>).</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig2"><h3 class="obj_head">Fig. 2.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cf3d3140fcc9/41598_2025_15753_Fig2_HTML.jpg" loading="lazy" id="MO2" height="334" width="642" alt="Fig. 2"></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The face verification (FV)system misclassifies the two images belonging to the same person.</p></figcaption></figure><p id="Par6">Several known methods for crafting adversarial examples vary significantly in terms of their challenges, complexity, computational cost, and the objectives of the attacks. The level of information available to the attacker is classified into three categories (i) White-box attack<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>, (ii) Black-box attack<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>, and (iii) Semi-white box attack<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>. Depending on the attacker’s objective, creating adverse face images can be seen as a security threat. In addition to the apparent security risk of identity theft, this poses an ethical justification to conduct face testing<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>. Another reason to study adversarial attacks on face verification is from the perspective of the machine learning researcher<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>. Identifying new modes of attacks has shown that training on the perturbed instances can help negate the very same line of attacks<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. Thus, to build robust models, it is necessary to break them up and then adversarially retrain the previously weak model to make it more robust.</p>
<p id="Par7">Due to the users’ privacy issues related to spoofed systems, the requirement to prevent face attacks is becoming increasingly critical. Due to the extensive use of automated face verification systems for border control, failing to detect face attacks might pose a significant security risk. With the introduction of smartphones, we now all have automated facial recognition algorithms integrated into our pockets. Face recognition on our phones makes it easier to (i) unlock the device, (ii) execute financial transactions, and (iii) access privileged content stored on the device.</p>
<p id="Par8">As a result, considerable literature on defending deep neural networks against adversarial cases has emerged. As indicated in Fig. <a href="#Fig3" class="usa-link">3</a>, there are two types of defenses against adversarial attacks. First, Robust Optimization (i.e., Adversarial Training) is the most popular defense method, which modifies the net training procedures or architectures and aims to improve the classifier’s robustness<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a>,<a href="#CR23" class="usa-link" aria-describedby="CR23">23</a>–<a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>. Although these algorithms are safe against specific attacks, these defenses are still vulnerable to attacks from other mechanisms. However, because online adversarial example generation requires additional computation, adversarial training takes longer than training on clean images alone. Second, the pre-processing strategy leaves the training procedure and architecture unchanged but modifies the data by aiming to detect, remove, or purify the images. For example, in the case of adversarial examples detection, which involves training a binary classifier to distinguish between real and adversarial ones<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a>–<a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>. In the case of removing adversarial noise<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a>,<a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>, which aims to remove the adversarial perturbation by applying transformations as a preprocessing on the input data and then sending these inputs to the target models. But in the case of the purification, it is removed from the input adversarial images only<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup> to avoid purifying the real images and consequently avoid the high false reject rates.</p>
<figure class="fig xbox font-sm" id="Fig3"><h3 class="obj_head">Fig. 3.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/23b0e71dc0ca/41598_2025_15753_Fig3_HTML.jpg" loading="lazy" id="MO3" height="314" width="749" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Defense strategies in literature.</p></figcaption></figure><p id="Par9">The main contributions of this survey are as follows:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par10">Highlighting the major shortcomings and key areas in facial verification, we present a comparison and analysis of publicly available databases that are vital for both model training and testing.</p></li>
<li><p id="Par11">Analyzing the state-of-the-art adversarial attacks on FV systems to give an overview of the main techniques and contributions to adversarial attacks.</p></li>
<li><p id="Par12">Analyzing major defense methods commonly used in the literature and summarizing the most recent related work concerned with detection challenges of Adversarial face images.</p></li>
</ul>
<p>The subsequent sections of this paper are structured as follows: Section "Terms and Definitions" introduces pivotal definitions and concepts commonly applied in the realm of adversarial attacks and their corresponding defenses within the framework of the FV system. Section “Face Verification” provides an in-depth examination of the Face Verification system, including diverse network architectures, loss functions, and a comprehensive overview of facial processing algorithms and datasets. Sections "FV Systems Vulnerabilities and Defense" are dedicated to scrutinizing adversarial attack generation methodologies designed to subvert the FV mission and exploring various defensive strategies. The paper concludes in Section “Conclusion”.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Terms and definitions</h2>
<p id="Par13">In this section, we provide a concise overview of the fundamental elements concerning model attacks and defenses within the context of the FV system. Precise terminology definitions play a pivotal role in facilitating comprehension of the primary aspects explored in prior research about adversarial attacks and their corresponding countermeasures. The subsequent sections of this scholarly work consistently adhere to the established definitions of these terms.</p>
<section id="Sec3"><h3 class="pmc_sec_title">General terms</h3>
<ol class="list" style="list-style-type:decimal">
<li><p id="Par15"><strong>Adversarial example/image:</strong> A deliberately altered variant of the original image, achieved through the introduction of perturbations such as noise, with the objective of misleading deep convolutional neural networks (DCNN) and machine learning models (ML), including (FV) models.</p></li>
<li><p id="Par16"><strong>Adversarial perturbation:</strong> A form of interference is introduced into the clean image to transform it into an adversarial example.</p></li>
<li><p id="Par17"><strong>Adversarial training:</strong> A model training procedure incorporating adversarial images in conjunction with unaltered ones.</p></li>
<li><p id="Par18"><strong>Transferability: </strong> The capacity of a perturbed example to influence models other than those employed in its creation.</p></li>
</ol></section><section id="Sec4"><h3 class="pmc_sec_title">Specific terms</h3>
<p id="Par19">We can talk about attacks from several aspects, namely: (i) the objective of the attacks and (ii) the information available to the attacker through which the attacks are classified into three categories: (i) White-box attack, (ii) Black-box attack, and (iii) Semi-white box attack.</p>
<section id="Sec5"><h4 class="pmc_sec_title">Objective of the attacks</h4>
<p id="Par20"><em>Poisoning Attack vs Evasion Attack.</em> Evasion<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup> is the most common attack performed during production. It refers to designing an input, which seems normal for a human but is wrongly classified by deep learning models.</p>
<p id="Par21">A poisoning attack<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup> happens when the adversary can inject poisoned data into your model’s training, hence getting it to learn something it shouldn’t. This attack frequently appears when the adversary has access to the training database.</p>
<p id="Par22"><em>Target Attack vs Non-Target Attack:</em> Indeed, targeted attacks are more complicated than non-targeted ones. The purpose of the non-target attack is to make the model misclassify the adversarial image. In contrast, the targeted attack makes the model classify the adversarial image as a specific target class, which is different from the true class.</p>
<p id="Par23"><em>Obfuscation Attack vs Impersonation Attack:</em> Obfuscation Attacks  (OA) and Impersonation Attacks  (IA) are frauds. In OA, the attacker seeks to avoid being verified by the FV system, but in IA, the attacker seeks to be incorrectly verified as a different legitimate user by the FV system. In both cases, the fraud process is done by adding an imperceptible perturbation to probe the image. This amount of perturbation is imperceptible to the human eye and differs from one person to another.</p></section><section id="Sec6"><h4 class="pmc_sec_title">Attacker’s information</h4>
<ul class="list" style="list-style-type:disc">
<li><p id="Par25">White-Box Attack : In a white-box setting, the attacker has full information about the deep learning models, such as parameters, architecture, defense methods, and gradients . Then, it uses this information to add a small imperceptible perturbation to an inquiry image<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a>,<a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. Perhaps this is not available in the real world for two reasons. First, the attacker cannot access the model because model designers usually do not open their model parameters for special reasons. Second, the model may have been trained in this type of attack, so the model will have the ability to detect this type of attack. As a result of the popularity of the white-box type of attack, designers are creating a robustness model that cannot be deceived by these attacks.</p></li>
<li><p id="Par26">Black-Box Attack : In a Black-box setting, the attacker does not know the details of target models such as architectures, parameters, and defense methods. They use different models to generate adversarial images, hoping these will transfer to the target model. Additionally, the adversary may have only partial knowledge about  (i) the classifier’s data domain, for example, handwritten digits, photographs, and human faces, and (ii) the architecture of the classifier, such as CNNs and RNNs.</p></li>
<li><p id="Par27">Semi-white Box Attack : In a semi-white box attack setting, the attacker trains a generative model for producing adversarial examples in a white-box setting. Once the generative model is trained, the attacker does not need the victim model anymore and can craft adversarial examples in a black-box setting.</p></li>
</ul></section></section></section><section id="Sec7"><h2 class="pmc_sec_title">Face verification</h2>
<p id="Par28">Biometrics encompasses technologies designed to authenticate or identify individuals based on physiological attributes or behavioral characteristics. Physiological attributes, such as the iris, fingerprints, and facial features, are generally considered more reliable than behavioral traits like voice patterns, typing style, or walking gait. The domain of deep learning, essential for developing robust verification algorithms, necessitates substantial training data. Obtaining facial images from online sources is notably more straightforward than collecting iris or fingerprint data, primarily due to the widespread availability of digital cameras in affordable smartphones. In contrast, specialized hardware sensors are required for fingerprint and iris recognition. Consequently, the advancement of deep learning techniques has significantly improved face verification performance.</p>
<p id="Par29">FV technologies are presently deployed in numerous critical commercial and governmental applications. FV plays a pivotal role in preventing identity card duplication, thwarting individuals from obtaining multiple identification documents, such as driver’s licenses and passports, under different aliases. Despite the accomplishments of FV systems in the scenarios above, their performance remains constrained, particularly under unconstrained conditions.</p>
<p id="Par30">As depicted in Fig. <a href="#Fig4" class="usa-link">4</a>, factors like varying poses, lighting conditions, facial expressions, age, and obstructions can significantly distort the appearance of an individual’s face, highlighting the need to reduce intra-personal variations while accentuating inter-personal distinctions as a central focus in face verification. Moreover, face attacks have materialized in physical realms, such as 3D face masks, and digital domains, encompassing adversarial and digitally manipulated facial images. Malicious actors, called attackers, are increasingly challenging the security of FV pipelines utilized for government services, access control, and financial transactions, often bypassing human operator verification of the legitimacy of facial image acquisition. This section provides an essential foundation for comprehending deep face verification and offers an overview of relevant research in this domain.</p>
<figure class="fig xbox font-sm" id="Fig4"><h3 class="obj_head">Fig. 4.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/4de7737c9e3d/41598_2025_15753_Fig4_HTML.jpg" loading="lazy" id="MO4" height="834" width="750" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Sources of intra-personal variations: (<strong>a</strong>) pose, (<strong>b</strong>) illumination, (<strong>c</strong>) expression, and (<strong>d</strong>) Occlusion. Each row shows intra-personal variations for the same individual (Image Credit: Google Images).</p></figcaption></figure><p id="Par31">In Face Recognition (FR), two primary categories exist: Face Identification (FI), denoted as one-to-many face recognition, and face verification (FV), referred to as one-to-one face recognition. FI is concerned with classifying a face image into a specific identity. At the same time, FV is tasked with ascertaining whether or not two given face images correspond to the same identity. It is important to note that a more efficient FV system directly contributes to the overall efficiency of the FR system. Consequently, face verification can be conceptualized as evaluating the degree of similarity between a pair of facial images.</p>
<section id="Sec8"><h3 class="pmc_sec_title">Face verification pipeline</h3>
<p id="Par32">Face verification can be simplified as the problem of comparing the similarity between a pair of face images. The whole deep FV system pipeline consists of (i) Input Image (Images or Image Frames), (ii) Face Detector, which localizes faces in images, (iii) Alignment, aligned to normalized canonical coordinates, (iv) Face processing to handle intra-personal variations, (v) Feature Extraction, and (vi) Face Matching. The verification pipeline is shown in Figure <a href="#Fig5" class="usa-link">5</a>. The following subsections provide a detailed description of each component of the pipeline.</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/43d9cd486391/41598_2025_15753_Fig5_HTML.jpg" loading="lazy" id="MO5" height="275" width="706" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The end-to-end pipeline of the deep FV system.</p></figcaption></figure><section id="Sec9"><h4 class="pmc_sec_title">Face detector</h4>
<p id="Par33">One of the most formidable challenges within the field of computer vision pertains to face detection, primarily attributable to the substantial intra-class variability inherent in facial appearances. This variability encompasses factors like skin complexion, background interference, facial orientation, and lighting conditions. Face detection (FD) assumes critical importance within Facial Verification (FV) systems, as it involves precisely localizing and isolating the facial region within an image. FD, in essence, encompasses the capability to identify and delineate one or more faces within a photograph, irrespective of their spatial orientation, lighting variations, attire, accessories, hair color, presence of facial hair, application of makeup, and age of the individuals. In this context, the localization phase involves the placement of a bounding box around the facial region’s spatial coordinates within the image. In contrast, the location phase pertains to the precise determination of these coordinates. It is worth noting that classical feature-based methods, exemplified by the “Haar Cascade classifier,” have historically provided a reasonably effective solution to this challenge, and such approaches remain popular for face detection<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>. However, recent years have witnessed significant advancements in face detection techniques, with deep learning methodologies emerging as the forefront contenders for achieving state-of-the-art results on established benchmark datasets. Noteworthy examples in this domain include the Multi-task Cascaded Convolutional Neural Network (MTCNN)<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup> and RetinaFace<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>, which have garnered recognition as leading approaches in the realm of face detection.</p></section><section id="Sec10"><h4 class="pmc_sec_title">Alignment</h4>
<p id="Par34">Face alignment entails identifying correspondences among facial features, relying on landmark fiducial points, including the eyes, nose, mouth, and jaw. This phase assumes critical importance in the context of face verification. As an illustrative example, Schroff et al.<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup> underscored the significance of face alignment following face detection, demonstrating an enhancement in the FaceNet model’s accuracy from 98.87% to 99.63%. The simplest approach to alignment involves the application of a basic 2D rigid affine transformation to align the eyes, accounting for variations in facial size and head rotation<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a>,<a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>. More advanced techniques employ 3D modeling methods to achieve frontal face alignment. Nevertheless, it is worth noting that 3D face alignment methods are often associated with heightened computational complexity and cost implications.</p></section><section id="Sec11"><h4 class="pmc_sec_title">Face processing</h4>
<p id="Par35">The preprocessing stage is frequently regarded as a pivotal phase in constructing machine learning models. Preceding the training and testing phases, it addresses inherent intra-personal variations, encompassing poses, illuminations, expressions, and occlusions. Research conducted by Ghazi et al.<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup> conclusively demonstrates that these diverse conditions continue to exert a discernible impact on the efficacy of deep face verification systems. Face-processing methodologies are classified into two distinct categories, namely, (i) one-to-many and (ii) many-to-one approaches.</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par36">One-to-many approaches: These encompass methodologies such as data augmentation, 3D modeling, autoencoder modeling, and GAN modeling<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a>–<a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>. Their primary function is to generate multiple patches or images that encapsulate pose variations derived from a single image. This facilitates the training of deep neural networks in acquiring pose-invariant representations. These strategies address the challenges associated with data acquisition by augmenting training data and expanding the gallery of test data.</p></li>
<li><p id="Par37">Many-to-one approaches: These include techniques like autoencoder modeling, CNN modeling, and GAN modeling<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a>–<a href="#CR49" class="usa-link" aria-describedby="CR49">49</a></sup>. They are designed to transform facial images, specifically by producing frontal views and reducing the variability in appearance within the test data. The objective is to standardize facial alignment and enhance comparability, simplifying face-matching.</p></li>
</ul></section><section id="Sec12"><h4 class="pmc_sec_title">Feature extraction</h4>
<p id="Par38">In developing an FV system, a critical phase involves extracting a numerical value set referred to as a feature vector or representation. It is imperative to meticulously design the feature vector to prevent the inclusion of superfluous and potentially redundant features, as this can adversely affect verification rates. In recent years, FV systems have been categorized into three distinct approaches for facial representation: (i) holistic, (ii) local, and (iii) shallow and deep learning methods.</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par39">The Holistic Face Representations methodology entails utilizing all pixels present in the input facial image to construct a low-dimensional representation, guided by specific distribution assumptions like the linear subspace<sup><a href="#CR50" class="usa-link" aria-describedby="CR50">50</a>,<a href="#CR51" class="usa-link" aria-describedby="CR51">51</a></sup> and sparse representation<sup><a href="#CR52" class="usa-link" aria-describedby="CR52">52</a>,<a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>. Nonetheless, it is widely recognized that these theoretically sound, holistic approaches demonstrate limited generalizability when applied to datasets that were not part of their training regimen.</p></li>
<li><p id="Par40">To build local face representations, face features can also be retrieved from overlapping patches in the face image at several sizes. Local features can be concatenated into a final feature vector summarizing the input face image to add holistic information. The final face representation is usually over-complete, with redundant data and excessive dimensionality. Feature selection, boosting, and dimensionality reduction techniques such as PCA and LDA are utilized to build a more compact face representation. Ahonen et al.<sup><a href="#CR54" class="usa-link" aria-describedby="CR54">54</a></sup> proposed Local Binary Patterns (LBP) for face recognition. They divide the face image into a grid to exploit local and global facial features. A histogram of LBP characteristics is generated for each cell in the grid, and the resulting face representation is concatenated.</p></li>
<li>
<p id="Par41">In facial verification, Convolutional Neural Networks (CNNs) have outperformed human capabilities on various benchmark assessments<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>. The proliferation of extensive facial datasets and improved computational resources, notably Graphics Processing Units (GPUs), has led to a notable surge in interest over the past few years in automatic feature extraction techniques centered around Convolutional Neural Networks (CNNs)<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>, as evidenced by the data in Table <a href="#Tab1" class="usa-link">1</a>. Diverse architectural configurations and loss functions have been employed to extract distinguishing identity attributes from facial images through the utilization of Deep Convolutional Neural Networks (DCNNs). This has been achieved by carefully designing loss functions to augment discriminative capacity during the training process<sup><a href="#CR55" class="usa-link" aria-describedby="CR55">55</a></sup>.</p>
<p id="Par42">One of the primary challenges faced by the facial verification (FV) research community was the one-shot learning problem. This challenge involved the development of FV systems capable of verifying a person’s identity using just a single example of their face. Historically, deep learning algorithms struggled with this scenario, as discussed in reference<sup><a href="#CR56" class="usa-link" aria-describedby="CR56">56</a></sup>—however, DeepFace<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup> successfully addressed this issue and achieved an impressive accuracy rate of 97.35% on the Labeled Faces in the Wild (LFW) benchmark dataset, approaching human-level performance. Additionally, it significantly reduced the error rate of the YouTube Face database by more than 50%. This achievement was accomplished by training a nine-layer deep neural network (CNN) with over 120 million parameters on a dataset containing four million facial images with 3D alignment for face processing.</p>
<p id="Par43">Furthermore, an end-to-end metric learning approach was tested using a Siamese neural network replicated twice during training. This network takes two images as input and outputs the degree of difference between their features, followed by a top fully connected layer that maps this information into a single logistic unit. Schroff et al.<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup> introduced FaceNet, which utilized a triplet loss function to learn a Euclidean distance metric for measuring face similarity. This approach achieved remarkable accuracies of 99.63%±0.09 with additional face alignment, 98.87%±0.15 when using a fixed center crop on LFW, and 95.12% on the YouTube Faces dataset.</p>
<p id="Par44">Notably, these methods emphasize end-to-end learning, representing the entire system directly from facial pixels rather than relying on engineered features. They also require minimal alignment, typically focusing on a tight crop around the facial region<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>.</p>
</li>
</ul>
<section class="tw xbox font-sm" id="Tab1"><h5 class="obj_head">Table 1.</h5>
<div class="caption p"><p>Face verification methods evaluated on LFW and YTF datasets.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Method</th>
<th align="left" colspan="1" rowspan="1">Loss</th>
<th align="left" colspan="1" rowspan="1">Architecture</th>
<th align="left" colspan="1" rowspan="1">Training Set</th>
<th align="left" colspan="1" rowspan="1">LFW</th>
<th align="left" colspan="1" rowspan="1">YTF</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">DeepFace<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Softmax</td>
<td align="left" colspan="1" rowspan="1">AlexNet</td>
<td align="left" colspan="1" rowspan="1">Facebook SFC (4.4M, 4K)</td>
<td align="left" colspan="1" rowspan="1">97.35%</td>
<td align="left" colspan="1" rowspan="1">91.4%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">FaceNet<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Triplet</td>
<td align="left" colspan="1" rowspan="1">Inception</td>
<td align="left" colspan="1" rowspan="1">Google (200M, 8M)</td>
<td align="left" colspan="1" rowspan="1">99.63%</td>
<td align="left" colspan="1" rowspan="1">95.12%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VGGFace<sup><a href="#CR57" class="usa-link" aria-describedby="CR57">57</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Softmax</td>
<td align="left" colspan="1" rowspan="1">VGG-16</td>
<td align="left" colspan="1" rowspan="1">VGGFace (2.6M, 2.6K)</td>
<td align="left" colspan="1" rowspan="1">98.95%</td>
<td align="left" colspan="1" rowspan="1">97.3%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SphereFace<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">A-Softmax</td>
<td align="left" colspan="1" rowspan="1">ResNet-64</td>
<td align="left" colspan="1" rowspan="1">CASIA-WebFace (0.49M, 10K)</td>
<td align="left" colspan="1" rowspan="1">99.42%</td>
<td align="left" colspan="1" rowspan="1">95.0%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ArcFace<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">ArcFace</td>
<td align="left" colspan="1" rowspan="1">ResNet-100</td>
<td align="left" colspan="1" rowspan="1">MS1M (5.8M, 85K)</td>
<td align="left" colspan="1" rowspan="1">99.83%</td>
<td align="left" colspan="1" rowspan="1">98.02%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Gate-FV<sup><a href="#CR58" class="usa-link" aria-describedby="CR58">58</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Angular</td>
<td align="left" colspan="1" rowspan="1">MDCNN</td>
<td align="left" colspan="1" rowspan="1">CASIA-WebFace (0.49M, 10K)</td>
<td align="left" colspan="1" rowspan="1">99.38%</td>
<td align="left" colspan="1" rowspan="1">94.3%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec13"><h4 class="pmc_sec_title">Face matching</h4>
<p id="Par45">In the context of facial feature extraction, the primary objective of an FV system is to determine the degree of similarity between these extracted features. This is achieved by applying similarity measurement techniques, with commonly used methods including cosine similarity<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup> and Euclidean distance<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. While Euclidean distance is a straightforward choice for comparing feature vectors, other distance metrics such as cosine similarity, Manhattan distance, histogram intersection, log-likelihood statistics, and chi-square statistics have been explored to enhance face verification performance.</p></section></section><section id="Sec14"><h3 class="pmc_sec_title">Benchmark dataset</h3>
<p id="Par46">In recent years, a discernible pattern has arisen, characterized by a transition from small-scale to large-scale experimentation, a shift from reliance on single sources to incorporating diverse sources, and a progression from laboratory-controlled settings to unconstrained real-world conditions. Table <a href="#Tab2" class="usa-link">2</a> presents a comprehensive compilation of data about a collection of benchmark datasets utilized within the academic literature. This compilation encompasses various elements, including database size quantified by the number of images, the presence of identifiable faces, and the intended applications.</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par47">Faces in the Wild (LFW)<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup> stands as one of the pioneering databases specifically tailored for the investigation of uncontrolled, “in-the-wild” face verification scenarios. This repository encompasses a considerably larger volume of images, which serve as essential evaluative material for algorithms designed for practical, real-world applications. Within the domain of face verification, LFW continues to hold its status as a key benchmark. The inception of this dataset dates back to its initial release in 2007<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>, and subsequently, it underwent updates in 2014<sup><a href="#CR65" class="usa-link" aria-describedby="CR65">65</a></sup>. It comprises 13,233 facial images, each sized at 250x250 pixels, representing 5,749 distinct individuals, with 4,069 of these individuals featuring in only a single image.</p></li>
<li><p id="Par48">YouTube Faces (YTF)<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup> The YTF dataset was curated to investigate unconstrained videos featuring matched background similarities. Comprising 3,425 videos drawn from 1,595 distinct subjects, the dataset exhibits an average of 2.15 videos per subject. Notably, video durations within the dataset range from 48 frames for the shortest clip to 6,070 frames for the longest, with an average video clip length of 181.3 frames.</p></li>
<li><p id="Par49">IARPA Janus Benchmark A (IJB-A)<sup><a href="#CR60" class="usa-link" aria-describedby="CR60">60</a></sup> encompasses a heterogeneous collection of visual data, including both images and videos, originating from a pool of 500 subjects captured in diverse real-world scenarios. Notably, for each subject included in the dataset, a minimum of five images and one video is available. The IJB-A dataset comprises 5,712 images and 2,085 videos, translating to an average of 11.4 images and 4.2 videos per subject. At the granularity of individual subjects, the dataset is structured into ten distinct, randomly generated training and testing splits, each encompassing all 500 subjects in IJB-A. For each of these splits, a subset of 333 subjects is randomly allocated to the training set, serving as a foundation for algorithmic model development and acquiring insights into facial variations germane to the Janus challenge. The remaining 167 individuals are assigned to the testing set for evaluation and validation.</p></li>
<li><p id="Par50">Cross-Pose LFW (CPLFW)<sup><a href="#CR63" class="usa-link" aria-describedby="CR63">63</a></sup> database represents a revitalized iteration of the Labeled Faces in the Wild (LFW) dataset, which serves as the prevailing benchmark for evaluating unconstrained face verification algorithms. Within the LFW framework, ten distinct sets of image pairs have been meticulously constructed for cross-validation, each containing 300 positive and negative pairs. These subsets are organized based on unique subject identities, ensuring each identity is exclusively featured in a single subgroup. In contrast, the CPLFW dataset employs an analogous partitioning approach, creating ten partitions or “folds,” mirroring the identity distributions found in the original LFW folds. Notably, each individual within the CPLFW dataset is represented by a set of two to three images.</p></li>
<li><p id="Par51">MegaFace<sup><a href="#CR61" class="usa-link" aria-describedby="CR61">61</a></sup> stands as a substantial publicly available face recognition training dataset that has established itself as an industry benchmark. Within MegaFace, a comprehensive compilation of 4,753,320 facial images is available , representing a diverse pool of 672,057 distinct identities sourced from a repository of 3,311,471 photographs from the personal albums of 48,383 users on the Flickr platform. Notably, while the photos featured in MegaFace predominantly possessed Creative Commons licenses, most did not permit commercial usage.</p></li>
<li><p id="Par52">CASIA-Webface<sup><a href="#CR59" class="usa-link" aria-describedby="CR59">59</a></sup> dataset is a valuable resource for addressing face verification and recognition challenges. Comprising a collection of 500,000 facial images featuring 10,575 distinct celebrities, these images were sourced from publicly available online sources, capturing subjects in uncontrolled, real-world settings, thus characterizing the “in the wild” nature of the dataset acquisition process.</p></li>
<li><p id="Par53">VGGFace2<sup><a href="#CR62" class="usa-link" aria-describedby="CR62">62</a></sup> consists of 3.3 million facial images of celebrities drawn from a pool of 9,000 unique identities, with an average of 362 images available per subject, the creators of the dataset prioritized the meticulous reduction of label inaccuracies, alongside the deliberate inclusion of a wide spectrum of facial poses and age groups. These meticulous efforts have rendered the VGGFace2 dataset an optimal selection for training advanced deep-learning models designed to excel in tasks related to facial analysis.</p></li>
<li><p id="Par54">VGGface<sup><a href="#CR57" class="usa-link" aria-describedby="CR57">57</a></sup> dataset comprises an extensive compilation of 2.6 million facial images, encompassing 2,622 unique identities. Each identity is accompanied by an associated text file with image URLs and corresponding facial detection information.</p></li>
</ul>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Common face verification datasets</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Database Name</th>
<th align="left" colspan="1" rowspan="1"># Images</th>
<th align="left" colspan="1" rowspan="1"># Identities</th>
<th align="left" colspan="1" rowspan="1">Availability</th>
<th align="left" colspan="1" rowspan="1">Type</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">LFW<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">13,233</td>
<td align="left" colspan="1" rowspan="1">5,749</td>
<td align="left" colspan="1" rowspan="1">Public</td>
<td align="left" colspan="1" rowspan="1">Test</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">YTF<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">3,425 videos</td>
<td align="left" colspan="1" rowspan="1">1,595</td>
<td align="left" colspan="1" rowspan="1">Public</td>
<td align="left" colspan="1" rowspan="1">Test</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Facebook<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">4.4M</td>
<td align="left" colspan="1" rowspan="1">4K</td>
<td align="left" colspan="1" rowspan="1">Private</td>
<td align="left" colspan="1" rowspan="1">Train</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CASIA-WebFace<sup><a href="#CR59" class="usa-link" aria-describedby="CR59">59</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">494,414</td>
<td align="left" colspan="1" rowspan="1">10,575</td>
<td align="left" colspan="1" rowspan="1">Public</td>
<td align="left" colspan="1" rowspan="1">Train</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Google-FaceNet<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">200M</td>
<td align="left" colspan="1" rowspan="1">8M</td>
<td align="left" colspan="1" rowspan="1">Private</td>
<td align="left" colspan="1" rowspan="1">Train</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VGGFace<sup><a href="#CR57" class="usa-link" aria-describedby="CR57">57</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">2.6M</td>
<td align="left" colspan="1" rowspan="1">2.6K</td>
<td align="left" colspan="1" rowspan="1">Public</td>
<td align="left" colspan="1" rowspan="1">Train</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">IJB-A<sup><a href="#CR60" class="usa-link" aria-describedby="CR60">60</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">5,712</td>
<td align="left" colspan="1" rowspan="1">500</td>
<td align="left" colspan="1" rowspan="1">Public</td>
<td align="left" colspan="1" rowspan="1">Test</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MegaFace<sup><a href="#CR61" class="usa-link" aria-describedby="CR61">61</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">4.7M</td>
<td align="left" colspan="1" rowspan="1">672,057</td>
<td align="left" colspan="1" rowspan="1">Public</td>
<td align="left" colspan="1" rowspan="1">Train</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VGGFace2<sup><a href="#CR62" class="usa-link" aria-describedby="CR62">62</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">3.31M</td>
<td align="left" colspan="1" rowspan="1">9,131</td>
<td align="left" colspan="1" rowspan="1">Public</td>
<td align="left" colspan="1" rowspan="1">Train</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CPLFW<sup><a href="#CR63" class="usa-link" aria-describedby="CR63">63</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">11,652</td>
<td align="left" colspan="1" rowspan="1">3,968</td>
<td align="left" colspan="1" rowspan="1">Public</td>
<td align="left" colspan="1" rowspan="1">Test</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">WebFace260M<sup><a href="#CR64" class="usa-link" aria-describedby="CR64">64</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">260M</td>
<td align="left" colspan="1" rowspan="1">4M</td>
<td align="left" colspan="1" rowspan="1">Public</td>
<td align="left" colspan="1" rowspan="1">Train</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec15"><h3 class="pmc_sec_title">Evaluation metrics</h3>
<p id="Par55">The facial matching procedure involves the computation of the dissimilarity measure between a given pair of facial images, which is subsequently compared to a predefined threshold. When the calculated dissimilarity measure falls below this threshold, the pair of faces is classified as belonging to the same individual; otherwise, they are deemed to represent distinct individuals. This categorization identifies correctly matched pairs as either true positives (indicating same-person pairs) or true negatives (indicating different-person pairs). Within this context, two types of errors may occur: (i) false positives, also known as false acceptances, correspond to instances where different individuals are erroneously identified as the same person, and (ii) false negatives, or false rejections, occur when the same individual is mistakenly categorized as distinct individuals. The assessment of facial verification performance relies on the evaluation of these two error types, with the utilization of the subsequent metrics:</p>
<p id="Par56">Accuracy represents the percentage of truly recognized pairs, both positive and negative.</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/2b269dfbf281/d33e999.gif" loading="lazy" id="d33e999" alt="graphic file with name d33e999.gif"></td>
<td class="label">1</td>
</tr></table>
<p>where the numerator represents the Number of sucessful pairwise matches and the denominator represents the Total number of image pairs. Verification Accuracy (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/61c6e424cbc5/d33e1006.gif" loading="lazy" id="d33e1006" alt="Inline graphic"></span>) represents the acceptance threshold and is determined using cross-validation.</p>
<p id="Par57">We also report the True Accept Rate at a pre-determined False Accept Rate. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/61c6e424cbc5/d33e1014.gif" loading="lazy" id="d33e1014" alt="Inline graphic"></span> is determined via a Receiver Operating Characteristic (ROC) curve. Formally,</p>
<table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/51ebcbc9f0ca/d33e1020.gif" loading="lazy" id="d33e1020" alt="graphic file with name d33e1020.gif"></td>
<td class="label">2</td>
</tr></table>
<table class="disp-formula p" id="Equ3"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/134794a3aa18/d33e1026.gif" loading="lazy" id="d33e1026" alt="graphic file with name d33e1026.gif"></td>
<td class="label">3</td>
</tr></table>
<p>Equal Error Rate (EER) is the error when false positive rate (FPR) and false negative rate (FNR) are the same, which is found by varying the threshold. Receiver Operating Characteristic (ROC) is the curve of true positive rate (TPR) against false positive rate (FPR) that is calculated by varying the threshold.</p></section></section><section id="Sec16"><h2 class="pmc_sec_title">FV systems vulnerabilities</h2>
<p id="Par59">Despite the impressive verification performance achieved using deep learning models, the FV systems remain vulnerable to the growing threat of face attacks, such as face spoofing and adversarial perturbations, in both the physical and digital domains. For example, an attacker can hide his identity by using a printed photograph, a worn mask<sup><a href="#CR66" class="usa-link" aria-describedby="CR66">66</a></sup>, or even an image displayed on another electronic device to present a fake face to the biometric sensor, or intruders can assume a victim’s identity by digitally swapping their face with the victim’s face image<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>.</p>
<p id="Par60">There are three types of facial attacks depicted in Fig. <a href="#Fig6" class="usa-link">6</a>: (i) Spoofing attacks: physical domain artifacts such as 3D masks, eyeglasses, and replaying videos<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>, (ii) Adversarial perturbation attacks: imperceptible noises added to probes to evade FV systems, and (iii) Digital manipulation attacks: entirely or partially modified photo-realistic faces using generative models<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. There are various attack types in each of these categories. For example, 13 common types of spoofing attacks<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>. Similarly, in adversarial and digital manipulation attacks, each attack model is designed with unique objectives and losses and can be regarded as one attack type.</p>
<figure class="fig xbox font-sm" id="Fig6"><h3 class="obj_head">Fig. 6.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/6693af9d4982/41598_2025_15753_Fig6_HTML.jpg" loading="lazy" id="MO6" height="337" width="766" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The broad categorization of facial attack types aimed to deceive the FV systems.</p></figcaption></figure><p id="Par61">Several face-attack defense approaches have been proposed to protect FV systems from these attacks. Because the exact type of face attack may not be known a priori, a generalizable defense that can defend an FV system against any of the three attack categories is critical.</p>
<ul class="list" style="list-style-type:disc">
<li>
<p id="Par62">Spoof Attacks: are presentation attacks targeting facial recognition systems, as illustrated in Fig. <a href="#Fig7" class="usa-link">7</a>, encompass various physical counterfeiting techniques necessitating active engagement by actors. These methods encompass the utilization of tangible counterfeits like 3D printed masks, printed images on paper, or digital tools such as video replays on mobile devices, all of which enable the impersonation of an individual’s identity or the concealing of the attacker’s identity. The ease with which an assailant can employ these tactics, such as displaying videos featuring the victim’s visage or submitting printed representations of the victim to a Facial Verification (FV) system<sup><a href="#CR67" class="usa-link" aria-describedby="CR67">67</a></sup>.</p>
<p id="Par63">Even if a countermeasure system were in place, leveraging depth sensors to detect face presentation attacks, it would still be susceptible to more advanced subterfuge techniques. Attackers may resort to using 3D masks<sup><a href="#CR68" class="usa-link" aria-describedby="CR68">68</a></sup>, cosmetic disguises, or even virtual reality simulations<sup><a href="#CR69" class="usa-link" aria-describedby="CR69">69</a></sup>, thereby enabling the execution of more intricate and sophisticated attacks.</p>
</li>
<li>
<p id="Par64">Adversarial Perturbation Attacks: Most facial verification (FV) models are predominantly constructed using Deep Convolutional Neural Networks (DCNNs) and have consistently demonstrated impressive performance and high accuracy in recent years. Nonetheless, DCNNs exhibit susceptibility to adversarial examples, which are generated through minor perturbations introduced into input samples<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a>–<a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. Adversarial perturbations, exemplified in Fig. <a href="#Fig8" class="usa-link">8</a>, can be defined as minimal alterations represented by <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cd8253eb5575/d33e1108.gif" loading="lazy" id="d33e1108" alt="Inline graphic"></span>, where the addition of this perturbation to the input image x, denoted as (x + <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cd8253eb5575/d33e1114.gif" loading="lazy" id="d33e1114" alt="Inline graphic"></span>), results in the misclassification of the input by deep learning models. Despite the imperceptibility of the perturbation to the human eye, it constitutes an adversarial example in image classification, capable of causing CNNs to misclassify the image. According to certain seminal works, such as<sup><a href="#CR70" class="usa-link" aria-describedby="CR70">70</a></sup>, the emergence of adversarial examples can be attributed to the limited generalization capabilities of DNN models, possibly stemming from the high complexity of their architectural design. The investigation into the existence of adversarial examples holds significance, as it can provide valuable insights for designing more robust models and enhancing our comprehension of existing deep learning frameworks.</p>
<p id="Par65">Malicious actors can manipulate their facial images to deceive FV systems, leading to two primary types of attacks: impersonation attacks, where the attacker aims to be recognized as a target victim, and obfuscation attacks, where the attacker seeks to be matched with a different identity within the system. However, the adversarial facial image generated by such attacks should appear legitimate to human observers. In contrast, face presentation attacks, as depicted in Fig. <a href="#Fig7" class="usa-link">7</a>, involve the attacker physically presenting a fake face as the target identity to the FV system. These attacks are typically more conspicuous to human observers, especially in situations involving human operators, such as those in airports.</p>
<p id="Par66">Notably, adversarial examples possess the characteristic of transferability, meaning that adversarial examples created to target one specific victim model are also highly likely to mislead other models. This property of transferability is often exploited in black-box attack techniques<sup><a href="#CR71" class="usa-link" aria-describedby="CR71">71</a></sup>. If attackers obscure the model’s parameters, they can resort to attacking alternative models, thereby leveraging the portability of the generated adversarial samples. Defense methods also harness the property of portability, as demonstrated in<sup><a href="#CR72" class="usa-link" aria-describedby="CR72">72</a></sup>, by utilizing adversarial training with samples created to perturb one type of model to bolster the defense of another type of model.</p>
<p id="Par67">In this section, our primary focus is on exploring adversarial perturbations, as detailed in Section 4.1.</p>
</li>
<li><p id="Par68">Digital Manipulation Attacks: is the process of affording the capacity to comprehensively or partially alter genuine facial images using Variational Auto Encoders (VAEs) and Generative Adversarial Networks (GANs)<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. Figure <a href="#Fig9" class="usa-link">9</a> shows different face presentation attacks based on digital manipulation. These digital manipulation attacks may be categorized into distinct types, as outlined below:</p></li>
</ul>
<figure class="fig xbox font-sm" id="Fig7"><h3 class="obj_head">Fig. 7.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/782d73e56f61/41598_2025_15753_Fig7_HTML.jpg" loading="lazy" id="MO7" height="242" width="750" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Face presentation attacks require a physical artifact. (<strong>a</strong>) real face, from (<strong>b-d</strong>) represents three types of face presentation attacks: (<strong>b</strong>) printed photograph, (<strong>c</strong>) replaying the targeted person’s video on a smartphone, and (<strong>d</strong>) a 3D mask of the target’s face (Image Credit: Google Images).</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig8"><h3 class="obj_head">Fig. 8.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig8_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/5c62854ff3a1/41598_2025_15753_Fig8_HTML.jpg" loading="lazy" id="MO8" height="162" width="692" alt="Fig. 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Image samples for probe face images and their corresponding synthesized ones. (<strong>a</strong>) gallery enrolled images and (<strong>b</strong>) probed images for the same person (<strong>c</strong>) FGSM (<strong>d</strong>) PGD (<strong>e</strong>) AdvFaces. Euclidean distance scores were obtained by comparing (<strong>b-e</strong>) to the enrolled images.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig9"><h3 class="obj_head">Fig. 9.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig9_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/ef01b2d450a3/41598_2025_15753_Fig9_HTML.jpg" loading="lazy" id="MO9" height="738" width="747" alt="Fig. 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Examples of digitally manipulated faces from different sources such as FFHQ, CelebA, FaceForensics++, FaceAPP and StarGAN, PGGAN, StyleGAN datasets (Image Credit:<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>).</p></figcaption></figure><p id="Par69">
<strong><em>Identity Swapping:</em></strong>
</p>
<p id="Par70">These methods replace one person’s face with another person’s face digitally. For example, FaceSwap<sup><a href="#CR73" class="usa-link" aria-describedby="CR73">73</a></sup> contains well-known actors in movie scenes in which they have never been featured before. DeepFakes also uses deep learning algorithms to produce face swaps.</p>
<p id="Par71">
<strong><em>Expression Swapping:</em></strong>
</p>
<p id="Par72">These methods exchange expressions in real time using only RGB cameras. Expressions in the facial image can be digitally and artificially replaced by others<sup><a href="#CR74" class="usa-link" aria-describedby="CR74">74</a></sup>.</p>
<p id="Par73">
<strong><em>Attribute Manipulation:</em></strong>
</p>
<p id="Par74">studies like StarGAN<sup><a href="#CR75" class="usa-link" aria-describedby="CR75">75</a></sup> and STGAN<sup><a href="#CR76" class="usa-link" aria-describedby="CR76">76</a></sup> use the latest GANs to manipulate attributes by changing single or multiple traits in a facial image, such as gender, age, skin color, hair, and glasses.</p>
<p id="Par75">
<strong><em>Entire Face Synthesis:</em></strong>
</p>
<p id="Par76">An attacker can easily synthesize entire facial images of unknown identities, which are so realistic that even humans have difficulty assessing whether they are authentic or manipulated<sup><a href="#CR77" class="usa-link" aria-describedby="CR77">77</a></sup>. Due to the advent of GANs and large-scale, high-resolution facial data sets.</p>
<section id="Sec17"><h3 class="pmc_sec_title">Adversarial example generation</h3>
<p id="Par77">In the realm of computer vision, a multitude of techniques for generating adversarial examples have been developed. These methodologies aim to introduce subtle perturbations into specific images, thereby inducing erroneous classifications by machine learning models<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a>,<a href="#CR78" class="usa-link" aria-describedby="CR78">78</a></sup>. To formalize the concept of adversarial examples, we introduce the following notation: let x represent an input that is accurately classified as y, and f(x) denote the classification decision made by the machine learning model. An adversarial example, denoted as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq5"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/f69f3d5555dd/d33e1273.gif" loading="lazy" id="d33e1273" alt="Inline graphic"></span> = x + <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq6"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cd8253eb5575/d33e1279.gif" loading="lazy" id="d33e1279" alt="Inline graphic"></span>, is formed through the addition of a perturbation <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq7"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cd8253eb5575/d33e1285.gif" loading="lazy" id="d33e1285" alt="Inline graphic"></span>, satisfying the condition:</p>
<table class="disp-formula p" id="Equ4"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cfa7ab347475/d33e1291.gif" loading="lazy" id="d33e1291" alt="graphic file with name d33e1291.gif"></td>
<td class="label">4</td>
</tr></table>
<p>Generating adversarial examples ultimately involves identifying perturbations in input data that remain imperceptible to human observers yet lead to misclassifications by vulnerable machine learning models. Additionally, researchers have demonstrated the creation of a general perturbation applied to a dataset, resulting in the high likelihood of misclassification for numerous normal images<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>. Szegedy et al.<sup><a href="#CR70" class="usa-link" aria-describedby="CR70">70</a></sup> unveiled the existence of adversarial examples and introduced the first algorithm capable of reliably detecting adversarial perturbations. The Fast Gradient Sign Method (FGSM), proposed by Goodfellow et al.<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup> and illustrated in Fig. <a href="#Fig10" class="usa-link">10</a>, forms the basis of this algorithm. Kurakin et al.<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup> extended the FGSM method to generate larger quantities of adversarial examples, enhance the quality of the generated adversarial models, and enable the execution of targeted attacks. However, it is worth noting that some of these extensions come at the expense of increased computational resources.</p>
<figure class="fig xbox font-sm" id="Fig10"><h4 class="obj_head">Fig. 10.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig10_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/3dca259d7e74/41598_2025_15753_Fig10_HTML.jpg" loading="lazy" id="MO10" height="283" width="747" alt="Fig. 10"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig10/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>By adding small perturbations (distortions) to the original image, which results in the model labeling this image as a gibbon, with high confidence (Image Credit:<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>).</p></figcaption></figure><p id="Par78">Moosavi-Dezfooli et al.<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup> introduced Deepfool, designed initially for untargeted attacks with a focus on improving perturbations in the L<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq8"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/e00f5e278321/d33e1324.gif" loading="lazy" id="d33e1324" alt="Inline graphic"></span> norm but adaptable to any L<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq9"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/826ff17db394/d33e1330.gif" loading="lazy" id="d33e1330" alt="Inline graphic"></span> norm. Their approach is highly effective and can discover smaller perturbations compared to Szegedy et al.’s L-BFGS approach<sup><a href="#CR70" class="usa-link" aria-describedby="CR70">70</a></sup>. The fundamental idea behind their proposed algorithm is an iterative approximation of the hyperplanes that separate distinct classes and the distances between perturbed inputs and decision boundaries, estimated through orthogonal projections. While Deepfool cannot guarantee the discovery of the optimal solution with the minimal perturbation for a given input, the authors assert that the resulting perturbation approximates the minimal perturbation effectively.</p>
<p id="Par79">Papernot et al.<sup><a href="#CR79" class="usa-link" aria-describedby="CR79">79</a></sup> introduced the Jacobian-based Saliency Map Attack (JSMA), optimized for the L0 distance metric. This method leverages the gradients of the records or softmax units with respect to the input image to compute a saliency map, which approximates the impact of each pixel on the image’s classification. JSMA perturbs the most significant pixels until a targeted attack succeeds or the number of perturbed pixels exceeds a predefined threshold.</p>
<p id="Par80">Carlini and Wagner<sup><a href="#CR80" class="usa-link" aria-describedby="CR80">80</a></sup> proposed three algorithmic models for generating optimized adversarial examples with L<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq10"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/04e67736b5b5/d33e1352.gif" loading="lazy" id="d33e1352" alt="Inline graphic"></span>, L<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq11"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/e00f5e278321/d33e1358.gif" loading="lazy" id="d33e1358" alt="Inline graphic"></span> or L<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq12"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/1a049ef7896f/d33e1364.gif" loading="lazy" id="d33e1364" alt="Inline graphic"></span> norms. All three variants can perform targeted attacks by minimizing the respective objective functions, which can also govern prediction confidence. Finally, Sabbour et al.<sup><a href="#CR81" class="usa-link" aria-describedby="CR81">81</a></sup> focus on directly manipulating deep representations through imperceptibly small perturbations instead of inducing explicit misclassifications. While the previously mentioned methods aim to create adversarial models leading to misclassified labels, this approach seeks to transform the internal representations of an image to closely resemble those of an image from a different category with imperceptible perturbations.</p>
<p id="Par81">Recent methods have focused on improving the transferability of adversarial examples, ensuring effectiveness even when attackers have no direct knowledge of the target model. Zhou et al.<sup><a href="#CR82" class="usa-link" aria-describedby="CR82">82</a></sup> introduced the Diverse Parameters Augmentation (DPA) method, which diversifies surrogate models by training multiple intermediate checkpoints with varied parameter initializations, significantly enhancing adversarial transferability for face recognition tasks. Complementarily, research by Yu et al.<sup><a href="#CR83" class="usa-link" aria-describedby="CR83">83</a></sup> has shown that embedding carefully designed triggers within adversarial examples (trigger activation techniques) can effectively evade traditional defensive mechanisms, further increasing the robustness and transferability of generated adversarial examples.</p></section><section id="Sec18"><h3 class="pmc_sec_title">Adversarial attacks on image classification</h3>
<p id="Par82">In image classification, adversarial examples refer to intentionally crafted images that closely resemble their original counterparts but can elicit erroneous predictions from the classifier. These adversarial perturbations must be imperceptible to human observers. Therefore, the investigation of adversarial examples within the domain of images holds significant importance for two primary reasons: (a) the perceptual similarity between counterfeit and genuine images is readily discernible to human observers, and (b) the structural simplicity of both image data and image classifiers, in contrast to other domains such as graphs or audio, has led to numerous studies treating image classifiers as a standard case.</p>
<p id="Par83">Many adversarial example generation methods have been proposed in recent years, as documented in Table <a href="#Tab3" class="usa-link">3</a>. As evidenced in previous research, many of these attack methods can be categorized into intensity-based attacks<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup> and geometry-based attacks<sup><a href="#CR84" class="usa-link" aria-describedby="CR84">84</a>,<a href="#CR85" class="usa-link" aria-describedby="CR85">85</a></sup>. For instance, Goodfellow et al.<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup> introduced a white-box attack named the Fast Gradient Sign method (FGSM), which introduces subtle perturbations to various regions of the original image via back-propagation through the target model, causing the model to confidently misclassify the adversarial image into another category, as illustrated in Fig. <a href="#Fig10" class="usa-link">10</a>. However, their approach is associated with several drawbacks, such as the excessive computational time required for generating adversarial examples and the resultant degradation in the perceptual quality of the generated images. Additionally, it relies on the white-box attack paradigm, which proves impractical in real-world scenarios. Moreover, the requirement of applying perturbations to all regions of the image and relying on softmax probabilities for evading an image classifier is not viable, especially in cases like the FV system, where the classifier does not employ a fixed set of classes (identities). Moosavi-Dezfooli et al.<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup> proposed image-agnostic adversarial attacks, which entail the generation of universal perturbations capable of deceiving the classifier across a wide range of image types and models. Recent work also shows that backdoor-style triggers, e.g., universal ‘master-key’ patterns, can force a network to verify an impostor as the genuine user<sup><a href="#CR86" class="usa-link" aria-describedby="CR86">86</a></sup>.</p>
<section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Comparison of different adversarial attack methods.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Attack Method</th>
<th align="left" colspan="1" rowspan="1">Attack Settings</th>
<th align="left" colspan="1" rowspan="1">Similarity Metric</th>
<th align="left" colspan="1" rowspan="1">Scope</th>
<th align="left" colspan="1" rowspan="1">Domain</th>
<th align="left" colspan="1" rowspan="1">Attack Objectives</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">White-Box</td>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq13"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/5014320c08cc/d33e1480.gif" loading="lazy" id="d33e1480" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq14"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/89ac728b71d6/d33e1486.gif" loading="lazy" id="d33e1486" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">Universal</td>
<td align="left" colspan="1" rowspan="1">Classification</td>
<td align="left" colspan="1" rowspan="1">Obfuscation</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Face Recognition<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">White-Box</td>
<td align="left" colspan="1" rowspan="1">Physical</td>
<td align="left" colspan="1" rowspan="1">Image-specific</td>
<td align="left" colspan="1" rowspan="1">Recognition</td>
<td align="left" colspan="1" rowspan="1">Impersonation</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PGD<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">White-Box</td>
<td align="left" colspan="1" rowspan="1"><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq15"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/5014320c08cc/d33e1525.gif" loading="lazy" id="d33e1525" alt="Inline graphic"></span></td>
<td align="left" colspan="1" rowspan="1">Universal</td>
<td align="left" colspan="1" rowspan="1">Classification</td>
<td align="left" colspan="1" rowspan="1">Obfuscation</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">A3GN<sup><a href="#CR84" class="usa-link" aria-describedby="CR84">84</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">White-Box</td>
<td align="left" colspan="1" rowspan="1">Cosine</td>
<td align="left" colspan="1" rowspan="1">Image-specific</td>
<td align="left" colspan="1" rowspan="1">Recognition</td>
<td align="left" colspan="1" rowspan="1">Impersonation</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Evolutionary Optimization<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Black-Box</td>
<td align="left" colspan="1" rowspan="1">–</td>
<td align="left" colspan="1" rowspan="1">Image-specific</td>
<td align="left" colspan="1" rowspan="1">Recognition</td>
<td align="left" colspan="1" rowspan="1">Impersonation</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">GFLM<sup><a href="#CR85" class="usa-link" aria-describedby="CR85">85</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">White-Box</td>
<td align="left" colspan="1" rowspan="1">–</td>
<td align="left" colspan="1" rowspan="1">Image-specific</td>
<td align="left" colspan="1" rowspan="1">Recognition</td>
<td align="left" colspan="1" rowspan="1">Obfuscation</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">AdvFaces<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Semi-White-Box</td>
<td align="left" colspan="1" rowspan="1"><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq16"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/89ac728b71d6/d33e1598.gif" loading="lazy" id="d33e1598" alt="Inline graphic"></span></td>
<td align="left" colspan="1" rowspan="1">Image-specific</td>
<td align="left" colspan="1" rowspan="1">Recognition</td>
<td align="left" colspan="1" rowspan="1">Both</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">GAP++<sup><a href="#CR88" class="usa-link" aria-describedby="CR88">88</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">White-Box</td>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq17"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/5014320c08cc/d33e1620.gif" loading="lazy" id="d33e1620" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq18"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/89ac728b71d6/d33e1626.gif" loading="lazy" id="d33e1626" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq19"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/efe051c26727/d33e1632.gif" loading="lazy" id="d33e1632" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">Universal</td>
<td align="left" colspan="1" rowspan="1">–</td>
<td align="left" colspan="1" rowspan="1">Obfuscation</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par84">In image classification scenarios, adversarial threats have evolved towards more stealthy and practical black-box attack strategies. For instance, Park et al.<sup><a href="#CR87" class="usa-link" aria-describedby="CR87">87</a></sup> proposed the <em>Mind the Gap</em> technique, which analyzes incremental query updates to systematically craft adversarial images under black-box conditions. This approach demonstrates how iterative query strategies can bypass detection mechanisms, underscoring the growing complexity and sophistication of adversarial attacks on image classification models.</p></section><section id="Sec19"><h3 class="pmc_sec_title">Adversarial attacks on face verification (FV) systems</h3>
<p id="Par85">Utilizing deep learning models, the Facial Verification (FV) system can attain a noteworthy True Accept Rate (TAR) of 99.27% while maintaining an impressively low False Accept Rate (FAR) of 0.001% with genuine face pairs when leveraging FaceNet<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. This level of performance is attributed to the ample availability of extensive facial datasets for training these models and the incorporation of Convolutional Neural Network (CNN) architectures, as illustrated in Table <a href="#Tab1" class="usa-link">1</a>. However, it should be noted that CNN models are susceptible to adversarial perturbations, as elucidated in Table <a href="#Tab3" class="usa-link">3</a>. Even minute imperceptible perturbations, undetectable to the human eye, can lead to misclassification by the CNN<sup><a href="#CR70" class="usa-link" aria-describedby="CR70">70</a></sup>.</p>
<p id="Par86">Notwithstanding their commendable verification capabilities, mainstream FV systems are still exposed to an escalating risk posed by adversarial examples, as depicted in Figure <a href="#Fig8" class="usa-link">8</a>. To compromise the FV system, an adversary can intentionally manipulate their facial image to deceive the FV system into incorrectly identifying them as the intended target (impersonation attack) or as a different individual (obfuscation attack). Crucially, the manipulated facial image must convincingly appear as a legitimate representation of the adversary to human observers, as illustrated in Fig. <a href="#Fig11" class="usa-link">11</a>.</p>
<figure class="fig xbox font-sm" id="Fig11"><h4 class="obj_head">Fig. 11.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig11_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/fdd9a2d442f4/41598_2025_15753_Fig11_HTML.jpg" loading="lazy" id="MO11" height="533" width="746" alt="Fig. 11"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig11/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Examples of adversarial Attacks: (<strong>a</strong>) AdvFaces, (<strong>b</strong>) FGSM, (<strong>c</strong>) PGD.</p></figcaption></figure><p id="Par87">However, it is essential to note that in the context of adversarial faces, the adversary need not actively engage in the authentication process when comparing their probe and gallery images. Conversely, in scenarios involving presentation attacks, such as the use of masks or the replay of images/videos of genuine individuals, the adversary must actively participate. Such active participation may be discernible in situations involving human operators. Hence, it is imperative to thoroughly investigate the spectrum of attacks to which CNN models are susceptible to comprehensively assess their vulnerabilities and limitations. This understanding should inform the design and modification of CNN models by developers in the future.</p>
<p id="Par88">Recently, a prominent area of research has emerged in the field of face verification, focusing on the generation of adversarial examples. Researchers such as Bose et al.<sup><a href="#CR89" class="usa-link" aria-describedby="CR89">89</a></sup> have pursued this inquiry by employing constrained optimization techniques to create adversarial examples that elude face detection systems. Similarly, Dong et al.<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup> have introduced an evolutionary optimization approach for generating adversarial faces, particularly in black-box scenarios. Nevertheless, this method necessitates a substantial number of queries to yield satisfactory results.</p>
<p id="Par89">Song et al.<sup><a href="#CR84" class="usa-link" aria-describedby="CR84">84</a></sup> have contributed to this discourse by proposing an attention-driven adversarial attack generative network (A3GN) tailored for producing counterfeit face images within a white-box framework, emphasizing impersonation attacks. However, their method exhibits certain limitations, including a requirement for access to gallery-enrolled face images, which may be impractical in real-world settings. Furthermore, it mandates at least five images of the target individual for training and is confined to targeting a single subject. The image generation process is characterized by low-quality and time-consuming operations.</p>
<p id="Par90">Many of these shortcomings have been addressed by Deb et al.<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>, who introduced a semi-white box attack known as ’Advfaces.’ This method generates adversarial examples by feeding the network and necessitates only a single face image of the target subject for both training and inference. Importantly, Advfaces produces adversarial images of high quality and perceptual realism , posing no discernible threat to the human eye. Its efficacy is demonstrated by its ability to outperform state-of-the-art face matches, achieving an impressive attack success rate of 97.22% in obfuscation attacks. Moreover, Advfaces exhibits the property of transferability, wherein adversarial examples designed to target one victim model have a high likelihood of confounding other models.</p>
<p id="Par91">We provide a succinct overview of the three popular obfuscation adversarial attacks FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, PGD<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>, and Advfaces <sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>, on face verification systems, elucidating the process by which the magnitude of perturbation is precisely determined.</p>
<ul class="list" style="list-style-type:disc">
<li><div class="p" id="Par92">Fast Gradient Sign Method (FGSM)<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>: FGSM is a fast method to generate an adversarial perturbation. It computes the gradient of the loss function <em>J</em> of the model concerning the image vector <em>x</em> to get the direction of pixel change and generates adversarial examples <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq20"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/4d1ee549bb85/d33e1738.gif" loading="lazy" id="d33e1738" alt="Inline graphic"></span> by minimizing the probability of the true class. FGSM perturbations can be computed by minimizing either the L1, L2 or L<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq21"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/2441afcd9634/d33e1744.gif" loading="lazy" id="d33e1744" alt="Inline graphic"></span> norms according to the following equations<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>: <table class="disp-formula p" id="Equ10"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/5ebf73b46809/d33e1755.gif" loading="lazy" id="d33e1755" alt="graphic file with name d33e1755.gif"></td></tr></table> , where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq22"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cd8253eb5575/d33e1761.gif" loading="lazy" id="d33e1761" alt="Inline graphic"></span> controls the perturbation magnitude and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq23"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/edd700580a15/d33e1767.gif" loading="lazy" id="d33e1767" alt="Inline graphic"></span> is the model parameters.</div></li>
<li><div class="p" id="Par93">Projected Gradient Descent (PGD)<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>: PGD is an improved version of FGSM by applying it multiple times with a small step size <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq24"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/c858c226cc91/d33e1780.gif" loading="lazy" id="d33e1780" alt="Inline graphic"></span>. The adversarial examples were generated in multiple iterations by the following equation: <table class="disp-formula p" id="Equ5"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/49b775709eda/d33e1786.gif" loading="lazy" id="d33e1786" alt="graphic file with name d33e1786.gif"></td>
<td class="label">5</td>
</tr></table> , where Clip<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq25"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/1357e259694e/d33e1793.gif" loading="lazy" id="d33e1793" alt="Inline graphic"></span>(<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq26"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/d9aa3149d268/d33e1799.gif" loading="lazy" id="d33e1799" alt="Inline graphic"></span>) clips updated images to constrain it within the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq27"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cd8253eb5575/d33e1806.gif" loading="lazy" id="d33e1806" alt="Inline graphic"></span>-ball of X (i.e.,limits the change of the generated adversarial image in each iteration). The initial perturbation was a random point within the allowed <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq28"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cd8253eb5575/d33e1812.gif" loading="lazy" id="d33e1812" alt="Inline graphic"></span>-ball, and the search was repeated multiple times to avoid falling into the local minimum.</div></li>
<li><div class="p" id="Par94">AdvFaces<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>: is a neural network model developed by Debayan et al. to generate perturbations in the silent regions of the face images without reducing the image quality. It consists of three components: a generator, a discriminator, and a face matcher. For the generator <em>G</em>, which takes an input image <em>x</em> and generates an adversarial image, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq29"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/8590d3f82470/d33e1831.gif" loading="lazy" id="d33e1831" alt="Inline graphic"></span>, by adding an adversarial mask <em>G</em>(<em>x</em>) with minimal perturbation that is similar to the original image, using the following <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq30"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/89ac728b71d6/d33e1844.gif" loading="lazy" id="d33e1844" alt="Inline graphic"></span> norm function: <table class="disp-formula p" id="Equ6"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/74202513fe90/d33e1850.gif" loading="lazy" id="d33e1850" alt="graphic file with name d33e1850.gif"></td>
<td class="label">6</td>
</tr></table> , where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq31"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cd8253eb5575/d33e1857.gif" loading="lazy" id="d33e1857" alt="Inline graphic"></span> represents the minimum amount of perturbation. During the training process, AdvFace used a face matcher, <em>F</em>, to supervise the training process. AdvFace minimizes the cosine similarity between the original image and the generated perturbed one using the following identity loss function: <table class="disp-formula p" id="Equ7"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/f50d92927816/d33e1866.gif" loading="lazy" id="d33e1866" alt="graphic file with name d33e1866.gif"></td>
<td class="label">7</td>
</tr></table> As the goal of the obfuscation attack is to reject the claimed identity, the attack model uses a discriminator, <em>D</em>, that distinguishes between the probed image and the generated adversarial one by using the following GAN loss function: <table class="disp-formula p" id="Equ8"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/ca628025994a/d33e1877.gif" loading="lazy" id="d33e1877" alt="graphic file with name d33e1877.gif"></td>
<td class="label">8</td>
</tr></table> Consequently, the whole picture of the AdvFaces attack model is working based on the following objective loss function: <table class="disp-formula p" id="Equ9"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/9d94c33c9780/d33e1884.gif" loading="lazy" id="d33e1884" alt="graphic file with name d33e1884.gif"></td>
<td class="label">9</td>
</tr></table>
</div></li>
</ul>
<p><em>Generation of Perturbation</em> Figure <a href="#Fig12" class="usa-link">12</a> illustrates the synthesis of adversarial facial images using three distinct attack methods: FGSM, PGD, and AdvFace. Each row in the figure corresponds to adversarial images with their respective perturbation values denoted as  <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq32"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/cd8253eb5575/d33e1897.gif" loading="lazy" id="d33e1897" alt="Inline graphic"></span>. Both FGSM and PGD rely on the gradients of the loss function and apply perturbations to every pixel in the facial image, resulting in low-quality adversarial images. In contrast, AdvFaces<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup> autonomously learns to perturb specific regions of the face, such as the eyes, nose, and mouth, which are generally considered non-distracting or “silent.” Consequently, AdvFaces produces higher-quality adversarial images than those generated by the FGSM and PGD attack methods.</p>
<figure class="fig xbox font-sm" id="Fig12"><h4 class="obj_head">Fig. 12.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373911_41598_2025_15753_Fig12_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/aa49e783a8d6/41598_2025_15753_Fig12_HTML.jpg" loading="lazy" id="MO12" height="372" width="758" alt="Fig. 12"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig12/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>(Upper Row) Adversarial face images synthesized via three attacks: FGSM, PGD, and AdvFace. (Lower Row) Corresponding adversarial perturbations. FaceNet Euclidean distance scores between the adversarial and unaltered gallery images (not shown). A score above 1.1 indicates dissimilar subjects. (Image Credit:<sup><a href="#CR90" class="usa-link" aria-describedby="CR90">90</a></sup>).</p></figcaption></figure></section><section id="Sec20"><h3 class="pmc_sec_title">Adversarial attacks on large vision-language models (VLMs)</h3>
<p id="Par95">As face verification systems increasingly integrate vision-language models (VLMs), they have become significantly more powerful due to their ability to jointly interpret visual and textual cues. Yet, this integration has inadvertently broadened their vulnerability to sophisticated adversarial attacks. Unlike traditional face verification methods, VLM-based systems process multimodal information, which not only enriches their performance but also creates new attack surfaces that adversaries can exploit. Understanding these novel threats is essential for both researchers and practitioners to build more trustworthy verification systems.</p>
<p id="Par96">For instance, AnyAttack<sup><a href="#CR91" class="usa-link" aria-describedby="CR91">91</a></sup> revealed that adversarial images crafted through self-supervised methods can deceive VLMs even without predefined labels or textual prompts. This approach, which leverages large-scale multimodal datasets, has demonstrated remarkable effectiveness and transferability across numerous commercial models. Such findings demonstrate that attackers no longer need intimate knowledge of the target systems; instead, they can exploit common embedding structures to launch potent impersonation attacks. This exposes an urgent need for defenses focusing not only on detecting adversarial patterns but also on fortifying the multimodal embeddings themselves. In addition to digital attacks, the physical world also presents surprising avenues for adversaries. The ProjAttacker method<sup><a href="#CR92" class="usa-link" aria-describedby="CR92">92</a></sup> exemplifies how subtle, dynamic projections of adversarial patterns onto faces can deceive verification systems, circumventing traditional defenses such as liveness detection. This attack notably shifts the adversarial paradigm from physically intrusive methods (such as masks or makeup) to subtle, environmental manipulations. Thus, attackers have become less conspicuous, prompting the need for systems that recognize subtle environmental tampering rather than overt spoofing methods.</p>
<p id="Par97">Further complicating matters, adversaries have begun to exploit generative AI in novel ways. Adv-CPG<sup><a href="#CR93" class="usa-link" aria-describedby="CR93">93</a></sup>, for instance, incorporates adversarial perturbations directly into generative portrait models. By embedding identity-masking mechanisms at the point of image generation, adversaries can effectively block unauthorized facial recognition, thus undermining the reliability of verification systems. This proactive integration of adversarial intent into generative processes marks a significant shift, highlighting how the boundary between data creation and attack execution is becoming increasingly blurred.</p>
<p id="Par98">Moreover, new strategies like the DPA Attack<sup><a href="#CR82" class="usa-link" aria-describedby="CR82">82</a></sup> illustrate the ingenuity of adversaries who now diversify surrogate models, rather than inputs alone, to enhance attack transferability across unknown systems. Simultaneously, methods such as Cross-Modal Adversarial Patches<sup><a href="#CR94" class="usa-link" aria-describedby="CR94">94</a></sup> exploit the very interactions between image and text, strategically placing patches that disrupt multimodal associations while remaining visually plausible. Such developments underscore that adversaries are effectively leveraging the strengths of multimodal models against them, emphasizing that security solutions must now anticipate threats at the intersection of different modalities rather than within individual modalities alone.</p>
<p id="Par99">Recent studies have extended adversarial research to transformer-based and multimodal face-verification systems. Kong et al.<sup><a href="#CR95" class="usa-link" aria-describedby="CR95">95</a></sup> demonstrates that multimodal architectures, which combine visual and auxiliary sensor inputs, can still be vulnerable to adversarial perturbations targeting individual modalities, exposing weaknesses in cross-modal fusion. Cai et al.<sup><a href="#CR96" class="usa-link" aria-describedby="CR96">96</a></sup> further show that Vision Transformers, a core component in many large vision-language models, can be adapted for face anti-spoofing but remain susceptible to adversarial triggers due to the high-dimensional token interactions. Complementing these findings, Digital and Physical Face Attacks<sup><a href="#CR97" class="usa-link" aria-describedby="CR97">97</a></sup> provides a systematic review of recent digital and real-world attack strategies, highlighting the emerging risks posed by advanced transformer- and multimodal-based face-verification pipelines.</p>
<p id="Par100">Collectively, these insights highlight a critical shift in the landscape of adversarial attacks targeting VLM-based face verification systems. As attacks become increasingly subtle, sophisticated, and multimodal, defenders face the growing challenge of maintaining user trust and system reliability.</p></section></section><section id="Sec21"><h2 class="pmc_sec_title">Defense</h2>
<p id="Par101">Face verification systems have achieved widespread adoption, ubiquitously integrated into our smartphones. These systems facilitate various functions, including device unlocking, financial transactions, and access to premium content stored on the device. In this context, the robustness of face verification systems has emerged as a critical consideration to ensure their reliability. The failure to detect adversarial smartphone attacks poses a significant risk to confidential information, encompassing emails, bank records, social media content, and personal photos. Consequently, the presence of such adversarial examples has spurred research efforts among academic and industry groups and social media platforms to develop generalizable defenses against ever-evolving adversarial attacks. As a result, the urgency of countering face attacks has intensified, driven by mounting concerns regarding user privacy. The failure to detect such attacks poses a substantial security threat, particularly given the widespread use of face verification systems in contexts such as border control. Despite the remarkable performance exhibited by face verification systems, attributed to advances in deep learning and the availability of extensive datasets, these systems remain susceptible to the increasing menace of adversarial attacks, as indicated by various studies<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a>,<a href="#CR17" class="usa-link" aria-describedby="CR17">17</a>,<a href="#CR21" class="usa-link" aria-describedby="CR21">21</a>,<a href="#CR23" class="usa-link" aria-describedby="CR23">23</a>,<a href="#CR98" class="usa-link" aria-describedby="CR98">98</a></sup>. Attackers invest significant time and effort into manipulating faces through physical<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a>,<a href="#CR98" class="usa-link" aria-describedby="CR98">98</a></sup> and digital<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup> means, with the objective of evading face verification systems. It has been demonstrated that these systems are vulnerable to adversarial attacks stemming from perturbations introduced to the images under scrutiny<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a>,<a href="#CR21" class="usa-link" aria-describedby="CR21">21</a>,<a href="#CR85" class="usa-link" aria-describedby="CR85">85</a></sup>. Notably, even when such perturbations are imperceptible to the human eye, they can undermine the performance of face verification. In the literature focusing on defense strategies against adversarial examples, these strategies primarily fall within two main categories: robust optimization and pre-processing techniques, as illustrated in Fig. <a href="#Fig3" class="usa-link">3</a>. Robust optimization, a widely employed defense approach, involves altering the training procedures or architectures of neural networks to enhance their resistance to adversarial perturbations<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a>,<a href="#CR23" class="usa-link" aria-describedby="CR23">23</a>–<a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>. While these algorithms offer some protection against specific attack methods, they remain susceptible to other adversarial mechanisms. It is essential to note that adversarial training, a component of robust optimization, requires more time and computational resources than training models solely on clean images, as it necessitates additional computations for generating online adversarial examples . Conversely, pre-processing strategies maintain the core training procedures and network architectures unchanged and instead focus on identifying, eliminating, or purifying adversarial elements. For instance, in the context of detecting adversarial examples, this involves training a binary classifier to distinguish between genuine and adversarial instances<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a>–<a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>. In the case of removing adversarial noise<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a>,<a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>, the objective is to eliminate adversarial perturbations by applying preprocessing transformations to input data before feeding it to target models. Conversely, in the context of purification, the perturbations are exclusively removed from input images containing adversarial elements<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>, preventing the inadvertent alteration of genuine images and the associated high false rejection rates. Securing resilient face verification systems against adversarial examples represents a complex and ongoing challenge. A multitude of adversarial defense mechanisms have been employed to safeguard FV systems from such threats. The current body of academic literature concerning defense strategies can be categorized into two primary domains: robust optimization and preprocessing. This survey offers an overview of prior research pertinent to our investigations in adversarial defenses, with a particular emphasis on preprocessing techniques. Specifically, our research centers on perturbation removal and detection strategies. Table <a href="#Tab4" class="usa-link">4</a> lists common benchmark methods for each defense strategy in the literature.</p>
<section class="tw xbox font-sm" id="Tab4"><h3 class="obj_head">Table 4.</h3>
<div class="caption p"><p>Common methods for each defense strategy in the literature.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Defense Strategy</th>
<th align="left" colspan="1" rowspan="1">Authors</th>
<th align="left" colspan="1" rowspan="1">Method</th>
<th align="left" colspan="1" rowspan="1">Datasets</th>
<th align="left" colspan="1" rowspan="1">Attacks</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="2" colspan="1">Robustness</td>
<td align="left" colspan="1" rowspan="1">Kurakin et al.<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Adversarial training with FGSM</td>
<td align="left" colspan="1" rowspan="1">ImageNet<sup><a href="#CR101" class="usa-link" aria-describedby="CR101">101</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Jang et al.<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Training with adversarial examples</td>
<td align="left" colspan="1" rowspan="1">MNIST<sup><a href="#CR105" class="usa-link" aria-describedby="CR105">105</a></sup>, CIFAR-10<sup><a href="#CR106" class="usa-link" aria-describedby="CR106">106</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, C&amp;W<sup><a href="#CR80" class="usa-link" aria-describedby="CR80">80</a></sup>, PGD<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>
</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">Transformations</td>
<td align="left" colspan="1" rowspan="1">Guo et al.<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Quilting, TVM, cropping, rescaling</td>
<td align="left" colspan="1" rowspan="1">ImageNet<sup><a href="#CR101" class="usa-link" aria-describedby="CR101">101</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">DeepFool<sup><a href="#CR78" class="usa-link" aria-describedby="CR78">78</a></sup>, FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, C&amp;W<sup><a href="#CR80" class="usa-link" aria-describedby="CR80">80</a></sup>, I-FGSM<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Shaham et al.<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">PCA, wavelet, JPEG compression</td>
<td align="left" colspan="1" rowspan="1">NIPS 2017 competition</td>
<td align="left" colspan="1" rowspan="1">FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, C&amp;W<sup><a href="#CR80" class="usa-link" aria-describedby="CR80">80</a></sup>, I-FGSM<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>
</td>
</tr>
<tr>
<td align="left" rowspan="6" colspan="1">Detection</td>
<td align="left" colspan="1" rowspan="1">Gong et al.<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Binary CNN</td>
<td align="left" colspan="1" rowspan="1">MNIST<sup><a href="#CR105" class="usa-link" aria-describedby="CR105">105</a></sup>, CIFAR-10<sup><a href="#CR106" class="usa-link" aria-describedby="CR106">106</a></sup>, SVHN</td>
<td align="left" colspan="1" rowspan="1">FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, TGSM<sup><a href="#CR107" class="usa-link" aria-describedby="CR107">107</a></sup>, JSMA<sup><a href="#CR79" class="usa-link" aria-describedby="CR79">79</a></sup>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Massoli et al.<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">MLP/LSTM on AFR filters</td>
<td align="left" colspan="1" rowspan="1">VGGFace2<sup><a href="#CR62" class="usa-link" aria-describedby="CR62">62</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, C&amp;W<sup><a href="#CR80" class="usa-link" aria-describedby="CR80">80</a></sup>, BIM<sup><a href="#CR107" class="usa-link" aria-describedby="CR107">107</a></sup>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Goel et al.<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Adaptive noise detection</td>
<td align="left" colspan="1" rowspan="1">Yale Face<sup><a href="#CR108" class="usa-link" aria-describedby="CR108">108</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">DeepFool<sup><a href="#CR78" class="usa-link" aria-describedby="CR78">78</a></sup>, FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, EAD<sup><a href="#CR109" class="usa-link" aria-describedby="CR109">109</a></sup>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Goswami et al.<sup><a href="#CR110" class="usa-link" aria-describedby="CR110">110</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">SVM on AFR filters</td>
<td align="left" colspan="1" rowspan="1">MEDS<sup><a href="#CR111" class="usa-link" aria-describedby="CR111">111</a></sup>, PaSC<sup><a href="#CR112" class="usa-link" aria-describedby="CR112">112</a></sup>, MBGC<sup><a href="#CR113" class="usa-link" aria-describedby="CR113">113</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">EAD<sup><a href="#CR109" class="usa-link" aria-describedby="CR109">109</a></sup>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Agarwal et al.<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">PCA + SVM</td>
<td align="left" colspan="1" rowspan="1">PaSC<sup><a href="#CR112" class="usa-link" aria-describedby="CR112">112</a></sup>, MEDS<sup><a href="#CR111" class="usa-link" aria-describedby="CR111">111</a></sup>, Multi-PIE<sup><a href="#CR114" class="usa-link" aria-describedby="CR114">114</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Universal Perturbation<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>, Fast Feature Fool<sup><a href="#CR115" class="usa-link" aria-describedby="CR115">115</a></sup>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Awany et al.<sup><a href="#CR116" class="usa-link" aria-describedby="CR116">116</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Detection framework</td>
<td align="left" colspan="1" rowspan="1">CASIA-WebFace<sup><a href="#CR59" class="usa-link" aria-describedby="CR59">59</a></sup>, LFW<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, PGD<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>, AdvFaces<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Purification</td>
<td align="left" colspan="1" rowspan="1">Debayan et al.<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Generator + detector + purifier</td>
<td align="left" colspan="1" rowspan="1">CASIA-WebFace<sup><a href="#CR59" class="usa-link" aria-describedby="CR59">59</a></sup>, LFW<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>, CelebA<sup><a href="#CR117" class="usa-link" aria-describedby="CR117">117</a></sup>, FFHQ<sup><a href="#CR118" class="usa-link" aria-describedby="CR118">118</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">FGSM<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, PGD<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>, DeepFool<sup><a href="#CR78" class="usa-link" aria-describedby="CR78">78</a></sup>, AdvFaces<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>, GFLM<sup><a href="#CR85" class="usa-link" aria-describedby="CR85">85</a></sup>, Semantic <sup><a href="#CR119" class="usa-link" aria-describedby="CR119">119</a></sup>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section id="Sec22"><h3 class="pmc_sec_title">Perturbation removing</h3>
<p id="Par102">This type of defense aims to remove adversarial perturbations by applying transformations, such as preprocessing the input data and then sending these inputs to the target models. As Guo et al.<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup> applied image transformations, such as total variance minimization<sup><a href="#CR99" class="usa-link" aria-describedby="CR99">99</a></sup>, image quilting<sup><a href="#CR100" class="usa-link" aria-describedby="CR100">100</a></sup>, image cropping and rescaling, and bit-depth reduction to smooth input images. Applied on ImageNet<sup><a href="#CR101" class="usa-link" aria-describedby="CR101">101</a></sup> dataset and shown that these defenses can be surprisingly effective against existing three attacks, they are as follows, (i) countering the (iterative) fast gradient sign method<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>, (ii) Deepfool<sup><a href="#CR78" class="usa-link" aria-describedby="CR78">78</a></sup>, and (iii) Carlini-Wagner attack<sup><a href="#CR80" class="usa-link" aria-describedby="CR80">80</a></sup> in particular, when the convolutional network is trained on the images on which these transformations are performed. Dziugaite et al.<sup><a href="#CR102" class="usa-link" aria-describedby="CR102">102</a></sup> and Das et al.<sup><a href="#CR103" class="usa-link" aria-describedby="CR103">103</a></sup> suggested applying JPEG compression to insert images before feeding them over the network. Hendrycks et al.<sup><a href="#CR72" class="usa-link" aria-describedby="CR72">72</a></sup>; and Li et al.<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>, proposed defense methods based on principal component analysis (PCA). Liu et al. proposed the DNN-favorable JPEG compression, namely “feature distillation,” by redesigning the standard JPEG compression algorithm to maximize the defense efficiency while assuring the DNN testing accuracy<sup><a href="#CR104" class="usa-link" aria-describedby="CR104">104</a></sup>. As a result of the good performance of these methods on an ImageNet<sup><a href="#CR101" class="usa-link" aria-describedby="CR101">101</a></sup> dataset in<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>.</p></section><section id="Sec23"><h3 class="pmc_sec_title">Perturbation detection</h3>
<p id="Par103">Another strategic direction that defends against adversarial attacks on the FV system is detecting adversarial examples. Adversarial detection techniques have recently gained attention within the scientific community, and many adversarial detection mechanisms are deployed as a preprocessing step. The attacks addressed in this study<sup><a href="#CR120" class="usa-link" aria-describedby="CR120">120</a>–<a href="#CR122" class="usa-link" aria-describedby="CR122">122</a></sup> were initially suggested in object recognition and often fail to detect attacks in a feature extraction network setting, such as in face verification. Therefore, prevalent detectors against hostile faces have only been effective in a highly restricted environment where the number of people is limited and constant during training and testing<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a>,<a href="#CR28" class="usa-link" aria-describedby="CR28">28</a>,<a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>. Defending against adversarial attacks by detection involves creating robust systems that consist of a weak model and a detection system that indicates the occurrence of attacks. Detecting subsystems are often implemented as binary detectors that discriminate between authentic and adversarial inputs. For instance, Gong et al.<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup> proposed to train an additional binary classifier that decides whether the input image is pure or adversarial. Grosse et al.<sup><a href="#CR123" class="usa-link" aria-describedby="CR123">123</a></sup> adopted statistical tests in pixel space to prove that adversarial images could be distinguished and suggested introducing the “adversarial” category in the original category trained according to the model. Similarly, Metzen et al.<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> proposed a detection subnetwork based on intermediate representations generated by the model at the time of inference. However, it has been shown that many detection schemes can be bypassed<sup><a href="#CR120" class="usa-link" aria-describedby="CR120">120</a></sup>. Guo et al.<sup><a href="#CR124" class="usa-link" aria-describedby="CR124">124</a></sup> propose CCA-UD, a universal backdoor-detection framework that first partitions the training data with density-based clustering and then applies a centroid-shift test. Representative features of each cluster are superimposed on benign images; a cluster is deemed poisoned when these composites consistently induce misclassification. Because the method exploits a trigger-agnostic property—general misclassification under feature overlay—it remains effective across clean-label and dirty-label attacks and with global, local, sample-specific, and source-specific triggers, achieving superior detection rates compared with prior defences.</p></section><section id="Sec24"><h3 class="pmc_sec_title">Adversarial training for CNN-based face verification</h3>
<p id="Par104">Conventional CNN-based face verification models, including ArcFace<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup> and CosFace<sup><a href="#CR125" class="usa-link" aria-describedby="CR125">125</a></sup>, have seen notable improvements in robustness via adversarial fine-tuning using techniques such as Projected Gradient Descent (PGD) and Fast Gradient Sign Method (FGSM)<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a>,<a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup> . However, integrating adversarial perturbations directly into training has often induced a noticeable trade-off, enhancing robustness but simultaneously diminishing accuracy on clean images due to distortions in the model’s embedding space. To mitigate these negative side effects, regularization strategies like TRADES<sup><a href="#CR126" class="usa-link" aria-describedby="CR126">126</a></sup> have been explored to strike a balance, aiming to preserve discriminative capabilities critical for facial recognition tasks. An alternative approach involves auxiliary defense mechanisms, exemplified by FaceGuard<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>, which deploys a self-supervised purification and detection module without modifying the verification model itself. Such modular defenses have demonstrated high efficacy, achieving a high detection accuracy of adversarial perturbations and maintaining original accuracy for legitimate users. Nonetheless, while auxiliary modules show promise in isolating adversarial perturbations, their effectiveness relies heavily on accurately modeling diverse attack patterns, highlighting an ongoing challenge of overfitting to specific adversarial strategies and underscoring the need for continually adaptive defenses. Recent extensions, such as Adversarial Weight Perturbation and Feature-Denoising FaceNet, further improve robustness while reducing the drop in clean accuracy, indicating a move toward parameter-efficient and embedding-aware training strategies.</p></section><section id="Sec25"><h3 class="pmc_sec_title">Adversarial training in large vision-language models (VLMs)</h3>
<p id="Par105">Adversarial training in Vision–Language Models (VLMs) presents unique challenges due to their joint multimodal embedding spaces<sup><a href="#CR127" class="usa-link" aria-describedby="CR127">127</a></sup>. Early supervised adversarial fine-tuning of CLIP-like models improved robustness but often degraded zero-shot generalization when trained on fixed-label datasets<sup><a href="#CR128" class="usa-link" aria-describedby="CR128">128</a></sup>.</p>
<p id="Par106">Recent approaches focus on maintaining robustness while preserving generalization. Robust CLIP<sup><a href="#CR129" class="usa-link" aria-describedby="CR129">129</a></sup> adversarially fine-tunes the vision encoder while freezing the text encoder, leading to significant robustness improvements without compromising zero-shot capabilities. Anchor-RFT<sup><a href="#CR130" class="usa-link" aria-describedby="CR130">130</a></sup> constrains fine-tuning to anchor points in the joint vision–text space, which helps preserve out-of-distribution accuracy while enhancing robustness against PGD-style and Auto-Attack perturbations. Hyper-AT<sup><a href="#CR131" class="usa-link" aria-describedby="CR131">131</a></sup> employs a hyper-network to generate adversarial perturbations dynamically during training, reducing computational overhead while improving robust accuracy.</p>
<p id="Par107">Beyond adversarial training, multimodal approaches have emerged as promising solutions to enhance face-verification robustness. Kong et al.<sup><a href="#CR97" class="usa-link" aria-describedby="CR97">97</a></sup> proposed Echo-FAS, an acoustic-based face anti-spoofing system that relies only on a speaker and microphone for liveness detection. Although Echo-FAS does not directly employ adversarial training, it illustrates how incorporating auxiliary modalities can mitigate spoofing attacks and can complement adversarially trained multimodal architectures such as M3FA<sup><a href="#CR95" class="usa-link" aria-describedby="CR95">95</a></sup>. Similarly, methods that detect manipulated facial regions using combined semantic and noise-level features, such as the framework by Kong et al.<sup><a href="#CR132" class="usa-link" aria-describedby="CR132">132</a></sup>, provide complementary defences that can be integrated with transformer-based models to improve resilience to face forgeries.</p>
<p id="Par108">Parameter-efficient strategies, including adversarial prompt tuning<sup><a href="#CR133" class="usa-link" aria-describedby="CR133">133</a></sup> and low-rank adaptation<sup><a href="#CR131" class="usa-link" aria-describedby="CR131">131</a></sup>, further support robust fine-tuning by maintaining clean accuracy and zero-shot performance while requiring minimal parameter updates. Transformer-based models such as S-adapter<sup><a href="#CR96" class="usa-link" aria-describedby="CR96">96</a></sup> and multimodal fusion systems like M3FAS demonstrate the benefits of combining adversarial training with auxiliary defences and multiple modalities. Collectively, these developments reflect a clear trend toward modular, scalable, and attack-agnostic approaches that enhance the robustness of VLM-based face-verification systems while mitigating the trade-off with generalization ability.</p></section></section><section id="Sec26"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par109">In conclusion, this survey has examined recent developments in face verification systems. While advancements in recent years have enhanced accuracy and efficiency, particularly through the integration of deep learning, the path forward requires a keen awareness of the human element. Challenges surrounding privacy, ethical considerations, and the persistent issue of bias in datasets and algorithms demand our continued attention.</p>
<p id="Par110">Future research must strive for greater technical sophistication in handling variations like pose, illumination, and aging, and prioritize the development of systems that are fair, transparent, and respectful of individual rights. By focusing on creating more diverse and representative datasets, establishing robust ethical guidelines, and ensuring accountability in deployment, we can harness the power of face verification technology for societal good while safeguarding fundamental human values.</p></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>We would like to thank our colleagues for their feedback on the earlier version of this survey.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>All authors contributed to the study conception and design. Sohair Kilany: Conceptualization, methodology, data collection, investigation, visualization, writing original draft, review and editing. Ahmed Mahfouz: Conceptualization, methodology, investigation, validation, supervision, writing review an editing.</p></section><section id="notes2"><h2 class="pmc_sec_title">Funding</h2>
<p>Open access funding provided by The Science, Technology &amp; Innovation Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank (EKB).</p></section><section id="notes3"><h2 class="pmc_sec_title">Data availability</h2>
<p>The datasets used and/or analysed during the current study are available from the corresponding author on reasonable request.</p></section><section id="notes4"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par116">The authors declare no competing interests.</p></section><section id="FPar2"><h3 class="pmc_sec_title">Consent for publication</h3>
<p id="Par115">The article was submitted with the consent of all authors and institutions for publication.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="_ci93_" lang="en" class="contrib-info"><h2 class="pmc_sec_title">Contributor Information</h2>
<p>Sohair Kilany, Email: sohair_kilany@mu.edu.eg.</p>
<p>Ahmed Mahfouz, Email: ahmed.m@aou.edu.om.</p></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Fathy, M. E., Patel, V. M., &amp; Chellappa, R. Face-based active authentication on mobile devices. In <em>Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference On</em>, pp. 1687–1691 (2015). 10.1109/ICASSP.2015.7178258</cite>
</li>
<li id="CR2">
<span class="label">2.</span><cite>Crouse, D., Han, H., Chandra, D., Barbello, B., &amp; Jain, A. K. Continuous authentication of mobile user: Fusion of face image and inertial measurement unit data. In <em>2015 International Conference on Biometrics (ICB)</em>, pp. 135–142 (2015). 10.1109/ICB.2015.7139043</cite>
</li>
<li id="CR3">
<span class="label">3.</span><cite>Samangouei, P., &amp; Chellappa, R. Convolutional neural networks for attribute-based active authentication on mobile devices. In <em>2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems (BTAS)</em>, pp. 1–8 (2016). 10.1109/BTAS.2016.7791163</cite>
</li>
<li id="CR4">
<span class="label">4.</span><cite>Acien, A., Morales, A., Vera-Rodriguez, R., Fierrez, J., &amp; Tolosana, R. Multilock: Mobile active authentication based on multiple biometric and behavioral patterns. In <em>ACM Intl. Conf. on Multimedia, Workshop on Multimodal Understanding and Learning for Embodied Applications (MULEA)</em>, pp. 53–59 (2019). 10.1145/3347450.3357663</cite>
</li>
<li id="CR5">
<span class="label">5.</span><cite>Huang, G., Liu, Z., Van Der Maaten, L., &amp; Weinberger, K. Q. Densely connected convolutional networks. In <em>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 2261–2269 (2017). 10.1109/CVPR.2017.243</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>He, K., Zhang, X., Ren, S., &amp; Sun, J. Deep residual learning for image recognition. In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 770–778. IEEE, (2016). 10.1109/CVPR.2016.90</cite>
</li>
<li id="CR7">
<span class="label">7.</span><cite>Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., &amp; Song, L. Sphereface: Deep hypersphere embedding for face recognition. CoRR <a href="http://arxiv.org/abs/1704.08063" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1704.08063</a> (2017).</cite>
</li>
<li id="CR8">
<span class="label">8.</span><cite>Deng, J., Guo, J., Xue, N., &amp; Zafeiriou, S. Arcface: Additive angular margin loss for deep face recognition, 4690–4699 (2019).</cite> [<a href="https://doi.org/10.1109/TPAMI.2021.3087709" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34106845/" class="usa-link">PubMed</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>Schroff, F., Kalenichenko, D., &amp; Philbin, J. Facenet: A unified embedding for face recognition and clustering. In <em>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 815–823 (2015). 10.1109/CVPR.2015.7298682</cite>
</li>
<li id="CR10">
<span class="label">10.</span><cite>Taigman, Y., Yang, M., Ranzato, M., &amp; Wolf, L. Deepface: Closing the gap to human-level performance in face verification. In <em>2014 IEEE Conference on Computer Vision and Pattern Recognition</em>, pp. 1701–1708 (2014). 10.1109/CVPR.2014.220</cite>
</li>
<li id="CR11">
<span class="label">11.</span><cite>Huang, G. B., Mattar, M., Berg, T., &amp; Learned-Miller, E. Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. In <em>Workshop on Faces in’Real-Life’Images: Detection, Alignment, and Recognition</em> (2008)</cite>
</li>
<li id="CR12">
<span class="label">12.</span><cite>Wolf, L., Hassner, T., &amp; Maoz, I. Face recognition in unconstrained videos with matched background similarity, pp. 529–534 (2011). 10.1109/CVPR.2011.5995566</cite>
</li>
<li id="CR13">
<span class="label">13.</span><cite>Liu, Y., Stehouwer, J., Jourabloo, A., &amp; Liu, X. Deep tree learning for zero-shot face anti-spoofing. In <em>In Proceeding of IEEE Computer Vision and Pattern Recognition</em>, pp. 4680–4689. IEEE, (2019). 10.1109/CVPR.2019.00481</cite>
</li>
<li id="CR14">
<span class="label">14.</span><cite>Dang, H., Liu, F., Stehouwer, J., Liu, X., &amp; Jain, A. On the detection of digital face manipulation. In <em>In Proceeding of IEEE Computer Vision and Pattern Recognition</em>, Seattle, WA (2020)</cite>
</li>
<li id="CR15">
<span class="label">15.</span><cite>Dong, Y., Liao, F., Pang, T., Hu, X., &amp; Zhu, J. Discovering adversarial examples with momentum. CoRR <a href="http://arxiv.org/abs/1710.06081" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1710.06081</a> (2017).</cite>
</li>
<li id="CR16">
<span class="label">16.</span><cite>Moosavi-Dezfooli, S., Fawzi, A., Fawzi, O., &amp; Frossard, P. Universal adversarial perturbations. CoRR <a href="http://arxiv.org/abs/1610.08401" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1610.08401</a> (2016).</cite>
</li>
<li id="CR17">
<span class="label">17.</span><cite>Goodfellow, I. J., Shlens, J., &amp; Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint <a href="http://arxiv.org/abs/1412.6572" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1412.6572</a> (2014)</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>Carlini, N. A complete list of all (arxiv) adversarial example papers. (2019). <a href="https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html</a></cite>
</li>
<li id="CR19">
<span class="label">19.</span><cite>Sharif, M., Bhagavatula, S., Bauer, L., &amp; Reiter, M. K. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>, pp. 1528–1540. Association for Computing Machinery, New York, NY, USA (2016). 10.1145/2976749.2978392 .</cite>
</li>
<li id="CR20">
<span class="label">20.</span><cite>Dong, Y., Su, H., Wu, B., Li, Z., Liu, W., Zhang, T., &amp; Zhu, J. Efficient decision-based black-box adversarial attacks on face recognition. In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 7706–7714 (2019). 10.1109/CVPR.2019.00790 . IEEE</cite>
</li>
<li id="CR21">
<span class="label">21.</span><cite>Deb, D., Zhang, J., &amp; Jain, A. K. Advfaces: Adversarial face synthesis. CoRR <a href="http://arxiv.org/abs/1908.05008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1908.05008</a> (2019)</cite>
</li>
<li id="CR22">
<span class="label">22.</span><cite>Fastpass- a harmonized, modular reference system for all european automated bordercrossing points. <a href="https://www.fastpass-project.eu" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.fastpass-project.eu</a></cite>
</li>
<li id="CR23">
<span class="label">23.</span><cite>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp; Vladu, A. Towards deep learning models resistant to adversarial attacks. In <em>6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings</em>. OpenReview.net, (2018). <a href="https://openreview.net/forum?id=rJzIBfZAb" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=rJzIBfZAb</a></cite>
</li>
<li id="CR24">
<span class="label">24.</span><cite>Tramèr, F. et al. Ensemble Adversarial Training: Attacks and Defenses (2020).</cite>
</li>
<li id="CR25">
<span class="label">25.</span><cite>Kurakin, A., Goodfellow, I., &amp; Bengio, S. Adversarial Machine Learning at Scale (2017)</cite>
</li>
<li id="CR26">
<span class="label">26.</span><cite>Jang, Y., Zhao, T., Hong, S., &amp; Lee, H. Adversarial defense via learning to generate diverse attacks. In <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp. 2740–2749 (2019). 10.1109/ICCV.2019.00283</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Massoli, F. V., Carrara, F., Amato, G. &amp; Falchi, F. Detection of face recognition adversarial attacks. <em>Computer Vision and Image Understanding</em><strong>202</strong>, 103103 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Massoli,%20F.%20V.,%20Carrara,%20F.,%20Amato,%20G.%20&amp;%20Falchi,%20F.%20Detection%20of%20face%20recognition%20adversarial%20attacks.%20Computer%20Vision%20and%20Image%20Understanding202,%20103103%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Goel, A., Singh, A., Agarwal, A., Vatsa, M., &amp; Singh, R. Smartbox: Benchmarking adversarial detection and mitigation algorithms for face recognition. In <em>2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)</em>, pp. 1–7 (2018). 10.1109/BTAS.2018.8698567</cite>
</li>
<li id="CR29">
<span class="label">29.</span><cite>Gong, Z., Wang, W., &amp; Ku, W.-S. Adversarial and clean data are not twins. arXiv preprint <a href="http://arxiv.org/abs/1704.04960" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1704.04960</a> (2017).</cite>
</li>
<li id="CR30">
<span class="label">30.</span><cite>Agarwal, A., Singh, R., Vatsa, M., &amp; Ratha, N. Are image-agnostic universal adversarial perturbations for face recognition difficult to detect? In <em>2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)</em>, pp. 1–7 (2018). 10.1109/BTAS.2018.8698548</cite>
</li>
<li id="CR31">
<span class="label">31.</span><cite>Li, X., &amp; Li, F. Adversarial examples detection in deep networks with convolutional filter statistics. In <em>2017 IEEE International Conference on Computer Vision (ICCV)</em>, pp. 5775–5783 (2017). 10.1109/ICCV.2017.615</cite>
</li>
<li id="CR32">
<span class="label">32.</span><cite>Metzen, J. H., Genewein, T., Fischer, V., &amp; Bischoff, B. On Detecting Adversarial Perturbations (2017).</cite>
</li>
<li id="CR33">
<span class="label">33.</span><cite>Zantedeschi, V., Nicolae, M.-I., &amp; Rawat, A. Efficient Defenses Against Adversarial Attacks, pp. 39–49. Association for Computing Machinery, (2017). 10.1145/3128572.3140449</cite>
</li>
<li id="CR34">
<span class="label">34.</span><cite>Shaham, U. et al. Defending against Adversarial Images using Basis Functions Transformations (2018). <a href="https://api.semanticscholar.org/CorpusID:4549456" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://api.semanticscholar.org/CorpusID:4549456</a></cite>
</li>
<li id="CR35">
<span class="label">35.</span><cite>Guo, C., Rana, M., Cisse, M., &amp; Maaten, L. Countering adversarial images using input transformations. In <em>International Conference on Learning Representations</em> (2018). <a href="https://openreview.net/forum?id=SyJ7ClWCb" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=SyJ7ClWCb</a></cite>
</li>
<li id="CR36">
<span class="label">36.</span><cite>Deb, D., Liu, X., &amp; Jain, A. K. FaceGuard: A Self-Supervised Defense Against Adversarial Face Images. <em>IEEE Computer Society, Los Alamitos,</em> CA, USA (2023). 10.1109/FG57933.2023.10042617 .</cite>
</li>
<li id="CR37">
<span class="label">37.</span><cite>Evtimov, I. et al. Robust physical-world attacks on machine learning models. CoRR <a href="http://arxiv.org/abs/1707.08945" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1707.08945</a> (2017).</cite>
</li>
<li id="CR38">
<span class="label">38.</span><cite>Zügner, D., Akbarnejad, A., &amp; Günnemann, S. Adversarial attacks on neural networks for graph data. In <em>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, pp. 2847–2856 (2018).</cite>
</li>
<li id="CR39">
<span class="label">39.</span><cite>Viola, P., &amp; Jones, M. Rapid object detection using a boosted cascade of simple features. In <em>Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001</em>, vol. 1, p. (2001). 10.1109/CVPR.2001.990517</cite>
</li>
<li id="CR40">
<span class="label">40.</span><cite>Zhang, K., Zhang, Z., Li, Z. &amp; Qiao, Y. Joint face detection and alignment using multitask cascaded convolutional networks. <em>IEEE Signal Process. Lett.</em><strong>23</strong>, 1499–1503 (2016).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20K.,%20Zhang,%20Z.,%20Li,%20Z.%20&amp;%20Qiao,%20Y.%20Joint%20face%20detection%20and%20alignment%20using%20multitask%20cascaded%20convolutional%20networks.%20IEEE%20Signal%20Process.%20Lett.23,%201499%E2%80%931503%20(2016)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR41">
<span class="label">41.</span><cite>Deng, J., Guo, J., Ververas, E., Kotsia, I., &amp; Zafeiriou, S. Retinaface: Single-shot multi-level face localisation in the wild. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 5202–5211 (2020). 10.1109/CVPR42600.2020.00525</cite>
</li>
<li id="CR42">
<span class="label">42.</span><cite>Wang, D., Otto, C. &amp; Jain, A. K. Face search at scale. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em><strong>39</strong>(6), 1122–1136. 10.1109/TPAMI.2016.2582166 (2017).
</cite> [<a href="https://doi.org/10.1109/TPAMI.2016.2582166" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27333599/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wang,%20D.,%20Otto,%20C.%20&amp;%20Jain,%20A.%20K.%20Face%20search%20at%20scale.%20IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence39(6),%201122%E2%80%931136.%2010.1109/TPAMI.2016.2582166%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR43">
<span class="label">43.</span><cite>Ghazi, M. M., &amp; Ekenel, H. K. A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition (2016).</cite>
</li>
<li id="CR44">
<span class="label">44.</span><cite>Dou, P., Shah, S. K., &amp; Kakadiaris, I. A. End-to-end 3D face reconstruction with deep neural networks (2017).</cite>
</li>
<li id="CR45">
<span class="label">45.</span><cite>Zhao, J. et al. Dual-agent gans for photorealistic and identity preserving profile face synthesis. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc., (2017). <a href="https://proceedings.neurips.cc/paper/2017/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://proceedings.neurips.cc/paper/2017/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf</a></cite>
</li>
<li id="CR46">
<span class="label">46.</span><cite>Sun, Y., Wang, X., &amp; Tang, X. Sparsifying Neural Network Connections for Face Recognition (2016).</cite>
</li>
<li id="CR47">
<span class="label">47.</span><cite>Yang, J., Reed, S., Yang, M.-H. &amp; Lee, H. <em>Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis</em> (MIT Press, 2015).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yang,%20J.,%20Reed,%20S.,%20Yang,%20M.-H.%20&amp;%20Lee,%20H.%20Weakly-supervised%20Disentangling%20with%20Recurrent%20Transformations%20for%203D%20View%20Synthesis%20(MIT%20Press,%202015)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR48">
<span class="label">48.</span><cite>Zhou, E., Cao, Z., &amp; Sun, J. GridFace: Face Rectification via Learning Local Homography Transformations (2018). <a href="http://arxiv.org/abs/1808.06210" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1808.06210</a></cite>
</li>
<li id="CR49">
<span class="label">49.</span><cite>Deng, J., Cheng, S., Xue, N., Zhou, Y., &amp; Zafeiriou, S. UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition (2018).</cite>
</li>
<li id="CR50">
<span class="label">50.</span><cite>Belhumeur, P. N., Hespanha, J. P. &amp; Kriegman, D. J. Eigenfaces vs. fisherfaces: Recognition using class specific linear projection. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em><strong>19</strong>(7), 711–720 (1997).</cite> [<a href="https://scholar.google.com/scholar_lookup?Belhumeur,%20P.%20N.,%20Hespanha,%20J.%20P.%20&amp;%20Kriegman,%20D.%20J.%20Eigenfaces%20vs.%20fisherfaces:%20Recognition%20using%20class%20specific%20linear%20projection.%20IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.19(7),%20711%E2%80%93720%20(1997)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR51">
<span class="label">51.</span><cite>Deng, W., Hu, J., Lu, J. &amp; Guo, J. Transform-invariant pca: A unified approach to fully automatic facealignment, representation, and recognition. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em><strong>36</strong>(06), 1275–1284. 10.1109/TPAMI.2013.194 (2014).
</cite> [<a href="https://doi.org/10.1109/TPAMI.2013.194" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26353287/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Deng,%20W.,%20Hu,%20J.,%20Lu,%20J.%20&amp;%20Guo,%20J.%20Transform-invariant%20pca:%20A%20unified%20approach%20to%20fully%20automatic%20facealignment,%20representation,%20and%20recognition.%20IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence36(06),%201275%E2%80%931284.%2010.1109/TPAMI.2013.194%20(2014)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR52">
<span class="label">52.</span><cite>Deng, W., Guo, J. &amp; Hu, J. Extended src: Undersampled face recognition via intraclass variant dictionary. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em><strong>34</strong>(09), 1864–1870. 10.1109/TPAMI.2012.30 (2012).
</cite> [<a href="https://doi.org/10.1109/TPAMI.2012.30" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22813959/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Deng,%20W.,%20Guo,%20J.%20&amp;%20Hu,%20J.%20Extended%20src:%20Undersampled%20face%20recognition%20via%20intraclass%20variant%20dictionary.%20IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence34(09),%201864%E2%80%931870.%2010.1109/TPAMI.2012.30%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR53">
<span class="label">53.</span><cite>Wright, J., Yang, A. Y., Ganesh, A., Sastry, S. S. &amp; Ma, Y. Robust face recognition via sparse representation. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em><strong>31</strong>(2), 210–227. 10.1109/TPAMI.2008.79 (2009).
</cite> [<a href="https://doi.org/10.1109/TPAMI.2008.79" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19110489/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wright,%20J.,%20Yang,%20A.%20Y.,%20Ganesh,%20A.,%20Sastry,%20S.%20S.%20&amp;%20Ma,%20Y.%20Robust%20face%20recognition%20via%20sparse%20representation.%20IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence31(2),%20210%E2%80%93227.%2010.1109/TPAMI.2008.79%20(2009)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR54">
<span class="label">54.</span><cite>Ahonen, T., Hadid, A. &amp; Pietikäinen, M. Face recognition with local binary patterns. In <em>Computer Vision - ECCV 2004</em> (eds Pajdla, T. &amp; Matas, J.) 469–481 (Springer, Berlin, Heidelberg, 2004).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ahonen,%20T.,%20Hadid,%20A.%20&amp;%20Pietik%C3%A4inen,%20M.%20Face%20recognition%20with%20local%20binary%20patterns.%20In%20Computer%20Vision%20-%20ECCV%202004%20(eds%20Pajdla,%20T.%20&amp;%20Matas,%20J.)%20469%E2%80%93481%20(Springer,%20Berlin,%20Heidelberg,%202004)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR55">
<span class="label">55.</span><cite>Wen, Y., Zhang, K., Li, Z., &amp; Qiao, Y. A discriminative feature learning approach for deep face recognition. In <em>Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14</em>, pp. 499–515 (2016). Springer</cite>
</li>
<li id="CR56">
<span class="label">56.</span><cite>Chopra, S., Hadsell, R., &amp; LeCun, Y. Learning a similarity metric discriminatively, with application to face verification. In <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)</em>, vol. 1, pp. 539–5461 (2005). 10.1109/CVPR.2005.202</cite>
</li>
<li id="CR57">
<span class="label">57.</span><cite>Parkhi, O. M., Vedaldi, A., &amp; Zisserman, A. Deep face recognition. In <em>Proceedings of the British Machine Vision Conference (BMVC)</em>, pp. 41–14112. BMVA Press, (2015). 10.5244/C.29.41</cite>
</li>
<li id="CR58">
<span class="label">58.</span><cite>Huang, X. et al. Face verification based on deep learning for person tracking in hazardous goods factories. <em>Processes</em><strong>10</strong>(2), 380. 10.3390/pr10020380 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Huang,%20X.%20et%20al.%20Face%20verification%20based%20on%20deep%20learning%20for%20person%20tracking%20in%20hazardous%20goods%20factories.%20Processes10(2),%20380.%2010.3390/pr10020380%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR59">
<span class="label">59.</span><cite>Yi, D., Lei, Z., Liao, S., &amp; Li, S. Z. Learning face representation from scratch. CoRR <a href="http://arxiv.org/abs/1411.7923" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1411.7923</a> (2014).</cite>
</li>
<li id="CR60">
<span class="label">60.</span><cite>Klare, B. F. et al. Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a. In <em>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 1931–1939 (2015). 10.1109/CVPR.2015.7298803</cite>
</li>
<li id="CR61">
<span class="label">61.</span><cite>Miller, D., Kemelmacher-Shlizerman, I., &amp; Seitz, S. M. Megaface: A million faces for recognition at scale. CoRR <a href="http://arxiv.org/abs/1505.02108" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1505.02108</a> (2015).</cite>
</li>
<li id="CR62">
<span class="label">62.</span><cite>Cao, Q., Shen, L., Xie, W., Parkhi, O. M., &amp; Zisserman, A. Vggface2: A dataset for recognising faces across pose and age. In <em>2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)</em>, pp. 67–74 (2018). 10.1109/FG.2018.00020 . IEEE</cite>
</li>
<li id="CR63">
<span class="label">63.</span><cite>Zheng, T., &amp; Deng, W. Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments. <em>Technical Report 18-01, Beijing University of Posts and Telecommunications</em> (2018).</cite>
</li>
<li id="CR64">
<span class="label">64.</span><cite>Zhu, Z. et al. Masked face recognition challenge: The webface260m track report. CoRR <a href="http://arxiv.org/abs/2108.07189" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2108.07189</a> (2021).</cite>
</li>
<li id="CR65">
<span class="label">65.</span><cite>Learned-Miller, G. B. H. E. Labeled faces in the wild: Updates and new reporting procedures. Technical Report UM-CS-2014-003, University of Massachusetts, Amherst (2014).</cite>
</li>
<li id="CR66">
<span class="label">66.</span><cite>Jia, S., Guo, G. &amp; Xu, Z. A survey on 3d mask presentation attack detection and countermeasures. <em>Pattern Recognition</em><strong>98</strong>, 107032. 10.1016/j.patcog.2019.107032 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Jia,%20S.,%20Guo,%20G.%20&amp;%20Xu,%20Z.%20A%20survey%20on%203d%20mask%20presentation%20attack%20detection%20and%20countermeasures.%20Pattern%20Recognition98,%20107032.%2010.1016/j.patcog.2019.107032%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR67">
<span class="label">67.</span><cite>Sebastien, M., Nixon, M. &amp; Li, S. <em>Handbook of Biometric Anti-Spoofing: Trusted Biometrics Under Spoofing Attacks</em> (Springer, 2014).</cite> [<a href="https://scholar.google.com/scholar_lookup?Sebastien,%20M.,%20Nixon,%20M.%20&amp;%20Li,%20S.%20Handbook%20of%20Biometric%20Anti-Spoofing:%20Trusted%20Biometrics%20Under%20Spoofing%20Attacks%20(Springer,%202014)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR68">
<span class="label">68.</span><cite>Manjani, I., Tariyal, S., Vatsa, M., Singh, R. &amp; Majumdar, A. Detecting silicone mask-based presentation attack via deep dictionary learning. <em>IEEE Transactions on Information Forensics and Security</em><strong>12</strong>(7), 1713–1723. 10.1109/TIFS.2017.2676720 (2017).</cite> [<a href="https://scholar.google.com/scholar_lookup?Manjani,%20I.,%20Tariyal,%20S.,%20Vatsa,%20M.,%20Singh,%20R.%20&amp;%20Majumdar,%20A.%20Detecting%20silicone%20mask-based%20presentation%20attack%20via%20deep%20dictionary%20learning.%20IEEE%20Transactions%20on%20Information%20Forensics%20and%20Security12(7),%201713%E2%80%931723.%2010.1109/TIFS.2017.2676720%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR69">
<span class="label">69.</span><cite>Xu, Y., Price, T., Frahm, J.-M., &amp; Monrose, F. Virtual u: Defeating face liveness detection by building virtual models from your public photos. In <em>25th USENIX Security Symposium (USENIX Security 16)</em>, pp. 497–512. USENIX Association, (2016). <a href="https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/xu" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/xu</a></cite>
</li>
<li id="CR70">
<span class="label">70.</span><cite>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., &amp; Fergus, R. Intriguing properties of neural networks. arXiv preprint <a href="http://arxiv.org/abs/1312.6199" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1312.6199</a> (2013).</cite>
</li>
<li id="CR71">
<span class="label">71.</span><cite>Papernot, N. et al. Practical Black-Box Attacks against Machine Learning (2017).</cite>
</li>
<li id="CR72">
<span class="label">72.</span><cite>Hendrycks, D., &amp; Gimpel, K. Early methods for detecting adversarial images. arXiv preprint <a href="http://arxiv.org/abs/1608.00530" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1608.00530</a> (2016).</cite>
</li>
<li id="CR73">
<span class="label">73.</span><cite>Thies, J., Zollhöfer, M., Stamminger, M., Theobalt, C., &amp; Nießner, M. Face2Face: Real-time Face Capture and Reenactment of RGB Videos (2020).</cite>
</li>
<li id="CR74">
<span class="label">74.</span><cite>Rossler, A. et al. Faceforensics++: Learning to detect manipulated facial images. In <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp. 1–11 (2019). 10.1109/ICCV.2019.00009</cite>
</li>
<li id="CR75">
<span class="label">75.</span><cite>Choi, Y. et al. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, pp. 8789–8797 (2018). 10.1109/CVPR.2018.00916</cite>
</li>
<li id="CR76">
<span class="label">76.</span><cite>Liu, M. et al. Stgan: A unified selective transfer network for arbitrary image attribute editing. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 3673–3682 (2019).</cite>
</li>
<li id="CR77">
<span class="label">77.</span><cite>Karras, T. et al. Analyzing and improving the image quality of stylegan. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 8110–8119 (2020). 10.1109/CVPR42600.2020.00813</cite>
</li>
<li id="CR78">
<span class="label">78.</span><cite>Moosavi-Dezfooli, S.-M., Fawzi, A., &amp; Frossard, P. DeepFool: a simple and accurate method to fool deep neural networks (2016).</cite>
</li>
<li id="CR79">
<span class="label">79.</span><cite>Papernot, N. et al. The limitations of deep learning in adversarial settings. In <em>2016 IEEE European Symposium on Security and Privacy (EuroS &amp;P)</em>, pp. 372–387 (2016). IEEE</cite>
</li>
<li id="CR80">
<span class="label">80.</span><cite>Carlini, N., &amp; Wagner, D. Towards Evaluating the Robustness of Neural Networks (2017).</cite>
</li>
<li id="CR81">
<span class="label">81.</span><cite>Sabour, S., Cao, Y., Faghri, F., &amp; Fleet, D. J. Adversarial Manipulation of Deep Representations (2016)</cite>
</li>
<li id="CR82">
<span class="label">82.</span><cite>Zhou, F., Yin, B., Ling, H., Zhou, Q., &amp; Wang, W. Improving the transferability of adversarial attacks on face recognition with diverse parameters augmentation. In <em>Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)</em>, pp. 3516–3527 (2025)</cite>
</li>
<li id="CR83">
<span class="label">83.</span><cite>Yu, Y. et al. Toward model resistant to transferable adversarial examples via trigger activation. <em>IEEE Transactions on Information Forensics and Security</em><strong>20</strong>, 3745–3757. 10.1109/TIFS.2025.3553043 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yu,%20Y.%20et%20al.%20Toward%20model%20resistant%20to%20transferable%20adversarial%20examples%20via%20trigger%20activation.%20IEEE%20Transactions%20on%20Information%20Forensics%20and%20Security20,%203745%E2%80%933757.%2010.1109/TIFS.2025.3553043%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR84">
<span class="label">84.</span><cite>Song, Q., Wu, Y., &amp; Yang, L. Attacks on state-of-the-art face recognition using attentional adversarial attack generative network. CoRR <a href="http://arxiv.org/abs/1811.12026" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1811.12026</a> (2018)</cite>
</li>
<li id="CR85">
<span class="label">85.</span><cite>Dabouei, A., Soleymani, S., Dawson, J., &amp; Nasrabadi, N. Fast geometrically-perturbed adversarial faces. In <em>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, pp. 1979–1988. IEEE Computer Society, (2019). 10.1109/WACV.2019.00215 .</cite>
</li>
<li id="CR86">
<span class="label">86.</span><cite>Guo, W., Tondi, B. &amp; Barni, M. A master key backdoor for universal impersonation attack against dnn-based face verification. <em>Pattern Recognition Letters</em><strong>144</strong>, 61–67 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Guo,%20W.,%20Tondi,%20B.%20&amp;%20Barni,%20M.%20A%20master%20key%20backdoor%20for%20universal%20impersonation%20attack%20against%20dnn-based%20face%20verification.%20Pattern%20Recognition%20Letters144,%2061%E2%80%9367%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR87">
<span class="label">87.</span><cite>Park, J., McLaughlin, N., &amp; Alouani, I. Mind the gap: Detecting black-box adversarial attacks in the making through query update analysis. In <em>Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)</em>, pp. 10235–10243 (2025).</cite>
</li>
<li id="CR88">
<span class="label">88.</span><cite>Mao, X., Chen, Y., Li, Y., He, Y., &amp; Xue, H. GAP++: learning to generate target-conditioned adversarial examples. CoRR <a href="http://arxiv.org/abs/2006.05097" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2006.05097</a> (2020).</cite>
</li>
<li id="CR89">
<span class="label">89.</span><cite>Bose, A., &amp; Aarabi, P. Adversarial attacks on face detectors using neural net based constrained optimization. In <em>2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP)</em>, pp. 1–6 (2018). IEEE</cite>
</li>
<li id="CR90">
<span class="label">90.</span><cite>Kilany, S. A., Mahfouz, A., Zaki, A. M., &amp; Sayed, A. Analysis of adversarial attacks on face verification systems. In <em>Proceedings of the International Conference on Artificial Intelligence and Computer Vision (AICV2021)</em>, pp. 463–472. Springer, Cham (2021).</cite>
</li>
<li id="CR91">
<span class="label">91.</span><cite>Zhang, J. et al. Anyattack: Towards large-scale self-supervised adversarial attacks on vision-language models. In <em>Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)</em>, pp. 19900–19909 (2025).</cite>
</li>
<li id="CR92">
<span class="label">92.</span><cite>Liu, Y. et al. Projattacker: A configurable physical adversarial attack for face recognition via projector. In <em>Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)</em>, pp. 21248–21257 (2025).</cite>
</li>
<li id="CR93">
<span class="label">93.</span><cite>Wang, J., Zhang, H., &amp; Yuan, Y. Adv-cpg: A customized portrait generation framework with facial adversarial attacks. In <em>Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)</em>, pp. 21001–21010 (2025).</cite>
</li>
<li id="CR94">
<span class="label">94.</span><cite>Kong, D., Liang, S., Zhu, X., Zhong, Y. &amp; Ren, W. Patch is enough: naturalistic adversarial patch against vision-language pre-training models. <em>Visual Intelligence</em><strong>2</strong>(1), 1–10 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Kong,%20D.,%20Liang,%20S.,%20Zhu,%20X.,%20Zhong,%20Y.%20&amp;%20Ren,%20W.%20Patch%20is%20enough:%20naturalistic%20adversarial%20patch%20against%20vision-language%20pre-training%20models.%20Visual%20Intelligence2(1),%201%E2%80%9310%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR95">
<span class="label">95.</span><cite>Kong, C. et al. M<img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5604/12373911/171ab7e2fed4/41598_2025_15753_Article_IEq34.gif" loading="lazy" alt="Inline graphic">3fas: An accurate and robust multimodal mobile face anti-spoofing system. <em>IEEE Transactions on Dependable and Secure Computing</em><strong>21</strong>(6), 5650–5666. 10.1109/TDSC.2024.3381598 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Kong,%20C.%20et%20al.%20M3fas:%20An%20accurate%20and%20robust%20multimodal%20mobile%20face%20anti-spoofing%20system.%20IEEE%20Transactions%20on%20Dependable%20and%20Secure%20Computing21(6),%205650%E2%80%935666.%2010.1109/TDSC.2024.3381598%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR96">
<span class="label">96.</span><cite>Cai, R. et al. S-adapter: Generalizing vision transformer for face anti-spoofing with statistical tokens. <em>IEEE Transactions on Information Forensics and Security</em><strong>19</strong>, 8385–8397. 10.1109/TIFS.2024.3420699 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Cai,%20R.%20et%20al.%20S-adapter:%20Generalizing%20vision%20transformer%20for%20face%20anti-spoofing%20with%20statistical%20tokens.%20IEEE%20Transactions%20on%20Information%20Forensics%20and%20Security19,%208385%E2%80%938397.%2010.1109/TIFS.2024.3420699%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR97">
<span class="label">97.</span><cite>Kong, C. et al. Digital and physical face attacks: Reviewing and one step further. <em>APSIPA Transactions on Signal and Information Processing</em>, <strong>12</strong>(1), (2022)</cite>
</li>
<li id="CR98">
<span class="label">98.</span><cite>Liu, Y., Stehouwer, J., &amp; Liu, X. On Disentangling Spoof Trace for Generic Face Anti-Spoofing (2020).</cite>
</li>
<li id="CR99">
<span class="label">99.</span><cite>Rudin, L. I., Osher, S. &amp; Fatemi, E. Nonlinear total variation based noise removal algorithms. <em>Physica D: Nonlinear Phenomena</em><strong>60</strong>(1), 259–268 (1992).</cite> [<a href="https://scholar.google.com/scholar_lookup?Rudin,%20L.%20I.,%20Osher,%20S.%20&amp;%20Fatemi,%20E.%20Nonlinear%20total%20variation%20based%20noise%20removal%20algorithms.%20Physica%20D:%20Nonlinear%20Phenomena60(1),%20259%E2%80%93268%20(1992)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR100">
<span class="label">100.</span><cite>Efros, A. A., &amp; Freeman, W. T. Image quilting for texture synthesis and transfer. In <em>Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</em>, pp. 341–346 (2001)</cite>
</li>
<li id="CR101">
<span class="label">101.</span><cite>Deng, J. et al. Imagenet: A large-scale hierarchical image database. In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, pp. 248–255 (2009). 10.1109/CVPR.2009.5206848</cite>
</li>
<li id="CR102">
<span class="label">102.</span><cite>Dziugaite, G. K., Ghahramani, Z., &amp; Roy, D. M. A <em>study of the effect of JPG compression on adversarial images</em> (2016).</cite>
</li>
<li id="CR103">
<span class="label">103.</span><cite>Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Chen, L., Kounavis, M. E., &amp; Chau, D. H. Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression. <a href="http://arxiv.org/abs/1705.02900" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1705.02900</a> (2017)</cite>
</li>
<li id="CR104">
<span class="label">104.</span><cite>Liu, Z. et al. Feature distillation: Dnn-oriented jpeg compression against adversarial examples. In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 860–868 (2019). IEEE.</cite>
</li>
<li id="CR105">
<span class="label">105.</span><cite>Y., L. The mnist database of handwritten digits. <a href="http://yann.lecun.com/exdb/mnist/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://yann.lecun.com/exdb/mnist/</a> (1998)</cite>
</li>
<li id="CR106">
<span class="label">106.</span><cite>Krizhevsky, A., &amp; Hinton, G. Learning multiple layers of features from tiny images. <em>Master’s thesis, Department of Computer Science</em>, University of Toronto, 32–33 (2009).</cite>
</li>
<li id="CR107">
<span class="label">107.</span><cite>Kurakin, A., Goodfellow, I. J., &amp; Bengio, S. Adversarial examples in the physical world. CoRR <a href="http://arxiv.org/1607.02533" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:abs/1607.02533</a> (2016).</cite>
</li>
<li id="CR108">
<span class="label">108.</span><cite>Georghiades, A. S., Belhumeur, P. N. &amp; Kriegman, D. J. From few to many: illumination cone models for face recognition under variable lighting and pose. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em><strong>23</strong>(6), 643–660. 10.1109/34.927464 (2001).</cite> [<a href="https://scholar.google.com/scholar_lookup?Georghiades,%20A.%20S.,%20Belhumeur,%20P.%20N.%20&amp;%20Kriegman,%20D.%20J.%20From%20few%20to%20many:%20illumination%20cone%20models%20for%20face%20recognition%20under%20variable%20lighting%20and%20pose.%20IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence23(6),%20643%E2%80%93660.%2010.1109/34.927464%20(2001)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR109">
<span class="label">109.</span><cite>Chen, P.-Y., Sharma, Y., Zhang, H., Yi, J., &amp; Hsieh, C.-J. Ead: Elastic-net attacks to deep neural networks via adversarial examples <strong>32</strong>(1), 10–17 (2018).</cite>
</li>
<li id="CR110">
<span class="label">110.</span><cite>Goswami, G., Agarwal, A., Ratha, N., Singh, R. &amp; Vatsa, M. Detecting and mitigating adversarial perturbations for robust face recognition. <em>International Journal of Computer Vision</em><strong>127</strong>(6), 719–742 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Goswami,%20G.,%20Agarwal,%20A.,%20Ratha,%20N.,%20Singh,%20R.%20&amp;%20Vatsa,%20M.%20Detecting%20and%20mitigating%20adversarial%20perturbations%20for%20robust%20face%20recognition.%20International%20Journal%20of%20Computer%20Vision127(6),%20719%E2%80%93742%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR111">
<span class="label">111.</span><cite>Founds, A., Orlans, N., Genevieve, W., &amp; Watson, C. NIST Special Databse 32 - Multiple Encounter Dataset II (MEDS-II). NIST Interagency/Internal Report (NISTIR), National Institute of Standards and Technology, Gaithersburg, MD (2011). 10.6028/NIST.IR.7807 . <a href="https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=908383" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=908383</a></cite>
</li>
<li id="CR112">
<span class="label">112.</span><cite>Beveridge, J. R. et al. The challenge of face recognition from digital point-and-shoot cameras. In <em>2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)</em>, pp. 1–8. IEEE, (2013). 10.1109/BTAS.2013.6712704</cite>
</li>
<li id="CR113">
<span class="label">113.</span><cite>Phillips, P. et al. Overview of the Multiple Biometrics Grand Challenge. NIST Interagency/Internal Report (NISTIR), National Institute of Standards and Technology, Gaithersburg, MD (2009). <a href="https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=903086" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=903086</a></cite>
</li>
<li id="CR114">
<span class="label">114.</span><cite>Gross, R., Matthews, I., Cohn, J., Kanade, T. &amp; Baker, S. Multi-pie. <em>Image and Vision Computing</em><strong>28</strong>(5), 807–813. 10.1016/j.imavis.2009.08.002 (2010).</cite> [<a href="https://doi.org/10.1016/j.imavis.2009.08.002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC2873597/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20490373/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Gross,%20R.,%20Matthews,%20I.,%20Cohn,%20J.,%20Kanade,%20T.%20&amp;%20Baker,%20S.%20Multi-pie.%20Image%20and%20Vision%20Computing28(5),%20807%E2%80%93813.%2010.1016/j.imavis.2009.08.002%20(2010)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR115">
<span class="label">115.</span><cite>Mopuri, K. R., Garg, U., &amp; Babu, R. V. Fast feature fool: A data independent approach to universal adversarial perturbations. CoRR <a href="http://arxiv.org/abs/1707.05572" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1707.05572</a> (2017).</cite>
</li>
<li id="CR116">
<span class="label">116.</span><cite>Sayed, A., Kinlany, S., Zaki, A. &amp; Mahfouz, A. Veriface: Defending against adversarial attacks in face verification systems. <em>Computers, Materials &amp; Continua</em><strong>76</strong>(3), 3151–3166. 10.32604/cmc.2023.040256 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Sayed,%20A.,%20Kinlany,%20S.,%20Zaki,%20A.%20&amp;%20Mahfouz,%20A.%20Veriface:%20Defending%20against%20adversarial%20attacks%20in%20face%20verification%20systems.%20Computers,%20Materials%20&amp;%20Continua76(3),%203151%E2%80%933166.%2010.32604/cmc.2023.040256%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR117">
<span class="label">117.</span><cite>Liu, Z., Luo, P., Wang, X., &amp; Tang, X. Deep learning face attributes in the wild, 3730–3738 (2015).</cite>
</li>
<li id="CR118">
<span class="label">118.</span><cite>Karras, T., Laine, S., &amp; Aila, T. A style-based generator architecture for generative adversarial networks. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 4401–4410 (2019). 10.1109/CVPR.2019.00453</cite> [<a href="https://doi.org/10.1109/TPAMI.2020.2970919" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32012000/" class="usa-link">PubMed</a>]</li>
<li id="CR119">
<span class="label">119.</span><cite>Qiu, H. et al. Semanticadv: Generating adversarial examples via attribute-conditional image editing. CoRR <a href="http://arxiv.org/abs/1906.07927" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1906.07927</a> (2019).</cite>
</li>
<li id="CR120">
<span class="label">120.</span><cite>Carlini, N., &amp; Wagner, D. Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods (2017).</cite>
</li>
<li id="CR121">
<span class="label">121.</span><cite>Athalye, A., Carlini, N., &amp; Wagner, D. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples (2018).</cite>
</li>
<li id="CR122">
<span class="label">122.</span><cite>Carlini, N., &amp; Wagner, D. MagNet and “Efficient Defenses Against Adversarial Attacks” are Not Robust to Adversarial Examples (2017).</cite>
</li>
<li id="CR123">
<span class="label">123.</span><cite>Grosse, K., Manoharan, P., Papernot, N., Backes, M., &amp; McDaniel, P. On the (Statistical) Detection of Adversarial Examples (2017).</cite>
</li>
<li id="CR124">
<span class="label">124.</span><cite>Guo, W., Tondi, B. &amp; Barni, M. Universal detection of backdoor attacks via density-based clustering and centroids analysis. <em>IEEE Transactions on Information Forensics and Security</em><strong>19</strong>, 970–984. 10.1109/TIFS.2023.3329426 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Guo,%20W.,%20Tondi,%20B.%20&amp;%20Barni,%20M.%20Universal%20detection%20of%20backdoor%20attacks%20via%20density-based%20clustering%20and%20centroids%20analysis.%20IEEE%20Transactions%20on%20Information%20Forensics%20and%20Security19,%20970%E2%80%93984.%2010.1109/TIFS.2023.3329426%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR125">
<span class="label">125.</span><cite>Wang, H. et al. Cosface: Large margin cosine loss for deep face recognition. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> (2018).</cite>
</li>
<li id="CR126">
<span class="label">126.</span><cite>Zhang, H. et al. Theoretically principled trade-off between robustness and accuracy. In <em>International Conference on Machine Learning</em>, pp. 7472–7482 (2019). PMLR</cite>
</li>
<li id="CR127">
<span class="label">127.</span><cite>Radford, A. et al. Learning transferable visual models from natural language supervision. In Meila, M., Zhang, T. (eds.) <em>Proceedings of the 38th International Conference on Machine Learning. Proceedings of Machine Learning Research</em>, vol. 139, pp. 8748–8763. PMLR, (2021). <a href="https://proceedings.mlr.press/v139/radford21a.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://proceedings.mlr.press/v139/radford21a.html</a></cite>
</li>
<li id="CR128">
<span class="label">128.</span><cite>Mao, C., Geng, S., Yang, J., Wang, X., &amp; Vondrick, C. Understanding zero-shot adversarial robustness for large-scale models. arXiv preprint <a href="http://arxiv.org/abs/2212.07016" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2212.07016</a> (2022).</cite>
</li>
<li id="CR129">
<span class="label">129.</span><cite>Schlarmann, C., Singh, N. D., Croce, F., &amp; Hein, M. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models. arXiv preprint <a href="http://arxiv.org/abs/2402.12336" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2402.12336</a> (2024).</cite>
</li>
<li id="CR130">
<span class="label">130.</span><cite>Zhou, W., Bai, S., Mandic, D. P., Zhao, Q., &amp; Chen, B. Revisiting the adversarial robustness of vision language models: a multimodal perspective. arXiv preprint <a href="http://arxiv.org/abs/2404.19287" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2404.19287</a> (2024).</cite>
</li>
<li id="CR131">
<span class="label">131.</span><cite>Ghiasvand, S., Oskouie, H. E., Alizadeh, M., &amp; Pedarsani, R. Few-shot adversarial low-rank fine-tuning of vision-language models. arXiv preprint <a href="http://arxiv.org/abs/2505.15130" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2505.15130</a> (2025).</cite>
</li>
<li id="CR132">
<span class="label">132.</span><cite>Kong, C. et al. Detect and locate: Exposing face manipulation by semantic- and noise-level telltales. <em>IEEE Transactions on Information Forensics and Security</em><strong>17</strong>, 1741–1756. 10.1109/TIFS.2022.3169921 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Kong,%20C.%20et%20al.%20Detect%20and%20locate:%20Exposing%20face%20manipulation%20by%20semantic-%20and%20noise-level%20telltales.%20IEEE%20Transactions%20on%20Information%20Forensics%20and%20Security17,%201741%E2%80%931756.%2010.1109/TIFS.2022.3169921%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR133">
<span class="label">133.</span><cite>Zhang, J. et al. Adversarial prompt tuning for vision-language models. In <em>European Conference on Computer Vision</em>, pp. 56–72 (2024). Springer.</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets used and/or analysed during the current study are available from the corresponding author on reasonable request.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-15753-8"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_15753.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (3.6 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12373911/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12373911/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373911%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373911/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12373911/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12373911/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40847113/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12373911/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40847113/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12373911/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12373911/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="yrZWgUz3NNTqOeU6RUN0Qz2K6gpcJpLaMFNkYLN2VTNXgHCMCwoSpc9uCcz2aLHk">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
