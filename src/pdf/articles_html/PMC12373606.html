
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Decoding target discriminability and time pressure using eye and head movement features in a foraging search task - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E49F7A8AF1AC03059F7A0055A92922.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="cogresprinimp">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373606/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Cognitive Research: Principles and Implications">
<meta name="citation_title" content="Decoding target discriminability and time pressure using eye and head movement features in a foraging search task">
<meta name="citation_author" content="Anthony J Ries">
<meta name="citation_author_institution" content="Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA">
<meta name="citation_author_institution" content="Warfighter Effectiveness Research Center, U.S. Air Force Academy, Colorado Springs, CO 80840 USA">
<meta name="citation_author" content="Chloe Callahan-Flintoft">
<meta name="citation_author_institution" content="Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA">
<meta name="citation_author" content="Anna Madison">
<meta name="citation_author_institution" content="Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA">
<meta name="citation_author_institution" content="Warfighter Effectiveness Research Center, U.S. Air Force Academy, Colorado Springs, CO 80840 USA">
<meta name="citation_author" content="Louis Dankovich">
<meta name="citation_author_institution" content="Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA">
<meta name="citation_author" content="Jonathan Touryan">
<meta name="citation_author_institution" content="Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA">
<meta name="citation_publication_date" content="2025 Aug 22">
<meta name="citation_volume" content="10">
<meta name="citation_firstpage" content="53">
<meta name="citation_doi" content="10.1186/s41235-025-00657-y">
<meta name="citation_pmid" content="40846822">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373606/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373606/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373606/pdf/41235_2025_Article_657.pdf">
<meta name="description" content="In military operations, rapid and accurate decision-making is crucial, especially in visually complex and high-pressure environments. This study investigates how eye and head movement metrics can infer changes in search behavior during a ...">
<meta name="og:title" content="Decoding target discriminability and time pressure using eye and head movement features in a foraging search task">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="In military operations, rapid and accurate decision-making is crucial, especially in visually complex and high-pressure environments. This study investigates how eye and head movement metrics can infer changes in search behavior during a ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373606/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12373606">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1186/s41235-025-00657-y"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41235_2025_Article_657.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373606%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12373606/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12373606/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373606/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-cogresprinimp.gif" alt="Cognitive Research: Principles and Implications logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Cognitive Research: Principles and Implications" title="Link to Cognitive Research: Principles and Implications" shape="default" href="https://cognitiveresearchjournal.springeropen.com/" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Cogn Res Princ Implic</button></div>. 2025 Aug 22;10:53. doi: <a href="https://doi.org/10.1186/s41235-025-00657-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1186/s41235-025-00657-y</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Cogn%20Res%20Princ%20Implic%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cogn%20Res%20Princ%20Implic%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Cogn%20Res%20Princ%20Implic%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Cogn%20Res%20Princ%20Implic%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decoding target discriminability and time pressure using eye and head movement features in a foraging search task</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ries%20AJ%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Anthony J Ries</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Anthony J Ries</span></h3>
<div class="p">
<sup>1</sup>Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA </div>
<div class="p">
<sup>2</sup>Warfighter Effectiveness Research Center, U.S. Air Force Academy, Colorado Springs, CO 80840 USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ries%20AJ%22%5BAuthor%5D" class="usa-link"><span class="name western">Anthony J Ries</span></a>
</div>
</div>
<sup>1,</sup><sup>2,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Callahan-Flintoft%20C%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Chloe Callahan-Flintoft</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Chloe Callahan-Flintoft</span></h3>
<div class="p">
<sup>1</sup>Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Callahan-Flintoft%20C%22%5BAuthor%5D" class="usa-link"><span class="name western">Chloe Callahan-Flintoft</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Madison%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Anna Madison</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Anna Madison</span></h3>
<div class="p">
<sup>1</sup>Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA </div>
<div class="p">
<sup>2</sup>Warfighter Effectiveness Research Center, U.S. Air Force Academy, Colorado Springs, CO 80840 USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Madison%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Anna Madison</span></a>
</div>
</div>
<sup>1,</sup><sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Dankovich%20L%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Louis Dankovich</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Louis Dankovich</span></h3>
<div class="p">
<sup>1</sup>Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Dankovich%20L%22%5BAuthor%5D" class="usa-link"><span class="name western">Louis Dankovich</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Touryan%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Jonathan Touryan</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Jonathan Touryan</span></h3>
<div class="p">
<sup>1</sup>Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Touryan%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Jonathan Touryan</span></a>
</div>
</div>
<sup>1</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Humans in Complex Systems, U.S. Army DEVCOM Army Research Laboratory, 7101 Mulberry Point Rd, Aberdeen Proving Ground, MD 21005 USA </div>
<div id="Aff2">
<sup>2</sup>Warfighter Effectiveness Research Center, U.S. Air Force Academy, Colorado Springs, CO 80840 USA </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Jan 31; Accepted 2025 Jul 4; Collection date 2025 Dec.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12373606  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40846822/" class="usa-link">40846822</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">In military operations, rapid and accurate decision-making is crucial, especially in visually complex and high-pressure environments. This study investigates how eye and head movement metrics can infer changes in search behavior during a naturalistic shooting scenario in virtual reality (VR). Thirty-one participants performed a foraging search task using a head-mounted display (HMD) with integrated eye tracking. Participants searched for targets among distractors under varying levels of target discriminability (easy vs. hard) and time pressure (low vs. high). As expected, behavioral results indicated that increased discrimination difficulty and greater time pressure negatively impacted performance, leading to slower response times and reduced d-prime. Support vector classifiers assigned a search condition, discriminability and time pressure, to each trial based on eye and head movement features. Combined eye and head features produced the most accurate classification model for capturing tasked-induced changes in search behavior, with the combined model outperforming those based on eye or head features alone. While eye features demonstrated strong predictive power, the inclusion of head features significantly enhanced model performance. Across the ensemble of eye metrics, fixation-related features were the most robust for classifying target discriminability, while saccadic-related features played a similar role for time pressure. In contrast, models constrained to head metrics emphasized global movement (amplitude, velocity) for classifying discriminability but shifted toward kinematic intensity (acceleration, jerk) in time pressure condition. Together these results speak to the complementary role of eye and head movements in understanding search behavior under changing task parameters.</p>
<section id="sec1"><h3 class="pmc_sec_title">Supplementary Information</h3>
<p>The online version contains supplementary material available at 10.1186/s41235-025-00657-y.</p></section><section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Virtual reality, Eye tracking, Head movements, Visual search, Task prediction, Cognitive state, Visual foraging, Multiclass classification, Machine learning, Support vector machine</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">In modern military operations, making quick, accurate decisions in visually complex environments is critical to mission success. This is particularly true for soldiers in dynamic, high-stress scenarios, requiring rapid ability to locate targets (e.g., potential threats in the environment) and differentiate them from distractors, which are sometimes visually similar (e.g., friendlies among enemy soldiers). The significant consequences of delayed or incorrect responses in these situations underscore the critical role of a soldier’s attentional focus and their capacity to manage competing, time-sensitive demands. Collectively, these factors shape how soldiers perceive and respond to information, ultimately affecting their decision-making and the effectiveness of their actions.</p>
<p id="Par3">The military has shown an increasing interest in leveraging the computational power of artificial intelligence (AI) to help soldiers in complex, time-constrained decision-making. However, seamless integration with human teams requires providing AI with ongoing contextual information, such as changes in mission parameters or operator goals. This contextual information can potentially be inferred from changes in observable behaviors and human–system interactions. For example, delays in operator response times or atypical information requests from the AI may indicate that task parameters or environmental conditions have changed. Having this information and drawing such inferences would enable the AI to provide tailored assistance without imposing additional cognitive load on the operator by requiring explicit reports of task parameter changes. Real-time measurement and prediction of task-induced changes in search performance could significantly enhance decision-making, enabling effective adaptation to situational demands (Brunyé et al., <a href="#CR8" class="usa-link" aria-describedby="CR8">2020</a>). For instance, AI could identify imbalances in cognitive load across a team and recommend task reallocation. As the military integrates AI into combat scenarios, detecting fluctuations in search behavior could inform autonomous systems of changes in the soldier’s cognitive state or mission parameters, allowing the AI to adjust its behavior for real-time support to improve team efficiency (Marathe et al., <a href="#CR63" class="usa-link" aria-describedby="CR63">2018</a>; Metcalfe et al., <a href="#CR65" class="usa-link" aria-describedby="CR65">2021</a>; Reis et al., <a href="#CR75" class="usa-link" aria-describedby="CR75">2021</a>).</p>
<p id="Par4">Changes in search behavior often reflect underlying shifts in cognitive state (Boot et al., <a href="#CR7" class="usa-link" aria-describedby="CR7">2006</a>; Doshi &amp; Trivedi, <a href="#CR23" class="usa-link" aria-describedby="CR23">2012</a>; Eckstein, <a href="#CR28" class="usa-link" aria-describedby="CR28">2011</a>; HEnderson et al., <a href="#CR40" class="usa-link" aria-describedby="CR40">2013</a>; MacInnes et al., <a href="#CR61" class="usa-link" aria-describedby="CR61">2018</a>). These changes can originate from both endogenous factors such as fatigue and memory (Ganesan et al., <a href="#CR35" class="usa-link" aria-describedby="CR35">2018</a>; Le-Hoa Võ &amp; Wolfe, <a href="#CR58" class="usa-link" aria-describedby="CR58">2015</a>) and exogenous factors of the search array itself, like visual saliency (Christ &amp; Abrams, <a href="#CR15" class="usa-link" aria-describedby="CR15">2006</a>; Itti &amp; Koch, <a href="#CR48" class="usa-link" aria-describedby="CR48">2000</a>; Theeuwes, <a href="#CR86" class="usa-link" aria-describedby="CR86">1994</a>; Yantis &amp; Jonides, <a href="#CR93" class="usa-link" aria-describedby="CR93">1990</a>). Classically, search behavior differences due to changes in task parameters have been studied using central tendency metrics calculated from a block of trials (e.g., average response times are slower when targets are heterogeneous compared to homogeneous, (Duncan &amp; Humphreys, <a href="#CR26" class="usa-link" aria-describedby="CR26">1989</a>). While this method is effective and has provided insight into numerous cognitive mechanisms, it does not allow for a trial-by-trial assessment of task-induced behavioral variance. Understanding these task-induced changes in behavior are essential to infer or “predict” the current task parameters. It is this prediction that could have meaningful impact in the design of future AI integration into human teams. As such, the current work seeks to demonstrate whether head and eye movement data (either together or separately) can be used to infer the current parameters of the human’s search task.</p>
<section id="Sec2"><h3 class="pmc_sec_title">Eye and head movements</h3>
<p id="Par5">Human eye gaze provides numerous metrics for assessing changes in search performance and information processing demands (GilChrist &amp; Harvey, <a href="#CR37" class="usa-link" aria-describedby="CR37">2006</a>; König et al., <a href="#CR51" class="usa-link" aria-describedby="CR51">2016</a>; Marshall, <a href="#CR64" class="usa-link" aria-describedby="CR64">2007</a>). Saccade characteristics, such as frequency and peak velocity, predict target detection accuracy and reflect workload variations during visually complex decision-making tasks (Boot et al., <a href="#CR7" class="usa-link" aria-describedby="CR7">2006</a>; Di Stasi et al., <a href="#CR21" class="usa-link" aria-describedby="CR21">2011</a>). Fixation metrics, including duration, frequency, and sequence, are strong predictors of target selection in object search tasks (Huang et al., <a href="#CR45" class="usa-link" aria-describedby="CR45">2015</a>) and are effective for distinguishing between classes of images (Karessli et al., <a href="#CR49" class="usa-link" aria-describedby="CR49">2017</a>). Scanpath length captures search strategies, with longer scanpaths suggesting a global or ambient processing approach that prioritizes information outside the immediate field of view (Groner et al., <a href="#CR39" class="usa-link" aria-describedby="CR39">1984</a>; Velichkovsky et al., <a href="#CR88" class="usa-link" aria-describedby="CR88">2002</a>). Lower decision certainty is associated with fewer, longer fixations and slower saccades (Brunyé &amp; Gardony, <a href="#CR9" class="usa-link" aria-describedby="CR9">2017</a>). In high-stress shooting scenarios, elite police officers exhibit longer fixations and faster response times on relevant targets compared to novices, reflecting more efficient search strategies (Vickers &amp; Lewinski, <a href="#CR89" class="usa-link" aria-describedby="CR89">2012</a>). Additionally, fixation frequency, duration, and pupil diameter reliably track fluctuations in cognitive workload (Enders et al., <a href="#CR30" class="usa-link" aria-describedby="CR30">2021</a>; Mallick et al., <a href="#CR62" class="usa-link" aria-describedby="CR62">2016</a>; Pomplun &amp; Sunkara, <a href="#CR72" class="usa-link" aria-describedby="CR72">2019</a>; Van Orden et al., <a href="#CR87" class="usa-link" aria-describedby="CR87">2001</a>). Collectively, these gaze-based metrics illustrate how both spatial and temporal characteristics of eye movements can be used to evaluate task-induced changes in search behavior across various scenarios.</p>
<p id="Par6">Head movements, though less frequently measured in visual search studies, complement eye-tracking metrics by providing unique information on the spatiotemporal characteristics of search behavior in visually complex and dynamic tasks (Agtzidis et al., <a href="#CR1" class="usa-link" aria-describedby="CR1">2019</a>; Bischof et al., <a href="#CR6" class="usa-link" aria-describedby="CR6">2024</a>; Callahan-Flintoft et al., <a href="#CR12" class="usa-link" aria-describedby="CR12">2021</a>; Doshi &amp; Trivedi, <a href="#CR23" class="usa-link" aria-describedby="CR23">2012</a>). Head movements reveal broader shifts in attention and spatial orientation, especially during large gaze shifts or rapid reorienting (Pelz et al., <a href="#CR69" class="usa-link" aria-describedby="CR69">2001</a>; Stahl, <a href="#CR84" class="usa-link" aria-describedby="CR84">2001b</a>). Day to day visual search often requires scanning large regions of the visual field, using peripheral vision to detect and respond to multiple, potentially hidden targets. Integrating head movement metrics into search performance assessments, therefore, may enhance predictive power, contributing to more comprehensive and ecologically valid models of perception and decision-making.</p>
<p id="Par7">Visual search in the real world occurs, most often, in a cluttered and dynamic three-dimensional environment where the head and eyes must move in concert to acquire necessary information for target detection. It is imperative then to explore these two movement systems in tandem for a deeper understanding of the cognitive processes underlying search behavior (Bischof et al., <a href="#CR6" class="usa-link" aria-describedby="CR6">2024</a>; Sidenmark &amp; Gellersen, <a href="#CR79" class="usa-link" aria-describedby="CR79">2019</a>). Head and eye movements may differentially support cognitive function depending on whether task-relevant information lies in the peripheral or central regions of the current reference frame (David et al., <a href="#CR20" class="usa-link" aria-describedby="CR20">2020</a>; Draschkow et al., <a href="#CR24" class="usa-link" aria-describedby="CR24">2021</a>; Solman &amp; Kingstone, <a href="#CR81" class="usa-link" aria-describedby="CR81">2014</a>). For instance, when relevant information is spatially dispersed, participants may engage a trade-off between making head movements and engaging working memory storage (Draschkow et al., <a href="#CR24" class="usa-link" aria-describedby="CR24">2021</a>). Additionally, individuals differ in their reliance on head versus eye movements during visual search, with some favoring head movements for reorienting; a distinction that may reflect different cognitive strategies, such as"head movers"versus"non-movers"(Fuller, <a href="#CR34" class="usa-link" aria-describedby="CR34">1992</a>; Stahl, <a href="#CR83" class="usa-link" aria-describedby="CR83">2001a</a>, <a href="#CR84" class="usa-link" aria-describedby="CR84">2001b</a>). Together, these findings speak to the importance of both eye and head movements in supporting visual search execution and suggest that both systems may be affected by changes in task parameters.</p></section><section id="Sec3"><h3 class="pmc_sec_title">Virtual reality (VR) and visual foraging</h3>
<p id="Par8">Virtual reality (VR) offers unique capabilities to study visual search and other tasks that closely mimic real-world interactions (Bischof et al., <a href="#CR6" class="usa-link" aria-describedby="CR6">2024</a>; Callahan-Flintoft et al., <a href="#CR12" class="usa-link" aria-describedby="CR12">2021</a>; Clay et al., <a href="#CR17" class="usa-link" aria-describedby="CR17">2019</a>). Traditional visual search paradigms, typically confined to two-dimensional displays, offer precise control over experimental variables but lack the immersive, spatial interactions essential for capturing natural head and eye movements. VR provides realistic, three-dimensional environments that replicate the spatial complexity of real-world search tasks, enabling the study of cognitive processes and the natural coordination of eye and head movements in controlled, yet realistic settings (Callahan-Flintoft et al., <a href="#CR12" class="usa-link" aria-describedby="CR12">2021</a>, <a href="#CR13" class="usa-link" aria-describedby="CR13">2024</a>; Clay et al., <a href="#CR17" class="usa-link" aria-describedby="CR17">2019</a>; Sidenmark &amp; Gellersen, <a href="#CR79" class="usa-link" aria-describedby="CR79">2019</a>). Capturing both eye and head movements through integrated head-mounted display (HMD) systems, VR provides data on focal and peripheral search behavior which are essential elements of situational awareness in military operations (Brunyé &amp; Giles, <a href="#CR10" class="usa-link" aria-describedby="CR10">2023</a>; Pettersson et al., <a href="#CR70" class="usa-link" aria-describedby="CR70">2024</a>; Sidenmark &amp; Gellersen, <a href="#CR79" class="usa-link" aria-describedby="CR79">2019</a>).</p>
<p id="Par9">Similarly, visual foraging tasks provide an ecologically valid model for studying decision-making and attention allocation in multi-target scenarios (Cain et al., <a href="#CR11" class="usa-link" aria-describedby="CR11">2012</a>; Kristjánsson et al., <a href="#CR54" class="usa-link" aria-describedby="CR54">2014</a>; Wolfe, <a href="#CR92" class="usa-link" aria-describedby="CR92">2013</a>). Unlike traditional single-target search tasks, visual foraging involves searching for and selecting multiple targets within a scene, reflecting real-world situations where multiple potential targets may arise. This complexity closely resembles challenges faced by soldiers who must identify and respond to multiple threats in combat, where prioritizing targets and filtering distractions is critical (Kristjánsson et al., <a href="#CR55" class="usa-link" aria-describedby="CR55">2022</a>). Research shows that participants adapt their search strategies in foraging tasks based on target/non-target discriminability and time constraints, with more rapid switching between targets under high time pressure and less target switching when target discrimination is difficult (T. Kristjánsson et al., <a href="#CR56" class="usa-link" aria-describedby="CR56">2018</a>). Such conditions are also associated with increased saccade amplitude and higher fixation frequency (Tagu &amp; Kristjánsson, <a href="#CR85" class="usa-link" aria-describedby="CR85">2022</a>). These findings highlight the flexibility of search strategies in foraging tasks and underscore the value of VR for examining cognitive processes underlying naturalistic visual search, particularly in environments where target discrimination and time pressure impose distinct task demands.</p></section><section id="Sec4"><h3 class="pmc_sec_title">Current study</h3>
<p id="Par10">The current study explores the potential of eye and head movement metrics to predict changes in target discriminability and time pressure. Participants engaged in a naturalistic foraging search task within a VR forest environment using a head-mounted display (HMD) equipped with integrated eye and head tracking. Participants were required to identify and engage targets among distractors under varying levels of discrimination difficulty (easy vs. hard) and time pressure (low vs. high). By analyzing the collected eye and head movement data, we aimed to determine the predictive power of these metrics in classifying task parameter changes.</p>
<p id="Par11">Here, participants performed a foraging task while standing in one spot (i.e., without ambulation), enabling us to capture the natural coordination of eye and head movements during immersive search while still providing strict control on the spatial relationship with the viewer and the search array. This set-up serves as a midway point for ultimately understanding search in a fully ambulatory context. However, even though the participant is stationary, allowing the eyes and head to move freely during search provides a wealth of spatiotemporal features, any number of which could be influenced by changes in task parameters. As such, the current work adopts a data-driven approach in employing machine learning to disentangle whether eye and head movement is predictive of changes in search context. Support vector machines (SVM) and neural networks have been effective in classifying task-induced changes in behavior based on eye movement data (El Iskandarani et al., <a href="#CR29" class="usa-link" aria-describedby="CR29">2024</a>; Hulle et al., <a href="#CR46" class="usa-link" aria-describedby="CR46">2024</a>; Kothari et al., <a href="#CR52" class="usa-link" aria-describedby="CR52">2020</a>; MacInnes et al., <a href="#CR61" class="usa-link" aria-describedby="CR61">2018</a>). However, incorporating head movement metrics has the potential to significantly improve classification accuracy or provide an alternative for predicting behavioral changes when eye-tracking measurements are not feasible or practical (Hu et al., <a href="#CR44" class="usa-link" aria-describedby="CR44">2023</a>). Recent evidence demonstrates that the temporal derivatives associated with head movements, velocity, acceleration, and jerk can predict task-relevant physiological responses in VR environments (Salehi et al., <a href="#CR76" class="usa-link" aria-describedby="CR76">2024</a>). In addition to eye metrics, integrating head kinematic data, including rotation velocity, acceleration, jerk, and more traditional features such as amplitude, duration, and frequency, into machine learning models may capture complimentary information relevant to classifying search behaviors and provide insights that more classic analysis approaches could miss.</p></section></section><section id="Sec5"><h2 class="pmc_sec_title">Method</h2>
<section id="Sec6"><h3 class="pmc_sec_title">Participants</h3>
<p id="Par12">The Institutional Review Boards at the U.S. Air Force Academy (USAFA) and the U.S. Army Combat Capabilities Development Command (DEVCOM) approved this study (ARL 21–132), and all procedures followed the guidelines of the Declaration of Helsinki. A total of 33 United States Air Force Academy (USAFA) cadets participated in the study. They were recruited through the Sona Systems subject pool and received course credit for participation. Two participants experienced technical difficulties with the system, and one experienced vertigo and did not complete the task. The data from these three participants were not analyzed. Of the 30 final participants, 24 were male, and 6 were female, average age of 19.1 years. Self-reported handedness showed 3 were left-handed, 26 were right-handed and 1 ambidextrous. Before beginning the experiment, participants provided written informed consent. To ensure participants had normal color vision they completed the Snellen chart (at least 20/40 vision required) and Ishihara color plates. Each of the 30 participants completed the vision test successfully without eyeglasses.</p></section><section id="Sec7"><h3 class="pmc_sec_title">Materials</h3>
<p id="Par13">The experiment was programmed in Unity (2020.3.44f1) and presented in the HTC Vive Pro Eye VR headset (1440 × 1600 pixels per eye, 90 Hz refresh, 110 deg field of view, VIVE SRanipal SDK) with embedded Tobii eye tracking (120 Hz sampling rate, Tobii XR SDK). The experimental environment was presented using a Corsair One PC (Windows 10, Intel Core i9 CPU @ 3.6 GHz, 64-bit, Nvidia GeForce RTX2080Ti, 32 GB RAM). Two external lighthouses tracked the head, the torso using a Vive Tracker, and hand controller positioned in a ForceTube gunstock (ProTube VR) to simulate a weapon.</p></section><section id="Sec8"><h3 class="pmc_sec_title">Design and procedure</h3>
<p id="Par14">After signing informed consent and undergoing the vision tests, participants completed a demographics form and the Simulator Sickness Questionnaire (SSQ) (Kennedy et al., <a href="#CR50" class="usa-link" aria-describedby="CR50">1993</a>). A Vive tracker was then attached to the center of the chest, and the participant received general instruction on how to hold the weapon and perform the foraging task. Next participants were centered on a foam pad (2 × 3 ft) between the lighthouses. Participants could rotate their torso but were instructed to always keep their feet on the foam pad. The pad was used as it provided a proprioceptive reminder (i.e., the contrast between the foam and the floor) to participants to stay within the experimental space. Once positioned, the participant was handed the weapon controller to use in whatever hand they preferred and then the VR headset was donned and adjusted for fit and comfort. Next, participants adjusted the inter-pupil distance to ideal parameters determined by Tobii software and performed a 5-point eye-tracking calibration. After a successful calibration (determined by Tobii software), participants were given practice with the task to get familiar with the environment, the target and distractor objects, and aiming and shooting the weapon.</p>
<p id="Par15">Participants performed a foraging search task within a forested environment (Meadow Environment—Dynamic Nature Version 2.9.3 created by Nature Manufacture) by shooting targets while trying to not shoot distractors. Participants shot the weapon by depressing the trigger on the hand controller, which produced an auditory gunshot. A visual ray (red line) extended from the weapon into the environment to assist aiming. Targets were kneeling avatars holding a weapon and wearing a specific camouflage pattern (Fig. <a href="#Fig1" class="usa-link">1</a>). Targets were intermixed with distractors. Distractors were avatars also holding a weapon, in the same kneeling positions as targets, but wearing a camouflage pattern that was either easy to discriminate (Easy condition) or difficult to discriminate (Hard condition) from the target camouflage pattern. Four dominant colors were used in each avatar.<a href="#Fn1" class="usa-link">1</a> Targets and distractors shot their weapons to generate background gunshot sounds but did not aim at the participant or fire projectiles in the virtual environment. If a target or distractor was shot, it fell to the ground and ceased shooting. The environment was rendered 360 degrees; however, camouflage targets and distractors were spawned without environment occlusion, only within 180 degrees of the game origin point (i.e., forward center position). Spawn points were a random distance between 10 (4.24 × 1.72 degrees of visual angle (dva)) and 100 m (1.1 × .5 dva). Trials were completed with low time pressure (45 s in the Low condition) or high time pressure (18 s in the High condition). Hard discrimination difficulty and high time pressure levels were determined a priori through pilot testing to ensure they were challenging yet accomplishable. The angular separation between targets within the scene varied by trial with half of the trials having a target dispersion of 5 dva and the other half 10 dva. For the purposes of this study, trials were collapsed across dispersion levels, leaving a 2 × 2 within-subjects design with target discriminability (Easy vs. Hard) and time pressure (Low vs Hard) as the main factors.</p>
<figure class="fig xbox font-sm" id="Fig1"><h4 class="obj_head">Fig. 1.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373606_41235_2025_657_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4fb5/12373606/81eb0293909b/41235_2025_657_Fig1_HTML.jpg" loading="lazy" id="MO1" height="480" width="674" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Task environment. <strong>A</strong>. Screenshot of the easy condition in the foraging shooting task. The red ray extending from the weapon indicates current aiming position. <strong>B</strong>. Examples of the search target and distractors in the easy and hard conditions. C. Spatial coordinates of the VR environment and head position (+ X right, + Y up, and + Z forward, left-handed coordinate system) with corresponding yaw, pitch, and roll rotations. Eye position data were recorded in a right-handed coordinate system (+ X left, + Y up, and + Z forward)</p></figcaption></figure><p id="Par16">Each participant completed 4 blocks of 20 trials. Due to a pre-configuration error, one participant completed 6 additional trials having a target dispersion of 8 dva. These trials only occurred on the first block and were not included in the analysis. Each trial contained between 12 and 18 targets (uniformly distributed) and 12 distractors. This was implemented to prevent participants counting a fixed number of targets. Prior to each block a 5-point eye tracker calibration was performed. Each trial started with a blank background containing a plus sign + at the world forward position (0,0,1). When the gaze position remained continuously centered within 2dva and the head within 3dva of the central marker for 500 ms, the forested environment with targets and distractors was displayed. Participants were instructed to shoot all targets using the trigger button on the controller/weapon and press down on the controller track pad with their thumb when they were finished searching. If the trial time limit was reached (i.e., 45 or 18 s), the trial ended. Discriminability (as well as target dispersion) was randomized within each block with the constraint that each difficulty/dispersion trial combination was equally represented in the block. The first two blocks were low time pressure, and the last two blocks were high time pressure. Time pressure was blocked such that the first two blocks were always low pressure, and the final two blocks were high pressure. This fixed order was used to minimize carryover effects observed during pilot testing, where beginning with high pressure caused participants to adopt a sustained fast-paced search strategy that persisted into subsequent low-pressure blocks. Presenting low pressure first allowed participants to engage in more deliberate search behavior before transitioning into a more time-constrained strategy, better preserving the intended contrast between conditions. After the experiment the participants completed another SSQ as well as the three-component iGroup Presence Questionnaire (IPQ) (ScHubert, <a href="#CR77" class="usa-link" aria-describedby="CR77">2003</a>). Summary results of the SSQ and IPQ can be found in Supplementary Materials.</p></section><section id="Sec9"><h3 class="pmc_sec_title">Data acquisition</h3>
<p id="Par17">Each unity frame, eye, head, and controller samples were timestamped, recorded, and synchronized with other experimental events (e.g., trial start/stop, target/distractor labels and positions) through the Lab Streaming Layer (LSL) and Lab Recorder (Kothe et al., <a href="#CR53" class="usa-link" aria-describedby="CR53">2024</a>). LSL serves as a framework for synchronizing time-series data from various sources and sampling rates, offering a language-independent, real-time communication protocol for data recording and sharing. Due to a coding error, the Vive tracker used to track torso rotation did not record properly for any participant. Unfortunately, as a result, these data could not analyzed.</p></section><section id="Sec10"><h3 class="pmc_sec_title">Data processing</h3>
<p id="Par18">X, Y and Z coordinates from the left eye and head were used to calculate multiple metrics for the analyses (see Fig. <a href="#Fig1" class="usa-link">1</a> and Table <a href="#Tab1" class="usa-link">1</a>) (Nyström &amp; Holmqvist, <a href="#CR66" class="usa-link" aria-describedby="CR66">2010</a>; Vlaskamp et al., <a href="#CR91" class="usa-link" aria-describedby="CR91">2005</a>). Eye-tracking samples were deemed invalid if the pupil diameter was recorded as less than or equal to zero. If the time window of consecutive invalid samples was under 50 ms in duration, a linear interpolation was used to fill this gap. If the period extended longer than that, those samples, along with the neighboring 2 samples on either side of the drop-out, were marked as unusable and not included in saccade classification. Saccades and fixations were identified with a velocity-based algorithm adapted from the EYE-EEG toolbox (Dimigen et al., <a href="#CR22" class="usa-link" aria-describedby="CR22">2011</a>; Engbert &amp; Mergenthaler, <a href="#CR31" class="usa-link" aria-describedby="CR31">2006</a>). Each participant’s eye unit vector was used to calculate angular velocity. The average velocity and standard deviation in each direction (x, y, z) were calculated, and a 3-dimensional, elliptical velocity threshold was set at six standard deviations from the mean in each direction. In this way, each participant had an automated, custom saccade threshold. Saccades were rejected if they were under 12 ms in duration (Hooge et al., <a href="#CR42" class="usa-link" aria-describedby="CR42">2007</a>).</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>Eye and head features used in classification models to infer task-induced effects of target discriminability and time pressure</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Metric</th>
<th align="left" colspan="1" rowspan="1">Definition</th>
<th align="left" colspan="1" rowspan="1">Significance</th>
<th align="left" colspan="1" rowspan="1">SupportingLiterature</th>
</tr></thead>
<tbody>
<tr><td align="left" colspan="4" rowspan="1">Fixation</td></tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<p>Fixation Frequency</p>
<p>Avg Fixation Duration</p>
<p>Fixation Duration Proportion</p>
<p>Target Fixation Frequency</p>
<p>Avg Target Fixation Duration</p>
<p>Target Fixation Duration Proportion</p>
<p>Distractor Fixation Frequency</p>
<p>Avg Distractor Fixation Duration</p>
<p>Distractor Fixation Duration Proportion</p>
<p>Target Fixations per Target</p>
<p>Distractor Fixations per Distractor</p>
<p>Target Distractor Fixation Ratio</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>Total fixation count divided by trial time (sec)</p>
<p>Average fixation time (sec)</p>
<p>Sum of all fixation durations divided by trial time</p>
<p>Total target fixation count divided by trial time</p>
<p>Average target fixation time (sec)</p>
<p>Sum of all target fixation durations divided by trial time</p>
<p>Total distractor fixation count divided bytrial time</p>
<p>Average distractor fixation time (sec)</p>
<p>Sum of all distractor fixation durations divided by trial time</p>
<p>Total number of target fixations divided by total number of targets</p>
<p>Total number of distractor fixations divided by total number of distractors</p>
<p>Target fixation preference compared to other fixations</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>FixationFrequency: Reflects how often attention shifts to</p>
<p>specific visual location. Higher frequencies may indicate</p>
<p>increased visual samplingor search effort. Fixation Duration:</p>
<p>Longer fixations typically reflect deeper processing, uncertainty,</p>
<p>or hesitation; shorter durations may indicate more confident or</p>
<p>automatic responses. Fixation Duration Proportion: Indicates</p>
<p>how much of the trial is spent fixating; higher values suggest</p>
<p>greter engagment or deliberation. Target vs. Distractor</p>
<p>Fixations: Target-focused metrics reflect goal-directed</p>
<p>attention while distractor-focused metrics may indicate</p>
<p>inefficient search or difficulty in discrimination</p>
</td>
<td align="left" colspan="1" rowspan="1">Drew et al., (<a href="#CR25" class="usa-link" aria-describedby="CR25">2017</a>); Over et al., (<a href="#CR42" class="usa-link" aria-describedby="CR42">2007</a>); Reingold and Galhot, (<a href="#CR74" class="usa-link" aria-describedby="CR74">2014</a>); Lu and Sarter, (<a href="#CR97" class="usa-link" aria-describedby="CR97">2019</a>); Huanget al., (<a href="#CR45" class="usa-link" aria-describedby="CR45">2015</a>); Karessli et al., (<a href="#CR49" class="usa-link" aria-describedby="CR49">2017</a>); Iskandarani et al., (<a href="#CR29" class="usa-link" aria-describedby="CR29">2024</a>)</td>
</tr>
<tr><td align="left" colspan="4" rowspan="1">Saccade</td></tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<p>Saccade Frequency</p>
<p>AvgSaccade Size</p>
<p>Saccade Size Per Second</p>
<p>AvgTarget Saccade Size</p>
<p>AvgDistractor Saccade Size</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>Total saccade count divided bytrial time (sacades/s)</p>
<p>Average size of a saccade(dva)</p>
<p>Sum of all saccade magnitudes divided bytrial time (°/s)</p>
<p>Average size of saccade to a target (dva)</p>
<p>Average size of saccade to a distractor (dva)</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>Saccade Frequency: Higher frequencymayreflect rapid</p>
<p>scanningor uncertaintyin searchstrategy. Saccade Size and</p>
<p>Size per Second: Larger or more frequent saccades may</p>
<p>suggest a global search strategywhile smaller saccades may</p>
<p>indicate more focused, precisesearch behavior. Target vs</p>
<p>Distractor Saccade Size: Differentiates the visual distance the</p>
<p>eye travels totask-relevant vs. irrelevant stimuli, potentially</p>
<p>reflectingdiscrimination efficiency</p>
</td>
<td align="left" colspan="1" rowspan="1">Watson et al., (<a href="#CR98" class="usa-link" aria-describedby="CR98">2010</a>); Over et al., (<a href="#CR42" class="usa-link" aria-describedby="CR42">2007</a>); Di Stasi et al., (<a href="#CR21" class="usa-link" aria-describedby="CR21">2011</a>); Velichkovski et al., (<a href="#CR88" class="usa-link" aria-describedby="CR88">2002</a>); Gordonet al., (<a href="#CR38" class="usa-link" aria-describedby="CR38">2024</a>)</td>
</tr>
<tr><td align="left" colspan="4" rowspan="1">Head</td></tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<p>Head Movement Frequency</p>
<p>AvgHead Movement Size</p>
<p>Head Movement Size Per Second</p>
<p>AvgHead Fixation Duration</p>
<p>Head Fixation Duration Proportion</p>
<p>Head Fixation Frequency</p>
<p>Global Head Movement Per Second</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>Total head movement count divided bytrial time</p>
<p>Average size of a head movement (degrees of visual angle)</p>
<p>Sum of all head movement magnitudes divided bytrial time (°/s)</p>
<p>Average time the head remains below the threshold of a head movement (sec)</p>
<p>Sum of all head fixation durations divided bytrial time</p>
<p>Total head fixation count divided bytrial time</p>
<p>Sum of the angular distancethe head moved dividedbytrial time Independent of</p>
<p>head movement classification</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>Head Movement Frequencyand Size: Capturehow often and</p>
<p>how far users reorient their field of view; higher values may</p>
<p>relfect broader or more effortful searchstrategies. Head</p>
<p>FixationDurationand Proportion: Longer or more frequent</p>
<p>headfixations mayreflect more focused attention or reduced</p>
<p>exploratorybehavior. Global Head Movement: Indicates spatial</p>
<p>engagement withthe scene</p>
</td>
<td align="left" colspan="1" rowspan="1">Hu et al., (<a href="#CR44" class="usa-link" aria-describedby="CR44">2023</a>); David et al., (<a href="#CR19" class="usa-link" aria-describedby="CR19">2018</a>); Fuller, (<a href="#CR34" class="usa-link" aria-describedby="CR34">1992</a>); Stahl, (<a href="#CR82" class="usa-link" aria-describedby="CR82">1999</a>); Oommen and Stahl, (<a href="#CR67" class="usa-link" aria-describedby="CR67">2005</a>); Freedman and Sparks, (<a href="#CR33" class="usa-link" aria-describedby="CR33">1997</a>)</td>
</tr>
<tr><td align="left" colspan="4" rowspan="1">Eye</td></tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<p>AvgEye in Head Angle</p>
<p>Global Eye Movement Per Second</p>
<p>AvgEye to Gaze Proportion</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>Average rotation of the eye within the head (dva)</p>
<p>Sum of the angular distancethe eye moved divided bytrial time. Independent of saccade classification</p>
<p>Average proportion of eye contribution to gazeangle. Gaze Angle = Eye Angle + Head Angle</p>
</td>
<td align="left" colspan="1" rowspan="1">Eye inHead Angle: Larger anglesmayreflect relianceon eye movements rather than head shifts. Eye-to-Gaze Proportion: important for undertandingsearch strategypreference (e.g."eye movers"vs."head movers")</td>
<td align="left" colspan="1" rowspan="1">Sidenmark &amp; Gellersen, (<a href="#CR79" class="usa-link" aria-describedby="CR79">2019</a>, <a href="#CR80" class="usa-link" aria-describedby="CR80">2023</a>); Stahl, (<a href="#CR83" class="usa-link" aria-describedby="CR83">2001a</a>, <a href="#CR84" class="usa-link" aria-describedby="CR84">2001b</a>); Freedman, (<a href="#CR32" class="usa-link" aria-describedby="CR32">2008</a>); Zangemeister and Stark, (<a href="#CR96" class="usa-link" aria-describedby="CR96">1982</a>); Doshi and Trividi, (<a href="#CR23" class="usa-link" aria-describedby="CR23">2012</a>)</td>
</tr>
<tr><td align="left" colspan="4" rowspan="1">Kinematic</td></tr>
<tr>
<td align="left" colspan="1" rowspan="1">Head</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1"></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<p>AvgHead Velocity</p>
<p>Std of Head Velocity</p>
<p>AvgHead VelocityX,Y,Z</p>
<p>Std of Head VelocityX,Y,Z</p>
<p>AvgHead Acceleration</p>
<p>Std of Head Acceleration</p>
<p>AvgHead Acceleratiaround the X,Y,Z</p>
<p>Std of Head Acceleratiaround the X,Y,Z</p>
<p>AvgHead Jerk</p>
<p>Std of Head Jerk</p>
<p>AvgHead Jerk X,Y,Z</p>
<p>Std of Head Jerk X,Y,Z</p>
<p>Eye</p>
<p>AvgEye Velocity</p>
<p>Std of Eye Velocity</p>
<p>AvgEye VelocityX,Y,Z</p>
<p>Std Eye VelocityX,Y,Z</p>
<p>AvgEye Acceleration</p>
<p>Std of Eye Acceleration</p>
<p>AvgEye Acceleratiaround the X,Y,Z</p>
<p>Std Eye Acceleratiaround the X,Y,Z</p>
<p>AvgEye Jerk</p>
<p>Std of Eye Jerk</p>
<p>AvgEye Jerk X,Y,Z</p>
<p>Std Eye Jerk X,Y,Z</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>Average headvelocity(°/s)</p>
<p>Standard deviation of head velocity(°/s)</p>
<p>Average headvelocityaround the X, Y, and Zaxes (°/s)</p>
<p>Standard deviation of head velocityaround the X, Y, and Zaxes(°/s)</p>
<p>Average headacceleration (°/s<sup>2</sup>)</p>
<p>Standard deviation of head acceleration(°/s<sup>2</sup>)</p>
<p>Average headacceleration around the X, Y, and Zaxes(°/s<sup>2</sup>)</p>
<p>Standard deviation of head acceleration around the X, Y, and Zaxes (°/s<sup>2</sup>)</p>
<p>Average headjerk (°/s<sup>3</sup>)</p>
<p>Standard deviation of head jerk (°/s<sup>3</sup>)</p>
<p>Average headjerk around the X, Y, and Zaxes (°/s<sup>3</sup>)</p>
<p>Standard deviation of head jerk around the X, Y, and Zaxes(°/s<sup>3</sup>)</p>
<p>Average eye velocity(°/s)</p>
<p>Standard deviation of eye velocity(°/s)</p>
<p>Average eye velocityaroundthe X, Y, and Zaxes (°/s)</p>
<p>Standard deviation eye velocityaround the X, Y, and Zaxes(°/s)</p>
<p>Average eye acceleration (°/s<sup>2</sup>)</p>
<p>Standard deviation of eye acceleration (°/s<sup>2</sup>)</p>
<p>Average eye acceleration around the X, Y, and Zaxes (°/s<sup>2</sup>)</p>
<p>Standard deviation eye acceleration around the X, Y, and Zaxes (°/s<sup>2</sup>)</p>
<p>Average eye jerk (°/s<sup>3</sup>)</p>
<p>Standard deviation of eye jerk (°/s<sup>3</sup>)</p>
<p>Average eye jerk around the X, Y, and Zaxes (°/s<sup>3</sup>)</p>
<p>Standard deviation eye jerk around the X, Y, and Zaxes(°/s<sup>3</sup>)</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>Velocity: Indicates speed of eye or head movement; higher</p>
<p>values mayreflect urgencyor less deliberate behavior</p>
<p>Acceleration: Measures the rate of movement change. Jerk:</p>
<p>Captures abruptnessor smoothness of movement; higher jerk</p>
<p>suggestsmore reactiveor less controlled movements</p>
<p>Variability(Standard Deviation): Grater variabilityacross</p>
<p>velocity, acceleration, or jerkmayreflect lessstabel or more</p>
<p>exploratorysearch strategies</p>
</td>
<td align="left" colspan="1" rowspan="1">Di Stasi et al., (<a href="#CR21" class="usa-link" aria-describedby="CR21">2011</a>); Salehi et al., (<a href="#CR76" class="usa-link" aria-describedby="CR76">2024</a>); Hu et al., (<a href="#CR44" class="usa-link" aria-describedby="CR44">2023</a>); Lisberger et al., (<a href="#CR59" class="usa-link" aria-describedby="CR59">1981</a>)</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par19">To exclude biologically implausible saccades, an additional set of thresholds was applied. Specifically, saccades longer than 120 ms or with peak velocities below 25 deg/sec or above 1000 deg/sec were removed (Enders et al., <a href="#CR30" class="usa-link" aria-describedby="CR30">2021</a>; Holmqvist et al., <a href="#CR41" class="usa-link" aria-describedby="CR41">2011</a>; Long et al., <a href="#CR60" class="usa-link" aria-describedby="CR60">2024</a>). Saccades and head movements with amplitudes ≤ 1 deg and fixations (eye and head) shorter than 75 ms were also excluded from the analysis (Hooge et al., <a href="#CR43" class="usa-link" aria-describedby="CR43">2022</a>). Saccade and fixation-based metrics were calculated after removing these outlier events.</p>
<p id="Par20">To classify head movements, we applied an adapted algorithm from Chen and Walton (<a href="#CR14" class="usa-link" aria-describedby="CR14">2005</a>), originally designed to classify head movements in macaques but shown to be highly reliable when classifying human head movements (Callahan-Flintoft et al., <a href="#CR13" class="usa-link" aria-describedby="CR13">2024</a>). Head movements were detected by applying a sliding 100 ms window to the head’s angular speed. Head motion onset began when at least 72% of data points within this window exceeded a threshold of 6°/s, with fewer than three consecutive time points falling below the threshold. Head movement onset was recorded as the first time point in the window exceeding the threshold. Similarly, motion offset was determined using a 22 ms window, where 72% of data points were below the threshold. The exact head movement offset time was defined as the first time point within this qualifying window that dropped below the threshold. Head ‘fixations’ were periods between classified head movements where the head sustained a low angular speed and remained relatively stable.</p></section><section id="Sec11"><h3 class="pmc_sec_title">Metrics calculation</h3>
<p id="Par21">Trial completion time, average inter-target engagement time and d-prime were evaluated to assess behavioral performance. Target completion time was measured from scene onset to when the participant pressed the trial end button indicating search was complete or when time expired, whichever came first. Inter-target time was the average time taken between successive target hits. D-prime was used to distinguish between signal and noise by quantifying the difference in hit and false alarm rates. This provided a better measure of sensitivity to the differences between targets and distractors while accounting for potential response bias when compared to target accuracy alone. Analyses were performed in R (RStudio 2023.12.1 + 402"Ocean Storm"Release).</p>
<p id="Par22">Multiple metrics were computed using the rotational data of eye and head movements along the X (pitch), Y (yaw), and Z (roll) axes, with many derived across both effectors. We focused on core temporal and spatial characteristics of movement, including frequency, amplitude, and duration, features commonly used to characterize attentional engagement and search strategy in visually demanding tasks. For example, fixation duration and frequency have been linked to visual processing load and search efficiency (Drew et al., <a href="#CR25" class="usa-link" aria-describedby="CR25">2017</a>; Reingold &amp; Glaholt, <a href="#CR74" class="usa-link" aria-describedby="CR74">2014</a>). Saccadic peak velocity has been proposed as an index for detecting variations in mental workload (Di Stasi et al., <a href="#CR21" class="usa-link" aria-describedby="CR21">2011</a>), and saccade size has been shown to predict the onset time of object processing during visual search tasks (Gordon et al., <a href="#CR38" class="usa-link" aria-describedby="CR38">2024</a>). Similarly, head movement amplitude and frequency reflect spatial reorientation and can signal increased search demands or target uncertainty (David et al., <a href="#CR19" class="usa-link" aria-describedby="CR19">2018</a>; Freedman &amp; Sparks, <a href="#CR33" class="usa-link" aria-describedby="CR33">1997</a>; Oommen &amp; Stahl, <a href="#CR67" class="usa-link" aria-describedby="CR67">2005</a>). We also incorporated gaze and head kinematics which have shown promise in identifying cognitive state changes and user experience during complex VR tasks (El Iskandarani et al., <a href="#CR29" class="usa-link" aria-describedby="CR29">2024</a>; Hu et al., <a href="#CR44" class="usa-link" aria-describedby="CR44">2023</a>).</p>
<p id="Par23">To capture the dynamics of eye and head movements more comprehensively, we calculated kinematic features based on velocity, acceleration, and jerk (Culmer et al., <a href="#CR18" class="usa-link" aria-describedby="CR18">2009</a>; Di Stasi et al., <a href="#CR21" class="usa-link" aria-describedby="CR21">2011</a>; Hu et al., <a href="#CR44" class="usa-link" aria-describedby="CR44">2023</a>; Kothari et al., <a href="#CR52" class="usa-link" aria-describedby="CR52">2020</a>; Lisberger et al., <a href="#CR59" class="usa-link" aria-describedby="CR59">1981</a>; Plamondon, <a href="#CR71" class="usa-link" aria-describedby="CR71">1995</a>; Salehi et al., <a href="#CR76" class="usa-link" aria-describedby="CR76">2024</a>). Velocity refers to the rate of change of angular position over time. It provides insight into how quickly the eyes and head move during gaze shifts, reflecting the speed of orienting behavior. Velocity was computed as the first derivative of the angular position with respect to time for each axis. Acceleration is the rate of change of velocity over time. It indicates how rapidly the speed of eye and head movements increases or decreases, offering information about the smoothness and control of movements. Acceleration was calculated as the first derivative of velocity or the second derivative of angular position. Jerk represents the rate of change of acceleration over time. It captures the abruptness or smoothness of movements, with higher jerk values indicating more abrupt changes. Jerk is particularly relevant for detecting sudden adjustments in gaze direction. Jerk was computed as the first derivative of acceleration or the third derivative of angular position.</p>
<p id="Par24">Eye-specific features were also considered, such as the average eye-in-head angle and the proportion of eye contribution to overall gaze, where gaze is defined as the sum of eye and head angles (Doshi &amp; Trivedi, <a href="#CR23" class="usa-link" aria-describedby="CR23">2012</a>; Freedman, <a href="#CR32" class="usa-link" aria-describedby="CR32">2008</a>; Sidenmark &amp; Gellersen, <a href="#CR79" class="usa-link" aria-describedby="CR79">2019</a>; Sidenmark et al., <a href="#CR80" class="usa-link" aria-describedby="CR80">2023</a>; Stahl, <a href="#CR82" class="usa-link" aria-describedby="CR82">1999</a>, <a href="#CR84" class="usa-link" aria-describedby="CR84">2001b</a>; Zangemeister &amp; Stark, <a href="#CR96" class="usa-link" aria-describedby="CR96">1982</a>). The proportion of eye contribution indicates the relative reliance on eye versus head movements during gaze shifts.</p>
<p id="Par25">To glean insights into differences in information processing, separate metrics for target and distractor processing were also calculated, such as fixation frequency, duration on targets and distractors, and target-distractor fixation ratio. These metrics were exclusively available for eye measurements as head movements lack the same spatial granularity.</p>
<p id="Par26">Where applicable, metrics were normalized by trial duration to account for differences in trial lengths. Each metric was calculated separately for each trial. See Table <a href="#Tab1" class="usa-link">1</a> for the complete list and descriptions of all computed metrics. Data files and processing scripts will be made available upon reasonable request.</p></section><section id="Sec12"><h3 class="pmc_sec_title">Machine learning classification</h3>
<p id="Par27">The Boruta algorithm, a multivariant wrapper feature selection method (Kursa et al., <a href="#CR57" class="usa-link" aria-describedby="CR57">2010</a>), was used to assess feature importance prior to building classification models. Boruta identifies the most relevant features in a model-agnostic manner by comparing the importance of actual features to shuffled versions of the same data, referred to as"shadow features."A random forest model was run for 50 iterations, and the importance of each original feature was compared to the maximum importance of the shadow features in each iteration. At the end of the iterations, a binomial distribution (α = 0.05) was used to determine whether each feature was significantly important. By applying Boruta prior to running machine learning pipelines, we eliminated redundant and irrelevant features. Using only the important and independent features to train models reduces noise, helps avoid overfitting issues, and improves the human interpretability of model features (Dwivedi et al., <a href="#CR27" class="usa-link" aria-describedby="CR27">2024</a>).</p>
<p id="Par28">The subset of important features from Boruta were incorporated into a machine learning pipeline in Python using sklearn (Pedregosa et al., <a href="#CR68" class="usa-link" aria-describedby="CR68">2011</a>) consisting of Standard Scaling and a support vector classifier (SVC) with a radial basis function kernel. Model validation was performed via a tenfold cross-validation method, which divides the data into 10 equal portions with roughly equivalent class distribution. In each fold, 9 of the portions are used to train the pipeline, and the last is used to test the model using classification accuracy as the primary evaluation metric.</p>
<p id="Par29">To compare model performance, nonparametric Wilcoxon tests were conducted using the SciPy library (Virtanen et al., <a href="#CR90" class="usa-link" aria-describedby="CR90">2020</a>). The Bonferroni–Holm modifier was applied to avoid false positives, and effect sizes were calculated using Hedges’ <em>g.</em></p>
<p id="Par30">While Boruta provides a binary indication of whether a feature is significant or not, it does not provide the importance ranking of these features on model predictions. Feature importance within the models was performed using Shapley additive explanations (SHAP) Python library (Scott &amp; Su-In, <a href="#CR78" class="usa-link" aria-describedby="CR78">2017</a>). This provided a ranking of the features selected through Boruta. As in prior research, we selected a 20% representative subset of samples to train SHAP (Christoph, <a href="#CR16" class="usa-link" aria-describedby="CR16">2020</a>).</p>
<p id="Par31">To investigate the contribution of both head and eye movements to model performance, as well as to tease apart how task parameter changes might affect movement systems differently, the following models were run. First, a 4-way classification omnibus model was run using all eye and head features designated as significantly important (Eye and Head). This model labeled each trial based on predicted target discriminability (Easy vs Hard) and time pressure (Low vs High), such that any given trial could be classified as Easy-Low, Easy-High, Hard-Low, or Hard-High. Next, a 4-way classification (with the same labels listed above) model was fit using only features derived from eye-tracking data (Eye Only) and another model was fit using only features derived from head-tracking data (Head Only). Finally, to assess whether eye and head features differ in their ability to infer changes in search parameters two separate 2-way classification models were run for each feature subset, focusing separately on target discriminability and time pressure.</p></section></section><section id="Sec13"><h2 class="pmc_sec_title">Results</h2>
<section id="Sec14"><h3 class="pmc_sec_title">Eye and head movement summary</h3>
<p id="Par32">To demonstrate data quality, Fig. <a href="#Fig2" class="usa-link">2</a> provides a quantitative summary of eye and head movement characteristics. In the left column, the distribution of saccade and head movement amplitudes reveals a bimodal pattern for saccades, with average saccade amplitudes exceeding those of head movements. The second column illustrates main sequence plots for both eye and head movements, showing a clear velocity–amplitude relationship typical of these effectors (Bahill et al., <a href="#CR4" class="usa-link" aria-describedby="CR4">1975</a>; Zangemeister et al., <a href="#CR95" class="usa-link" aria-describedby="CR95">1981</a>). The fixation duration distributions display similar patterns for eye and head, though head fixations were generally longer on average than eye fixations. Angular distributions in the top right and bottom right plots show that eye position, relative to the head, remains within approximately 15 degrees of head center, while head orientation is predominantly distributed along the horizontal plane.</p>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373606_41235_2025_657_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4fb5/12373606/2a2036b23260/41235_2025_657_Fig2_HTML.jpg" loading="lazy" id="MO2" height="325" width="709" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Eye (top row) and head movement (bottom row) summary from 20,000 randomly selected trial events across all participants. Left column—Saccade (top) and head movement (bottom) amplitude distribution. Second column from left—Main sequence plots for saccades (top) and head movements (bottom), illustrating the relationship between movement amplitude and peak velocity. Third column from left—distribution of eye and head fixation durations. Right column—heat map showing the angular distribution of eye in head (top) and head in the world (bottom). deg = degrees; ms = milliseconds</p></figcaption></figure></section><section id="Sec15"><h3 class="pmc_sec_title">Behavioral performance</h3>
<p id="Par33">Dependent measures were analyzed using a random effects repeated measures analysis of variance (ANOVA) with the ‘ezANOVA'R package. The within-subjects factors were target discriminability (Easy, Hard) and time pressure (Low, High) with participant as a random effect (Fig. <a href="#Fig3" class="usa-link">3</a><a href="#Fn2" class="usa-link">2</a>). Trial completion analysis revealed a significant main effect for target discriminability F(1,29) = 155.8, <em>p</em> &lt;.001, η<sup>2</sup><sub>g</sub> =.11, with easy discrimination trials completed faster than hard. The main effect of time pressure was also significant F(1,29) = 65.2, <em>p</em> &lt;.001, η<sup>2</sup><sub>g</sub> =.42 with high time pressure trials completed faster than low time pressure. The discrimination by time pressure interaction was significant F(1,29) = 96.5, <em>p</em> &lt;.001, η<sup>2</sup><sub>g</sub> =.04 indicating an increased impact of time pressure in the difficult with respect to easy discrimination trials. Similar findings were found for inter-target time which showed main effects for target discriminability F(1,29) = 227.22, <em>p</em> &lt;.001, η<sup>2</sup><sub>g</sub> =.33 and time pressure F(1,29) = 97.88, <em>p</em> &lt;.001, η<sup>2</sup><sub>g</sub> =.39 indicating the average time between target engagement was faster for easy discrimination and high time pressure targets relative to hard discrimination and low time pressure targets. The interaction was significant F(1,29) = 40.9, <em>p</em> &lt;.001, η<sup>2</sup><sub>g</sub> =.06 suggesting a larger effect of time pressure in the hard discrimination condition. The d-prime analysis revealed a main effect for target discriminability F(1,29) = 43.5, <em>p</em> &lt;.001, η<sup>2</sup><sub>g</sub> =.17 showing higher d-prime in the easy compared to hard discrimination condition. However, the main effect for time pressure and the interaction between discrimination difficulty and time pressure were not significant (<em>p</em> &gt;.05). Together these results show that target discriminability and time pressure significantly influenced behavioral performance, with faster completion and inter-target times and higher d-prime under easier conditions.</p>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373606_41235_2025_657_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4fb5/12373606/18a61130b9ee/41235_2025_657_Fig3_HTML.jpg" loading="lazy" id="MO3" height="255" width="708" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Behavioral performance. Target discriminability and time pressure had a significant main an interactive effect on trial completion time (left) and inter-target engagement time (middle). Target discriminability significantly impacted d-prime (right)</p></figcaption></figure><p id="Par34">Behavioral data indicated that 38% of high time pressure trials reached the 18-s time limit, suggesting the trial ended automatically rather than by participant decision. However, this does not necessarily imply insufficient time to complete the task as many participants still achieved 100% hit rate, even at the maximum duration, indicating that they may have completed a first pass and continued scanning or verifying targets. Similarly, there was a wide range of hit rates in trials that ended earlier, suggesting that participants often had time remaining but chose to terminate. Notably, some of the lowest hit percentages occurred in trials participants ended on their own, further implying that time constraints were not the sole limiting factor. In contrast, only 3% of low time pressure trials ended due to the time limit, while 33% extended beyond 18 s, reinforcing the impact of time pressure on search behavior.</p></section><section id="Sec16"><h3 class="pmc_sec_title">Classification</h3>
<p id="Par35">The Boruta algorithm identified the most significant features from each feature set, and these were used in the 4-way classification (Multiclass model; Fig. <a href="#Fig4" class="usa-link">4</a>). Model performance when using 49 of the 76 combined eye and head features was significantly better than random chance <em>(M</em> = 0.66, <em>SD</em> = 0.03, <em>p</em> = <em>.002 g</em> = <em>14.15).</em> Similarly, when using 39 out of 45 Eye Only features, the model achieved significant classification above chance (<em>M</em> = 0.64, <em>SD</em> = 0.03, <em>p</em> = <em>.002, g</em> = <em>7.70</em>) and 17 of 31 Head Only features, also performed significantly above chance in the 4-way classification model (<em>M</em> = 0.39, <em>SD</em> = 0.02, <em>p</em> = <em>.002</em>, <em>g</em> = <em>6.77</em>). Notably, incorporating head features with eye features significantly improved classification accuracy relative to Eye Only features (<em>p</em> = <em>0.03, g</em> = <em>0.93)</em> highlighting the modest but complementary value of combining head movement features with eye movement data. Eye Only features significantly outperformed Head Only features (<em>p</em> = 0.019, <em>g</em> = 5.61).</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373606_41235_2025_657_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4fb5/12373606/ff0e55d0443a/41235_2025_657_Fig4_HTML.jpg" loading="lazy" id="MO4" height="248" width="709" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Multiclass model accuracy. Significant detection of task-induced effects of target discriminability and time pressure was achieved across all feature sets, with combined eye and head features having the highest model accuracy</p></figcaption></figure><p id="Par36">The top 10 features, ranked in order of importance, for each model are included in Table <a href="#Tab2" class="usa-link">2</a>.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Top 10 features and mean accuracy for each model</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" colspan="4" rowspan="1">4-Way Classification</th>
<th align="left" colspan="2" rowspan="1">Target Discriminability</th>
<th align="left" colspan="2" rowspan="1">Time Pressure</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">Feature Rank</th>
<th align="left" colspan="1" rowspan="1">Eye &amp;Head M = 0.66</th>
<th align="left" colspan="1" rowspan="1">Eye M = 0.63</th>
<th align="left" colspan="1" rowspan="1">Head M = 0.39</th>
<th align="left" colspan="1" rowspan="1">Eye M = 0.85</th>
<th align="left" colspan="1" rowspan="1">Head M = 0.65</th>
<th align="left" colspan="1" rowspan="1">Eye M = 0.77</th>
<th align="left" colspan="1" rowspan="1">Head M = 0.64</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>1</strong></td>
<td align="left" colspan="1" rowspan="1">Target Fixations per Target</td>
<td align="left" colspan="1" rowspan="1">Target Fixations per Target</td>
<td align="left" colspan="1" rowspan="1">Global Head Movement per Sec</td>
<td align="left" colspan="1" rowspan="1">Distractor Fixation Frequency</td>
<td align="left" colspan="1" rowspan="1">Global Head Movement per Sec</td>
<td align="left" colspan="1" rowspan="1">Target Fixations per Target</td>
<td align="left" colspan="1" rowspan="1">Avg Head Acceleration X</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>2</strong></td>
<td align="left" colspan="1" rowspan="1">Distractor Fixations per Distractor</td>
<td align="left" colspan="1" rowspan="1">Distractor Fixations per Distractor</td>
<td align="left" colspan="1" rowspan="1">Std Head Velocity X</td>
<td align="left" colspan="1" rowspan="1">Distractor Fixation Frequency</td>
<td align="left" colspan="1" rowspan="1">Avg Head Velocity Z</td>
<td align="left" colspan="1" rowspan="1">Target Fixation Frequency</td>
<td align="left" colspan="1" rowspan="1">Std Head Velocity Z</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>3</strong></td>
<td align="left" colspan="1" rowspan="1">Saccade Frequency</td>
<td align="left" colspan="1" rowspan="1">Saccade Frequency</td>
<td align="left" colspan="1" rowspan="1">Avg Head Velocity X</td>
<td align="left" colspan="1" rowspan="1">Distractor Fixations per Distractor</td>
<td align="left" colspan="1" rowspan="1">Avg Head Fixation Duration</td>
<td align="left" colspan="1" rowspan="1">Avg Target Fixation Duration</td>
<td align="left" colspan="1" rowspan="1">Std Head Velocity X</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>4</strong></td>
<td align="left" colspan="1" rowspan="1">Distractor Fixation Duration Proportion</td>
<td align="left" colspan="1" rowspan="1">Distractor Fixation Duration Proportion</td>
<td align="left" colspan="1" rowspan="1">Avg Head Acceleration X</td>
<td align="left" colspan="1" rowspan="1">Target Fixations per Target</td>
<td align="left" colspan="1" rowspan="1">Avg Head Jerk</td>
<td align="left" colspan="1" rowspan="1">Saccade Size per Sec</td>
<td align="left" colspan="1" rowspan="1">Head Amplitude per Sec</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>5</strong></td>
<td align="left" colspan="1" rowspan="1">Distractor Fixation Frequency</td>
<td align="left" colspan="1" rowspan="1">Target Fixation Frequency</td>
<td align="left" colspan="1" rowspan="1">Head Amplitude per Sec</td>
<td align="left" colspan="1" rowspan="1">Target Fixation Duration Proportion</td>
<td align="left" colspan="1" rowspan="1">Head Movement Frequency</td>
<td align="left" colspan="1" rowspan="1">Saccade Frequency</td>
<td align="left" colspan="1" rowspan="1">Std Head Jerk X</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>6</strong></td>
<td align="left" colspan="1" rowspan="1">Target Fixation Frequency</td>
<td align="left" colspan="1" rowspan="1">Distractor Fixation Frequency</td>
<td align="left" colspan="1" rowspan="1">Head Fixation Frequency</td>
<td align="left" colspan="1" rowspan="1">Fixation Duration Proportion</td>
<td align="left" colspan="1" rowspan="1">Head Fixation Duration Proportion</td>
<td align="left" colspan="1" rowspan="1">Avg Eye Acceleration X</td>
<td align="left" colspan="1" rowspan="1">Avg head Fixation Duration</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>7</strong></td>
<td align="left" colspan="1" rowspan="1">Fixation Duration Proportion</td>
<td align="left" colspan="1" rowspan="1">Target Fixation Duration Proportion</td>
<td align="left" colspan="1" rowspan="1">Avg Head Fixation Duration</td>
<td align="left" colspan="1" rowspan="1">Avg Eye Velocity X</td>
<td align="left" colspan="1" rowspan="1">Std Head Velocity X</td>
<td align="left" colspan="1" rowspan="1">Target Fixation Duration Proportion</td>
<td align="left" colspan="1" rowspan="1">Head Fixation Frequency</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>8</strong></td>
<td align="left" colspan="1" rowspan="1">
<p>Target Fixation Duration</p>
<p>Proportion</p>
</td>
<td align="left" colspan="1" rowspan="1">Fixation Duration Proportion</td>
<td align="left" colspan="1" rowspan="1">Avg Head Jerk</td>
<td align="left" colspan="1" rowspan="1">Avg Eye Jerk Y</td>
<td align="left" colspan="1" rowspan="1">Avg Head Velocity</td>
<td align="left" colspan="1" rowspan="1">Avg Eye Jerk X</td>
<td align="left" colspan="1" rowspan="1">Avg Head Jerk X</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>9</strong></td>
<td align="left" colspan="1" rowspan="1">Avg Eye in Head Angle</td>
<td align="left" colspan="1" rowspan="1">Avg Eye in Head Angle</td>
<td align="left" colspan="1" rowspan="1">Avg Head Velocity X</td>
<td align="left" colspan="1" rowspan="1">Avg Eye Acceleration Y</td>
<td align="left" colspan="1" rowspan="1">Avg Head Velocity X</td>
<td align="left" colspan="1" rowspan="1">Distractor Fixations per Distractor</td>
<td align="left" colspan="1" rowspan="1">Avg Head Jerk</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>10</strong></td>
<td align="left" colspan="1" rowspan="1">Saccade Size per Sec</td>
<td align="left" colspan="1" rowspan="1">Avg Eye Acceleration X</td>
<td align="left" colspan="1" rowspan="1">Avg Head Jerk X</td>
<td align="left" colspan="1" rowspan="1">Avg Saccade Size</td>
<td align="left" colspan="1" rowspan="1">Avg Head Movement Size</td>
<td align="left" colspan="1" rowspan="1">Avg Eye Velocity</td>
<td align="left" colspan="1" rowspan="1">Std Head Velocity Y</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p184"><p>Avg = Average; Std = Standard Deviation; Sec = Second; See Table <a href="#Tab1" class="usa-link">1</a> for feature definitions</p></div></div></section><p id="Par37">Target discriminability and time pressure were modeled separately using either Eye Only or Head Only features, and all models performed significantly above chance (Fig. <a href="#Fig5" class="usa-link">5</a>). The target discriminability model trained on Eye Only features achieved a mean accuracy of 0.85 (SD = 0.03, <em>p</em> = 0.002, g = 7.41), utilizing 34 of the 45 available eye features. In comparison, the time pressure model using Eye Only features reached a mean accuracy of 0.77 (SD = 0.02, <em>p</em> = 0.002, g = 8.57), with 35 of 45 features selected. Similarly, Head Only feature models also exceeded chance performance: The target discriminability model attained a mean accuracy of 0.64 (SD = 0.04, <em>p</em> = 0.001, g = 1.93) using 12 of the 31 available head features, while the time pressure model achieved 0.65 (SD = 0.04, <em>p</em> = 0.002, g = 2.48) with 20 of the 31 features. Eye movement features were better at classifying the target discriminability of a trial (<em>M</em> = 0.85, <em>SD</em> = 0.03) compared to the time pressure (<em>M</em> = 0.77, <em>SD</em> = 0.02), <em>p</em> = 0.002, <em>g</em> = 2.46. When using only head movement features, classification accuracies between target discriminability (M = 0.64, SD = 0.04) and time pressure (M = 0.65, SD = 0.04) were not statistically significant (<em>p</em> &gt; 0.5).</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373606_41235_2025_657_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4fb5/12373606/809c6d4c4795/41235_2025_657_Fig5_HTML.jpg" loading="lazy" id="MO5" height="183" width="709" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Classification accuracy for the target discriminability and time pressure models from Eye Only and Head Only features</p></figcaption></figure></section></section><section id="Sec17"><h2 class="pmc_sec_title">Discussion</h2>
<p id="Par38">The primary objective of this study was to investigate how task parameters—target discriminability and time pressure—affects foraging behavior and whether eye and head movement metrics can be used to reliably infer these task-induced changes. By employing a naturalistic VR foraging task, we aimed to examine the extent to which these movement systems contribute independently and collectively to adaptive search strategies under varying task conditions.</p>
<p id="Par39">Behavioral results indicated that both target discriminability and time pressure significantly influenced search performance. Consistent with prior research, trials with easily discriminable targets from distractors (Easy condition) led to faster trial completion times, shorter inter-target times, and higher d-prime scores, indicating more efficient and selective search behavior (Kristjánsson et al., <a href="#CR56" class="usa-link" aria-describedby="CR56">2018</a>; Tagu &amp; Kristjánsson, <a href="#CR85" class="usa-link" aria-describedby="CR85">2022</a>). In contrast, the hard condition was associated with slower response times and reduced d-prime, highlighting the greater perceptual demands required when targets were more visually similar to distractors. Time pressure also had a significant impact, with high-pressure trials showing faster completion and inter-target times particularly under harder discriminability. This interaction between difficulty and time pressure reflects the competing demands of speed and perceptual decision-making during visual search. Participants may shift toward a strategy that prioritizes speed over accuracy, suggesting a potential tradeoff between rapid decision-making and visual discrimination during search.</p>
<p id="Par40">While it is well known that changes in task parameters, as manipulated here, produce changes in behavior, these metrics are typically reported as descriptive statistics in post hoc analyses, after averaging across multiple trials. To provide adaptive support, any intelligent system will need to use moment-to-moment human behavior to infer the given task and environmental parameters. As an intermediate step toward such a system, the current work used a machine learning approach to classify the target discriminability and time pressure level of individual trials. Our classification approach demonstrated that eye and head movement features can be used to accurately detect the task conditions that shape search behavior. As expected, eye features alone achieved high classification accuracy. However, when combined with head movement features, classification accuracy improved significantly. By identifying which metrics carry predictive value across trials, this work establishes a foundational feature space that could be translated into finer temporal windows in future work.</p>
<section id="Sec18"><h3 class="pmc_sec_title">The feasibility of using head tracking to infer task parameters</h3>
<p id="Par41">Modern eye-tracking devices, such as those in HMDs or mobile glasses, often include head-tracking capabilities, offering an additional resource for analyzing search behavior. By comparing Eye Only and Head Only models our findings suggest that head movement data alone can infer task-induced changes in search behavior, though with reduced accuracy compared to eye-tracking data. However, when combined with eye-tracking metrics, head movement data provided unique information, enhancing the overall predictive power of the model. This highlights the potential of head movement data as both a standalone substitute when eye tracking is unavailable and a complementary feature for enhancing the analyses of cognitive processes and search behavior in dynamic environments (Clay et al., <a href="#CR17" class="usa-link" aria-describedby="CR17">2019</a>). Despite advances in camera-based mobile eye tracking, capturing relative or absolute gaze angle across varied real-world environments remains challenging due to factors like variable illumination or device slippage (Ghiani et al., <a href="#CR36" class="usa-link" aria-describedby="CR36">2024</a>). In contrast, head-tracking sensors, such as accelerometers or IMUs, offer a more robust while less precise means to estimate gaze direction, making them an attractive alternative for capturing data in challenging environments. Our findings underscore the potential of head-tracking data to quantify task-induced changes in behavior, even when eye-tracking data are unavailable. Furthermore, the simplicity and durability of head-tracking sensors make them a practical solution for applications requiring lightweight, unobtrusive, and cost-effective systems.</p>
<p id="Par42">The results from our multiclass classification showed head movement metrics provided unique information not fully captured by eye metrics alone. The most informative features in the 4-way classification model from head-only data were the total angular distance of head movement during the trial, normalized for trial duration, followed by kinematic features such as velocity and acceleration. Notably, head movement distance and stability (i.e., head fixation duration) were more sensitive to task-induced changes for target discriminability, while kinematic features were ranked higher for time pressure. These findings align with prior research demonstrating the utility of head kinematics for inferring search behavior (Hu et al., <a href="#CR44" class="usa-link" aria-describedby="CR44">2023</a>; Salehi et al., <a href="#CR76" class="usa-link" aria-describedby="CR76">2024</a>). For example, Hu et al. (<a href="#CR44" class="usa-link" aria-describedby="CR44">2023</a>) showed that head movement velocity and acceleration could reliably distinguish between different types of visual search, such as free viewing, target search, and tracking. Similarly, research by (Aivar et al., <a href="#CR2" class="usa-link" aria-describedby="CR2">2024</a>), demonstrated that body movement velocity increases with repeated search in the same context, reflecting an optimization of movement strategies for faster and more efficient search execution. This, combined with the fact that there was no significant difference in performance between the 2-way classification models of target discriminability and time pressure using head data alone, suggests that head tracking may provide some, though limited insight into the parameters of an individual’s search.</p></section><section id="Sec19"><h3 class="pmc_sec_title">The role of eye movements in target discriminability and time pressure</h3>
<p id="Par43">In the target discriminability model, fixation behaviors emerged as the most informative features, including Distractor Fixation Frequency, Target Fixation Duration Proportion, and Distractor Fixation Duration Proportion. This pattern suggests that eye movement metrics are sensitive to the perceptual details between targets and distractors. Specifically, under more difficult conditions, participants likely increased feature-level inspection, fixating on distractors more frequently and for longer durations as they made their decision. This sensitivity indicates that eye movement behavior can serve as a dynamic, implicit signal of perceptual difficulty.</p>
<p id="Par44">Conversely, the time pressure model prioritized fixation frequency and saccadic behaviors, with top features including Target Fixations per Target, Target Fixation Frequency, Saccade Size per Second, and Saccade Frequency. This suggests that under high time pressure, participants may have adopted a faster, more efficient search strategy with frequent saccades and brief fixations, likely to maximize the rate of information acquisition. Therefore, these eye movement features provide the potential for tracking cognitive state changes associated with search urgency.</p>
<p id="Par45">The current results are consistent with previous findings that show eye movement metrics such as saccade amplitude, fixation duration, and gaze velocity are highly sensitive to changes in search demands and target discriminability. For example, increased saccade amplitude and fixation frequency have been linked to harder visual search tasks, where targets are more difficult to discriminate from distractors (Pomplun et al., <a href="#CR73" class="usa-link" aria-describedby="CR73">2013</a>; Tagu &amp; Kristjánsson, <a href="#CR85" class="usa-link" aria-describedby="CR85">2022</a>). Similarly, fixation duration tends to increase as target discriminability becomes more challenging (Becker, <a href="#CR5" class="usa-link" aria-describedby="CR5">2011</a>; Pomplun et al., <a href="#CR73" class="usa-link" aria-describedby="CR73">2013</a>), while tasks with high perceptual and attentional demands decrease peak saccadic velocity (BacHurina &amp; Arsalidou, <a href="#CR3" class="usa-link" aria-describedby="CR3">2022</a>). Together our results are in agreement with current theories of visual search, such as the functional viewing field (FVF), which suggest that task demands, particularly search difficulty, modulate the amount of information processed during a fixation. These demands subsequently influence the spatial and temporal characteristics of saccades and fixations as participants adjust their strategies to effectively process visual information (Hulleman &amp; Olivers, <a href="#CR47" class="usa-link" aria-describedby="CR47">2017</a>; Young &amp; Hulleman, <a href="#CR94" class="usa-link" aria-describedby="CR94">2013</a>).</p></section></section><section id="Sec20"><h2 class="pmc_sec_title">Practical implications</h2>
<p id="Par46">The ability to infer task conditions from combined eye and head movement metrics has meaningful implications for the development of adaptive systems that monitor user state. IMUs, used to track head movements, are affordable, lightweight, and robust sensor. The current work indicates that such a sensor, easily deployed to the field, could provide insight into target discrimination difficulty experienced by a soldier triggering adjustments in system support. For example, this information could be shared with other squad members or a commander to inform the reallocation of resources such as deploying autonomous agents to aid in search and target interrogation. While head movement data provide valuable insight into user state, the results indicate that this signal alone is not as informative as eye movement data. Our results show that eye, compared to head data, provide higher classification accuracy and should be prioritized when available. In contexts with degraded or unavailable eye tracking, due to occlusion, motion, lighting, etc., head-tracking data offer a viable alternative.</p>
<p id="Par47">Importantly, the utility of head movement data as a substitute will also depend on the search context. In constrained environments, such as narrow hallways, head movements are limited, and eye movements are more likely to carry the critical information needed for search. In contrast, in broader or more open environments where potential search targets fall beyond the customary oculomotor range (COMR), head movements become more informative and can better reflect search dynamics (Stahl, <a href="#CR82" class="usa-link" aria-describedby="CR82">1999</a>, <a href="#CR84" class="usa-link" aria-describedby="CR84">2001b</a>). Thus, inferring contextual information from head movement data is most appropriate when search demands extend beyond the natural range of eye movements alone.</p></section><section id="Sec21"><h2 class="pmc_sec_title">Limitations and future work</h2>
<p id="Par48">A notable aspect of our approach was the inclusion of target- and distractor-specific metrics, such as fixation frequency and duration, which emerged as some of the most discriminative features in our models. For example, Distractor Fixation Frequency was a top-ranked feature for target discriminability, reflecting participants’ increased reliance on distractor fixations when distinguishing targets in the hard condition. While the results remain meaningful and demonstrate that these features offer valuable insights into task-induced search behavior, environmental labels may not be readily available in real-world settings.<a href="#Fn3" class="usa-link">3</a> However, our approach is a critical step toward developing a robust inferential model of task-induced changes in foraging behavior. Quantifying and understanding the utility of all eye and head movement metrics, regardless of their accessibility in a real-world context, provides a broad foundation for multimodal application spaces. For example, augmented reality (AR) applications would potentially have some context as to the task-relevance of overlaid information (e.g., a UI display) as well as some understanding of the environment (e.g., geospatial location). Head and eye movement features could be combined with other sensors, such as cameras, to provide a rich understanding of how environmental factors change behavior of individuals and groups. Moreover, having such a model to characterize “hard” or “easy” visual search may provide AI systems valuable information about the effectiveness of the current AI support. For instance, if eye and head movement signatures indicate an operator is having difficulty in searching the environment, this may indicate too much visual clutter in the AR display.</p>
<p id="Par49">Several factors limit the direct applicability of our findings to more naturalistic scenarios. The current study was conducted in a stationary context, with the task confined to 180 degrees of space. While this setup allowed for controlled measurement of eye and head movements during the task, it may not fully capture the spatiotemporal demands encountered in all foraging search environments. Future work should examine the generalizability of these findings to more ambulatory tasks with natural navigation and dynamic, occluded targets to better reflect real-world search complexity. The reduced field of view imposed by the HMD likely influenced search behavior, leading to more exaggerated head movements as participants compensated for limited peripheral vision. Similarly, field testing scenarios should consider the effect of helmets or other gear on head movements, as such equipment could substantially alter behavioral patterns compared to those observed in the current laboratory environment.</p>
<p id="Par50">Our study used a generalized classification model which applied a uniform approach across all participants. While this is useful for identifying overarching patterns in search behavior, it may overlook individual differences in movement strategy, such as varying reliance on head versus eye movement (Fuller, <a href="#CR34" class="usa-link" aria-describedby="CR34">1992</a>; Stahl, <a href="#CR83" class="usa-link" aria-describedby="CR83">2001a</a>). An individualized model that accounts for participant-specific movement patterns or preferences could improve predictive accuracy by incorporating eye–head coordination variability and other effector interactions. Refining classification models with personalized metrics would also provide a more detailed perspective on the cognitive mechanisms driving search behavior.</p>
<p id="Par51">In the present study, we analyzed behavior and eye/head movements across the entire duration of the search process within each trial, without segmenting distinct phases such as the time to first target acquisition, inter-target intervals during the middle of the trial, or the final search segment preceding the last response. This approach was intended to reflect the continuous nature of foraging behavior and to validate that our experimental manipulations influenced task performance. However, we acknowledge that the final segment of a search may involve qualitatively different decisional processes, potentially inflating inter-target times. Future work could examine subcomponents of the search process, such as time to first target, mid-trial search dynamics, or search termination behavior, to better isolate the distinct cognitive processes of each phase.</p></section><section id="Sec22"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par52">This study highlights the benefit of integrating eye and head movement metrics to infer task-induced changes in search behavior. The findings demonstrate that head movements, while often underutilized, offer unique and complementary information to eye metrics. These insights have practical implications for designing adaptive systems for near-real-time monitoring of operator states and informing adaptive systems designed to dynamically adjust their support.</p></section><section id="Sec23"><h2 class="pmc_sec_title">Supplementary Information</h2>
<section class="sm xbox font-sm" id="MOESM1"><div class="media p"><div class="caption">
<a href="/articles/instance/12373606/bin/41235_2025_657_MOESM1_ESM.eps" data-ga-action="click_feat_suppl" class="usa-link">Additional file 1</a><sup> (1.5MB, eps) </sup>
</div></div></section><section class="sm xbox font-sm" id="MOESM2"><div class="media p"><div class="caption">
<a href="/articles/instance/12373606/bin/41235_2025_657_MOESM2_ESM.eps" data-ga-action="click_feat_suppl" class="usa-link">Additional file 2</a><sup> (1.4MB, eps) </sup>
</div></div></section><section class="sm xbox font-sm" id="MOESM3"><div class="media p"><div class="caption">
<a href="/articles/instance/12373606/bin/41235_2025_657_MOESM3_ESM.png" data-ga-action="click_feat_suppl" class="usa-link">Additional file 3</a><sup> (91.5KB, png) </sup>
</div></div></section><section class="sm xbox font-sm" id="MOESM4"><div class="media p"><div class="caption">
<a href="/articles/instance/12373606/bin/41235_2025_657_MOESM4_ESM.xlsx" data-ga-action="click_feat_suppl" class="usa-link">Additional file 4</a><sup> (11.9KB, xlsx) </sup>
</div></div></section></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>AR, CCF, and AM conceived the study and methodology. AR, AM, CCF, and LD performed programming, data curation, and formal analysis. AR, CCF, LD, and JT wrote original draft. All authors read, edited, and approved the final manuscript. We extend our gratitude to Alex Stauff and Christian Barentine for their expertise and dedication in programming the experimental paradigm and C1C Dan Park, C1C Phil Luba, and C1C Khava Tsarni for their assistance with data collection.</p></section><section id="notes2"><h2 class="pmc_sec_title">Funding</h2>
<p>This research was supported by U.S. Army DEVCOM U.S. ARL project number: W911NF-23–2-0205.</p></section><section id="notes3"><h2 class="pmc_sec_title">Availability of data and materials</h2>
<p>The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request.</p></section><section id="notes4"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Ethics approval and consent to participate</h3>
<p id="Par53">Participants participated for extra course credit and provided informed consent. This work was approved by U.S. Army DEVCOM U.S ARL under protocol #ARL-21–132.</p></section><section id="FPar2"><h3 class="pmc_sec_title">Consent for publication</h3>
<p id="Par54">All authors have approved the manuscript for review.</p></section><section id="FPar3"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par55">The authors declare that they have no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm">
<div class="fn p" id="Fn1">
<sup>1</sup><p class="display-inline" id="Par57">RGB color values. Target – (1) 129, 103, 81, (2) 82, 67, 60, (3) 60, 65, 63, (4) 59, 60, 51. Easy distractor – (1) 97, 71, 44, (2) 94, 93, 53, (3) 199, 169, 86, (4) 59, 62, 47. Hard distractor – (1) 113, 86, 68, (2) 78, 86, 56, (3) 88, 80, 68, (4) 77, 71, 63.</p>
</div>
<div class="fn p" id="Fn2">
<sup>2</sup><p class="display-inline" id="Par58">box plots were created using the boxchart MATLAB (2021b) function. The horizontal line inside each box corresponds to the sample median while the top and bottom edges of the box are the upper and lower quartiles, respectively. Values that are more than 1.5 times the interquartile range (IQR) from the top or bottom are designated as outliers (points beyond whiskers). Each whisker above and below the box represents the maximum value that is not considered an outlier. The top and bottom edges of the notched region correspond to <em>m</em> + (1.57⋅<em>IQR</em>)/√<em>n</em> and <em>m</em> − (1.57⋅<em>IQR</em>)/√<em>n</em>, respectively, where <em>m</em> is the median, <em>IQR</em> is the interquartile range, and <em>n</em> is the number of data points. The green diamond represents the sample mean with gray error bars showing 95% confidence intervals.</p>
</div>
<div class="fn p" id="Fn3">
<sup>3</sup><p class="display-inline" id="Par59">Top features were extracted and models evaluated without target/distractor feature information and showed the same pattern of results though classification accuracy was generally lower. See Supplementary Materials.</p>
</div>
<div class="fn p" id="fn4">
<p><strong>Publisher's Note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div>
</div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ol class="ref-list font-sm">
<li id="CR1"><cite>Agtzidis, I., Startsev, M., &amp; Dorr, M. (2019). 360-degree Video Gaze Behaviour: A Ground-Truth Data Set and a Classification Algorithm for Eye Movements. <em>Proceedings of the 27th ACM International Conference on Multimedia</em>, 1007–1015. 10.1145/3343031.3350947</cite></li>
<li id="CR2">
<cite>Aivar, M. P., Li, C.-L., Tong, M. H., Kit, D. M., &amp; Hayhoe, M. M. (2024). Knowing where to go: Spatial memory guides eye and body movements in a naturalistic visual search task. <em>Journal of Vision,</em><em>24</em>(9), 1. 10.1167/jov.24.9.1
</cite> [<a href="https://doi.org/10.1167/jov.24.9.1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11373708/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39226069/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Aivar,%20M.%20P.,%20Li,%20C.-L.,%20Tong,%20M.%20H.,%20Kit,%20D.%20M.,%20&amp;%20Hayhoe,%20M.%20M.%20(2024).%20Knowing%20where%20to%20go:%20Spatial%20memory%20guides%20eye%20and%20body%20movements%20in%20a%20naturalistic%20visual%20search%20task.%20Journal%20of%20Vision,24(9),%201.%2010.1167/jov.24.9.1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<cite>Bachurina, V., &amp; Arsalidou, M. (2022). Multiple levels of mental attentional demand modulate peak saccade velocity and blink rate. <em>Heliyon</em>. 10.1016/j.heliyon.2022.e08826
</cite> [<a href="https://doi.org/10.1016/j.heliyon.2022.e08826" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8800024/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35128110/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Bachurina,%20V.,%20&amp;%20Arsalidou,%20M.%20(2022).%20Multiple%20levels%20of%20mental%20attentional%20demand%20modulate%20peak%20saccade%20velocity%20and%20blink%20rate.%20Heliyon.%2010.1016/j.heliyon.2022.e08826" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR4">
<cite>Bahill, A. T., Clark, M. R., &amp; Stark, L. (1975). The main sequence, a tool for studying human eye movements. <em>Mathematical Biosciences,</em><em>24</em>(3), 191–204. 10.1016/0025-5564(75)90075-9</cite> [<a href="https://scholar.google.com/scholar_lookup?Bahill,%20A.%20T.,%20Clark,%20M.%20R.,%20&amp;%20Stark,%20L.%20(1975).%20The%20main%20sequence,%20a%20tool%20for%20studying%20human%20eye%20movements.%20Mathematical%20Biosciences,24(3),%20191%E2%80%93204.%2010.1016/0025-5564(75)90075-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<cite>Becker, S. I. (2011). Determinants of dwell time in visual search: Similarity or perceptual difficulty? <em>PLoS ONE,</em><em>6</em>(3), Article e17740. 10.1371/journal.pone.0017740
</cite> [<a href="https://doi.org/10.1371/journal.pone.0017740" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3050928/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21408139/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Becker,%20S.%20I.%20(2011).%20Determinants%20of%20dwell%20time%20in%20visual%20search:%20Similarity%20or%20perceptual%20difficulty?%20PLoS%20ONE,6(3),%20Article%20e17740.%2010.1371/journal.pone.0017740" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR6">
<cite>Bischof, W. F., Anderson, N. C., &amp; Kingstone, A. (2024). A tutorial: Analyzing eye and head movements in virtual reality. <em>Behavior Research Methods,</em><em>56</em>(8), 8396–8421. 10.3758/s13428-024-02482-5
</cite> [<a href="https://doi.org/10.3758/s13428-024-02482-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39117987/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Bischof,%20W.%20F.,%20Anderson,%20N.%20C.,%20&amp;%20Kingstone,%20A.%20(2024).%20A%20tutorial:%20Analyzing%20eye%20and%20head%20movements%20in%20virtual%20reality.%20Behavior%20Research%20Methods,56(8),%208396%E2%80%938421.%2010.3758/s13428-024-02482-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<cite>Boot, W. R., Kramer, A. F., Becic, E., Wiegmann, D. A., &amp; Kubose, T. (2006). Detecting transient changes in dynamic displays: The more you look, the less you see. <em>Human Factors,</em><em>48</em>(4), 759–773. 10.1518/001872006779166424
</cite> [<a href="https://doi.org/10.1518/001872006779166424" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17240723/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Boot,%20W.%20R.,%20Kramer,%20A.%20F.,%20Becic,%20E.,%20Wiegmann,%20D.%20A.,%20&amp;%20Kubose,%20T.%20(2006).%20Detecting%20transient%20changes%20in%20dynamic%20displays:%20The%20more%20you%20look,%20the%20less%20you%20see.%20Human%20Factors,48(4),%20759%E2%80%93773.%2010.1518/001872006779166424" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<cite>Brunyé, T. T., Brou, R., Doty, T. J., Gregory, F. D., Hussey, E. K., Lieberman, H. R., Loverro, K. L., Mezzacappa, E. S., Neumeier, W. H., Patton, D. J., Soares, J. W., Thomas, T. P., &amp; Yu, A. B. (2020). A review of US army research contributing to cognitive enhancement in military contexts. <em>Journal of Cognitive Enhancement,</em><em>4</em>(4), 453–468. 10.1007/s41465-020-00167-3</cite> [<a href="https://scholar.google.com/scholar_lookup?Bruny%C3%A9,%20T.%20T.,%20Brou,%20R.,%20Doty,%20T.%20J.,%20Gregory,%20F.%20D.,%20Hussey,%20E.%20K.,%20Lieberman,%20H.%20R.,%20Loverro,%20K.%20L.,%20Mezzacappa,%20E.%20S.,%20Neumeier,%20W.%20H.,%20Patton,%20D.%20J.,%20Soares,%20J.%20W.,%20Thomas,%20T.%20P.,%20&amp;%20Yu,%20A.%20B.%20(2020).%20A%20review%20of%20US%20army%20research%20contributing%20to%20cognitive%20enhancement%20in%20military%20contexts.%20Journal%20of%20Cognitive%20Enhancement,4(4),%20453%E2%80%93468.%2010.1007/s41465-020-00167-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9">
<cite>Brunyé, T. T., &amp; Gardony, A. L. (2017). Eye tracking measures of uncertainty during perceptual decision making. <em>International Journal of Psychophysiology,</em><em>120</em>, 60–68. 10.1016/j.ijpsycho.2017.07.008
</cite> [<a href="https://doi.org/10.1016/j.ijpsycho.2017.07.008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28732659/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Bruny%C3%A9,%20T.%20T.,%20&amp;%20Gardony,%20A.%20L.%20(2017).%20Eye%20tracking%20measures%20of%20uncertainty%20during%20perceptual%20decision%20making.%20International%20Journal%20of%20Psychophysiology,120,%2060%E2%80%9368.%2010.1016/j.ijpsycho.2017.07.008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR10">
<cite>Brunyé, T. T., &amp; Giles, G. E. (2023). Methods for eliciting and measuring behavioral and physiological consequences of stress and uncertainty in virtual reality. <em>Frontiers in Virtual Reality</em>. 10.3389/frvir.2023.951435</cite> [<a href="https://scholar.google.com/scholar_lookup?Bruny%C3%A9,%20T.%20T.,%20&amp;%20Giles,%20G.%20E.%20(2023).%20Methods%20for%20eliciting%20and%20measuring%20behavioral%20and%20physiological%20consequences%20of%20stress%20and%20uncertainty%20in%20virtual%20reality.%20Frontiers%20in%20Virtual%20Reality.%2010.3389/frvir.2023.951435" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<cite>Cain, M. S., Vul, E., Clark, K., &amp; Mitroff, S. R. (2012). A Bayesian optimal foraging model of human visual search. <em>Psychological Science,</em><em>23</em>(9), 1047–1054. 10.1177/0956797612440460
</cite> [<a href="https://doi.org/10.1177/0956797612440460" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22868494/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Cain,%20M.%20S.,%20Vul,%20E.,%20Clark,%20K.,%20&amp;%20Mitroff,%20S.%20R.%20(2012).%20A%20Bayesian%20optimal%20foraging%20model%20of%20human%20visual%20search.%20Psychological%20Science,23(9),%201047%E2%80%931054.%2010.1177/0956797612440460" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<cite>Callahan-Flintoft, C., Barentine, C., Touryan, J., &amp; Ries, A. J. (2021). A case for studying naturalistic eye and head movements in virtual environments. <em>Frontiers in Psychology</em>. 10.3389/fpsyg.2021.650693
</cite> [<a href="https://doi.org/10.3389/fpsyg.2021.650693" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8759101/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35035362/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Callahan-Flintoft,%20C.,%20Barentine,%20C.,%20Touryan,%20J.,%20&amp;%20Ries,%20A.%20J.%20(2021).%20A%20case%20for%20studying%20naturalistic%20eye%20and%20head%20movements%20in%20virtual%20environments.%20Frontiers%20in%20Psychology.%2010.3389/fpsyg.2021.650693" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<cite>Callahan-Flintoft, C., Jensen, E., Naeem, J., Nonte, M. W., Madison, A. M., &amp; Ries, A. J. (2024). A comparison of head movement classification methods. <em>Sensors.,</em><em>24</em>(4), 1260. 10.3390/s24041260
</cite> [<a href="https://doi.org/10.3390/s24041260" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10893452/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38400418/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Callahan-Flintoft,%20C.,%20Jensen,%20E.,%20Naeem,%20J.,%20Nonte,%20M.%20W.,%20Madison,%20A.%20M.,%20&amp;%20Ries,%20A.%20J.%20(2024).%20A%20comparison%20of%20head%20movement%20classification%20methods.%20Sensors.,24(4),%201260.%2010.3390/s24041260" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<cite>Chen, L. L., &amp; Walton, M. M. G. (2005). Head movement evoked by electrical stimulation in the supplementary eye field of the Rhesus monkey. <em>Journal of Neurophysiology,</em><em>94</em>(6), 4502–4519. 10.1152/jn.00510.2005
</cite> [<a href="https://doi.org/10.1152/jn.00510.2005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16148273/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Chen,%20L.%20L.,%20&amp;%20Walton,%20M.%20M.%20G.%20(2005).%20Head%20movement%20evoked%20by%20electrical%20stimulation%20in%20the%20supplementary%20eye%20field%20of%20the%20Rhesus%20monkey.%20Journal%20of%20Neurophysiology,94(6),%204502%E2%80%934519.%2010.1152/jn.00510.2005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR15">
<cite>Christ, S. E., &amp; Abrams, R. A. (2006). Abrupt onsets cannot be ignored. <em>Psychonomic Bulletin &amp; Review,</em><em>13</em>(5), 875–880. 10.3758/BF03194012
</cite> [<a href="https://doi.org/10.3758/bf03194012" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17328388/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Christ,%20S.%20E.,%20&amp;%20Abrams,%20R.%20A.%20(2006).%20Abrupt%20onsets%20cannot%20be%20ignored.%20Psychonomic%20Bulletin%20&amp;%20Review,13(5),%20875%E2%80%93880.%2010.3758/BF03194012" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR16"><cite>Christoph, M. (2020). <em>Interpretable machine learning: A guide for making black box models explainable</em>. Leanpub.</cite></li>
<li id="CR17">
<cite>Clay, V., König, P., &amp; Koenig, S. (2019). Eye tracking in virtual reality. <em>Journal of Eye Movement Research.,</em><em>12</em>(1), 10–6910.</cite> [<a href="https://doi.org/10.16910/jemr.12.1.3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7903250/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33828721/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Clay,%20V.,%20K%C3%B6nig,%20P.,%20&amp;%20Koenig,%20S.%20(2019).%20Eye%20tracking%20in%20virtual%20reality.%20Journal%20of%20Eye%20Movement%20Research.,12(1),%2010%E2%80%936910." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR18">
<cite>Culmer, P. R., Levesley, M. C., Mon-Williams, M., &amp; Williams, J. H. G. (2009). A new tool for assessing human movement: The kinematic assessment tool. <em>Journal of Neuroscience Methods,</em><em>184</em>(1), 184–192. 10.1016/j.jneumeth.2009.07.025
</cite> [<a href="https://doi.org/10.1016/j.jneumeth.2009.07.025" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19646475/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Culmer,%20P.%20R.,%20Levesley,%20M.%20C.,%20Mon-Williams,%20M.,%20&amp;%20Williams,%20J.%20H.%20G.%20(2009).%20A%20new%20tool%20for%20assessing%20human%20movement:%20The%20kinematic%20assessment%20tool.%20Journal%20of%20Neuroscience%20Methods,184(1),%20184%E2%80%93192.%2010.1016/j.jneumeth.2009.07.025" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19"><cite>David, E. J., Gutiérrez, J., Coutrot, A., Da Silva, M. P., &amp; Callet, P. L. (2018). A dataset of head and eye movements for 360° videos. <em>Proceedings of the 9th ACM Multimedia Systems Conference</em>, 432–437. 10.1145/3204949.3208139</cite></li>
<li id="CR20">
<cite>David, E., Beitner, J., &amp; Võ, M.L.-H. (2020). Effects of transient loss of vision on head and eye movements during visual search in a virtual environment. <em>Brain Sciences,</em><em>10</em>(11), 841. 10.3390/brainsci10110841
</cite> [<a href="https://doi.org/10.3390/brainsci10110841" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7696943/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33198116/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?David,%20E.,%20Beitner,%20J.,%20&amp;%20V%C3%B5,%20M.L.-H.%20(2020).%20Effects%20of%20transient%20loss%20of%20vision%20on%20head%20and%20eye%20movements%20during%20visual%20search%20in%20a%20virtual%20environment.%20Brain%20Sciences,10(11),%20841.%2010.3390/brainsci10110841" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR21">
<cite>Di Stasi, L. L., Antolí, A., &amp; Cañas, J. J. (2011). Main sequence: An index for detecting mental workload variation in complex tasks. <em>Applied Ergonomics,</em><em>42</em>(6), 807–813. 10.1016/j.apergo.2011.01.003
</cite> [<a href="https://doi.org/10.1016/j.apergo.2011.01.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21316645/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Di%20Stasi,%20L.%20L.,%20Antol%C3%AD,%20A.,%20&amp;%20Ca%C3%B1as,%20J.%20J.%20(2011).%20Main%20sequence:%20An%20index%20for%20detecting%20mental%20workload%20variation%20in%20complex%20tasks.%20Applied%20Ergonomics,42(6),%20807%E2%80%93813.%2010.1016/j.apergo.2011.01.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<cite>Dimigen, O., Sommer, W., Hohlfeld, A., Jacobs, A. M., &amp; Kliegl, R. (2011). Coregistration of eye movements and EEG in natural reading: Analyses and review. <em>Journal of Experimental Psychology: General,</em><em>140</em>(4), 552–572. 10.1037/a0023885
</cite> [<a href="https://doi.org/10.1037/a0023885" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21744985/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Dimigen,%20O.,%20Sommer,%20W.,%20Hohlfeld,%20A.,%20Jacobs,%20A.%20M.,%20&amp;%20Kliegl,%20R.%20(2011).%20Coregistration%20of%20eye%20movements%20and%20EEG%20in%20natural%20reading:%20Analyses%20and%20review.%20Journal%20of%20Experimental%20Psychology:%20General,140(4),%20552%E2%80%93572.%2010.1037/a0023885" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR23">
<cite>Doshi, A., &amp; Trivedi, M. M. (2012). Head and eye gaze dynamics during visual attention shifts in complex environments. <em>Journal of Vision,</em><em>12</em>(2), 9–9. 10.1167/12.2.9
</cite> [<a href="https://doi.org/10.1167/12.2.9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22323822/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Doshi,%20A.,%20&amp;%20Trivedi,%20M.%20M.%20(2012).%20Head%20and%20eye%20gaze%20dynamics%20during%20visual%20attention%20shifts%20in%20complex%20environments.%20Journal%20of%20Vision,12(2),%209%E2%80%939.%2010.1167/12.2.9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<cite>Draschkow, D., Kallmayer, M., &amp; Nobre, A. C. (2021). When natural behavior engages working memory. <em>Current Biology,</em><em>31</em>(4), 869-874.e5. 10.1016/j.cub.2020.11.013
</cite> [<a href="https://doi.org/10.1016/j.cub.2020.11.013" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7902904/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33278355/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Draschkow,%20D.,%20Kallmayer,%20M.,%20&amp;%20Nobre,%20A.%20C.%20(2021).%20When%20natural%20behavior%20engages%20working%20memory.%20Current%20Biology,31(4),%20869-874.e5.%2010.1016/j.cub.2020.11.013" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR25">
<cite>Drew, T., Boettcher, S. E. P., &amp; Wolfe, J. M. (2017). One visual search, many memory searches: An eye-tracking investigation of hybrid search. <em>Journal of Vision,</em><em>17</em>(11), 5. 10.1167/17.11.5
</cite> [<a href="https://doi.org/10.1167/17.11.5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5596794/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28892812/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Drew,%20T.,%20Boettcher,%20S.%20E.%20P.,%20&amp;%20Wolfe,%20J.%20M.%20(2017).%20One%20visual%20search,%20many%20memory%20searches:%20An%20eye-tracking%20investigation%20of%20hybrid%20search.%20Journal%20of%20Vision,17(11),%205.%2010.1167/17.11.5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<cite>Duncan, J., &amp; Humphreys, G. W. (1989). Visual search and stimulus similarity. <em>Psychological Review,</em><em>96</em>(3), 433–458. 10.1037/0033-295X.96.3.433
</cite> [<a href="https://doi.org/10.1037/0033-295x.96.3.433" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/2756067/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Duncan,%20J.,%20&amp;%20Humphreys,%20G.%20W.%20(1989).%20Visual%20search%20and%20stimulus%20similarity.%20Psychological%20Review,96(3),%20433%E2%80%93458.%2010.1037/0033-295X.96.3.433" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR27">
<cite>Dwivedi, R., Tiwari, A., Bharill, N., Ratnaparkhe, M., &amp; Tiwari, A. K. (2024). A taxonomy of unsupervised feature selection methods including their pros, cons, and challenges. <em>The Journal of Supercomputing,</em><em>80</em>(16), 24212–24240. 10.1007/s11227-024-06368-3</cite> [<a href="https://scholar.google.com/scholar_lookup?Dwivedi,%20R.,%20Tiwari,%20A.,%20Bharill,%20N.,%20Ratnaparkhe,%20M.,%20&amp;%20Tiwari,%20A.%20K.%20(2024).%20A%20taxonomy%20of%20unsupervised%20feature%20selection%20methods%20including%20their%20pros,%20cons,%20and%20challenges.%20The%20Journal%20of%20Supercomputing,80(16),%2024212%E2%80%9324240.%2010.1007/s11227-024-06368-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<cite>Eckstein, M. P. (2011). Visual search: A retrospective. <em>Journal of Vision</em>. 10.1167/11.5.14
</cite> [<a href="https://doi.org/10.1167/11.5.14" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22209816/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Eckstein,%20M.%20P.%20(2011).%20Visual%20search:%20A%20retrospective.%20Journal%20of%20Vision.%2010.1167/11.5.14" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29"><cite>El Iskandarani, M., Atweh, J. A., &amp; Riggs, S. L. (2024). Time Pressure Detection in a Visual Search Task Using Eye Tracking Metrics: A Virtual Reality Study. <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em>, 10711813241261288. 10.1177/10711813241261288</cite></li>
<li id="CR30">
<cite>Enders, L. R., Smith, R. J., Gordon, S. M., Ries, A. J., &amp; Touryan, J. (2021). Gaze behavior during navigation and visual search of an open-world virtual environment. <em>Frontiers in Psychology.,</em><em>12</em>, 681042. 10.3389/fpsyg.2021.681042
</cite> [<a href="https://doi.org/10.3389/fpsyg.2021.681042" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8380848/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34434140/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Enders,%20L.%20R.,%20Smith,%20R.%20J.,%20Gordon,%20S.%20M.,%20Ries,%20A.%20J.,%20&amp;%20Touryan,%20J.%20(2021).%20Gaze%20behavior%20during%20navigation%20and%20visual%20search%20of%20an%20open-world%20virtual%20environment.%20Frontiers%20in%20Psychology.,12,%20681042.%2010.3389/fpsyg.2021.681042" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<cite>Engbert, R., &amp; Mergenthaler, K. (2006). Microsaccades are triggered by low retinal image slip. <em>Proceedings of the National Academy of Sciences,</em><em>103</em>(18), 7192–7197. 10.1073/pnas.0509557103</cite> [<a href="https://doi.org/10.1073/pnas.0509557103" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC1459039/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16632611/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Engbert,%20R.,%20&amp;%20Mergenthaler,%20K.%20(2006).%20Microsaccades%20are%20triggered%20by%20low%20retinal%20image%20slip.%20Proceedings%20of%20the%20National%20Academy%20of%20Sciences,103(18),%207192%E2%80%937197.%2010.1073/pnas.0509557103" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<cite>Freedman, E. G. (2008). Coordination of the eyes and head during visual orienting. <em>Experimental Brain Research,</em><em>190</em>(4), 369–387. 10.1007/s00221-008-1504-8
</cite> [<a href="https://doi.org/10.1007/s00221-008-1504-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC2605952/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/18704387/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Freedman,%20E.%20G.%20(2008).%20Coordination%20of%20the%20eyes%20and%20head%20during%20visual%20orienting.%20Experimental%20Brain%20Research,190(4),%20369%E2%80%93387.%2010.1007/s00221-008-1504-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33">
<cite>Freedman, E. G., &amp; Sparks, D. L. (1997). Eye-head coordination during head-unrestrained gaze shifts in rhesus monkeys. <em>Journal of Neurophysiology,</em><em>77</em>(5), 2328–2348. 10.1152/jn.1997.77.5.2328
</cite> [<a href="https://doi.org/10.1152/jn.1997.77.5.2328" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/9163361/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Freedman,%20E.%20G.,%20&amp;%20Sparks,%20D.%20L.%20(1997).%20Eye-head%20coordination%20during%20head-unrestrained%20gaze%20shifts%20in%20rhesus%20monkeys.%20Journal%20of%20Neurophysiology,77(5),%202328%E2%80%932348.%2010.1152/jn.1997.77.5.2328" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR34">
<cite>Fuller, J. H. (1992). Head movement propensity. <em>Experimental Brain Research,</em><em>92</em>(1), 152–164. 10.1007/BF00230391
</cite> [<a href="https://doi.org/10.1007/BF00230391" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/1486950/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Fuller,%20J.%20H.%20(1992).%20Head%20movement%20propensity.%20Experimental%20Brain%20Research,92(1),%20152%E2%80%93164.%2010.1007/BF00230391" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<cite>Ganesan, A., Alakhras, M., Brennan, P. C., &amp; Mello-Thoms, C. (2018). A review of factors influencing radiologists’ visual search behaviour. <em>Journal of Medical Imaging and Radiation Oncology,</em><em>62</em>(6), 747–757. 10.1111/1754-9485.12798
</cite> [<a href="https://doi.org/10.1111/1754-9485.12798" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30198628/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ganesan,%20A.,%20Alakhras,%20M.,%20Brennan,%20P.%20C.,%20&amp;%20Mello-Thoms,%20C.%20(2018).%20A%20review%20of%20factors%20influencing%20radiologists%E2%80%99%20visual%20search%20behaviour.%20Journal%20of%20Medical%20Imaging%20and%20Radiation%20Oncology,62(6),%20747%E2%80%93757.%2010.1111/1754-9485.12798" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<cite>Ghiani, A., Mann, D., &amp; Brenner, E. (2024). Methods matter: Exploring how expectations influence common actions. <em>iScience</em>. 10.1016/j.isci.2024.109076
</cite> [<a href="https://doi.org/10.1016/j.isci.2024.109076" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10867666/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38361615/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ghiani,%20A.,%20Mann,%20D.,%20&amp;%20Brenner,%20E.%20(2024).%20Methods%20matter:%20Exploring%20how%20expectations%20influence%20common%20actions.%20iScience.%2010.1016/j.isci.2024.109076" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR37">
<cite>Gilchrist, I. D., &amp; Harvey, M. (2006). Evidence for a systematic component within scan paths in visual search. <em>Visual Cognition</em>. 10.1080/13506280500193719</cite> [<a href="https://scholar.google.com/scholar_lookup?Gilchrist,%20I.%20D.,%20&amp;%20Harvey,%20M.%20(2006).%20Evidence%20for%20a%20systematic%20component%20within%20scan%20paths%20in%20visual%20search.%20Visual%20Cognition.%2010.1080/13506280500193719" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR38">
<cite>Gordon, S. M., Dalangin, B., &amp; Touryan, J. (2024). Saccade size predicts onset time of object processing during visual search of an open world virtual environment. <em>NeuroImage,</em><em>298</em>, Article 120781. 10.1016/j.neuroimage.2024.120781
</cite> [<a href="https://doi.org/10.1016/j.neuroimage.2024.120781" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39127183/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Gordon,%20S.%20M.,%20Dalangin,%20B.,%20&amp;%20Touryan,%20J.%20(2024).%20Saccade%20size%20predicts%20onset%20time%20of%20object%20processing%20during%20visual%20search%20of%20an%20open%20world%20virtual%20environment.%20NeuroImage,298,%20Article%20120781.%2010.1016/j.neuroimage.2024.120781" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR39"><cite>Groner, R., Walder, F., &amp; Groner, M. (1984). Looking at Faces: Local and Global Aspects of Scanpaths. In A. G. Gale &amp; F. Johnson (Eds.), <em>Advances in Psychology</em> (Vol. 22, pp. 523–533). North-Holland. 10.1016/S0166-4115(08)61874-9</cite></li>
<li id="CR40">
<cite>Henderson, J. M., Shinkareva, S. V., Wang, J., Luke, S. G., &amp; Olejarczyk, J. (2013). Predicting cognitive state from eye movements. <em>PLoS ONE,</em><em>8</em>(5), Article e64937. 10.1371/journal.pone.0064937
</cite> [<a href="https://doi.org/10.1371/journal.pone.0064937" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3666973/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23734228/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Henderson,%20J.%20M.,%20Shinkareva,%20S.%20V.,%20Wang,%20J.,%20Luke,%20S.%20G.,%20&amp;%20Olejarczyk,%20J.%20(2013).%20Predicting%20cognitive%20state%20from%20eye%20movements.%20PLoS%20ONE,8(5),%20Article%20e64937.%2010.1371/journal.pone.0064937" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR41"><cite>Holmqvist, K., Nyström, M., Andersson, R., Dewhurst, R., Jarodzka, H., &amp; Van de Weijer, J. (2011). <em>Eye tracking: A comprehensive guide to methods and measures</em>. Oxford University Press. <a href="http://dspace.ou.nl/handle/1820/3441" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://dspace.ou.nl/handle/1820/3441</a></cite></li>
<li id="CR42"><cite>Hooge, I. Th. C., Vlaskamp, B. N. S., &amp; Over, E. A. B. (2007). Chapter 27 - Saccadic search: On the duration of a fixation. In R. P. G. Van Gompel, M. H. Fischer, W. S. Murray, &amp; R. L. Hill (Eds.), <em>Eye Movements</em> (pp. 581–595). Elsevier. 10.1016/B978-008044980-7/50029-X</cite></li>
<li id="CR43">
<cite>Hooge, I. T. C., Niehorster, D. C., Nyström, M., Andersson, R., &amp; Hessels, R. S. (2022). Fixation classification: How to merge and select fixation candidates. <em>Behavior Research Methods,</em><em>54</em>(6), 2765–2776. 10.3758/s13428-021-01723-1
</cite> [<a href="https://doi.org/10.3758/s13428-021-01723-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9729319/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35023066/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Hooge,%20I.%20T.%20C.,%20Niehorster,%20D.%20C.,%20Nystr%C3%B6m,%20M.,%20Andersson,%20R.,%20&amp;%20Hessels,%20R.%20S.%20(2022).%20Fixation%20classification:%20How%20to%20merge%20and%20select%20fixation%20candidates.%20Behavior%20Research%20Methods,54(6),%202765%E2%80%932776.%2010.3758/s13428-021-01723-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR44">
<cite>Hu, Z., Bulling, A., Li, S., &amp; Wang, G. (2023). EHTask: Recognizing user tasks from eye and head movements in immersive virtual reality. <em>IEEE Transactions on Visualization and Computer Graphics,</em><em>29</em>(4), 1992–2004. 10.1109/TVCG.2021.3138902
</cite> [<a href="https://doi.org/10.1109/TVCG.2021.3138902" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34962869/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Hu,%20Z.,%20Bulling,%20A.,%20Li,%20S.,%20&amp;%20Wang,%20G.%20(2023).%20EHTask:%20Recognizing%20user%20tasks%20from%20eye%20and%20head%20movements%20in%20immersive%20virtual%20reality.%20IEEE%20Transactions%20on%20Visualization%20and%20Computer%20Graphics,29(4),%201992%E2%80%932004.%2010.1109/TVCG.2021.3138902" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR45">
<cite>Huang, C.-M., Andrist, S., Sauppé, A., &amp; Mutlu, B. (2015). Using gaze patterns to predict task intent in collaboration. <em>Frontiers in Psychology</em>, <em>6</em>. 10.3389/fpsyg.2015.01049</cite> [<a href="https://doi.org/10.3389/fpsyg.2015.01049" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4513212/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26257694/" class="usa-link">PubMed</a>]</li>
<li id="CR46"><cite>Hulle, N., Aroca-Ouellette, S., Ries, A. J., Brawer, J., von der Wense, K., &amp; Roncone, A. (2024). Eyes on the Game: Deciphering Implicit Signals to Infer Human Intentions, Trust, and Skill. <em>RO-MAN</em>, x–x.</cite></li>
<li id="CR47">
<cite>Hulleman, J., &amp; Olivers, C. N. L. (2017). The impending demise of the item in visual search. <em>Behavioral and Brain Sciences,</em><em>40</em>, Article e132. 10.1017/S0140525X15002794
</cite> [<a href="https://doi.org/10.1017/S0140525X15002794" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26673054/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Hulleman,%20J.,%20&amp;%20Olivers,%20C.%20N.%20L.%20(2017).%20The%20impending%20demise%20of%20the%20item%20in%20visual%20search.%20Behavioral%20and%20Brain%20Sciences,40,%20Article%20e132.%2010.1017/S0140525X15002794" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR48">
<cite>Itti, L., &amp; Koch, C. (2000). A saliency-based search mechanism for overt and covert shifts of visual attention. <em>Vision Research,</em><em>40</em>(10), 1489–1506. 10.1016/S0042-6989(99)00163-7
</cite> [<a href="https://doi.org/10.1016/s0042-6989(99)00163-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/10788654/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Itti,%20L.,%20&amp;%20Koch,%20C.%20(2000).%20A%20saliency-based%20search%20mechanism%20for%20overt%20and%20covert%20shifts%20of%20visual%20attention.%20Vision%20Research,40(10),%201489%E2%80%931506.%2010.1016/S0042-6989(99)00163-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR49"><cite>Karessli, N., Akata, Z., Schiele, B., &amp; Bulling, A. (2017). <em>Gaze Embeddings for Zero-Shot Image Classification</em>. 4525–4534. <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Karessli_Gaze_Embeddings_for_CVPR_2017_paper.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://openaccess.thecvf.com/content_cvpr_2017/html/Karessli_Gaze_Embeddings_for_CVPR_2017_paper.html</a></cite></li>
<li id="CR50">
<cite>Kennedy, R. S., Lane, N. E., Berbaum, K. S., &amp; Lilienthal, M. G. (1993). Simulator sickness questionnaire: An enhanced method for quantifying simulator sickness. <em>The International Journal of Aviation Psychology,</em><em>3</em>(3), 203–220. 10.1207/s15327108ijap0303_3</cite> [<a href="https://scholar.google.com/scholar_lookup?Kennedy,%20R.%20S.,%20Lane,%20N.%20E.,%20Berbaum,%20K.%20S.,%20&amp;%20Lilienthal,%20M.%20G.%20(1993).%20Simulator%20sickness%20questionnaire:%20An%20enhanced%20method%20for%20quantifying%20simulator%20sickness.%20The%20International%20Journal%20of%20Aviation%20Psychology,3(3),%20203%E2%80%93220.%2010.1207/s15327108ijap0303_3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR51">
<cite>König, P., Wilming, N., Kietzmann, T. C., Ossandón, J. P., Onat, S., Ehinger, B. V., Gameiro, R. R., &amp; Kaspar, K. (2016). Eye movements as a window to cognitive processes. <em>Journal of Eye Movement Research.,</em><em>9</em>(5), 25.</cite> [<a href="https://scholar.google.com/scholar_lookup?K%C3%B6nig,%20P.,%20Wilming,%20N.,%20Kietzmann,%20T.%20C.,%20Ossand%C3%B3n,%20J.%20P.,%20Onat,%20S.,%20Ehinger,%20B.%20V.,%20Gameiro,%20R.%20R.,%20&amp;%20Kaspar,%20K.%20(2016).%20Eye%20movements%20as%20a%20window%20to%20cognitive%20processes.%20Journal%20of%20Eye%20Movement%20Research.,9(5),%2025." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR52">
<cite>Kothari, R., Yang, Z., Kanan, C., Bailey, R., Pelz, J. B., &amp; Diaz, G. J. (2020). Gaze-in-wild: A dataset for studying eye and head coordination in everyday activities. <em>Scientific Reports,</em><em>10</em>(1), 2539. 10.1038/s41598-020-59251-5
</cite> [<a href="https://doi.org/10.1038/s41598-020-59251-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7018838/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32054884/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Kothari,%20R.,%20Yang,%20Z.,%20Kanan,%20C.,%20Bailey,%20R.,%20Pelz,%20J.%20B.,%20&amp;%20Diaz,%20G.%20J.%20(2020).%20Gaze-in-wild:%20A%20dataset%20for%20studying%20eye%20and%20head%20coordination%20in%20everyday%20activities.%20Scientific%20Reports,10(1),%202539.%2010.1038/s41598-020-59251-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR53"><cite>Kothe, C., Shirazi, S. Y., Stenner, T., Medine, D., Boulay, C., Grivich, M. I., Mullen, T., Delorme, A., &amp; Makeig, S. (2024). <em>The Lab Streaming Layer for Synchronized Multimodal Recording</em> (p. 2024.02.13.580071). bioRxiv. 10.1101/2024.02.13.580071</cite></li>
<li id="CR54">
<cite>Kristjánsson, Á., Jóhannesson, Ó. I., &amp; Thornton, I. M. (2014). Common attentional constraints in visual foraging. <em>PLoS ONE,</em><em>9</em>(6), Article e100752. 10.1371/journal.pone.0100752
</cite> [<a href="https://doi.org/10.1371/journal.pone.0100752" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4071029/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24964082/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Kristj%C3%A1nsson,%20%C3%81.,%20J%C3%B3hannesson,%20%C3%93.%20I.,%20&amp;%20Thornton,%20I.%20M.%20(2014).%20Common%20attentional%20constraints%20in%20visual%20foraging.%20PLoS%20ONE,9(6),%20Article%20e100752.%2010.1371/journal.pone.0100752" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR55">
<cite>Kristjánsson, T., Draschkow, D., Pálsson, Á., Haraldsson, D., Jónsson, P. Ö., &amp; Kristjánsson, Á. (2022). Moving foraging into three dimensions: Feature- versus conjunction-based foraging in virtual reality. <em>Quarterly Journal of Experimental Psychology,</em><em>75</em>(2), 313–327. 10.1177/1747021820937020</cite> [<a href="https://doi.org/10.1177/1747021820937020" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32519926/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Kristj%C3%A1nsson,%20T.,%20Draschkow,%20D.,%20P%C3%A1lsson,%20%C3%81.,%20Haraldsson,%20D.,%20J%C3%B3nsson,%20P.%20%C3%96.,%20&amp;%20Kristj%C3%A1nsson,%20%C3%81.%20(2022).%20Moving%20foraging%20into%20three%20dimensions:%20Feature-%20versus%20conjunction-based%20foraging%20in%20virtual%20reality.%20Quarterly%20Journal%20of%20Experimental%20Psychology,75(2),%20313%E2%80%93327.%2010.1177/1747021820937020" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR56">
<cite>Kristjánsson, T., Thornton, I. M., &amp; Kristjánsson, Á. (2018). Time limits during visual foraging reveal flexible working memory templates. <em>Journal of Experimental Psychology: Human Perception and Performance,</em><em>44</em>(6), 827–835. 10.1037/xhp0000517
</cite> [<a href="https://doi.org/10.1037/xhp0000517" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29809049/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Kristj%C3%A1nsson,%20T.,%20Thornton,%20I.%20M.,%20&amp;%20Kristj%C3%A1nsson,%20%C3%81.%20(2018).%20Time%20limits%20during%20visual%20foraging%20reveal%20flexible%20working%20memory%20templates.%20Journal%20of%20Experimental%20Psychology:%20Human%20Perception%20and%20Performance,44(6),%20827%E2%80%93835.%2010.1037/xhp0000517" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR57">
<cite>Kursa, M. B., Jankowski, A., &amp; Rudnicki, W. R. (2010). Boruta–A system for feature selection. <em>Fundamenta Informaticae,</em><em>101</em>(4), 271–285. 10.3233/FI-2010-288</cite> [<a href="https://scholar.google.com/scholar_lookup?Kursa,%20M.%20B.,%20Jankowski,%20A.,%20&amp;%20Rudnicki,%20W.%20R.%20(2010).%20Boruta%E2%80%93A%20system%20for%20feature%20selection.%20Fundamenta%20Informaticae,101(4),%20271%E2%80%93285.%2010.3233/FI-2010-288" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR58">
<cite>Le-Hoa Võ, M., &amp; Wolfe, J. M. (2015). The role of memory for visual search in scenes. <em>Annals of the New York Academy of Sciences,</em><em>1339</em>(1), 72–81. 10.1111/nyas.12667
</cite> [<a href="https://doi.org/10.1111/nyas.12667" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4376654/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25684693/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Le-Hoa%20V%C3%B5,%20M.,%20&amp;%20Wolfe,%20J.%20M.%20(2015).%20The%20role%20of%20memory%20for%20visual%20search%20in%20scenes.%20Annals%20of%20the%20New%20York%20Academy%20of%20Sciences,1339(1),%2072%E2%80%9381.%2010.1111/nyas.12667" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR59">
<cite>Lisberger, S. G., Evinger, C., Johanson, G. W., &amp; Fuchs, A. F. (1981). Relationship between eye acceleration and retinal image velocity during foveal smooth pursuit in man and monkey. <em>Journal of Neurophysiology,</em><em>46</em>(2), 229–249. 10.1152/jn.1981.46.2.229
</cite> [<a href="https://doi.org/10.1152/jn.1981.46.2.229" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/7264712/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Lisberger,%20S.%20G.,%20Evinger,%20C.,%20Johanson,%20G.%20W.,%20&amp;%20Fuchs,%20A.%20F.%20(1981).%20Relationship%20between%20eye%20acceleration%20and%20retinal%20image%20velocity%20during%20foveal%20smooth%20pursuit%20in%20man%20and%20monkey.%20Journal%20of%20Neurophysiology,46(2),%20229%E2%80%93249.%2010.1152/jn.1981.46.2.229" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR60">
<cite>Long, R. J., Ariyo, A. G., Iwu, Z. I., Azari, D. P., Madison, A. M., Aroca-Ouellette, S., Roncone, A., Tossell, C. C., &amp; Ries, A. J. (2024). Towards an adaptive system for investigating human-agent teaming: Let’s get cooking! <em>Systems and Information Engineering Design Symposium (SIEDS),</em><em>2024</em>, 1–6. 10.1109/SIEDS61124.2024.10534646</cite> [<a href="https://scholar.google.com/scholar_lookup?Long,%20R.%20J.,%20Ariyo,%20A.%20G.,%20Iwu,%20Z.%20I.,%20Azari,%20D.%20P.,%20Madison,%20A.%20M.,%20Aroca-Ouellette,%20S.,%20Roncone,%20A.,%20Tossell,%20C.%20C.,%20&amp;%20Ries,%20A.%20J.%20(2024).%20Towards%20an%20adaptive%20system%20for%20investigating%20human-agent%20teaming:%20Let%E2%80%99s%20get%20cooking!%20Systems%20and%20Information%20Engineering%20Design%20Symposium%20(SIEDS),2024,%201%E2%80%936.%2010.1109/SIEDS61124.2024.10534646" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR97">
<cite>Lu, Y., &amp; Sarter, N. (2019). Eye tracking: A process-oriented method for inferring trust in automation as a function of priming and
system reliability. <em>IEEE Transactions on Human-Machine Systems,</em><em>49</em>(6), 560–568. 10.1109/THMS.2019.2930980.</cite> [<a href="https://scholar.google.com/scholar_lookup?Lu,%20Y.,%20&amp;%20Sarter,%20N.%20(2019).%20Eye%20tracking:%20A%20process-oriented%20method%20for%20inferring%20trust%20in%20automation%20as%20a%20function%20of%20priming%20and%20system%20reliability.%20IEEE%20Transactions%20on%20Human-Machine%20Systems,49(6),%20560%E2%80%93568.%2010.1109/THMS.2019.2930980." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR61">
<cite>MacInnes, W., Hunt, A. R., Clarke, A. D. F., &amp; Dodd, M. D. (2018). A generative model of cognitive state from task and eye movements. <em>Cognitive Computation,</em><em>10</em>(5), 703–717. 10.1007/s12559-018-9558-9
</cite> [<a href="https://doi.org/10.1007/s12559-018-9558-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6367733/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30740186/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?MacInnes,%20W.,%20Hunt,%20A.%20R.,%20Clarke,%20A.%20D.%20F.,%20&amp;%20Dodd,%20M.%20D.%20(2018).%20A%20generative%20model%20of%20cognitive%20state%20from%20task%20and%20eye%20movements.%20Cognitive%20Computation,10(5),%20703%E2%80%93717.%2010.1007/s12559-018-9558-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR62">
<cite>Mallick, R., Slayback, D., Touryan, J., Ries, A. J., &amp; Lance, B. J. (2016). The use of eye metrics to index cognitive workload in video games. <em>IEEE Second Workshop on Eye Tracking and Visualization (ETVIS),</em><em>2016</em>, 60–64. 10.1109/ETVIS.2016.7851168</cite> [<a href="https://scholar.google.com/scholar_lookup?Mallick,%20R.,%20Slayback,%20D.,%20Touryan,%20J.,%20Ries,%20A.%20J.,%20&amp;%20Lance,%20B.%20J.%20(2016).%20The%20use%20of%20eye%20metrics%20to%20index%20cognitive%20workload%20in%20video%20games.%20IEEE%20Second%20Workshop%20on%20Eye%20Tracking%20and%20Visualization%20(ETVIS),2016,%2060%E2%80%9364.%2010.1109/ETVIS.2016.7851168" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR63"><cite>Marathe, A. R., Schaefer, K. E., Evans, A. W., spsampsps Metcalfe, J. S. (2018). Bidirectional Communication for Effective Human-Agent Teaming. In J. Y. C. Chen spsampsps G. Fragomeni (Eds.), <em>Virtual, Augmented and Mixed Reality: Interaction, Navigation, Visualization, Embodiment, and Simulation</em> (pp. 338–350). Springer International Publishing. 10.1007/978-3-319-91581-4_25</cite></li>
<li id="CR64">
<cite>Marshall, S. P. (2007). Identifying Cognitive State from Eye Metrics. <em>Aviation, Space, and Environmental Medicine,</em><em>78</em>(5), B165–B175.
</cite> [<a href="https://pubmed.ncbi.nlm.nih.gov/17547317/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Marshall,%20S.%20P.%20(2007).%20Identifying%20Cognitive%20State%20from%20Eye%20Metrics.%20Aviation,%20Space,%20and%20Environmental%20Medicine,78(5),%20B165%E2%80%93B175." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR65"><cite>Metcalfe, J. S., Perelman, B. S., Boothe, D. L., &amp; Mcdowell, K. (2021). Systemic Oversimplification Limits the Potential for Human-AI Partnership. <em>IEEE Access</em>, <em>9</em>, 70242–70260. IEEE Access. 10.1109/ACCESS.2021.3078298</cite></li>
<li id="CR66">
<cite>Nyström, M., &amp; Holmqvist, K. (2010). An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data. <em>Behavior Research Methods,</em><em>42</em>(1), 188–204. 10.3758/BRM.42.1.188
</cite> [<a href="https://doi.org/10.3758/BRM.42.1.188" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20160299/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Nystr%C3%B6m,%20M.,%20&amp;%20Holmqvist,%20K.%20(2010).%20An%20adaptive%20algorithm%20for%20fixation,%20saccade,%20and%20glissade%20detection%20in%20eyetracking%20data.%20Behavior%20Research%20Methods,42(1),%20188%E2%80%93204.%2010.3758/BRM.42.1.188" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR67">
<cite>Oommen, B. S., &amp; Stahl, J. S. (2005). Amplitudes of head movements during putative eye-only saccades. <em>Brain Research,</em><em>1065</em>(1), 68–78. 10.1016/j.brainres.2005.10.029
</cite> [<a href="https://doi.org/10.1016/j.brainres.2005.10.029" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16300748/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Oommen,%20B.%20S.,%20&amp;%20Stahl,%20J.%20S.%20(2005).%20Amplitudes%20of%20head%20movements%20during%20putative%20eye-only%20saccades.%20Brain%20Research,1065(1),%2068%E2%80%9378.%2010.1016/j.brainres.2005.10.029" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR68">
<cite>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., &amp; Vanderplas, J. (2011). Scikit-learn: Machine learning in python. <em>The Journal of Machine Learning Research.,</em><em>12</em>, 2825–2830.</cite> [<a href="https://scholar.google.com/scholar_lookup?Pedregosa,%20F.,%20Varoquaux,%20G.,%20Gramfort,%20A.,%20Michel,%20V.,%20Thirion,%20B.,%20Grisel,%20O.,%20Blondel,%20M.,%20Prettenhofer,%20P.,%20Weiss,%20R.,%20Dubourg,%20V.,%20&amp;%20Vanderplas,%20J.%20(2011).%20Scikit-learn:%20Machine%20learning%20in%20python.%20The%20Journal%20of%20Machine%20Learning%20Research.,12,%202825%E2%80%932830." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR69">
<cite>Pelz, J., Hayhoe, M., &amp; Loeber, R. (2001). The coordination of eye, head, and hand movements in a natural task. <em>Experimental Brain Research,</em><em>139</em>(3), 266–277. 10.1007/s002210100745
</cite> [<a href="https://doi.org/10.1007/s002210100745" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11545465/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Pelz,%20J.,%20Hayhoe,%20M.,%20&amp;%20Loeber,%20R.%20(2001).%20The%20coordination%20of%20eye,%20head,%20and%20hand%20movements%20in%20a%20natural%20task.%20Experimental%20Brain%20Research,139(3),%20266%E2%80%93277.%2010.1007/s002210100745" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR70">
<cite>Pettersson, K., Tervonen, J., Heininen, J., &amp; Mäntyjärvi, J. (2024). Head-area sensing in virtual reality: Future visions for visual perception and cognitive state estimation. <em>Frontiers in Virtual Reality</em>. 10.3389/frvir.2024.1423756</cite> [<a href="https://scholar.google.com/scholar_lookup?Pettersson,%20K.,%20Tervonen,%20J.,%20Heininen,%20J.,%20&amp;%20M%C3%A4ntyj%C3%A4rvi,%20J.%20(2024).%20Head-area%20sensing%20in%20virtual%20reality:%20Future%20visions%20for%20visual%20perception%20and%20cognitive%20state%20estimation.%20Frontiers%20in%20Virtual%20Reality.%2010.3389/frvir.2024.1423756" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR71">
<cite>Plamondon, R. (1995). A kinematic theory of rapid human movements: Part I. <em>Movement Representation and Generation. Biological Cybernetics,</em><em>72</em>(4), 295–307. 10.1007/BF00202785
</cite> [<a href="https://doi.org/10.1007/BF00202785" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/7748959/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Plamondon,%20R.%20(1995).%20A%20kinematic%20theory%20of%20rapid%20human%20movements:%20Part%20I.%20Movement%20Representation%20and%20Generation.%20Biological%20Cybernetics,72(4),%20295%E2%80%93307.%2010.1007/BF00202785" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR72"><cite>Pomplun, M., &amp; Sunkara, S. (2019). Pupil Dilation as an Indicator of Cognitive Workload in Human-Computer Interaction. In <em>Human-Centered Computing</em> (pp. 542–546). CRC Press.</cite></li>
<li id="CR73">
<cite>Pomplun, M., Garaas, T. W., &amp; Carrasco, M. (2013). The effects of task difficulty on visual search strategy in virtual 3D displays. <em>Journal of Vision,</em><em>13</em>(3), 24. 10.1167/13.3.24
</cite> [<a href="https://doi.org/10.1167/13.3.24" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3756766/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23986539/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Pomplun,%20M.,%20Garaas,%20T.%20W.,%20&amp;%20Carrasco,%20M.%20(2013).%20The%20effects%20of%20task%20difficulty%20on%20visual%20search%20strategy%20in%20virtual%203D%20displays.%20Journal%20of%20Vision,13(3),%2024.%2010.1167/13.3.24" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR74">
<cite>Reingold, E. M., &amp; Glaholt, M. G. (2014). Cognitive control of fixation duration in visual search: The role of extrafoveal processing. <em>Visual Cognition,</em><em>22</em>(3–4), 610–634. 10.1080/13506285.2014.881443</cite> [<a href="https://scholar.google.com/scholar_lookup?Reingold,%20E.%20M.,%20&amp;%20Glaholt,%20M.%20G.%20(2014).%20Cognitive%20control%20of%20fixation%20duration%20in%20visual%20search:%20The%20role%20of%20extrafoveal%20processing.%20Visual%20Cognition,22(3%E2%80%934),%20610%E2%80%93634.%2010.1080/13506285.2014.881443" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR75">
<cite>Reis, J., Cohen, Y., Melão, N., Costa, J., &amp; Jorge, D. (2021). High-tech defense industries: Developing autonomous intelligent systems. <em>Applied Sciences</em>. 10.3390/app11114920</cite> [<a href="https://scholar.google.com/scholar_lookup?Reis,%20J.,%20Cohen,%20Y.,%20Mel%C3%A3o,%20N.,%20Costa,%20J.,%20&amp;%20Jorge,%20D.%20(2021).%20High-tech%20defense%20industries:%20Developing%20autonomous%20intelligent%20systems.%20Applied%20Sciences.%2010.3390/app11114920" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR76"><cite>Salehi, M., Javadpour, N., Beisner, B., Sanaei, M., spsampsps Gilbert, S. B. (2024). Cybersickness Detection Through Head Movement Patterns: A Promising Approach. In H. Degen spsampsps S. Ntoa (Eds.), <em>Artificial Intelligence in HCI</em> (pp. 239–254). Springer Nature Switzerland. 10.1007/978-3-031-60611-3_18</cite></li>
<li id="CR77">
<cite>Schubert, T. W. (2003). The sense of presence in virtual environments: A three-component scale measuring spatial presence, involvement, and realness. <em>Z. Für Medienpsychologie,</em><em>15</em>(2), 69–71.</cite> [<a href="https://scholar.google.com/scholar_lookup?Schubert,%20T.%20W.%20(2003).%20The%20sense%20of%20presence%20in%20virtual%20environments:%20A%20three-component%20scale%20measuring%20spatial%20presence,%20involvement,%20and%20realness.%20Z.%20F%C3%BCr%20Medienpsychologie,15(2),%2069%E2%80%9371." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR78">
<cite>Scott, M., &amp; Su-In, L. (2017). A unified approach to interpreting model predictions. <em>Advances in Neural Information Processing Systems,</em><em>30</em>, 4765–4774.</cite> [<a href="https://scholar.google.com/scholar_lookup?Scott,%20M.,%20&amp;%20Su-In,%20L.%20(2017).%20A%20unified%20approach%20to%20interpreting%20model%20predictions.%20Advances%20in%20Neural%20Information%20Processing%20Systems,30,%204765%E2%80%934774." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR79">
<cite>Sidenmark, L., &amp; Gellersen, H. (2019). Eye, head and torso coordination during gaze shifts in virtual reality. <em>ACM Transactions on Computer-Human Interaction (TOCHI).,</em><em>27</em>(1), 1–40. 10.1145/3361218</cite> [<a href="https://scholar.google.com/scholar_lookup?Sidenmark,%20L.,%20&amp;%20Gellersen,%20H.%20(2019).%20Eye,%20head%20and%20torso%20coordination%20during%20gaze%20shifts%20in%20virtual%20reality.%20ACM%20Transactions%20on%20Computer-Human%20Interaction%20(TOCHI).,27(1),%201%E2%80%9340.%2010.1145/3361218" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR80">
<cite>Sidenmark, L., Prummer, F., Newn, J., &amp; Gellersen, H. (2023). Comparing gaze, head and controller selection of dynamically revealed targets in head-mounted displays. <em>IEEE Transactions on Visualization and Computer Graphics,</em><em>29</em>(11), 4740–4750. 10.1109/TVCG.2023.3320235
</cite> [<a href="https://doi.org/10.1109/TVCG.2023.3320235" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37782604/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Sidenmark,%20L.,%20Prummer,%20F.,%20Newn,%20J.,%20&amp;%20Gellersen,%20H.%20(2023).%20Comparing%20gaze,%20head%20and%20controller%20selection%20of%20dynamically%20revealed%20targets%20in%20head-mounted%20displays.%20IEEE%20Transactions%20on%20Visualization%20and%20Computer%20Graphics,29(11),%204740%E2%80%934750.%2010.1109/TVCG.2023.3320235" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR81">
<cite>Solman, G. J. F., &amp; Kingstone, A. (2014). Balancing energetic and cognitive resources: Memory use during search depends on the orienting effector. <em>Cognition,</em><em>132</em>(3), 443–454. 10.1016/j.cognition.2014.05.005
</cite> [<a href="https://doi.org/10.1016/j.cognition.2014.05.005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24946208/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Solman,%20G.%20J.%20F.,%20&amp;%20Kingstone,%20A.%20(2014).%20Balancing%20energetic%20and%20cognitive%20resources:%20Memory%20use%20during%20search%20depends%20on%20the%20orienting%20effector.%20Cognition,132(3),%20443%E2%80%93454.%2010.1016/j.cognition.2014.05.005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR82">
<cite>Stahl, J. S. (1999). Amplitude of human head movements associated with horizontal saccades. <em>Experimental Brain Research,</em><em>126</em>(1), 41–54. 10.1007/s002210050715
</cite> [<a href="https://doi.org/10.1007/s002210050715" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/10333006/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Stahl,%20J.%20S.%20(1999).%20Amplitude%20of%20human%20head%20movements%20associated%20with%20horizontal%20saccades.%20Experimental%20Brain%20Research,126(1),%2041%E2%80%9354.%2010.1007/s002210050715" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR83">
<cite>Stahl, J. S. (2001a). Adaptive plasticity of head movement propensity. <em>Experimental Brain Research,</em><em>139</em>(2), 201–208. 10.1007/s002210100749
</cite> [<a href="https://doi.org/10.1007/s002210100749" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11497062/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Stahl,%20J.%20S.%20(2001a).%20Adaptive%20plasticity%20of%20head%20movement%20propensity.%20Experimental%20Brain%20Research,139(2),%20201%E2%80%93208.%2010.1007/s002210100749" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR84">
<cite>Stahl, J. S. (2001b). Eye-head coordination and the variation of eye-movement accuracy with orbital eccentricity. <em>Experimental Brain Research,</em><em>136</em>(2), 200–210. 10.1007/s002210000593
</cite> [<a href="https://doi.org/10.1007/s002210000593" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11206282/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Stahl,%20J.%20S.%20(2001b).%20Eye-head%20coordination%20and%20the%20variation%20of%20eye-movement%20accuracy%20with%20orbital%20eccentricity.%20Experimental%20Brain%20Research,136(2),%20200%E2%80%93210.%2010.1007/s002210000593" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR85">
<cite>Tagu, J., &amp; Kristjánsson, Á. (2022). Dynamics of attentional and oculomotor orienting in visual foraging tasks. <em>Quarterly Journal of Experimental Psychology,</em><em>75</em>(2), 260–276. 10.1177/1747021820919351</cite> [<a href="https://doi.org/10.1177/1747021820919351" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32238034/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Tagu,%20J.,%20&amp;%20Kristj%C3%A1nsson,%20%C3%81.%20(2022).%20Dynamics%20of%20attentional%20and%20oculomotor%20orienting%20in%20visual%20foraging%20tasks.%20Quarterly%20Journal%20of%20Experimental%20Psychology,75(2),%20260%E2%80%93276.%2010.1177/1747021820919351" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR86">
<cite>Theeuwes, J. (1994). Stimulus-driven capture and attentional set: Selective search for color and visual abrupt onsets. <em>Journal of Experimental Psychology: Human Perception and Performance,</em><em>20</em>(4), 799–806. 10.1037/0096-1523.20.4.799
</cite> [<a href="https://doi.org/10.1037//0096-1523.20.4.799" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/8083635/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Theeuwes,%20J.%20(1994).%20Stimulus-driven%20capture%20and%20attentional%20set:%20Selective%20search%20for%20color%20and%20visual%20abrupt%20onsets.%20Journal%20of%20Experimental%20Psychology:%20Human%20Perception%20and%20Performance,20(4),%20799%E2%80%93806.%2010.1037/0096-1523.20.4.799" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR87">
<cite>Van Orden, K. F., Limbert, W., Makeig, S., &amp; Jung, T. P. (2001). Eye activity correlates of workload during a visuospatial memory task. <em>Human Factors,</em><em>43</em>(1), 111–121. 10.1518/001872001775992570
</cite> [<a href="https://doi.org/10.1518/001872001775992570" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11474756/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Van%20Orden,%20K.%20F.,%20Limbert,%20W.,%20Makeig,%20S.,%20&amp;%20Jung,%20T.%20P.%20(2001).%20Eye%20activity%20correlates%20of%20workload%20during%20a%20visuospatial%20memory%20task.%20Human%20Factors,43(1),%20111%E2%80%93121.%2010.1518/001872001775992570" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR88">
<cite>Velichkovsky, B. M., Rothert, A., Kopf, M., Dornhöfer, S. M., &amp; Joos, M. (2002). Towards an express-diagnostics for level of processing and hazard perception. <em>Transportation Research Part f: Traffic Psychology and Behaviour,</em><em>5</em>(2), 145–156. 10.1016/S1369-8478(02)00013-X</cite> [<a href="https://scholar.google.com/scholar_lookup?Velichkovsky,%20B.%20M.,%20Rothert,%20A.,%20Kopf,%20M.,%20Dornh%C3%B6fer,%20S.%20M.,%20&amp;%20Joos,%20M.%20(2002).%20Towards%20an%20express-diagnostics%20for%20level%20of%20processing%20and%20hazard%20perception.%20Transportation%20Research%20Part%20f:%20Traffic%20Psychology%20and%20Behaviour,5(2),%20145%E2%80%93156.%2010.1016/S1369-8478(02)00013-X" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR89">
<cite>Vickers, J. N., &amp; Lewinski, W. (2012). Performing under pressure: Gaze control, decision making and shooting performance of elite and rookie police officers. <em>Human Movement Science,</em><em>31</em>(1), 101–117. 10.1016/j.humov.2011.04.004
</cite> [<a href="https://doi.org/10.1016/j.humov.2011.04.004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21807433/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Vickers,%20J.%20N.,%20&amp;%20Lewinski,%20W.%20(2012).%20Performing%20under%20pressure:%20Gaze%20control,%20decision%20making%20and%20shooting%20performance%20of%20elite%20and%20rookie%20police%20officers.%20Human%20Movement%20Science,31(1),%20101%E2%80%93117.%2010.1016/j.humov.2011.04.004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR90">
<cite>Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., &amp; Van Der Walt, S. J. (2020). SciPy 1.0: Fundamental algorithms for scientific computing in Python. <em>Nature Methods.,</em><em>17</em>(3), 261–272. 10.1038/s41592-019-0686-2
</cite> [<a href="https://doi.org/10.1038/s41592-019-0686-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7056644/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32015543/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Virtanen,%20P.,%20Gommers,%20R.,%20Oliphant,%20T.%20E.,%20Haberland,%20M.,%20Reddy,%20T.,%20Cournapeau,%20D.,%20Burovski,%20E.,%20Peterson,%20P.,%20Weckesser,%20W.,%20Bright,%20J.,%20&amp;%20Van%20Der%20Walt,%20S.%20J.%20(2020).%20SciPy%201.0:%20Fundamental%20algorithms%20for%20scientific%20computing%20in%20Python.%20Nature%20Methods.,17(3),%20261%E2%80%93272.%2010.1038/s41592-019-0686-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR91">
<cite>Vlaskamp, B. N., Over, E. A., &amp; Hooge, I. T. (2005). Saccadic search performance: The effect of element spacing. <em>Experimental Brain Research.,</em><em>167</em>(2), 246–259. 10.1007/s00221-005-0032-z
</cite> [<a href="https://doi.org/10.1007/s00221-005-0032-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16078032/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Vlaskamp,%20B.%20N.,%20Over,%20E.%20A.,%20&amp;%20Hooge,%20I.%20T.%20(2005).%20Saccadic%20search%20performance:%20The%20effect%20of%20element%20spacing.%20Experimental%20Brain%20Research.,167(2),%20246%E2%80%93259.%2010.1007/s00221-005-0032-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR98">
<cite>Watson, M. R., Brennan, A. A., Kingstone, A., &amp; Enns, J. T. (2010). Looking versus seeing: Strategies alter eye movements during
visual search. <em>Psychonomic Bulletin &amp; Review,</em><em>17</em>(4), 543–549. 10.3758/PBR.17.4.543.
</cite> [<a href="https://doi.org/10.3758/PBR.17.4.543" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20702875/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Watson,%20M.%20R.,%20Brennan,%20A.%20A.,%20Kingstone,%20A.,%20&amp;%20Enns,%20J.%20T.%20(2010).%20Looking%20versus%20seeing:%20Strategies%20alter%20eye%20movements%20during%20visual%20search.%20Psychonomic%20Bulletin%20&amp;%20Review,17(4),%20543%E2%80%93549.%2010.3758/PBR.17.4.543." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR92">
<cite>Wolfe, J. M. (2013). When is it time to move to the next raspberry bush? Foraging rules in human visual search. <em>Journal of Vision,</em><em>13</em>(3), 10. 10.1167/13.3.10
</cite> [<a href="https://doi.org/10.1167/13.3.10" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4521330/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23641077/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wolfe,%20J.%20M.%20(2013).%20When%20is%20it%20time%20to%20move%20to%20the%20next%20raspberry%20bush?%20Foraging%20rules%20in%20human%20visual%20search.%20Journal%20of%20Vision,13(3),%2010.%2010.1167/13.3.10" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR93">
<cite>Yantis, S., &amp; Jonides, J. (1990). Abrupt visual onsets and selective attention: Voluntary versus automatic allocation. <em>Journal of Experimental Psychology. Human Perception and Performance,</em><em>16</em>(1), 121–134.
</cite> [<a href="https://doi.org/10.1037//0096-1523.16.1.121" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/2137514/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Yantis,%20S.,%20&amp;%20Jonides,%20J.%20(1990).%20Abrupt%20visual%20onsets%20and%20selective%20attention:%20Voluntary%20versus%20automatic%20allocation.%20Journal%20of%20Experimental%20Psychology.%20Human%20Perception%20and%20Performance,16(1),%20121%E2%80%93134." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR94">
<cite>Young, A. H., &amp; Hulleman, J. (2013). Eye movements reveal how task difficulty moulds visual search. <em>Journal of Experimental Psychology: Human Perception and Performance,</em><em>39</em>(1), 168–190. 10.1037/a0028679
</cite> [<a href="https://doi.org/10.1037/a0028679" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22642215/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Young,%20A.%20H.,%20&amp;%20Hulleman,%20J.%20(2013).%20Eye%20movements%20reveal%20how%20task%20difficulty%20moulds%20visual%20search.%20Journal%20of%20Experimental%20Psychology:%20Human%20Perception%20and%20Performance,39(1),%20168%E2%80%93190.%2010.1037/a0028679" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR95">
<cite>Zangemeister, W. H., Jones, A., &amp; Stark, L. (1981). Dynamics of head movement trajectories: Main sequence relationship. <em>Experimental Neurology,</em><em>71</em>(1), 76–91. 10.1016/0014-4886(81)90072-8
</cite> [<a href="https://doi.org/10.1016/0014-4886(81)90072-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/7449898/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Zangemeister,%20W.%20H.,%20Jones,%20A.,%20&amp;%20Stark,%20L.%20(1981).%20Dynamics%20of%20head%20movement%20trajectories:%20Main%20sequence%20relationship.%20Experimental%20Neurology,71(1),%2076%E2%80%9391.%2010.1016/0014-4886(81)90072-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR96">
<cite>Zangemeister, W. H., &amp; Stark, L. (1982). Gaze latency: Variable interactions of head and eye latency. <em>Experimental Neurology,</em><em>75</em>(2), 389–406. 10.1016/0014-4886(82)90169-8
</cite> [<a href="https://doi.org/10.1016/0014-4886(82)90169-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/7106221/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Zangemeister,%20W.%20H.,%20&amp;%20Stark,%20L.%20(1982).%20Gaze%20latency:%20Variable%20interactions%20of%20head%20and%20eye%20latency.%20Experimental%20Neurology,75(2),%20389%E2%80%93406.%2010.1016/0014-4886(82)90169-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ol></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adsm93_" lang="en" class="supplementary-materials"><h3 class="pmc_sec_title">Supplementary Materials</h3>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373606/bin/41235_2025_657_MOESM1_ESM.eps" data-ga-action="click_feat_suppl" class="usa-link">Additional file 1</a><sup> (1.5MB, eps) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373606/bin/41235_2025_657_MOESM2_ESM.eps" data-ga-action="click_feat_suppl" class="usa-link">Additional file 2</a><sup> (1.4MB, eps) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material3_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373606/bin/41235_2025_657_MOESM3_ESM.png" data-ga-action="click_feat_suppl" class="usa-link">Additional file 3</a><sup> (91.5KB, png) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material4_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373606/bin/41235_2025_657_MOESM4_ESM.xlsx" data-ga-action="click_feat_suppl" class="usa-link">Additional file 4</a><sup> (11.9KB, xlsx) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Cognitive Research: Principles and Implications are provided here courtesy of <strong>Springer</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1186/s41235-025-00657-y"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41235_2025_Article_657.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (1.8 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12373606/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12373606/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373606%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373606/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12373606/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12373606/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40846822/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12373606/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40846822/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12373606/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12373606/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="SDm7a171sCXxmGTkL59Z5WBEI0cCN9JQnXh013IBCYgyR3O6QsWhMpUJuR8Jusht">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
