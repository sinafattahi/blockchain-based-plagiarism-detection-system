
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Deep dictionary learning with reconstruction for texture recognition - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="AE53129E8AEF2D2305129E00071A6154.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12375791/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Deep dictionary learning with reconstruction for texture recognition">
<meta name="citation_author" content="Pengwen Xiong">
<meta name="citation_author_institution" content="School of Advanced Manufacturing, Nanchang University, Nanchang, 330031 China">
<meta name="citation_author_institution" content="Jiangxi Key Laboratory of Intelligent Robot, Nanchang University, Nanchang, 330031 China">
<meta name="citation_author" content="Ke Zhang">
<meta name="citation_author_institution" content="School of Advanced Manufacturing, Nanchang University, Nanchang, 330031 China">
<meta name="citation_author_institution" content="Jiangxi Key Laboratory of Intelligent Robot, Nanchang University, Nanchang, 330031 China">
<meta name="citation_author" content="Zhi Shi">
<meta name="citation_author_institution" content="School of Advanced Manufacturing, Nanchang University, Nanchang, 330031 China">
<meta name="citation_author_institution" content="Jiangxi Key Laboratory of Intelligent Robot, Nanchang University, Nanchang, 330031 China">
<meta name="citation_author" content="MengChu Zhou">
<meta name="citation_author_institution" content="Helen and John C. Hartmann Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, 07102 USA">
<meta name="citation_author" content="Aiguo Song">
<meta name="citation_author_institution" content="School of Instrument Science and Engineering, Southeast University, Nanjing, 210096 China">
<meta name="citation_publication_date" content="2025 Aug 25">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="31164">
<meta name="citation_doi" content="10.1038/s41598-025-16456-w">
<meta name="citation_pmid" content="40850961">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12375791/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12375791/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12375791/pdf/41598_2025_Article_16456.pdf">
<meta name="description" content="Texture recognition underpins critical applications in industrial quality control, robotic manipulation, and biomedical imaging. Traditional deep dictionary learning methods for texture recognition often emphasize deep feature extraction. However, ...">
<meta name="og:title" content="Deep dictionary learning with reconstruction for texture recognition">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Texture recognition underpins critical applications in industrial quality control, robotic manipulation, and biomedical imaging. Traditional deep dictionary learning methods for texture recognition often emphasize deep feature extraction. However, ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12375791/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12375791">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-16456-w"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_16456.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12375791%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12375791/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12375791/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12375791/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 25;15:31164. doi: <a href="https://doi.org/10.1038/s41598-025-16456-w" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-16456-w</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Deep dictionary learning with reconstruction for texture recognition</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Xiong%20P%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Pengwen Xiong</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Pengwen Xiong</span></h3>
<div class="p">
<sup>1</sup>School of Advanced Manufacturing, Nanchang University, Nanchang, 330031 China </div>
<div class="p">
<sup>2</sup>Jiangxi Key Laboratory of Intelligent Robot, Nanchang University, Nanchang, 330031 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Xiong%20P%22%5BAuthor%5D" class="usa-link"><span class="name western">Pengwen Xiong</span></a>
</div>
</div>
<sup>1,</sup><sup>2,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20K%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Ke Zhang</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Ke Zhang</span></h3>
<div class="p">
<sup>1</sup>School of Advanced Manufacturing, Nanchang University, Nanchang, 330031 China </div>
<div class="p">
<sup>2</sup>Jiangxi Key Laboratory of Intelligent Robot, Nanchang University, Nanchang, 330031 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20K%22%5BAuthor%5D" class="usa-link"><span class="name western">Ke Zhang</span></a>
</div>
</div>
<sup>1,</sup><sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Shi%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Zhi Shi</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Zhi Shi</span></h3>
<div class="p">
<sup>1</sup>School of Advanced Manufacturing, Nanchang University, Nanchang, 330031 China </div>
<div class="p">
<sup>2</sup>Jiangxi Key Laboratory of Intelligent Robot, Nanchang University, Nanchang, 330031 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Shi%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zhi Shi</span></a>
</div>
</div>
<sup>1,</sup><sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhou%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">MengChu Zhou</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">MengChu Zhou</span></h3>
<div class="p">
<sup>3</sup> Helen and John C. Hartmann Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, 07102 USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhou%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">MengChu Zhou</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Song%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Aiguo Song</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Aiguo Song</span></h3>
<div class="p">
<sup>4</sup>School of Instrument Science and Engineering, Southeast University, Nanjing, 210096 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Song%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Aiguo Song</span></a>
</div>
</div>
<sup>4</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>School of Advanced Manufacturing, Nanchang University, Nanchang, 330031 China </div>
<div id="Aff2">
<sup>2</sup>Jiangxi Key Laboratory of Intelligent Robot, Nanchang University, Nanchang, 330031 China </div>
<div id="Aff3">
<sup>3</sup> Helen and John C. Hartmann Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, 07102 USA </div>
<div id="Aff4">
<sup>4</sup>School of Instrument Science and Engineering, Southeast University, Nanjing, 210096 China </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Feb 11; Accepted 2025 Aug 14; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12375791  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40850961/" class="usa-link">40850961</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Texture recognition underpins critical applications in industrial quality control, robotic manipulation, and biomedical imaging. Traditional deep dictionary learning methods for texture recognition often emphasize deep feature extraction. However, they tend to lose crucial features as model depth increases, which can reduce their overall effectiveness. To address this issue, we propose a dictionary-reconstruction-based deep learning approach by incorporating a novel hybrid fusion method designed to enhance the accuracy of texture recognition. Our approach involves the successive fusion of multimodality and multi-level features. By reconstructing dictionaries learned at different levels, we integrate both deep and intuitive features. Additionally, we introduce a grouping optimization technique, based on single-sample learning, to train these reconstructed dictionaries, thereby improving feature learning and training efficiency. The proposed approach fuses feature data from various multimodal sources and constructs dictionaries at different learning levels, which enables effective feature fusion across these levels. We evaluate our approach against recent deep learning methods by using the LMT-108 and SpectroVision datasets. The results demonstrate its 97.7% and 89.4% accuracy rates, respectively, outperforming its peers and validating its robustness when handling diverse and challenging data.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Feature fusion, Dictionary reconstruction, Deep dictionary learning, Texture recognition</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Electrical and electronic engineering, Mechanical engineering</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Texture is a fundamental attribute of objects<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>. Recognizing fine-grained textures is essential across multiple domains: in manufacturing lines, automated inspection systems must detect minute surface defects to prevent product failures<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>; in service and industrial robotics, discerning surface roughness and material properties informs safe and adaptive grasping strategies<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>; and in medical diagnostics, texture analysis of histological images aids in early disease detection<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>. Texture recognition, therefore, plays a crucial role in various applications, and significant advancements have been made in this field. Traditional approaches, such as single-layer dictionary learning, can capture shallow features but often overlook deeper, more effective features. On the other hand, deep learning techniques require large datasets and significant computational resources. This has led to a growing interest in combining dictionary learning<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a>,<a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup> and deep learning<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a>–<a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup> to address texture recognition challenges.</p>
<p id="Par3">Dictionary learning is a form of matrix decomposition. It extracts essential features from data, making it effective for recognition<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a>,<a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup> and data clustering<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>. By leveraging the robust learning and representation capabilities of deep learning, researchers have started integrating it with dictionary learning. Tariyal et al.<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup> pioneered this by proposing a multi-level dictionary learning approach, which iteratively trains each layer to converge, enabling the extraction of deeper features from the original data. This concept laid the groundwork for deep dictionary learning. A variety of methods have been developed since then. Building upon the K-singular value decomposition (K-SVD), Montazeri et al.<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup> introduced a dictionary learning approach termed "multilayered K-singular value decomposition (MLK-SVD)" which serves as a multi-tiered classification technique. Tang et al.<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup> presented a novel deep dictionary learning coding network that integrates neural networks with dictionary learning for image recognition tasks, particularly in scenarios with limited data availability. The model incorporates most of the standard deep learning layers (e.g., input/output, pooling, and fully connected layers), but it replaces the conventional convolutional layer with a compound dictionary learning and coding layer. Zhang et al.<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup> designed a dictionary pair learning network, which integrated the end-to-end network with jump connections and multi-layer deep sparse dictionary learning into a unified architecture.</p>
<p id="Par4">The above-mentioned deep dictionary learning models all emphasize the extraction of deep-level features. In contrast to single-layer dictionary learning, the features extracted from deeper layers are often referred to as hidden features. In the process of layer-by-layer training, it is equivalent to the extraction of hidden features and the elimination of some features, which may cause low utilization of data. Therefore, it is necessary to improve the effective use of data. The proposed method fuses shallow and deep features through a novel dictionary reconstruction process, preventing the loss of key information, and realize the ability of superior recognition accuracy.</p>
<p id="Par5">Texture recognition often involves multimodal data, such as visual, tactile, and acceleration information, which provides complementary insights into surface characteristics that are difficult to capture with a single modality. This combination of modalities makes texture recognition a complex, yet promising multimodal fusion problem, where integrating information from different sources can significantly enhance classification accuracy. Researchers have explored various fusion strategies to address this challenge<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. Feature-level (or early) fusion combines data from different modalities at the initial stages, maximizing the feature set but often requiring more complex preprocessing. In contrast, decision-level (or late) fusion integrates the outputs of individual classifiers, making it more adaptable to real-time applications. Mixed fusion approaches, which blend early and late fusion strategies, have also shown promise by balancing computational cost and accuracy. This study introduces a new method for feature fusion in texture recognition, which can create a more comprehensive feature representation by integrating features from all levels of the network. The method first performs early fusion of multimodal data to form rich initial inputs. Then a new dictionary reconstruction stage is introduced, in which the dictionaries learned at each level are combined.</p>
<p id="Par6">The development of tactile sensors has furthered research in visual-tactile data fusion, with studies proposing various models and frameworks for effective multimodal integration. These include a hybrid joint group kernel sparse coding model for hybrid fusion<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>, a visual-tactile fusion framework for object recognition<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>, and continuous learning frameworks for multimodal integration<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>. Tactile sensors, particularly through measurements from accelerometers or pressure sensors, are also applied in texture recognition<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a>–<a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. These approaches have demonstrated notable success in object and texture recognition, especially in applications where the availability of training data is limited.</p>
<p id="Par7">In this context, the need for a robust and efficient multimodal fusion approach becomes apparent. Existing deep learning models like Multilayer Perceptrons (MLP) show potential in texture recognition but require extensive computational resources and large datasets, which limits their applicability in resource-constrained environments<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>. In contrast, dictionary learning is more efficient in capturing essential texture features. However, traditional methods like K-SVD<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup> struggle to extract deep-level features, resulting in subpar classification performance. Current methods combining deep learning with dictionary learning<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a>–<a href="#CR16" class="usa-link" aria-describedby="CR16">16</a>,<a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup> often prioritize deep features, neglecting some effective features as the model depth increases.</p>
<p id="Par8">The primary motivation for this work stems from the observation that while deep learning models excel at extracting abstract, high-level features, they often do so at the cost of discarding valuable, low-level textural details learned in the initial layers. This trade-off is a critical bottleneck that limits performance. Our Dictionary-Reconstruction-based Deep Learning (DRDL) method is designed to overcome this limitation. Unlike traditional deep dictionary learning, which only trains dense features for subsequent layers, our approach aims to retain and fully utilize both shallow and deep features. The method involves a multi-stage fusion process: first, multimodal fusion based on feature matching is employed to obtain accurate and comprehensive features. Next, we learn sparse coding and dense feature matrices across different dictionary layers. Instead of simple multiplication, we fuse these features at all levels, ensuring a more comprehensive utilization of learned features, as shown in Fig. <a href="#Fig1" class="usa-link">1</a>.</p>
<figure class="fig xbox font-sm" id="Fig1"><h3 class="obj_head">Fig. 1.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12375791_41598_2025_16456_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/6e0c4c96a481/41598_2025_16456_Fig1_HTML.jpg" loading="lazy" id="MO2" height="377" width="667" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>DRDL model. The data of the three modes were divided into training set and testing set after feature extraction and reduction. The training was divided into three stages: layer-by-layer training stage, reconstruction stage, and overall loss fine-tuning stage. In the right part of the figure, the dotted line separates the layer-by-layer training stage from the dictionary reconstruction stage and fine-tuning stage.</p></figcaption></figure><p id="Par9">This work makes several key contributions to the field of texture recognition: </p>
<ol class="list" style="list-style-type:decimal">
<li><p id="Par10">Proposing a new model that enhances classification ability through successive fusion of features, starting with early fusion of multimodal features and followed by multi-level feature fusion.</p></li>
<li><p id="Par11">Introducing a dictionary reconstruction method that combines deep and intuitive features by reconstructing dictionaries across different levels, thereby improving learning performance.</p></li>
<li><p id="Par12">Developing a grouping optimization method based on single-sample training to enhance the effectiveness of feature learning and dictionary reconstruction.</p></li>
<li><p id="Par13">Conducting extensive experiments and comparative studies with state-of-the-art methods on two challenging texture recognition benchmarks, the LMT-108 and the SpectroVision, to validate the proposed approach.</p></li>
</ol>
<p>The following is a summary of the remainder of this paper. Section “Related work” briefs related work and presents the motivation of this work. Section “Methodology” introduces the proposed method. Section "Experimental process and result analysis" gives the experimental results and their analysis. Section “Conclusion” concludes this paper.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Related work</h2>
<section id="Sec3"><h3 class="pmc_sec_title">Texture classification</h3>
<p id="Par14">Traditional texture classification methods typically rely on a combination of manual feature extraction and machine learning. Common approaches include the gray level co-occurrence matrix (GLCM)<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>, which examines spatial relationships between pixel intensities to form textures. Gabor filters are also widely used for texture representation, especially in texture classification and segmentation. Selvaraj et al.<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup> and Han et al.<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup> applied Gabor filters to texture images, calculating the mean and variance of filter coefficients for different orientations and scales as features for classification.</p>
<p id="Par15">Another popular method is the local binary pattern (LBP), valued for its computational simplicity and robustness to illumination changes. Ojala et al.<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> introduced the LBP algorithm for encoding local neighborhood features of texture images, with these encoded histograms serving as classification features. Shu et al.<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup> advanced this by developing the Local Sorted Difference Refined Pattern (LSDRP), which improved feature representation by ordering local neighborhood points and calculating their differences.</p>
<p id="Par16">While effective, traditional texture recognition methods<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a>–<a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> that rely on surface-level image features struggle with complex and varied textures, limiting their accuracy and efficiency. In recent years, deep learning has advanced at an unprecedented pace and found applications across a wide range of tasks<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a>,<a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>. These developments have naturally extended to texture recognition, where convolutional and graph-based networks learn hierarchical and relational texture features more effectively than hand-crafted descriptors<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a>,<a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>. However, directly applying such models to multimodal tactile–visual texture data remains challenging, motivating our proposed deep dictionary–driven reconstruction method.</p>
<p id="Par17">To address these limitations, recent advances have focused on deep learning methods, which have shown remarkable success in texture analysis by automatically learning hierarchical feature representations. For instance, a study<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup> introduced a novel texture feature descriptor using convolutional neural network (CNN) filter banks, demonstrating improved texture classification performance. Another study<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup> developed the locally shifted Fisher vector (LFV) method, enhancing feature learning by utilizing locally shared filters in combination with CNNs. Further advancements include the application of global pooling CNNs (GP-CNN)<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>, which effectively leverage features from different depth levels of a pre-trained model to obtain high-quality texture information. A ranking-based method, Rank GP-CNN, was also proposed to automatically select and calculate texture feature vectors, enhancing classification accuracy. Finally, a multilevel texture coding and representation network<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup> was developed, which fused features from different levels to preserve texture details and spatial information, further improving classification outcomes.</p></section><section id="Sec4"><h3 class="pmc_sec_title">Dictionary learning</h3>
<p id="Par18">Sparse representation theory has proven highly effective in feature extraction and automatic learning, with the quality of the dictionary being crucial to the success of sparse coding. Dictionary learning aims to learn an overcomplete dictionary that approximates the original signal through a linear combination of dictionary atoms<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>. The K-SVD algorithm<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>, proposed by Aharon et al., is fundamental to this process, iteratively optimizing dictionary atoms to minimize reconstruction error.</p>
<p id="Par19">As application scenarios have become more complex, dictionary learning methods have evolved. For example, a novel Online Convolutional Dictionary Learning (OCDL) algorithm<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup> extends traditional patch-based online dictionary learning by updating the dictionary in a memory-efficient manner.</p>
<p id="Par20">Dictionary learning is closely linked with sparse representation, involving two key phases: dictionary update and sparse coding. The goal is to represent the original data by combining dictionary atoms, typically within an overcomplete dictionary where the number of atoms exceeds the data’s dimensionality. Sparse regularization, often achieved by minimizing the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/b4b1c0215af0/41598_2025_16456_Article_IEq1.gif" loading="lazy" alt="Inline graphic"></span> norm, ensures that the coding matrix remains sparse, thereby improving the representation’s efficiency.</p>
<p id="Par21">Deep learning, inspired by the brain’s cognitive processes, has further enhanced dictionary learning. By integrating deep feature extraction with dictionary learning, new models have emerged that combine the strengths of both approaches, leading to more accurate and robust feature representations<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a>,<a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup>.</p></section></section><section id="Sec5"><h2 class="pmc_sec_title">Methodology</h2>
<section id="Sec6"><h3 class="pmc_sec_title">Task formulation</h3>
<p id="Par22">This paper presents the DRDL method to perform a texture recognition task. Based on the feature matching method, the texture features of different modalities are fused. They are then used as the input of the first layer of dictionary learning to learn the dictionary matrix, sparse representation matrix and dense feature matrix. The dense feature matrix learned from the first layer is used as the input of the second layer to learn the dictionary matrix, sparse representation matrix and dense feature matrix. That learned from the second layer is used as the input of the third layer to learn the dictionary matrix and sparse representation matrix. The third layer introduces an <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/4ca879f7a0bc/41598_2025_16456_Article_IEq2.gif" loading="lazy" alt="Inline graphic"></span>-norm regularization term to reduce the variance among similar sample features. The learned dictionary is reconstructed, i.e., the features learned at different layers, and the sparse representation matrices learned at different levels are fused. In other words, shallow features and deep ones are fused. According to the reconstructed dictionary obtained by training, the sparse representation matrix of the test set is learned, and the test set label is obtained according to the cosine similarity principle to complete the texture recognition task.</p></section><section id="Sec7"><h3 class="pmc_sec_title">Deep dictionary learning model</h3>
<p id="Par23">Both deep learning and dictionary learning have achieved success in many fields. We propose a Deep Dictionary Learning method based on Dictionary Reconstruction which involves three phases: 1) a pre-training phase to learn multi-level dictionary matrices and features; 2) a dictionary reconstruction phase where the learned matrices are fused into a new, comprehensive dictionary; and 3) a fine-tuning phase to optimize the fused dictionary and generate the final sparse representation.</p>
<section id="Sec8"><h4 class="pmc_sec_title">Pre-training phase</h4>
<p id="Par24">Deep dictionary learning plays a critical role in dictionary pre-training. Taking a three-layer architecture as an example, each layer can be regarded as an independent shallow dictionary learning module. In the deep dictionary learning framework, feature extraction is performed in a layer-wise manner: the features learned from the first layer serve as the input to the second layer, and the output of the second layer is further used as the input to the final layer.</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/bf5caf91247a/41598_2025_16456_Article_Equ1.gif" loading="lazy" height="64" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ1.gif"></td>
<td class="label">1</td>
</tr></table>
<p>where <em>X</em> represents the data to be represented, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/b47a8d115ddd/41598_2025_16456_Article_IEq3.gif" loading="lazy" alt="Inline graphic"></span> represent the data of three modalities, respectively. <em>d</em> represents the data dimension, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/3c45bb21ada6/41598_2025_16456_Article_IEq4.gif" loading="lazy" alt="Inline graphic"></span> represents the total number of samples, <em>n</em> represents the number of classes, and <em>N</em> represents the number of samples in one class. In the following formula <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq5"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/fee21f47a1ab/41598_2025_16456_Article_IEq5.gif" loading="lazy" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq6"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/0b84c98dc4fd/41598_2025_16456_Article_IEq6.gif" loading="lazy" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq7"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/cde3fcc1682e/41598_2025_16456_Article_IEq7.gif" loading="lazy" alt="Inline graphic"></span> represent the dictionary matrix of each layer respectively, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq8"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/63ed949d8d1a/41598_2025_16456_Article_IEq8.gif" loading="lazy" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq9"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/f0109c29991d/41598_2025_16456_Article_IEq9.gif" loading="lazy" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq10"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/8b5b3b44e5fc/41598_2025_16456_Article_IEq10.gif" loading="lazy" alt="Inline graphic"></span> represent the corresponding coding vectors of each layer respectively. For the first layer, our goal is to train the corresponding dictionary matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq11"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/fee21f47a1ab/41598_2025_16456_Article_IEq5.gif" loading="lazy" alt="Inline graphic"></span> and coding matrix (vector) to represent data <em>X</em>, and the corresponding representation is:</p>
<table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/b3dd785cdbb2/41598_2025_16456_Article_Equ2.gif" loading="lazy" height="44" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ2.gif"></td>
<td class="label">2</td>
</tr></table>
<p>In the training phase, add the corresponding regularization term to the coding matrix (vector) in Eq. (<a href="#Equ2" class="usa-link">2</a>). We then have the following goal:</p>
<table class="disp-formula p" id="Equ3"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/847617012323/41598_2025_16456_Article_Equ3.gif" loading="lazy" height="33" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ3.gif"></td>
<td class="label">3</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq12"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/084c313c5f5e/41598_2025_16456_Article_IEq12.gif" loading="lazy" alt="Inline graphic"></span> is the coefficient of sparse term. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq13"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/ab991be1e92c/41598_2025_16456_Article_IEq13.gif" loading="lazy" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq14"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/86bc70cf36ae/41598_2025_16456_Article_IEq14.gif" loading="lazy" alt="Inline graphic"></span> represent the dictionary matrix and sparse coding matrix (vector) of the first layer, respectively. The content of the dictionary matrix calculated here is different from that of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq15"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/fee21f47a1ab/41598_2025_16456_Article_IEq5.gif" loading="lazy" alt="Inline graphic"></span>. When the original data is represented by the dictionary matrix, the corresponding coding matrix (vector) is sparse.</p>
<p id="Par25">The above formula realizes the sparse processing of the coding matrix (vector), achieves the purpose of sparsity of the original data features, which is equivalent to eliminating some redundant attributes in the original data. In our model, except for the last level where only sparse features need to be learned, the other layers need to learn dense features and sparse features, respectively. Starting from the second layer, the dense features learned from the previous layer are used as input to the next layer. The dense coding vector retains the characteristics of the original data to some extent and refines the original data. It can be regarded as the low dimensional representation of the original data mapped to the corresponding dimension (generally lower dimension) through the dictionary matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq16"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/fee21f47a1ab/41598_2025_16456_Article_IEq5.gif" loading="lazy" alt="Inline graphic"></span>. Therefore, the two loss functions of the second layer are as follows:</p>
<table class="disp-formula p" id="Equ4"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/da360104c1a6/41598_2025_16456_Article_Equ4.gif" loading="lazy" height="60" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ4.gif"></td>
<td class="label">4</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq17"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/55dde96a74e6/41598_2025_16456_Article_IEq17.gif" loading="lazy" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq18"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/33b2f11cd50a/41598_2025_16456_Article_IEq18.gif" loading="lazy" alt="Inline graphic"></span> represents the corresponding dictionary matrix and coding sparse matrix (vector) respectively.</p>
<p id="Par26">The loss function for the third layer is as follows:</p>
<table class="disp-formula p" id="Equ5"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/86de0247b9f2/41598_2025_16456_Article_Equ5.gif" loading="lazy" height="30" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ5.gif"></td>
<td class="label">5</td>
</tr></table>
<p>Sparse regularization terms are added at the last layer to eliminate redundant features.</p>
<p id="Par27">In the process of three-layer dictionary training, the original data from different levels is represented in low dimension. Compared to single-layer dictionary learning, deeper dictionary matrices can be learned by multi-layer dictionary learning models, resulting in a deeper representation of the original data. In other words, through different levels of dictionary learning model, the potential representation of data can be learned from different perspectives. Taking face recognition as an example, the corresponding projection matrix as well as the features can be learned from the pose perspective by the first layer dictionary learning model. The projection matrix is the dictionary matrix in the model, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq19"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/eba74f380ecc/41598_2025_16456_Article_IEq19.gif" loading="lazy" alt="Inline graphic"></span>. Similarly, the corresponding projection matrix as well as features can be learned from the viewpoint of expressions by dictionary learning models of the second layer. Dictionary matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq20"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/54a4b37c1988/41598_2025_16456_Article_IEq20.gif" loading="lazy" alt="Inline graphic"></span> takes the form: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq21"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/e061f0da2a26/41598_2025_16456_Article_IEq21.gif" loading="lazy" alt="Inline graphic"></span>. In the third layer, we learn the corresponding projection matrix through the model from an identity perspective. Dictionary matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq22"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/592b6c42db4c/41598_2025_16456_Article_IEq22.gif" loading="lazy" alt="Inline graphic"></span> takes the form: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq23"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/d5bb54b23698/41598_2025_16456_Article_IEq23.gif" loading="lazy" alt="Inline graphic"></span>. Although machine learning features can be difficult to interpret, the projection matrices learned at different levels can project the original data into different attribute spaces. The whole model uses three levels of dictionary learning to represent data from different levels. The projection matrix of the corresponding attribute can be trained by representing the data from different angles, in which the corresponding dictionary matrix is trained from different levels. The meaning of the basis vectors in these dictionaries is also different.</p></section><section id="Sec9"><h4 class="pmc_sec_title">Dictionary reconstruction phase</h4>
<p id="Par28">In this phase, the dictionaries learned in the pre-training stage are integrated to reconstruct the dictionaries. The process is described as follows:</p>
<table class="disp-formula p" id="Equ6"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/3b17ef33b729/41598_2025_16456_Article_Equ6.gif" loading="lazy" height="105" width="139" alt="graphic file with name 41598_2025_16456_Article_Equ6.gif"></td>
<td class="label">6</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq24"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/ea93277e5e69/41598_2025_16456_Article_IEq24.gif" loading="lazy" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq25"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/54a4b37c1988/41598_2025_16456_Article_IEq20.gif" loading="lazy" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq26"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/592b6c42db4c/41598_2025_16456_Article_IEq22.gif" loading="lazy" alt="Inline graphic"></span> represents the dictionary matrix of three layers, respectively. <em>D</em> represents the dictionary matrix after fusion, and it retains the dictionary matrix learned from different levels. Therefore, the fusion of features vectors learned at each level is realized in the final data representation. The detailed process is shown in Fig. <a href="#Fig2" class="usa-link">2</a>. The blue and yellow solid line arrows represent the solving process of sparse and dense features, respectively.</p>
<figure class="fig xbox font-sm" id="Fig2"><h5 class="obj_head">Fig. 2.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12375791_41598_2025_16456_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/88aafc168e6e/41598_2025_16456_Fig2_HTML.jpg" loading="lazy" id="MO3" height="657" width="667" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The process of dictionary reconstruction. Blue background boxes represent two different ways of dictionary reconstruction.</p></figcaption></figure></section><section id="Sec10"><h4 class="pmc_sec_title">Fine-tuning phase</h4>
<p id="Par29">After the two steps above, dictionaries at various levels are obtained through training and the dictionaries trained at each layer have been fused. Then, according to Eq.(<a href="#Equ7" class="usa-link">7</a>), the dictionary matrix <em>D</em> and coding matrix (vector) in the objective function are adjusted by alternating minimization method to reduce the total reconstruction error of the model.</p>
<table class="disp-formula p" id="Equ7"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/9c941e8d6d0e/41598_2025_16456_Article_Equ7.gif" loading="lazy" height="21" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ7.gif"></td>
<td class="label">7</td>
</tr></table>
<p>where <em>L</em> represents the loss function, and <em>Z</em> represents the sparse coding matrix (vector).</p></section></section><section id="Sec11"><h3 class="pmc_sec_title">Optimization algorithm</h3>
<p id="Par30">In the pre-training stage, we adopt the neural network layer-by-layer training method to update and optimize the parameters. Since the optimization functions for each layer are non-convex and cannot be solved directly, an optimal solution is approached through multiple iterations using a traditional dictionary learning method. This is achieved by alternately fixing the dictionary matrix and the coding matrix to update the respective optimal solutions.</p>
<section id="Sec12"><h4 class="pmc_sec_title">Layer-by-layer training</h4>
<p id="Par31">The solution of all features and corresponding dictionary matrices can be solved by alternating iterations. Take the first layer as an example to illustrate the solution methods for dense and sparse features and corresponding dictionary matrices, respectively.</p>
<p id="Par32">In the first layer, the original data is taken as input, and the corresponding dictionary matrices and dense coding matrices (vectors) are solved by dictionary learning. The specific implementation is to minimize performing the following minimization by fixing the dictionary matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq27"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/fee21f47a1ab/41598_2025_16456_Article_IEq5.gif" loading="lazy" alt="Inline graphic"></span> and coding matrix (vector) <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq28"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/63ed949d8d1a/41598_2025_16456_Article_IEq8.gif" loading="lazy" alt="Inline graphic"></span> alternately:</p>
<table class="disp-formula p" id="Equ8"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/22db695a6386/41598_2025_16456_Article_Equ8.gif" loading="lazy" height="31" width="140" alt="graphic file with name 41598_2025_16456_Article_Equ8.gif"></td>
<td class="label">8</td>
</tr></table>
<p>Similarly, performing the following minimization by alternating the fixed dictionary matrix and the coding matrix (vector) eliminates the corresponding sparse features and the optimal solution of the dictionary:</p>
<table class="disp-formula p" id="Equ9"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/847617012323/41598_2025_16456_Article_Equ9.gif" loading="lazy" height="33" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ9.gif"></td>
<td class="label">9</td>
</tr></table>
<figure class="fig xbox font-sm" id="Figa"><h5 class="obj_head">Algorithm 1.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12375791_41598_2025_16456_Figa_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/a29fcf09990b/41598_2025_16456_Figa_HTML.jpg" loading="lazy" id="MO1" height="184" width="669" alt="Algorithm 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Figa/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Solve <em>D</em> by Adagrad</p></figcaption></figure><p id="Par34">where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq29"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/4ded82fca005/41598_2025_16456_Article_IEq29.gif" loading="lazy" alt="Inline graphic"></span>represents the regular coefficient, which means to find a norm for all columns and sum the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq30"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/b4b1c0215af0/41598_2025_16456_Article_IEq30.gif" loading="lazy" alt="Inline graphic"></span> norm of all columns. Using <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq31"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/b4b1c0215af0/41598_2025_16456_Article_IEq30.gif" loading="lazy" alt="Inline graphic"></span> norm as a regularization term can gain the sparse representation<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup>.</p></section><section id="Sec13"><h4 class="pmc_sec_title">Fine-tuning</h4>
<p id="Par35">After all layers’ training is completed, the dictionaries are reconstructed and fused. To reduce the error after reconstruction, the dictionary matrix is fine-tuned by minimizing the loss function Eq.(<a href="#Equ3" class="usa-link">3</a>), and the corresponding encoding matrix (vector) is solved. For dictionary update method, dictionary matrix <em>D</em> is updated by fixing <em>Z</em> and using gradient descent method. The specific algorithm is shown in Algorithm 1, where t represents the number of iterations. The solution of coding matrix (vector) needs to fix dictionary matrix <em>D</em>, which can be solved by Eq.(<a href="#Equ10" class="usa-link">10</a>). The iterative procedure in Algorithm 1 terminates when the relative change in the reconstruction error <em>L</em> (as defined in Eq. (<a href="#Equ7" class="usa-link">7</a>)), between successive iterations falls below a predefined tolerance <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq32"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/badfbec476e9/41598_2025_16456_Article_IEq32.gif" loading="lazy" alt="Inline graphic"></span>, or when a preset maximum number of iterations is reached.</p>
<p id="Par36">Compared with the traditional sparse coding solution, an inter- class regular term is introduced as follows:</p>
<table class="disp-formula p" id="Equ10"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/3181a035a5c8/41598_2025_16456_Article_Equ10.gif" loading="lazy" height="21" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ10.gif"></td>
<td class="label">10</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq33"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/e1dcc285c8de/41598_2025_16456_Article_IEq33.gif" loading="lazy" alt="Inline graphic"></span> represents another form of original data. <em>u</em> is the coefficient of the inter-class regular term. Inter-class regular is realized by <em>1, 2-norm</em>. The process is to solve the 1-norm for each row of vector of matrix <em>Z</em>, and then solve the <em>2-norm</em> for the vector composed of all <em>1-norm</em>. Its purpose is to maximize the distance between samples of different classes.</p>
<p id="Par37">Due to the introduction of inter-class regular term, it is necessary to divide the original data into several groups to solve its sparse representation. If there is only one sample for each class of the original sample <em>X</em>, then <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq34"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/404b6daf9054/41598_2025_16456_Article_IEq34.gif" loading="lazy" alt="Inline graphic"></span>, and <em>S</em> is the same as the original sample. If there are multiple samples in each class, the original samples should be divided into <em>n</em>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq35"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/299279d26b2f/41598_2025_16456_Article_IEq35.gif" loading="lazy" alt="Inline graphic"></span> groups according to the principle of selecting one sample from each class, and the sparse coding is solved respectively.</p>
<p id="Par38">Combine each set of encoding matrices solved in groups by column into a single encoding matrix. The grouped sample <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq36"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/e1dcc285c8de/41598_2025_16456_Article_IEq33.gif" loading="lazy" alt="Inline graphic"></span> and the optimal solution of sparse coding are: </p>
<table class="disp-formula p" id="Equ11"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/979bcba0a64a/41598_2025_16456_Article_Equ11.gif" loading="lazy" height="80" width="193" alt="graphic file with name 41598_2025_16456_Article_Equ11.gif"></td>
<td class="label">11</td>
</tr></table>
<table class="disp-formula p" id="Equ12"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/32b0731aabcd/41598_2025_16456_Article_Equ12.gif" loading="lazy" height="80" width="169" alt="graphic file with name 41598_2025_16456_Article_Equ12.gif"></td>
<td class="label">12</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq37"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/5b87139f0afe/41598_2025_16456_Article_IEq37.gif" loading="lazy" alt="Inline graphic"></span> represents the dimension of sparse coding matrix. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq38"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/7d1d8b8f7d3a/41598_2025_16456_Article_IEq38.gif" loading="lazy" alt="Inline graphic"></span> represents the optimal solution of sparse coding matrix. In the pre-training stage, the Block Gradient Descent (BCD) method is selected to update the dictionary during the optimization process of each layer. The coding matrix (vector) can be solved by Python convex optimization toolkit CVXPY and other tools, no matter it is dense features or sparse features. At the beginning of training, we need to initialize the dictionary, and then solve the convex optimization to obtain the coding matrix. Loss functions converge after a limited number of iterations by iterating the dictionary matrix and the encoding matrix alternately.</p>
<p id="Par39">In a fine-tuning stage, different levels of dictionaries are fused for dictionary reconstruction. The dictionary matrix and the encoding matrix alternately iterate as described above, until the loss function converges.</p></section></section><section id="Sec14"><h3 class="pmc_sec_title">Classifier design</h3>
<p id="Par40">After the pre-training phase, dictionary reconstruction phase and fine-tuning phase, the loss function converges after multiple iterations. There by the dictionary matrix optimal solution <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq39"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/f5379c61a133/41598_2025_16456_Article_IEq39.gif" loading="lazy" alt="Inline graphic"></span> and the sparse encoding matrix (vector) optimal solution <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq40"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/7d1d8b8f7d3a/41598_2025_16456_Article_IEq38.gif" loading="lazy" alt="Inline graphic"></span>are obtained. For the test set, we use the idea of KNN algorithm to design the classifier. For the input sample, the corresponding encoding vector is solved as:</p>
<table class="disp-formula p" id="Equ13"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/6f09c9fda847/41598_2025_16456_Article_Equ13.gif" loading="lazy" height="22" width="157" alt="graphic file with name 41598_2025_16456_Article_Equ13.gif"></td>
<td class="label">13</td>
</tr></table>
<table class="disp-formula p" id="Equ14"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/7dce2b77cbfe/41598_2025_16456_Article_Equ14.gif" loading="lazy" height="20" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ14.gif"></td>
<td class="label">14</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq41"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/e00c790eb826/41598_2025_16456_Article_IEq41.gif" loading="lazy" alt="Inline graphic"></span> is the input sample and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq42"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/2d6c39042c8f/41598_2025_16456_Article_IEq42.gif" loading="lazy" alt="Inline graphic"></span> is the corresponding coding vector.</p>
<p id="Par41">The label of the test sample is obtained as:</p>
<table class="disp-formula p" id="Equ15"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/ad0a800cee0a/41598_2025_16456_Article_Equ15.gif" loading="lazy" height="42" width="200" alt="graphic file with name 41598_2025_16456_Article_Equ15.gif"></td>
<td class="label">15</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq43"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/c9db48b07879/41598_2025_16456_Article_IEq43.gif" loading="lazy" alt="Inline graphic"></span> can be obtained via Eq.(<a href="#Equ13" class="usa-link">13</a>) or Eq.(<a href="#Equ14" class="usa-link">14</a>), <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq44"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/3bc9c844c1fa/41598_2025_16456_Article_IEq44.gif" loading="lazy" alt="Inline graphic"></span> represents the sub-vector of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq45"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/7d1d8b8f7d3a/41598_2025_16456_Article_IEq38.gif" loading="lazy" alt="Inline graphic"></span>, <em>l</em> represents its related label, and Eq.(<a href="#Equ15" class="usa-link">15</a>) uses the cosine similarity.</p></section></section><section id="Sec15"><h2 class="pmc_sec_title">Experimental process and result analysis</h2>
<section id="Sec16"><h3 class="pmc_sec_title">Experimental setting</h3>
<p id="Par42">To validate our method, we utilized the LMT-108 dataset<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup> containing 108 different objects which are divided into 9 categories: 1) Meshes; 2) Stones; 3) Glossy; 4) Wood Types; 5) Rubbers; 6) Fibers; 7) Foams; 8) Foils and Papers; and 9) Textiles and Fabrics. The LMT-108 dataset provides various forms of data, including acceleration, friction, image, metal detection, IR reflection and sound. We have applied image, sound and acceleration data to our experiments. Acceleration signals are recorded by a three-axis ADXL335 accelerometer (±3g, 10kHz), and sound is captured using a CMP-MIC8 microphone (44.1kHz). Images have a resolution of 320<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq46"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/05c878d129aa/41598_2025_16456_Article_IEq46.gif" loading="lazy" alt="Inline graphic"></span>480. The dataset contains 108 objects, each with 10 samples (totaling 1080). For evaluation, we randomly split each object’s samples in half for training and testing, ensuring no object overlaps between sets.The samples from the LMT-108 dataset are shown in Fig. <a href="#Fig3" class="usa-link">3</a>.</p>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12375791_41598_2025_16456_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/296ce41eb56a/41598_2025_16456_Fig3_HTML.jpg" loading="lazy" id="MO4" height="133" width="668" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Sample from the LMT-108 dataset.</p></figcaption></figure><p id="Par43">To verify the robustness and adaptability of DRDL, we need to conduct experiments on another more challenging dataset. We utilized the SpectroVision dataset–a multimodal collection of 14,400 paired samples capturing near-infrared (NIR) spectral measurements and high-resolution texture images (1,600 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq47"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/05c878d129aa/41598_2025_16456_Article_IEq46.gif" loading="lazy" alt="Inline graphic"></span> 1,200 pixels) from 144 household objects<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup>. Data was gathered non-invasively using a PR2 mobile manipulator equipped with a SCiO spectrometer (740–1,070 nm range) and a 2MP endoscope camera with 12-LED ring lighting for consistent illumination. The objects spanned eight material categories: ceramic, fabric, foam, glass, metal, paper, plastic, and wood. Each object underwent 100 randomized interactions at diverse surface points/orientations (vertical: height/roll variations; horizontal: planar position sampling), ensuring real-world generalizability. This dataset enables robust material recognition without physical contact. Select 4 objects from each material for testing, totaling 32 unseen object data.</p></section><section id="Sec17"><h3 class="pmc_sec_title">Feature representation via data dimension reduction</h3>
<p id="Par44">Due to the small number of samples, in order to improve the generalization ability of our model, it is necessary to reduce the dimension of acceleration signal and extract features of trimodal data using some conventional methods. The following four methods<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup> are used to reduce the dimensionality of the acceleration signal: </p>
<ol class="list" style="list-style-type:decimal">
<li><p id="Par45">SA-x/SA-y/SA-z: Single Axis (SA) is the simplest method, which only respectively represent the acceleration signals measured on the corresponding x, y and z axes.</p></li>
<li><p id="Par46">SoC: The implementation of Shadow o fClustering (SoC) method is relatively simple, which only adds the data from each direction.</p></li>
<li><p id="Par47">Mag: Compared with the former two methods, the complexity of Magnitude (Mag) method is slightly increased, and its implementation process is roughly to find the sum of squares of the data measured by different methods, and then open the root. It is a commonly used method, but it will change some negative data into positive data after processing, which will make part of the process is roughly to find the sum of squares of the data measured by different methods, and then open the root. It is a commonly used method, but it will change some negative data into positive data after processing, which will make part of the original information missing.</p></li>
<li><p id="Par48">PCA: Principal component analysis (PCA) is a widely used dimension reduction method. The main idea of PCA is to map ndimension features to k-dimension, which is a new orthogonal feature, also known as principal component, and a k-dimension feature reconstructed from the original n-dimension features.</p></li>
</ol>
<p>In order to select the most appropriate method for dimension reduction, we choose the most effective method SA-z for dimensionality reduction of acceleration signal, combining with our previous work and the comparative experiment of each method in<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup>.</p></section><section id="Sec18"><h3 class="pmc_sec_title">Feature extraction</h3>
<p id="Par49">Features of the corresponding three modal data are extracted, which is also a key step. Based on the analysis of previous experimental results in previous work in<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>, we use the optimal feature extraction algorithm to extract LBP features from the image, and extract Mel-Frequency Cepstral Coefficients (MFCC) features from the sound signal and acceleration signal.</p></section><section id="Sec19"><h3 class="pmc_sec_title">Performance comparison</h3>
<p id="Par50">In order to give a performance comparison of our proposed method with the traditional and common methods, ten methods are applied to our experiments. </p>
<ol class="list" style="list-style-type:decimal">
<li><p id="Par51">K-SVD: The dictionary is updated by K-SVD. Orthogonal matching pursuit (OMP) is used to solve the sparse coding problem.</p></li>
<li><p id="Par52">Support vector machine (SVM): It is a class of generalized linear classifiers that classify data by supervised learning. Its decision boundary is the maximum margin hyperplane of learning samples.</p></li>
<li><p id="Par53">MLP: A classic feedforward neural network with at least three layers of nodes that uses backpropagation for training.</p></li>
<li><p id="Par54">Convolutional Neural Network based on Vision (CNN-V): CNN has been used as a common method for texture recognition task in recent years due to its powerful ability to extract texture features. Considering the problem of multimodal fusion, we take the visual modal information as the input in CNN method.</p></li>
<li><p id="Par55">Greedy Deep Dictionary Learning (GDDL)<sup><a href="#CR49" class="usa-link" aria-describedby="CR49">49</a></sup>: A deep dictionary learning method using greedy iteration without any fusion method. Experiments were carried out with visual, sound and acceleration signals as inputs.</p></li>
<li><p id="Par56">Twin-incoherent Self-expressive Latent Dictionary Pair Learning (SLatDPL)<sup><a href="#CR50" class="usa-link" aria-describedby="CR50">50</a></sup>: A dictionary learning model integrates feature extraction and coding coefficient representation, and introduces two non-coherent local constraints with self-expression and self-adaptability.</p></li>
<li><p id="Par57">Robust Adaptive Projective<sup><a href="#CR51" class="usa-link" aria-describedby="CR51">51</a></sup> Dictionary Pair Learning (RA-DPL): A dictionary pair learning model retains the local neighborhood relationship of intra-class sparse coding, and the learned coding has discriminative ability.</p></li>
<li><p id="Par58">Relaxed Block-diagonal Dictionary Pair Learning with a Locality Constraint (RBD-DPL)<sup><a href="#CR52" class="usa-link" aria-describedby="CR52">52</a></sup>: Relaxed block diagonal structure is introduced to improve the discriminability of the dictionary.</p></li>
<li><p id="Par59">One-Shot Learning Method for Texture Recognition (OSL)<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup>: Multimodal fusion and dictionary learning are combined to solve the problem of texture recognition. The model uses only one sample as the training sample, which solves the disadvantage that the model depends on enough samples.</p></li>
<li><p id="Par60">DRDL: We set up a 3-layer deep dictionary learning model, and get the corresponding experimental results through the test set.</p></li>
</ol>
<p>The classification accuracy of the ten experimental methods is shown in Fig.<a href="#Fig4" class="usa-link">4</a>. By comparing the classification accuracy of ten texture recognition methods, the accuracy of DRDL is higher than that of any method, and the effectiveness of the proposed DRDL is proved. To provide a more explicit comparison, our analysis is as follows:</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12375791_41598_2025_16456_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/efa07df8875c/41598_2025_16456_Fig4_HTML.jpg" loading="lazy" id="MO5" height="351" width="676" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Comparison of classification results of each method in LMT-108.</p></figcaption></figure><p id="Par61">Comparison with Single-Layer Methods: Methods like K-SVD are based on traditional single-layer dictionary learning. While effective at capturing shallow features, they lack the ability to learn deeper, more abstract representations, which limits their accuracy to around 89%. Our DRDL model, by incorporating a deep architecture, significantly outperforms these methods.</p>
<p id="Par62">Comparison with Standard Deep Learning: The CNN-V model, a standard deep learning approach, achieves around 90% accuracy using only visual data. This demonstrates the power of deep feature extraction. However, our DRDL model surpasses it by not only using deep learning principles but also by fusing multimodal data, providing a richer source of information.</p>
<p id="Par63">Comparison with Other Deep Dictionary Learning Methods: The GDDL model, another deep dictionary learning approach, shows poor performance when used with single modalities. This highlights a key problem that our model solves: traditional deep dictionary learning can lose important features. Our dictionary reconstruction method explicitly addresses this by fusing features from all levels, which is critical for high performance.</p>
<p id="Par64">Comparison with a Strong Fusion-Based Model: The OSL method, which also uses multimodal fusion, is a very strong competitor. However, it is still based on single-layer dictionary learning. Our DRDL model gains its final performance edge by combining both early multimodal fusion and a deep architecture with multi-level feature fusion. This unique combination of strategies allows our model to achieve the highest accuracy of 97.7%, demonstrating that both rich input data and a comprehensive feature hierarchy are essential for state-of-the-art performance.</p></section><section id="Sec20"><h3 class="pmc_sec_title">Statistical performance validation</h3>
<p id="Par65">To rigorously validate our performance comparisons, we conducted statistical significance tests. We performed a 10-fold cross-validation to obtain 10 accuracy scores for the top 4 comparison methods. We then used the Wilcoxon signed-rank test to compare our DRDL model against each competing method individually, with a significance level (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq48"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/a53b66b8ab75/41598_2025_16456_Article_IEq48.gif" loading="lazy" alt="Inline graphic"></span>) of 0.05.</p>
<p id="Par66">The results of the statistical analysis are presented in Table <a href="#Tab1" class="usa-link">1</a>. The p-values for the comparison between DRDL and all other methods were below 0.05. This indicates that the performance improvement of our proposed model is statistically significant and not due to random chance. This provides strong evidence for the superior feature fusion and dictionary reconstruction capabilities of the DRDL framework.</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>Table of results of the statistical analysis.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1"></th>
<th align="left" colspan="1" rowspan="1">DRDL</th>
<th align="left" colspan="1" rowspan="1">OSL</th>
<th align="left" colspan="1" rowspan="1">SLat-DPL</th>
<th align="left" colspan="1" rowspan="1">SVM</th>
<th align="left" colspan="1" rowspan="1">RBD-DPL</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">count</td>
<td align="left" colspan="1" rowspan="1">10</td>
<td align="left" colspan="1" rowspan="1">10</td>
<td align="left" colspan="1" rowspan="1">10</td>
<td align="left" colspan="1" rowspan="1">10</td>
<td align="left" colspan="1" rowspan="1">10</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">mean</td>
<td align="left" colspan="1" rowspan="1">0.9766</td>
<td align="left" colspan="1" rowspan="1">0.9685</td>
<td align="left" colspan="1" rowspan="1">0.9608</td>
<td align="left" colspan="1" rowspan="1">0.9615</td>
<td align="left" colspan="1" rowspan="1">0.9554</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">std</td>
<td align="left" colspan="1" rowspan="1">0.0046</td>
<td align="left" colspan="1" rowspan="1">0.0027</td>
<td align="left" colspan="1" rowspan="1">0.0018</td>
<td align="left" colspan="1" rowspan="1">0.0029</td>
<td align="left" colspan="1" rowspan="1">0.0033</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">min</td>
<td align="left" colspan="1" rowspan="1">0.9690</td>
<td align="left" colspan="1" rowspan="1">0.9650</td>
<td align="left" colspan="1" rowspan="1">0.9580</td>
<td align="left" colspan="1" rowspan="1">0.9580</td>
<td align="left" colspan="1" rowspan="1">0.9510</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">max</td>
<td align="left" colspan="1" rowspan="1">0.9840</td>
<td align="left" colspan="1" rowspan="1">0.9730</td>
<td align="left" colspan="1" rowspan="1">0.9630</td>
<td align="left" colspan="1" rowspan="1">0.9680</td>
<td align="left" colspan="1" rowspan="1">0.9600</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">p-value</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">
<em>p</em> &lt; 0.05</td>
<td align="left" colspan="1" rowspan="1">
<em>p</em> &lt; 0.05</td>
<td align="left" colspan="1" rowspan="1">
<em>p</em> &lt; 0.05</td>
<td align="left" colspan="1" rowspan="1">
<em>p</em> &lt; 0.05</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec21"><h3 class="pmc_sec_title">Additional verification</h3>
<p id="Par67">To further validate the robustness and generalizability of our proposed DRDL model, we conducted experiments on the more challenging SpectroVision dataset. We extract LBP features from texture images and only perform PCA dimensionality reduction on spectral data. As shown in Fig.<a href="#Fig5" class="usa-link">5</a>, our DRDL method once again achieved state-of-the-art performance, with an accuracy of 89.4%. It surpassed all other compared methods, including deep learning approaches like CNN-V and dictionary learning methods like OSL and K-SVD. Notably, the performance gap between DRDL and the next-best method was 3.3%, which is more pronounced than on the LMT-108 dataset. This demonstrates our model’s superior ability to learn discriminative features from complex and varied texture data.</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12375791_41598_2025_16456_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/7961ba17c509/41598_2025_16456_Fig5_HTML.jpg" loading="lazy" id="MO6" height="351" width="676" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Comparison of classification results of each method in SpectroVision.</p></figcaption></figure></section><section id="Sec22"><h3 class="pmc_sec_title">Influence of layer count on performance</h3>
<p id="Par68">The number of model layers will directly affect the performance of our whole model. When the number of layers is set to 1, the model is a normal single-layer dictionary learning model. We set number of layers to 1, 2, 3 and 4 respectively, and carry out experiments. Additionally, we also set up another experiment, in which only the features learned in the deepest part are utilized without using Dictionary Reconstruction in the 3-layer model. The results are shown in the Table <a href="#Tab2" class="usa-link">2</a>.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Table of comparison of the effects of different depth models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Depth Of Model</th>
<th align="left" colspan="1" rowspan="1">Features Utilized</th>
<th align="left" colspan="1" rowspan="1">Accuracy</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Layer-1</td>
<td align="left" colspan="1" rowspan="1">First layer</td>
<td align="left" colspan="1" rowspan="1">0.893</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Layer-2</td>
<td align="left" colspan="1" rowspan="1">All layers</td>
<td align="left" colspan="1" rowspan="1">0.935</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Layer-3</td>
<td align="left" colspan="1" rowspan="1">All layers</td>
<td align="left" colspan="1" rowspan="1">0.977</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Layer-4</td>
<td align="left" colspan="1" rowspan="1">All layers</td>
<td align="left" colspan="1" rowspan="1">0.969</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Layer-3</td>
<td align="left" colspan="1" rowspan="1">Only last layer</td>
<td align="left" colspan="1" rowspan="1">0.931</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par69">The Table <a href="#Tab2" class="usa-link">2</a> shows that the 3-layer DRDL model has the best performance in this experiment. With the increase of the depth of the model, the performance is improving, but the performance of the 4-layer model is inferior to that of the 3-layer model. The reason is that the increase of the depth will cause the redundancy of the fused features.</p>
<p id="Par70">It is found that increasing the number of model layers appropriately improves the accuracy of classification. As the number of layers increases, the accuracy improvement decreases. Therefore, it is particularly important to select the appropriate number of layers. Experimental results show that if only considering deep features we fail to achieve the best results, and our proposed DRDL is superior. Because the learning characteristics of all layers are considered in the reconstruction stage, the depth of the model can affect the training speed of the model. As the number of model layers increases, so does the training time. According to the comprehensive consideration of the recognition performance and computational cost of the algorithm, the number of dictionary layers is set to 3 in our following experiments.</p></section><section id="Sec23"><h3 class="pmc_sec_title">Material comparison</h3>
<p id="Par71">The LMT-108 dataset contains objects of nine different materials. Therefore, we have conducted further experiments to test the effect of applying the methods of this paper on surface texture classification of different materials.</p>
<p id="Par72">In<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>, the author proposed an ensemble learning method (ELM) with optimized features for multimodal surface material recognition. The method proposed in this paper is compared with the method proposed in<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>. In this comparative experiment, in order to be consistent with the sample number setting in the experiment in<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>, the sample number in both the training set and the test set is set to 1080.</p>
<p id="Par73">The experimental results of the material identification task are shown in Fig. <a href="#Fig6" class="usa-link">6</a>, and the recognition accuracy is 0.978. The comparison of the experimental results of the two methods is shown in Table <a href="#Tab3" class="usa-link">3</a>. For all materials, our proposed DRDL for material identification accuracy rate is higher than EML, except for meshes.</p>
<figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12375791_41598_2025_16456_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/2ac9eb713046/41598_2025_16456_Fig6_HTML.jpg" loading="lazy" id="MO7" height="625" width="795" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Confusion Matrix in Material Identification Tasks Using DRDL. Numbers on the diagonal represent the number of correct classifications, while numbers off the diagonal represent the number of incorrect classifications.</p></figcaption></figure><section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Table of accuracy of methods materials identification task.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Material</th>
<th align="left" colspan="1" rowspan="1">EML</th>
<th align="left" colspan="1" rowspan="1">DRDL</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Meshes</td>
<td align="left" colspan="1" rowspan="1">99.23%</td>
<td align="left" colspan="1" rowspan="1">96.92%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Stones</td>
<td align="left" colspan="1" rowspan="1">94.44%</td>
<td align="left" colspan="1" rowspan="1">100%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glossy</td>
<td align="left" colspan="1" rowspan="1">98.89%</td>
<td align="left" colspan="1" rowspan="1">98.89%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Wood</td>
<td align="left" colspan="1" rowspan="1">97.69%</td>
<td align="left" colspan="1" rowspan="1">99.23%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Rubbers</td>
<td align="left" colspan="1" rowspan="1">70.00%</td>
<td align="left" colspan="1" rowspan="1">96.00%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Fibers</td>
<td align="left" colspan="1" rowspan="1">92.00%</td>
<td align="left" colspan="1" rowspan="1">94.67%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Foams</td>
<td align="left" colspan="1" rowspan="1">78.33%</td>
<td align="left" colspan="1" rowspan="1">98.33%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Foils and Papers</td>
<td align="left" colspan="1" rowspan="1">89.33%</td>
<td align="left" colspan="1" rowspan="1">98.67%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Textiles and Fabrics</td>
<td align="left" colspan="1" rowspan="1">88.82%</td>
<td align="left" colspan="1" rowspan="1">97.65%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec24"><h3 class="pmc_sec_title">Influence of parameters</h3>
<p id="Par74">We analyzed the effects of parameters of the results, as shown in Fig. <a href="#Fig7" class="usa-link">7</a>. We performed a joint analysis of parameters <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq49"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/084c313c5f5e/41598_2025_16456_Article_IEq12.gif" loading="lazy" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq50"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/654d70eb8b20/41598_2025_16456_Article_IEq50.gif" loading="lazy" alt="Inline graphic"></span>, set from 0.0001 to 100, respectively. In Fig. <a href="#Fig7" class="usa-link">7</a>, the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq51"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/c7354634518f/41598_2025_16456_Article_IEq51.gif" loading="lazy" alt="Inline graphic"></span>-axis represents accuracy, while the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq52"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/1ebc7122be4c/41598_2025_16456_Article_IEq52.gif" loading="lazy" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq53"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/c93a823f92db/41598_2025_16456_Article_IEq53.gif" loading="lazy" alt="Inline graphic"></span> axes are <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq54"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/3a1b2d1f715b/41598_2025_16456_Article_IEq54.gif" loading="lazy" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq55"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/852b2caef036/41598_2025_16456_Article_IEq55.gif" loading="lazy" alt="Inline graphic"></span>. When it comes to the other experiments, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq56"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/084c313c5f5e/41598_2025_16456_Article_IEq12.gif" loading="lazy" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq57"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/654d70eb8b20/41598_2025_16456_Article_IEq50.gif" loading="lazy" alt="Inline graphic"></span> are set to 0.15 and 1.8, respectively, and the dictionary sizes of the three-layer model are set to 196, 196 and 256, respectively.</p>
<figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12375791_41598_2025_16456_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/23ff9d7c00ea/41598_2025_16456_Fig7_HTML.jpg" loading="lazy" id="MO8" height="702" width="744" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The influence of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq58"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/084c313c5f5e/41598_2025_16456_Article_IEq12.gif" loading="lazy" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq59"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/654d70eb8b20/41598_2025_16456_Article_IEq50.gif" loading="lazy" alt="Inline graphic"></span> on the classification accuracy, in which the two coordinates of the horizontal plane are <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq60"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/3a1b2d1f715b/41598_2025_16456_Article_IEq54.gif" loading="lazy" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq61"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/acab/12375791/852b2caef036/41598_2025_16456_Article_IEq55.gif" loading="lazy" alt="Inline graphic"></span> respectively.</p></figcaption></figure></section><section id="Sec25"><h3 class="pmc_sec_title">Ablation study</h3>
<p id="Par75">We performed three ablation sub-experiments, including Experiment A, Experiment B and Experiment C. The details are as follows: 1) Experiment A: We separate the multi-modal fusion method from the DRDL, and individually took three kinds of data modes, including images, sound and acceleration, as the input of the model, which were marked as Experiment A-V, Experiment A-S and Experiment A-A respectively. 2) Experiment B: We separate dictionary reconstruction and fine-tuning stages from the DRDL. 3) Experiment C: We separate the fine-tuning stages from the DRDL. The final results are shown in Table <a href="#Tab4" class="usa-link">4</a>.</p>
<section class="tw xbox font-sm" id="Tab4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>Table of results of ablation studies.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Experimental</th>
<th align="left" colspan="1" rowspan="1">Accuracy</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Experiment A-V</td>
<td align="left" colspan="1" rowspan="1">0.711</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Experiment A-S</td>
<td align="left" colspan="1" rowspan="1">0.689</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Experiment A-A</td>
<td align="left" colspan="1" rowspan="1">0.702</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Experiment B</td>
<td align="left" colspan="1" rowspan="1">0.831</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Experiment C</td>
<td align="left" colspan="1" rowspan="1">0.952</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DRDL</td>
<td align="left" colspan="1" rowspan="1">0.977</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec26"><h3 class="pmc_sec_title">Computational efficiency analysis</h3>
<p id="Par77">DRDL takes 10.57s to classify the test set containing 540 samples. The execution time of all compared methods for classifying test samples is shown in Table <a href="#Tab5" class="usa-link">5</a>.</p>
<section class="tw xbox font-sm" id="Tab5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>Execution time for classifying test samples by different methods.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Method</th>
<th align="left" colspan="1" rowspan="1">Time (sec)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">GDDL</td>
<td align="left" colspan="1" rowspan="1">9.83</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">K-SVD</td>
<td align="left" colspan="1" rowspan="1">8.12</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CNN-V</td>
<td align="left" colspan="1" rowspan="1">0.28</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MLP</td>
<td align="left" colspan="1" rowspan="1">0.22</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SVM</td>
<td align="left" colspan="1" rowspan="1">0.33</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">OSL</td>
<td align="left" colspan="1" rowspan="1">8.99</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DRDL</td>
<td align="left" colspan="1" rowspan="1">10.57</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par78">Compared with CNN-V, MLP and SVM, our DRDL involving dictionary learning requires longer time to classify test samples, because it uses CVXPY to solve a convex optimization problem for dictionary learning. Compared with the single-layer dictionary learning method i.e., OSL and K- SVD, the deep dictionary learning method needs to consider the deep-level characteristics, which leads its longer execution time. Compared with GDDL, a greedy iteration method without any fusion, our DRDL needs longer time because it considers characteristics of multiple layers, which achieves a big increase of accuracy, as shown in Fig. <a href="#Fig7" class="usa-link">7</a>.</p>
<p id="Par79">Future work will focus on the following detailed strategies to improve its efficiency and make it more suitable for real-time applications.</p>
<p id="Par80">Faster Solver with ADMM: Instead of the general-purpose CVXPY, we’ll use the Alternating Direction Method of Multipliers (ADMM). ADMM splits the lasso problem into smaller subproblems that can be solved (often analytically) in parallel, cutting down runtime significantly.</p>
<p id="Par81">Dictionary Pruning: Once the dictionary <em>D</em> is trained, many atoms may be redundant. We’ll rank atoms by how often and how much they reduce reconstruction error, then drop the weakest ones. A smaller dictionary speeds up sparse coding by reducing the number of variables.</p>
<p id="Par82">Quantization for Deployment To save memory and boost speed on limited hardware, we’ll convert 32?bit floats in the dictionary and features to lower precision (e.g. 16?bit floats or 8?bit ints). This slashes storage needs and taps into faster, hardware?accelerated matrix operations.</p>
<p id="Par83">In our experiments, all methods are implemented in Python 3.8 on a computer platform (2.6-GHz CPU and 16-G RAM).</p></section><section id="Sec27"><h3 class="pmc_sec_title">Limitations and Generalizability</h3>
<p id="Par84">Although our proposed DRDL model demonstrates state-of-the-art performance, it is important to acknowledge its limitations and consider the context of its evaluation.</p>
<p id="Par85">(1)Model limitations</p>
<p id="Par86">The primary limitation of the DRDL model is its computational complexity. The multi-stage process, which includes layer-by-layer pre-training, dictionary reconstruction, and fine-tuning, is inherently more complicated than end-to-end deep learning models or single-layer dictionary methods. The use of CVXPY to solve the convex optimization problem for sparse coding is a significant bottleneck. While this trade-off yields higher accuracy, the model’s current implementation may not be suitable for real-time applications where low latency is critical.</p>
<p id="Par87">(2) Potential biases in the data</p>
<p id="Par88">While our evaluation used two established texture recognition benchmarks, their controlled laboratory acquisition may limit the model’s robustness in real-world settings with variable lighting, background clutter, or dynamic contact conditions.</p></section></section><section id="Sec28"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par89">This paper presents a dictionary-reconstruction-based deep learning approach (DRDL) that effectively combines dictionary learning and deep learning. By learning the features of different levels and fusing the dictionaries of different levels, the features learned of the data at different levels are well fused. In this paper, a texture recognition experiment is carried out by using the LMT-108 and SpectroVision dataset combined with multi-modal integration. Our newly proposed method has achieved the highest the highest after its comparison with the state-of-the-art methods. The consistent high performance across two distinct and challenging datasets confirms the robustness and superior feature fusion capabilities of our model.</p>
<p id="Par90">This work uses visual, sound and acceleration information as the input of the algorithm in our experiments. The uncertainty in pattern recognition is reduced by early multimodal integration and later dictionary reconstruction. Our model takes both intuitive and deep features into account. Therefore, it can be easily extended to perform other related tasks, e.g., expression recognition, face recognition, gestural recognition, and identity recognition. Taking an identity recognition task as an example, the model can fuse data of different modalities, and then learn different levels of features, such as posture and expression. It can potentially improve the accuracy of a recognition task.</p>
<p id="Par91">Although our proposed DRDL method has achieved state-of-the-art performance in texture recognition tasks, several directions warrant further exploration in future work, including: improving computational efficiency; expanding application domains; and investigating more advanced dictionary fusion strategies.</p></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>This work was supported by the National Natural Science Foundation of China (Grant Nos. 62373181, 62163024 and 61903175); and in part by Jiangxi Science Fund for Distinguished Young Scholars (Grant Nos. 20232ACB212002); and in part by Jiangxi Double Thousand Plan Project (Grant Nos. jxsq2023201097); and in part by Jiangxi Key R&amp;D Program Project (Grant Nos.20252BCE310017); and in part by National Key R&amp;D Program of China (Grant Nos.2023YFB4704900).</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>P.X.: Conceptualization, Methodology, Supervision, Writing-Reviewing and Editing, Funding acquisition. K.Z.: Software, Visualization, Data curation, Writing Original Draft. Z.S.: Software, Methodology, Data curation. M.Z.: Investigation, Project administration, Validation. A.S.: Conceptualization, Formal analysis, Supervision. All authors reviewed the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The data used in this article are publicly available online and have been cited in the article.</p></section><section id="notes3"><h2 class="pmc_sec_title">Code availability</h2>
<p>The source code and models generated during this study are not publicly available due to institutional policy restrictions. However, to support academic collaboration and ensure the reproducibility of our results, the code can be made available from the corresponding author upon reasonable request. Interested parties may contact steven.xpw@ncu.edu.cn to initiate an application for access, which will be processed in accordance with the policies of our laboratory and institution.</p></section><section id="notes4"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar2"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par92">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Rongala, U. B., Mazzoni, A. &amp; Oddo, C. M. Neuromorphic artificial touch for categorization of naturalistic textures. <em>IEEE Trans. Neural Netw. Learn. Syst.</em><strong>28</strong>, 819–829 (2017).
</cite> [<a href="https://doi.org/10.1109/TNNLS.2015.2472477" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26372658/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Rongala,%20U.%20B.,%20Mazzoni,%20A.%20&amp;%20Oddo,%20C.%20M.%20Neuromorphic%20artificial%20touch%20for%20categorization%20of%20naturalistic%20textures.%20IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.28,%20819%E2%80%93829%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Ma, Y., Yin, J., Huang, F. &amp; Li, Q. Surface defect inspection of industrial products with object detection deep networks: A systematic review. <em>Artif Intell Rev</em><strong>57</strong>, 333 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ma,%20Y.,%20Yin,%20J.,%20Huang,%20F.%20&amp;%20Li,%20Q.%20Surface%20defect%20inspection%20of%20industrial%20products%20with%20object%20detection%20deep%20networks:%20A%20systematic%20review.%20Artif%20Intell%20Rev57,%20333%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Cao, G., Zhou, Y., Bollegala, D. &amp; Luo, S. Spatio-temporal attention model for tactile texture recognition. In <em>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 9896–9902 (IEEE, 2020).</cite>
</li>
<li id="CR4">
<span class="label">4.</span><cite>Silva, J., Bispo, B. C., Rodrigues, P. M. &amp; Initiative, A. D. N. Structural MRI texture analysis for detecting alzheimer’s disease. <em>Med. Biol. Eng.</em><strong>43</strong>, 227–238 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Silva,%20J.,%20Bispo,%20B.%20C.,%20Rodrigues,%20P.%20M.%20&amp;%20Initiative,%20A.%20D.%20N.%20Structural%20MRI%20texture%20analysis%20for%20detecting%20alzheimer%E2%80%99s%20disease.%20Med.%20Biol.%20Eng.43,%20227%E2%80%93238%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Han, Y. et al. LDCT image denoising algorithm based on two-dimensional variational mode decomposition and dictionary learning. <em>Sci Rep</em><strong>14</strong>, 17487 (2024).
</cite> [<a href="https://doi.org/10.1038/s41598-024-68668-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11289268/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39080367/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Han,%20Y.%20et%20al.%20LDCT%20image%20denoising%20algorithm%20based%20on%20two-dimensional%20variational%20mode%20decomposition%20and%20dictionary%20learning.%20Sci%20Rep14,%2017487%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR6">
<span class="label">6.</span><cite>Khalid, M. U., Nauman, M. M., Akram, S. &amp; Ali, K. Three layered sparse dictionary learning algorithm for enhancing the subject wise segregation of brain networks. <em>Sci Rep</em><strong>14</strong>, 19070 (2024).
</cite> [<a href="https://doi.org/10.1038/s41598-024-69647-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11330533/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39154133/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Khalid,%20M.%20U.,%20Nauman,%20M.%20M.,%20Akram,%20S.%20&amp;%20Ali,%20K.%20Three%20layered%20sparse%20dictionary%20learning%20algorithm%20for%20enhancing%20the%20subject%20wise%20segregation%20of%20brain%20networks.%20Sci%20Rep14,%2019070%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<span class="label">7.</span><cite>Shakya, M., Patel, R. &amp; Joshi, S. A comprehensive analysis of deep learning and transfer learning techniques for skin cancer classification. <em>Sci Rep</em><strong>15</strong>, 4633 (2025).
</cite> [<a href="https://doi.org/10.1038/s41598-024-82241-w" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11805976/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39920179/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Shakya,%20M.,%20Patel,%20R.%20&amp;%20Joshi,%20S.%20A%20comprehensive%20analysis%20of%20deep%20learning%20and%20transfer%20learning%20techniques%20for%20skin%20cancer%20classification.%20Sci%20Rep15,%204633%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>Sagayaraj, A. S. &amp; Devi, T. K. Combination of gray level features with deep transfer learning for copra classification using machine learning and neural networks. <em>Sci Rep</em><strong>15</strong>, 1579 (2025).
</cite> [<a href="https://doi.org/10.1038/s41598-025-85490-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11724001/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39794394/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Sagayaraj,%20A.%20S.%20&amp;%20Devi,%20T.%20K.%20Combination%20of%20gray%20level%20features%20with%20deep%20transfer%20learning%20for%20copra%20classification%20using%20machine%20learning%20and%20neural%20networks.%20Sci%20Rep15,%201579%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>Mahdizadehaghdam, S., Panahi, A., Krim, H. &amp; Dai, L. Deep dictionary learning: A parametric network approach. <em>IEEE Trans. Image Process.</em><strong>28</strong>, 4790–4802 (2019).</cite> [<a href="https://doi.org/10.1109/TIP.2019.2914376" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31071039/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Mahdizadehaghdam,%20S.,%20Panahi,%20A.,%20Krim,%20H.%20&amp;%20Dai,%20L.%20Deep%20dictionary%20learning:%20A%20parametric%20network%20approach.%20IEEE%20Trans.%20Image%20Process.28,%204790%E2%80%934802%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR10">
<span class="label">10.</span><cite>Zhu, W., Peng, B., Chen, C. &amp; Chen, H. Deep discriminative dictionary pair learning for image classification. <em>Appl Intell</em><strong>53</strong>, 22017–22030 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhu,%20W.,%20Peng,%20B.,%20Chen,%20C.%20&amp;%20Chen,%20H.%20Deep%20discriminative%20dictionary%20pair%20learning%20for%20image%20classification.%20Appl%20Intell53,%2022017%E2%80%9322030%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>He, H., Liang, J., Hou, Z., Di, L. &amp; Xia, Y. Multi-pose face reconstruction and gabor-based dictionary learning for face recognition. <em>Appl Intell</em><strong>53</strong>, 16648–16662 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?He,%20H.,%20Liang,%20J.,%20Hou,%20Z.,%20Di,%20L.%20&amp;%20Xia,%20Y.%20Multi-pose%20face%20reconstruction%20and%20gabor-based%20dictionary%20learning%20for%20face%20recognition.%20Appl%20Intell53,%2016648%E2%80%9316662%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Yang, J., Chen, C., Dai, H., Fu, L. &amp; Zheng, Z. A structure noise-aware tensor dictionary learning method for high-dimensional data clustering. <em>Inf. Sci.</em><strong>612</strong>, 87–106 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yang,%20J.,%20Chen,%20C.,%20Dai,%20H.,%20Fu,%20L.%20&amp;%20Zheng,%20Z.%20A%20structure%20noise-aware%20tensor%20dictionary%20learning%20method%20for%20high-dimensional%20data%20clustering.%20Inf.%20Sci.612,%2087%E2%80%93106%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>Tariyal, S., Aggarwal, H. &amp; Majumdar, A. Greedy deep dictionary learning for hyperspectral image classification. In <em>2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)</em>, 1–4 (Los Angeles, CA, USA, 2016).</cite>
</li>
<li id="CR14">
<span class="label">14.</span><cite>Montazeri, A., Shamsi, M. &amp; Dianat, R. Using a new approach in deep dictionary learning to handwriting number classification. In <em>Int. Comput. Conf., Comput. Soc. Iran</em>, 1–8 (Tehran, Iran, 2020).</cite>
</li>
<li id="CR15">
<span class="label">15.</span><cite>Tang, H., Liu, H., Xiao, W. &amp; Sebe, N. When dictionary learning meets deep learning: Deep dictionary learning and coding network for image recognition with limited data. <em>IEEE Trans. Neural Netw. Learn. Syst.</em><strong>32</strong>, 2129–2141 (2021).
</cite> [<a href="https://doi.org/10.1109/TNNLS.2020.2997289" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32516113/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Tang,%20H.,%20Liu,%20H.,%20Xiao,%20W.%20&amp;%20Sebe,%20N.%20When%20dictionary%20learning%20meets%20deep%20learning:%20Deep%20dictionary%20learning%20and%20coding%20network%20for%20image%20recognition%20with%20limited%20data.%20IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.32,%202129%E2%80%932141%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>Zhang, Z. <em>et al.</em> MDPLnet: Multi-layer dictionary learning network with added skip dense connections. In <em>Proc. IEEE Int. Conf. Data Min.</em>, 811–820 (2020).</cite>
</li>
<li id="CR17">
<span class="label">17.</span><cite>Atrey, P. K., Hossain, M. A., El Saddik, A. &amp; Kankanhalli, M. S. Multimodal fusion for multimedia analysis: A survey. <em>Multimedia Syst.</em><strong>16</strong>, 345–379 (2010).</cite> [<a href="https://scholar.google.com/scholar_lookup?Atrey,%20P.%20K.,%20Hossain,%20M.%20A.,%20El%20Saddik,%20A.%20&amp;%20Kankanhalli,%20M.%20S.%20Multimodal%20fusion%20for%20multimedia%20analysis:%20A%20survey.%20Multimedia%20Syst.16,%20345%E2%80%93379%20(2010)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR18">
<span class="label">18.</span><cite>Xiong, P. et al. Human exploratory procedures based hybrid measurement fusion for material recognition. <em>IEEE/ASME Trans. Mechatron.</em><strong>27</strong>, 1093–1104 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Xiong,%20P.%20et%20al.%20Human%20exploratory%20procedures%20based%20hybrid%20measurement%20fusion%20for%20material%20recognition.%20IEEE/ASME%20Trans.%20Mechatron.27,%201093%E2%80%931104%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19">
<span class="label">19.</span><cite>Liu, H., Yu, Y., Sun, F. &amp; Gu, J. Visual-tactile fusion for object recognition. <em>IEEE Trans. Autom. Sci. Eng.</em><strong>14</strong>, 996–1008 (2017).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20H.,%20Yu,%20Y.,%20Sun,%20F.%20&amp;%20Gu,%20J.%20Visual-tactile%20fusion%20for%20object%20recognition.%20IEEE%20Trans.%20Autom.%20Sci.%20Eng.14,%20996%E2%80%931008%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Xiong, P., Tong, X., Song, A. &amp; Liu, P. X. Robotic multifinger grasping state recognition based on adaptive multikernel dictionary learning. <em>IEEE Trans. Instrum. Meas.</em><strong>71</strong>, 1–14 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Xiong,%20P.,%20Tong,%20X.,%20Song,%20A.%20&amp;%20Liu,%20P.%20X.%20Robotic%20multifinger%20grasping%20state%20recognition%20based%20on%20adaptive%20multikernel%20dictionary%20learning.%20IEEE%20Trans.%20Instrum.%20Meas.71,%201%E2%80%9314%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR21">
<span class="label">21.</span><cite>Rasouli, M., Chen, Y., Basu, A., Kukreja, S. L. &amp; Thakor, N. V. An extreme learning machine-based neuromorphic tactile sensing system for texture recognition. <em>IEEE Trans. Biomed. Circuits Syst.</em><strong>12</strong>, 313–325 (2018).
</cite> [<a href="https://doi.org/10.1109/TBCAS.2018.2805721" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29570059/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Rasouli,%20M.,%20Chen,%20Y.,%20Basu,%20A.,%20Kukreja,%20S.%20L.%20&amp;%20Thakor,%20N.%20V.%20An%20extreme%20learning%20machine-based%20neuromorphic%20tactile%20sensing%20system%20for%20texture%20recognition.%20IEEE%20Trans.%20Biomed.%20Circuits%20Syst.12,%20313%E2%80%93325%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Orii, H., Tsuji, S., Kouda, T. &amp; Kohama, T. Recurrent neural network for tactile texture recognition using pressure and 6-axis acceleration sensor data. In <em>TENCON 2017 - 2017 IEEE Region 10 Conference</em>, 2012–2016 (Penang, Malaysia, 2017).</cite>
</li>
<li id="CR23">
<span class="label">23.</span><cite>Ahanat, K., Juan, A. &amp; Veronique, P. Tactile sensing in dexterous robot hands–review. <em>Robot. Auton. Syst.</em><strong>74</strong>, 195–220 (2015).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ahanat,%20K.,%20Juan,%20A.%20&amp;%20Veronique,%20P.%20Tactile%20sensing%20in%20dexterous%20robot%20hands%E2%80%93review.%20Robot.%20Auton.%20Syst.74,%20195%E2%80%93220%20(2015)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Wang, T. et al. Salient double reconstruction-based discriminative projective dictionary pair learning for crowd counting. <em>Appl Intell</em><strong>53</strong>, 1981–1996 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20T.%20et%20al.%20Salient%20double%20reconstruction-based%20discriminative%20projective%20dictionary%20pair%20learning%20for%20crowd%20counting.%20Appl%20Intell53,%201981%E2%80%931996%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR25">
<span class="label">25.</span><cite>Aharon, M., Elad, M. &amp; Bruckstein, A. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. <em>IEEE Trans. Signal Process.</em><strong>54</strong>, 4311–4322 (2006).</cite> [<a href="https://scholar.google.com/scholar_lookup?Aharon,%20M.,%20Elad,%20M.%20&amp;%20Bruckstein,%20A.%20K-SVD:%20An%20algorithm%20for%20designing%20overcomplete%20dictionaries%20for%20sparse%20representation.%20IEEE%20Trans.%20Signal%20Process.54,%204311%E2%80%934322%20(2006)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<span class="label">26.</span><cite>Huang, J. &amp; Dragotti, P. L. A deep dictionary model for image superresolution. In <em>Proc. IEEE Int. Conf. Acoust. Speech Signal Process.</em>, 6777–6781 (Calgary, AB, Canada, 2018).</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Baraldi, A. &amp; Parmiggiani, F. Investigation of the textural characteristics associated with gray level cooccurrence matrix statistical parameters. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>33</strong>, 293–304 (1995).</cite> [<a href="https://scholar.google.com/scholar_lookup?Baraldi,%20A.%20&amp;%20Parmiggiani,%20F.%20Investigation%20of%20the%20textural%20characteristics%20associated%20with%20gray%20level%20cooccurrence%20matrix%20statistical%20parameters.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.33,%20293%E2%80%93304%20(1995)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Arivazhagan, S., Ganesan, L. &amp; Priyal, S. P. Texture classification using Gabor wavelets based rotation invariant features. <em>Pattern Recognit. Lett.</em><strong>27</strong>, 1976–1982 (2006).</cite> [<a href="https://scholar.google.com/scholar_lookup?Arivazhagan,%20S.,%20Ganesan,%20L.%20&amp;%20Priyal,%20S.%20P.%20Texture%20classification%20using%20Gabor%20wavelets%20based%20rotation%20invariant%20features.%20Pattern%20Recognit.%20Lett.27,%201976%E2%80%931982%20(2006)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29">
<span class="label">29.</span><cite>Ju, H. &amp; KaiKuang, M. Rotation-invariant and scale-invariant gabor features for texture image retrieval. <em>Image Vis. Comput.</em><strong>25</strong>, 1474–1481 (2007).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ju,%20H.%20&amp;%20KaiKuang,%20M.%20Rotation-invariant%20and%20scale-invariant%20gabor%20features%20for%20texture%20image%20retrieval.%20Image%20Vis.%20Comput.25,%201474%E2%80%931481%20(2007)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR30">
<span class="label">30.</span><cite>Ojala, T., Pietikainen, M. &amp; Maenpaa, T. Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em><strong>24</strong>, 971–987 (2002).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ojala,%20T.,%20Pietikainen,%20M.%20&amp;%20Maenpaa,%20T.%20Multiresolution%20gray-scale%20and%20rotation%20invariant%20texture%20classification%20with%20local%20binary%20patterns.%20IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.24,%20971%E2%80%93987%20(2002)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Shu, X., Pan, H., Shao, C., Shi, J. &amp; Wu, X. Texture image classification based on local sorted difference refinement pattern. <em>J. Comput. Aided Des. Comput. Graph.</em><strong>32</strong>, 1948–1956 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Shu,%20X.,%20Pan,%20H.,%20Shao,%20C.,%20Shi,%20J.%20&amp;%20Wu,%20X.%20Texture%20image%20classification%20based%20on%20local%20sorted%20difference%20refinement%20pattern.%20J.%20Comput.%20Aided%20Des.%20Comput.%20Graph.32,%201948%E2%80%931956%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Wang, K., Liu, J., Xu, S., Wang, K. &amp; Gan, Z. Sparse representation classification for battlefield textual information. In <em>2017 IEEE 2nd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)</em>, 731–734 (Chongqing, China, 2017).</cite>
</li>
<li id="CR33">
<span class="label">33.</span><cite>Zhang, Z., Liu, H., Zhou, M. &amp; Wang, J. Solving Dynamic Traveling Salesman Problems with Deep Reinforcement Learning. <em>IEEE Trans. Neural Netw. Learn.</em><strong>34</strong>(4), 2119–2132 (2023).</cite> [<a href="https://doi.org/10.1109/TNNLS.2021.3105905" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34520362/" class="usa-link">PubMed</a>]</li>
<li id="CR34">
<span class="label">34.</span><cite>Mohammadzadeh-Vardin, T., Ghareyazi, A., Gharizadeh, A., Abbasi, K. &amp; Rabiee, H. R. Deepdra: Drug repurposing using multi-omics data integration with autoencoders. <em>Plos one</em><strong>19</strong>, e0307649 (2024).
</cite> [<a href="https://doi.org/10.1371/journal.pone.0307649" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11280260/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39058696/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Mohammadzadeh-Vardin,%20T.,%20Ghareyazi,%20A.,%20Gharizadeh,%20A.,%20Abbasi,%20K.%20&amp;%20Rabiee,%20H.%20R.%20Deepdra:%20Drug%20repurposing%20using%20multi-omics%20data%20integration%20with%20autoencoders.%20Plos%20one19,%20e0307649%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Fan, Y., Hong, C., Zeng, G. &amp; Liu, L. A deep convolutional encoder-decoder-restorer architecture for image deblurring. <em>Neural Process Lett</em><strong>56</strong>, 27 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Fan,%20Y.,%20Hong,%20C.,%20Zeng,%20G.%20&amp;%20Liu,%20L.%20A%20deep%20convolutional%20encoder-decoder-restorer%20architecture%20for%20image%20deblurring.%20Neural%20Process%20Lett56,%2027%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<span class="label">36.</span><cite>Zhang, P., Huang, Y., Chen, M. &amp; Yusuf Al-Turki. A Novel Deep-Learning-based QoS Prediction Model for Service Recommendation Utilizing Multi-Stage Multi-Scale Feature Fusion With Individual Evaluations. <em>IEEE Trans. Autom. Sci. Eng.</em><strong>21</strong>(2), 1740–1753 (2024).</cite>
</li>
<li id="CR37">
<span class="label">37.</span><cite>Cimpoi, M., Maji, S., Kokkinos, I. &amp; Vedaldi, A. Deep filter banks for texture recognition, description, and segmentation. <em>Int. J. Comput. Vis.</em><strong>118</strong>, 65–94 (2016).
</cite> [<a href="https://doi.org/10.1007/s11263-015-0872-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4946812/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27471340/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Cimpoi,%20M.,%20Maji,%20S.,%20Kokkinos,%20I.%20&amp;%20Vedaldi,%20A.%20Deep%20filter%20banks%20for%20texture%20recognition,%20description,%20and%20segmentation.%20Int.%20J.%20Comput.%20Vis.118,%2065%E2%80%9394%20(2016)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR38">
<span class="label">38.</span><cite>Song, Y. <em>et al.</em> Locally-transferred fisher vectors for texture classification. In <em>Proc. IEEE Int. Conf. Comput. Vis.</em>, 4922–4930 (2017).</cite>
</li>
<li id="CR39">
<span class="label">39.</span><cite>Condori, R. &amp; Bruno, O. M. Analysis of activation maps through global pooling measurements for texture classification. <em>Inf. Sci.</em><strong>555</strong>, 260–279 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Condori,%20R.%20&amp;%20Bruno,%20O.%20M.%20Analysis%20of%20activation%20maps%20through%20global%20pooling%20measurements%20for%20texture%20classification.%20Inf.%20Sci.555,%20260%E2%80%93279%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Hu, Y., Long, Z. &amp; AlRegib, G. Multi-level texture encoding and representation (multer) based on deep neural networks. In <em>Proc. IEEE Int. Conf. Image Process.</em>, 4410–4414 (2019).</cite>
</li>
<li id="CR41">
<span class="label">41.</span><cite>Patel, V. M. &amp; Chellappa, R. Sparse representations, compressive sensing and dictionaries for pattern recognition. In <em>Asian Conf. Pattern Recognit.</em>, 325–329 (2011).</cite>
</li>
<li id="CR42">
<span class="label">42.</span><cite>Zeng, Y., Chen, J. &amp; Huang, G. Slice-based online convolutional dictionary learning. <em>IEEE Trans. Cybern.</em><strong>51</strong>, 5116–5129 (2021).
</cite> [<a href="https://doi.org/10.1109/TCYB.2019.2931914" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31443059/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Zeng,%20Y.,%20Chen,%20J.%20&amp;%20Huang,%20G.%20Slice-based%20online%20convolutional%20dictionary%20learning.%20IEEE%20Trans.%20Cybern.51,%205116%E2%80%935129%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR43">
<span class="label">43.</span><cite>Wu, Z., El-Maghraby, M. &amp; Pathak, S. Applications of deep learning for smart water networks. <em>Procedia Eng.</em><strong>119</strong>, 479–485 (2015).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wu,%20Z.,%20El-Maghraby,%20M.%20&amp;%20Pathak,%20S.%20Applications%20of%20deep%20learning%20for%20smart%20water%20networks.%20Procedia%20Eng.119,%20479%E2%80%93485%20(2015)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR44">
<span class="label">44.</span><cite>Hinton, G. et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. <em>IEEE Signal Process. Mag.</em><strong>29</strong>, 82–97 (2012).</cite> [<a href="https://scholar.google.com/scholar_lookup?Hinton,%20G.%20et%20al.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition:%20The%20shared%20views%20of%20four%20research%20groups.%20IEEE%20Signal%20Process.%20Mag.29,%2082%E2%80%9397%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR45">
<span class="label">45.</span><cite>Kou, S. et al. Structure-aware subspace clustering. <em>IEEE Trans. Knowl. Data Eng.</em><strong>35</strong>, 10569–10582 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Kou,%20S.%20et%20al.%20Structure-aware%20subspace%20clustering.%20IEEE%20Trans.%20Knowl.%20Data%20Eng.35,%2010569%E2%80%9310582%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR46">
<span class="label">46.</span><cite>Strese, M., Boeck, Y. &amp; Steinbach, E. Content-based surface material retrieval. In <em>IEEE World Haptics Conf.</em>, 352–357 (2017).</cite>
</li>
<li id="CR47">
<span class="label">47.</span><cite>Erickson, Z., Xing, E., Srirangam, B., Chernova, S. &amp; Kemp, C. C. Multimodal material classification for robots using spectroscopy and high resolution texture imaging. In <em>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 10452–10459 (IEEE, 2020).</cite>
</li>
<li id="CR48">
<span class="label">48.</span><cite>Xiong, P., He, K., Song, A. &amp; Liu, P. A novel multimodal one-shot learning method for texture recognition. <em>IEEE Access</em><strong>7</strong>, 182538–182547 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Xiong,%20P.,%20He,%20K.,%20Song,%20A.%20&amp;%20Liu,%20P.%20A%20novel%20multimodal%20one-shot%20learning%20method%20for%20texture%20recognition.%20IEEE%20Access7,%20182538%E2%80%93182547%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR49">
<span class="label">49.</span><cite>Abed-Meraim, K. et al. A contemporary and comprehensive survey on streaming tensor decomposition. <em>IEEE Trans. Knowl. Data Eng.</em><strong>35</strong>, 10897–10921 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Abed-Meraim,%20K.%20et%20al.%20A%20contemporary%20and%20comprehensive%20survey%20on%20streaming%20tensor%20decomposition.%20IEEE%20Trans.%20Knowl.%20Data%20Eng.35,%2010897%E2%80%9310921%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR50">
<span class="label">50.</span><cite>Zhang, Z. et al. Twin-incoherent self-expressive locality-adaptive latent dictionary pair learning for classification. <em>IEEE Trans. Neural Netw. Learn. Syst.</em><strong>32</strong>, 947–961 (2021).
</cite> [<a href="https://doi.org/10.1109/TNNLS.2020.2979748" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32310782/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20Z.%20et%20al.%20Twin-incoherent%20self-expressive%20locality-adaptive%20latent%20dictionary%20pair%20learning%20for%20classification.%20IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.32,%20947%E2%80%93961%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR51">
<span class="label">51.</span><cite>Sun, Y. L., Zhang, Z. M., Jiang, W. C. &amp; et al. Robust discriminative projective dictionary pair learning by adaptive representations. In <em>Proc. Int. Conf. Pattern Recog.</em>, 621–626 (2018).</cite>
</li>
<li id="CR52">
<span class="label">52.</span><cite>Chen, Z., Wu, X. J. &amp; Kittler, J. Relaxed block-diagonal dictionary pair learning with locality constraint for image recognition. <em>IEEE Trans. Neural Netw. Learn. Syst.</em><strong>33</strong>, 3645–3659 (2022).
</cite> [<a href="https://doi.org/10.1109/TNNLS.2021.3053941" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33764879/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Chen,%20Z.,%20Wu,%20X.%20J.%20&amp;%20Kittler,%20J.%20Relaxed%20block-diagonal%20dictionary%20pair%20learning%20with%20locality%20constraint%20for%20image%20recognition.%20IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.33,%203645%E2%80%933659%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR53">
<span class="label">53.</span><cite>Liu, X., Wu, H. C., Fang, S. L. &amp; et al. Multimodal surface material classification based on ensemble learning with optimized features. In <em>IEEE Int. Conf. E-Health Netw. Appl. Serv.</em>, 1–6 (2021).</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The data used in this article are publicly available online and have been cited in the article.</p>
<p>The source code and models generated during this study are not publicly available due to institutional policy restrictions. However, to support academic collaboration and ensure the reproducibility of our results, the code can be made available from the corresponding author upon reasonable request. Interested parties may contact steven.xpw@ncu.edu.cn to initiate an application for access, which will be processed in accordance with the policies of our laboratory and institution.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-16456-w"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_16456.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (2.7 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12375791/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12375791/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12375791%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12375791/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12375791/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12375791/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40850961/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12375791/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40850961/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12375791/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12375791/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="a35L0TztRhuV78qNFftSqIlFJjCDOAARrtYAIhEXp2bReFGCyO8EJ8NEO78wfFu9">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
