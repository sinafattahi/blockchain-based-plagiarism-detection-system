
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Intelligent identification analysis and process design for highly similar categories using Platycerium as an example - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="152547638AF34A830547630002D061D9.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368074/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Intelligent identification analysis and process design for highly similar categories using Platycerium as an example">
<meta name="citation_author" content="Li-Wei Chen">
<meta name="citation_author_institution" content="Communications Engineering, Feng Chia University, Taichung, Taiwan">
<meta name="citation_author" content="Wei-Lun Lin">
<meta name="citation_author_institution" content="Communications Engineering, Feng Chia University, Taichung, Taiwan">
<meta name="citation_publication_date" content="2025 Aug 20">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30517">
<meta name="citation_doi" content="10.1038/s41598-025-12502-9">
<meta name="citation_pmid" content="40835621">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368074/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368074/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368074/pdf/41598_2025_Article_12502.pdf">
<meta name="description" content="This study tackles the challenge of image recognition for datasets with high inter-class similarity, using 18 native Platycerium species as a case study. Due to their substantial visual similarities, initial training with ResNet50 yielded a baseline ...">
<meta name="og:title" content="Intelligent identification analysis and process design for highly similar categories using Platycerium as an example">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="This study tackles the challenge of image recognition for datasets with high inter-class similarity, using 18 native Platycerium species as a case study. Due to their substantial visual similarities, initial training with ResNet50 yielded a baseline ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368074/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12368074">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-12502-9"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_12502.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12368074%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12368074/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12368074/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368074/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 20;15:30517. doi: <a href="https://doi.org/10.1038/s41598-025-12502-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-12502-9</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Intelligent identification analysis and process design for highly similar categories using Platycerium as an example</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20LW%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Li-Wei Chen</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Li-Wei Chen</span></h3>
<div class="p">
<sup>1</sup>Communications Engineering, Feng Chia University, Taichung, Taiwan </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20LW%22%5BAuthor%5D" class="usa-link"><span class="name western">Li-Wei Chen</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Lin%20WL%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Wei-Lun Lin</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Wei-Lun Lin</span></h3>
<div class="p">
<sup>1</sup>Communications Engineering, Feng Chia University, Taichung, Taiwan </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Lin%20WL%22%5BAuthor%5D" class="usa-link"><span class="name western">Wei-Lun Lin</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Communications Engineering, Feng Chia University, Taichung, Taiwan </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Dec 27; Accepted 2025 Jul 17; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12368074  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40835621/" class="usa-link">40835621</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">This study tackles the challenge of image recognition for datasets with high inter-class similarity, using 18 native Platycerium species as a case study. Due to their substantial visual similarities, initial training with ResNet50 yielded a baseline accuracy of less than 10%. To address this, we conducted a comprehensive analysis using multidimensional confusion matrices to identify seven primary confusion factors, such as image edges, textures, and shapes, and stratified the dataset into processed and unprocessed images optimized for these factors through adjustments in saturation, brightness, and sharpening. A refinement process leveraging confusion matrices and bootstrapping was proposed to address ambiguous classes, significantly improving recognition of highly similar species. Recognition accuracy increased to approximately 60% after applying confusion factor analysis and image optimization, with further gains to over 80% using EfficientNet-b4 and over 90% using EfficientNet-b7. These findings highlight the importance of feature selection and grouped analysis in recognizing highly similar images, offering a robust framework for optimizing recognition accuracy in challenging datasets and providing valuable insights for advancing image recognition technologies.</p>
<section id="sec1"><h3 class="pmc_sec_title">Supplementary Information</h3>
<p>The online version contains supplementary material available at 10.1038/s41598-025-12502-9.</p></section><section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Image recognition, Deep neural networks, Confusion category extraction refinement</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Engineering, Electrical and electronic engineering</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">In the contemporary field of technology, deep learning techniques have made significant strides across various domains, particularly in areas like image recognition, handwriting and text identification, speech recognition, and language translation, demonstrating their robust capabilities. The evolution of these technologies profoundly impacts the development across different academic disciplines, especially in achieving breakthroughs in accuracy and efficiency. Specifically, in the area of image analysis and object recognition, deep learning also shows tremendous potential for application<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>.</p>
<p id="Par3">However, one of the main challenges currently faced is that identifying the physical characteristics of plants through observation becomes increasingly difficult. Even experts in the field struggle to accurately identify some rare plant species, making the task even more daunting for the layperson. In this context, the application of artificial intelligence technologies becomes critically important. By using features such as leaf contour, eccentricity, centroid, and compactness as inputs to the system, plants can be accurately classified. Additionally, techniques like Principal Component Analysis (PCA), Hu’s moment invariants, and morphological feature-based analysis are widely used to develop efficient and accurate automated plant identification tools<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>.</p>
<p id="Par4">Other studies have shown, such as the research on the Taichung Metropolitan Park plant identification system, that combining multiple features of flowers and leaf images captured on smartphones can significantly improve identification accuracy<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>. Similarly, the application of deep learning in real-time flower identification systems has shown outstanding performance, with Google’s Inception v3 model achieving an accuracy rate of over 95%<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>. Furthermore, plant recognition systems based on iOS devices using CNN models have achieved recognition rates of up to 90% for specific plants<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup>. Research on the identification of poisonous plants also underscored its importance in public safety protection, covering 39 types of poisonous plants with an average recognition rate close to 90%<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>.</p>
<p id="Par5">According to literature<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>, a comparison of different connection methods found that cross-layer connections significantly improve classification accuracy. The issue of high confusion in plant identification has been effectively resolved through dataset processing and model optimization<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>. Plant species identification in images or videos poses challenges due to the diversity of species, changes in orientation, viewing angles, and cluttered backgrounds. The work in<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup> also demonstrates that both traditional and deep learning methods were used for identification. In traditional methods, features were extracted using Hu moments, Haralick textures, Local Binary Patterns, and color channel statistics, and classification was performed using multiple classifiers. In deep learning methods, the VGG 16 and VGG 19 CNN models showed higher accuracy in both standard and real-time datasets, surpassing traditional methods<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>.</p>
<p id="Par6">Moreover, the ECOUAN team’s performance in the LifeCLEF 2015 challenge was particularly notable. The team used deep learning methods, with the entire system learning without the need for manually designed components. Through the use of pre-training and fine-tuning strategies with 1.8 million images in convolutional neural networks, the team successfully transferred the recognition capabilities learned from general domains to the plant identification task. This approach exceeded the best results of 2014, ranking fourth among all competing teams and tenth in 18 competitions<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>.</p>
<p id="Par7">Finally, according to the most similar study in<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup> for reference, it is known that plants at the seedling stage are extremely similar to weeds, which fiercely compete with crops for nutrients and water, severely affecting crop yield losses, with the impact reaching nearly 100%. This paper adopted the ResNet network model to enhance accuracy in image classification, ultimately changing the impact of weeds on agriculture<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>.</p>
<p id="Par8">The aforementioned studies<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a>–<a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup> address recognition across a broad range of different animal and plant species, with most species having distinct differences, making it relatively easy to achieve effective recognition results through parameter tuning. However, studies on plants of the same species significantly increase the difficulty of recognition. The main goal of this study is to explore the high similarity of Platycerium, providing valuable contributions to solving future high-similarity recognition problems. The economic value of Platycerium also draws widespread attention in current plant cultivation, and the completion of an automated recognition neural network will aid in the development of this field.</p>
<p id="Par9">In summary, this paper establishes a framework applicable to various datasets and deep learning models. This framework includes directions such as dataset collection and organization, data preprocessing, model construction, model optimization, refined process design for extracting confused categories, and enhancement of high confusion factors. Through the establishment of this framework, we aim to provide more efficient solutions for subsequent recognition-related research, promote the development of image recognition applications across different fields, and further enhance the efficiency and accuracy of image recognition.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Construction—highly similar datasets and basic network models</h2>
<section id="Sec3"><h3 class="pmc_sec_title">Dataset introduction and construction</h3>
<p id="Par10">Platycerium, a distinctive plant within the Polypodiaceae family, primarily inhabit tropical and subtropical regions<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>. They survive as epiphytes, deriving nutrients from decomposing organic matter on trees, and are named for their antler-like fronds. The fronds are primarily divided into two types: the basal trophophores that protect the roots, store water, and facilitate photosynthesis, and the more conspicuous sporophores that, aside from conducting photosynthesis, also use the sporangia on the back of the leaves for reproduction. Due to the high similarity among various Platycerium species, this study explores different datasets and image optimization techniques, aiming to use this similar sample set as a case study to enhance the efficacy of artificial intelligence in image recognition and to provide valuable insights and inspiration for related fields.</p>
<p id="Par11">For performance experiments, two datasets were utilized. The original image dataset was trained using a neural network without any image optimization, while the complex background dataset employed a special image optimization technique. This technique retained the parts of the image necessary for recognition learning while removing irrelevant background elements to enhance recognition performance.</p>
<p id="Par12">Before training the network model with the datasets, all images with reading errors and related error messages were reviewed and displayed. These images were subsequently excluded from the training process to ensure the quality of the network model training. At the same time, the most suitable dataset for subsequent experiments was identified.</p>
<p id="Par13">The research utilized Platycerium as examples, obtaining samples through both botanical garden photography and online searches, labeled by the authors, and augmented using Keras with rotation, flipping, and scaling to enhance diversity, resulting in 18 different species including Alcicorne, Bifurcatum, Veitchii, Willinckii, Andinum, Elephantotis, Coronarium, Ellisii, Hillii, Quadridichotomum, Stemaria, Grande, Superbum, Wandae, Holttumii, Ridleyi, Madagascariense, and Wallichii, with an average of 90 images collected per species as shown in Fig. <a href="#Fig1" class="usa-link">1</a>. See Appendix A for detailed characteristics of the 18 Platycerium species. The sporophores of Alcicorne, Bifurcatum, Veitchii, and Willinckii exhibit bifurcation, with those of Alcicorne and Veitchii specifically growing upwards; Andinum and Elephantotis feature large, elongated oval leaves; Coronarium sporophores are spoon-shaped; Ellisii, Hillii, Quadridichotomum, and Stemaria have broad leaf shapes with slight differences; Grande, Superbum, Wandae, and Holttumii are large Platycerium, the differences between Grande and Superbum lie in the branching of the sporophores, while Wandae and Holttumii differ in the splitting patterns of their sporophores.; Ridleyi and Madagascariense sporophores feature brain-like patterns with minor differences; Wallichii sporophores resemble butterfly wings.</p>
<figure class="fig xbox font-sm" id="Fig1"><h4 class="obj_head">Fig. 1.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/992230325485/41598_2025_12502_Fig1_HTML.jpg" loading="lazy" id="MO1" height="349" width="692" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Dataset for Platycerium.</p></figcaption></figure></section></section><section id="Sec4"><h2 class="pmc_sec_title">Network model architecture optimization</h2>
<section id="Sec5"><h3 class="pmc_sec_title">Construction and optimization of network model architecture</h3>
<p id="Par14">This paper adopted the ResNet50 residual neural network as the core architecture due to its established role as a standard benchmark in plant identification studies, typically achieving high accuracies in less challenging datasets<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a>,<a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>. This choice allows us to highlight the specific difficulties posed by the high inter-class similarity of Platycerium species, which significantly reduces baseline performance. ResNet50 effectively addresses the performance degradation problem caused by training error accumulation<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a>,<a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>, thereby enhancing training efficiency and speed. To further optimize the model, we adjusted hyperparameters during the training process and introduced batch normalization modules, confirming their effectiveness in addressing the challenges of gradient vanishing and explosion.</p>
<p id="Par15">Our experimental results indicated that removing complex backgrounds from the dataset slightly improves the model’s accuracy on the validation set, demonstrating that our initially constructed network model still has room for overall performance improvement<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. Furthermore, we delved into the application of the deep residual learning framework, which achieves exemplary accuracy at a depth of 152 layers on ImageNet<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. We analyzed residual blocks and incorporated optimized units from<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>, enhancing identity mappings in ResNet50-v2 to improve training efficiency. Due to ResNet50-v2’s suboptimal performance (~ 60% accuracy) on this challenging dataset, we evaluated EfficientNet (b4 and b7) as a planned alternative, leveraging its compound scaling method<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup> for improved efficiency and accuracy.</p>
<p id="Par16">In this experiment, for the construction and performance evaluation of the baseline network model, we utilized datasets from 18 native species of staghorn ferns, each category averaging 90 images. By comparing the original image dataset with the dataset processed to remove complex backgrounds, we trained using the ResNet50 model. Although previous research showed higher recognition accuracy, our baseline model, facing an unoptimized dataset with high similarity, showed less than 10% accuracy, highlighting the need for improved handling of highly similar datasets. The reason can be primarily attributed to the high inter-class similarity among the 18 Platycerium species, which poses a significant challenge for standard deep learning models without tailored preprocessing or optimization. In the unoptimized dataset, visual features such as leaf shapes, textures, and edges are nearly indistinguishable across species, leading to poor discriminative performance.</p>
<p id="Par17">Simultaneously, this study also focuses on the problem of hyperparameter search in the machine learning field and its optimization challenges. Hyperparameter selection plays a crucial role in the model construction process, affecting the model’s final performance. The complexity and importance of this selection process further affirm that establishing disciplined and theoretically sound search strategies is critical for achieving efficient models<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>.</p>
<p id="Par18">Precise configuration of hyperparameters is vital for preventing overfitting. In our experiments, we adjusted batch size, learning rate, and iteration number to determine the optimal hyperparameter settings in the training of the ResNet50-v2 model, laying the foundation for subsequent experiments<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>. Additionally, we incorporated callback function modules to monitor the training process and automatically adjust the learning rate, effectively preventing overfitting<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>.</p>
<p id="Par19">Finally, the dataset was divided into 70% (1134 images) for training, 20% (324 images) for validation, and 10% (162 images) for testing. We trained using the ResNet50-v2 and EfficientNet-b3 models, and presented the data results through historical trend charts. Tables <a href="#Tab1" class="usa-link">1</a> and <a href="#Tab2" class="usa-link">2</a> clearly demonstrate the significant enhancements in recognition performance of the network models due to these optimization modules. We have conducted at least five independent training runs with different random seeds to obtain the provided accuracy.</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>ResNet50-v2 under various indicator modules.</p></div>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Tab1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/293a486ec69e/41598_2025_12502_Tab1_HTML.jpg" loading="lazy" id="MO100" height="151" width="692" alt="graphic file with name 41598_2025_12502_Tab1_HTML.jpg"></a></p>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>EfficientNet-b3 under various indicator modules.</p></div>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Tab2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/49b693b74661/41598_2025_12502_Tab2_HTML.jpg" loading="lazy" id="MO101" height="151" width="692" alt="graphic file with name 41598_2025_12502_Tab2_HTML.jpg"></a></p>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec6"><h3 class="pmc_sec_title">Data augmentation</h3>
<p id="Par20">In this study, we extensively explored Keras-based data augmentation to enhance the training dataset, generating diverse images from the original dataset to address insufficient data volume. We applied transformations including rotation angles (up to 30°), position shifts (up to 20% of image dimensions), shear transformations (up to 0.2 radians), scaling ratios (0.8 to 1.2), and horizontal and vertical flips to ensure dataset diversity<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>. To mitigate the risk of overfitting to superficial visual cues introduced by OpenCV-based enhancements (e.g., sharpening, brightness adjustments), we expanded the augmentation pipeline to include random noise injection (Gaussian noise with a standard deviation of 0.01) and color jittering (random adjustments to hue by ± 0.1, contrast by ± 0.2, and brightness by ± 0.2). These additions promote the learning of robust, semantically meaningful features, reducing reliance on specific visual artifacts.</p>
<p id="Par21">The large-scale image ontology database, ImageNet, built on WordNet, provides tens of millions of annotated images, covering 80,000 synsets, offering a rich and diverse resource for computer vision research<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>. In model training, we utilize the pre-trained weights from ImageNet to enrich the model’s image information, effectively reducing the errors associated with training from scratch and accelerating the convergence process of the model, significantly enhancing the image recognition performance of the target detection<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>.</p>
<p id="Par22">Referencing Table <a href="#Tab3" class="usa-link">3</a>, the deep learning model ResNet50, leveraging the strengths of convolutional operations, demonstrates superior performance in classification tasks, particularly with image-based datasets, compared to traditional machine learning approaches such as support vector machine (SVM), decision tree, and random forest. Moreover, the integration of Keras’s Data Augmentation and ImageNet pre-trained weights has significantly improved model training efficiency. After data augmentation, the accuracy of the model on the validation set has noticeably increased, demonstrating that optimizing image recognition and learning frameworks can effectively enhance recognition accuracy, achieving accuracy to around 60%<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>.</p>
<section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>ML approaches and ResNet50 with/without data augmentation.</p></div>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/b6d047c19e03/41598_2025_12502_Tab3_HTML.jpg" loading="lazy" id="MO102" height="400" width="628" alt="graphic file with name 41598_2025_12502_Tab3_HTML.jpg"></p>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par23">In this experimental approach, techniques such as hyperparameter tuning, callback functions, and data shuffling were introduced to enhance the foundational performance of the model. Combined with ImageNet’s pre-trained weights and Keras data augmentation techniques, a comparative study between the original image dataset and the optimized complex background dataset was conducted. Our model achieved higher diversity and cognitive performance during training. Although these optimization methods have increased the recognition accuracy to 60%, the issue of confusion among highly similar species remains.</p>
<p id="Par24">Comprehensive experimental results indicated that, while the network model structure enhanced through data augmentation and improved image cognitive performance achieves certain success in image recognition as demonstrated in<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a>–<a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>, there is still room for improvement in recognition performance for datasets with high degrees of similarity. In subsequent experiments, we incorporate refined process designs for extracting confused categories and use image data statistical reports and confusion matrices for detailed observation and analysis of the original images. Ultimately, the OpenCV programming library was utilized to customize enhancements for factors causing confusion, further optimizing the network model’s training efficiency.</p></section></section><section id="Sec7"><h2 class="pmc_sec_title">Confusion category extraction refinement method</h2>
<section id="Sec8"><h3 class="pmc_sec_title">Confusion category extraction refinement process design</h3>
<p id="Par25">We trained our deep learning model using the ResNet50-v2 architecture, combined with various optimization strategies such as Keras data augmentation, pre-trained ImageNet weights, and callback functions. The collective effect of these strategies results in the multidimensional confusion matrix displayed in Fig. <a href="#Fig2" class="usa-link">2</a><sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>, used to evaluate the actual versus predicted data across 18 classification categories.</p>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/3851132c5394/41598_2025_12502_Fig2_HTML.jpg" loading="lazy" id="MO2" height="588" width="703" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>18-dimensional confusion matrix.</p></figcaption></figure><p id="Par26">In preliminary experiments, we observed that the traditional ResNet50 model performed poorly in identifying high confusion rate categories. Therefore, we adopted ResNet50-v2 as the base model<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>, which optimized training loss through identity mapping and improved the network’s learning capabilities. Based on the evaluation of actual versus predicted data, we identified the need for a more in-depth analysis method. Consequently, we explored bootstrapping as an additional analytical approach.</p>
<p id="Par27">In our study’s various analytical method discussions, we referenced statistical methods to address the issue of class imbalance<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>, which enhances data accuracy and focus on solutions for supervised classification problems, encompassing classifier construction, object representation, and evaluation<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>. We also delved into the application of deep learning in solving partial differential equations, becoming familiar with information on Physics-Informed Neural Networks (PINNs)<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>. We learned that the research adopted the RAR method to improve the training efficiency of PINNs and the Python library DeepXDE to support applications in complex geometric domains. Additionally, deep learning calibration for option pricing models<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup> highlight the shortcomings of existing methods and propose improvements. Moreover, we have learned about the individual difference measurement methods for closed-demand<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>, and finally, the concept of rapid mixed-dimensionality reduction methods for feature selection and extraction is provided in<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>, which effectively eliminate redundant information.</p>
<p id="Par28">In view of the various analytical methods discussed above, this research designed a concept based on feature selection and grouped analysis methods. The refined design process for extracting confused categories obtained through the bootstrapping process is illustrated in Fig. <a href="#Fig3" class="usa-link">3</a>.</p>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/0f14d14b8742/41598_2025_12502_Fig3_HTML.jpg" loading="lazy" id="MO3" height="351" width="686" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Bootstrapping resamples data to evaluate stability, combined with grouped analysis to cluster similar features (e.g., edges, textures), enhancing confusion category extraction.</p></figcaption></figure><p id="Par29">In this study, we employed bootstrapping and grouped analysis to refine the extraction of confusion categories from the Platycerium dataset. Bootstrapping was used to resample the dataset iteratively, generating multiple subsets to assess the stability and robustness of the model’s performance across the 18 highly similar species. For each iteration, we evaluated and filtered feature combinations—such as edges, textures, and shapes—that contributed most to classification errors, as identified by the multidimensional confusion matrix. Subsequently, grouped analysis clustered these features into similarity-based groups, enabling us to pinpoint specific confusion factors (e.g., overlapping leaf textures or edge patterns) that hindered accurate recognition. This iterative process was repeated 10 times, balancing computational efficiency with comprehensive analysis of all categories. By resampling and clustering in this manner, we minimized redundant training efforts typically associated with bootstrapping alone, while enhancing the model’s ability to distinguish subtle differences. Furthermore, we observed that the effectiveness of this approach scaled with dataset size: larger datasets amplified the feedback from grouped analysis, leading to more precise identification of confusion factors.</p>
<p id="Par30">As shown in Fig. <a href="#Fig4" class="usa-link">4</a>, through the comparison between feature selection, grouped analysis methods, and bootstrapping, we found that the process using bootstrapping, combined with feature selection and grouped analysis, is more efficient when handling datasets with balanced and sufficient amount of samples. We thus propose a refined process for extracting confused categories. Initially, using the data results produced by the multidimensional confusion matrix and applying feature selection and grouped analysis, we enhanced the precise identification and classification of highly similar plant species. Finally, using image data statistical reports for visualization and analysis, in-depth observation and analysis of various categories of images are conducted during the refined extraction process. During the observation of images, we noticed plant species with apparent similarities showing only about 20% similarity in data results. Therefore, 20% is served as a benchmark for similarity assessment to identify factors causing high confusion rates. This study ultimately categorizes seven types of similar plant species, extracting key factors causing high confusion rates, such as image edges, textures, and shapes. This classification not only relies on physical features but also considers the performance and interrelations of categories within the dataset, providing a crucial methodology to enhance the overall efficiency of the image recognition system.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/97c35e4d8f4e/41598_2025_12502_Fig4_HTML.jpg" loading="lazy" id="MO4" height="305" width="686" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Algorithm flow for confusion class extraction refinement.</p></figcaption></figure><p id="Par31">Synthesizing the analyses above, this study not only enhances the efficiency of data processing, but also effectively reveals the key factors impacting the model’s recognition capabilities through precise bootstrapping processes and feature selection grouped analysis. This holds significant practical significance for understanding and improving the performance of classification models.</p></section><section id="Sec9"><h3 class="pmc_sec_title">Enhanced high confusion factors</h3>
<p id="Par32">Building on deep learning insights, we applied OpenCV to preprocess images, enhancing features like edges and textures identified as confusion factors. To address concerns about overfitting to superficial visual cues, we incorporated regularization and validation techniques to ensure the model learns semantically meaningful features. Specifically, we employed dropout (rate of 0.3) and L2 regularization (coefficient of 0.01) in the fully connected layers of EfficientNet-b4 and EfficientNet-b7, reducing reliance on enhanced features. Additionally, Grad-CAM visualization confirms that the model prioritizes biologically relevant features (e.g., leaf shapes and sporophore patterns) over superficial enhancements, validating its robustness<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup>.</p>
<p id="Par33">Additionally, this research provides a brief introduction to the fundamentals of OpenCV, particularly tailored for programmers at all levels, especially those focusing on image processing and computer vision research. This introduction not only offers a basic understanding of OpenCV’s extensive functionalities but also highlights its flexibility and practicality in various application scenarios<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>.</p>
<p id="Par34">Furthermore, this study delves into specific examples and steps of OpenCV in real-time image processing applications, including an in-depth analysis of its application in this field, covering its practicality and efficiency in solving real-time problems. This discussion is crucial for understanding the central role of OpenCV in modern image processing technologies<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>.</p>
<p id="Par35">The primary objective of this research is to explore the impact of high-confusion factor enhancement on the recognition efficiency of deep learning network models. We initially trained the ResNet50-v2 model to assess enhancement effects, optimizing hyperparameters (batch size, learning rate, iteration number) and using callback functions to dynamically adjust the learning rate, preventing overfitting as detailed in Section “<a href="#Sec5" class="usa-link">Construction and optimization of network model architecture</a>”<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a>,<a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>.</p>
<p id="Par36">Subsequently, the study further extends the scope of high-confusion factor enhancement by employing various techniques, such as sharpening, which involves aligning the kernel with the corresponding area of the image through convolution operations, multiplying the values within the kernel with the pixel values in the image area, and then summing the results to obtain the new pixel value at that position. The pixel value at the same coordinates after applying the convolution operation is provided as</p>
<table class="disp-formula p" id="Equa"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/1e5ca3ecdf6a/d33e488.gif" loading="lazy" id="d33e488" alt="graphic file with name d33e488.gif"></td></tr></table>
<p>where <em>f</em>(<em>x</em>,<em>y</em>) represents the pixel value of the original image at coordinates (<em>x</em>,<em>y</em>), and <em>k</em>(<em>m</em>,<em>n</em>) indicates the offset of the convolution kernel at coordinates (<em>m</em>,<em>n</em>) relative to the kernel center. This specific sharpening kernel is used to enhance the image’s edges, making it appear clearer and improving the contrast between a pixel and its surrounding neighborhood.</p>
<p id="Par37">Adjustments to the image’s brightness and saturation are made by manipulating the lightness and saturation channels in the HLS (hue, saturation, lightness) color space. The lightness and saturation formulae are provided as</p>
<table class="disp-formula p" id="Equb"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/8b5ca48ac023/d33e527.gif" loading="lazy" id="d33e527" alt="graphic file with name d33e527.gif"></td></tr></table>
<p>and</p>
<table class="disp-formula p" id="Equc"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/9d23b93b38ce/d33e533.gif" loading="lazy" id="d33e533" alt="graphic file with name d33e533.gif"></td></tr></table>
<p>respectively, where <em>L</em> represents the original lightness value and <em>L</em>′ is the adjusted lightness value; “lightness” refers to the percentage adjustment of brightness; Similarly, <em>S</em> denotes the original saturation value, and <em>S</em>′ is the adjusted saturation value; “saturation” indicates the percentage adjustment of saturation. If the computed new lightness or saturation values exceed 1, they are capped at 1 to ensure values remain within a reasonable range. This is because the values in the HLS color space’s L and S channels are confined to the [0,1] range, representing brightness and saturation percentages.</p>
<p id="Par38">Normalization was also employed. The objective is to linearly normalize the image data to the specified range. The computation is provide as following.</p>
<table class="disp-formula p" id="Equd"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/6cf175a126c3/d33e554.gif" loading="lazy" id="d33e554" alt="graphic file with name d33e554.gif"></td></tr></table>
<p>where X is the original image data, Y represents the normalized image data, and the parameters α and β are the scaling factor and bias, respectively, used to adjust the output range. Here, α and β are set to 350 and 10, respectively. max(X) and min(X) represent the maximum and minimum values in the image data, respectively. The formula linearly maps the original image data to a new range. By adjusting the values of α and β, the mapped data range can be controlled to enhance image contrast or adjust brightness. The normalization type used implies that the data will be linearly mapped to the range specified by α and β. Finally, gamma correction is employed to adjust the image brightness, The computation is as follows:</p>
<table class="disp-formula p" id="Eque"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/b6d0b25a04a2/d33e560.gif" loading="lazy" id="d33e560" alt="graphic file with name d33e560.gif"></td></tr></table>
<p>where γ is the gamma value, which is set to 0.4 throughout. Gamma correction is a nonlinear operation used to adjust image contrast, particularly enhancing the visibility of dark details. When γ &lt; 1, the dark details of the image are enhanced, and when γ &gt; 1, the bright details are enhanced. In our experiments, by setting γ to 0.4, we aim to enhance the dark details of images.</p>
<p id="Par39">To evaluate these techniques, we conducted experiments using EfficientNet-b4 and EfficientNet-b7, incorporating regularization and expanded data augmentation (Section “<a href="#Sec6" class="usa-link">Data augmentation</a>”) to mitigate overfitting. Table <a href="#Tab4" class="usa-link">4</a> compares model performance, with EfficientNet-b7 achieving the highest accuracy (98.46%), precision, recall, and F1 score, outperforming ResNet50-v2 due to its compound scaling<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>. Grad-CAM visualizations<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup> as show in Fig. <a href="#Fig5" class="usa-link">5</a> confirm that EfficientNet-b7 focuses on biologically relevant features, reducing the risk of overfitting to superficial cues like enhanced edges or features. Heatmaps showing that EfficientNet-b7 focuses on biologically relevant features like leaf shapes and sporophore patterns, confirming that the model learns semantically meaningful features rather than superficial enhancements.</p>
<section class="tw xbox font-sm" id="Tab4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>Performance comparison between ResNet50-v2 and EfficientNet.</p></div>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Tab4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/4b2a03dfe94d/41598_2025_12502_Tab4_HTML.jpg" loading="lazy" id="MO104" height="106" width="682" alt="graphic file with name 41598_2025_12502_Tab4_HTML.jpg"></a></p>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/5e76256533a8/41598_2025_12502_Fig5_HTML.jpg" loading="lazy" id="MO5" height="273" width="691" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Grad-CAM visualization when adopting EfficientNet-b7.</p></figcaption></figure><p id="Par40">Table <a href="#Tab4" class="usa-link">4</a> compares model performance, with EfficientNet-b7 achieving the highest accuracy, precision, recall, and F1 score, outperforming ResNet50-v2 due to its compound scaling<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>. In this experiment, we divided the dataset into images processed for high confusion factor enhancement and unprocessed original images. OpenCV was used for custom enhancement settings, improving recognition of difficult features. The combination of regularization, hyperparameter optimization, callback functions, and expanded data augmentation significantly improved recognition efficiency, with EfficientNet-b7 achieving over 98% accuracy. These results validate the effectiveness of the refined process for extracting confused categories and high-confusion factor enhancements, with EfficientNet-b7 outperforming other models due to its optimized feature extraction for highly similar datasets.</p>
<p id="Par41">To further illustrate the effectiveness of the proposed methods, Table <a href="#Tab5" class="usa-link">5</a> presents six figures that visualize the training dynamics of the network models, ResNet50-v2, EfficientNet-b4, and EfficientNet-b7. These plots clearly demonstrate that, as the number of epochs increases, both the accuracy and loss metrics stabilize, indicating convergence of the training process. The steady rise in accuracy and the consistent decline in loss, with minimal fluctuations in the later epochs, provide robust evidence that the training is complete and has reached a converged state. This convergence validates the reliability of the network architectures and the optimization strategies employed, including data augmentation, hyperparameter tuning, and high confusion factor enhancement, in effectively learning the features of the highly similar Platycerium dataset.</p>
<section class="tw xbox font-sm" id="Tab5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>Accuracy and loss for training and validation for ResNet50-v2, EfficientNet-b4, and EfficientNet-b7.</p></div>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Tab5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/370d865153f4/41598_2025_12502_Tab5_HTML.jpg" loading="lazy" id="MO105" height="621" width="781" alt="graphic file with name 41598_2025_12502_Tab5_HTML.jpg"></a></p>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par42">Furthermore, Fig. <a href="#Fig6" class="usa-link">6</a> illustrates the traditional image recognition model architecture alongside the refined process design for extracting confused categories and the high confusion factor enhancement treatment proposed in this study. By examining the performance differences of the ResNet50-v2 network model, it is evident from the figure that after applying the refined process design for extracting confused categories and high confusion factor enhancement treatment, there is a significant improvement in the model’s recognition rate on the test set, and a substantial reduction in test set loss values. Additionally, the experiments were conducted on an NVIDIA RTX 3080 GPU with 48 GB of memory. Training ResNet50-v2 took approximately 4 h per run, while EfficientNet-b7 required around 6 h due to its larger architecture. Memory consumption peaked at 40 GB for EfficientNet-b7.</p>
<figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/6bbad2477d29/41598_2025_12502_Fig6_HTML.jpg" loading="lazy" id="MO6" height="368" width="679" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Performance for high obfuscation factor enhancement.</p></figcaption></figure><p id="Par43">In summary, this study makes a significant contribution to enhancing image recognition performance by combining the refined process of extracting confused categories with high confusion factor enhancement techniques, along with various convolutional network models. This provides valuable guidance and reference for future research in related fields.</p></section><section id="Sec10"><h3 class="pmc_sec_title">Comparative discussion: CNNs versus transformer-based architectures on highly similar datasets</h3>
<p id="Par44">Although transformer-based models such as DaViT<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup> and Swin-B Transformer<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup> have demonstrated state-of-the-art performance in various large-scale image recognition tasks, recent studies<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a>–<a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup> suggest that convolutional neural networks (CNNs) remain highly competitive—and often superior—in scenarios involving small or medium-sized datasets with high intra-class similarity. In our experiments, EfficientNet-b7, a CNN with compound scaling, outperformed both DaViT and Swin-B on the Platycerium dataset, achieving 98.46% accuracy compared to 64.1% and 78.04% respectively, as shown in Table <a href="#Tab6" class="usa-link">6</a>, where both Transformer-based and CNN-based networks were trained under identical experimental settings, including regularization, hyperparameter optimization, callback mechanisms, and comprehensive data augmentation to ensure a fair and consistent performance comparison.</p>
<section class="tw xbox font-sm" id="Tab6"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>Performance comparison between transformer-based and CNN-based networks.</p></div>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368074_41598_2025_12502_Tab6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4873/12368074/062728bdfeb2/41598_2025_12502_Tab6_HTML.jpg" loading="lazy" id="MO106" height="85" width="688" alt="graphic file with name 41598_2025_12502_Tab6_HTML.jpg"></a></p>
<div class="p text-right font-secondary"><a href="table/Tab6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par45">This observation is consistent with prior research which demonstrates that CNNs better capture local visual patterns such as edges and textures, which are crucial for distinguishing between species with minor morphological differences<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a>,<a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>. Besides, transformers tend to require large datasets and substantial pretraining to generalize effectively<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>. Additionally, fine-grained classification tasks, such as plant species or medical image recognition, CNN-based architectures often yield more stable performance with lower risk of overfitting, especially in the absence of extensive pretraining<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a>,<a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>.</p>
<p id="Par46">Therefore, in high-similarity datasets where local texture, contour, and shape features dominate, CNN-based models—especially those with enhanced feature selection processes like in this study—remain the preferred choice over transformer-based counterparts. These findings reaffirm the importance of task-specific model selection rather than defaulting to the most recent architectural trend.</p></section></section><section id="Sec11"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par47">In this study, we delved into optimizing image recognition within datasets with high similarity, exploring four experimental avenues. These directions not only showcase novel technical applications but also yield three significant contributions toward bolstering the overall efficiency of image recognition technology. Firstly, we devised network model architectures tailored to diverse sample sets. Secondly, we crafted refined processes for extracting obfuscation categories, along with custom settings for enhancing high obfuscation factors.</p>
<p id="Par48">Our initial experiments training the ResNet-50 base network on such datasets revealed a recognition accuracy of less than 10%, underscoring the imperative for advancements in processing highly similar data. Despite employing optimization strategies such as hyperparameter tuning and data augmentation techniques, the recognition accuracy improved to 60%, underscoring persisting issues of confusion among closely resembling species.</p>
<p id="Par49">Our main contribution focused on refining the process of obfuscation category extraction, integrating bootstrapping techniques with multidimensional confusion matrices and feature selection concepts. This culminated in a comprehensive methodology utilizing statistical analysis for data visualization, pinpointing factors driving high confusion rates and enhancing overall recognition efficiency.</p>
<p id="Par50">In our performance enhancement pursuit, we leveraged OpenCV to amplify key factors identified in obfuscation category extraction, resulting in notable improvements in recognition rates. Notably, EfficientNet-b7 emerged as the superior model, outperforming others significantly across multiple assessments.</p>
<p id="Par51">While transformer-based models offer promising performance in large-scale general-purpose datasets, our experiments and recent comparative studies confirm that convolutional models remain more effective in fine-grained classification tasks involving high inter-class similarity and smaller dataset sizes. In particular, EfficientNet-b7 not only surpassed DaViT and Swin-B in accuracy but also required less training time, reinforcing its suitability for plant species recognition with limited and visually similar samples.</p>
<p id="Par52">These contributions not only elevate recognition performance but also unveil critical challenges in identifying highly similar datasets. They offer crucial directions and strategies for future image recognition technologies, particularly in tackling highly similar features, devising comprehensive optimization processes, and furnishing effective methodologies for subsequent research endeavors.</p></section><section id="Sec12"><h2 class="pmc_sec_title">Electronic supplementary material</h2>
<p>Below is the link to the electronic supplementary material.</p>
<section class="sm xbox font-sm" id="MOESM1"><div class="media p"><div class="caption">
<a href="/articles/instance/12368074/bin/41598_2025_12502_MOESM1_ESM.docx" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material 1</a><sup> (20.5KB, docx) </sup>
</div></div></section></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgement</h2>
<p>This work was supported by the National Science and Technology Council, Taiwan, under contract NSTC 114-2221-E-035-004.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>Li-Wei Chen and Wei-Lun Lin wrote the main manuscript text and prepared all figures all together. All authors reviewed the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The datasets used and analysed during the current study available from the corresponding author on reasonable request. LI-WEI CHEN undertook the formal identification of the plant material used in study.</p></section><section id="notes3"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par53">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Pan, H., Pang, Z., Wang, Y., Wang, Y. &amp; Chen, L. A new image recognition and classification method combining transfer learning algorithm and MobileNet model for welding defects. <em>IEEE Access</em><strong>8</strong>, 119951–119960 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Pan,%20H.,%20Pang,%20Z.,%20Wang,%20Y.,%20Wang,%20Y.%20&amp;%20Chen,%20L.%20A%20new%20image%20recognition%20and%20classification%20method%20combining%20transfer%20learning%20algorithm%20and%20MobileNet%20model%20for%20welding%20defects.%20IEEE%20Access8,%20119951%E2%80%93119960%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Sharma, S. &amp; Gupta, C. A review of plant recognition methods and algorithms. <em>Int. J. Innov. Res. Adv. Eng.</em><strong>2</strong>, 2349–2163 (2015).</cite> [<a href="https://scholar.google.com/scholar_lookup?Sharma,%20S.%20&amp;%20Gupta,%20C.%20A%20review%20of%20plant%20recognition%20methods%20and%20algorithms.%20Int.%20J.%20Innov.%20Res.%20Adv.%20Eng.2,%202349%E2%80%932163%20(2015)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Yang, Z.-H. A Plant Recognition System Based on Flowers and Leaves. Master’s thesis, National Chiao Tung University (2019).</cite>
</li>
<li id="CR4">
<span class="label">4.</span><cite>Lai, A.-C. Study on the Application of Deep Learning and Transfer Learning to a Real-Time Flower Recognition System. Master’s thesis, Chien Hsin University of Science and Technology (2022).</cite>
</li>
<li id="CR5">
<span class="label">5.</span><cite>Tung, Y.-T. A Plant Recognition System for iOS Devices - A Case Study of the First University of Science and Technology Campus. Master’s thesis, National Kaohsiung First University of Science and Technology (2015).</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>Wang, P.-L. Research on Image-Based Recognition of Poisonous Plants. Master’s thesis, Vanung University (2020).</cite>
</li>
<li id="CR7">
<span class="label">7.</span><cite>Su, P.-W. Neural Network Systems with Residual Fully Connected Layers. Master’s thesis, Feng Chia University (2022).</cite>
</li>
<li id="CR8">
<span class="label">8.</span><cite>Lin, T.-M. Plant Recognition and Survey System Based on Geographic and Image Information. Master’s thesis, National Chi Nan University (2014).</cite>
</li>
<li id="CR9">
<span class="label">9.</span><cite>Pearline, S. A., Kumar, V. S. &amp; Harini, S. A study on plant recognition using conventional image processing and deep learning approaches. <em>J. Intell. Fuzzy Syst.</em><strong>36</strong>, 1997–2004 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Pearline,%20S.%20A.,%20Kumar,%20V.%20S.%20&amp;%20Harini,%20S.%20A%20study%20on%20plant%20recognition%20using%20conventional%20image%20processing%20and%20deep%20learning%20approaches.%20J.%20Intell.%20Fuzzy%20Syst.36,%201997%E2%80%932004%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR10">
<span class="label">10.</span><cite>Reyes, K., Caicedo, J. C. &amp; Camargo, J. E. Fine-tuning deep convolutional networks for plant recognition. <em>Proc. CLEF Work. Notes</em><strong>1391</strong>, 467–475 (2015).</cite> [<a href="https://scholar.google.com/scholar_lookup?Reyes,%20K.,%20Caicedo,%20J.%20C.%20&amp;%20Camargo,%20J.%20E.%20Fine-tuning%20deep%20convolutional%20networks%20for%20plant%20recognition.%20Proc.%20CLEF%20Work.%20Notes1391,%20467%E2%80%93475%20(2015)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>Chauhan, P., Mandoria, H. L. &amp; Negi, A. Deep residual neural network for plant seedling image classification. <em>Agric. Inform.</em> 131–146 (2021).</cite>
</li>
<li id="CR12">
<span class="label">12.</span><cite>Poyu. 18 Native Species Platycerium Illustration Guide - Platycerium. <a href="https://doromon01.com/design/platycerium/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://doromon01.com/design/platycerium/</a> (2019).</cite>
</li>
<li id="CR13">
<span class="label">13.</span><cite>Freedman, D. A. Bootstrapping regression models. <em>Ann. Stat.</em><strong>9</strong>, 1218–1228 (1982).</cite> [<a href="https://scholar.google.com/scholar_lookup?Freedman,%20D.%20A.%20Bootstrapping%20regression%20models.%20Ann.%20Stat.9,%201218%E2%80%931228%20(1982)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <em>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</em> 770–778 (2016).</cite>
</li>
<li id="CR15">
<span class="label">15.</span><cite>He, K. et al. Identity mappings in deep residual networks. <em>arXiv</em> preprint <a href="http://arxiv.org/abs/1603.05027" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1603.05027</a> (2016).</cite>
</li>
<li id="CR16">
<span class="label">16.</span><cite>Tan, M. &amp; Le, Q. V. EfficientNet: Rethinking model scaling for convolutional neural networks. arXiv preprint <a href="http://arxiv.org/abs/1905.11946" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1905.11946</a> (2019).</cite>
</li>
<li id="CR17">
<span class="label">17.</span><cite>Claesen, M. &amp; De Moor, B. Hyperparameter search in machine learning. arXiv preprint <a href="http://arxiv.org/abs/1502.02127" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1502.02127</a> (2015).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>Ioffe, S. &amp; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint <a href="http://arxiv.org/abs/1502.03167v3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1502.03167v3</a> (2015).</cite>
</li>
<li id="CR19">
<span class="label">19.</span><cite>De, S. &amp; Smith, S. L. Batch normalization biases residual blocks towards the identity function in deep networks. arXiv preprint <a href="http://arxiv.org/abs/2002.10444" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2002.10444</a> (2020).</cite>
</li>
<li id="CR20">
<span class="label">20.</span><cite>Perez, L. &amp; Wang, J. The effectiveness of data augmentation in image classification using deep learning. arXiv preprint <a href="http://arxiv.org/abs/1712.04621" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1712.04621</a> (2017).</cite>
</li>
<li id="CR21">
<span class="label">21.</span><cite>Deng, J. et al. ImageNet: A large-scale hierarchical image database. In <em>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–255 (2009).</cite>
</li>
<li id="CR22">
<span class="label">22.</span><cite>Abney, S. Bootstrapping. In <em>Proceedings of the Annual Meeting of the Association for Computational Linguistics</em> 360–367 (2002).</cite>
</li>
<li id="CR23">
<span class="label">23.</span><cite>Shorten, C. &amp; Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. <em>J. Big Data</em><strong>6</strong>, 60 (2019).</cite> [<a href="https://doi.org/10.1186/s40537-021-00492-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8287113/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34306963/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Shorten,%20C.%20&amp;%20Khoshgoftaar,%20T.%20M.%20A%20survey%20on%20image%20data%20augmentation%20for%20deep%20learning.%20J.%20Big%20Data6,%2060%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Visa, S., Ramsay, B., Ralescu, A. &amp; Knaap, E. Confusion matrix-based feature selection. In <em>Proceedings of the Midwest Artificial Intelligence and Cognitive Science Conference (MAICS)</em> 120–127 (2011).</cite>
</li>
<li id="CR25">
<span class="label">25.</span><cite>Zhang, R., Li, W. &amp; Tong, M. Review of deep learning. <em>Inf. Control</em><strong>47</strong>, 385–397 (2018).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20R.,%20Li,%20W.%20&amp;%20Tong,%20M.%20Review%20of%20deep%20learning.%20Inf.%20Control47,%20385%E2%80%93397%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<span class="label">26.</span><cite>Kulkarni, A., Chong, D. &amp; Batarseh, F. A. Foundations of data imbalance and solutions for a data democracy. In <em>Data Democracy</em>, 83–106 (Academic Press, 2020).</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Loog, M. Supervised classification: Quite a brief overview. In <em>Machine Learning Techniques for Space Weather</em>, 113–145 (2018).</cite>
</li>
<li id="CR28">
<span class="label">28.</span><cite>Lu, L. et al. DeepXDE: A deep learning library for solving differential equations. arXiv preprint <a href="http://arxiv.org/abs/1907.04502" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1907.04502</a> (2019).</cite>
</li>
<li id="CR29">
<span class="label">29.</span><cite>Itkin, A. Deep learning calibration of option pricing models: Some pitfalls and solutions. arXiv preprint <a href="http://arxiv.org/abs/1906.03507" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:1906.03507</a> (2019).</cite>
</li>
<li id="CR30">
<span class="label">30.</span><cite>Webster, D. M. &amp; Kruglanski, A. W. Individual differences in need for cognitive closure. <em>J. Pers. Soc. Psychol.</em><strong>67</strong>, 1049–1062 (1994).
</cite> [<a href="https://doi.org/10.1037//0022-3514.67.6.1049" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/7815301/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Webster,%20D.%20M.%20&amp;%20Kruglanski,%20A.%20W.%20Individual%20differences%20in%20need%20for%20cognitive%20closure.%20J.%20Pers.%20Soc.%20Psychol.67,%201049%E2%80%931062%20(1994)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Li, M. et al. Fast hybrid dimensionality reduction method for classification based on feature selection and grouped feature extraction. <em>Expert Syst. Appl.</em><strong>150</strong>, 113277 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20M.%20et%20al.%20Fast%20hybrid%20dimensionality%20reduction%20method%20for%20classification%20based%20on%20feature%20selection%20and%20grouped%20feature%20extraction.%20Expert%20Syst.%20Appl.150,%20113277%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Gangal, A., Kumar, P. &amp; Kumari, S. Complete scanning application using OpenCV. arXiv preprint <a href="http://arxiv.org/abs/2107.03700" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2107.03700</a> (2021).</cite>
</li>
<li id="CR33">
<span class="label">33.</span><cite>Culjak, I. et al. A brief introduction to OpenCV. In <em>Proceedings of the International Convention MIPRO</em> 1725–1730 (2012).</cite>
</li>
<li id="CR34">
<span class="label">34.</span><cite>Naveenkumar, M. &amp; Vadivel, A. OpenCV for computer vision applications. In <em>Proceedings of National Conference on Big Data and Cloud Computing (NCBDC’15)</em> 52–56 (2015).</cite>
</li>
<li id="CR35">
<span class="label">35.</span><cite>Selvaraju, R. R. et al. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In <em>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</em>, 618–626 (2017).</cite>
</li>
<li id="CR36">
<span class="label">36.</span><cite>Ding, J., Liu, Z., Bai, Y., Jia, J. &amp; Luo, P. DaViT: Dual attention vision transformers. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 16210–16219 (2022).</cite>
</li>
<li id="CR37">
<span class="label">37.</span><cite>Liu, Z. et al. Swin Transformer: Hierarchical vision transformer using shifted windows. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> 10012–10022 (2021).</cite>
</li>
<li id="CR38">
<span class="label">38.</span><cite>Azizi, S. et al. Big self-supervised models advance medical image classification. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 3478–3488 (2022).</cite>
</li>
<li id="CR39">
<span class="label">39.</span><cite>Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C. &amp; Dosovitskiy, A. Do vision transformers see like convolutional neural networks?. <em>Adv. Neural Inf. Process. Syst.</em><strong>34</strong>, 12116–12128 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Raghu,%20M.,%20Unterthiner,%20T.,%20Kornblith,%20S.,%20Zhang,%20C.%20&amp;%20Dosovitskiy,%20A.%20Do%20vision%20transformers%20see%20like%20convolutional%20neural%20networks?.%20Adv.%20Neural%20Inf.%20Process.%20Syst.34,%2012116%E2%80%9312128%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Sinha, A., Zhang, H. &amp; Li, Y. CNNs still outperform vision transformers on small data with fine-grained classes. arXiv preprint <a href="http://arxiv.org/abs/2205.07676" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2205.07676</a> (2022).</cite>
</li>
<li id="CR41">
<span class="label">41.</span><cite>Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. In <em>Proceedings of the International Conference on Learning Representations</em> (2021).</cite>
</li>
<li id="CR42">
<span class="label">42.</span><cite>Ma, L., Ma, J. &amp; Liu, Y. Comparative analysis of CNN and ViT models for leaf disease classification. <em>Comput. Electron. Agric.</em><strong>202</strong>, 107387 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ma,%20L.,%20Ma,%20J.%20&amp;%20Liu,%20Y.%20Comparative%20analysis%20of%20CNN%20and%20ViT%20models%20for%20leaf%20disease%20classification.%20Comput.%20Electron.%20Agric.202,%20107387%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adsm93_" lang="en" class="supplementary-materials"><h3 class="pmc_sec_title">Supplementary Materials</h3>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12368074/bin/41598_2025_12502_MOESM1_ESM.docx" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material 1</a><sup> (20.5KB, docx) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets used and analysed during the current study available from the corresponding author on reasonable request. LI-WEI CHEN undertook the formal identification of the plant material used in study.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-12502-9"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_12502.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (2.9 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12368074/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12368074/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12368074%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368074/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12368074/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12368074/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40835621/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12368074/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40835621/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12368074/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12368074/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="AWetKtZIJkiWFat8bwqXmWnCOcmZZl5NRKCrVeLzLCokpMD3KHWefkyOblKB8XRY">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
