
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Specialized curricula for training vision language models in retinal image analysis - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4B6FE8AF3CE6305B6FE0009642C48.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="npjdigitmed">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365215/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="NPJ Digital Medicine">
<meta name="citation_title" content="Specialized curricula for training vision language models in retinal image analysis">
<meta name="citation_author" content="Robbie Holland">
<meta name="citation_author_institution" content="Biomedical Image Analysis, Department of Computing, Imperial College London, London, United Kingdom">
<meta name="citation_author" content="Thomas R P Taylor">
<meta name="citation_author_institution" content="Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, United Kingdom">
<meta name="citation_author" content="Christopher Holmes">
<meta name="citation_author_institution" content="Moorfields Eye Hospital NHS Foundation Trust, London, United Kingdom">
<meta name="citation_author" content="Sophie Riedl">
<meta name="citation_author_institution" content="Ophthalmic Image Analysis Group (OPTIMA), Medical University of Vienna, Vienna, Austria">
<meta name="citation_author_institution" content="Department of Ophthalmology and Optometry, Medical University of Vienna, Vienna, Austria">
<meta name="citation_author" content="Julia Mai">
<meta name="citation_author_institution" content="Ophthalmic Image Analysis Group (OPTIMA), Medical University of Vienna, Vienna, Austria">
<meta name="citation_author_institution" content="Department of Ophthalmology and Optometry, Medical University of Vienna, Vienna, Austria">
<meta name="citation_author" content="Maria Patsiamanidi">
<meta name="citation_author_institution" content="Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, United Kingdom">
<meta name="citation_author" content="Dimitra Mitsopoulou">
<meta name="citation_author_institution" content="Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, United Kingdom">
<meta name="citation_author" content="Paul Hager">
<meta name="citation_author_institution" content="Department of Ophthalmology and Optometry, Medical University of Vienna, Vienna, Austria">
<meta name="citation_author" content="Philip Müller">
<meta name="citation_author_institution" content="Department of Ophthalmology and Optometry, Medical University of Vienna, Vienna, Austria">
<meta name="citation_author" content="Johannes C Paetzold">
<meta name="citation_author_institution" content="Biomedical Image Analysis, Department of Computing, Imperial College London, London, United Kingdom">
<meta name="citation_author_institution" content="Chair for AI in Healthcare and Medicine, Technical University of Munich, Munich, Germany">
<meta name="citation_author" content="Hendrik P N Scholl">
<meta name="citation_author_institution" content="Institute of Molecular and Clinical Ophthalmology Basel, Basel, Switzerland">
<meta name="citation_author_institution" content="Department of Ophthalmology, University of Basel, Basel, Switzerland">
<meta name="citation_author_institution" content="Department of Clinical Pharmacology, Medical University of Vienna, Vienna, Austria">
<meta name="citation_author" content="Hrvoje Bogunović">
<meta name="citation_author_institution" content="Institute of Artificial Intelligence, Centre for Medical Data Science, Medical University of Vienna, Vienna, Austria">
<meta name="citation_author" content="Ursula Schmidt-Erfurth">
<meta name="citation_author_institution" content="Ophthalmic Image Analysis Group (OPTIMA), Medical University of Vienna, Vienna, Austria">
<meta name="citation_author" content="Daniel Rueckert">
<meta name="citation_author_institution" content="Biomedical Image Analysis, Department of Computing, Imperial College London, London, United Kingdom">
<meta name="citation_author_institution" content="Chair for AI in Healthcare and Medicine, Technical University of Munich, Munich, Germany">
<meta name="citation_author_institution" content="Munich Center for Machine Learning, Munich, Germany">
<meta name="citation_author" content="Sobha Sivaprasad">
<meta name="citation_author_institution" content="Moorfields Eye Hospital NHS Foundation Trust, London, United Kingdom">
<meta name="citation_author" content="Andrew J Lotery">
<meta name="citation_author_institution" content="Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, United Kingdom">
<meta name="citation_author" content="Martin J Menten">
<meta name="citation_author_institution" content="Biomedical Image Analysis, Department of Computing, Imperial College London, London, United Kingdom">
<meta name="citation_author_institution" content="Chair for AI in Healthcare and Medicine, Technical University of Munich, Munich, Germany">
<meta name="citation_author_institution" content="Munich Center for Machine Learning, Munich, Germany">
<meta name="citation_author" content="On behalf of the PINNACLE consortium">
<meta name="citation_publication_date" content="2025 Aug 19">
<meta name="citation_volume" content="8">
<meta name="citation_firstpage" content="532">
<meta name="citation_doi" content="10.1038/s41746-025-01893-8">
<meta name="citation_pmid" content="40830259">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365215/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365215/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365215/pdf/41746_2025_Article_1893.pdf">
<meta name="description" content="Clinicians spend significant time reviewing medical images and transcribing findings. By integrating visual and textual data, foundation models have the potential to reduce workloads and boost efficiency, yet their practical clinical value remains ...">
<meta name="og:title" content="Specialized curricula for training vision language models in retinal image analysis">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Clinicians spend significant time reviewing medical images and transcribing findings. By integrating visual and textual data, foundation models have the potential to reduce workloads and boost efficiency, yet their practical clinical value remains ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365215/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12365215">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41746-025-01893-8"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41746_2025_Article_1893.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12365215%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12365215/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12365215/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365215/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-npjdigitmed.jpg" alt="NPJ Digital Medicine logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to NPJ Digital Medicine" title="Link to NPJ Digital Medicine" shape="default" href="https://www.nature.com/npjdigitalmed/" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">NPJ Digit Med</button></div>. 2025 Aug 19;8:532. doi: <a href="https://doi.org/10.1038/s41746-025-01893-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41746-025-01893-8</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22NPJ%20Digit%20Med%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22NPJ%20Digit%20Med%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22NPJ%20Digit%20Med%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22NPJ%20Digit%20Med%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Specialized curricula for training vision language models in retinal image analysis</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Holland%20R%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Robbie Holland</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Robbie Holland</span></h3>
<div class="p">
<sup>1</sup>Biomedical Image Analysis, Department of Computing, Imperial College London, London, United Kingdom </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Holland%20R%22%5BAuthor%5D" class="usa-link"><span class="name western">Robbie Holland</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Taylor%20TRP%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Thomas R P Taylor</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Thomas R P Taylor</span></h3>
<div class="p">
<sup>2</sup>Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, United Kingdom </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Taylor%20TRP%22%5BAuthor%5D" class="usa-link"><span class="name western">Thomas R P Taylor</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Holmes%20C%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Christopher Holmes</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Christopher Holmes</span></h3>
<div class="p">
<sup>3</sup>Moorfields Eye Hospital NHS Foundation Trust, London, United Kingdom </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Holmes%20C%22%5BAuthor%5D" class="usa-link"><span class="name western">Christopher Holmes</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Riedl%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Sophie Riedl</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Sophie Riedl</span></h3>
<div class="p">
<sup>4</sup>Ophthalmic Image Analysis Group (OPTIMA), Medical University of Vienna, Vienna, Austria </div>
<div class="p">
<sup>5</sup>Department of Ophthalmology and Optometry, Medical University of Vienna, Vienna, Austria </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Riedl%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Sophie Riedl</span></a>
</div>
</div>
<sup>4,</sup><sup>5</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mai%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Julia Mai</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Julia Mai</span></h3>
<div class="p">
<sup>4</sup>Ophthalmic Image Analysis Group (OPTIMA), Medical University of Vienna, Vienna, Austria </div>
<div class="p">
<sup>5</sup>Department of Ophthalmology and Optometry, Medical University of Vienna, Vienna, Austria </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mai%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Julia Mai</span></a>
</div>
</div>
<sup>4,</sup><sup>5</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Patsiamanidi%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Maria Patsiamanidi</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Maria Patsiamanidi</span></h3>
<div class="p">
<sup>2</sup>Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, United Kingdom </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Patsiamanidi%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Maria Patsiamanidi</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mitsopoulou%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Dimitra Mitsopoulou</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Dimitra Mitsopoulou</span></h3>
<div class="p">
<sup>2</sup>Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, United Kingdom </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mitsopoulou%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">Dimitra Mitsopoulou</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hager%20P%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Paul Hager</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Paul Hager</span></h3>
<div class="p">
<sup>5</sup>Department of Ophthalmology and Optometry, Medical University of Vienna, Vienna, Austria </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hager%20P%22%5BAuthor%5D" class="usa-link"><span class="name western">Paul Hager</span></a>
</div>
</div>
<sup>5</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22M%C3%BCller%20P%22%5BAuthor%5D" class="usa-link" aria-describedby="id9"><span class="name western">Philip Müller</span></a><div hidden="hidden" id="id9">
<h3><span class="name western">Philip Müller</span></h3>
<div class="p">
<sup>5</sup>Department of Ophthalmology and Optometry, Medical University of Vienna, Vienna, Austria </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22M%C3%BCller%20P%22%5BAuthor%5D" class="usa-link"><span class="name western">Philip Müller</span></a>
</div>
</div>
<sup>5</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Paetzold%20JC%22%5BAuthor%5D" class="usa-link" aria-describedby="id10"><span class="name western">Johannes C Paetzold</span></a><div hidden="hidden" id="id10">
<h3><span class="name western">Johannes C Paetzold</span></h3>
<div class="p">
<sup>1</sup>Biomedical Image Analysis, Department of Computing, Imperial College London, London, United Kingdom </div>
<div class="p">
<sup>6</sup>Chair for AI in Healthcare and Medicine, Technical University of Munich, Munich, Germany </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Paetzold%20JC%22%5BAuthor%5D" class="usa-link"><span class="name western">Johannes C Paetzold</span></a>
</div>
</div>
<sup>1,</sup><sup>6</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Scholl%20HPN%22%5BAuthor%5D" class="usa-link" aria-describedby="id11"><span class="name western">Hendrik P N Scholl</span></a><div hidden="hidden" id="id11">
<h3><span class="name western">Hendrik P N Scholl</span></h3>
<div class="p">
<sup>7</sup>Institute of Molecular and Clinical Ophthalmology Basel, Basel, Switzerland </div>
<div class="p">
<sup>8</sup>Department of Ophthalmology, University of Basel, Basel, Switzerland </div>
<div class="p">
<sup>9</sup>Department of Clinical Pharmacology, Medical University of Vienna, Vienna, Austria </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Scholl%20HPN%22%5BAuthor%5D" class="usa-link"><span class="name western">Hendrik P N Scholl</span></a>
</div>
</div>
<sup>7,</sup><sup>8,</sup><sup>9</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Bogunovi%C4%87%20H%22%5BAuthor%5D" class="usa-link" aria-describedby="id12"><span class="name western">Hrvoje Bogunović</span></a><div hidden="hidden" id="id12">
<h3><span class="name western">Hrvoje Bogunović</span></h3>
<div class="p">
<sup>10</sup>Institute of Artificial Intelligence, Centre for Medical Data Science, Medical University of Vienna, Vienna, Austria </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Bogunovi%C4%87%20H%22%5BAuthor%5D" class="usa-link"><span class="name western">Hrvoje Bogunović</span></a>
</div>
</div>
<sup>10</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Schmidt-Erfurth%20U%22%5BAuthor%5D" class="usa-link" aria-describedby="id13"><span class="name western">Ursula Schmidt-Erfurth</span></a><div hidden="hidden" id="id13">
<h3><span class="name western">Ursula Schmidt-Erfurth</span></h3>
<div class="p">
<sup>4</sup>Ophthalmic Image Analysis Group (OPTIMA), Medical University of Vienna, Vienna, Austria </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Schmidt-Erfurth%20U%22%5BAuthor%5D" class="usa-link"><span class="name western">Ursula Schmidt-Erfurth</span></a>
</div>
</div>
<sup>4</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Rueckert%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id14"><span class="name western">Daniel Rueckert</span></a><div hidden="hidden" id="id14">
<h3><span class="name western">Daniel Rueckert</span></h3>
<div class="p">
<sup>1</sup>Biomedical Image Analysis, Department of Computing, Imperial College London, London, United Kingdom </div>
<div class="p">
<sup>6</sup>Chair for AI in Healthcare and Medicine, Technical University of Munich, Munich, Germany </div>
<div class="p">
<sup>11</sup>Munich Center for Machine Learning, Munich, Germany </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Rueckert%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">Daniel Rueckert</span></a>
</div>
</div>
<sup>1,</sup><sup>6,</sup><sup>11,</sup><sup>#</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sivaprasad%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id15"><span class="name western">Sobha Sivaprasad</span></a><div hidden="hidden" id="id15">
<h3><span class="name western">Sobha Sivaprasad</span></h3>
<div class="p">
<sup>3</sup>Moorfields Eye Hospital NHS Foundation Trust, London, United Kingdom </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sivaprasad%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Sobha Sivaprasad</span></a>
</div>
</div>
<sup>3,</sup><sup>#</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Lotery%20AJ%22%5BAuthor%5D" class="usa-link" aria-describedby="id16"><span class="name western">Andrew J Lotery</span></a><div hidden="hidden" id="id16">
<h3><span class="name western">Andrew J Lotery</span></h3>
<div class="p">
<sup>2</sup>Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, United Kingdom </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Lotery%20AJ%22%5BAuthor%5D" class="usa-link"><span class="name western">Andrew J Lotery</span></a>
</div>
</div>
<sup>2,</sup><sup>#</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Menten%20MJ%22%5BAuthor%5D" class="usa-link" aria-describedby="id17"><span class="name western">Martin J Menten</span></a><div hidden="hidden" id="id17">
<h3><span class="name western">Martin J Menten</span></h3>
<div class="p">
<sup>1</sup>Biomedical Image Analysis, Department of Computing, Imperial College London, London, United Kingdom </div>
<div class="p">
<sup>6</sup>Chair for AI in Healthcare and Medicine, Technical University of Munich, Munich, Germany </div>
<div class="p">
<sup>11</sup>Munich Center for Machine Learning, Munich, Germany </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Menten%20MJ%22%5BAuthor%5D" class="usa-link"><span class="name western">Martin J Menten</span></a>
</div>
</div>
<sup>1,</sup><sup>6,</sup><sup>11,</sup><sup>#</sup>; <span class="collab">On behalf of the PINNACLE consortium</span>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Biomedical Image Analysis, Department of Computing, Imperial College London, London, United Kingdom </div>
<div id="Aff2">
<sup>2</sup>Clinical and Experimental Sciences, Faculty of Medicine, University of Southampton, Southampton, United Kingdom </div>
<div id="Aff3">
<sup>3</sup>Moorfields Eye Hospital NHS Foundation Trust, London, United Kingdom </div>
<div id="Aff4">
<sup>4</sup>Ophthalmic Image Analysis Group (OPTIMA), Medical University of Vienna, Vienna, Austria </div>
<div id="Aff5">
<sup>5</sup>Department of Ophthalmology and Optometry, Medical University of Vienna, Vienna, Austria </div>
<div id="Aff6">
<sup>6</sup>Chair for AI in Healthcare and Medicine, Technical University of Munich, Munich, Germany </div>
<div id="Aff7">
<sup>7</sup>Institute of Molecular and Clinical Ophthalmology Basel, Basel, Switzerland </div>
<div id="Aff8">
<sup>8</sup>Department of Ophthalmology, University of Basel, Basel, Switzerland </div>
<div id="Aff9">
<sup>9</sup>Department of Clinical Pharmacology, Medical University of Vienna, Vienna, Austria </div>
<div id="Aff10">
<sup>10</sup>Institute of Artificial Intelligence, Centre for Medical Data Science, Medical University of Vienna, Vienna, Austria </div>
<div id="Aff11">
<sup>11</sup>Munich Center for Machine Learning, Munich, Germany </div>
<div id="Aff12">
<sup>12</sup>Department of Biostatistics, University of Michigan, Ann Arbor, MI USA </div>
<div id="Aff13">
<sup>13</sup>Department of Ophthalmology, University of Bonn, Bonn, Germany </div>
<div class="author-notes p">
<div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div>
<div class="fn" id="_eqcntrb93pmc__">
<sup>#</sup><p class="display-inline">Contributed equally.</p>
</div>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Nov 20; Accepted 2025 Jul 15; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12365215  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40830259/" class="usa-link">40830259</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Clinicians spend significant time reviewing medical images and transcribing findings. By integrating visual and textual data, foundation models have the potential to reduce workloads and boost efficiency, yet their practical clinical value remains uncertain. In this study, we find that OpenAI’s ChatGPT-4o and two medical vision-language models (VLMs) significantly underperform ophthalmologists in key tasks for age-related macular degeneration (AMD). To address this, we developed a dedicated training curriculum, designed by domain specialists, to optimize VLMs for tasks related to clinical decision making. The resulting model, RetinaVLM-Specialist, significantly outperforms foundation medical VLMs and ChatGPT-4o in AMD disease staging (F1: 0.63 vs. 0.33) and referral (0.67 vs. 0.50), achieving performance comparable to junior ophthalmologists. In a reader study, two senior ophthalmologists confirmed that RetinaVLM’s reports were substantially more accurate than those written by ChatGPT-4o (64.3% vs. 14.3%). Overall, our curriculum-based approach offers a blueprint for adapting foundation models to real-world medical applications.</p>
<section id="kwd-group1" class="kwd-group"><p><strong>Subject terms:</strong> Health care, Medical imaging</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Medical images are central to many clinical decisions regarding patient diagnosis, referral, and treatment. Clinicians spend a significant amount of time transcribing image-based decisions into text in order to store and communicate their findings<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a>,<a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>. Visual-language models (VLM), which automatically interpret medical images and generate detailed textual descriptions, have enormous potential to alleviate clinical workloads and increase patient access to high-quality medical care<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a>,<a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>. To date, the majority of medical VLMs have been trained to output a finite set of pre-determined textual responses<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a>–<a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>. Only recently, the combination of large language models (LLM) with medical vision encoders has led to the development of more powerful and versatile <em>generative</em> VLMs that are able to write comprehensive text reports or answer complex questions<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a>–<a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>.</p>
<p id="Par3">This current generation of medical language models is fueled by vast amounts of unstructured training data that is extracted from medical textbooks, scientific publications or social media posts of healthcare professionals<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a>,<a href="#CR9" class="usa-link" aria-describedby="CR9">9</a>,<a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>. These <em>foundation</em> language models have stirred considerable interest among the medical community for their expert-level performance on standardized medical question-answering tasks, such as licensing exams and case studies<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a>,<a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>. However, it is unclear whether this general performance translates to clinical utility in specialist medical domains<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. Despite its impressive scale, the training data of foundation language models has been collected agnostically of their downstream application. The resulting model is unlikely to acquire the nuanced knowledge necessary for effective application in specialized clinical contexts.</p>
<p id="Par4">In this study, we identify this missing piece in foundation models towards developing generative medical VLMs with real-world clinical utility. We propose to deconstruct clinical problems into sets of mandatory capabilities required for their resolution and selectively train VLMs in these skills. To train VLMs in these skills, we develop a curriculum-based approach<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup> that draws from recent advances in instruction finetuning<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a>–<a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup> which iteratively refine models on datasets of increasing quality. We demonstrate the feasibility of this approach in ophthalmology, focusing our analysis on a single retinal disease in order to assess the clinical limitations of foundation VLMs, and the benefits of training on specialized curricula, in depth. To this end we introduce, RetinaVLM, is a generative medical VLM for OCT images (see Fig. <a href="#Fig1" class="usa-link">1</a>a). RetinaVLM is trained using a two-part dedicated curriculum that is specific to the clinical management of age-related macular degeneration (AMD), the leading cause of blindness in the elderly<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a>,<a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>. The resulting model is able to process optical coherence tomography (OCT) images of the retina and flexibly respond to instructions and questions (see Fig. <a href="#Fig1" class="usa-link">1</a>b). In particular, we evaluate RetinaVLM’s utility and versatility regarding disease staging, patient referral and biomarker analysis in AMD (see Fig. <a href="#Fig1" class="usa-link">1</a>c).</p>
<figure class="fig xbox font-sm" id="Fig1"><h3 class="obj_head">Fig. 1. A curriculum-based approach for training vision-language models in medical specialties.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365215_41746_2025_1893_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/257d/12365215/869dfd8b6c17/41746_2025_1893_Fig1_HTML.jpg" loading="lazy" id="d33e533" height="1018" width="736" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>We introduce RetinaVLM, a specialist medical generative vision-language model (VLM). <strong>a</strong> Using a curriculum-based approach, we trained RetinaVLM in specialist medical skills that medical foundation VLMs are currently lacking. <strong>b</strong> RetinaVLM is able to process retinal optical retinal optical coherence tomography (OCT) images and flexibly respond to text-based queries. <strong>c</strong> Its abilities entail the analysis of imaging biomarkers of age-related macular degeneration (AMD), disease staging, and the referral for treatment.</p></figcaption></figure></section><section id="Sec2"><h2 class="pmc_sec_title">Results</h2>
<section id="Sec3"><h3 class="pmc_sec_title">RetinaVLM, a specialist vision-language model for retinal image analysis</h3>
<p id="Par5">RetinaVLM combines two main components: an ophthalmic vision encoder that processes input OCT images, and a generative LLM that handles textual instructions and outputs the corresponding responses (see Fig. <a href="#Fig1" class="usa-link">1</a>a). The vision encoder was trained using self-supervised learning on images from the train set, and was found to perform on par with RETFound<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>, a large foundation model for retinal image analysis<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>. For the language model, we use Meta’s Llama 3 as generative LLM which was the most performant, openly available model at the time of this study<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. Both these deep neural networks have already been pre-trained on large OCT and natural language datasets, respectively. We combine these to create RetinaVLM, following the architectural design of MiniGPT-4 introduced by Zhu et al.<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>. This approach leaves the pretrained models unchanged, and instead trains a third, intermediary network to map visual information from the image encoder to the language model. For additional information regarding the model architecture, training and inference see the “RetinaVLM vision-language model architecture” section.</p></section><section id="Sec4"><h3 class="pmc_sec_title">A curriculum to encode the capabilities of retinal specialists into vision-language models</h3>
<p id="Par6">An intuitive strategy to specialize VLMs while preserving their ability to flexibly interact with text queries is to provide them with a set of medical images and corresponding <em>visual question-answer</em> (VQA) pairs. VQA-based training, a form of instruction fine-tuning<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a>,<a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>, guides VLMs to extract visual information and synthesize it into textual outputs that adhere to specific user directives. This approach also introduces diversity through varying instructions and responses, mitigating overfitting while enhancing the model’s capacity to selectively report relevant clinical features and contextualize findings. However, relevant VQA datasets are scarce for most medical specializations, including ophthalmology.</p>
<p id="Par7">Together with a large team of ophthalmologists, which are involved with the patient care and academic research of AMD, we created a curriculum of VQA datasets designed to train VLMs for assisting image-based clinical decisions regarding AMD. To this end, the ophthalmologists first defined a set of guidelines outlining essential capabilities of agents assisting the image-based clinical management of AMD (verbose versions are documented in Supplementary Fig. <a href="#MOESM1" class="usa-link">10</a>). Specifically, these include details relating to the identification of AMD biomarkers in OCT images, the linking of these to the AMD disease stage, and ultimately deciding on the required referral and treatment of the patient. These subsequently guided a combination of manual and automated efforts to curate a training curriculum, which consists of 41,926 OCT images, and 479,710 VQA pairs to progressively specialize VLMs in these capabilities.</p>
<p id="Par8">The first part of the curriculum, named <em>Introduction to retina</em>, primarily covers the appearance of the retina and AMD biomarkers in OCT images. Using automated data collection, we obtained tabular reports for 41,926 retrospectively collected OCT images of AMD patients (see Fig. <a href="#Fig2" class="usa-link">2</a>a). Each report describes the visible biomarkers, patient’s diagnosis, visual acuity and demographic information in 34 data fields. A full description of the retrospective OCT dataset and tabular field collection process can be found in the “Retrospective patient dataset” section. Four example tabular reports are shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">1</a>.</p>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2. Creating medical visual question-answers for a two-part training curriculum.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365215_41746_2025_1893_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/257d/12365215/c281df350e15/41746_2025_1893_Fig2_HTML.jpg" loading="lazy" id="d33e622" height="853" width="683" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>We curated a two-part curriculum to specialize medical VLMs for clinical use. <strong>a</strong>, <strong>b</strong> Based on a retrospectively collected OCT imaging dataset, we created a large number of tabular reports as well as a small number of comprehensive textual reports. <strong>c</strong>, <strong>d</strong> We then used an independent LLM to automatically generate visual question-answers based on these reports. <strong>e</strong>, <strong>f</strong> This yielded two VQA datasets, the first on basic imaging biomarkers of AMD and the second covering more advanced clinical skills. <strong>g</strong>, <strong>h</strong> Finally, we trained two specialist medical generative VLMs, RetinaVLM-Base and RetinaVLM-Specialist, using either the first or both VQA datasets.</p></figcaption></figure><p id="Par9">Next, we tasked an independent LLM to generate question-answer pairs based on these reports (see Fig. <a href="#Fig2" class="usa-link">2</a>c). The model processed the content of the tabular reports – but not the OCT images – to output a numbered list of question-answer pairs. We generated an average of ten question-answer pairs per report that are mostly related to the presence or absence of specific biomarkers (see Fig. <a href="#Fig2" class="usa-link">2</a>e). The LLM was instructed to create both closed-ended ’yes or no’ style questions, and simple open-ended questions. Detailed information on the LLM setup can be found in the “Report curation and question-answer pair generation (curriculum part 1)” section.</p>
<p id="Par10">This automated approach allowed us to generate a large dataset of 408,545 question-answer pairs. However, the questions were limited in scope to the set of biomarkers documented by the tabular reports. Training on these yielded the first of two specialist VLMs, <em>RetinaVLM-Base</em> (see Fig. <a href="#Fig2" class="usa-link">2</a>g).</p>
<p id="Par11">The second part of the curriculum, named <em>Advanced retinal specialism</em>, builds on top of the first part to link imaging biomarkers to AMD stage and the recommended course of treatment. As this reasoning cannot be fully conveyed via tabular information, we tasked two ophthalmologists with 3 and 10 years of experience, respectively, to create comprehensive textual reports for a subset of 330 OCT images (see Fig. <a href="#Fig2" class="usa-link">2</a>b). The ophthalmologists were asked to primarily describe the main pathological biomarkers related to AMD while also noting any other observations regarding the retinal anatomy. This task yielded high-quality reports that go beyond the short notes that are typically written by ophthalmologists in their clinical routine. Instructions given to the ophthalmologists are provided in the “Report curation and question-answer pair generation (curriculum part 2)” section, and six sample reports yielded by this process are shown Supplementary Fig. <a href="#MOESM1" class="usa-link">2</a>.</p>
<p id="Par12">Similar to before, an independent LLM was then employed to automatically generate question-answer pairs based on the reports (see Fig. <a href="#Fig2" class="usa-link">2</a>d). Due to the substantially increased depth and scope of the full-text reports compared to the tabular ones, we used several advanced LLM instructions to create 216 diverse question-answer pairs per image on average (see Fig. <a href="#Fig2" class="usa-link">2</a>f). These cover additional biomarkers and sub-categorize them based on their size, type, and location. Other question-answer pairs are related to the causal relationship between biomarkers and six AMD disease stages as well as three levels of patient referral urgency. Moreover, the question-answer pairs were more varied in their structure in order to preserve interactive capabilities of the foundation LLM. For example, some queries asked to summarize the existing reports or provide several answers in succession. An example interaction with the LLM to generate question-answer pairs with the LLM is shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">3</a>. Furthermore, a list of all the LLM instructions is provided in “Question-answer generation prompts” in the <a href="#MOESM1" class="usa-link">Supplementary material</a> and example question-answers yielded by this approach are shown in the ‘part 2’ section of Supplementary Fig. <a href="#MOESM1" class="usa-link">4</a>.</p>
<p id="Par13">This resulted in a dataset of 71,165 advanced question-answer pairs. By further training RetinaVLM-Base on the second part of the curriculum, we obtained our most performant VLM for the clinical management of AMD, <em>RetinaVLM-Specialist</em> (see Fig. <a href="#Fig2" class="usa-link">2</a>h).</p></section><section id="Sec5"><h3 class="pmc_sec_title">RetinaVLM-Specialist outperforms foundation models and approaches junior ophthalmologists in AMD disease staging and report writing</h3>
<p id="Par14">AMD is a debilitating and irreversible condition marked by the progressive loss of central vision, severely hindering essential activities like reading, driving, and recognizing faces. The demand for timely diagnosis and management currently exceeds the available ophthalmology expertise<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>. Moreover, largely due to aging populations, projections indicate a nearly 50% increase in AMD cases globally to nearly 300 million by 2040<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>. In this context, automated retinal image analysis emerges as an essential tool to support the interpretation and textual reporting of retinal images.</p>
<p id="Par15">A key aspect of image report generation involves estimating the disease stage indicated by the retinal image. We assessed the ability of five different generative VLMs to determine the AMD disease stage when writing reports on retinal OCT images. Specifically, we benchmarked two foundation VLMs, Med-Flamingo<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup> and LLaVA-Med<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>, with purported general abilities in medical image analysis. In addition, we assessed OpenAI’s ChatGPT-4o model<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>, developed by OpenAI, L.P. (San Francisco, California, USA). We compared these baseline models against RetinaVLM-Base and RetinaVLM-Specialist, which result from cumulative training on curriculum part 1 and part 2, respectively. Finally we compared these automated models against the overall performance of the six junior ophthalmologists. This experiment was conducted using a testing dataset of 276 previously unseen OCT images, on which VLMs were tasked to write descriptive reports before classifying the patient into one of six disease stages (see Fig. <a href="#Fig3" class="usa-link">3</a>a). The model predictions were compared to ground truth labels obtained from ophthalmologists. Each image was initially graded by two out of six junior ophthalmologists, who have 2, 3, 5, 8, 10 and 15 years of experience working full-time in ophthalmology clinics after receiving their medical degree. Inter-rater disagreements were resolved by a panel of two senior ophthalmologists with 25 and 32 years of experience (see “Definitions and roles of the junior and senior ophthalmologists” in the <a href="#MOESM1" class="usa-link">Supplementary material</a>). Further details regarding the derivation of testing labels are provided in the “Retrospective patient dataset” section, and the instruction given to all VLMs in the “Report generation for disease staging” section.</p>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3. Comparison of vision-language models with ophthalmologists in reporting AMD disease stage.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365215_41746_2025_1893_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/257d/12365215/76b3bb820daf/41746_2025_1893_Fig3_HTML.jpg" loading="lazy" id="d33e727" height="846" width="682" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> Comparison of the ability of four VLMs to write reports on retinal OCT images and derive the AMD stage. <strong>b</strong> Overall staging accuracy for each model was calculated using micro F1 scores with 95% CI, with tests of statistical significance calculated using McNemar’s test. <strong>c</strong> Confusion matrices between the senior ophthalmologists' assessments (rows) against the image-based clinical decision maker’s prediction (columns). <strong>d</strong> Qualitative comparison of reports written by human ophthalmologists and RetinaVLM-Specialist with text markings highlighting findings regarding biomarker observations and disease stage.</p></figcaption></figure><p id="Par16">We found that both the intermediate RetinaVLM-Base model and ChatGPT-4o perform significantly better than Med-Flamingo and LLaVA-Med, which lack the ophthalmological specialism to stage disease (see Fig. <a href="#Fig3" class="usa-link">3</a>b). By more effectively classifying conversion to late stages of AMD RetinaVLM-Base and ChatGPT-4o achieve F1 scores of 0.33 and 0.29, respectively. However, beyond this distinction, these models failed to differentiate finer disease stage variations and were markedly outperformed by RetinaVLM-Specialist which scored 0.63 F1. This performance approached, but did not match, the accuracy of the junior ophthalmologists who achieved an F1 score of 0.78. We analyze this last discrepancy in further detail in the Discussion. Moreover, foundation VLMs and RetinaVLM-Base returned a substantial number of invalid reports that did not conclude with one of the six disease stages (see Fig. <a href="#Fig3" class="usa-link">3</a>c). Conversely, all generated reports by RetinaVLM-Specialist were valid. Similar to human experts, RetinaVLM-Specialist struggled the most when diagnosing wet inactive AMD. We attribute this to the higher number of imaging biomarkers shared by intermediate, inactive late wet, and active late wet AMD, which sometimes led to misinterpretation by both ophthalmologists and RetinaVLM-Specialist (see Fig. <a href="#Fig3" class="usa-link">3c, d</a>). Four representative examples of success and failure cases of RetinaVLM-Specialist are shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">5a</a>. Moreover, full numerical results, including a comparison to a standard image-only classification model, as well as the confusion matrix for medical foundation models, are shown in Supplementary Figs. <a href="#MOESM1" class="usa-link">6a</a> and <a href="#MOESM1" class="usa-link">7</a>, respectively.</p>
<p id="Par17">Next, we performed a qualitative evaluation of imaging reports by senior ophthalmologists. Eight-four of the generated reports were scored by the two senior ophthalmologists for their correctness, completeness, and conciseness. They were shown 28 reports written by ChatGPT-4o, 28 by RetinaVLM-Specialist, and 28 by the two annotating junior ophthalmologists. We then randomly ordered the 84 reports to decrease the likelihood that multiple reports regarding the same image would appear in succession. Moreover, to mitigate bias we conducted this evaluation as a single-blind study in which the authorship of each report was concealed from the senior ophthalmologists. For each report, the senior ophthalmologist first reviewed the corresponding OCT image before rating the generated report in the three criteria on a five point Likert scale<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>. For details on the report generation instruction and evaluation criteria see the “Report generation for qualitative evaluation” section</p>
<p id="Par18">The senior ophthalmologists observed that ChatGPT-4o largely failed to compose factually correct image reports (see Fig. <a href="#Fig4" class="usa-link">4</a>a), even though it uses specialist terminology that may give the reports the initial appearance of being written by an ophthalmologist (see Fig. <a href="#Fig4" class="usa-link">4</a>b). ChatGPT-4o was found to consistently hallucinate the presence of subretinal fluid (further instances of hallucinations are shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">13</a>). Moreover, in every relevant instance ChatGPT-4o failed to detect presence of subretinal hyperreflective material and hypertransmission, which are crucial for diagnosing inactive late wet and late dry AMD, respectively. Further examples of errors made by ChatGPT-4o can be found in in Supplementary Fig. <a href="#MOESM1" class="usa-link">14</a>, including verbose versions of the three reports displayed in Fig. <a href="#Fig4" class="usa-link">4</a>b. Overall, the senior ophthalmologists found that only 4 out of 28 (14.3%) of the reports written by ChatGPT-4o were correct in their observations and conclusions.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4. Qualitative evaluations of image reports written by vision-language models and ophthalmologists.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365215_41746_2025_1893_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/257d/12365215/2df833310d52/41746_2025_1893_Fig4_HTML.jpg" loading="lazy" id="d33e789" height="847" width="678" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> Summary statistics of the quality of image reports written by ChatGPT-4o, RetinaVLM-Specialist and junior ophthalmologists, broken down by correctness, completeness and conciseness. Reports were scored for on each of the three criteria by senior ophthalmologists using a five-point Likert scale. <strong>b</strong> Representative reports with ratings by one of the senior ophthalmologists. As ChatGPT-4o tended to write excessively long reports, despite being prompted to shorten them, we display passages the senior ophthalmologists selected as the most important to their given rating. For verbose versions and additional sample reports by ChatGPT-4o see Supplementary Fig. <a href="#MOESM1" class="usa-link">14</a>.</p></figcaption></figure><p id="Par19">Overall senior ophthalmologists either agreed or strongly agreed that reports generated by RetinaVLM-Specialist were more correct (18 vs. 4, or 64.3% vs. 14.3%), complete (18 vs. 5) and concise (18 vs. 4) than those written by ChatGPT-4o. Compared to ChatGPT-4o, RetinaVLM-Specialist correctly detected a wider range of biomarkers, which more frequently led to a correct disease stage estimation. However, there remains a gap in overall performance between RetinaVLM-Specialist and the junior ophthalmologists, with the senior ophthalmologists rating their reports to be more correct (25 vs. 18), complete (24 vs. 18) and concise (22 vs. 18).</p>
<p id="Par20">An example of this discrepancy can be seen in the second sample in Fig. <a href="#Fig4" class="usa-link">4</a>b, where RetinaVLM-Specialist correctly identified that the image showed a healthy retina, but also detects a small cyst that was not found in the image (for additional examples of hallucinations see Supplementary Fig. <a href="#MOESM1" class="usa-link">13</a>). Junior ophthalmologists wrote a concise report, but incorrectly associated subretinal drusenoid deposits with intermediate AMD. Moreover, both RetinaVLM, which has currently only been trained to identify AMD from retinal imaging biomarkers, and ChatGPT-4o fail to report likely cases of retinal vascular disease. These characteristics of both the junior ophthalmologists and RetinaVLM-Specialist are discussed in more detail in the Discussion.</p></section><section id="Sec6"><h3 class="pmc_sec_title">RetinaVLM-Specialist surpasses opticians and approaches junior ophthalmologists in AMD patient screening and referral</h3>
<p id="Par21">As the prevalence of AMD is expected to further increase in the upcoming decades<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>, ocular screening programs are being introduced around the world. In the United Kingdom, some projects involve opticians and pharmacies that acquire and interpret OCT images. They may refer a patient to a specialist clinic, summarizing their findings and the estimated level of the patient’s risk in a letter. In the United Kingdom, treatment guidelines for AMD mandate that patients with signs of neovascularization are referred for immediate treatment within two weeks. However, non-specialists exhibit a tendency to over-diagnose these cases. An internal audit at Southampton Eye Unit found that 74.2% of the referrals made to the clinic do not have any form of treatable AMD. The processing and assessment of these false positives affects the clinic’s ability to care for the remaining patients with treatable forms of AMD.</p>
<p id="Par22">We evaluated the ability of VLMs to assess the level of referral urgency from OCT image (see Fig. <a href="#Fig5" class="usa-link">5</a>a). For each case, the VLMs were provided explicit referral guidelines, and asked to recommend which of three levels of referral urgency was most appropriate for the patient: <em>no referral</em> for healthy patients, <em>to be seen within 18 weeks (routine referral)</em> for patients that are at risk of progressing to active late wet AMD but do not require treatment yet, and <em>referral within two weeks</em> for patients with any signs of neovascularization that should be urgently referred for antiangiogenic treatment. Two junior ophthalmologists independently reviewed images of 95 patients that have previously been referred to the hospital for treatment of wet AMD. For each patient, they independently decided the most appropriate of the three levels of referral urgency, and disagreements were arbitrated by the two senior ophthalmologists. In line with previous audits, they found the false discovery rate for urgent referrals was 69.5%. We then calculated F1 scores for the highest risk patients in need of urgent referral between the VLM’s predictions and the ground truth. The full referral protocol and report generation instruction are provided in the “Report generation for patient referral” section.</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5. Comparison of vision-language models with ophthalmologists in writing image-based referral reports.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365215_41746_2025_1893_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/257d/12365215/511084a3074e/41746_2025_1893_Fig5_HTML.jpg" loading="lazy" id="d33e852" height="1020" width="749" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> Evaluation of the ability of four VLMs to assess the need for patient referral for treatment of wet AMD. <strong>b</strong> Overall referral accuracy was calculated using F1 score for urgent referral with a 95% CI. Tests of statistical significance were carried out using McNemar’s test. <strong>c</strong> Confusion matrices between the senior ophthalmologists assessment (rows) against the image-based clinical decision maker’s referral assessment (columns). <strong>d</strong> Image reports written by the non-specialist optician who originally referred the patient, compared with reports of the same patient written by RetinaVLM-Specialist.</p></figcaption></figure><p id="Par23">We found that both medical foundation VLMs and Retina-Base perform worse than opticians regarding their ability to refer patients in need of urgent treatment (see Fig. <a href="#Fig5" class="usa-link">5</a>b). While Med-Flamingo failed to refer any of the 29 high-risk patients cases, LLaVA-Med and RetinaVLM-Base were ineffective for differentiating high-risk patients from low- to moderate-risk patients (see Fig. <a href="#Fig5" class="usa-link">5</a>c). In comparison, ChatGPT-4o was relatively more effective for detecting urgent referrals, but still recommended the referral of 10 patients with low risk, and rarely classified a patient with moderate risk. RetinaVLM-Specialist was the best peforming VLM, and was able to detect 23 out of the 29 high-risk cases that require immediate treatment. At the same time, RetinaVLM’s false discovery rate, defined as the ratio of the number of false positives over the number of predicted positives, of 42.5% is substantially lower than that of opticians at 69.5%. Owing to their ability to better differentiate moderate from high-risk cases, the human ophthalmologists had the lowest false discovery rate of 9.1%, although they simultaneously missed three more cases in urgent need for treatment. A full table of F1 scores for this task, including a comparison to a standard image-only classification model, are shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">6a</a>.</p>
<p id="Par24">In practice, referral letters should communicate the reason for referral by citing suspected abnormalities in the OCT image that can inform the ophthalmologist’s initial diagnostic plan. As in the conciseness study in Fig. <a href="#Fig4" class="usa-link">4</a>, RetinaVLM-Specialist sometimes documents the presence of small biomarkers that cannot be found in the image. More often, RetinaVLM-Specialist wrote an accurate imaging report but did not accurately follow the complex set of referral guidelines provided in the instruction. This led RetinaVLM-Specialist to incorrectly recommend that 17 of the moderate-risk patients potentially require treatment. However, this occurred less for the 25 low-risk patients, where RetinaVLM-Specialist correctly identified patients with little or no abnormalities, which are often referred to the treatment clinic for a second opinion by non-specialists (samples 1 and 3 in Fig. <a href="#Fig5" class="usa-link">5</a>d). Crucially, we find that RetinaVLM-Specialist is effective in the detection of intraretinal cysts and fluid that differentiate high-risk from moderate-risk patients (samples 4 and 5). Four representative examples of success and failure cases of RetinaVLM-Specialist are shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">5b</a>.</p></section><section id="Sec7"><h3 class="pmc_sec_title">RetinaVLM accurately detects imaging biomarkers to make recommendations</h3>
<p id="Par25">It is important that clinical decision makers can provide evidence for their recommendations. Disease staging reports and written referral recommendations commonly contain descriptions of the most salient biomarkers that were detected in the scan. We tested the ability of four VLMs to correctly identify the presence or absence of 10 different biomarkers related to AMD. To this end, all VLMs were tasked with writing reports for 396 OCT images that conclude by stating the presence or absence of the biomarker in question (see Fig. <a href="#Fig6" class="usa-link">6</a>a). The VLMs predictions were compared against the ground truth labels obtained from junior ophthalmologists. The instruction used to generate these biomarker focused reports is provided in the “Report generation for biomarker analysis” section.</p>
<figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6. Comparison of vision-language models in analyzing imaging biomarkers.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365215_41746_2025_1893_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/257d/12365215/46b154532130/41746_2025_1893_Fig6_HTML.jpg" loading="lazy" id="d33e893" height="751" width="682" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> We test four VLMs on their ability to describe the presence of 10 important imaging biomarkers of AMD. <strong>b</strong> Overall detection accuracy was computed using F1 scores. <strong>c</strong> Detection sensitivity for each level of biomarker severity for the most important biomarkers. <strong>d</strong> Regions highlighted by RetinaVLM-Specialist that correspond with different biomarkers and disease stage assessments in its written report. Regions with greater than 25% and 50% importance are highlighted by yellow and red contours, respectively. Four additional examples are shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">8</a>.</p></figcaption></figure><p id="Par26">We find that RetinaVLM-Specialist outperforms both LLaVA-Med and Med-Flamingo in the detection of seven out of the ten of main biomarkers related to AMD (see Fig. <a href="#Fig6" class="usa-link">6</a>b). In overall performance, RetinaVLM-Specialist performed similarly well as ChatGPT-4o. In general, biomarkers that were more severe, larger, and more numerous were detected with higher accuracy by RetinaVLM-Specialist than less advanced presentations (see Fig. <a href="#Fig6" class="usa-link">6</a>c). Most of the smaller biomarkers, such as small amounts of intraretinal fluid, drusen and hyperreflective foci, which can be as small as 30 μm in size<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>, were detected with lower sensitivity. Overall, clinically important hallmarks of late AMD were detected with a very high sensitivity. Large volumes of subretinal and intraretinal fluid were detected in 80% and 78% of cases, respectively, and severe levels of hypertransmission in 84% of cases. We believe that stronger image encoders capable of detecting smaller features are necessary to improve the performance of RetinaVLM-Specialist on the detection of smaller biomarkers.</p>
<p id="Par27">Finally, we compute saliency maps which highlight regions of the image that were most important to the model in writing specific passages of the report (see Fig. <a href="#Fig6" class="usa-link">6</a>d). Qualitatively, we find that RetinaVLM-Specialist attends to relevant regions of the image containing fluid, hypertransmission and retinal pigment epithelium (RPE) irregularities in order to compose passages related to biomarkers and disease stage. Rather than performing biomarker segmentation, for which standard deep segmentation models by Isensee et al.<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> would be a more appropriate tool, these maps provide some explanation as to the choice of words and report passages made by the model. We calculate these using Grad-CAM<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup> as described in the “Computing language-based image saliency maps” section, and provide four additional examples in Supplementary Fig. <a href="#MOESM1" class="usa-link">8</a>.</p></section></section><section id="Sec8"><h2 class="pmc_sec_title">Discussion</h2>
<p id="Par28">In this study we have presented a curriculum-based approach for the specialization of medical vision-language models that directly leverages the knowledge of domain experts. Given a retinal OCT image, the resulting RetinaVLM-Specialist model generates accurate, detailed textual responses related to disease staging, referral or biomarker identification of AMD. While large foundation deep learning models have been employed for retinal image analysis before<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a>,<a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup>, our generative VLM is the first model that can flexibly process varied textual queries related to complex ophthalmological decisions and return detailed written responses. Through the use of language as primary communication medium, artificial intelligence systems are able to dynamically perform new tasks and meet the evolving requirements of image-based clinical decision makers.</p>
<p id="Par29">In extensive experiments, RetinaVLM-Specialist significantly outperformed ChatGPT-4o and two generative VLMs, Med-Flamingo and LLaVA-Med, designed for medical use. RetinaVLM-Base and ChatGPT-4o were more capable in classifying conversion to late stages of AMD than existing medical foundation models, Flamingo and LLaVA-Med. However, they also lacked the ability to identify subtle yet important differences between disease stages and patient risk. Specifically, we have shown that in disease staging, RetinaVLM-Specialist outperforms all baselines and is approaching the accuracy of junior ophthalmologists. Similarly, when testing the ability of VLMs to screen for high-risk patients, ChatGPT-4o outperforms non-specialist opticians on aggregate but significantly underperforms compared to RetinaVLM-Specialist and junior ophthalmologists. In comparison, RetinaVLM-Specialist’s reports reduced the number of incorrect urgent referrals by almost four times compared to opticians and had higher recall for urgent referrals than junior ophthalmologists. Finally, RetinaVLM is able to reinforce its decisions by citing observable biomarkers within the written report, and highlighting their corresponding regions within the image.</p>
<p id="Par30">We postulate that the poor performance of ChatGPT-4o and medical foundation models alike stems from their lack of detailed knowledge related retinal OCT and AMD. Current VLMs including ChatGPT-4o and LLaVA-Med are trained on broad, unstructured datasets that are extracted from medical textbooks, scientific publications or social media posts of healthcare professional<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. In the United Kingdom and the United States, clinical trainees aspiring to become specialists must undergo up to ten years of post-graduate training to obtain the grade of a board-certified consultant. Classifying AMD requires identifying subtle yet crucial differences between disease stages, particularly in biomarkers such as hypertransmission, drusenoid versus fibrous pigment epithelial detachments, and subretinal hyperreflective material — none of which ChatGPT-4o reported on. While ChatGPT-4o is capable of explaining these differences in language, our analysis indicates it cannot yet make these distinctions in images. This reinforces our insight that the paired image-text training data of current foundation VLMs lacks specialist and experiential knowledge, hindering their effective application to real-world clinical tasks.</p>
<p id="Par31">A core innovation of our work was the creation of a dedicated training curriculum that specializes VLMs in image-based clinical decision making. Analogously to current medical education, this curriculum deconstructs clinical problems into sets of mandatory capabilities required for their resolution and selectively trains VLMs in these skills. To this end, we obtained a large number of tabular reports by processing of retrospectively collected clinical data using advanced algorithms. Additionally, we tasked ophthalmologists to produce a limited number of highly specific textual reports. In total, our curriculum comprises 41,926 OCT images with 479,710 corresponding visual questions and answers. While still modest in size compared to substantially larger foundation datasets, we believe such curated needs-driven approaches are required to deploy language models specialist healthcare. In a similar vein, leading technology companies in artificial intelligence have also started to look beyond the internet’s image and text data to source specialized training data to train LLMs and VLMs in disciplines such as computer programming, journalism, mathematics<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a>,<a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>. Future work may study specializing generalist models such as ChatGPT-4o by finetuning them on specific, high-quality curricula such as the one introduced in this study.</p>
<p id="Par32">In the following, we discuss the technical limitations of our work. Naturally, the quality of our curriculum depends on the underlying imaging reports, and in particular those used to create curriculum part 2. The majority of the reports used to create RetinaVLM-Specialist were written by a junior ophthalmologist with three years experience. The remaining reports were written by an ophthalmologist with ten years of experience, and their reports were more comprehensive. While reports written by both were found to be of high quality, future work could evaluate the benefits of a third training curriculum derived by senior ophthalmologists.</p>
<p id="Par33">By updating instruction sets, VLMs have the potential to be more adaptable than image-only deep learning approaches to differences in clinical practice between countries. However, it is important to note that our curriculum was derived using UK-specific clinical definitions and workflows. As a result, the errors made by RetinaVLM were found to reflect differences in the biomarker and staging definitions used by the UK-based and Austria-based ophthalmologists involved in the testing of the model. In particular, the model was found to be conservative in classifying fibrovascular features that upgrade intermediate AMD to inactive late wet AMD. Thus, to match or surpass the performance of the average junior ophthalmologist, we also aim to establish a consistent cohort of ophthalmologists for training and testing the model.</p>
<p id="Par34">Similarly, we observed a discrepancy in image interpretation between junior and senior ophthalmologists. Junior ophthalmologists did not recommend patients for referral if it was likely that the retinal fluid observed was caused by traction rather than neovascularization, as it is not treatable with antiangiogenic drugs. Conversely, the senior ophthalmologist preferred that these patients be still referred for immediate assessment to rule out neovascularization. RetinaVLM was explicitly instructed to refer patients with any sign of fluid of any cause, and correctly referred more patients as a result.</p>
<p id="Par35">Another technical limitation of our approach was the sensitivity of the LLM generating the question-answer pairs to the specifics of its instructions. Extensive trial and error were required to arrive at several instructions, listed in the <a href="#MOESM1" class="usa-link">Supplementary material</a>, that resulted in diverse sets of high quality question-answer pairs. We discern that all aspects of dataset creation - deciding on the required capabilities, collecting specialized annotations and converting these to question-answer pairs - should be formalized to systematically compare different approaches and ultimately scale dataset curation in the future.</p>
<p id="Par36">RetinaVLM also inherits some of the fundamental limitations of language models. LLMs are prone to confidently present false or fabricated information, termed hallucinations, which has been identified as problematic in medical contexts<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a>,<a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. This phenomenon has also been observed in VLMs, where the generated text does not relate to any object or feature that is observable in the image<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a>,<a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>. Similarly, in several cases RetinaVLM was observed to hallucinate the presence of retinal fluid and consequently report a more advanced AMD stages than was necessary. RetinaVLM’s output was also sensitive to the wording of questions and instructions. While this had little impact on our qualitative analysis, extensive trial and error was necessary to ensure that RetinaVLM responded with one of the provided options in the quantitative analyses.</p>
<p id="Par37">We now outline factors limiting the clinical applicability of the current iteration of RetinaVLM, and propose directions for addressing these in future. Currently, RetinaVLM processes a single two-dimensional OCT image from one type of OCT scanner. In ophthalmological practice, decisions are made based on three-dimensional images from multiple time points, although many recent studies on the use of foundation models in ophthalmology also analyze two-dimensional images<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>. We mitigated the impact of this discrepancy on our study by tasking ophthalmologists to select the most relevant two-dimensional slice of the imaged volume before proceeding with the referral decision. In the future, a more sophisticated vision encoder, which is able to handle three-dimensional data from diverse OCT imaging devices, could be integrated with RetinaVLM. Our results also indicate that stronger image encoders capable of detecting smaller features are necessary to improve the performance of RetinaVLM-Specialist on the detection of smaller biomarkers. Similarly, one may opt to incorporate multimodal information, such as health questionnaires, clinical tests or the patient’s medical history, into the decision making process<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>. The fundamental model architecture and training would remain similar, but the level of reasoning required for differential diagnosis across multiple scans would potentially increase.</p>
<p id="Par38">Similarly, we exclusively trained RetinaVLM for the management of a single retinal disease, AMD, ignoring other retinal pathologies such as diabetic retinopathy or glaucoma, or imaging modalities, such as color fundus photography<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a>,<a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>. While this enabled us to explore the potential to encode advanced clinical levels of specialism into VLMs at depth, it would severely limit the applicability of the current version of our models for general ocular screening. It also led the model to classify all cases of retinal fluid as wet AMD, where a few cases showed evidence of diabetic retinopathy and other retinal conditions.</p>
<p id="Par39">In order to ready VLMs for deployment for ocular screening, future work would need to extend RetinaVLM’s curriculum with domain experts from an expanded range of ophthalmological conditions. This requires costly and time-consuming curation of specialized training datasets by medical experts. However, we believe that this is a necessary investment as routine clinical skills and patient management protocols are rarely documented in existing datasets used to train AI models.</p>
<p id="Par40">Overall, our results indicate that merely increasing the scale of training datasets is insufficient for the development of VLMs with real-world clinical utility. Instead, medical VLMs require high-quality data directly related to the challenges faced by clinicians in their daily practice. We believe our proposed curriculum-based approach provides a blueprint for specializing VLMs that generate true value in healthcare.</p></section><section id="Sec9"><h2 class="pmc_sec_title">Methods</h2>
<section id="Sec10"><h3 class="pmc_sec_title">Retinal image dataset curation</h3>
<p id="Par41">We use two retinal OCT datasets in this study. The first, described in the “Retrospective patient dataset” section, contains a cohort of patients with AMD collected retrospectively at the Southampton Eye Unit. The second, described in the “External testing dataset of referred patients” section, contains scans of the initial visits of patients referred, primarily by opticians, to the Southampton Eye Unit. The curation and use of this data is summarized in a flowchart diagram in Supplementary Fig. <a href="#MOESM1" class="usa-link">11</a>.</p>
<p id="Par42">All data was collected in the scope of the PINNACLE study (ClinicalTrials.gov <a href="https://clinicaltrials.gov/ct2/show/NCT04269304" class="usa-link" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">NCT04269304</a>), which received approval from the East Midlands-Leicester Central Research Ethics Committee in the United Kingdom (ref. 19/EM/0163) and the institutional review boards of all participating institutions. It complies with the principles of Good Clinical Practice and the Declaration of Helsinki. Informed consent procedures were followed according to the principles of Good Clinical Practice and the Declaration of Helsinki. All images were captured using Topcon 3D OCT scanners (Topcon Corporation, Tokyo, Japan). Both datasets contain images of size 416 × 512 with a pixel size of 3.5 × 11.7 μm<sup>2</sup>.</p></section><section id="Sec11"><h3 class="pmc_sec_title">Retrospective patient dataset</h3>
<p id="Par43">The retrospective dataset contains 44,633 OCT images from 6152 eyes belonging to 3468 patients, collected over eight years, between 2012 and 2020, at the Southampton Eye Unit and curated by the PINNACLE consortium. For each OCT scan we use the mediolateral 2D slice centered at the fovea. We then designated 41,926 of the 44,633 OCT images from 5547 eyes of 3057 patients for training purposes. Additionally, we reserved 2,311 images from 326 eyes of 187 patients for validation, and 396 images from 279 eyes of 224 patients for testing. We ensured that images from each patient do not appear in more than one of the training, validation or test sets. The training set was used to create both curriculum parts 1 and 2, detailed in Sections Report curation and question-answer pair generation (curriculum part 1) and Report curation and question-answer pair generation (curriculum part 2). The retrospective test set was used to evaluate the resulting model in the disease staging and biomarker verification section in the Results.</p>
<p id="Par44">To evaluate the accuracy of image report conclusions, we used a multi-rater process for curating high-quality labels. We now expand on this by detailing the breakdown of disease stage definitions used in this study. AMD remains a relatively poorly understood disease, with multiple grading systems that vary in stage classification and definitions<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a>–<a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup>. Its classification remains an active area of research<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup>. Classifying a retinal OCT image into a single disease stage is challenging, as overlapping features can indicate multiple AMD stages, often requiring ophthalmologists to make intuitive assessments beyond strict criteria. In this study, the ophthalmologists reported classifying healthy and early AMD based on the presence or absence of small drusen, and intermediate AMD by medium, or intermediate, to large drusen<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>. Late dry AMD was identified by atrophy of the retinal pigment epithelium, evidenced by hypertransmission. Presence of any subretinal hyperreflective material or other scarring advanced the classification to inactive wet AMD. Finally, presence of any subretinal or intraretinal fluid upgraded the image-based classification to active late wet AMD. These classifications yielded a testing set of 276 images that labeled 36 as healthy, 18 as early, 64 as intermediate, 25 as late dry, 75 as inactive late wet and 58 as active late wet AMD.</p>
<p id="Par45">Each image was labeled with the presence or absence of 10 biomarkers, and an additional 21 labels that record their size, number, and other applicable attributes. Due to the substantially increased number of variables compared to disease staging, each image was labeled only once by one of the six junior ophthalmologists. This process yielded a testing set of 396 images that labeled 107 as evidencing drusen, 70 with pigment epithelial detachments, 150 with pigment epithelial elevation, 27 with double-layer sign, 122 with hypertransmission, 155 with atrophic/degenerated pigment epithelium layers, 60 with subretinal hyperreflective material, 81 with hyperreflective foci, 25 with subretinal fluid, and 62 with intraretinal fluid.</p></section><section id="Sec12"><h3 class="pmc_sec_title">External testing dataset of referred patients</h3>
<p id="Par46">We also collected an external dataset of 95 patients that were referred primarily by opticians for urgent care at the Southampton Eye Unit between 02/2023 and 12/2023. None had yet received treatment for AMD, and mostly had no AMD, intermediate AMD or small features related to active wet AMD. This represents a distribution shift from the retrospective cohort, where many patients had already received treatment for AMD and were in the inactive late wet stage of AMD. As such, it enabled us to estimate the robustness of both variants of RetinaVLM to shifts in patient population. This dataset was not used for model training and was reserved for testing VLMs on patient referral as shown in Fig. <a href="#Fig5" class="usa-link">5</a>.</p>
<p id="Par47">For each patient we sourced scans of both their left and right eye that were acquired on their first visit to the clinic. We also collected the originally issued letter of referral, as depicted in Fig. <a href="#Fig5" class="usa-link">5</a>d. Then, two junior ophthalmologists independently reviewed the 3D OCT volumes of both eyes to label the patient’s risk and decide from three levels of increasing referral referral urgency. To help standardize their labels, the ophthalmologists referred to a set of agreed patient referral guidelines. For full documentation see &lt;PatientReferralGuidelines&gt; in Supplementary Fig. <a href="#MOESM1" class="usa-link">10c</a>.</p>
<p id="Par48">After labeling the level of referral urgency, the ophthalmologists then selected the image slice that most supported their assessment of the patient’s risk. In healthy patients where both volumes contained no pathological signs in any of the image slices, they were instructed to select the mediolateral fovea-centered 2D slice from one of the two volumes. Finally, any inter-rater disagreements were then arbitrated by a panel involving the two senior ophthalmologists. Of the 95 images in the external cohort, 25 did not evidence need of referral, 41 indicated the patient was at moderate risk and needed general attention by a specialist, while only 29 indicated the patient needed urgent referral.</p></section><section id="Sec13"><h3 class="pmc_sec_title">RetinaVLM vision-language model architecture</h3>
<p id="Par49">RetinaVLM follows the architectural design of MiniGPT4<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>. It consists of two main components: an ophthalmological vision encoder and a generative LLM (see Supplementary Fig. <a href="#MOESM1" class="usa-link">9</a>). For the ophthalmological vision encoder, we adopt a Resnet50 convolutional neural network with over 23 million parameters which was pre-trained with self-supervised contrastive learning on the 41,926 OCT images from the train set of the retrospective cohort. Specifically, it was trained with Bootstrap Your Own Latent (BYOL)<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup> using the same implementation details as the standard contrastive approaches in<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>, which consistently performed on par with RETFound<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup> on data from the Southampton Eye Unit. This vision encoder projects each 192 × 192 input image to a set of spatially arranged 6 × 6 visual embeddings, which are extracted from the last layer before global average pooling. Each embedding has a dimension of <em>h</em><sub><em>i</em><em>m</em><em>g</em></sub> = 2048. They also have a receptive field of size 336, so each embedding contains global knowledge of the image that is contextualized at its local position.</p>
<p id="Par50">For the LLM, we employ the 8 billion parameter instruction-tuned Llama3 model by Meta<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a>,<a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup> as the generative LLM, which was was the most performant openly available model at the time of our study. LLama3 uses an embedding dimension of <em>h</em><sub><em>l</em><em>a</em><em>n</em><em>g</em></sub> = 4096.</p>
<p id="Par51">The ophthalmological vision encoder provides visual information regarding the OCT image to the LLM via an adapter. The adapter is a linear layer of size <em>h</em><sub><em>i</em><em>m</em><em>g</em></sub> × <em>h</em><sub><em>l</em><em>a</em><em>n</em><em>g</em></sub> that processes visual information for use by the LLM. Specifically, it does so by independently mapping each of the visual embeddings, applying a linear transformation via matrix multiplication, into language embeddings used by the LLM. The design and application of the adapter follows the design used in MiniGPT4<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>, and results in an adapter with over 8 million parameters.</p></section><section id="Sec14"><h3 class="pmc_sec_title">Foundation vision-language models</h3>
<p id="Par52">We used the two most widely adopted foundation vision-language models for medical applications at the time of this study<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a>,<a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>. They were both trained on large biomedical datasets sourced from the Internet, and have been applied in chest x-ray<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup>. The first, Med-Flamingo<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>, which was built on Flamingo<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup> and finetuned on image and text data from medical textbooks and the PubMed Central Open Access (PMC-OA) dataset<sup><a href="#CR49" class="usa-link" aria-describedby="CR49">49</a></sup>. The second, LLaVA-Med<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>, developed by Microsoft, is a VLM built on LLaVA<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup> and finetuned to follow textual instructions regarding a broad range of biomedical images contained in PubMed Central 15M (PMC-15M)<sup><a href="#CR50" class="usa-link" aria-describedby="CR50">50</a></sup>. The training sets of both models contain retinal OCT images and associated text. As they were trained as generalist models on various imaging modalities, they were both purportedly capable of interpreting retinal OCT images. When provided a retinal OCT image we found both could correctly identify its modality and begin generating diagnoses, without being provided any prior information. More recently, other generative vision-language models for ophthalmology have been introduced but were either not designed for OCT imaging nor finetuned with instruction-tuning<sup><a href="#CR51" class="usa-link" aria-describedby="CR51">51</a>,<a href="#CR52" class="usa-link" aria-describedby="CR52">52</a></sup>. Our third baseline constitutes OpenAI’s GPT-4o model (endpoint ‘gpt-4o-2024-05-13’). Unlike the two aforementioned medical VLMs, GPT-4o is a generalist model.</p>
<p id="Par53">For Med-Flamingo, we then provide instructions using the following template provided in their code, replacing {question} with the instruction text:</p>
<blockquote class="text-italic"><pre id="code1">You are a helpful medical assistant. You are being provided with images, a question about the image and an answer. Follow the examples and answer the last question. &lt;image&gt;Question: {question} Answer:</pre></blockquote>
<p id="Par56">Similarly, for LLaVA-Med we use the following conversation template provided by the developers:</p>
<blockquote class="text-italic"><pre id="code2">A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: {question}###Assistant:</pre></blockquote>
<p id="Par59">Similarly, for the ChatGPT-4o API we provide the following system prompt:</p>
<blockquote class="text-italic"><pre id="code3">You are an intelligent and helpful assistant. You follow all the instructions in the user prompt, and answer all the questions they ask for.</pre></blockquote></section><section id="Sec15"><h3 class="pmc_sec_title">Report curation and question-answer pair generation (curriculum part 1)</h3>
<p id="Par62">To create the tabular reports for the first part of the curriculum we used a cluster-based approach to efficiently label the 41,926 training images with biomarker annotations<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>. This training cohort includes 25,825 female and 16,101 male patients with an average age of 81.8 years, with lower (Q1) and upper (Q3) quartiles of of 77.0 and 88.0 years.</p>
<p id="Par63">Contrastive learning is used to extract self-supervised features from the dataset. The dataset is then partitioned into 40 clusters of images that share common features. Labels are then assigned to these clusters by senior ophthalmologists. To this end, 20 images from each cluster were reviewed by senior ophthalmologists. If the majority of the images exhibited common features, such as ’large drusen’ or ’subretinal fluid’, these labels were assigned to the entire cluster. A review quantifying the accuracy of each cluster description is performed in Supplementary Section <a href="#MOESM1" class="usa-link">A.4</a>.</p>
<p id="Par64">These labels were used in in combination with the patient’s age, sex and their functional visual acuity score (measured on a LogMAR chart and converted to Letter score) to create the tabular reports. We also included the quality index of the image using an intensity-histogram-based approach<sup><a href="#CR54" class="usa-link" aria-describedby="CR54">54</a></sup>. We found that labeling the dataset’s quality index quartiles as ‘very poor’, ‘ok,’ ‘good,’ and ‘excellent’ effectively captured the characteristics of each quartile. Additionally, the reports list three biomarkers that are stated as not being present. These are drawn from a distribution of all biomarkers, weighted by their prevalence in the dataset, that were not among the cluster labels for that image. Counts of the prevalence of each tabular variable among the training images are shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">1a</a>, b, and a sample of four tabular reports they result in are shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">1c</a>.</p>
<p id="Par65">To generate question-answer pairs from the large volume of tabular reports we used an LLM available for download and local use. We chose WizardLLM-70B, though any freely-available LLM with instruction-following capabilities may be used for this purpose. This model was chosen as an open-weights model with strong performance in instruction following and general knowledge at the time of dataset creation. We then used the instruction template detailed in the “Question-answer generation prompt (curriculum part 1)” section in the <a href="#MOESM1" class="usa-link">Supplementary material</a>. This resulted in a total of 408,505 question-answer pairs, or an average of 9.75 question-answer pairs per report depending on its complexity. Examples of the question-answers pairs generated by this approach are shown in the ‘curriculum part 1’ section of Supplementary Fig. <a href="#MOESM1" class="usa-link">4a</a>, b.</p></section><section id="Sec16"><h3 class="pmc_sec_title">Report curation and question-answer pair generation (curriculum part 2)</h3>
<p id="Par66">The second part of the curriculum involves further turning on a subset of 330 images from the training curriculum in part 1. This training cohort includes 205 female and 125 male patients with an average age of 80.8 years, with lower (Q1) and upper (Q3) quartiles of of 75.0 and 87.0 years.</p>
<p id="Par67">Unlike curriculum part 1, images used in curriculum part 2 were annotated with high quality reports manually curated by retinal specialists. Specifically, two junior ophthalmologists were tasked with describing the main pathological biomarkers and diagnoses related to AMD, while also noting any other observation regarding the retinal anatomy. This yielded high-quality textual reports that go beyond the short notes that ophthalmologists typically write in clinical routine. The first junior ophthalmologist, with three years of experience specializing in ophthalmology, wrote the majority of 244 reports (see Supplementary Fig. <a href="#MOESM1" class="usa-link">2a</a>). While these were highly accurate, they were less comprehensive in their analysis than the remaining 86 reports written by the junior ophthalmologist with 10 years of experience (see Supplementary Fig. <a href="#MOESM1" class="usa-link">2b</a>).</p>
<p id="Par68">Simultaneously, the same two junior ophthalmologists used the same methodology to produce 28 reports on images from the test set. In our analysis, these were found to be of high quality by the two senior ophthalmologists (see Fig. <a href="#Fig4" class="usa-link">4</a>). As they were representative of the 330 reports collected on the training set, the senior ophthalmologists concluded that these results provide sufficient quality assurance for the reports used to generate curriculum part 2.</p>
<p id="Par69">After the imaging reports were curated, we used an external LLM with 10 different instructions to generate up to 230 questions per image. The exact instructions used are documented in the “Question-answer generation prompts (curriculum part 2)” section in the <a href="#MOESM1" class="usa-link">Supplementary material</a>. We take two preliminary steps before providing the instruction to the LLM. We firstly replace the &lt;ReportText&gt; identifier with the raw text of the image report. Additionally, many of the QA generation instructions make references to the guidelines that describe the mandatory capabilities of image-based clinical decision makers with regard to disease staging and patient referral for patients with AMD. The second step involves replacing any reference to the &lt;ObservationGuidelines&gt;, &lt;DiseaseStagingGuidelines&gt; or &lt;PatientReferralGuidelines&gt; by the text of the corresponding guidelines (documented in Supplementary Fig. <a href="#MOESM1" class="usa-link">10</a>). These guidelines were verified by senior ophthalmologists, and were instrumental for the generation of questions with improved diversity and coverage, and also for creating questions about biomarkers that were absent in the image (and typically not mentioned in the report).</p>
<p id="Par70">The smaller number of reports in the advanced curriculum permitted the use of the more performant proprietary models for generating question-answer pairs. We used the ‘gpt-4o’ API endpoint from OpenAI. An example interaction with ’gpt-4o’ for generating these question-answer pairs is shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">3</a>. A sample of question-answers yielded by this approach are shown in the ‘part 2’ section of both Supplementary Fig. <a href="#MOESM1" class="usa-link">4a</a>, b.</p></section><section id="Sec17"><h3 class="pmc_sec_title">RetinaVLM model inference</h3>
<p id="Par71">RetinaVLM processes retinal OCT images and textual instructions. To begin building the input provided to the model, we first set the system prompt of the constituent LLM to:</p>
<blockquote class="text-italic"><pre id="code4">You are a helpful ophthalmological specialist chatbot capable of interpreting retinal OCT images.</pre></blockquote>
<p id="Par73">We then begin the instruction that will be provided to the LLM with the following line:</p>
<p id="Par74">Here is an encoding of a retinal OCT image &lt;Img&gt;&lt;ImageHere&gt;&lt;/Img&gt;\n</p>
<p id="Par75">For each image in the batch, we next add the text of the specific question to the instruction. For example:</p>
<blockquote class="text-italic"><pre id="code5">Describe the OCT image in detail. Does it show evidence of retinal fluid?</pre></blockquote>
<p id="Par77">To complete the textual input to the model, we populate the LLM’s conversation template with this instruction and include the question’s answer exclusively during training. This forms the full textual input, which we project to the embedding space of the LLM using its pre-existing tokenizer and pretrained embedding layer.</p>
<p id="Par78">After embedding the input text, we process the retinal OCT image in question. To this end we downsample the image by a factor of 2 from 416 × 512 to 208 × 256 pixels. We then crop the image centrally in testing, and augment each image using the finetuning protocol outlined in ref. <sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup> in training. This results in a processed OCT image of size 192 × 192.</p>
<p id="Par79">Next, we use the ophthalmic vision encoder <em>E</em> to extract the 6 × 6 vision feature embeddings from the image. After flattening these, we apply the adapter to each embedding separately. This projects the visual embeddings to the input embedding space of the LLM. To create the final set of embeddings that are provided to the LLM, we replace the textual embeddings corresponding to the &lt;ImageHere&gt; phrase with the 36 adapted vision embeddings from the actual retinal OCT image.</p>
<p id="Par80">Finally, the resulting sequence of language embeddings and adapted visual embeddings are passed together through the frozen LLM. This yields a list of predicted token logits with the same length as the input sequence. We then compute the causal language modeling loss between the predicted answer logits and the ground truth answer tokens. We then optimize the adapter to minimize this loss.</p>
<p id="Par81">Crucially, when training RetinaVLM we keep both the vision encoder and LLM <em>frozen</em>. That is, they are not updated during the entire training process. This is key to preserving RetinaVLM’s inheritance of the pretrained LLM’s language and reasoning capabilities. These enable RetinaVLM to handle versatile questions and instructions beyond those encountered in the curriculum during training. Thus, during training, we only update the adapter that feeds visual information regarding the OCT image to the LLM.</p></section><section id="Sec18"><h3 class="pmc_sec_title">Training RetinaVLM on curriculum parts 1 and 2</h3>
<p id="Par82">RetinaVLM is trained sequentially on curriculum part 1 and part 2. After randomly initializing the adapter we first train the model on the 408,545 question-answer pairs regarding the 41,926 images in curriculum part 1 (introduction to retina) to obtain <em>RetinaVLM-Base</em>. The then continue to finetune the model on the 71,165 questions and answers regarding the 330 images in curriculum part 2 (advanced retinal specialism), resulting in the final <em>RetinaVLM-Specialist</em> model. For each of the curriculum parts we train RetinaVLM for 100,000 steps using a batch size of 12. In each step we randomly select one question-answer pair per image from the current curriculum. To update the network we use the AdamW optimizer with a learning rate of 10<sup>−4</sup>, and <em>β</em><sub>1</sub> = 0.9 and <em>β</em><sub>2</sub> = 0.999.</p>
<p id="Par83">Training on the two curriculum parts separately is important for assessing the accumulative benefits of progressively specialist training in two ways. Firstly, this enables us to measure the benefits of training on free-text medical reports over tabular data. Secondly, during development we found that reserving the highest quality data for the final training stage, curriculum part 2, improved the performance of the resulting model. This strategy is standard practice in the development of LLM-based models<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>.</p></section><section id="Sec19"><h3 class="pmc_sec_title">Experimental setup</h3>
<p id="Par84">After VLMs have been trained on image and text datasets, they can be used to generate responses to new questions and images. To use the baseline foundation VLMs for testing we take the central crop in each 416 × 512 OCT image, resulting in an image of size 384 × 384. After repeating this image along the color dimension, ChatGPT-4o accepts the resulting 3 × 384 × 384 RGB image, while the Med-Flamingo and LLaVA-Med baselines require we downsample this to 3 × 224 × 224. To provide the image to the RetinaVLM variants, we first downsample the 416 × 512 image by a factor of two to 208 × 256, before taking a central crop resulting in an image of size 192 × 192.</p>
<p id="Par85">We then employ the same method with all VLMs for generating responses to instructions. Provided the image and textual instruction, we build the output sequence of tokens by repeatedly appending the token to the output assigned the highest probability by the VLM. This is equivalent to using a temperature parameter set to 0, and is the standard approach for generating the most accurate and least creative output from LLM-based models. All other generation parameters, such as repetition penalty, were not used. This process is repeated until a stop token is generated, signaling the end of the VLM’s response. The model’s tokenizer is then used to convert the numeric output tokens to the final free text output.</p>
<p id="Par86">Our entire evaluation was conducted in <em>zero-shot</em>, that is, after training on curriculum part 1 and part 2 RetinaVLM requires no further finetuning in order to perform tasks related to disease staging, patient referral and biomarker analysis. Instead, for each test we designed a specific instruction that was provided to all VLMs to generate the application-specific reports that were used in our analyses. These instructions were derived through experimentation with all VLMs on the validation set, and are documented in the following four subsections. To inform their design, we assessed the effectiveness of each prompt by assessing the quality of outputs it produced in all VLMs on a subset of 30 images from the validation set. This led to two observations. Firstly, that all VLMs produced improved responses when prompted to first detail its ‘chain-of-thought’<sup><a href="#CR55" class="usa-link" aria-describedby="CR55">55</a></sup>. This led us to request that the model first describe the OCT image in detail before making any recommendations. Secondly, that simpler, more direct prompts resulted in all VLMs making fewer errors in following our subsequent, test-specific instructions.</p></section><section id="Sec20"><h3 class="pmc_sec_title">Report generation for disease staging</h3>
<p id="Par87">The following instruction was given to all VLMs to and obtain the reports of the 276 test images that were analyzed in Fig. <a href="#Fig3" class="usa-link">3</a>. The instruction requests VLMs to begin by describing the image, and then deduce the most advanced disease stage:</p>
<blockquote class="text-italic"><pre id="code6">Describe the OCT image in detail and list any biomarkers or abnormalities, including the most likely AMD stage of the patient.</pre></blockquote>
<blockquote class="text-italic"><pre id="code7">Then, based on those observations, state if the patient’s most advanced AMD stage is “healthy’, “early’, “intermediate’, “late dry’, “late wet (inactive)’ or “late wet (active)’?</pre></blockquote>
<p id="Par90">After the VLM generated its report (using a maximum of 500 tokens), we appended the following incomplete sentence to its output:</p>
<pre id="code8">Based off the image and those findings, the patient's most advanced AMD stage is</pre>
<p id="Par92">which the VLM them completes using up to 300 tokens. From these tokens, we extracted the final disease staging prediction by searching for the first instance of any of the listed disease stages. This post-processing step is only necessary for quantitative tests of accuracy, as it enables the reliable extraction of the disease stage from the free text report. In cases where the VLM discusses multiple disease stages, such as in ‘more advanced than early AMD, and is intermediate AMD as there is no evidence of late wet AMD’, the disease stage was manually extracted. In cases where no disease stage was provided or could be extracted manually, this counted as an ‘Invalid response’.</p></section><section id="Sec21"><h3 class="pmc_sec_title">Report generation for qualitative evaluation</h3>
<p id="Par93">For the qualitative evaluation by the senior ophthalmologists, we randomly selected 28 of the test images from the retrospective dataset, and tasked the two junior ophthalmologists with annotating the images. We provided the following instruction to RetinaVLM-Specialist and LLaVA-Med to generate their image reports:</p>
<blockquote class="text-italic"><pre id="code9">Write an extensive report describing the OCT image, noting any biomarkers or abnormalities related to AMD, and their qualities. Also comment on which biomarkers are absent.</pre></blockquote>
<blockquote class="text-italic"><pre id="code10">Finally, based on the image and these findings, your report should estimate the AMD disease stage of the patient.</pre></blockquote>
<blockquote class="text-italic"><pre id="code11">You should not include any patient referral recommendations in your report, but you can comment if they need treatment with anti-vegf.</pre></blockquote>
<p id="Par97">We found that ChatGPT-4o tended to write excessively long reports. To improve the performance of ChatGPT-4o in direct evaluations by senior ophthalmologists, we requested a ‘brief’ rather than an ‘extensive’ report, and appended the following text to the original instruction: ‘Your report should be no longer than 120 words long’. This helped make reports more concise with no observable loss in accuracy, though they still exceeded the length of reports written by junior ophthalmologists and RetinaVLM-Specialist.</p>
<p id="Par98">We then assigned 14 of the images and the corresponding 42 reports by ChatGPT-4o, RetinaVLM-Specialist and the junior ophthalmologists, to each of the two senior ophthalmologists, who evaluated them independently according to the correctness, completeness and conciseness. These criteria, also documented in Fig. <a href="#Fig4" class="usa-link">4</a>, were:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par99">Correctness - The report is accurate in its main observations and conclusions regarding the image</p></li>
<li><p id="Par100">Completeness - The report contains all relevant observations and conclusions that can be inferred from the image</p></li>
<li><p id="Par101">Conciseness - The report does not make observations and conclusions that are not supported by, or not seen in, the image</p></li>
</ul></section><section id="Sec22"><h3 class="pmc_sec_title">Report generation for patient referral</h3>
<p id="Par102">The following instruction was given to all VLMs to generate reports that focus on patient referral recommendations, which are analyzed in Fig. <a href="#Fig3" class="usa-link">3</a>. This instruction was run for the 95 referral images, introduced in the “External testing dataset of referred patients” section. In order to accurately convey the specific requirements of the wet AMD treatment clinic, we provided the comprehensive referral protocol used by the senior ophthalmologists in the instruction:</p>
<p id="Par103">Do not provide a disease stage, or referral recommendation yet.</p>
<p id="Par104">Being seen by a specialist at the Southampton clinic:</p>
<ul class="list" style="list-style-type:none">
<li>
<span class="label">A.</span><p class="display-inline" id="Par105">The Southampton clinic requires that patients with any sign of intraretinal fluid, any sign of subretinal fluid, or any sign of cyst(s), MUST be seen by a specialist at the Southampton clinic within the next two weeks.</p>
</li>
<li>
<span class="label">B.</span><p class="display-inline" id="Par106">The Southampton clinic requires that patients who do not have any sign of intraretinal fluid, any sign of subretinal fluid, or any sign of cyst(s), but do have some biomarkers of early or intermediate AMD, should be seen by a specialist at the Southampton clinic for routine referral.</p>
</li>
<li>
<span class="label">C.</span><p class="display-inline" id="Par107">The Southampton clinic requires that patients who do not have any sign of intraretinal fluid, any sign of subretinal fluid, or any sign of cyst(s), but do have medium to large drusen, drusenoid PED, hypertransmission or atrophy, should be seen by a specialist at the Southampton clinic for routine referral.</p>
</li>
<li>
<span class="label">D.</span><p class="display-inline" id="Par108">The Southampton clinic does not need to see patients who have no biomarkers and healthy retinas at all.</p>
</li>
</ul>
<blockquote class="text-italic"><pre id="code12">Southampton specialist visit: Next, tell me if your initial report of the OCT image indicates that the patient should be seen by a specialist at the Southampton clinic "within the next two weeks", to be seen "within 18 weeks (routine referral)", or "not be seen" at all?</pre></blockquote>
<p id="Par111">As before, after the VLM generated its report (using a maximum of 500 tokens), we added the following incomplete sentence to its output:</p>
<blockquote class="text-italic"><pre id="code13">My report indicates that the patient</pre></blockquote>
<p id="Par113">which is then completed by the VLM for up to a maximum of 300 tokens. We then searched these tokens for the first instance of one of the three levels of referral urgency, ‘within the next two weeks’, ‘within 18 weeks (routine referral)’ or ‘not be seen’, in the VLM’s output report. In cases where different wording is used, but with identical meaning, the VLM’s prediction is extracted manually. In the remainder of cases, it is listed as an ‘Invalid response’.</p></section><section id="Sec23"><h3 class="pmc_sec_title">Report generation for biomarker analysis</h3>
<p id="Par114">The following instruction was given to all VLMs to generate reports that conclude the presence of absence of the 10 different biomarkers, evaluated on the 396 test images in Fig. <a href="#Fig6" class="usa-link">6</a>:</p>
<blockquote class="text-italic"><pre id="code14">Describe the OCT image in detail and list all biomarkers or abnormalities. Detail if there are any signs indicating that biomarker might be present, even if there is only a small amount.</pre></blockquote>
<blockquote class="text-italic"><pre id="code15">Finally, conclude your findings by telling me if biomarker article "not present", or if potentially any amount of biomarker article "present" in the OCT image.</pre></blockquote>
<p id="Par117">For each of the 10 biomarkers, the phrase {biomarker} was replaced by the actual biomarker name (such as ‘subretinal fluid’), and the {article} replaced by 'is' for singular biomarkers or 'are' for plural biomarkers (such as drusen). After the VLM generated its report (using a maximum of 500 tokens), we appended the following incomplete sentence to its output:</p>
<blockquote class="text-italic"><pre id="code16">To conclude these findings, in the OCT image {biomarker} {article}</pre></blockquote>
<p id="Par119">which the VLM then completes using up to 300 tokens. We then searched for the first instance of not present or present to extract the model’s prediction of the absence or presence of the biomarker, respectively. In cases where different wording is used, but with identical meaning, such as stating the biomarker was ‘detected’ rather than ‘present’, the VLM’s prediction is extracted manually. In the remainder of cases, it is listed as an ‘Invalid response’.</p></section><section id="Sec24"><h3 class="pmc_sec_title">Computing language-based image saliency maps</h3>
<p id="Par120">We provide methodogolical details for the computation of the language-based saliency maps shown in Fig. <a href="#Fig6" class="usa-link">6</a>d and Supplementary Fig. <a href="#MOESM1" class="usa-link">8</a>. With saliency maps we aim to identify which regions of the image were most relevant to certain passages, such as large subretinal fluid, of RetinaVLM-Specialist’s responses. The most direct way to generate these visualizations to use attention maps, but we found Llama3’s pretrained attention maps did not result in any meaningful saliency maps. To address this, we used Grad-CAM<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>, a technique for highlighting the most relevant image regions to the prediction of an image classifier. Specifically, we apply Grad-CAM to the sum of the tokens in the output passage, effectively reframing the LLM as an image classifier where the output corresponds to a specific passage of the report. This allows us to generate saliency maps relative to that passage. A code implementation can be found at the repository referenced in the “Code availability” section.</p></section><section id="Sec25"><h3 class="pmc_sec_title">Comparison to an image-only classification-based deep learning baseline</h3>
<p id="Par121">VLMs are an emerging technology that hold great potential to automate language-based reporting and decisions regarding medical images. To provide a comparison between VLMs and more established approaches to classification we use RETFound, which was pretrained on over 700,000 2D OCT images and exhibits strong performance when finetuned on as few as 100 images<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a>,<a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>. RETFound makes predictions from the image alone and cannot interpret nor respond to written language. As an image encoder, RETFound instead outputs a single classification per image.</p>
<p id="Par122">We compare the VLMs against RETFound in both the disease staging and patient referral tasks. To this end, we formulated disease staging as a six-way classification task using the same stages as the experiment shown in Fig. <a href="#Fig3" class="usa-link">3</a>. Similarly, we formulated the patient referral task as a three-way classification task using the same referral levels as the experiment shown in Fig. <a href="#Fig4" class="usa-link">4</a>. In both cases, we manually extract training labels from the imaging reports of the same 330 training images used in curriculum part 2 to train RetinaVLM-Specialist (see the “Retrospective patient dataset” section).</p>
<p id="Par123">After formulating these tasks, we used the standard linear evaluation approach to evaluate the performance of the RETFound imaging encoder. This involves applying global average pooling to RETFound’s output tokens to yield a feature vector of size 1024. We then train a linear layer that maps this feature vector to the output classes of the given classification problem. This was done using an AdamW optimizer with learning rate of 3⋅10<sup>−4</sup> for 5000 training steps. In both cases, performance converged and little to no overfit was observed on the validation set. We then selected the step with the best performance on the validation set for evaluation. Finally, this classifier is tested on the same arbitrated test labels as all VLMs in Figs. <a href="#Fig3" class="usa-link">3</a> and <a href="#Fig4" class="usa-link">4</a>.</p>
<p id="Par124">The results of both experiments are shown in Supplementary Fig. <a href="#MOESM1" class="usa-link">6a</a>. For the disease staging task we find that RETFound (0.63 F1) significantly outperforms the best performing foundation medical VLMs (0.11 F1) and RetinaVLM-Base (0.30 F1). Notably, it performs as well as RetinaVLM-Specialist (0.63 F1). However, on the patient referral task RETFound (0.37 F1) performs on par with the best foundation VLM (0.39 F1) and significantly worse than RetinaVLM-Specialist (0.67 F1).</p>
<p id="Par125">The equal performance of RetinaVLM-Specialist and the image-only RETFound baseline in disease staging implies that both have reached an upper bound on performance for this task on the retrospective dataset. However, we observed relatively poor performance from the image-only baseline on the patient referral task, which we attribute to domain shift. Specifically, the retrospective dataset lacks images with intact retinal pigment epithelium layers that feature small fluid pockets, which represent many of the urgent referral cases in the referral dataset. This image-only model’s ability to generalize well to these cases. While accurate patient referral is achievable using an image-only encoder like RETFound, addressing this domain shift would necessitate the collection of a new training dataset that is more representative of the referral cohort. In contrast, RetinaVLM-Specialist is better able to mitigate this domain shift by utilizing task-specific textual instructions that provide details about the target domain – namely, the patient referral protocol outlined in the “Report generation for patient referral” section.</p></section><section id="Sec26"><h3 class="pmc_sec_title">Measurements of performance and statistical analysis</h3>
<p id="Par126">To calculate the performance of each VLM and retinal specialist in all multiple-choice question answering, we used the micro F1 score. This aggregates the total number of false positives (FP), false negatives (FN), true negatives (TN) and true positives (TP) over all classes before computing the F1 score using Eq. (<a href="#Equ1" class="usa-link">1</a>)</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><math id="d33e1650" display="block"><mrow><msub><mrow><mi>F</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mrow><mn>2</mn><mo>⋅</mo><mi>T</mi><mi>P</mi></mrow><mrow><mn>2</mn><mo>⋅</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></math></td>
<td class="label">1</td>
</tr></table>
<p id="Par127">In cases where the VLM returned an ‘Invalid response’ by failing to pick one of the listed options this was counted as a false negative for the ground truth class. After calculating the F1 score, we determined the 95%confidence interval through bootstrapping <em>N</em> = 1000 times with replacement.</p>
<p id="Par128">Tests of significance (aggregated in Supplementary Fig. <a href="#MOESM1" class="usa-link">6</a>) were calculated using a two-sided McNemar’s test<sup><a href="#CR56" class="usa-link" aria-describedby="CR56">56</a></sup>. This test assesses the difference in the number of correctly versus incorrectly predicted samples, focusing on cases where the models agree or disagree on the labels. A significant p-value from the McNemar test allows us to reject the null hypothesis that both models have identical classification performance. We then used the following notation to indicate levels of statistical significance: *** for <em>p</em> ≤ 0.001, ** for <em>p</em> ≤ 0.01, and * for <em>p</em> ≤ 0.05 and ‘ns’ (not significant) for <em>p</em> &gt; 0.05.</p></section><section id="Sec27"><h3 class="pmc_sec_title">Computing hardware and software</h3>
<p id="Par129">We use Python 3.12.2 to conduct all model question-answer generation, VLM training, and VLM evaluation. To generate the question-answer pairs for curriculum part 1 we used 3 40GB NVIDIA A40 GPUs. For both training RetinaVLM and for evaluating all VLMs we use a single 80GB NVIDIA A100 GPU and PyTorch<sup><a href="#CR57" class="usa-link" aria-describedby="CR57">57</a></sup> version 2.1.2. Training RetinaVLM on takes 1 day on curriculum part 1, and another day on curriculum part 2. Llama3 was downloaded via Huggingface with model ID ‘meta-llama/Meta-Llama-3-8B-Instruct’. The baseline VLM Med-Flamingo’s code and model weights were installed following the instructions at <a href="https://github.com/fastscience-ai/MedFlamingo" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/fastscience-ai/MedFlamingo</a>, and LLaVA-Med’s from <a href="https://github.com/microsoft/LLaVA-Med" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/LLaVA-Med</a>. Confusion matrices and results calculations were computed with scikit-learn version 1.4.1 and numpy version 1.26.4. Figures and tables were created in draw.io v24.4.0 using plots generated by matplotlib version 3.8.4 and seaborn version 0.13.1. Grad-CAM was computed using grad-cam version 1.5.0. McNemar’s tests of significance were calculated using statsmodels version 0.14.1.</p></section></section><section id="Sec28"><h2 class="pmc_sec_title">Supplementary information</h2>
<section class="sm xbox font-sm" id="MOESM1"><div class="media p"><div class="caption">
<a href="/articles/instance/12365215/bin/41746_2025_1893_MOESM1_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Information</a><sup> (23.3MB, pdf) </sup>
</div></div></section></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>We thank Angela Cree for her administrative support of this work. This research was funded in whole or in part by the Wellcome Trust [210572/Z/18/Z]. For the purpose of open access, the author has applied a CC- BY public copyright licence to any author accepted manuscript version arising from this submission. The project has also been funded by ERC Advanced Grant Deep4MI (884622) and by ERSRC grant EP/Y015665/1.. M.J.M. is funded by the German Research Foundation under project 532139938.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>R.H and M.J.M. conceived and designed the study, in addition to writing the original manuscript. T.R.P.T. and C.H. contributed textual annotations, and T.R.P.T., C.H, S.R., J.M., M.P. and D.M. contributed image labels. P.H., P.M., J.C.P. and D.R. provided technical guidance and testing of open source code. H.P.N.S., H.B. and U.S.E. provided feedback on the manuscript. S.S. and A.J.L. performed the reader study and label arbitration. All authors critically reviewed and approved the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The datasets used and analyzed during the current study are currently being curated and maintained by the Vienna Reading Center on behalf of the PINNACLE consortium. The data will be made available to once the PINNACLE study concludes in 2026. A minimal dataset is available from the corresponding author before 2026<sup><a href="#CR58" class="usa-link" aria-describedby="CR58">58</a></sup> upon reasonable request. Moreover, the code repository includes a minimal dataset that can be used to interpret, verify and extend the research in the article.</p></section><section id="notes3"><h2 class="pmc_sec_title">Code availability</h2>
<p>The code and guidelines used to create the question-answer pairs, train, and evaluate the models can be found at <a href="https://github.com/RobbieHolland/SpecialistVLMs" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/RobbieHolland/SpecialistVLMs</a>. The code may be used to develop the models can be repurposed for other medical specialties. Moreover, we make all model weights for RetinaVLM-Base and RetinaVLM-Specialist openly available at <a href="https://huggingface.co/RobbieHolland/RetinaVLM" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://huggingface.co/RobbieHolland/RetinaVLM</a>. These are only intended for research purposes related to retinal OCT images.</p></section><section id="FPar1"><h2 class="pmc_sec_title">Competing interests</h2>
<p id="Par130">The authors declare no competing interests.</p></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm">
<div class="fn p" id="fn1"><p><strong>Publisher’s note</strong> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>
<div class="fn p" id="fn2"><p>These authors contributed equally: Daniel Rueckert, Sobha Sivaprasad, Andrew J. Lotery, Martin J. Menten.</p></div>
<div class="fn p" id="fn3"><p>A list of authors and their affiliations appears at the end of the paper.</p></div>
</div></section><section id="_ci93_" lang="en" class="contrib-info"><h2 class="pmc_sec_title">Contributor Information</h2>
<p>Robbie Holland, Email: robbie.holland@stanford.edu.</p>
<p>On behalf of the PINNACLE consortium: </p>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Holland%20R%22%5BAuthor%5D" class="usa-link">Robbie Holland</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Taylor%20TRP%22%5BAuthor%5D" class="usa-link">Thomas R. P. Taylor</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Holmes%20C%22%5BAuthor%5D" class="usa-link">Christopher Holmes</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Riedl%20S%22%5BAuthor%5D" class="usa-link">Sophie Riedl</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mai%20J%22%5BAuthor%5D" class="usa-link">Julia Mai</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Patsiamanidi%20M%22%5BAuthor%5D" class="usa-link">Maria Patsiamanidi</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mitsopoulou%20D%22%5BAuthor%5D" class="usa-link">Dimitra Mitsopoulou</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Scholl%20HPN%22%5BAuthor%5D" class="usa-link">Hendrik P. N. Scholl</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Bogunovi%C4%87%20H%22%5BAuthor%5D" class="usa-link">Hrvoje Bogunović</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Schmidt-Erfurth%20U%22%5BAuthor%5D" class="usa-link">Ursula Schmidt-Erfurth</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Rueckert%20D%22%5BAuthor%5D" class="usa-link">Daniel Rueckert</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sivaprasad%20S%22%5BAuthor%5D" class="usa-link">Sobha Sivaprasad</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Lotery%20AJ%22%5BAuthor%5D" class="usa-link">Andrew J. Lotery</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Menten%20MJ%22%5BAuthor%5D" class="usa-link">Martin J. Menten</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Prevost%20T%22%5BAuthor%5D" class="usa-link">Toby Prevost</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Fritsche%20L%22%5BAuthor%5D" class="usa-link">Lars Fritsche</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Pfau%20K%22%5BAuthor%5D" class="usa-link">Kristina Pfau</a>, and <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Pfau%20M%22%5BAuthor%5D" class="usa-link">Maximilian Pfau</a></p></section><section id="sec29"><h2 class="pmc_sec_title">Supplementary information</h2>
<p>The online version contains supplementary material available at 10.1038/s41746-025-01893-8.</p></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Moy, A. J. et al. Measurement of clinical documentation burden among physicians and nurses using electronic health records: a scoping review. <em>J. Am. Med. Inform. Assoc.</em><strong>28</strong>, 998–1008 (2021).
</cite> [<a href="https://doi.org/10.1093/jamia/ocaa325" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8068426/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33434273/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Moy,%20A.%20J.%20et%20al.%20Measurement%20of%20clinical%20documentation%20burden%20among%20physicians%20and%20nurses%20using%20electronic%20health%20records:%20a%20scoping%20review.%20J.%20Am.%20Med.%20Inform.%20Assoc.28,%20998%E2%80%931008%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Acosta, J. N., Falcone, G. J., Rajpurkar, P. &amp; Topol, E. J. Multimodal biomedical AI. <em>Nat. Med.</em><strong>28</strong>, 1773–1784 (2022).
</cite> [<a href="https://doi.org/10.1038/s41591-022-01981-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36109635/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Acosta,%20J.%20N.,%20Falcone,%20G.%20J.,%20Rajpurkar,%20P.%20&amp;%20Topol,%20E.%20J.%20Multimodal%20biomedical%20AI.%20Nat.%20Med.28,%201773%E2%80%931784%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Moor, M. et al. Foundation models for generalist medical artificial intelligence. <em>Nature</em><strong>616</strong>, 259–265 (2023).
</cite> [<a href="https://doi.org/10.1038/s41586-023-05881-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37045921/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Moor,%20M.%20et%20al.%20Foundation%20models%20for%20generalist%20medical%20artificial%20intelligence.%20Nature616,%20259%E2%80%93265%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR4">
<span class="label">4.</span><cite>Rajpurkar, P. &amp; Lungren, M. P. The current and future state of ai interpretation of medical images. <em>N. Engl. J. Med.</em><strong>388</strong>, 1981–1990 (2023).
</cite> [<a href="https://doi.org/10.1056/NEJMra2301725" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37224199/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Rajpurkar,%20P.%20&amp;%20Lungren,%20M.%20P.%20The%20current%20and%20future%20state%20of%20ai%20interpretation%20of%20medical%20images.%20N.%20Engl.%20J.%20Med.388,%201981%E2%80%931990%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Zhang, Y., Jiang, H., Miura, Y., Manning, C. D. &amp; Langlotz, C. P. Contrastive learning of medical visual representations from paired images and text. In <em>Machine Learning for Healthcare Conference</em>, 2–25 (PMLR, 2022).</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. J. &amp; Zou, J. A visual–language foundation model for pathology image analysis using medical twitter. <em>Nat. Med.</em><strong>29</strong>, 2307–2316 (2023).
</cite> [<a href="https://doi.org/10.1038/s41591-023-02504-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37592105/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Huang,%20Z.,%20Bianchi,%20F.,%20Yuksekgonul,%20M.,%20Montine,%20T.%20J.%20&amp;%20Zou,%20J.%20A%20visual%E2%80%93language%20foundation%20model%20for%20pathology%20image%20analysis%20using%20medical%20twitter.%20Nat.%20Med.29,%202307%E2%80%932316%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<span class="label">7.</span><cite>Lu, M. Y. et al. A visual-language foundation model for computational pathology. <em>Nat. Med.</em><strong>30</strong>, 863–874 (2024).
</cite> [<a href="https://doi.org/10.1038/s41591-024-02856-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11384335/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38504017/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Lu,%20M.%20Y.%20et%20al.%20A%20visual-language%20foundation%20model%20for%20computational%20pathology.%20Nat.%20Med.30,%20863%E2%80%93874%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>Christensen, M., Vukadinovic, M., Yuan, N. &amp; Ouyang, D. Vision–language foundation model for echocardiogram interpretation. <em>Nat. Med.</em><strong>30</strong>, 1481–1488 (2024).</cite> [<a href="https://doi.org/10.1038/s41591-024-02959-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11108770/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38689062/" class="usa-link">PubMed</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>Li, C. et al. Llava-med: training a large language-and-vision assistant for biomedicine in one day. <em>Adv. Neural Inf. Proces. Syst.</em><strong>36</strong> 28541–28564 (2024).</cite>
</li>
<li id="CR10">
<span class="label">10.</span><cite>Moor, M. et al. Med-flamingo: a multimodal medical few-shot learner. In <em>Machine Learning for Health</em>, 353–367 (PMLR, 2023).</cite>
</li>
<li id="CR11">
<span class="label">11.</span><cite>Tu, T. et al. Towards generalist biomedical AI. <em>NEJM AI</em><strong>1</strong>, AIoa2300138 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Tu,%20T.%20et%20al.%20Towards%20generalist%20biomedical%20AI.%20NEJM%20AI1,%20AIoa2300138%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Kung, T. H. et al. Performance of ChatGPT on USMLE: potential for AI-assisted medical education using large language models. <em>PLoS Digital Health</em><strong>2</strong>, e0000198 (2023).
</cite> [<a href="https://doi.org/10.1371/journal.pdig.0000198" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9931230/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36812645/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Kung,%20T.%20H.%20et%20al.%20Performance%20of%20ChatGPT%20on%20USMLE:%20potential%20for%20AI-assisted%20medical%20education%20using%20large%20language%20models.%20PLoS%20Digital%20Health2,%20e0000198%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>Singhal, K. et al. Large language models encode clinical knowledge. <em>Nature</em><strong>620</strong>, 172–180 (2023).
</cite> [<a href="https://doi.org/10.1038/s41586-023-06291-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10396962/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37438534/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Singhal,%20K.%20et%20al.%20Large%20language%20models%20encode%20clinical%20knowledge.%20Nature620,%20172%E2%80%93180%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Hager, P. et al. Evaluation and mitigation of the limitations of large language models in clinical decision-making. <em>Nat. Med.</em><a href="https://www.nature.com/articles/s41591-024-03097-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.nature.com/articles/s41591-024-03097-1</a> (2024).</cite> [<a href="https://doi.org/10.1038/s41591-024-03097-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11405275/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38965432/" class="usa-link">PubMed</a>]</li>
<li id="CR15">
<span class="label">15.</span><cite>Bengio, Y., Louradour, J., Collobert, R. &amp; Weston, J. Curriculum learning. In <em>Proceedings of the 26th annual international conference on machine learning</em>, 41–48 (ICML, 2009).</cite>
</li>
<li id="CR16">
<span class="label">16.</span><cite>Ouyang, L. et al. Training language models to follow instructions with human feedback. <em>Adv. Neural Inf. Process. Syst.</em><strong>35</strong>, 27730–27744 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ouyang,%20L.%20et%20al.%20Training%20language%20models%20to%20follow%20instructions%20with%20human%20feedback.%20Adv.%20Neural%20Inf.%20Process.%20Syst.35,%2027730%E2%80%9327744%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR17">
<span class="label">17.</span><cite>Wei, J. et al. Finetuned language models are zero-shot learners. In <em>Proceedings of the International Conference on Learning Representations</em>, (ICLR, 2022).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>Liu, H., Li, C., Wu, Q. &amp; Lee, Y. J. Visual instruction tuning. In <em>Advances in neural information processing systems</em>, 36 (NeurIPS, 2024).</cite> [<a href="/articles/PMC11867732/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40017809/" class="usa-link">PubMed</a>]</li>
<li id="CR19">
<span class="label">19.</span><cite>Mitchell, P., Liew, G., Gopinath, B. &amp; Wong, T. Y. Age-related macular degeneration. <em>Lancet</em><strong>392</strong>, 1147–1159 (2018).
</cite> [<a href="https://doi.org/10.1016/S0140-6736(18)31550-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30303083/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Mitchell,%20P.,%20Liew,%20G.,%20Gopinath,%20B.%20&amp;%20Wong,%20T.%20Y.%20Age-related%20macular%20degeneration.%20Lancet392,%201147%E2%80%931159%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Wong, W. L. et al. Global prevalence of age-related macular degeneration and disease burden projection for 2020 and 2040: a systematic review and meta-analysis. <em>Lancet Glob. Health</em><strong>2</strong>, e106–e116 (2014).
</cite> [<a href="https://doi.org/10.1016/S2214-109X(13)70145-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25104651/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wong,%20W.%20L.%20et%20al.%20Global%20prevalence%20of%20age-related%20macular%20degeneration%20and%20disease%20burden%20projection%20for%202020%20and%202040:%20a%20systematic%20review%20and%20meta-analysis.%20Lancet%20Glob.%20Health2,%20e106%E2%80%93e116%20(2014)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR21">
<span class="label">21.</span><cite>Holland, R. et al. Metadata-enhanced contrastive learning from retinal optical coherence tomography images. <em>Med. Image Anal.</em><strong>97</strong>, 103296 (2024).
</cite> [<a href="https://doi.org/10.1016/j.media.2024.103296" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39154616/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Holland,%20R.%20et%20al.%20Metadata-enhanced%20contrastive%20learning%20from%20retinal%20optical%20coherence%20tomography%20images.%20Med.%20Image%20Anal.97,%20103296%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Zhou, Y. et al. A foundation model for generalizable disease detection from retinal images. <em>Nature</em><strong>622</strong>, 156–163 (2023).
</cite> [<a href="https://doi.org/10.1038/s41586-023-06555-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10550819/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37704728/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Zhou,%20Y.%20et%20al.%20A%20foundation%20model%20for%20generalizable%20disease%20detection%20from%20retinal%20images.%20Nature622,%20156%E2%80%93163%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR23">
<span class="label">23.</span><cite>Meta AI. Introducing Meta Llama 3: The most capable openly available LLM to date <a href="https://ai.meta.com/blog/meta-llama-3/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://ai.meta.com/blog/meta-llama-3/</a> (2024). Accessed: 2024-06-25.</cite>
</li>
<li id="CR24">
<span class="label">24.</span><cite>Zhu, D. et al. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In <em>Proceedings of the International Conference on Learning Representations</em>, (ICLR, 2024).</cite>
</li>
<li id="CR25">
<span class="label">25.</span><cite>Anderson, P. et al. Bottom-up and top-down attention for image captioning and visual question answering. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 6077–6086 (IEEE, 2018).</cite>
</li>
<li id="CR26">
<span class="label">26.</span><cite>Resnikoff, S., Felch, W., Gauthier, T.-M. &amp; Spivey, B. The number of ophthalmologists in practice and training worldwide: a growing gap despite more than 200 000 practitioners. <em>Br. J. Ophthalmol.</em><strong>96</strong>, 783–787 (2012).
</cite> [<a href="https://doi.org/10.1136/bjophthalmol-2011-301378" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22452836/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Resnikoff,%20S.,%20Felch,%20W.,%20Gauthier,%20T.-M.%20&amp;%20Spivey,%20B.%20The%20number%20of%20ophthalmologists%20in%20practice%20and%20training%20worldwide:%20a%20growing%20gap%20despite%20more%20than%20200%20000%20practitioners.%20Br.%20J.%20Ophthalmol.96,%20783%E2%80%93787%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR27">
<span class="label">27.</span><cite>OpenAI. Gpt-4o: Openai’s multimodal language model. <a href="https://openai.com/index/hello-gpt-4o/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://openai.com/index/hello-gpt-4o/</a> (2024). Accessed: 2025-02-17.</cite>
</li>
<li id="CR28">
<span class="label">28.</span><cite>Van Veen, D. et al. Adapted large language models can outperform medical experts in clinical text summarization. <em>Nat. Med.</em><strong>30</strong>, 1134–1142 (2024).</cite> [<a href="https://doi.org/10.1038/s41591-024-02855-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11479659/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38413730/" class="usa-link">PubMed</a>]</li>
<li id="CR29">
<span class="label">29.</span><cite>Fragiotta, S. et al. Significance of hyperreflective foci as an optical coherence tomography biomarker in retinal diseases: characterization and clinical implications. <em>J. Ophthalmol.</em><strong>2021</strong>, 6096017 (2021).
</cite> [<a href="https://doi.org/10.1155/2021/6096017" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8709761/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34956669/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Fragiotta,%20S.%20et%20al.%20Significance%20of%20hyperreflective%20foci%20as%20an%20optical%20coherence%20tomography%20biomarker%20in%20retinal%20diseases:%20characterization%20and%20clinical%20implications.%20J.%20Ophthalmol.2021,%206096017%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR30">
<span class="label">30.</span><cite>Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J. &amp; Maier-Hein, K. H. nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. <em>Nat. Methods</em><strong>18</strong>, 203–211 (2021).
</cite> [<a href="https://doi.org/10.1038/s41592-020-01008-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33288961/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Isensee,%20F.,%20Jaeger,%20P.%20F.,%20Kohl,%20S.%20A.,%20Petersen,%20J.%20&amp;%20Maier-Hein,%20K.%20H.%20nnu-net:%20a%20self-configuring%20method%20for%20deep%20learning-based%20biomedical%20image%20segmentation.%20Nat.%20Methods18,%20203%E2%80%93211%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Selvaraju, R. R. et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. In <em>Proceedings of the IEEE international conference on computer vision</em>, 618–626 (IEEE, 2017).</cite>
</li>
<li id="CR32">
<span class="label">32.</span><cite>De Fauw, J. et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. <em>Nat. Med.</em><strong>24</strong>, 1342–1350 (2018).
</cite> [<a href="https://doi.org/10.1038/s41591-018-0107-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30104768/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?De%20Fauw,%20J.%20et%20al.%20Clinically%20applicable%20deep%20learning%20for%20diagnosis%20and%20referral%20in%20retinal%20disease.%20Nat.%20Med.24,%201342%E2%80%931350%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33">
<span class="label">33.</span><cite>Sambasivan, N. et al. Everyone wants to do the model work, not the data work: data cascades in high-stakes AI. <em>SIGCHI, ACM,</em><a href="https://research.google/pubs/everyone-wants-to-do-the-model-work-not-the-data-work-data-cascades-in-high-stakes-ai/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://research.google/pubs/everyone-wants-to-do-the-model-work-not-the-data-work-data-cascades-in-high-stakes-ai/</a> (2021).</cite>
</li>
<li id="CR34">
<span class="label">34.</span><cite>Heikkilä, M. OpenAI’s hunger for data is coming back to bite it. MIT Technology Review, <a href="https://www.technologyreview.com/2023/04/19/1071789/openais-hunger-for-data-is-coming-back-to-bite-it/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.technologyreview.com/2023/04/19/1071789/openais-hunger-for-data-is-coming-back-to-bite-it/</a> (2023).</cite>
</li>
<li id="CR35">
<span class="label">35.</span><cite>Li, Y. et al. Evaluating object hallucination in large vision-language models. In <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, (EMNLP, Singapore, 2023).</cite>
</li>
<li id="CR36">
<span class="label">36.</span><cite>Liu, H. et al. A survey on hallucination in large vision-language models. Preprint at <a href="https://arxiv.org/abs/2402.00253" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2402.00253</a> (2024).</cite>
</li>
<li id="CR37">
<span class="label">37.</span><cite>Baltrušaitis, T., Ahuja, C. &amp; Morency, L.-P. Multimodal machine learning: a survey and taxonomy. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em><strong>41</strong>, 423–443 (2018).
</cite> [<a href="https://doi.org/10.1109/TPAMI.2018.2798607" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29994351/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Baltru%C5%A1aitis,%20T.,%20Ahuja,%20C.%20&amp;%20Morency,%20L.-P.%20Multimodal%20machine%20learning:%20a%20survey%20and%20taxonomy.%20IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.41,%20423%E2%80%93443%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR38">
<span class="label">38.</span><cite>Flaxman, S. R. et al. Global causes of blindness and distance vision impairment 1990–2020: a systematic review and meta-analysis. <em>Lancet Glob. Health</em><strong>5</strong>, e1221–e1234 (2017).
</cite> [<a href="https://doi.org/10.1016/S2214-109X(17)30393-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29032195/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Flaxman,%20S.%20R.%20et%20al.%20Global%20causes%20of%20blindness%20and%20distance%20vision%20impairment%201990%E2%80%932020:%20a%20systematic%20review%20and%20meta-analysis.%20Lancet%20Glob.%20Health5,%20e1221%E2%80%93e1234%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR39">
<span class="label">39.</span><cite>Abràmoff, M. D., Garvin, M. K. &amp; Sonka, M. Retinal imaging and image analysis. <em>IEEE Rev. Biomed. Eng.</em><strong>3</strong>, 169–208 (2010).
</cite> [<a href="https://doi.org/10.1109/RBME.2010.2084567" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3131209/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22275207/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Abr%C3%A0moff,%20M.%20D.,%20Garvin,%20M.%20K.%20&amp;%20Sonka,%20M.%20Retinal%20imaging%20and%20image%20analysis.%20IEEE%20Rev.%20Biomed.%20Eng.3,%20169%E2%80%93208%20(2010)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Bird, A. C. et al. An international classification and grading system for age-related maculopathy and age-related macular degeneration. <em>Surv. Ophthalmol.</em><strong>39</strong>, 367–374 (1995).
</cite> [<a href="https://doi.org/10.1016/s0039-6257(05)80092-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/7604360/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Bird,%20A.%20C.%20et%20al.%20An%20international%20classification%20and%20grading%20system%20for%20age-related%20maculopathy%20and%20age-related%20macular%20degeneration.%20Surv.%20Ophthalmol.39,%20367%E2%80%93374%20(1995)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR41">
<span class="label">41.</span><cite>Klein, R. et al. Harmonizing the classification of age-related macular degeneration in the three-continent amd consortium. <em>Ophthalmic Epidemiol.</em><strong>21</strong>, 14–23 (2014).
</cite> [<a href="https://doi.org/10.3109/09286586.2013.867512" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4029416/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24467558/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Klein,%20R.%20et%20al.%20Harmonizing%20the%20classification%20of%20age-related%20macular%20degeneration%20in%20the%20three-continent%20amd%20consortium.%20Ophthalmic%20Epidemiol.21,%2014%E2%80%9323%20(2014)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR42">
<span class="label">42.</span><cite>Ferris, F. L. et al. A simplified severity scale for age-related macular degeneration. <em>Arch. Ophthalmol.</em><strong>123</strong>, 1570–1574 (2005).
</cite> [<a href="https://doi.org/10.1001/archopht.123.11.1570" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC1473206/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16286620/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ferris,%20F.%20L.%20et%20al.%20A%20simplified%20severity%20scale%20for%20age-related%20macular%20degeneration.%20Arch.%20Ophthalmol.123,%201570%E2%80%931574%20(2005)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR43">
<span class="label">43.</span><cite>Ferris III, F. L. et al. Clinical classification of age-related macular degeneration. <em>Ophthalmology</em><strong>120</strong>, 844–851 (2013).
</cite> [<a href="https://doi.org/10.1016/j.ophtha.2012.10.036" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11551519/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23332590/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ferris%20III,%20F.%20L.%20et%20al.%20Clinical%20classification%20of%20age-related%20macular%20degeneration.%20Ophthalmology120,%20844%E2%80%93851%20(2013)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR44">
<span class="label">44.</span><cite>Sadda, S. R. et al. Consensus definition for atrophy associated with age-related macular degeneration on oct: classification of atrophy report 3. <em>Ophthalmology</em><strong>125</strong>, 537–548 (2018).
</cite> [<a href="https://doi.org/10.1016/j.ophtha.2017.09.028" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11366072/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29103793/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Sadda,%20S.%20R.%20et%20al.%20Consensus%20definition%20for%20atrophy%20associated%20with%20age-related%20macular%20degeneration%20on%20oct:%20classification%20of%20atrophy%20report%203.%20Ophthalmology125,%20537%E2%80%93548%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR45">
<span class="label">45.</span><cite>Grill, J.-B. et al. Bootstrap your own latent-a new approach to self-supervised learning. <em>Adv. Neural Inf. Process. Syst.</em><strong>33</strong>, 21271–21284 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Grill,%20J.-B.%20et%20al.%20Bootstrap%20your%20own%20latent-a%20new%20approach%20to%20self-supervised%20learning.%20Adv.%20Neural%20Inf.%20Process.%20Syst.33,%2021271%E2%80%9321284%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR46">
<span class="label">46.</span><cite>AI@Meta. Llama 3 model card, <a href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md</a> (2024).</cite>
</li>
<li id="CR47">
<span class="label">47.</span><cite>Chen, Z. et al. Chexagent: towards a foundation model for chest x-ray interpretation. Preprint at <a href="https://arxiv.org/abs/2401.12208" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2401.12208</a> (2024).</cite>
</li>
<li id="CR48">
<span class="label">48.</span><cite>Alayrac, J.-B. et al. Flamingo: a visual language model for few-shot learning. <em>Adv. neural Inf. Process. Syst.</em><strong>35</strong>, 23716–23736 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Alayrac,%20J.-B.%20et%20al.%20Flamingo:%20a%20visual%20language%20model%20for%20few-shot%20learning.%20Adv.%20neural%20Inf.%20Process.%20Syst.35,%2023716%E2%80%9323736%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR49">
<span class="label">49.</span><cite>Lin, W. et al. PMC-CLIP: contrastive language-image pre-training using biomedical documents. In <em>International Conference on Medical Image Computing and Computer-Assisted Intervention</em>, 525–536 (Springer, 2023).</cite>
</li>
<li id="CR50">
<span class="label">50.</span><cite>Zhang, S. et al. Large-scale domain-specific pretraining for biomedical vision-language processing. Preprint at <a href="https://arxiv.org/abs/2303.00915" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2303.00915</a> (2023).</cite>
</li>
<li id="CR51">
<span class="label">51.</span><cite>Deng, Z. et al. Ophglm: an ophthalmology large language-and-vision assistant. <em>Artif. Intell. Med.</em><strong>157</strong>, 103001 (2024).
</cite> [<a href="https://doi.org/10.1016/j.artmed.2024.103001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39490063/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Deng,%20Z.%20et%20al.%20Ophglm:%20an%20ophthalmology%20large%20language-and-vision%20assistant.%20Artif.%20Intell.%20Med.157,%20103001%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR52">
<span class="label">52.</span><cite>Zhang, K. et al. A generalist vision–language foundation model for diverse biomedical tasks. <em>Nat. Med.</em><strong>30</strong>, 3129–3141 (2024).</cite> [<a href="https://doi.org/10.1038/s41591-024-03185-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39112796/" class="usa-link">PubMed</a>]</li>
<li id="CR53">
<span class="label">53.</span><cite>Holland, R. et al. Deep-learning-based clustering of OCT images for biomarker discovery in age-related macular degeneration (PINNACLE study report 4). <em>Ophthalmol. Sci.</em><strong>4</strong>, 100543 (2024).</cite> [<a href="https://doi.org/10.1016/j.xops.2024.100543" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11321288/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39139544/" class="usa-link">PubMed</a>]</li>
<li id="CR54">
<span class="label">54.</span><cite>Stein, D. M. et al. A new quality assessment parameter for optical coherence tomography. <em>Br. J. Ophthalmol.</em><strong>90</strong>, 186–190 (2006).
</cite> [<a href="https://doi.org/10.1136/bjo.2004.059824" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC1860175/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16424531/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Stein,%20D.%20M.%20et%20al.%20A%20new%20quality%20assessment%20parameter%20for%20optical%20coherence%20tomography.%20Br.%20J.%20Ophthalmol.90,%20186%E2%80%93190%20(2006)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR55">
<span class="label">55.</span><cite>Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. <em>Adv. Neural Inf. Process. Syst.</em><strong>35</strong>, 24824–24837 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wei,%20J.%20et%20al.%20Chain-of-thought%20prompting%20elicits%20reasoning%20in%20large%20language%20models.%20Adv.%20Neural%20Inf.%20Process.%20Syst.35,%2024824%E2%80%9324837%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR56">
<span class="label">56.</span><cite>McNemar, Q. Note on the sampling error of the difference between correlated proportions or percentages. <em>Psychometrika</em><strong>12</strong>, 153–157 (1947).
</cite> [<a href="https://doi.org/10.1007/BF02295996" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20254758/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?McNemar,%20Q.%20Note%20on%20the%20sampling%20error%20of%20the%20difference%20between%20correlated%20proportions%20or%20percentages.%20Psychometrika12,%20153%E2%80%93157%20(1947)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR57">
<span class="label">57.</span><cite>Paszke, A. et al. PyTorch: An Imperative Style, High‑Performance Deep Learning Library. In <em>Advances in Neural Information Processing Systems 32</em>, pp. 8024–8035 (NeurIPS, 2019).</cite>
</li>
<li id="CR58">
<span class="label">58.</span><cite>Sutton, J. et al. Developing and validating a multivariable prediction model which predicts progression of intermediate to late age-related macular degeneration-the PINNACLE trial protocol. <em>Eye</em><strong>37</strong>, 1275–1283 (2023).
</cite> [<a href="https://doi.org/10.1038/s41433-022-02097-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9130980/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35614343/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Sutton,%20J.%20et%20al.%20Developing%20and%20validating%20a%20multivariable%20prediction%20model%20which%20predicts%20progression%20of%20intermediate%20to%20late%20age-related%20macular%20degeneration-the%20PINNACLE%20trial%20protocol.%20Eye37,%201275%E2%80%931283%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adsm93_" lang="en" class="supplementary-materials"><h3 class="pmc_sec_title">Supplementary Materials</h3>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12365215/bin/41746_2025_1893_MOESM1_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Information</a><sup> (23.3MB, pdf) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets used and analyzed during the current study are currently being curated and maintained by the Vienna Reading Center on behalf of the PINNACLE consortium. The data will be made available to once the PINNACLE study concludes in 2026. A minimal dataset is available from the corresponding author before 2026<sup><a href="#CR58" class="usa-link" aria-describedby="CR58">58</a></sup> upon reasonable request. Moreover, the code repository includes a minimal dataset that can be used to interpret, verify and extend the research in the article.</p>
<p>The code and guidelines used to create the question-answer pairs, train, and evaluate the models can be found at <a href="https://github.com/RobbieHolland/SpecialistVLMs" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/RobbieHolland/SpecialistVLMs</a>. The code may be used to develop the models can be repurposed for other medical specialties. Moreover, we make all model weights for RetinaVLM-Base and RetinaVLM-Specialist openly available at <a href="https://huggingface.co/RobbieHolland/RetinaVLM" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://huggingface.co/RobbieHolland/RetinaVLM</a>. These are only intended for research purposes related to retinal OCT images.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from NPJ Digital Medicine are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41746-025-01893-8"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41746_2025_Article_1893.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (19.1 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12365215/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12365215/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12365215%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365215/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12365215/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12365215/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40830259/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12365215/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40830259/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12365215/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12365215/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="4M1G9DswUdmjPjhgh9NXuCh1RJ6erne3CZuKd9kQPB3ecn1LxhEc8fcks9AgHMH6">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-574fdcc6.js"></script>
    
    

    </body>
</html>
