
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Integrating non-linear radon transformation for diabetic retinopathy grading - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4A9B78AF2A55305A9B70046FAE276.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370889/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Integrating non-linear radon transformation for diabetic retinopathy grading">
<meta name="citation_author" content="Farida Mohsen">
<meta name="citation_author_institution" content="College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar">
<meta name="citation_author" content="Samir Belhaouari">
<meta name="citation_author_institution" content="College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar">
<meta name="citation_author" content="Zubair Shah">
<meta name="citation_author_institution" content="College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar">
<meta name="citation_publication_date" content="2025 Aug 21">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30706">
<meta name="citation_doi" content="10.1038/s41598-025-14944-7">
<meta name="citation_pmid" content="40841568">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370889/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370889/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370889/pdf/41598_2025_Article_14944.pdf">
<meta name="description" content="Diabetic retinopathy is a serious ocular complication that poses a significant threat to patients’ vision and overall health. Early detection and accurate grading are essential to prevent vision loss. Current automatic grading methods rely heavily ...">
<meta name="og:title" content="Integrating non-linear radon transformation for diabetic retinopathy grading">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Diabetic retinopathy is a serious ocular complication that poses a significant threat to patients’ vision and overall health. Early detection and accurate grading are essential to prevent vision loss. Current automatic grading methods rely heavily ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370889/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12370889">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-14944-7"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_14944.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370889%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12370889/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12370889/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370889/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 21;15:30706. doi: <a href="https://doi.org/10.1038/s41598-025-14944-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-14944-7</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Integrating non-linear radon transformation for diabetic retinopathy grading</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mohsen%20F%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Farida Mohsen</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Farida Mohsen</span></h3>
<div class="p">
<sup>1</sup>College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mohsen%20F%22%5BAuthor%5D" class="usa-link"><span class="name western">Farida Mohsen</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Belhaouari%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Samir Belhaouari</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Samir Belhaouari</span></h3>
<div class="p">
<sup>1</sup>College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Belhaouari%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Samir Belhaouari</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Shah%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Zubair Shah</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Zubair Shah</span></h3>
<div class="p">
<sup>1</sup>College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Shah%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zubair Shah</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Oct 16; Accepted 2025 Aug 4; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12370889  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40841568/" class="usa-link">40841568</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Diabetic retinopathy is a serious ocular complication that poses a significant threat to patients’ vision and overall health. Early detection and accurate grading are essential to prevent vision loss. Current automatic grading methods rely heavily on deep learning applied to retinal fundus images, but the complex, irregular patterns of lesions in these images, which vary in shape and distribution, make it difficult to capture the subtle changes. This study introduces RadFuse, a multi-representation deep learning framework that integrates non-linear RadEx-transformed sinogram images with traditional fundus images to enhance diabetic retinopathy detection and grading. Our RadEx transformation, an optimized non-linear extension of the Radon transform, generates sinogram representations to capture complex retinal lesion patterns. By leveraging both spatial and transformed domain information, RadFuse enriches the feature set available to deep learning models, improving the differentiation of severity levels. We conducted extensive experiments on two benchmark datasets, APTOS-2019 and DDR, using three convolutional neural networks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant improvements over fundus-image-only models across all three CNN architectures and outperformed state-of-the-art methods on both datasets. For severity grading across five stages, RadFuse achieved a quadratic weighted kappa of 93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary classification between healthy and diabetic retinopathy cases, the method reached an accuracy of 99.09%, precision of 98.58%, and recall of 99.64%, surpassing previously established models. These results demonstrate RadFuse’s capacity to capture complex non-linear features, advancing diabetic retinopathy classification and promoting the integration of advanced mathematical transforms in medical image analysis. The source code will be available at <a href="https://github.com/Farida-Ali/RadEx-Transform/tree/main" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/Farida-Ali/RadEx-Transform/tree/main</a>.</p>
<section id="sec1"><h3 class="pmc_sec_title">Supplementary Information</h3>
<p>The online version contains supplementary material available at 10.1038/s41598-025-14944-7.</p></section><section id="kwd-group1" class="kwd-group"><p><strong>Subject terms:</strong> Biomedical engineering, Retinal diseases</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Diabetic retinopathy (DR) is a microvascular complication of diabetes mellitus and a leading cause of vision impairment and blindness among working-age adults worldwide. According to the International Diabetes Federation, the global prevalence of diabetes is projected to rise from 463 million in 2019 to 700 million by 2045, intensifying the burden of DR on healthcare systems<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>. DR is characterized by progressive damage to the retinal microvasculature, starting with microaneurysms and hemorrhages and advancing to proliferative diabetic retinopathy (PDR), where abnormal neovascularization can cause severe vision loss<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>.</p>
<p id="Par3">Early detection of DR is critical, as treatments are more effective before significant damage occurs. However, DR often progresses asymptomatically, and by the time symptoms are noticeable, considerable retinal damage has already taken place<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>. Hyperglycemia in diabetic patients damages retinal blood vessels, leading to leakage and the formation of exudates, hemorrhages, and microaneurysms. The extent and severity of these lesions are used to grade DR<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>. Despite the need for early detection, manual DR grading is labor-intensive, subject to inter-clinician variability, and requires specialized expertise<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>.</p>
<p id="Par4">Automated DR grading using deep learning offers a promising solution to these limitations. Convolutional neural networks (CNNs) have demonstrated significant potential in medical image analysis, particularly for DR detection and classification<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a>,<a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>. However, relying solely on retinal fundus images presents inherent limitations. CNN-based models often face challenges in capturing fine-grained retinal features, especially in early-stage DR, where lesions like microaneurysms and small hemorrhages are subtle and less pronounced<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>. These lesions are difficult to detect due to their small size, irregular shape, and sparse distribution within the retina<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a>–<a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. The complex, non-linear nature of retinal lesions, combined with the spatial variability of their distribution, makes it challenging for CNNs to extract detailed features and establish connections between different lesion regions<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a>–<a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>. Furthermore, adjacent DR stages often exhibit minimal visual differences, leading to difficulties in accurately distinguishing between them<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a>,<a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>.</p>
<p id="Par5">To address these challenges, researchers have explored the integration of advanced image transformations with deep learning models. The Radon transform, which computes projections of an image along various angles, has been used successfully in medical imaging, particularly in computed tomography, for image reconstruction and feature extraction<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a>,<a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>. Its key benefitis the ability to simplify a complex image structure into analyzable projections. By projecting image data along linear paths, the Radon transform can highlight critical structures and edges. For example, incorporating Radon transforms has improved feature extraction in tumor detection by highlighting linear structures and edges<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>. Tavakoli et al.<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup> demonstrated the Radon transform’s effectiveness in enhancing microaneurysm detection in retinal images. Another study by Raaj et al.<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup> applied the Radon transform to mammogram images to classify them into normal, benign, and malignant categories, achieving high performance using hybrid CNN architecture. However, the linear Radon transform struggles to capture non-linear features such as curved edges or irregular textures, typical in pathological conditions. Its linear assumptions may not accurately represent these complex features, potentially leading to missed diagnoses or inaccuracies. To overcome these limitations, we previously introduced the RadEx transformation, a non-linear extension of the Radon transform, designed to capture non-linear and complex subtle features in image data<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. While the RadEx transformation has been applied to chest X-rays for COVID-19 detection, its use in retinal image analysis for DR grading remains unexplored.</p>
<p id="Par6">In this study, we propose RadFuse, a novel multi-representation deep learning approach that integrates RadEx-based sinogram images with original fundus images, creating a robust multi-representation input and providing complementary perspectives for DR detection and severity grading. Although both image types originate from the same fundus image, the RadEx transformation generates a distinct feature space that complements the spatial information from the original images to capture non-linear lesion patterns and distributions that are not readily discernible in the raw images. The rationale behind generating and including RadEx-transformed images as additional input is to capture intricate, non-linear lesion patterns that are often subtle and distributed across the retinal surface. While CNNs are proficient at extracting diverse feature types, the complex, non-linear nature and spatial variability of retinal lesions present challenges for standard architectures when processing retinal disease images alone<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>. The RadEx transformation serves as a secondary representation, enriching the feature set and providing an additional layer of diagnostic information, thereby enhancing the model’s ability to detect subtle and complex lesion patterns that may be overlooked by single-representation models. This dual-representation approach enables the model to leverage complementary features, improving its ability to detect subtle and distributed lesions essential for accurate DR grading.</p>
<p id="Par7">This approach is supported by previous studies<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a>,<a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>, which demonstrated that transformations like the Radon transform emphasize unique features not easily discernible in the original spatial representations. These transformations provide a complementary view that, when combined with the original images, enables a more comprehensive analysis. The RadEx transformation, as a non-linear extension of the Radon transform, further enhances the model’s ability to capture complex, distributed lesion patterns in retinal images, which are critical for accurate DR grading. Incorporating multi-modal inputs or transformed images, therefore, provides complementary information, capturing lesion diversity more effectively and improving diagnostic accuracy<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>.</p>
<p id="Par8">To the best of our knowledge, this is the first study to use a multi-representation approach combining non-linear RadEx—transformed sinogram images with retinal fundus images for DR grading. These sinogram representations highlight non-linear features associated with DR and provide additional information that enriches the deep learning model, significantly boosting CNN performance in detection and grading. The main contributions of this work are:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par9">Innovative multi-representation approach utilizing non-linear RadEx transformation: We optimized and applied the non-linear RadEx transformation to retinal images, generating sinograms as a new representation of the data. These sinograms serve as an additional representation, offering a complementary perspective to traditional retinal images for DR detection and severity grading. This transformation detects non-linear features and microvascular abnormalities associated with different DR severity levels. We then fused the sinogram and fundus images to constitute a multi-representation input, enabling the model to learn complementary features from both representations.</p></li>
<li><p id="Par10">Comprehensive evaluation and validation of the proposed approach on two benchmark datasets, APTOS-2019 and DDR: We evaluated our approach on the Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 dataset, which was released as part of the Kaggle blindness detection challenge<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>, and the DDR dataset<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>, which is the second-largest publicly available dataset for DR grading. Extensive experiments, including comparisons using retinal image-only, RadEx-only, and multi-representation images with three different CNN architectures, were conducted to validate the effectiveness of our proposed approach. Our method demonstrated significant improvements over retinal image-only models and outperformed existing state-of-the-art (SOTA) methods on both datasets.</p></li>
</ul>
<p id="Par11">Our findings indicate that integrating non-linear Radon transformations provides an effective means of capturing complex non-linear features in retinal images, leading to accurate DR severity grading. This work advances the current state of DR classification and opens up new possibilities for integrating advanced mathematical transformations into medical image analysis pipelines.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Methods</h2>
<p id="Par12">This section outlines our proposed multi-representation deep-learning approach for DR detection and severity grading. The approach consists of two primary components: the generation of RadEx-sinogram images and the implementation of multi- representation deep learning.</p>
<section id="Sec3"><h3 class="pmc_sec_title">Proposed approach</h3>
<section id="Sec4"><h4 class="pmc_sec_title">RadEx transformation: theoretical overview</h4>
<p id="Par13">The non-linear RadEx transformation is an advanced extension of the traditional Radon transform, designed specifically to capture complex, non-linear patterns within medical images<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. Unlike the Radon transform, which relies on straight-line projections, the RadEx transformation employs parameterized curves that adapt to non-linear structures in the image. This allows for a more refined extraction of features critical to detecting and grading diabetic retinopathy, such as curvilinear blood vessels and scattered exudates. The RadEx transformation is defined by the following equation:</p>
<table class="disp-formula p" id="Equa"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/7afd9ca24c38/d33e299.gif" loading="lazy" id="d33e299" alt="graphic file with name d33e299.gif"></td></tr></table>
<p>where <em>z</em> is the transformed vertical coordinate, <em>p</em> is the horizontal coordinate, <em>M</em> is the image size (assuming a square image of dimensions <em>M</em> × <em>M</em>), The parameters <em>q</em> and <em>c</em> control the horizontal shift and the curvature of the transformation, respectively.</p>
<p id="Par14">Unlike the classical Radon transform, which relies on straight-line projections, the RadEx transformation parameterizes a family of curves in the plane. Each curve within this family is uniquely defined by two parameters: <em>q</em>, which serves as a horizontal shift parameter, and <em>c</em>, which controls the curvature of the projection trajectory. Specifically, adjusting <em>q</em> translates the curves horizontally across the image plane, allowing the transformation to systematically sample various vertical cross-sections of the image. Meanwhile, the curvature parameter <em>c</em> modulates the shape of these curves: positive <em>c</em> values produce upwardly curved paths, whereas negative <em>c</em> values produce downwardly curved paths, with the magnitude of <em>c</em> determining the degree of curvature. This parameter-driven adaptability allows RadEx trajectors to closely align with the intrinsic non-linear anatomical structures prevalent in different medical images, such as X-rays and retinal images. By varying <em>q</em> and <em>c</em>, RadEx generates projection curves that span a wide range of positions and curvatures, allowing it to effectively trace complex and arched anatomical features such as tortuous blood vessels and irregular lesion borders, which are often poorly captured by linear methods. Projecting image intensities along these non-linear trajectories generates richer and more discriminative sinogram representations (see Fig. <a href="#Fig5" class="usa-link">5</a>) compared to linear projection methods. This enhanced feature representation provided by RadEx is particularly beneficial in improving the diagnostic accuracy of deep learning models in the detection of DR and severity classification.</p>
<figure class="fig xbox font-sm" id="Fig5"><h5 class="obj_head">Fig. 5.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/75b6d400f187/41598_2025_14944_Fig5_HTML.jpg" loading="lazy" id="d33e583" height="936" width="749" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Comparison of Radon and RadEx Sinograms across DR Stages. From left to right: Original color fundus images (Healthy, Mild, Moderate, Severe, PDR); Preprocessed grayscale fundus images; Linear Radon sinograms highlighting straight-line projections; RadEx sinograms generated along parameterized curved trajectories, enhancing visibility of non-linear features.</p></figcaption></figure></section></section><section id="Sec5"><h3 class="pmc_sec_title">Optimization of the RadEx transformation</h3>
<p id="Par15">Although the original RadEx transformation shows potential, it faces limitations when applied to high-resolution retinal fundus images as demonstrated in Fig. <a href="#Fig1" class="usa-link">1</a>. This becomes especially problematic with larger images (e.g., 512 × 512 pixels and above), where significant regions of the image may remain underrepresented due to suboptimal parameter selection.</p>
<figure class="fig xbox font-sm" id="Fig1"><h4 class="obj_head">Fig. 1.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/7299d685c32e/41598_2025_14944_Fig1_HTML.jpg" loading="lazy" id="d33e373" height="308" width="730" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Visualization of pixel coverage by the RadEx transform across different image resolutions. Blank images of identical dimensions are used, with activated pixels representing areas covered by the transform. Black areas indicate uncovered regions.</p></figcaption></figure><p id="Par16">In retinal context, where lesions are distributed unevenly across the retina, the failure to cover these non-uniformly distributed areas poses a limitation to achieving high accuracy. Therefore, an optimized version of RadEx is necessary to ensure full coverage of the retinal image and enhance the detection of subtle, non-linear, and distributed lesions. To ensure full image coverage and enhance feature extraction, we propose an optimized RadEx transformation focused on refining the selection of parameters <em>q</em> and <em>c</em>. This optimization improves the transformation’s ability to cover the entire image uniformly, reduce sparsity, and better capture non-linear retinal structures.</p>
<p id="Par17"><em>Selection of q Values (Horizontal Shift)</em> The parameter <em>q</em> acts as a horizontal shift, determining the location of transfor- mation curves across the image width. Extreme values of <em>q</em> (either too negative or positive) can cause the curves to cluster at the image’s edges, resulting in redundant information and inadequate coverage. To avoid this, we select <em>q</em> values at regular intervals to ensure uniform sampling across the entire image width. The <em>q</em> values are computed as:</p>
<table class="disp-formula p" id="Equb"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/fca5e8479cf5/d33e398.gif" loading="lazy" id="d33e398" alt="graphic file with name d33e398.gif"></td></tr></table>
<p>where Δ<em>q</em> represents the step size, calculated as Δ<em>q</em> = M/m, where <em>M</em> is the image dimension (512 pixels in our case), and <em>m</em> is the number of increments or divisions along the <em>q</em> axis. As shown in Fig. <a href="#Fig2" class="usa-link">2</a> (left), when <em>q</em> values are spaced with increments of approximately 10 pixels (Δ<em>q ≈ </em>10), the resulting curves demonstrate that the entire width of the image is covered uniformly. For comparison, Fig. <a href="#Fig2" class="usa-link">2</a> (right) illustrates the curves when larger increments of approximately 44 pixels are used (Δ<em>q ≈</em> 44), resulting in less frequent coverage across the image width. This comparison reinforces the need for appropriate selection of <em>m</em> to ensure adequate coverage of the image. For this study, we set <em>m </em>= 50, resulting in Δ<em>q</em> = M/50 <em>≈ </em>10. This choice was made after conducting a sensitivity analysis of different step sizes to evaluate the transformation behavior across various settings and select the optimal one that balances model performance and computational cost.</p>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/9a9c85fd3b83/41598_2025_14944_Fig2_HTML.jpg" loading="lazy" id="d33e465" height="329" width="722" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Plots show pixel selections for varying <em>q</em> values from 0 to <em>M</em> at different increments, keeping <em>c</em> constant. The X-axis represents the image width, and the Y-axis represents the image height.</p></figcaption></figure><p id="Par18"><em>Selection of c values (Curvature Control)</em> The parameter <em>c</em> controls the curvature of the transformation curves. Excessively large or small <em>c</em> values cause the curves to quickly reach the image edges, potentially missing important regions. To avoid this, we constrain <em>c</em> within a moderate range (<em>− </em>1 ≤ <em>c</em> ≤ 1) and use gradual increments to achieve a balanced distribution of curves across the image (see Fig. <a href="#Fig3" class="usa-link">3</a>). This ensures more uniform coverage and improves pixel selection granularity, which is crucial for detecting subtle lesions. We reformulate the RadEx transformation to precalculate <em>c</em> for each <em>z</em> using the inverse RadEx equation:</p>
<table class="disp-formula p" id="Equc"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/a9c80916bc2c/d33e495.gif" loading="lazy" id="d33e495" alt="graphic file with name d33e495.gif"></td></tr></table>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/597f73ca750c/41598_2025_14944_Fig3_HTML.jpg" loading="lazy" id="d33e513" height="257" width="722" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Plotting the pixels chosen for changing values of <em>c</em> between − 1 and 1 at different intervals, keeping <em>q</em> constant at zero. The X-axis depicts image pixels in the horizontal direction while the y-axis depicts image pixels in the vertical direction.</p></figcaption></figure><p id="Par19">This reformulation ensures that the transformation spans the full vertical range of the image, capturing essential features across all regions. We conclude that the optimal count of specialized <em>c</em> values should approximate half the image size to achieve nearly 99% coverage, as shown in Fig. <a href="#Fig4" class="usa-link">4</a>, where the count of specialized <em>c</em> values is <em>M</em>/2. Algorithm 1 outlines the steps for applying the transformation and generating RadEx-transformed images (sinograms), which are essential for our multi-representation deep learning model.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/c91c67f9d913/41598_2025_14944_Fig4_HTML.jpg" loading="lazy" id="d33e541" height="334" width="714" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Visualization of pixel coverage by the optimized RadEx transform for different image sizes when the number of <em>c</em> is <em>M</em>/2. Blank images of identical dimensions are used, with activated pixels representing areas covered by the transform. White areas indicate covered regions.</p></figcaption></figure></section><section id="Sec6"><h3 class="pmc_sec_title">Image preprocessing and RadEx sinogram generation</h3>
<p id="Par20">Several preprocessing steps were performed before applying the RadEx transformation to standardize the images and enhance feature visibility. First, we cropped the images to remove the black background, focusing solely on the retinal region. We then employed the Ben-Graham preprocessing method, a widely used technique in retinal image analysis<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>. This method involves multiple stages aimed at standardizing and improving image quality. The images were resized to a consistent resolution to ensure uniformity across the dataset. Following this, pixel intensity values were normalized to standardize brightness and contrast, reducing variability caused by different imaging conditions and equipment. A Gaussian blur was also applied to minimize noise while preserving important retinal structures.</p>
<p id="Par21">After image preprocessing, we generated sinogram representations of the fundus images using our optimized RadEx transformation. For the optimized RadEx transformation, <em>q</em> values are selected uniformly between 0 and <em>M</em>, with increments of <em>M</em>/50. For each <em>q</em>, we use <em>M</em>/2 values of <em>c</em> (e.g., 112 for <em>M</em> = 224) to ensure dense vertical coverage. The resulting sinograms were then normalized to ensure compatibility with the original images during fusion. Figure <a href="#Fig5" class="usa-link">5</a> illustrates both classical Radon and optimized RadEx sinograms for representative images at each DR severity level (Healthy, Mild, Moderate, Severe, PDR). The third column shows traditional linear Radon projections, while the fourth column presents the corresponding RadEx sinograms. By juxtaposing these panels, RadEx-sinograms provide enhanced feature visibility, and the differentiation between DR grades is more pronounced in the RadEx-sinograms, facilitating more accurate classification of severity levels by deep learning models.</p>
<p id="Par22">The integration of these sinograms with the original fundus images forms the basis of the multi-representation (multimodal) input framework. This framework combines the non-linear, lesion-focused representation provided by the RadEx-transformed sinograms with the spatial details inherent in the original images. Together, these inputs enrich the feature space available to deep learning models, enabling better feature extraction. This dual-input approach is particularly valuable for early disease detection, where subtle indicators carry critical diagnostic and prognostic significance.</p>
<figure class="fig xbox font-sm" id="Figa"><h4 class="obj_head">Algorithm 1.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Figa_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/1281bdb8765b/41598_2025_14944_Figa_HTML.jpg" loading="lazy" id="d33e594" height="376" width="677" alt="Algorithm 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Figa/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Optimized RadEx Image Transformation.</p></figcaption></figure></section><section id="Sec7"><h3 class="pmc_sec_title">Multi-representation deep learning framework</h3>
<p id="Par24">We used three CNN architectures to evaluate the efficacy of our multi-representation approach: ResNeXt-50<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>, MobileNetV2<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>, and VGG19<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. ResNeXt-50 is known for its strong feature extraction capabilities through its grouped convolution strategy, while MobileNetV2 is optimized for efficient performance, making it suitable for deployment in resource-constrained environments. VGG19, although deeper and more computationally demanding, has demonstrated high accuracy in various image classification tasks.</p>
<p id="Par250">To maximize the potential of our proposed RadEx transformation, we integrated the sinogram representations with the original fundus images (RadFuse) through an early fusion strategy<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>. In this approach, the RadEx-transformed sinogram and the original image were horizontally concatenated to form a multi-representation input, allowing the CNNs to process both the spatial and transformed-domain features simultaneously (Fig. <a href="#Fig6" class="usa-link">6</a>). This multi-representation framework enables the network to capture complementary information from both domains, thus enhancing feature representation and improving the classification of DR severity. Additionally, we conducted experiments on two benchmark datasets—APTOS-2019 and DDR—to validate the generalizability and robustness of our RadFuse framework. These datasets provide diverse retinal image samples, ensuring that our approach performs consistently across different datasets.</p>
<figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/1a0c6b3acb21/41598_2025_14944_Fig6_HTML.jpg" loading="lazy" id="d33e627" height="348" width="749" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Our proposed multi-representation RadFuse framework for DR detection and grading. The input comprises two images: the original retinal fundus image and the RadEx-sinogram image. These two modalities are concatenated and processed through the CNN architecture. The final output layer utilizes a sigmoid activation function for binary DR detection and a SoftMax activation function for multi-class DR severity grading. This multi-representation approach enables the model to leverage both spatial and RadEx-transformed features, improving the accuracy and robustness of DR detection and severity classification.</p></figcaption></figure></section><section id="Sec8"><h3 class="pmc_sec_title">Time complexity</h3>
<p id="Par26">The time complexity of the optimized RadEx transformation algorithm can be analyzed based on the following factors:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par27">Outer loop over <em>q</em> values<strong>:</strong> The loop runs from 0 to <em>M</em> with a step of M/10 Hence, it runs 10 iterations (since the step size is M/10 and the total range is <em>M</em>).</p></li>
<li><p id="Par28">Inner loop over <em>z</em> values<strong>:</strong> For each value of <em>q</em>, the inner loop runs from M/2 to <em>M</em>, resulting in <em>M/2 </em>iterations.</p></li>
<li><p id="Par29">Computing <em>c</em> values<strong>:</strong> In each iteration of the inner loop, a <em>c</em> value is computed, resulting in M/2 computations for each <em>q </em>value.</p></li>
<li><p id="Par30">Sorting and range operations for <em>c</em> values: Sorting the <em>c</em> list takes <em>O</em>(<em>n</em> log <em>n</em>) time complexity, where <em>n</em> is the size of the list, approximately 10 × M/2 = 5<em> M</em>.</p></li>
<li><p id="Par31">Nested loops over <em>q</em> and <em>cs</em> values: For each <em>q</em> (10 iterations), the loop over <em>cs</em> runs. The length of <em>cs</em> is determined by the number of selected <em>c</em> values, which is approximately M/2.</p></li>
</ul>
<p id="Par32">Thus, the dominant factor in the time complexity is the nested loops over <em>q</em> values and <em>cs</em>, resulting in a time complexity of approximately <em>O</em>(10 × <em>M</em>/2 × <em>M</em>) = <em>O</em>(5<em>M</em><sup>2</sup>). Therefore, the overall time complexity is approximately <em>O</em>(<em>M</em><sup><em>2</em></sup>), considering the significant computational steps involved.</p></section></section><section id="Sec9"><h2 class="pmc_sec_title">Experiments and results</h2>
<section id="Sec10"><h3 class="pmc_sec_title">Datasets</h3>
<p id="Par33">To evaluate the robustness and generalizability of the proposed RadFuse framework, we conducted experiments on two distinct datasets: APTOS-2019 and DDR.</p>
<p id="Par34">APTOS-2019 Dataset<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>: This dataset, released as part of the 2019 Kaggle blindness detection challenge, contains 3,662 high-resolution color fundus images captured with various clinical cameras in controlled lab environments. The images in the dataset are categorized into five stages of DR: no DR (0), mild (1), moderate (2), severe (3), and PDR (4).</p>
<p id="Par35">The dataset is categorized into five stages of DR: no DR (0), mild (1), moderate (2), severe (3), and PDR (4). The images were divided into training (80%), and testing (20%) sets for diagnosing and grading DR stages. As the data is highly unbalanced, we selected 10% of the training set for validation. To align with established literature, we designed the test set to include 20% of each category from the original APTOS-2019 dataset. This approach allowed for direct comparison with previous studies that used a similar evaluation fraction. The same test data was employed across all experiments to ensure consistency in evaluations. The distribution of images for both binary and multi-class classification tasks is detailed in Table <a href="#Tab1" class="usa-link">1</a>.</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>Proportion of images in the training, validation, and test sets for multi-class and binary classification of DR stages.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Classification</th>
<th align="left" colspan="1" rowspan="1">DR stage</th>
<th align="left" colspan="1" rowspan="1">Number of images</th>
<th align="left" colspan="1" rowspan="1">Train</th>
<th align="left" colspan="1" rowspan="1">Validation</th>
<th align="left" colspan="1" rowspan="1">Test</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="5" colspan="1">Multi-class</td>
<td align="left" colspan="1" rowspan="1">No DR</td>
<td align="center" colspan="1" rowspan="1">1805</td>
<td align="center" colspan="1" rowspan="1">1264</td>
<td align="center" colspan="1" rowspan="1">180</td>
<td align="center" colspan="1" rowspan="1">361</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Mild</td>
<td align="center" colspan="1" rowspan="1">370</td>
<td align="center" colspan="1" rowspan="1">259</td>
<td align="center" colspan="1" rowspan="1">37</td>
<td align="center" colspan="1" rowspan="1">74</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Moderate</td>
<td align="center" colspan="1" rowspan="1">999</td>
<td align="center" colspan="1" rowspan="1">701</td>
<td align="center" colspan="1" rowspan="1">99</td>
<td align="center" colspan="1" rowspan="1">199</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Severe</td>
<td align="center" colspan="1" rowspan="1">193</td>
<td align="center" colspan="1" rowspan="1">136</td>
<td align="center" colspan="1" rowspan="1">19</td>
<td align="center" colspan="1" rowspan="1">38</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PDR</td>
<td align="center" colspan="1" rowspan="1">295</td>
<td align="center" colspan="1" rowspan="1">207</td>
<td align="center" colspan="1" rowspan="1">29</td>
<td align="center" colspan="1" rowspan="1">59</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">Binary</td>
<td align="left" colspan="1" rowspan="1">No DR</td>
<td align="center" colspan="1" rowspan="1">1805</td>
<td align="center" colspan="1" rowspan="1">1264</td>
<td align="center" colspan="1" rowspan="1">180</td>
<td align="center" colspan="1" rowspan="1">361</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DR</td>
<td align="center" colspan="1" rowspan="1">1857</td>
<td align="center" colspan="1" rowspan="1">1303</td>
<td align="center" colspan="1" rowspan="1">184</td>
<td align="center" colspan="1" rowspan="1">370</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par36">DDR Dataset<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>: This dataset is the second largest publicly available dataset, comprising 13,673 fundus images divided into 6835 training, 2733 validation, and 4105 test images. Each image is graded into six categories by seven trained graders based on the International Classification of DR. Images of poor quality without clearly visible lesions are labeled as ungradable. Thus, the six levels are: no DR, mild DR, moderate DR, severe DR, PDR, and ungradable. For our experiments, we focused on the five-class DR grading task, excluding ungradable images. Consequently, our final dataset included 6320 training, 2503 validation, and 3759 test images. The distribution of the dataset is imbalanced in that the normal images are more than the DR images. To ensure consistency, we applied the same preprocessing steps and model configurations as with the APTOS-2019 dataset.</p></section><section id="Sec11"><h3 class="pmc_sec_title">Experimental setup</h3>
<p id="Par37">We implemented our proposed strategy using PyTorch 2.1.2 on a Linux operating system, leveraging an Nvidia GeForce RTX 2080 Ti GPU for both training and testing. The AdamW optimizer was used with a learning rate of 1 × 10<sup><em>−</em>4</sup>, which includes weight decay to improve generalization. A batch size of 16 was used for all experiments, and images were resized to 512 × 512 pixels for both the training and testing phases. The models, initialized with ImageNet pre-trained weights, were fine-tuned on the APTOS-2019 and DDR datasets. The training spanned 100 epochs, with early stopping applied after 10 epochs to prevent overfitting. Cross-entropy loss was the loss function, and data augmentation included blurring, flipping (vertical and horizontal), random rotation, sharpening, and adjustments to brightness and contrast. For our multi-representation model, we first concatenate the fundus image and the RadEx-transformed image to create a composite image. Then, augmentations are applied to the composite image.</p></section><section id="Sec12"><h3 class="pmc_sec_title">Evaluation metrics</h3>
<p id="Par38">The imbalanced nature of the datasets presents challenges when using accuracy as a performance metric, as it tends to favor the majority class, often at the expense of the minority class. To overcome this, We prioritize the Quadratic Weighted Kappa (QWK) score as our primary evaluation metric to overcome this. QWK is designed to measure inter-rater agreement in multi-class classification by comparing expected and predicted scores, with values ranging from − 1 (indicating complete disagreement) to 1 (indicating perfect agreement). A higher QWK score reflects stronger model performance and agreement with true labels. In addition to QWK, we report other key metrics such as Area Under the Curve (AUC), F1-score, recall, precision, accuracy, and Matthews correlation coefficient (MCC), ensuring a comprehensive evaluation of the model’s performance, particularly in handling imbalanced data.</p></section><section id="Sec13"><h3 class="pmc_sec_title">Comparative experiments</h3>
<p id="Par39">This section evaluates the effectiveness of our RadFuse approach on both the APTOS-2019 and DDR datasets on the five-class severity grading, a clinically relevant and more challenging task. We used three CNN architectures—ResNeXt-50, MobileNetV2, and VGG19- to ensure the improvements are not architecture-dependent, as described in the Methods section. We first compare our RadFuse results with image-only, and RadEx-only models and then benchmark against several state-of-the-art (SOTA) models on both datasets. All experiments followed the same setup and data augmentation techniques for fair comparison.</p></section><section id="Sec14"><h3 class="pmc_sec_title">Results on APTOS-2019 dataset</h3>
<p id="Par40">For severity grading, Table <a href="#Tab2" class="usa-link">2</a> shows the performance of our RadFuse models compared to the image-only and RadEx-only configurations across multiple metrics. RadFuse consistently showed improved performance across all three architectures: ResNeXt-50, MobileNet, and VGG-19. Notably, the ResNeXt-50-based RadFuse model achieved the highest metrics, with a QWK of 93.24%, outperforming the image-only (90.62%) and RadEx-only (78.78%) models. It also attained an AUC of 96.41%, and an F1-score of 87.17%, significantly outperforming its image-only and RadEx-only counterparts. Moreover, the RadFuse models showed a significant increase in MCC across all architectures compared to image-only and RadEx-only models. For example, the ResNeXt-50 RadFuse model achieved an MCC of 80.50%, an improvement of 5.96 percentage points over the image-only model. This substantial increase indicates that the RadFuse model is more effective in handling class imbalance and accurately detecting underrepresented classes. VGG19 with RadFuse also showed enhanced performance, with the MCC increasing from 76.55%, in the image-only model to 79.4%, thereby improving the DR classification accuracy. MobileNetV2 showed more modest gains but still improved with the RadFuse approach, indicating the consistent benefits of the multi-representation approach.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Comparison of model performance for severity-level grading using different configurations.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Model</th>
<th align="left" colspan="1" rowspan="1">Configuration</th>
<th align="left" colspan="1" rowspan="1">QWK (%)</th>
<th align="left" colspan="1" rowspan="1">AUC</th>
<th align="left" colspan="1" rowspan="1">F1 (%)</th>
<th align="left" colspan="1" rowspan="1">MCC (%)</th>
<th align="left" colspan="1" rowspan="1">Accuracy (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="3" colspan="1">VGG19</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>91.90</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>0.957</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>86.15</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>79.40</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>86.5</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>86.4</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>85.9</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Image-only</td>
<td align="center" colspan="1" rowspan="1">90.80</td>
<td align="center" colspan="1" rowspan="1">0.950</td>
<td align="center" colspan="1" rowspan="1">84.19</td>
<td align="center" colspan="1" rowspan="1">76.55</td>
<td align="center" colspan="1" rowspan="1">84.4</td>
<td align="center" colspan="1" rowspan="1">84.6</td>
<td align="center" colspan="1" rowspan="1">83.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">70.99</td>
<td align="center" colspan="1" rowspan="1">0.876</td>
<td align="center" colspan="1" rowspan="1">75.11</td>
<td align="center" colspan="1" rowspan="1">61.66</td>
<td align="center" colspan="1" rowspan="1">75.12</td>
<td align="center" colspan="1" rowspan="1">75.12</td>
<td align="center" colspan="1" rowspan="1">75.1</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">MobileNetV2</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>90.91</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>0.954</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>86.11</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>78.96</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>86.12</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>86.13</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>86.1</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Image-only</td>
<td align="center" colspan="1" rowspan="1">90.90</td>
<td align="center" colspan="1" rowspan="1">0.954</td>
<td align="center" colspan="1" rowspan="1">83.3</td>
<td align="center" colspan="1" rowspan="1">76.54</td>
<td align="center" colspan="1" rowspan="1">83.25</td>
<td align="center" colspan="1" rowspan="1">83.3</td>
<td align="center" colspan="1" rowspan="1">83.25</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">77.67</td>
<td align="center" colspan="1" rowspan="1">0.887</td>
<td align="center" colspan="1" rowspan="1">76.37</td>
<td align="center" colspan="1" rowspan="1">63.67</td>
<td align="center" colspan="1" rowspan="1">76.4</td>
<td align="center" colspan="1" rowspan="1">76.4</td>
<td align="center" colspan="1" rowspan="1">76.35</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">ResNeXt-50</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>93.24</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>0.964</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>87.18</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>80.5</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>87.07</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>87.2</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>87.17</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Image-only</td>
<td align="center" colspan="1" rowspan="1">90.62</td>
<td align="center" colspan="1" rowspan="1">0.954</td>
<td align="center" colspan="1" rowspan="1">83.67</td>
<td align="center" colspan="1" rowspan="1">74.54</td>
<td align="center" colspan="1" rowspan="1">83.65</td>
<td align="center" colspan="1" rowspan="1">83.7</td>
<td align="center" colspan="1" rowspan="1">83.65</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">78.78</td>
<td align="center" colspan="1" rowspan="1">0.889</td>
<td align="center" colspan="1" rowspan="1">77.7</td>
<td align="center" colspan="1" rowspan="1">65.94</td>
<td align="center" colspan="1" rowspan="1">77.68</td>
<td align="center" colspan="1" rowspan="1">77.7</td>
<td align="center" colspan="1" rowspan="1">77.7</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p51"><p>The best results are in bold. Image-only denotes models trained and tested using only fundus images, RadEx-only uses solely RadEx-transformed images, and RadFuse represents multi-representation models integrating both fundus images and RadEx-transformed images.</p></div></div></section><p id="Par41">RadEx-only models performed moderately well but did not reach the high-performance levels of image-only or RadFuse models, indicating that while RadEx transformation captures valuable features, these are most effective when combined with original images. The improvements across various CNN architectures underscore the reliable advantages of integrating RadEx-transformed sinogram images with traditional fundus images within the RadFuse framework.</p>
<p id="Par42">To further verify the superiority of our results, we compare our best-performing ResNeXt-50-based RadFuse model with SOTA models for DR severity level grading. These include ADCNet<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>, CLS<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>,MIL-ViT<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>, CANet<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>, and Dual-Branch with Augmentation<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>. Moreover, we also included some SOTA generic models such as Swin<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>, ViG*<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>, and MDGNet<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>. All the comparison results are presented in Table <a href="#Tab3" class="usa-link">3</a>. It should be noted, however, that differences in model architectures, methodologies, and experimental setups among these SOTA methods introduce variability in performance, which may limit the direct comparability to our results. Despite these variations, our proposed RadFuse consistently outperforms all these models by a significant margin. It can be seen that our proposed RadFuse outperforms all these models by a large margin. The crucial performance metric, QWK, reflects the superior reliability of our model, with a score of 93.24%, which is higher than the 92.00 reported by MIL-ViT<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>, 89.03 % from<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>, and 90.0 % from CANet<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>, showcasing RadFuse’s consistent ability to grade DR severity. In the APTOS-2019 dataset, recall is an important metric due to the imbalanced distribution of severity levels between DRs. This demonstrates how accurately the model is able to identify true positive cases, especially in minority groups. Our ResNeXt-50-based RadFuse model achieved a recall of 87.2 %, outperforming all other models by at least 5 percentage points, indicating its effectiveness in detecting various DR levels, including underrepresented categories. In addition to the recall, RadFuse achieved the highest accuracy (87.07 %), F1-score (87.17 %), and AUC (0.964), underscoring its robustness in DR severity classification and its ability to balance precision and recall.</p>
<section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Comparison with SOTA DR grading methods on APTOS-2019.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Method</th>
<th align="left" colspan="1" rowspan="1">Accuracy (%)</th>
<th align="left" colspan="1" rowspan="1">F1 (%)</th>
<th align="left" colspan="1" rowspan="1">QWK (%)</th>
<th align="left" colspan="1" rowspan="1">AUC</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Swin<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">81.44</td>
<td align="center" colspan="1" rowspan="1">60.58</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">0.773</td>
<td align="center" colspan="1" rowspan="1">64.43</td>
<td align="center" colspan="1" rowspan="1">59.42</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViG<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">82.81</td>
<td align="center" colspan="1" rowspan="1">66.48</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">0.80</td>
<td align="center" colspan="1" rowspan="1">69.42</td>
<td align="center" colspan="1" rowspan="1">64.52</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MDGNet<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">84.31</td>
<td align="center" colspan="1" rowspan="1">69.69</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">0.723</td>
<td align="center" colspan="1" rowspan="1">67.84</td>
<td align="center" colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CANet<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">83.2</td>
<td align="center" colspan="1" rowspan="1">81.3</td>
<td align="center" colspan="1" rowspan="1">90.0</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DANIL<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">83.8</td>
<td align="center" colspan="1" rowspan="1">67.2</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">GREEN-ResNet50<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">84.4</td>
<td align="center" colspan="1" rowspan="1">83.6</td>
<td align="center" colspan="1" rowspan="1">90.8</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">GREEN-SE-ResNext50<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">85.7</td>
<td align="center" colspan="1" rowspan="1">85.2</td>
<td align="center" colspan="1" rowspan="1">91.2</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ADCNet<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">83.40</td>
<td align="center" colspan="1" rowspan="1">67.02</td>
<td align="center" colspan="1" rowspan="1">74.78</td>
<td align="center" colspan="1" rowspan="1">0.9426</td>
<td align="center" colspan="1" rowspan="1">69.66</td>
<td align="center" colspan="1" rowspan="1">67.70</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CLS<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">84.36</td>
<td align="center" colspan="1" rowspan="1">70.49</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">0.938</td>
<td align="center" colspan="1" rowspan="1">70.49</td>
<td align="center" colspan="1" rowspan="1">70.51</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MIL-ViT<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">85.5</td>
<td align="center" colspan="1" rowspan="1">85.30</td>
<td align="center" colspan="1" rowspan="1">92.00</td>
<td align="center" colspan="1" rowspan="1"><strong>0.979</strong></td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Dual-Branch with Augmentation<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">83.17</td>
<td align="center" colspan="1" rowspan="1">82.64</td>
<td align="center" colspan="1" rowspan="1">89.03</td>
<td align="center" colspan="1" rowspan="1">0.898</td>
<td align="center" colspan="1" rowspan="1">82.66</td>
<td align="center" colspan="1" rowspan="1">83.17</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadFuse (ResNeXt-50)</td>
<td align="center" colspan="1" rowspan="1"><strong>87.07</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>87.18</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>93.24</strong></td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">0.964</span></td>
<td align="center" colspan="1" rowspan="1"><strong>87.17</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>87.2</strong></td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p55"><p>The SOTA results were taken from<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a>,<a href="#CR27" class="usa-link" aria-describedby="CR27">27</a>,<a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>. The best results are in bold. The second best results are underlined.</p></div></div></section><p id="Par43">The lower performance in the Severe and PDR categories can be attributed to the limited number of training samples for these classes in the APTOS-2019 dataset. As highlighted in Table <a href="#Tab1" class="usa-link">1</a>, the dataset contains only 136 training images for the Severe category—the smallest among all categories—which hampers the model’s ability to learn distinctive features for accurate classification. Further analysis of the misclassifications in the figure below reveals that errors of the categories other than the PDR category are identified as neighboring categories. For instance, the MDGNet<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup> discriminated 51% of the Sever category as Moderate and 28% of the Mild category as Moderate. The reason for this may be that the difference between the DR images of the neighboring categories is very small, which leads to the misidentification of all the models. This confusion is understandable, given that the clinical differences between adjacent DR stages can be minimal and difficult to discern, even for experienced clinicians. Compared to the models discussed in the literature, our method demonstrates superior performance in recognizing the Severe category. While MDGNet<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup> achieved only 38% correct identification for Severe cases, our model correctly classifies 64%, indicating a significant improvement despite the small training samples for this category. This enhancement could be attributed to the inclusion of the non-linear Radex-based sinograms, which amplify critical features associated with advanced DR stages.</p>
<p id="Par44"><em>Stage-Specific Performance Metrics and Misclassification Rates on APTOS-2019</em> To provide a clear stage-wise error analysis, we reported the F1 score, recall true positive rate, and precision for each DR severity level in Table <a href="#Tab4" class="usa-link">4</a> and Fig. <a href="#Fig7" class="usa-link">7</a>. The Normal category achieves an outstanding true positive rate of 99.0%, ensuring healthy patients are rarely misdiagnosed with DR—a critical aspect for reliable screening. The Moderate category also performs strongly, with an 88.02% true positive rate, which is essential for timely intervention. However, we observe elevated false negative rates in the Mild and Severe categories. Specifically, only 71.67% of Mild cases are true positive, with 23.33% of cases misclassified as Moderate. The Severe category’s true positive rate drops to 63.64%, with 30.30% misclassified as Moderate. The PDR stage exhibits the lowest true positive rate at 57.14%, including 28.57% misclassified as Moderate and 14.29% misclassified as Severe. These misclassification patterns highlight the challenge of distinguishing adjacent DR grades. This behavior aligns with prior work by CLS<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup> and MDGNet<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>, which similarly reported difficulties in Severe and PDR classification (see Fig. <a href="#Fig7" class="usa-link">7</a>a, b).</p>
<section class="tw xbox font-sm" id="Tab4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>ResNeXt-50-based RadFuse Class-wise performance metrics on Aptos-2019 dataset.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Class</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">F1 Score (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="center" colspan="1" rowspan="1">99.0</td>
<td align="center" colspan="1" rowspan="1">98.67</td>
<td align="center" colspan="1" rowspan="1">98.83</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Mild</td>
<td align="center" colspan="1" rowspan="1">71.67</td>
<td align="center" colspan="1" rowspan="1">75.44</td>
<td align="center" colspan="1" rowspan="1">73.50</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Moderate</td>
<td align="center" colspan="1" rowspan="1">88.02</td>
<td align="center" colspan="1" rowspan="1">79.46</td>
<td align="center" colspan="1" rowspan="1">83.52</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Severe</td>
<td align="center" colspan="1" rowspan="1">63.64</td>
<td align="center" colspan="1" rowspan="1">60.00</td>
<td align="center" colspan="1" rowspan="1">61.76</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PDR</td>
<td align="center" colspan="1" rowspan="1">57.14</td>
<td align="center" colspan="1" rowspan="1">90.32</td>
<td align="center" colspan="1" rowspan="1">70.0</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/534bb9c4eb64/41598_2025_14944_Fig7_HTML.jpg" loading="lazy" id="d33e1598" height="198" width="729" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Confusion matrices of three models on the APTOS-2019 dataset.</p></figcaption></figure><p id="Par45">In addition to the multi-class severity grading, we evaluated the performance of our models on the binary classification task of distinguishing between DR and healthy cases. This task is critical for screening purposes, where the primary goal is to identify patients who require further ophthalmic examination. We trained and tested the same three CNN architectures—MobileNetV2, VGG19, and ResNeXt-50—on a re-labeled version of the dataset for binary classification, differentiating between healthy (Grade 0) and DR (Grades 1–4). The key performance metrics of our RadFuse-based approach are presented in Table <a href="#Tab5" class="usa-link">5</a>. As shown, RadFuse outperformed both image-only and RadEx-only models across all three CNNs. ResNeXt-50-based RadFuse achieved the best overall performance, with the highest QWK (98.17%), accuracy (99.09%) and F1-score (99.11%), indicating superior classification capability. MobileNetV2 and VGG19 also showed strong discriminative ability, with AUC values exceeding 0.996 and QWK scores over 0.97. The confusion matrices (Fig. <a href="#Fig8" class="usa-link">8</a>) offer insights into the classification outcomes. ResNeXt-50 demonstrated the fewest misclassifications, incorrectly predicting one DR case as healthy, and misclassifying four healthy cases as DR. VGG19 and MobileNetV2 had slightly higher error rates, with six and eight misclassifications, respectively. However, all models tended to misclassify healthy cases as DR, a trend that is acceptable in screening contexts, where minimizing missed diagnoses is crucial. This analysis further emphasizes that the models prioritize DR detection, which is essential in clinical screening to prevent undiagnosed DR progression. For a comprehensive evaluation, we also compared our best ResNeXt-50-based RadFuse model against recent SOTA methods for binary classification. As shown in Table <a href="#Tab6" class="usa-link">6</a>, our approach outperformed all referenced methods, achieving the highest accuracy (99.09%) and recall (99.64%). Additionally, the high precision (98.58%) indicates a lower false-positive rate compared to other approaches, underscoring its effectiveness in screening applications.</p>
<section class="tw xbox font-sm" id="Tab5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>Detailed performance metrics of the proposed models for APTOS dataset binary classification.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Model</th>
<th align="left" colspan="1" rowspan="1">Configuration</th>
<th align="left" colspan="1" rowspan="1">Accuracy%</th>
<th align="left" colspan="1" rowspan="1">F1-Score%</th>
<th align="left" colspan="1" rowspan="1">MCC%</th>
<th align="left" colspan="1" rowspan="1">QWK%</th>
<th align="left" colspan="1" rowspan="1">Precision%</th>
<th align="left" colspan="1" rowspan="1">Recall %</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="3" colspan="1">MobileNetV2</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>98.54</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>98.57</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>97.08</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>97.08</strong></td>
<td align="center" colspan="1" rowspan="1">98.22</td>
<td align="center" colspan="1" rowspan="1"><strong>98.93</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Image-only</td>
<td align="center" colspan="1" rowspan="1">97.79</td>
<td align="center" colspan="1" rowspan="1">97.63</td>
<td align="center" colspan="1" rowspan="1">95.60</td>
<td align="center" colspan="1" rowspan="1">95.59</td>
<td align="center" colspan="1" rowspan="1"><strong>97.7</strong></td>
<td align="center" colspan="1" rowspan="1">97.57</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">94.49</td>
<td align="center" colspan="1" rowspan="1">94.49</td>
<td align="center" colspan="1" rowspan="1">88.99</td>
<td align="center" colspan="1" rowspan="1">88.97</td>
<td align="center" colspan="1" rowspan="1">94.49</td>
<td align="center" colspan="1" rowspan="1">94.49</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">VGG19</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>98.91</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>98.92</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>97.81</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>97.81</strong></td>
<td align="center" colspan="1" rowspan="1">98.57</td>
<td align="center" colspan="1" rowspan="1"><strong>99.28</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Image-only</td>
<td align="center" colspan="1" rowspan="1">97.24</td>
<td align="center" colspan="1" rowspan="1">97.24</td>
<td align="center" colspan="1" rowspan="1">94.51</td>
<td align="center" colspan="1" rowspan="1">94.48</td>
<td align="center" colspan="1" rowspan="1">97.24</td>
<td align="center" colspan="1" rowspan="1">97.24</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">94.85</td>
<td align="center" colspan="1" rowspan="1">94.85</td>
<td align="center" colspan="1" rowspan="1">89.76</td>
<td align="center" colspan="1" rowspan="1">89.70</td>
<td align="center" colspan="1" rowspan="1"><strong>94.85</strong></td>
<td align="center" colspan="1" rowspan="1">94.85</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">ResNeXt-50</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>99.09</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>99.11</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>98.18</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>98.17</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>98.58</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>99.64</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Image-only</td>
<td align="center" colspan="1" rowspan="1">97.43</td>
<td align="center" colspan="1" rowspan="1">97.43</td>
<td align="center" colspan="1" rowspan="1">94.85</td>
<td align="center" colspan="1" rowspan="1">94.85</td>
<td align="center" colspan="1" rowspan="1">97.43</td>
<td align="center" colspan="1" rowspan="1">97.43</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">95.59</td>
<td align="center" colspan="1" rowspan="1">95.59</td>
<td align="center" colspan="1" rowspan="1">91.17</td>
<td align="center" colspan="1" rowspan="1">91.17</td>
<td align="center" colspan="1" rowspan="1">95.59</td>
<td align="center" colspan="1" rowspan="1">95.59</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p62"><p>Best results are highlighted in bold.</p></div></div></section><figure class="fig xbox font-sm" id="Fig8"><h4 class="obj_head">Fig. 8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig8_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/900885e22047/41598_2025_14944_Fig8_HTML.jpg" loading="lazy" id="d33e1828" height="195" width="728" alt="Fig. 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Binary classification confusion matrices for our three different models on the APTOS-2019 dataset.</p></figcaption></figure><section class="tw xbox font-sm" id="Tab6"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>Comparison with SOTA Methods for Binary Classification. Results were taken from<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Reference</th>
<th align="left" colspan="1" rowspan="1">Accuracy (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Blended features + DNN<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">96.10</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Hybrid model (VGG16 + Capsule network)<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">97.05</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="center" colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CLS<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">98.36</td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">98.36</span></td>
<td align="center" colspan="1" rowspan="1">98.37</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Dual-Branch with Augmentation<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>
</td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">98.50</span></td>
<td align="center" colspan="1" rowspan="1">97.61</td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">99.46</span></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ResNeXt-50 with RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>99.09</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>98.58</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>99.64</strong></td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p65"><p>Best results are highlighted in bold, and the second-best are underlined.</p></div></div></section></section><section id="Sec15"><h3 class="pmc_sec_title">Results on DDR dataset</h3>
<p id="Par46">To substantiate the generalizability of our RadFuse framework beyond the APTOS-2019 dataset, we conducted additional experiments using the DDR benchmark dataset. Table <a href="#Tab7" class="usa-link">7</a> presents the performance metrics of our RadFuse models compared to Image-only and RadEx-only configurations across three CNN architectures: ResNeXt-50, MobileNet, and VGG-19. With the ResNeXt-50 architecture, RadFuse achieved a QWK score of 85.50%, outperforming the image-only model’s 81.00% by 4.50 percentage points. Key metrics including MCC, F1 Score, and AUC also improved, with the AUC rising from 0.921 to 0.948. For the VGG19 architecture, RadFuse demonstrated even substantial gains, achieving a QWK score of 84.82% and an AUC increase from 0.903 to 0.925. This trend continued with MobileNetV2, where RadFuse outperformed the image-only model in QWK (78.90% vs. 76.66%) and AUC (0.916 vs. 0.893), underscoring the effectiveness of integrating RadEx-transformedimages with original fundus images within the RadFuse framework across various CNN architectures.</p>
<section class="tw xbox font-sm" id="Tab7"><h4 class="obj_head">Table 7.</h4>
<div class="caption p"><p>Performance Metrics on the DDR Dataset. </p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Model</th>
<th align="left" colspan="1" rowspan="1">Configuration</th>
<th align="left" colspan="1" rowspan="1">QWK%</th>
<th align="left" colspan="1" rowspan="1">MCC %</th>
<th align="left" colspan="1" rowspan="1">F1 Score%</th>
<th align="left" colspan="1" rowspan="1">AUC</th>
<th align="left" colspan="1" rowspan="1">Accuracy%</th>
<th align="left" colspan="1" rowspan="1">Sensitivity</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="3" colspan="1">MobileNetV2</td>
<td align="left" colspan="1" rowspan="1">Image-only</td>
<td align="center" colspan="1" rowspan="1">76.66</td>
<td align="center" colspan="1" rowspan="1">61.62</td>
<td align="center" colspan="1" rowspan="1">77.12</td>
<td align="center" colspan="1" rowspan="1">0.893</td>
<td align="center" colspan="1" rowspan="1">77.12</td>
<td align="center" colspan="1" rowspan="1">77.12</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">43.55</td>
<td align="center" colspan="1" rowspan="1">24.64</td>
<td align="center" colspan="1" rowspan="1">57.73</td>
<td align="center" colspan="1" rowspan="1">0.743</td>
<td align="center" colspan="1" rowspan="1">57.73</td>
<td align="center" colspan="1" rowspan="1">57.73</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>78.90</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>65.90</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>79.30</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>0.915</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>79.3</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>79.3</strong></td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">VGG-19</td>
<td align="left" colspan="1" rowspan="1">Image-only</td>
<td align="center" colspan="1" rowspan="1">76.74</td>
<td align="center" colspan="1" rowspan="1">60.23</td>
<td align="center" colspan="1" rowspan="1">76.14</td>
<td align="center" colspan="1" rowspan="1">0.903</td>
<td align="center" colspan="1" rowspan="1">76.14</td>
<td align="center" colspan="1" rowspan="1">76.14</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">56.56</td>
<td align="center" colspan="1" rowspan="1">36.55</td>
<td align="center" colspan="1" rowspan="1">62.84</td>
<td align="center" colspan="1" rowspan="1">0.773</td>
<td align="center" colspan="1" rowspan="1">62.84</td>
<td align="center" colspan="1" rowspan="1">62.84</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>84.82</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>71.16</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>82.36</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>0.925</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>82.36</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>82.36</strong></td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">ResNeXt-50</td>
<td align="left" colspan="1" rowspan="1">Image-only</td>
<td align="center" colspan="1" rowspan="1">81.00</td>
<td align="center" colspan="1" rowspan="1">69.67</td>
<td align="center" colspan="1" rowspan="1">81.51</td>
<td align="center" colspan="1" rowspan="1">0.921</td>
<td align="center" colspan="1" rowspan="1">81.50</td>
<td align="center" colspan="1" rowspan="1">81.50</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">54.18</td>
<td align="center" colspan="1" rowspan="1">39.55</td>
<td align="center" colspan="1" rowspan="1">65.07</td>
<td align="center" colspan="1" rowspan="1">0.776</td>
<td align="center" colspan="1" rowspan="1">65.00</td>
<td align="center" colspan="1" rowspan="1">65.0</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>85.50</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>72.72</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>83.32</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>0.948</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>83.32</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>83.32</strong></td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p68"><p>Comparison of model performance for severity-level grading using different configurations. Best results are in bold. “Image-only” indicates models trained and tested using fundus images only, "RadEx-only" uses only RadEx-transformed images, and “RadFuse” indicates multi-representation models integrating both fundus images and RadEx-based sinograms.</p></div></div></section><p id="Par47">To further validate the efficacy of our proposed RadFuse framework, we compared our best-performing ResNeXt-50- based RadFuse model with several SOTA models for five-class DR severity level grading. Table <a href="#Tab8" class="usa-link">8</a> presents a comprehensive comparison. Our RadFuse model achieved an accuracy of 83.32% and a superior QWK of 85.50%, positioning it as the second-highest performer in accuracy and the top performer in QWK among the compared models. Compared to attention-based models like CABNet<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup> and FA-Net<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>, which achieved QWK scores of 78.63% and 82.68%, respectively, RadFuse not only matches but exceeds their performance, underscoring its reliability across all DR classes. These results suggest that while attention mechanisms contribute significantly to focusing on specific lesion types and enhancing grading performance data, the integration of RadEx-transformed features in RadFuse provides an additional layer of diagnostic information, further enhancing its discriminative capability. ViT + CSRA<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup> achieves an accuracy of 82.35% but lacks the QWK metric, suggesting that while Vision Transformers are effective for DR grading, they may not capture fine lesion details as effectively as Radon-transformed representations or specialized attention models. In multimodal and hybrid models, strategies like DeepMT-DR<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup> and FA + KC- Net + R1<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup> highlight the strengths of multitask and multimodal learning. DeepMT-DR achieved 83.60% accuracy and 80.20% QWK, illustrating the value of multitask learning in DR grading, while FA + KC-Net + R1 slightly outperformed with 83.96% accuracy and 84.76% QWK, showing the benefits of combining fine-grained attention with knowledge-based methods. Notably, RadFuse matches or slightly surpasses these complex models despite its relative simplicity, suggesting that non-linear Radon transformations as an added data representation enable effective capture of subtle lesion features similar to intricate multimodal systems.</p>
<section class="tw xbox font-sm" id="Tab8"><h4 class="obj_head">Table 8.</h4>
<div class="caption p"><p>Performance comparison on five-class DR grading on the DDR dataset of our RadFuse model against several SOTA models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Model</th>
<th align="left" colspan="1" rowspan="1">Accuracy%</th>
<th align="left" colspan="1" rowspan="1">QWK%</th>
<th align="left" colspan="1" rowspan="1">Remarks</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">ResNet50<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">75.57</td>
<td align="center" colspan="1" rowspan="1">74.27</td>
<td align="left" colspan="1" rowspan="1">Standard CNN baseline without attention mechanisms</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DenseNet-121<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">76.69</td>
<td align="center" colspan="1" rowspan="1">74.38</td>
<td align="left" colspan="1" rowspan="1">Standard CNN baseline without attention mechanisms</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CABNet<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">78.98</td>
<td align="center" colspan="1" rowspan="1">78.63</td>
<td align="left" colspan="1" rowspan="1">Utilizes Category Attention Block (CAB) for imbalance handling</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeepMT-DR<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">83.60</td>
<td align="center" colspan="1" rowspan="1">80.20</td>
<td align="left" colspan="1" rowspan="1">Employs a multitask approach to improve DR grading</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT + CSRA<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">82.35</td>
<td align="center" colspan="1" rowspan="1">–</td>
<td align="left" colspan="1" rowspan="1">Combines Vision Transformer with Class-Specific Representation Align- ment (CSRA) for feature learning</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">FA-Net<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>
</td>
<td align="center" colspan="1" rowspan="1">82.10</td>
<td align="center" colspan="1" rowspan="1">82.68</td>
<td align="left" colspan="1" rowspan="1">Fine-grained attention modules, strong performance on five-class grading</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">FA + KC-Net + R1<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>
</td>
<td align="center" colspan="1" rowspan="1"><strong>83.96</strong></td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">84.76</span></td>
<td align="left" colspan="1" rowspan="1">Fusion of fine-grained attention and knowledge-based network</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadFuse (ResNeXt-50)</td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">83.32</span></td>
<td align="center" colspan="1" rowspan="1"><strong>85.50</strong></td>
<td align="left" colspan="1" rowspan="1">RadFuse with non-linear Radon transformations for enhanced lesion focus</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p71"><p>Benchmark results are taken from<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup> and<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>. Best results are in bold, and second-best results are underlined.</p></div></div></section></section><section id="Sec16"><h3 class="pmc_sec_title">Stage-specific performance metrics and misclassification rates on DDR dataset</h3>
<p id="Par48">To extend our stage-wise performance and error analysis, we display the full five-class confusion matrices for the image-only and RadFuse setups across all three architectures in Fig. <a href="#Fig9" class="usa-link">9</a>. Notably, the reduction in misclassification rates for severe and PDR stages underscores the efficiency of RadFuse for enhancing their detection, as accurate detection of these stages is critical for timely intervention and prevention of vision loss. A detailed analysis of the confusion matrices for the ResNeXt-50 backbone shows that the image-only model exhibits a false negative rate of 70.42% for Severe (50/71 cases misclassified) and 22.55% for PDR (62/275 cases misclassified). In contrast, RadFuse reduces these false negatives to 52.11% for Severe (37/71) and 16.36% for PDR (45/275). This represents an 18.31% reduction in Severe grade misclassification and a 6.19% reduction in PDR grade misclassification, respectively. Similarly, the MobileNet architecture demonstrated a reduction in misclassification rates from 66.20% (47 out of 71 the severe cases misclassified) in the image-only model to 52.11% (37 out of 71 the severe cases misclassified) in the RadFuse model for the severe grade, and from 31.70% (87 out of 275 PDR cases misclassified) to 16.36% (45 out of 275 PDR cases misclassified) for PDR grade. For the VGG-19 architecture, the image-only model showed a severe grade misclassification rate of 57.75% (41 out of 71 cases misclassified), which was not reduced in the RadFuse model.</p>
<figure class="fig xbox font-sm" id="Fig9"><h4 class="obj_head">Fig. 9.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig9_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/63d78dac64d1/41598_2025_14944_Fig9_HTML.jpg" loading="lazy" id="d33e2328" height="991" width="716" alt="Fig. 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Confusion matrices for severity grading for the Image-only and RadFuse models on the DDR dataset.</p></figcaption></figure><p id="Par49">Although this indicates a slight increase in DR3 misclassification for VGG-19, the overall performance metrics, including QWK and MCC, improved significantly, suggesting enhanced performance in other classes that compensate for this increase. The reduction in misclassification rates for these grades highlights RadFuse’s efficacy in capturing complex, non-linear lesion patterns. The per-class metrics in Table <a href="#Tab9" class="usa-link">9</a> further illustrate RadFuse’s advantages. RadFuse achieves an AUC of 0.973 for No DR (vs. 0.960 image-only) and substantially improves AUC for Mild (0.904 vs. 0.806), Moderate (0.921 vs. 0.909), Severe (0.958 vs. 0.857), and PDR (0.987 vs. 0.968). In terms of true positive rate, RadFuse boosts Mild from 77.12 to 82.36%, Moderate from 74.40 to 82.36%, Severe from 29.58 to 47.89%, and PDR from 22.55 to 23.00%. While the PDR gain is modest, the pronounced improvement in Severe detection underscores RadFuse’s clinical value. Additionally, F1 score and precision metrics consistently improve across all classes, reflecting a balanced enhancement in sensitivity and specificity.</p>
<section class="tw xbox font-sm" id="Tab9"><h4 class="obj_head">Table 9.</h4>
<div class="caption p"><p>Class-wise performance metrics on the DDR dataset for Image-only and RadFuse models (ResNeXt-50 architecture).</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">Class</th>
<th align="left" colspan="4" rowspan="1">Image-only</th>
<th align="left" colspan="4" rowspan="1">RadFuse</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">Recall%</th>
<th align="left" colspan="1" rowspan="1">Precision%</th>
<th align="left" colspan="1" rowspan="1">F1Score %</th>
<th align="left" colspan="1" rowspan="1">AUC</th>
<th align="left" colspan="1" rowspan="1">Recall%</th>
<th align="left" colspan="1" rowspan="1">Precision%</th>
<th align="left" colspan="1" rowspan="1">F1 Score%</th>
<th align="left" colspan="1" rowspan="1">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="center" colspan="1" rowspan="1">98.29</td>
<td align="center" colspan="1" rowspan="1">84.54</td>
<td align="center" colspan="1" rowspan="1">90.90</td>
<td align="center" colspan="1" rowspan="1">0.960</td>
<td align="center" colspan="1" rowspan="1"><strong>98.35</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>85.96</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>91.74</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>0.973</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Mild</td>
<td align="center" colspan="1" rowspan="1">21.16</td>
<td align="center" colspan="1" rowspan="1">32.52</td>
<td align="center" colspan="1" rowspan="1">25.64</td>
<td align="center" colspan="1" rowspan="1">0.806</td>
<td align="center" colspan="1" rowspan="1"><strong>28.57</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>41.22</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>33.75</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>0.904</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Moderate</td>
<td align="center" colspan="1" rowspan="1"><strong>74.40</strong></td>
<td align="center" colspan="1" rowspan="1">84.89</td>
<td align="center" colspan="1" rowspan="1"><strong>79.30</strong></td>
<td align="center" colspan="1" rowspan="1">0.909</td>
<td align="center" colspan="1" rowspan="1">71.80</td>
<td align="center" colspan="1" rowspan="1"><strong>85.55</strong></td>
<td align="center" colspan="1" rowspan="1">78.07</td>
<td align="center" colspan="1" rowspan="1"><strong>0.921</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Severe</td>
<td align="center" colspan="1" rowspan="1">29.58</td>
<td align="center" colspan="1" rowspan="1"><strong>61.76</strong></td>
<td align="center" colspan="1" rowspan="1">40.00</td>
<td align="center" colspan="1" rowspan="1">0.857</td>
<td align="center" colspan="1" rowspan="1"><strong>47.89</strong></td>
<td align="center" colspan="1" rowspan="1">50.00</td>
<td align="center" colspan="1" rowspan="1"><strong>48.92</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>0.958</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PDR</td>
<td align="center" colspan="1" rowspan="1">77.45</td>
<td align="center" colspan="1" rowspan="1"><strong>89.50</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>83.04</strong></td>
<td align="center" colspan="1" rowspan="1">0.968</td>
<td align="center" colspan="1" rowspan="1"><strong>83.64</strong></td>
<td align="center" colspan="1" rowspan="1">81.85</td>
<td align="center" colspan="1" rowspan="1">82.73</td>
<td align="center" colspan="1" rowspan="1"><strong>0.986</strong></td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p76"><p>Best results are highlighted in bold, and the second-best are underlined.</p></div></div></section></section><section id="Sec17"><h3 class="pmc_sec_title">Sensitivity analysis of augmentation strategies</h3>
<p id="Par50">We evaluated the effect of data augmentation strategies on the performance of RadFuse using the DDR dataset and the ResNeXt-50 architecture. Two approaches were compared: (1) joint augmentation of pre-generated RadEx-transformed images concatenated with fundus images and (2) dynamic recalculation of RadEx for each augmented version of the fundus images during training.</p>
<p id="Par51">The results, detailed in Supplementary Table <a href="#MOESM1" class="usa-link">S1</a>, demonstrate that RadFuse consistently outperformed both Image-only and RadEx-only models across both strategies, underscoring the value of integrating RadEx-transformed images with fundus images. Notably, recalculating RadEx for each augmentation improved performance, with QWK increasing from 85.0 to 86.0% and the F1 score rising from 83.0 to 85.0%. However, this improvement came at a significant computational cost, with training time per epoch increasing approximately 20-fold, from 3 min with joint augmentation to over an hour with recalculated augmentation. Considering this trade-off, we adopted the joint augmentation strategy for its computational efficiency. Future work will focus on optimizing the RadEx calculation process to reduce its computational demands during dynamic image augmentation while further enhancing its performance benefits.</p>
<section id="Sec18"><h4 class="pmc_sec_title">Sensitivity analysis of RadEx transformation’s parameters</h4>
<p id="Par52">We conducted a sensitivity analysis to determine optimal values for the RadEx transformation parameters Δq and c. To evaluate the impact of step size Δq on model performance, we assessed three step sizes: Δq = M/100, Δq = M/50, and Δq = M/25, where M = 512 pixels represents the image dimension (for a 512 × 512 pixel image). The number of curvature parameters c was fixed at M. Table <a href="#Tab10" class="usa-link">10</a> summarizes the performance of the RadFuse and RadEx-only models at different Δq values on the APTOS-2019 dataset. Results show that performance differences between step sizes Δq = M/50 and Δq = M/100 are minimal for both RadFuse and RadEx-only models. Specifically, for RadFuse, QWK scores range from 91.41 to 93.10%, MCC from 79.53 to 80.86%, and AUC from 0.955 to 0.964 across these step sizes. Similarly, RadEx-only exhibits QWK scores between 72.18% and 78.78%, MCC from 61.99% to 66.06%, and AUC from 88.38% to 88.88%. These minor variations suggest that both models are relatively robust within the tested Δq range.</p>
<section class="tw xbox font-sm" id="Tab10"><h5 class="obj_head">Table 10.</h5>
<div class="caption p"><p>Performance comparison of RadFuse and RadEx-only models across different step sizes Δ<em>q.</em></p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Step Size Δ<em>q</em>
</th>
<th align="left" colspan="1" rowspan="1">Model</th>
<th align="left" colspan="1" rowspan="1">QWK%</th>
<th align="left" colspan="1" rowspan="1">MCC %</th>
<th align="left" colspan="1" rowspan="1">F1%</th>
<th align="left" colspan="1" rowspan="1">Accuracy%</th>
<th align="left" colspan="1" rowspan="1">AUC</th>
<th align="left" colspan="1" rowspan="1">Recall %</th>
<th align="left" colspan="1" rowspan="1">Precision%</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="2" colspan="1">
<em>M/</em>25</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">91.51</span></td>
<td align="center" colspan="1" rowspan="1">79.53</td>
<td align="center" colspan="1" rowspan="1">86.36</td>
<td align="center" colspan="1" rowspan="1">86.36</td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">0.957</span></td>
<td align="center" colspan="1" rowspan="1">86.36</td>
<td align="center" colspan="1" rowspan="1">86.36</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">72.18</td>
<td align="center" colspan="1" rowspan="1">61.99</td>
<td align="center" colspan="1" rowspan="1">75.44</td>
<td align="center" colspan="1" rowspan="1">75.44</td>
<td align="center" colspan="1" rowspan="1">0.883</td>
<td align="center" colspan="1" rowspan="1">75.44</td>
<td align="center" colspan="1" rowspan="1">75.44</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">
<em>M/</em>50</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>93.10</strong></td>
<td align="center" colspan="1" rowspan="1">80.55</td>
<td align="center" colspan="1" rowspan="1">87.16</td>
<td align="center" colspan="1" rowspan="1">87.16</td>
<td align="center" colspan="1" rowspan="1"><strong>0.964</strong></td>
<td align="center" colspan="1" rowspan="1">87.16</td>
<td align="center" colspan="1" rowspan="1">87.16</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">78.78</td>
<td align="center" colspan="1" rowspan="1">65.93</td>
<td align="center" colspan="1" rowspan="1">77.69</td>
<td align="center" colspan="1" rowspan="1">77.69</td>
<td align="center" colspan="1" rowspan="1">0.888</td>
<td align="center" colspan="1" rowspan="1">77.69</td>
<td align="center" colspan="1" rowspan="1">77.69</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">
<em>M/</em>100</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1">91.41</td>
<td align="center" colspan="1" rowspan="1"><strong>80.86</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>87.32</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>87.32</strong></td>
<td align="center" colspan="1" rowspan="1">0.955</td>
<td align="center" colspan="1" rowspan="1"><strong>87.32</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>87.32</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">75.97</td>
<td align="center" colspan="1" rowspan="1">66.06</td>
<td align="center" colspan="1" rowspan="1">77.69</td>
<td align="center" colspan="1" rowspan="1">77.69</td>
<td align="center" colspan="1" rowspan="1">0.884</td>
<td align="center" colspan="1" rowspan="1">77.69</td>
<td align="center" colspan="1" rowspan="1">77.69</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab10/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p81"><p>Best results are highlighted in bold, and the second-best are underlined.</p></div></div></section><p id="Par53">Among the evaluated step sizes, Δ<em>q </em>= M/50 emerged as the optimal choice for both configurations. This setting achieves the best trade-off between performance metrics and computational cost, with RadFuse reaching peak values of QWK (93.10%), MCC (80.55%), and AUC (0.964), while RadEx-only attains its highest QWK and MCC at this step size. Additionally, this</p>
<p id="Par54">configuration shows the lowest misclassification rates for severe and PDR stages (Supplementary Figure <a href="#MOESM1" class="usa-link">S1</a>). Misclassifications between severe and PDR (e.g., severe predicted as PDR and vice versa) remain consistent across Δ<em>q</em> values, indicating that step size mainly affects misclassifications with less severe grades like DR2 (Supplementary Figure <a href="#MOESM1" class="usa-link">S1</a>). Thus, Δ<em>q</em> = M/50 provides balanced performance with minimal misclassification. Overall, the RadEx transformation demonstrates robustness to parameter selection within this range. While slight performance fluctuations are observed, they do not detract from overall efficacy, suggesting that while RadEx benefits from tuning Δ<em>q</em>.</p>
<p id="Par55">The sensitivity analysis on the curvature parameter c also reveals that the RadFuse model performs reliably across different configurations, while careful tuning of c can enhance specific performance metrics. We evaluated three c settings as shown in Table <a href="#Tab11" class="usa-link">11</a> and Fig. <a href="#Fig10" class="usa-link">10</a>, where M = 512 and Δq = M/2. At <em>c</em> = <em>M/2</em>, the RadFuse model achieves an optimal balance of performance and computational efficiency. Recall rates for Severe and PDR stages reach 51.52% and 54.00%, respectively, with the lowest rate of misclassifications among severe stages. This setting effectively captures essential features while avoiding major performance trade-offs, indicating that moderate adjustments to <em>c</em> enhance classification accuracy. Increasing <em>c</em> to M yields further improvements, especially for PDR recall (68.00%), with slight increases in misclassifications for the Severe stage. This suggests that while performance gains are achievable with higher <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/906e271ec85e/d33e2750.gif" loading="lazy" id="d33e2750" alt="Inline graphic"></span> values, the model’s stability remains strong across the tested range, and <em>c</em> = <em>M/2</em> offers a balanced solution with lower cost. Specifically, for the RadFuse model, <em>c</em> = <em>M/2</em> achieved a balanced performance across all metrics, attaining the highest AUC of 0.964. Increasing c to M resulted in marginal improvements, with peak values for QWK (93.26%), MCC (82.81%), F1 Score (88.60%), and Accuracy (88.60%), though at increased computational cost. This configuration enhances the detection of specific features but does not universally improve severe stage performance, reinforcing c = M/2 as the optimal setting for balancing accuracy and efficiency. Detailed confusion matrices for these analyses are provided in Supplementary Figure <a href="#MOESM1" class="usa-link">S2</a>.</p>
<section class="tw xbox font-sm" id="Tab11"><h5 class="obj_head">Table 11.</h5>
<div class="caption p"><p>Performance comparison of ResNeXt-50-based RadFuse and RadEx-only models across varying curvature parameter settings <em>c</em>.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1"><em>c</em></th>
<th align="left" colspan="1" rowspan="1">Model</th>
<th align="left" colspan="1" rowspan="1">QWK %</th>
<th align="left" colspan="1" rowspan="1">MCC %</th>
<th align="left" colspan="1" rowspan="1">F1%</th>
<th align="left" colspan="1" rowspan="1">Accuracy %</th>
<th align="left" colspan="1" rowspan="1">AUC</th>
<th align="left" colspan="1" rowspan="1">Recall %</th>
<th align="left" colspan="1" rowspan="1">Precision %</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="2" colspan="1">
<em>M/</em>4</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1">91.98</td>
<td align="center" colspan="1" rowspan="1">80.26</td>
<td align="center" colspan="1" rowspan="1">87.00</td>
<td align="center" colspan="1" rowspan="1">86.99</td>
<td align="center" colspan="1" rowspan="1">0.952</td>
<td align="center" colspan="1" rowspan="1">86.99</td>
<td align="center" colspan="1" rowspan="1">86.99</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1">71.03</td>
<td align="center" colspan="1" rowspan="1">63.88</td>
<td align="center" colspan="1" rowspan="1">76.57</td>
<td align="center" colspan="1" rowspan="1">76.56</td>
<td align="center" colspan="1" rowspan="1">0.889</td>
<td align="center" colspan="1" rowspan="1">76.56</td>
<td align="center" colspan="1" rowspan="1">76.56</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">
<em>M</em>/2</td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">93.10</span></td>
<td align="center" colspan="1" rowspan="1">80.55</td>
<td align="center" colspan="1" rowspan="1">87.16</td>
<td align="center" colspan="1" rowspan="1">87.16</td>
<td align="center" colspan="1" rowspan="1"><strong>0.964</strong></td>
<td align="center" colspan="1" rowspan="1">87.16</td>
<td align="center" colspan="1" rowspan="1">87.16</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1"><strong>78.78</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>65.93</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>77.69</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>77.68</strong></td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">0.888</span></td>
<td align="center" colspan="1" rowspan="1"><strong>77.69</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>77.69</strong></td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1"><em>N</em></td>
<td align="left" colspan="1" rowspan="1">RadFuse</td>
<td align="center" colspan="1" rowspan="1"><strong>93.26</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>82.81</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>88.60</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>88.60</strong></td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">0.960</span></td>
<td align="center" colspan="1" rowspan="1"><strong>88.60</strong></td>
<td align="center" colspan="1" rowspan="1"><strong>88.60</strong></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RadEx-only</td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">73.49</span></td>
<td align="center" colspan="1" rowspan="1">63.88</td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">76.24</span></td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">76.24</span></td>
<td align="center" colspan="1" rowspan="1"><strong>0.893</strong></td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">76.24</span></td>
<td align="center" colspan="1" rowspan="1"><span class="text-underline">76.24</span></td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab11/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p86"><p>The best model in each configuration (RadFuse and RadEx-only) is highlighted in bold, and the second best is underlined.</p></div></div></section><figure class="fig xbox font-sm" id="Fig10"><h5 class="obj_head">Fig. 10.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370889_41598_2025_14944_Fig10_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b248/12370889/bed4b6aafff9/41598_2025_14944_Fig10_HTML.jpg" loading="lazy" id="d33e2986" height="539" width="691" alt="Fig. 10"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig10/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Recall rates for severe and PDR grades across different curvature parameter configurations in the ResNeXt-50-based RadFuse model. The bar chart shows the specific recall rates for each grade under three configurations of the curvature parameter <em>c</em>.</p></figcaption></figure></section></section><section id="Sec19"><h3 class="pmc_sec_title">Limitations and future work</h3>
<p id="Par56">Despite the promising results of RadFuse, several key limitations warrant further investigation. First, our evaluation on the APTOS-2019 and DDR datasets—while covering different camera systems and patient populations—does not encompass the full spectrum of ethnic diversity. We explicitly note this as a limitation and emphasize the importance of future validation on truly multi-ethnic cohorts (e.g., the AFIQ dataset) to assess and mitigate potential racial bias. Second, although APTOS and DDR differ in imaging protocols and demographic composition, we have not yet performed formal cross-dataset validation to quantify domain shift. While our independent experiments on each dataset demonstrate robustness, future work will include training on one dataset and testing on the other, as well as exploring domain-adaptation techniques to ensure consistent performance across varied clinical environments. Another limitation of our study is the absence of a dedicated explainability analysis, such as Grad-CAM visualizations, to interpret the decision-making process of the proposed models. Although our quantitative results demonstrate the performance gains from RadEx integration, future work will leverage explainability techniques to better understand and communicate the model’s behavior. This will help elucidate how sinogram features influence decision-making and further support the clinical trustworthiness and transparency of our approach. Moreover, while our optimized RadEx transform has proven effective in improving the feature representation of retinal images, its computational complexity could limit scalability, particularly in large-scale or resource-constrained environments. Addressing these limitations in future research will enhance both the practical applicability and robustness of the RadFuse approach. Future work will also focus on exploring synergies with emerging technologies, including the integration of RadEx with liquid neural networks for dynamic lesion tracking in longitudinal fundus series, and extending the RadEx transformation to three dimensions for Optical Coherence Tomography (OCT) volume analysis to capture volumetric lesion morphology. Finally, translating our methodological advancements into practical clinical tools necessitates the development of a structured clinical deployment framework. Future efforts will address real-time processing requirements to ensure efficient clinical integration, seamless compatibility with existing DICOM workflows, and navigation of regulatory pathways such as FDA approval for fusion-based diagnostic systems. Such steps will be vital to enable the real-world applicability and clinical adoption.</p></section><section id="Sec20"><h3 class="pmc_sec_title">Related work</h3>
<p id="Par57">Deep learning techniques have become increasingly popular for DR detection and grading, outperforming traditional machine learning methods in accuracy and effectiveness<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>. Early work by Gulshan et al.<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup> and Ting et al.<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup> demonstrated that CNN could surpass general ophthalmologists in detecting referable DR. Subsequent studies aimed to refine DR grading granularity. Several researchers have developed various methods for the diagnosis of DR using different approaches. For example, Pratet al.<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup> used CNNs with data augmentation to improve model generalization, while Shanthi et al.<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup> enhanced the AlexNet architecture for better grading accuracy. More recent developments include Islam et al.’s supervised contrastive learning (SCL) approach, which significantly improved binary classification and multi-stage DR grading using the APTOS 2019 dataset<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>. The SCL model attained an accuracy of 98.36% and an AUC of 0.985 for binary classification, and 84.36% accuracy for five-stage grading. Hossein et al.<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup> also leveraged transfer learning with ResNet50 and EfficientNetB0 to address class imbalances, utilizing data from multiple datasets. More recently, MDGNet introduced a novel architecture that integrates local and global lesion features for improved DR grading<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>. Despite these advancements, accurately capturing early-stage DR features, such as microaneurysms and small hemorrhages, remains challenging due to their subtle nature and variability<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>. CNN-based models often struggle with capturing subtle and non-linear in DR-related lesions, making it difficult to differentiate between adjacent severity levels.</p>
<p id="Par58">Image transformation techniques are essential in medical imaging, facilitating effective feature extraction crucial for precise diagnostics<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>. The Radon transform is a mathematical integral transform that computes projections of an image along specified directions. It has been widely used in computed tomography for image reconstruction from projection data<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>. The transform effectively captures linear features and has been applied in medical image analysis for tasks such lung and breast cancer classification<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a>,<a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup>. Its key benefit is its ability to simplify a complex image structure into analyzable projections, essential for both image reconstruction and feature extraction. Its integral nature enables it to accumulate all image data along specified lines through an object, highlighting critical structures and features for disease detection. TAVAKOLI et al.<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup> demonstrated the Radon transform effectiveness in enhancing microaneurysm detection in retinal images. It is worth mentioning that these studies primarily focused on using extracted features from the sinogram rather than directly utilizing the sinogram images for classification tasks. However, the linear Radon transform struggles to capture non-linear features such as curved edges or irregular textures, typical in pathological conditions. Its linear assumptions may not accurately represent these complex features. Therefore, Non-linear extensions of the Radon transform have been proposed to capture more complex patterns in image data. Our previous work introduced the RadEx transform, a customized non-linear Radon transformation designed to improve feature extraction in chest X-ray images for COVID-19 detection<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. The RadEx transform demonstrated the ability to highlight non-linear features not readily apparent in the spatial domain, suggesting its potential applicability to other medical imaging tasks. However, the application of non-linear Radon transformations for DR grading remains relatively unexplored. Therefore, building on the concept of the Radex transform, this study explores the use of an optimized version of RadEx, for enhanced feature extraction from fundus images in the context of DR grading.</p>
<p id="Par59">Combining multiple imaging modalities can provide complementary information, leading to improved diagnostic performance multimodal deep learning models integrate data from different sources to enhance feature representation and capture complex patterns<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>. In the context of DR, multimodal approaches have been less common due to the reliance on retinal fundus images as the primary data source. Some studies have explored the use of optical coherence tomography (OCT) alongside fundus images to improve DR detection<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>. However, OCT imaging is not always readily available, especially in resource-limited settings. Our work builds upon these insights by introducing a new multi-representation deep learning framework that integrates non-linear RadEx transformations-based sinogram images with retinal fundus images. This approach enhances feature extraction and improves DR severity grading accuracy.</p></section></section><section id="Sec21"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par60">In this study, we introduced RadFuse, a multi-representation deep learning approach designed to enhance DR detection and severity grading by integrating an optimized non-linear RadEx transformation with original fundus images. Our method significantly improves CNN performance in detecting and classifying DR severity levels. Extensive experiments on the APTOS-2019 and DDR datasets demonstrate that RadFuse multi-representation consistently outperforms traditional image-only models and several SOTA methods. By capturing complex non-linear patterns and distributed retinal lesions, the RadEx-sinogram provides valuable discriminative information not easily visible in the original images. Moreover, because RadFuse fuses RadEx sinograms with spatial images at the input level, it remains entirely <em>architecture-agnostic</em>. This design allows seamless integration with any backbone—whether convolutional, transformer-based, or hybrid—making RadFuse broadly applicable to current and future state-of-the-art models in medical image analysis. These findings highlight the potential of combining spatial and transformed domain information to improve DR detection and grading.</p></section><section id="Sec50"><h2 class="pmc_sec_title">Supplementary Information</h2>
<p>Below is the link to the electronic supplementary material.</p>
<section class="sm xbox font-sm" id="MOESM1"><div class="media p"><div class="caption">
<a href="/articles/instance/12370889/bin/41598_2025_14944_MOESM1_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material 1</a><sup> (348.2KB, pdf) </sup>
</div></div></section></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>Open Access funding provided by the Qatar National Library.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>F.M. and S.B. contributed to the conceptualization of the study. F.M. performed the simulations and contributed to the writing of the original manuscript. S.B. and Z.S. analyzed the results. S.B. and Z.S. revised the manuscript. S.B. and Z.S. supervised the study. All authors read and approved the final manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The APTOS dataset is openly available at: <a href="https://www.kaggle.com/competitions/aptos2019-blindness-detection" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/competitions/aptos2019-blindness-detection</a> (accessed on 12 August 2024). DDR dataset is available from <a href="https://github.com/nkicsl/DDR-dataset" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nkicsl/DDR-dataset</a> (accessed on 15 November 2024.)</p></section><section id="notes3"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar2"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par61">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>International Diabetes Federation. <em>IDF Diabetes Atlas</em> 9th edn. (International Diabetes Federation, 2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?International%20Diabetes%20Federation.%20IDF%20Diabetes%20Atlas%209th%20edn.%20(International%20Diabetes%20Federation,%202019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Thanikachalam, V., Kabilan, K. &amp; Erramchetty, S. Optimized deep CNN for detection and classification of diabetic retinopathy and diabetic macular edema. <em>BMC Med. Imaging</em><strong>24</strong>, 227. 10.1186/s12880-024-01406-1 (2024).
</cite> [<a href="https://doi.org/10.1186/s12880-024-01406-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11350985/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39198741/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Thanikachalam,%20V.,%20Kabilan,%20K.%20&amp;%20Erramchetty,%20S.%20Optimized%20deep%20CNN%20for%20detection%20and%20classification%20of%20diabetic%20retinopathy%20and%20diabetic%20macular%20edema.%20BMC%20Med.%20Imaging24,%20227.%2010.1186/s12880-024-01406-1%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Kusakunniran, W. et al. Detecting and staging diabetic retinopathy in retinal images using multi-branch CNN. <em>Appl. Comput. Inform.</em> (2022).</cite>
</li>
<li id="CR4">
<span class="label">4.</span><cite>Abràmoff, M. D. et al. Automated analysis of retinal images for detection of referable diabetic retinopathy. <em>JAMA Ophthalmol.</em><strong>131</strong>, 351–357 (2013).
</cite> [<a href="https://doi.org/10.1001/jamaophthalmol.2013.1743" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23494039/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Abr%C3%A0moff,%20M.%20D.%20et%20al.%20Automated%20analysis%20of%20retinal%20images%20for%20detection%20of%20referable%20diabetic%20retinopathy.%20JAMA%20Ophthalmol.131,%20351%E2%80%93357%20(2013)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. <em>JAMA</em><strong>316</strong>, 2402–2410 (2016).
</cite> [<a href="https://doi.org/10.1001/jama.2016.17216" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27898976/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Gulshan,%20V.%20et%20al.%20Development%20and%20validation%20of%20a%20deep%20learning%20algorithm%20for%20detection%20of%20diabetic%20retinopathy%20in%20retinal%20fundus%20photographs.%20JAMA316,%202402%E2%80%932410%20(2016)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR6">
<span class="label">6.</span><cite>Ting, D. S. W. et al. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. <em>JAMA</em><strong>318</strong>, 2211–2223 (2017).
</cite> [<a href="https://doi.org/10.1001/jama.2017.18152" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5820739/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29234807/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ting,%20D.%20S.%20W.%20et%20al.%20Development%20and%20validation%20of%20a%20deep%20learning%20system%20for%20diabetic%20retinopathy%20and%20related%20eye%20diseases%20using%20retinal%20images%20from%20multiethnic%20populations%20with%20diabetes.%20JAMA318,%202211%E2%80%932223%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<span class="label">7.</span><cite>Pratt, H., Coenen, F., Broadbent, D. M., Harding, S. P. &amp; Zheng, Y. Convolutional neural networks for diabetic retinopathy. <em>Procedia Comput. Sci.</em><strong>90</strong>, 200–205 (2016).</cite> [<a href="https://scholar.google.com/scholar_lookup?Pratt,%20H.,%20Coenen,%20F.,%20Broadbent,%20D.%20M.,%20Harding,%20S.%20P.%20&amp;%20Zheng,%20Y.%20Convolutional%20neural%20networks%20for%20diabetic%20retinopathy.%20Procedia%20Comput.%20Sci.90,%20200%E2%80%93205%20(2016)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>Gadekallu, T. R. et al. Early detection of diabetic retinopathy using PCA-firefly based deep learning model. <em>Electronics</em><strong>9</strong>, 274 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Gadekallu,%20T.%20R.%20et%20al.%20Early%20detection%20of%20diabetic%20retinopathy%20using%20PCA-firefly%20based%20deep%20learning%20model.%20Electronics9,%20274%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>Seoud, L., Hurtut, T., Chelbi, J., Cheriet, F. &amp; Langlois, J. P. Red lesion detection using dynamic shape features for diabetic retinopathy screening. <em>IEEE Trans. Med. Imaging</em><strong>35</strong>, 1116–1126 (2015).
</cite> [<a href="https://doi.org/10.1109/TMI.2015.2509785" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26701180/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Seoud,%20L.,%20Hurtut,%20T.,%20Chelbi,%20J.,%20Cheriet,%20F.%20&amp;%20Langlois,%20J.%20P.%20Red%20lesion%20detection%20using%20dynamic%20shape%20features%20for%20diabetic%20retinopathy%20screening.%20IEEE%20Trans.%20Med.%20Imaging35,%201116%E2%80%931126%20(2015)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR10">
<span class="label">10.</span><cite>De Fauw, J. et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. <em>Nat. Med.</em><strong>24</strong>, 1342–1350 (2018).
</cite> [<a href="https://doi.org/10.1038/s41591-018-0107-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30104768/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?De%20Fauw,%20J.%20et%20al.%20Clinically%20applicable%20deep%20learning%20for%20diagnosis%20and%20referral%20in%20retinal%20disease.%20Nat.%20Med.24,%201342%E2%80%931350%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>Bala, R., Sharma, A. &amp; Goel, N. CTNet: Convolutional transformer network for diabetic retinopathy classification. <em>Neural Comput. Appl.</em><strong>36</strong>, 4787–4809 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Bala,%20R.,%20Sharma,%20A.%20&amp;%20Goel,%20N.%20CTNet:%20Convolutional%20transformer%20network%20for%20diabetic%20retinopathy%20classification.%20Neural%20Comput.%20Appl.36,%204787%E2%80%934809%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Tavakoli, M. et al. Automated microaneurysms detection in retinal images using radon transform and supervised learning: Application to mass screening of diabetic retinopathy. <em>IEEE Access</em><strong>9</strong>, 67302–67314 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Tavakoli,%20M.%20et%20al.%20Automated%20microaneurysms%20detection%20in%20retinal%20images%20using%20radon%20transform%20and%20supervised%20learning:%20Application%20to%20mass%20screening%20of%20diabetic%20retinopathy.%20IEEE%20Access9,%2067302%E2%80%9367314%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>Wang, Y., Wang, L., Guo, Z., Song, S. &amp; Li, Y. A graph convolutional network with dynamic weight fusion of multi-scale local features for diabetic retinopathy grading. <em>Sci. Rep.</em><strong>14</strong>, 5791 (2024).
</cite> [<a href="https://doi.org/10.1038/s41598-024-56389-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10924962/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38461342/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wang,%20Y.,%20Wang,%20L.,%20Guo,%20Z.,%20Song,%20S.%20&amp;%20Li,%20Y.%20A%20graph%20convolutional%20network%20with%20dynamic%20weight%20fusion%20of%20multi-scale%20local%20features%20for%20diabetic%20retinopathy%20grading.%20Sci.%20Rep.14,%205791%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Toft, P. The radon transform. <em>Theory Implementation </em>(Ph. D. Diss. Tech. Univ. Denmark, 1996).</cite>
</li>
<li id="CR15">
<span class="label">15.</span><cite>Deans, S. R. <em>The Radon Transform and Some of Its Applications</em> (Courier Corporation, 2007).</cite> [<a href="https://scholar.google.com/scholar_lookup?Deans,%20S.%20R.%20The%20Radon%20Transform%20and%20Some%20of%20Its%20Applications%20(Courier%20Corporation,%202007)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>Raaj, R. S. Breast cancer detection and diagnosis using hybrid deep learning architecture. <em>Biomed. Signal Process. Control.</em><strong>82</strong>, 104558 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Raaj,%20R.%20S.%20Breast%20cancer%20detection%20and%20diagnosis%20using%20hybrid%20deep%20learning%20architecture.%20Biomed.%20Signal%20Process.%20Control.82,%20104558%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR17">
<span class="label">17.</span><cite>Islam, A., Mohsen, F., Shah, Z. &amp; Belhaouari, S. B. Introducing novel radon based transform for disease detection from chest x-ray images. In <em>2024 6th International Conference on Pattern Analysis and Intelligent Systems (PAIS)</em> 1–5 (IEEE, 2024).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>(APTOS), A. P. T.-O. S. Messidor-adcis. <a href="https://www.kaggle.com/c/aptos2019-blindness-detection" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/c/aptos2019-blindness-detection</a> (2019). Accessed: Sep. 11, 2021. [Online].</cite>
</li>
<li id="CR19">
<span class="label">19.</span><cite>Li, T. et al. Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening. <em>Inf. Sci.</em><strong>501</strong>, 511–522 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20T.%20et%20al.%20Diagnostic%20assessment%20of%20deep%20learning%20algorithms%20for%20diabetic%20retinopathy%20screening.%20Inf.%20Sci.501,%20511%E2%80%93522%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Graham, B. Kaggle diabetic retinopathy detection competition report (2015). <a href="https://www.kaggle.com/c/diabetic-retinopathy-detection/discussion/15801" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/c/diabetic-retinopathy-detection/discussion/15801</a>.</cite>
</li>
<li id="CR21">
<span class="label">21.</span><cite>Xie, S., Girshick, R., Dollár, P., Tu, Z. &amp; He, K. Aggregated residual transformations for deep neural networks. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> 1492–1500 (2017).</cite>
</li>
<li id="CR22">
<span class="label">22.</span><cite>Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. &amp; Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 4510–4520 (2018).</cite>
</li>
<li id="CR23">
<span class="label">23.</span><cite>Simonyan, K. &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. Preprint at <a href="https://arxiv.org/abs/1409.1556" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1409.1556</a> (2014).</cite>
</li>
<li id="CR24">
<span class="label">24.</span><cite>Mohsen, F., Ali, H., El Hajj, N. &amp; Shah, Z. Artificial intelligence-based methods for fusion of electronic health records and imaging data. <em>Sci. Rep.</em><strong>12</strong>, 17981 (2022).
</cite> [<a href="https://doi.org/10.1038/s41598-022-22514-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9605975/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36289266/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Mohsen,%20F.,%20Ali,%20H.,%20El%20Hajj,%20N.%20&amp;%20Shah,%20Z.%20Artificial%20intelligence-based%20methods%20for%20fusion%20of%20electronic%20health%20records%20and%20imaging%20data.%20Sci.%20Rep.12,%2017981%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR25">
<span class="label">25.</span><cite>Yue, G. et al. Attention-driven cascaded network for diabetic retinopathy grading from fundus images. <em>Biomed. Signal Process. Control.</em><strong>80</strong>, 104370 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yue,%20G.%20et%20al.%20Attention-driven%20cascaded%20network%20for%20diabetic%20retinopathy%20grading%20from%20fundus%20images.%20Biomed.%20Signal%20Process.%20Control.80,%20104370%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<span class="label">26.</span><cite>Islam, M. R. et al. Applying supervised contrastive learning for the detection of diabetic retinopathy and its severity levels from fundus images. <em>Comput. Biol. Med.</em><strong>146</strong>, 105602 (2022).
</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2022.105602" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35569335/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Islam,%20M.%20R.%20et%20al.%20Applying%20supervised%20contrastive%20learning%20for%20the%20detection%20of%20diabetic%20retinopathy%20and%20its%20severity%20levels%20from%20fundus%20images.%20Comput.%20Biol.%20Med.146,%20105602%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR27">
<span class="label">27.</span><cite>Bi, Q. et al. Mil-vit: A multiple instance vision transformer for fundus image classification. <em>J. Vis. Commun. Image Represent.</em><strong>97</strong>, 103956. 10.1016/j.jvcir.2023.103956 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Bi,%20Q.%20et%20al.%20Mil-vit:%20A%20multiple%20instance%20vision%20transformer%20for%20fundus%20image%20classification.%20J.%20Vis.%20Commun.%20Image%20Represent.97,%20103956.%2010.1016/j.jvcir.2023.103956%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Li, X. et al. Canet: Cross-disease attention network for joint diabetic retinopathy and diabetic macular edema grading. <em>IEEE Trans. Med. Imaging</em><strong>39</strong>, 1483–1493 (2019).
</cite> [<a href="https://doi.org/10.1109/TMI.2019.2951844" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31714219/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Li,%20X.%20et%20al.%20Canet:%20Cross-disease%20attention%20network%20for%20joint%20diabetic%20retinopathy%20and%20diabetic%20macular%20edema%20grading.%20IEEE%20Trans.%20Med.%20Imaging39,%201483%E2%80%931493%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29">
<span class="label">29.</span><cite>Shakibania, H., Raoufi, S., Pourafkham, B., Khotanlou, H. &amp; Mansoorizadeh, M. Dual branch deep learning network for detection and stage grading of diabetic retinopathy. <em>Biomed. Signal Process. Control.</em><strong>93</strong>, 106168 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Shakibania,%20H.,%20Raoufi,%20S.,%20Pourafkham,%20B.,%20Khotanlou,%20H.%20&amp;%20Mansoorizadeh,%20M.%20Dual%20branch%20deep%20learning%20network%20for%20detection%20and%20stage%20grading%20of%20diabetic%20retinopathy.%20Biomed.%20Signal%20Process.%20Control.93,%20106168%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR30">
<span class="label">30.</span><cite>Liu, Z. <em>et al.</em> Swin transformer: Hierarchical vision transformer using shifted windows. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> 10012–10022 (2021).</cite>
</li>
<li id="CR31">
<span class="label">31.</span><cite>Han, K., Wang, Y., Guo, J., Tang, Y. &amp; Wu, E. Vision GNN: An image is worth graph of nodes. <em>Adv. Neural Inf. Process. Syst.</em><strong>35</strong>, 8291–8303 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Han,%20K.,%20Wang,%20Y.,%20Guo,%20J.,%20Tang,%20Y.%20&amp;%20Wu,%20E.%20Vision%20GNN:%20An%20image%20is%20worth%20graph%20of%20nodes.%20Adv.%20Neural%20Inf.%20Process.%20Syst.35,%208291%E2%80%938303%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Gong, L., Ma, K. &amp; Zheng, Y. Distractor-aware neuron intrinsic learning for generic 2d medical image classifications. In <em>Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part II 23</em> 591–601 (Springer, 2020).</cite>
</li>
<li id="CR33">
<span class="label">33.</span><cite>Liu, S., Gong, L., Ma, K. &amp; Zheng, Y. Green: a graph residual re-ranking network for grading diabetic retinopathy. In <em>Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V 23</em> 585–594 (Springer, 2020).</cite>
</li>
<li id="CR34">
<span class="label">34.</span><cite>Bodapati, J. D. et al. Blended multi-modal deep convnet features for diabetic retinopathy severity prediction. <em>Electronics</em><strong>9</strong>, 914 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Bodapati,%20J.%20D.%20et%20al.%20Blended%20multi-modal%20deep%20convnet%20features%20for%20diabetic%20retinopathy%20severity%20prediction.%20Electronics9,%20914%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Kumar, G., Chatterjee, S. &amp; Chattopadhyay, C. Dristi: A hybrid deep neural network for diabetic retinopathy diagnosis. <em>Signal Image Video Process.</em><strong>15</strong>, 1679–1686 (2021).
</cite> [<a href="https://doi.org/10.1007/s11760-021-01904-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8051933/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33897905/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Kumar,%20G.,%20Chatterjee,%20S.%20&amp;%20Chattopadhyay,%20C.%20Dristi:%20A%20hybrid%20deep%20neural%20network%20for%20diabetic%20retinopathy%20diagnosis.%20Signal%20Image%20Video%20Process.15,%201679%E2%80%931686%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<span class="label">36.</span><cite>He, A., Li, T., Li, N., Wang, K. &amp; Fu, H. Cabnet: Category attention block for imbalanced diabetic retinopathy grading. <em>IEEE Trans. Med. Imaging</em><strong>40</strong>, 143–153 (2020).
</cite> [<a href="https://doi.org/10.1109/TMI.2020.3023463" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32915731/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?He,%20A.,%20Li,%20T.,%20Li,%20N.,%20Wang,%20K.%20&amp;%20Fu,%20H.%20Cabnet:%20Category%20attention%20block%20for%20imbalanced%20diabetic%20retinopathy%20grading.%20IEEE%20Trans.%20Med.%20Imaging40,%20143%E2%80%93153%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR37">
<span class="label">37.</span><cite>Tian, M. et al. Fine-grained attention &amp; knowledge-based collaborative network for diabetic retinopathy grading. <em>Heliyon</em><strong>9</strong> (2023).</cite> [<a href="https://doi.org/10.1016/j.heliyon.2023.e17217" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10336422/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37449186/" class="usa-link">PubMed</a>]</li>
<li id="CR38">
<span class="label">38.</span><cite>Gu, Z. et al. Classification of diabetic retinopathy severity in fundus images using the vision transformer and residual attention. <em>Comput. Intell. Neurosci.</em><strong>2023</strong>, 1305583 (2023).
</cite> [<a href="https://doi.org/10.1155/2023/1305583" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9831706/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36636467/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Gu,%20Z.%20et%20al.%20Classification%20of%20diabetic%20retinopathy%20severity%20in%20fundus%20images%20using%20the%20vision%20transformer%20and%20residual%20attention.%20Comput.%20Intell.%20Neurosci.2023,%201305583%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR39">
<span class="label">39.</span><cite>Wang, X. et al. Joint learning of multi-level tasks for diabetic retinopathy grading on low-resolution fundus images. <em>IEEE J. Biomed. Heal. Inform.</em><strong>26</strong>, 2216–2227 (2021).</cite> [<a href="https://doi.org/10.1109/JBHI.2021.3119519" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34648460/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wang,%20X.%20et%20al.%20Joint%20learning%20of%20multi-level%20tasks%20for%20diabetic%20retinopathy%20grading%20on%20low-resolution%20fundus%20images.%20IEEE%20J.%20Biomed.%20Heal.%20Inform.26,%202216%E2%80%932227%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Shanthi, T. &amp; Sabeenian, R. Modified alexnet architecture for classification of diabetic retinopathy images. <em>Comput. Electr. Eng.</em><strong>76</strong>, 56–64 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Shanthi,%20T.%20&amp;%20Sabeenian,%20R.%20Modified%20alexnet%20architecture%20for%20classification%20of%20diabetic%20retinopathy%20images.%20Comput.%20Electr.%20Eng.76,%2056%E2%80%9364%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR41">
<span class="label">41.</span><cite>Lam, C., Yu, C., Huang, L. &amp; Rubin, D. Retinal lesion detection with deep learning using image patches. <em>Investig. Ophthalmol. Vis. Sci.</em><strong>59</strong>, 590–596 (2018).
</cite> [<a href="https://doi.org/10.1167/iovs.17-22721" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5788045/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29372258/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Lam,%20C.,%20Yu,%20C.,%20Huang,%20L.%20&amp;%20Rubin,%20D.%20Retinal%20lesion%20detection%20with%20deep%20learning%20using%20image%20patches.%20Investig.%20Ophthalmol.%20Vis.%20Sci.59,%20590%E2%80%93596%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR42">
<span class="label">42.</span><cite>Arulmurugan, R. &amp; Anandakumar, H. Early detection of lung cancer using wavelet feature descriptor and feed forward back propagation neural networks classifier. In <em>Computational Vision and Bio Inspired Computing</em> 103–110 (Springer, 2018).</cite>
</li>
<li id="CR43">
<span class="label">43.</span><cite>Rani, K. V., Sumathy, G., Shoba, L., Shermila, P. J. &amp; Prince, M. E. Radon transform-based improved single seeded region growing segmentation for lung cancer detection using AMPWSVM classification approach. <em>Signal Image Video Process.</em><strong>17</strong>, 4571–4580 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Rani,%20K.%20V.,%20Sumathy,%20G.,%20Shoba,%20L.,%20Shermila,%20P.%20J.%20&amp;%20Prince,%20M.%20E.%20Radon%20transform-based%20improved%20single%20seeded%20region%20growing%20segmentation%20for%20lung%20cancer%20detection%20using%20AMPWSVM%20classification%20approach.%20Signal%20Image%20Video%20Process.17,%204571%E2%80%934580%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adsm93_" lang="en" class="supplementary-materials"><h3 class="pmc_sec_title">Supplementary Materials</h3>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12370889/bin/41598_2025_14944_MOESM1_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material 1</a><sup> (348.2KB, pdf) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The APTOS dataset is openly available at: <a href="https://www.kaggle.com/competitions/aptos2019-blindness-detection" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/competitions/aptos2019-blindness-detection</a> (accessed on 12 August 2024). DDR dataset is available from <a href="https://github.com/nkicsl/DDR-dataset" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nkicsl/DDR-dataset</a> (accessed on 15 November 2024.)</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-14944-7"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_14944.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (4.4 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12370889/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12370889/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370889%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370889/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12370889/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12370889/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40841568/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12370889/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40841568/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12370889/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12370889/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="fTCHRZLZicvcaA1Mqm49G3mgHyn1kMng1tGl9pIrpSVSUJzP1W8LsrrwW8njNcpv">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
