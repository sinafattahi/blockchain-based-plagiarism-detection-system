
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Evidence for compositionality in fMRI visual representations via Brain Algebra - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4A38C8AF2095305A38C00150DBA46.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="commbiol">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373870/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Communications Biology">
<meta name="citation_title" content="Evidence for compositionality in fMRI visual representations via Brain Algebra">
<meta name="citation_author" content="Matteo Ferrante">
<meta name="citation_author_institution" content="Department of Biomedicine and Prevention, University of Rome, Tor Vergata (IT), Roma, Italy">
<meta name="citation_author" content="Tommaso Boccato">
<meta name="citation_author_institution" content="Department of Biomedicine and Prevention, University of Rome, Tor Vergata (IT), Roma, Italy">
<meta name="citation_author" content="Nicola Toschi">
<meta name="citation_author_institution" content="Martinos Center For Biomedical Imaging, MGH and Harvard Medical School (USA), Charlestown, USA">
<meta name="citation_author" content="Rufin VanRullen">
<meta name="citation_author_institution" content="CerCo, CNRS UMR5549, Toulouse, France">
<meta name="citation_author_institution" content="Universite de Toulouse, Toulouse, France">
<meta name="citation_author_institution" content="ANITI, Toulouse, France">
<meta name="citation_publication_date" content="2025 Aug 22">
<meta name="citation_volume" content="8">
<meta name="citation_firstpage" content="1263">
<meta name="citation_doi" content="10.1038/s42003-025-08706-4">
<meta name="citation_pmid" content="40847014">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373870/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373870/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373870/pdf/42003_2025_Article_8706.pdf">
<meta name="description" content="Electrophysiological and neuroimaging studies have revealed how the brain encodes various visual categories and concepts. An open question is how combinations of multiple visual concepts are represented in terms of the component brain patterns: are ...">
<meta name="og:title" content="Evidence for compositionality in fMRI visual representations via Brain Algebra">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Electrophysiological and neuroimaging studies have revealed how the brain encodes various visual categories and concepts. An open question is how combinations of multiple visual concepts are represented in terms of the component brain patterns: are ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373870/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12373870">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s42003-025-08706-4"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/42003_2025_Article_8706.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373870%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12373870/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12373870/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373870/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-commbiol.jpg" alt="Communications Biology logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Communications Biology" title="Link to Communications Biology" shape="default" href="https://www.nature.com/commsbio/" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Commun Biol</button></div>. 2025 Aug 22;8:1263. doi: <a href="https://doi.org/10.1038/s42003-025-08706-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s42003-025-08706-4</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Commun%20Biol%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Commun%20Biol%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Commun%20Biol%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Commun%20Biol%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Evidence for compositionality in fMRI visual representations via Brain Algebra</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ferrante%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Matteo Ferrante</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Matteo Ferrante</span></h3>
<div class="p">
<sup>1</sup>Department of Biomedicine and Prevention, University of Rome, Tor Vergata (IT), Roma, Italy </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ferrante%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Matteo Ferrante</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Boccato%20T%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Tommaso Boccato</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Tommaso Boccato</span></h3>
<div class="p">
<sup>1</sup>Department of Biomedicine and Prevention, University of Rome, Tor Vergata (IT), Roma, Italy </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Boccato%20T%22%5BAuthor%5D" class="usa-link"><span class="name western">Tommaso Boccato</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Toschi%20N%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Nicola Toschi</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Nicola Toschi</span></h3>
<div class="p">
<sup>2</sup>Martinos Center For Biomedical Imaging, MGH and Harvard Medical School (USA), Charlestown, USA </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Toschi%20N%22%5BAuthor%5D" class="usa-link"><span class="name western">Nicola Toschi</span></a>
</div>
</div>
<sup>2,</sup><sup>✉,</sup><sup>#</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22VanRullen%20R%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Rufin VanRullen</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Rufin VanRullen</span></h3>
<div class="p">
<sup>3</sup>CerCo, CNRS UMR5549, Toulouse, France </div>
<div class="p">
<sup>4</sup>Universite de Toulouse, Toulouse, France </div>
<div class="p">
<sup>5</sup>ANITI, Toulouse, France </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22VanRullen%20R%22%5BAuthor%5D" class="usa-link"><span class="name western">Rufin VanRullen</span></a>
</div>
</div>
<sup>3,</sup><sup>4,</sup><sup>5,</sup><sup>✉,</sup><sup>#</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Department of Biomedicine and Prevention, University of Rome, Tor Vergata (IT), Roma, Italy </div>
<div id="Aff2">
<sup>2</sup>Martinos Center For Biomedical Imaging, MGH and Harvard Medical School (USA), Charlestown, USA </div>
<div id="Aff3">
<sup>3</sup>CerCo, CNRS UMR5549, Toulouse, France </div>
<div id="Aff4">
<sup>4</sup>Universite de Toulouse, Toulouse, France </div>
<div id="Aff5">
<sup>5</sup>ANITI, Toulouse, France </div>
<div class="author-notes p">
<div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div>
<div class="fn" id="_eqcntrb93pmc__">
<sup>#</sup><p class="display-inline">Contributed equally.</p>
</div>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Jan 13; Accepted 2025 Aug 11; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12373870  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40847014/" class="usa-link">40847014</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Electrophysiological and neuroimaging studies have revealed how the brain encodes various visual categories and concepts. An open question is how combinations of multiple visual concepts are represented in terms of the component brain patterns: are brain responses to individual concepts composed according to algebraic rules? To explore this, we generated “conceptual perturbations" in neural space by averaging fMRI responses to images with a shared concept (e.g., “winter" or “summer"). After thresholding to ensure specificity, we applied these perturbations to the neural pattern associated with a base image, forming new brain patterns that incorporate the added concept. These modified brain patterns were then decoded into images using a pretrained fMRI-to-image decoding model. Qualitative and quantitative inspection of the resulting images provides insight into how the brain might combine visual concepts. For example, adding a “winter" perturbation to the brain pattern of a man on a skateboard yields a new pattern representing a man on a snowboard in a winter scene—even when the perturbation modifies only a small subset of voxels. Our findings reveal that compositional processes in neural representations may lead to predictable perceptual outcomes, as interpreted by our decoding model. This suggests that the brain’s combinatory encoding of concepts may follow a systematic, algebraic-like process—what we term “brain algebra." Although our study is model-driven, it opens avenues for future empirical work into the mechanisms of compositionality in the brain.</p>
<section id="kwd-group1" class="kwd-group"><p><strong>Subject terms:</strong> Neural decoding, Neural encoding</p></section></section><section class="abstract" id="Abs2"><hr class="headless">
<p id="Par2">Algebraic combinations of fMRI patterns reveal compositional neural representations of visual concepts, showing that brain activity can be perturbed in predictable ways to generate interpretable and meaningful perceptual transformations.</p></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par3">The compositionality of latent representations in artificial intelligence (AI) systems has contributed to recent advancements in deep learning. Model-based techniques like word embeddings have demonstrated that semantic relationships between concepts can be captured through vector arithmetic—for example, “king” minus “man” plus “woman” yields a vector close to “queen"<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>. Similarly, image and text representations in AI models exhibit compositional properties that allow for the manipulation and combination of visual and semantic concepts<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a>–<a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>.</p>
<p id="Par4">In neuroscience, machine learning has spurred significant progress through the development of encoding and decoding models. These models have established bidirectional mappings between visual or linguistic inputs and corresponding brain activity<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a>–<a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>. Notably, the use of larger, multimodal, and more complex models—which often exhibit some amount of compositionality—has led to significant improvements in predicting brain activity with encoding models. This suggests that better embeddings, enriched with compositional properties, capture more nuanced information that aligns more closely with the brain’s representations<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a>–<a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>. This raises a fundamental question: <strong>does the human brain employ a similar compositional structure in its neural code for vision</strong><sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>? Recent evidence suggests that brain-pattern compositionality may indeed occur in specific linguistic contexts<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>: information regarding analogy questions can be effectively retrieved through the addition and subtraction of functional Magnetic Resonance Imaging (fMRI) patterns. In their study, participants were presented with sequences of related concepts, such as professions, tools, and places (e.g., “doctor", “stethoscope", “hospital"). The researchers demonstrated that the algebraic combination of fMRI activation patterns could reflect analogical reasoning, akin to vector operations in word embeddings (e.g., “mechanic-doctor+stethoscope=wrench").Moreover, the vector space representations utilized by AI models appear to exhibit key properties essential for supporting cognition, such as high-dimensional representations, compositionality, concept distances, and similarity measures<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>.</p>
<p id="Par5">Building on these findings, we investigate whether <strong>brain-pattern compositionality holds for visual representations</strong>. Specifically, we aim to determine whether the neural activity elicited by viewing a composite image can be approximated by algebraically combining the neural patterns associated with its constituent parts—a concept we refer to as <strong>"brain algebra."</strong> In this framework, adding the neural pattern associated with a particular concept to the neural pattern of a base image should yield a new neural pattern corresponding to the perception of the base image modified by the added concept. This idea mirrors the compositional operations observed in AI models, where vector arithmetic in latent spaces captures semantic relationships between concepts. By testing this hypothesis in the context of visual perception, we aim to uncover whether the brain employs a similar mechanism for combining visual information.</p>
<p id="Par6">In doing so, we also test a related hypothesis: if compositional structure is embedded in neural representations, then it should manifest in the ability to perturb only a sparse subset of voxels and still observe meaningful transformations in perceptual content. This hypothesis enables us to examine whether compositionality is supported by localized coding, distributed patterns, or a hybrid of both. While we do not aim to resolve the broader debate on semantic brain organization, our findings provide a novel lens through which this question can be revisited.</p>
<p id="Par7">While compositionality has been extensively studied in the context of language models and visual generative systems in AI<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a>,<a href="#CR6" class="usa-link" aria-describedby="CR6">6</a>,<a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>, and recent neuroimaging work has shown compositional effects in linguistic reasoning tasks<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>, little is known about whether similar compositional mechanisms operate in the brain’s visual representations. Our study aims to bridge this gap by testing whether algebraic operations on neural patterns derived from real fMRI data can yield predictable and interpretable visual transformations. In doing so, we extend the investigation of compositionality from semantic and multimodal embeddings to empirical neural representations of visual cognition.</p>
<p id="Par8">To address this question, we use the Natural Scenes Dataset (NSD)<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>, a large-scale fMRI dataset where participants viewed approximately 10,000 natural images while their brain activity was recorded using a 7T fMRI scanner. We define “base” brain patterns as the fMRI responses to individual test images from this dataset. We also define “concept” brain patterns, where we average the fMRI responses to multiple training set images that share a specific concept. The presence of these concepts in training images is identified using semantic embedding models (i.e., CLIP;<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>), ensuring that the selected images are strongly related, from a semantic point of view, to the target concept. By algebraically combining these patterns in brain space, we create a perturbed brain pattern that hypothetically represents the base image with the added concept. This is done by adding the thresholded concept pattern to the base pattern. For example, starting with a base brain pattern corresponding to <em>an image of a man on a skateboard</em>, we might add the concept pattern for “<em>winter</em>" to generate a perturbed pattern that should represent <em>a man on a snowboard during winter</em>.</p>
<p id="Par9">A critical challenge is evaluating whether this perturbed brain pattern truly corresponds to the brain representation of the base image with the added concept. One approach would be to create corresponding composite images—for instance, using generative AI models to synthesize images that combine the base image with the added concept—and then present them to participants in an fMRI experiment. By recording the brain activity elicited by these composite images, we could compare the observed neural patterns with the predicted ones derived from our “brain algebra” operations (analogous to the approach used in ref. <sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>). However, this method presents several difficulties. First, the number of possible combinations of base images and concepts leads to a combinatorial explosion in the number of stimuli required. Testing a wide range of concepts and their combinations would necessitate a prohibitive number of experimental trials. fMRI experiments are inherently long and expensive, with limitations on how long participants can be scanned and associated financial costs with data acquisition. These logistical constraints make it impractical to collect sufficient data to robustly evaluate compositionality across diverse concepts. Second, even if such extensive experiments were feasible, interpreting the results would remain challenging. Differences between the expected and observed brain patterns could arise from various sources, including fMRI noise, individual variability in neural responses, or limitations in the quality and realism of the generated stimuli. These confounding factors would challenge any definitive conclusions about compositionality in the brain’s neural code.</p>
<p id="Par10">Additionally, existing neurostimulation technologies do not currently permit to directly manipulate specific voxel activations in the brain to test compositionality. We cannot selectively stimulate or alter precise patterns of neural activity at the voxel level to create the exact perturbed brain patterns hypothesized by our “brain algebra” model. This limitation means that we cannot empirically test the predicted neural patterns by artificially inducing them in the brain.</p>
<p id="Par11">Given these challenges, we employ an alternative approach that evaluates the “brain algebra” results using a decoding model. By transforming the perturbed brain patterns into reconstructed images, we can indirectly assess whether the algebraic combination of neural patterns corresponds to meaningful composite perceptions. This method leverages existing fMRI data and advanced decoding algorithms to infer the perceptual content associated with the combined neural patterns, providing a practical lens to explore our research question within the constraints of current technology.</p>
<p id="Par12">Thus, instead of attempting to collect new fMRI data or manipulating brain activity directly, we employ Brain-Diffuser<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>, a well-established decoding model that maps brain activity into the latent space of a generative model. By decoding the perturbed brain patterns, we can reconstruct images that represent the hypothetical perception resulting from our brain algebra operations. The resulting images can be assessed qualitatively—through visual inspection—or quantitatively using automated semantic analysis tools based on AI systems like CLIP<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>. In summary, our study explores whether the compositionality observed in AI systems and linguistic brain representations extends to visual processing in the human brain. We introduce a novel method to assess “brain algebra," combining base and concept brain patterns derived from actual fMRI data, and employ the Brain-Diffuser model to decode these patterns into images. Through this method, we aim to provide evidence for or against the existence of compositional neural codes in visual cognition. In the following sections, we detail our methodology for defining and combining the base and concept brain patterns, describe how we employ the Brain-Diffuser model for decoding, and present our qualitative and quantitative analyses of the reconstructed images to evaluate compositionality. See Fig. <a href="#Fig1" class="usa-link">1</a> for a visual explanation of perturbation definition and Fig. <a href="#Fig2" class="usa-link">2</a> for a visual overview of our approach.</p>
<figure class="fig xbox font-sm" id="Fig1"><h3 class="obj_head">Fig. 1. A visual overview of the process used to define conceptual perturbation vectors based on “winter” and “summer” concepts.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373870_42003_2025_8706_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8de9/12373870/aea50cb9af07/42003_2025_8706_Fig1_HTML.jpg" loading="lazy" id="d33e377" height="399" width="800" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>First, the textual representations of the concepts are encoded using the CLIP Text model, generating 512-dimensional embeddings. Simultaneously, images from the NSD training set are processed through the CLIP Vision model to obtain vision embeddings. Cosine similarity is calculated between the vision embeddings and the textual concept embeddings (e.g., “winter” and “summer”), allowing us to select the top-matching images that best represent each concept. The fMRI patterns corresponding to these selected images are then averaged to generate concept-specific perturbation patterns in brain space, such as <em>z</em><sub><em>w</em><em>i</em><em>n</em><em>t</em><em>e</em><em>r</em></sub> for winter and <em>z</em><sub><em>s</em><em>u</em><em>m</em><em>m</em><em>e</em><em>r</em></sub> for summer. These perturbation vectors are later thresholded, and combined with base fMRI patterns in the brain algebra framework to modulate visual representations (see Fig. <a href="#Fig2" class="usa-link">2</a>).</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig2"><h3 class="obj_head">Fig. 2. Illustration of the “brain algebra” approach used in our study.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373870_42003_2025_8706_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8de9/12373870/491887348df5/42003_2025_8706_Fig2_HTML.jpg" loading="lazy" id="d33e389" height="402" width="699" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The leftmost image represents the initial visual stimulus presented to the participant, with corresponding fMRI activations shown as heatmaps across different brain regions. Perturbations are introduced by summing the base brain pattern with a concept-specific perturbation vector, such as “summer,” “winter,” “man,” or “woman.” The perturbation vector is computed as a thresholded average of brain patterns evoked by visual perception of images with that content (see Fig. <a href="#Fig1" class="usa-link">1</a>). The perturbed brain patterns (center) are subsequently decoded using a pretrained fMRI decoder, producing modified images that reflect the added conceptual information (right). The results demonstrate how small changes in neural patterns can lead to predictable and meaningful changes in visual perception, supporting the hypothesis of compositionality in neural representations.</p></figcaption></figure></section><section id="Sec2"><h2 class="pmc_sec_title">Results</h2>
<p id="Par13">We explored 12 different semantic concepts encompassing themes such as season (winter, summer), gender (man, woman), lighting (night, day), numerosity (empty, crowded), location (indoor, outdoor) and emotions (happy, sad). Each corresponding perturbation vector was thresholded by a variable amount (retaining between 5% and 100% of the voxels, the rest being set to zero), and scaled by various factors <em>α</em> (from 1 to 4) before being summed with base fMRI patterns corresponding to a random subset composed of 100 test images. We begin this section by discussing the qualitative results, focusing on the exemplary Figs. <a href="#Fig3" class="usa-link">3</a> and <a href="#Fig4" class="usa-link">4</a>, which were generated using a scaling factor of <em>α</em> = 2 and a 50% threshold to visually highlight the key outcomes. Additional figures with varying scaling values and thresholds are provided in the <a href="#MOESM2" class="usa-link">supplementary materials</a> for a more comprehensive evaluation.</p>
<figure class="fig xbox font-sm" id="Fig3"><h3 class="obj_head">Fig. 3. Qualitative evaluation of brain algebra perturbations applied to base images (images best viewed digitally).</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373870_42003_2025_8706_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8de9/12373870/c5e39b6eeb6d/42003_2025_8706_Fig3_HTML.jpg" loading="lazy" id="d33e417" height="815" width="800" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Starting from the central base images, decoded from a (non-perturbed) base fMRI pattern, perturbations corresponding to various concepts---such as “summer,” “winter,” “day,” “night,” “man,” “woman,” and more---are applied to the brain patterns. The resulting decoded images show how the base visual perception is altered by the addition of each conceptual perturbation, reflecting changes in environmental conditions, the presence or absence of people, and other context-specific details. This demonstrates the ability of brain algebra to generate compositional modifications in visual representations based on abstract conceptual inputs.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig4"><h3 class="obj_head">Fig. 4.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373870_42003_2025_8706_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8de9/12373870/ecd9a6141d19/42003_2025_8706_Fig4_HTML.jpg" loading="lazy" id="d33e428" height="808" width="800" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>More examples as in Fig. <a href="#Fig3" class="usa-link">3</a>.</p></figcaption></figure><p id="Par14">In the first set of images (top of Fig. <a href="#Fig3" class="usa-link">3</a>), showing horses in a field and an indoor bathroom, compositionality is evident. Concepts like “summer," “winter," and “night” alter the landscape and lighting in the horse scene, while “woman” and “man” introduce an additional person. In the bathroom scene, “crowded” and “empty” adjust the number of objects or people, and “summer” and “winter” change the mood. The bottom row, showing a skater and a social gathering, also demonstrates compositional transformations. “Summer” and “winter” modify the environment, “woman” changes the skater’s gender, and “crowded” and “happy” alter social dynamics and expressions.</p>
<p id="Par15">Similarly, perturbations in Fig. <a href="#Fig4" class="usa-link">4</a> introduce clear modifications based on the perturbation concepts. For the bus scene, “summer” brings brighter environments with outdoor activities (however, the bus is no longer present), while “night” darkens the scene with illuminated elements. The “indoor scene” concept transforms the bus into a more enclosed space, like a terminal. Perturbations applied to the man holding a sandwich show clear changes—“woman” alters the subject’s gender, “empty” reduces the person’s size within the scene, and “crowded” adds individuals to the scene. In the outdoor table scene and paragliding activity, compositional adjustments are also evident. These results suggest that the model effectively generates coherent and predictable changes in response to targeted brain perturbations. Overall, these results support the hypothesis that compositionality in brain patterns can be decoded into visually meaningful images. The perturbations introduced lead to expected modifications in both human-related and environmental aspects, while generally maintaining the integrity of the original base scenes. This indicates that the brain perturbation patterns successfully capture abstract conceptual information and translate it into visual content, providing strong evidence for compositionality in the brain’s visual representations.</p>
<p id="Par16">While these qualitative examples generally support our hypothesis of visual concept compositionality, they also include some perturbations for which the desired concept did not obviously appear or was difficult to evaluate (e.g., paragliding+day appears similar to the base image stimulus), and perturbations that replaced the initial content rather than complementing it (such as the disappearing bus in the bus+summer perturbation). To provide a more systematic evaluation of compositionality, we quantitatively measured the presence of both the perturbation concept and the base image concepts in the decoded images from perturbed brain patterns, by leveraging cosine similarity in the CLIP latent space as a measure of semantic content (since this metric is shown to be aligned with human judgments on image similarity<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a>,<a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>).</p>
<p id="Par17">This quantitative evaluation of the decoded perturbed images reveals clear trends in the similarity between the images and two targets: the original image and the concept used for perturbation. In the bottom panel of Fig. <a href="#Fig5" class="usa-link">5</a>, which assesses the similarity (in the CLIP-vision latent space) between the decoded perturbed images and the original images, we observe that similarity decreases as the scaling value increases. This is expected, since larger scaling values apply a stronger perturbation, causing more deviation from the original image. The similarity between the decoded perturbed images and the original images remains significantly above the baseline for all conditions (calculated by contrasting decoded images against randomly chosen base images), indicating that elements of the original image are still retained even as the perturbation intensifies.</p>
<figure class="fig xbox font-sm" id="Fig5"><h3 class="obj_head">Fig. 5.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373870_42003_2025_8706_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8de9/12373870/3ac184135f63/42003_2025_8706_Fig5_HTML.jpg" loading="lazy" id="d33e465" height="999" width="744" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Top</strong>: Average similarity between decoded perturbed images and the target concept (across CLIP-vision and CLIP-text latent spaces, respectively) across different thresholds (0, 25, 50, 75, 90, 95th percentiles of the voxel distribution) and scaling values (0 to 4, with zero corresponding to the base pattern without perturbation). The similarity increases with higher scaling values, reflecting that larger perturbations align the decoded image more closely with the target concept. The green shaded region represents variability (standard deviations across 4 subjects, 100 images per concept, 12 concepts) in similarity, while the red dashed line represents the baseline similarity between random images and the target concept. <strong>Bottom</strong>: Average similarity (in the CLIP-vision latent space) between decoded perturbed images and the original images across the same thresholds and scaling values. The similarity decreases as the scaling value increases, indicating that larger perturbations deviate more from the original image. The red dashed line shows the baseline similarity between the original image and random images.These results indicate a trade-off between maintaining original image features and introducing conceptual modifications, depending on the scaling value and threshold.</p></figcaption></figure><p id="Par18">The top panel of Fig. <a href="#Fig5" class="usa-link">5</a>, which compares the decoded perturbed images to the target concept (across CLIP-vision and CLIP-text latent spaces, respectively), shows the opposite trend: similarity increases as the scaling value rises. This suggests that larger scaling values make the images more representative of the added concept. Lower thresholds allow the perturbation to have a broader effect, leading to faster increases in similarity to the target concept, while higher thresholds limit the impact of the perturbation, resulting in slightly more modest increases. In all cases, the similarity between the decoded images and the target concept remains consistently above the baseline (calculated by contrasting random images with the target concept), demonstrating that the perturbation effectively introduces the desired conceptual information into the images. Interestingly, even at high thresholds, where only a small portion of the brain’s activity is perturbed, we still observe notable changes in similarity to the target concept. The fact that meaningful conceptual shifts occur even when the perturbation is restricted to higher thresholds indicates that the brain might encode these abstract concepts in localized areas, and only subtle changes in activity within these regions are required to reflect concept-driven modifications in the decoded images. Overall, these results illustrate a trade-off between maintaining similarity to the original image and introducing conceptual modifications. As scaling increases, the perturbed images deviate more from the original content but become more aligned with the target concept. The thresholding mechanism provides a way to control the extent of the perturbation, with higher thresholds preserving more of the original image and lower thresholds allowing for greater conceptual compositionality.</p>
<p id="Par19">While our focus is on subject-specific decoding, we note that the compositional effects observed in our perturbation experiments generalize across the four participants included in our study. This is evident both in the consistency of decoding trends and in the qualitative similarity of reconstructed outputs across subjects (see Fig. <a href="#Fig5" class="usa-link">5</a>).</p>
<p id="Par20">To further explore the spatial characteristics of conceptual perturbations, we visualized the top 10% most active voxels (thresholded at the 90th percentile) for each of the 12 semantic concepts in a representative subject (Fig. <a href="#Fig6" class="usa-link">6</a>). These maps reveal that different concepts elicit distinct spatial patterns in visual cortex. Scene-related concepts such as indoor and outdoor show broad bilateral activation in occipital and parahippocampal regions, while social categories like man, woman, and crowded engage more lateral and ventral regions, consistent with areas implicated in face and body perception (e.g., FFA, EBA). Emotion-related concepts such as happy and sad exhibit more diffuse patterns, yet still evoke reproducible changes in localized patches. Importantly, conceptually opposing categories (e.g., summer vs. winter, crowded vs. empty) result in spatially distinct perturbation maps, suggesting that these brain-based concept representations are separable and consistent with a compositional structure. These findings are in line with our hypothesis that the brain can perform algebraic operations within concept-specific subspaces of neural representation. What appears to matter is not solely the anatomical localization of activity, but rather the patterned distribution of neural responses across the voxel space. This supports the notion that conceptual information is embedded in a vectorial format, enabling systematic operations akin to those observed in semantic embeddings and artificial models of compositionality.</p>
<figure class="fig xbox font-sm" id="Fig6"><h3 class="obj_head">Fig. 6. Spatial distribution of the top 10% most active voxels (90th percentile) for each concept for Subj01, visualized on cortical surfaces.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373870_42003_2025_8706_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8de9/12373870/1a6a56d2cede/42003_2025_8706_Fig6_HTML.jpg" loading="lazy" id="d33e488" height="999" width="653" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Distinct patterns emerge across categories---e.g., scene-related ("indoor", “outdoor"), social ("man", “woman", “crowded"), and emotional ("happy", “sad")---highlighting the diversity and specificity of conceptual representations in brain space.</p></figcaption></figure></section><section id="Sec3"><h2 class="pmc_sec_title">Discussion</h2>
<p id="Par21">The findings from this study provide a promising indication of compositionality in neural representations. By manipulating brain patterns in a “brain algebra” framework—combining a base neural state with a thresholded and scaled perturbation vector—we observed distinct, meaningful changes in the decoded images. This suggests that visual processing in the brain may follow a compositional structure, much like language, where basic elements can be combined to create more complex representations. The ability to successfully decode these perturbations aligns with broader theories on compositionality in cognition, such as in language, where concepts are combined to produce new meanings (e.g., “queen” = “king” - “man” + “woman”). This parallel between vision and language highlights how the brain may generalize compositional principles across different domains of cognition, supporting flexible and dynamic perceptual and cognitive processes<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a>,<a href="#CR4" class="usa-link" aria-describedby="CR4">4</a>,<a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>.</p>
<p id="Par22">The use of natural images as stimuli is integral to our approach, as it enables the study of brain patterns in conditions that closely mimic real-world visual experiences. These images contain a variety of visual and semantic elements that reflect everyday interactions with the environment. By leveraging these stimuli, we can investigate how the brain processes complex compositional patterns that are more representative of natural vision, compared to more controlled or artificial stimuli.</p>
<p id="Par23">There are, however, important limitations to consider. While our results suggest compositionality in neural representations, our evaluation relies on a decoding model to interpret the perturbed brain patterns. Although the two brain patterns we are combining are derived from actual fMRI data, the interpretation of their combination is model-driven because it depends on the decoder’s ability to accurately reconstruct images from brain activity. This means that our conclusions are contingent upon the performance and limitations of the decoding model, and we are not directly observing brain processes in real-time but interpreting them through the lens of the model. This limits the extent to which we can confirm that similar compositional operations happen naturally in the brain. Additionally, we are constrained by the need for sufficient training data—only concepts with ample representation in the training set allow us to generate reliable perturbation vectors. As a result, our exploration of neural compositionality is bounded by the availability of data, limiting the range of concepts we can examine. Furthermore, some concepts are not orthogonal, and their representations in the training set can lead to biased perturbations in brain patterns. For instance, visual inspection shows that adding the concept of “happiness” occasionally introduces food elements, likely because in datasets like COCO, “happiness” is often associated with, or co-occurs alongside, images of food. Similarly, concepts like numerosity or emotion might also be biased due to their frequent co-occurrence with humans. As a result, applying a “crowded” perturbation may add humans to scenes with animals, even when the intended effect is only to increase the number of animals.</p>
<p id="Par24">One intriguing aspect of our results is that meaningful changes were observed even when the perturbation involved only a small subset of voxels (e.g., the top 5% of voxels). This suggests that relatively small, localized regions of the brain can significantly influence the representation of specific concepts. This finding contributes to the ongoing debate between distributed and localized cortical representations in visual processing<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a>–<a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>. On the one hand, proponents of distributed representations, such as Haxby and colleagues, argue that visual information is encoded across widespread patterns of neural activity, with object and category information represented in distributed and overlapping voxel patterns<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>. On the other hand, researchers like Kanwisher propose that certain visual categories are processed in specialized, localized cortical regions—for example, the fusiform face area (FFA) for face perception<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>. Our observation that concept perturbations are effective even when modifying only a small portion of voxels aligns with the idea that specific cortical areas play a crucial role in representing certain concepts. These perturbations, when decoded, yield image reconstructions that reflect conceptually altered visual content, suggesting a shift in perceptual representation as inferred by the decoding model.</p>
<p id="Par25">These findings invite a nuanced interpretation of how compositionality and modularity coexist in neural coding. While compositionality typically refers to operations within a single high-dimensional space, our results suggest that different concepts may be encoded in partially distinct, functionally specialized subspaces within that space. This modular organization does not preclude algebraic manipulation but rather supports it: perturbations restricted to voxels most strongly associated with a concept can still generate coherent semantic transformations when combined with unrelated base patterns. This suggests that the brain’s representational geometry may be composed of overlapping but structured subspaces that enable both modular specialization and compositional generalization.</p>
<p id="Par26">In summary, our findings provide evidence suggesting compositionality in the brain’s processing of visual stimuli. This compositional structure could be key to understanding how the brain flexibly combines sensory information to form different percepts, furthering our understanding of neural coding, perception, and learning.</p></section><section id="Sec4"><h2 class="pmc_sec_title">Conclusions</h2>
<p id="Par27">In this study, we explored the compositionality of neural representations through the novel framework of “brain algebra," combining base fMRI patterns with conceptual perturbations to decode visual representations. Our findings provide evidence that the brain may employ compositional mechanisms similar to those seen in language and cognition, where smaller elements are combined to form more complex representations. The results demonstrate that neural patterns can be manipulated to create distinct and predictable changes in decoded images, aligning with the target concepts, even when only small portions of brain activity are perturbed.</p>
<p id="Par28">The findings suggest that neural representations of visual concepts involve both localized and distributed processing. The ability to change perceived images by modifying a small number of voxels indicates that certain brain regions are specialized for processing specific visual information, supporting the idea of localized specialization. However, these localized changes also integrate with broader neural networks, aligning with the view that visual perception involves distributed representations. This dual role of localized and distributed coding contributes to the debate in neuroscience and underscores the complexity of how the brain processes and combines visual information.</p>
<p id="Par29">Overall, this work contributes to a deeper understanding of neural compositionality in vision and highlights the brain’s capacity to integrate conceptual information. By offering new perspectives on how perturbations in neural space correspond to changes in perception, this research provides a foundation for future studies on neural coding, perceptual constancy, and cognitive flexibility. Expanding this line of inquiry could also reveal insights into how the brain composes and generalizes across other cognitive domains, such as language and broader semantic representations. This suggests that the principles underlying “brain algebra” in vision might extend to other brain functions, hence providing a more comprehensive framework for understanding compositional processes in neural representations.</p></section><section id="Sec5"><h2 class="pmc_sec_title">Methods</h2>
<p id="Par30">In this section, we describe the proposed method and the data we used. The data are publicly available and can be requested at <a href="https://naturalscenesdataset.org/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://naturalscenesdataset.org/</a>. All experiments and models were trained on a server equipped with eight NVIDIA A100 GPU cards (80GB RAM each connected through NVLINK) and 2 TB of System RAM.</p>
<section id="Sec6"><h3 class="pmc_sec_title">Data</h3>
<p id="Par31">The study employs the Natural Scenes Dataset (NSD)<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>, an extensive fMRI dataset gathered from eight participants who were shown images from the COCO21 dataset. Our analysis focused on four subjects, resulting in a specialized training set containing 8859 images and 24,980 fMRI trials per subject, as well as a shared dataset consisting of 982 images and 2770 trials per subject. To reduce the spatial dimensionality of the fMRI signals (with a resolution of 1.8mm isotropic), we applied a mask using the provided NSDGeneral ROI, targeting multiple visual areas. This deliberate selection of ROIs improved the signal-to-noise ratio and reduced data complexity, enabling the investigation of both low- and high-level visual features. Temporal dimensionality was further minimized by leveraging precomputed betas derived from a general linear model (GLM;<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a>,<a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>) with an adjusted hemodynamic response function (HRF) and a denoising process as detailed in the NSD publication.</p></section><section id="Sec7"><h3 class="pmc_sec_title">BrainDiffuser</h3>
<p id="Par32">The “Brain-Diffuser” model<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup> is a two-stage framework designed to reconstruct natural scenes from fMRI signals. In the first stage, a Very Deep Variational Autoencoder (VDVAE) generates an “initial guess” of the reconstruction, capturing low-level details. This guess is then refined using high-level semantic features from CLIP-Text and CLIP-Vision models, and a latent diffusion model (Versatile Diffusion;<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>) is used for the final image generation. The model takes fMRI signals as input and produces reconstructed images that reflect both low-level properties and the overall scene layout. Brain-Diffuser, was trained subject-wise with data from Subj01, Subj02, Subj05, Subj07). More information about the decoding model is detailed in the original paper. While this specific pipeline is used in our study, our proposed method is universally applicable and can enhance any single-subject decoding pipeline. It offers a versatile, adaptable tool that can seamlessly integrate with novel, advanced pipelines. By focusing on preprocessing input data, our approach enables the underlying pipeline—regardless of its unique aspects—to effectively work with single-subject fMRI data to generate images, without requiring direct modifications to the pipeline itself.</p></section><section id="Sec8"><h3 class="pmc_sec_title">Main experiment</h3>
<p id="Par33">Here we outline our main experiment, which aims to decode synthetic brain patterns derived from the algebraic sum of real brain patterns, and examine compositionality in the decoded images. The NSD dataset provides paired fMRI data and corresponding images. Let’s define fMRI data as <em>z</em> and images as <em>x</em>, giving us a training set of pairs (<em>z</em><sub><em>t</em><em>r</em></sub>, <em>x</em><sub><em>t</em><em>r</em></sub>) and a test set (<em>z</em><sub><em>t</em><em>s</em></sub>, <em>x</em><sub><em>t</em><em>s</em></sub>). Additionally, we have a decoder <em>d</em>, a function that maps <em>z</em> to <em>x</em>, such that <em>x</em> ≈ <em>d</em>(<em>z</em>).</p>
<p id="Par34">The essence of our work is as follows: we explore the outcome when decoding <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><math id="d33e640"><msup><mrow><mi>z</mi></mrow><mrow><mo>′</mo></mrow></msup></math></span>, defined as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><math id="d33e653"><msup><mrow><mi>z</mi></mrow><mrow><mo>′</mo></mrow></msup><mo>=</mo><msub><mrow><mi>z</mi></mrow><mrow><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow><mi>z</mi></mrow><mrow><mi>p</mi><mi>e</mi><mi>r</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>b</mi></mrow></msub></math></span>, where <em>α</em> is a scaling factor, <em>z</em><sub><em>b</em><em>a</em><em>s</em><em>e</em></sub> is a brain pattern drawn from the test set, and <em>z</em><sub><em>p</em><em>e</em><em>r</em><em>t</em><em>u</em><em>r</em><em>b</em></sub> is a perturbation pattern computed by averaging training set brain patterns associated with a specific concept. One way to investigate compositionality in the brain is to hypothesize that the resulting image, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><math id="d33e722"><msup><mrow><mi>x</mi></mrow><mrow><mo>′</mo></mrow></msup><mo>=</mo><mi>d</mi><mrow><mo>(</mo><mrow><msup><mrow><mi>z</mi></mrow><mrow><mo>′</mo></mrow></msup></mrow><mo>)</mo></mrow></math></span>, represents a combination of the base image <em>x</em><sub><em>b</em><em>a</em><em>s</em><em>e</em></sub> and an additional concept.</p>
<p id="Par35">For instance, if the base pattern <em>z</em><sub><em>b</em><em>a</em><em>s</em><em>e</em></sub> corresponds to an image of an indoor scene <em>x</em><sub><em>b</em><em>a</em><em>s</em><em>e</em></sub>, and we add a perturbation brain pattern <em>z</em><sub><em>p</em><em>e</em><em>r</em><em>t</em><em>u</em><em>r</em><em>b</em></sub> related to the concept of a <em>man</em>, then decoding <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><math id="d33e806"><msup><mrow><mi>z</mi></mrow><mrow><mo>′</mo></mrow></msup></math></span> might produce an image of a <em>man in an indoor scene</em>. Similarly, if the perturbation pattern corresponds to a <em>woman</em>, we might expect the decoded image to depict a <em>woman</em> in the scene, and so on. We tested our framework for a random subset (identical for all subjects) of 100 images drawn from the test set.</p>
<section id="Sec9"><h4 class="pmc_sec_title">Pattern definition and thresholding</h4>
<p id="Par36">A natural question arises: how do we define the perturbation pattern relative to a specific concept? We adopted a straightforward approach by filtering the image-fMRI pairs in the training dataset based on their similarity to the concept, which is defined in natural language and measured using a CLIP-based cosine similarity.</p>
<p id="Par37">We explored 12 different semantic concepts: ["man", “woman", “indoor", “outdoor", “summer", “winter", “day", “night", “crowded", “empty", “happy", “sad"]. These concepts encompass themes such as season, gender, lighting, numerosity, emotions.</p>
<p id="Par38">Each concept was represented as a word and encoded using the CLIP Text model<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>, producing a 512-dimensional representation (using version <em>clip-vit-base-patch32</em>). We then used the CLIP Vision encoder to encode all the images in the training set and calculated the cosine similarity between each image and all the concepts. This resulted in a similarity matrix of shape (8859, 12). For each concept, we selected the top 100 pairs with the highest similarity scores to extract the corresponding fMRI and image indices. The perturbation patterns were then defined by averaging the fMRI patterns associated with these top-100 pairs, thereby establishing their representation in brain space.</p>
<p id="Par39">We applied various threshold values to the perturbation patterns based on their percentile values. Specifically, in our experiments, we evaluated the outcomes by thresholding <em>z</em><sub><em>p</em><em>e</em><em>r</em><em>t</em><em>u</em><em>r</em><em>b</em></sub> and retaining only the values above the 0th, 25th, 50th, 75th, 90th, or 95th percentiles. This allows us to assess whether the representation of the chosen broad semantic concepts is distributed across the entire visual cortex or if only a small region is responsible for encoding changes, with the composition of values in these small regions being sufficient to produce compositional images when decoded. Importantly, it is worth emphasizing that all compositional operations in our framework are performed directly in brain space—that is, on real fMRI-derived neural patterns—before any decoding takes place. This distinction, coupled with sparsity and non-linearity introduced by thresholding procedure avoids circularity and ensures that any emerging semantic effects are a result of the structure present in the fMRI data itself, rather than learned priors in the decoding model. Moreover, our use of voxel-wise thresholding enables us to probe whether concept representations are localized or distributed, and to what extent localized subspaces are sufficient for compositional decoding. This design choice allows us to explore the neural geometry underlying compositionality without assuming full distribution or strict modularity. Please see Supplementary Table <a href="#MOESM2" class="usa-link">1</a> for an assessment of Brain-Diffuser performances on threhsolded patterns and <a href="#MOESM2" class="usa-link">Supplementary Figs.</a> for activation patterns for all concepts and subjects.</p></section></section><section id="Sec10"><h3 class="pmc_sec_title">Evaluation</h3>
<p id="Par40">The first part of our evaluation is qualitative, focusing on visually assessing decoded images from perturbed brain patterns to examine the compositionality of the original stimulus image and the perturbation concept. However, a quantitative measure is necessary to rigorously evaluate this compositionality. Compositionality can be loosely defined as the co-occurrence of two concepts within an image. In our framework, we adopt a practical working definition of compositionality that aligns with approaches in large-scale neural models such as CLIP or GPT. Specifically, we define compositionality as the ability to algebraically combine neural patterns associated with distinct concepts to produce a new, coherent representation that reflects both components. This does not imply formal semantic compositionality in the linguistic sense (i.e., deriving a compound meaning strictly from parts and syntax), but rather refers to the integration of multiple conceptual features into a unified and plausible perceptual scene. Thus, we need to quantify how closely the decoded image resembles the target perturbation concept while retaining similarity to the original content. If the scaling factor <em>α</em> is too large, the perturbation may replace the original content entirely, leading to misleading results if we only measure the similarity between the decoded images and the perturbation concept.</p>
<p id="Par41">To address this, we calculated the CLIP cosine similarity between the decoded perturbed images and the original stimuli to ensure that the original content was not entirely replaced. Simultaneously, we measured the CLIP cosine similarity between the decoded perturbed images and the perturbation concept. In the first case, we measured cosine similarity between images, while in the second case, the similarity was measured between images and text. As these two metrics may have different baselines, we also computed a random baseline by measuring the cosine similarity between each base image and 100 randomly selected images from the training set. Similarly, we established a baseline for each concept using 100 random images.</p>
<p id="Par42">Finally, we averaged the results as a function of the scaling factor <em>α</em> for each threshold across all decoded images and subjects.</p></section><section id="Sec11"><h3 class="pmc_sec_title">Statistics and Reproducibility</h3>
<p id="Par43">All experiments were conducted using data from four participants in the publicly available Natural Scenes Dataset (NSD). For each subject, the training set included 24,980 fMRI trials corresponding to 8859 unique images, and the test set included 2770 fMRI trials for 982 unique images. A fixed random subset of 100 test images was used consistently across subjects in the main decoding experiments. For each semantic concept, perturbation vectors were computed by averaging the fMRI patterns of the top 100 training images most semantically related to the target concept (measured via cosine similarity in CLIP-embedding space). Quantitative analyses were based on computing cosine similarity in the CLIP latent space between the decoded images and both the original base images and the textual representations of target concepts. These measurements were averaged across subjects, concepts, and test images, and variability was reported as standard deviation across the four subjects. No formal statistical hypothesis testing (e.g., <em>p</em>-values or confidence intervals) was conducted, as the objective was to characterize systematic decoding trends rather than population-level inference.</p>
<p id="Par44">Reproducibility is supported by the use of a public dataset (NSD), standard pretrained models (CLIP and Brain-Diffuser), and a fixed experimental pipeline. All custom code and scripts used for preprocessing, perturbation generation, and decoding will be made publicly available upon publication.</p></section><section id="Sec12"><h3 class="pmc_sec_title">Reporting summary</h3>
<p id="Par45">Further information on research design is available in the <a href="#MOESM6" class="usa-link">Nature Portfolio Reporting Summary</a> linked to this article.</p></section></section><section id="Sec13"><h2 class="pmc_sec_title">Supplementary information</h2>
<section class="sm xbox font-sm" id="MOESM1"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM1_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Transparent Peer Review file</a><sup> (187.7KB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="MOESM2"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM2_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material</a><sup> (1.5MB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="MOESM3"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM3_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">42003_2025_8706_MOESM3_ESM.pdf</a><sup> (18.7KB, pdf) </sup><p>Description of additional supplementary file</p>
</div></div></section><section class="sm xbox font-sm" id="MOESM4"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM4_ESM.csv" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Data 1</a><sup> (1.5KB, csv) </sup>
</div></div></section><section class="sm xbox font-sm" id="MOESM5"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM5_ESM.csv" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Data 2</a><sup> (1.5KB, csv) </sup>
</div></div></section><section class="sm xbox font-sm" id="MOESM6"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM6_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Reporting summary</a><sup> (2.6MB, pdf) </sup>
</div></div></section></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>This work was supported by NEXTGENERATIONEU (NGEU) and funded by the Italian Ministry of University and Research (MUR), National Recovery and Resilience Plan (NRRP), project MNESYS (PE0000006) (to NT)- A Multiscale integrated approach to the study of the nervous system in health and disease (DN. 1553 11.10.2022); by the MUR-PNRR M4C2I1.3 PE6 project PE00000019 Heal Italia (to NT); by the NATIONAL CENTRE FOR HPC, BIG DATA AND QUANTUM COMPUTING, within the spoke “Multiscale Modeling and Engineering Applications” (to NT); the EXPERIENCE project (European Union’s Horizon 2020 Research and Innovation Programme under grant agreement No. 101017727); the CROSSBRAIN project (European Union’s European Innovation Council under grant agreement No. 101070908), ANITI Chair (ANR grant ANR-19-PI3A-004), an ANR grant AI-REPS ANR-18-CE37-0007-01 and an ERC Advanced Grant GLoW (grant 101096017) to RV.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>M.F. and R.V.R. conceived the study. M.F. implemented the code, conducted the analyses, and drafted the initial manuscript. T.B. contributed to the interpretation of results and critically revised the manuscript. R.V.R. and N.T. provided scientific guidance, mentorship, and overall supervision throughout the project. All authors reviewed and approved the final version of the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Peer review</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Peer review information</h3>
<p id="Par46"><em>Communications Biology</em> thanks Olaf Hauk and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Primary Handling Editor: Jasmine Pan. A peer review file is available.</p></section></section><section id="notes3"><h2 class="pmc_sec_title">Data availability</h2>
<p>The data supporting the findings of this study are publicly available. The fMRI data and corresponding image stimuli were obtained from the Natural Scenes Dataset (NSD)<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>. All preprocessing scripts and derived datasets used in the analyses will be public released upon acceptance. Supplementary Data <a href="#MOESM4" class="usa-link">1</a> and Supplementary Data <a href="#MOESM5" class="usa-link">2</a> provide numerical sources to reproduce Fig. <a href="#Fig5" class="usa-link">5</a>.</p></section><section id="notes4"><h2 class="pmc_sec_title">Code availability</h2>
<p>The custom code used for preprocessing fMRI data, generating brain perturbation patterns, and performing image decoding with the Brain-Diffuser model will be made publicly available upon acceptance of this manuscript at this link: <a href="https://github.com/matteoferrante/BrainAlgebra" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/matteoferrante/BrainAlgebra</a>. The Brain-Diffuser model itself is described in detail in its original publication<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>.</p></section><section id="FPar2"><h2 class="pmc_sec_title">Competing interests</h2>
<p id="Par47">The authors declare no competing interests.</p></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm">
<div class="fn p" id="fn1"><p><strong>Publisher’s note</strong> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>
<div class="fn p" id="fn2"><p>These authors contributed equally: Nicola Toschi, Rufin VanRullen.</p></div>
</div></section><section id="_ci93_" lang="en" class="contrib-info"><h2 class="pmc_sec_title">Contributor Information</h2>
<p>Matteo Ferrante, Email: matteo.ferrante@uniroma2.it.</p>
<p>Tommaso Boccato, Email: tommaso.boccato@uniroma2.it.</p>
<p>Nicola Toschi, Email: toschi@med.uniroma2.it.</p>
<p>Rufin VanRullen, Email: rufin.vanrullen@cnrs.fr.</p></section><section id="sec14"><h2 class="pmc_sec_title">Supplementary information</h2>
<p>The online version contains supplementary material available at 10.1038/s42003-025-08706-4.</p></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Mikolov, T., Yih, W.-t. &amp; Zweig, G. Linguistic regularities in continuous space word representations. In Vanderwende, L., Daumé III, H. &amp; Kirchhoff, K. (eds.) <em>Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 746–751 (Association for Computational Linguistics, Atlanta, Georgia, 2013). <a href="https://aclanthology.org/N13-1090" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://aclanthology.org/N13-1090</a>.</cite>
</li>
<li id="CR2">
<span class="label">2.</span><cite>Lepori, M. A., Serre, T. &amp; Pavlick, E. Break it down: Evidence for structural compositionality in neural networks <a href="https://arxiv.org/abs/2301.10884" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2301.10884</a> (2023).</cite>
</li>
<li id="CR3">
<span class="label">3.</span><cite>Lake, B. M. &amp; Baroni, M. Human-like systematic generalization through a meta-learning neural network. <em>Nature</em><strong>623</strong>, 115–121 (2023).
</cite> [<a href="https://doi.org/10.1038/s41586-023-06668-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10620072/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37880371/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Lake,%20B.%20M.%20&amp;%20Baroni,%20M.%20Human-like%20systematic%20generalization%20through%20a%20meta-learning%20neural%20network.%20Nature623,%20115%E2%80%93121%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR4">
<span class="label">4.</span><cite>Russin, J., McGrath, S. W., Williams, D. J. &amp; Elber-Dorozko, L. From frege to chatgpt: Compositionality in language, cognition, and deep neural networks <a href="https://arxiv.org/abs/2405.15164" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2405.15164</a> (2024).</cite>
</li>
<li id="CR5">
<span class="label">5.</span><cite>Shi, C. et al. Exploring compositional visual generation with latent classifier guidance <a href="https://arxiv.org/abs/2304.12536" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2304.12536</a> (2023).</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>Radford, A. et al. Learning transferable visual models from natural language supervision (2021).</cite>
</li>
<li id="CR7">
<span class="label">7.</span><cite>Goh, G. et al. Multimodal neurons in artificial neural networks. <em>Distill</em><a href="https://distill.pub/2021/multimodal-neurons" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://distill.pub/2021/multimodal-neurons</a> (2021).</cite>
</li>
<li id="CR8">
<span class="label">8.</span><cite>Oota, S. R. et al. Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey) <a href="http://arxiv.org/abs/2307.10246" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2307.10246</a> (2023). ArXiv:2307.10246 [cs, q-bio].</cite>
</li>
<li id="CR9">
<span class="label">9.</span><cite>Scotti, P. S. et al. Reconstructing the mind’s eye: fmri-to-image with contrastive learning and diffusion priors (2023).</cite>
</li>
<li id="CR10">
<span class="label">10.</span><cite>Ozcelik, F. &amp; VanRullen, R. Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion (2023).</cite> [<a href="https://doi.org/10.1038/s41598-023-42891-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10511448/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37731047/" class="usa-link">PubMed</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>Caucheteux, C. &amp; King, J. Brains and algorithms partially converge in natural language processing. <em>Commun. Biol.</em><strong>5</strong>, 134 (2022).
</cite> [<a href="https://doi.org/10.1038/s42003-022-03036-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8850612/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35173264/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Caucheteux,%20C.%20&amp;%20King,%20J.%20Brains%20and%20algorithms%20partially%20converge%20in%20natural%20language%20processing.%20Commun.%20Biol.5,%20134%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Tang, J. et al. Semantic reconstruction of continuous language from non-invasive brain recordings. <em>Nat. Neurosci.</em><strong>26</strong>, 858–866 (2023).
</cite> [<a href="https://doi.org/10.1038/s41593-023-01304-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11304553/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37127759/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Tang,%20J.%20et%20al.%20Semantic%20reconstruction%20of%20continuous%20language%20from%20non-invasive%20brain%20recordings.%20Nat.%20Neurosci.26,%20858%E2%80%93866%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>Huth, A., Nishimoto, S., Vu, A. &amp; Gallant, J. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. <em>Neuron</em><strong>76</strong>, 1210–1224 (2012).
</cite> [<a href="https://doi.org/10.1016/j.neuron.2012.10.014" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3556488/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23259955/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Huth,%20A.,%20Nishimoto,%20S.,%20Vu,%20A.%20&amp;%20Gallant,%20J.%20A%20continuous%20semantic%20space%20describes%20the%20representation%20of%20thousands%20of%20object%20and%20action%20categories%20across%20the%20human%20brain.%20Neuron76,%201210%E2%80%931224%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Ferrante, M., Boccato, T. &amp; Toschi, N. Semantic brain decoding: from fmri to conceptually similar image reconstruction of visual stimuli (2023).</cite>
</li>
<li id="CR15">
<span class="label">15.</span><cite>Ferrante, M., Boccato, T., Ozcelik, F., VanRullen, R. &amp; Toschi, N. Multimodal decoding of human brain activity into images and text. In <em>UniReps: the First Workshop on Unifying Representations in Neural Models</em><a href="https://openreview.net/forum?id=rGCabZfV3d" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=rGCabZfV3d</a> (2023).</cite>
</li>
<li id="CR16">
<span class="label">16.</span><cite>Ferrante, M., Boccato, T. &amp; Toschi, N. Through their eyes: multi-subject brain decoding with simple alignment techniques (2023).</cite> [<a href="https://doi.org/10.1162/imag_a_00170" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC12247601/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40800361/" class="usa-link">PubMed</a>]</li>
<li id="CR17">
<span class="label">17.</span><cite>Antonello, R., Vaidya, A. &amp; Huth, A. G. Scaling laws for language encoding models in fmri (2023).</cite> [<a href="/articles/PMC11258918/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39035676/" class="usa-link">PubMed</a>]</li>
<li id="CR18">
<span class="label">18.</span><cite>Caucheteux, C., Gramfort, A. &amp; King, J. Evidence of a predictive coding hierarchy in the human brain listening to speech. <em>Nat. Hum. Behav.</em><strong>7</strong>, 430–441 (2023).
</cite> [<a href="https://doi.org/10.1038/s41562-022-01516-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10038805/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36864133/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Caucheteux,%20C.,%20Gramfort,%20A.%20&amp;%20King,%20J.%20Evidence%20of%20a%20predictive%20coding%20hierarchy%20in%20the%20human%20brain%20listening%20to%20speech.%20Nat.%20Hum.%20Behav.7,%20430%E2%80%93441%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19">
<span class="label">19.</span><cite>Takagi, Y. &amp; Nishimoto, S. High-resolution image reconstruction with latent diffusion models from human brain activity. <em>bioRxiv</em><a href="https://www.biorxiv.org/content/early/2023/03/11/2022.11.18.517004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.biorxiv.org/content/early/2023/03/11/2022.11.18.517004</a> (2023).</cite>
</li>
<li id="CR20">
<span class="label">20.</span><cite>Chen, Z., Qing, J., Xiang, T., Yue, W. L. &amp; Zhou, J. H. Seeing beyond the brain: conditional diffusion model with sparse masked modeling for vision decoding (2022).</cite>
</li>
<li id="CR21">
<span class="label">21.</span><cite>Lin, S., Sprague, T. &amp; Singh, A. K. Mind reader: Reconstructing complex images from brain activities (2022).</cite>
</li>
<li id="CR22">
<span class="label">22.</span><cite>Ferrante, M., Ciferri, M. &amp; Toschi, N. R&amp;b – rhythm and brain: cross-subject decoding of music from human brain activity <a href="https://arxiv.org/abs/2406.15537" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2406.15537</a> (2024).</cite>
</li>
<li id="CR23">
<span class="label">23.</span><cite>Choksi, B., Mozafari, M., VanRullen, R. &amp; Reddy, L. Multimodal neural networks better explain multivoxel patterns in the hippocampus. <em>Neural Netw.</em><strong>154</strong>, 538–542 (2022).
</cite> [<a href="https://doi.org/10.1016/j.neunet.2022.07.033" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35995019/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Choksi,%20B.,%20Mozafari,%20M.,%20VanRullen,%20R.%20&amp;%20Reddy,%20L.%20Multimodal%20neural%20networks%20better%20explain%20multivoxel%20patterns%20in%20the%20hippocampus.%20Neural%20Netw.154,%20538%E2%80%93542%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Antonello, R. &amp; Huth, A. Predictive coding or just feature discovery? an alternative account of why language models fit brain data. <em>Neurobiol. Lang.</em><strong>5</strong>, 64–79 (2024).</cite> [<a href="https://doi.org/10.1162/nol_a_00087" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11025645/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38645616/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Antonello,%20R.%20&amp;%20Huth,%20A.%20Predictive%20coding%20or%20just%20feature%20discovery?%20an%20alternative%20account%20of%20why%20language%20models%20fit%20brain%20data.%20Neurobiol.%20Lang.5,%2064%E2%80%9379%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR25">
<span class="label">25.</span><cite>Gifford, A. T. et al. The algonauts project 2023 challenge: How the human brain makes sense of natural scenes (2023).</cite>
</li>
<li id="CR26">
<span class="label">26.</span><cite>Piantadosi, S. T. et al. Why concepts are (probably) vectors. <em>Trends Cogn. Sci.</em><strong>28</strong>, 844–856 (2024).
</cite> [<a href="https://doi.org/10.1016/j.tics.2024.06.011" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39112125/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Piantadosi,%20S.%20T.%20et%20al.%20Why%20concepts%20are%20(probably)%20vectors.%20Trends%20Cogn.%20Sci.28,%20844%E2%80%93856%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR27">
<span class="label">27.</span><cite>Wu, M.-H., Anderson, A. J., Jacobs, R. A. &amp; Raizada, R. D. S. Analogy-related information can be accessed by simple addition and subtraction of fMRI activation patterns, without participants performing any analogy task. <em>Neurobiol. Lang.</em><strong>3</strong>, 1–17 (2022).</cite> [<a href="https://doi.org/10.1162/nol_a_00045" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10158578/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37215331/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wu,%20M.-H.,%20Anderson,%20A.%20J.,%20Jacobs,%20R.%20A.%20&amp;%20Raizada,%20R.%20D.%20S.%20Analogy-related%20information%20can%20be%20accessed%20by%20simple%20addition%20and%20subtraction%20of%20fMRI%20activation%20patterns,%20without%20participants%20performing%20any%20analogy%20task.%20Neurobiol.%20Lang.3,%201%E2%80%9317%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Allen, E. J. et al. A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. <em>Nat. Neurosci.</em><strong>25</strong>, 116–126 (2022).
</cite> [<a href="https://doi.org/10.1038/s41593-021-00962-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34916659/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Allen,%20E.%20J.%20et%20al.%20A%20massive%207t%20fmri%20dataset%20to%20bridge%20cognitive%20neuroscience%20and%20artificial%20intelligence.%20Nat.%20Neurosci.25,%20116%E2%80%93126%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29">
<span class="label">29.</span><cite>Hernández-Cámara, P., Vila-Tomás, J., Malo, J. &amp; Laparra, V. Measuring human-CLIP alignment at different abstraction levels. In <em>ICLR 2024 Workshop on Representational Alignment</em><a href="https://openreview.net/forum?id=xQyhHjLGmj" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=xQyhHjLGmj</a> (2024).</cite>
</li>
<li id="CR30">
<span class="label">30.</span><cite>Muttenthaler, L. et al. Aligning machine and human visual representations across abstraction levels <a href="https://arxiv.org/abs/2409.06509" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2409.06509</a> (2024).</cite>
</li>
<li id="CR31">
<span class="label">31.</span><cite>Tsao, D. Y., Freiwald, W. A., Knutsen, T. A., Mandeville, J. B. &amp; Tootell, R. B. Faces and objects in macaque cerebral cortex. <em>Nat. Neurosci.</em><strong>6</strong>, 989–995 (2003).
</cite> [<a href="https://doi.org/10.1038/nn1111" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8117179/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/12925854/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Tsao,%20D.%20Y.,%20Freiwald,%20W.%20A.,%20Knutsen,%20T.%20A.,%20Mandeville,%20J.%20B.%20&amp;%20Tootell,%20R.%20B.%20Faces%20and%20objects%20in%20macaque%20cerebral%20cortex.%20Nat.%20Neurosci.6,%20989%E2%80%93995%20(2003)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Reddy, L. &amp; Kanwisher, N. Coding of visual objects in the ventral stream. <em>Curr. Opin. Neurobiol.</em><strong>16</strong>, 408–414 (2006).
</cite> [<a href="https://doi.org/10.1016/j.conb.2006.06.004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16828279/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Reddy,%20L.%20&amp;%20Kanwisher,%20N.%20Coding%20of%20visual%20objects%20in%20the%20ventral%20stream.%20Curr.%20Opin.%20Neurobiol.16,%20408%E2%80%93414%20(2006)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33">
<span class="label">33.</span><cite>Grill-Spector, K. &amp; Weiner, K. The functional architecture of the ventral temporal cortex and its role in categorization. <em>Nat. Rev. Neurosci.</em><strong>15</strong>, 536–548 (2014).
</cite> [<a href="https://doi.org/10.1038/nrn3747" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4143420/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24962370/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Grill-Spector,%20K.%20&amp;%20Weiner,%20K.%20The%20functional%20architecture%20of%20the%20ventral%20temporal%20cortex%20and%20its%20role%20in%20categorization.%20Nat.%20Rev.%20Neurosci.15,%20536%E2%80%93548%20(2014)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR34">
<span class="label">34.</span><cite>Haxby, J. V. et al. Distributed and overlapping representations of faces and objects in ventral temporal cortex. <em>Science</em><strong>293</strong>, 2425–2430 (2001).
</cite> [<a href="https://doi.org/10.1126/science.1063736" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11577229/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Haxby,%20J.%20V.%20et%20al.%20Distributed%20and%20overlapping%20representations%20of%20faces%20and%20objects%20in%20ventral%20temporal%20cortex.%20Science293,%202425%E2%80%932430%20(2001)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Kanwisher, N., McDermott, J. &amp; Chun, M. M. The fusiform face area: a module in human extrastriate cortex specialized for face perception. <em>J. Neurosci.</em><strong>17</strong>, 4302–4311 (1997).
</cite> [<a href="https://doi.org/10.1523/JNEUROSCI.17-11-04302.1997" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6573547/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/9151747/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Kanwisher,%20N.,%20McDermott,%20J.%20&amp;%20Chun,%20M.%20M.%20The%20fusiform%20face%20area:%20a%20module%20in%20human%20extrastriate%20cortex%20specialized%20for%20face%20perception.%20J.%20Neurosci.17,%204302%E2%80%934311%20(1997)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<span class="label">36.</span><cite>Prince, J. S. et al. Improving the accuracy of single-trial fmri response estimates using glmsingle. <em>eLife</em><strong>11</strong>, e77599 (2022).
</cite> [<a href="https://doi.org/10.7554/eLife.77599" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9708069/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36444984/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Prince,%20J.%20S.%20et%20al.%20Improving%20the%20accuracy%20of%20single-trial%20fmri%20response%20estimates%20using%20glmsingle.%20eLife11,%20e77599%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR37">
<span class="label">37.</span><cite>Kay, K., Rokem, A., Winawer, J., Dougherty, R. &amp; Wandell, B. Glmdenoise: a fast, automated technique for denoising task-based fmri data. <em>Frontiers in Neuroscience</em><strong>7</strong>, <a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2013.00247" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2013.00247</a> (2013).</cite> [<a href="https://doi.org/10.3389/fnins.2013.00247" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3865440/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24381539/" class="usa-link">PubMed</a>]</li>
<li id="CR38">
<span class="label">38.</span><cite>Xu, X., Wang, Z., Zhang, E., Wang, K. &amp; Shi, H. Versatile diffusion: Text, images and variations all in one diffusion model <a href="https://arxiv.org/abs/2211.08332" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2211.08332</a> (2024).</cite>
</li>
<li id="CR39">
<span class="label">39.</span><cite>Ozcelik, F. &amp; VanRullen, R. Natural scene reconstruction from fmri signals using generative latent diffusion. <em>Sci. Rep.</em><strong>13</strong>, 15666 (2023).
</cite> [<a href="https://doi.org/10.1038/s41598-023-42891-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10511448/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37731047/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ozcelik,%20F.%20&amp;%20VanRullen,%20R.%20Natural%20scene%20reconstruction%20from%20fmri%20signals%20using%20generative%20latent%20diffusion.%20Sci.%20Rep.13,%2015666%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adsm93_" lang="en" class="supplementary-materials"><h3 class="pmc_sec_title">Supplementary Materials</h3>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM1_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Transparent Peer Review file</a><sup> (187.7KB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM2_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Material</a><sup> (1.5MB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material3_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM3_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">42003_2025_8706_MOESM3_ESM.pdf</a><sup> (18.7KB, pdf) </sup><p>Description of additional supplementary file</p>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material4_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM4_ESM.csv" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Data 1</a><sup> (1.5KB, csv) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material5_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM5_ESM.csv" data-ga-action="click_feat_suppl" class="usa-link">Supplementary Data 2</a><sup> (1.5KB, csv) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material6_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373870/bin/42003_2025_8706_MOESM6_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Reporting summary</a><sup> (2.6MB, pdf) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The data supporting the findings of this study are publicly available. The fMRI data and corresponding image stimuli were obtained from the Natural Scenes Dataset (NSD)<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>. All preprocessing scripts and derived datasets used in the analyses will be public released upon acceptance. Supplementary Data <a href="#MOESM4" class="usa-link">1</a> and Supplementary Data <a href="#MOESM5" class="usa-link">2</a> provide numerical sources to reproduce Fig. <a href="#Fig5" class="usa-link">5</a>.</p>
<p>The custom code used for preprocessing fMRI data, generating brain perturbation patterns, and performing image decoding with the Brain-Diffuser model will be made publicly available upon acceptance of this manuscript at this link: <a href="https://github.com/matteoferrante/BrainAlgebra" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/matteoferrante/BrainAlgebra</a>. The Brain-Diffuser model itself is described in detail in its original publication<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Communications Biology are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s42003-025-08706-4"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/42003_2025_Article_8706.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (3.1 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12373870/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12373870/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373870%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373870/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12373870/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12373870/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40847014/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12373870/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40847014/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12373870/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12373870/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="EwbmCpYYrmksDAfpQCNUOAA9PzYUDJHxKAPXFVllYwFTUMIP8iOuFv0MxbW7UNX5">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-574fdcc6.js"></script>
    
    

    </body>
</html>
