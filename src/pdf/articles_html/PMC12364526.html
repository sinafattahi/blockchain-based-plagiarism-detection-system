
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Generative Artificial Intelligence in the Metaverse Era: A Review on Models and Applications - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4A2668AF1E83305A266002E0A78D2.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="research">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364526/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Research">
<meta name="citation_title" content="Generative Artificial Intelligence in the Metaverse Era: A Review on Models and Applications">
<meta name="citation_author" content="Han Zhou">
<meta name="citation_author_institution" content="School of Instrumentation and Optoelectronic Engineering, Beihang University, Beijing 100191, China.">
<meta name="citation_author" content="Xinyi Chen">
<meta name="citation_author_institution" content="School of Instrumentation and Optoelectronic Engineering, Beihang University, Beijing 100191, China.">
<meta name="citation_author" content="Jin Li">
<meta name="citation_author_institution" content="School of Instrumentation and Optoelectronic Engineering, Beihang University, Beijing 100191, China.">
<meta name="citation_author" content="Zichen Zhang">
<meta name="citation_author_institution" content="National Key Lab of Spintronics, School of Integrated Circuit Science and Engineering, Beihang University, Beijing 100191, China.">
<meta name="citation_author_institution" content="Shanxi Key Laboratory of Advanced Semiconductor Optoelectronic Devices and Integrated Systems, Shanxi 048000, China.">
<meta name="citation_author" content="Yao Fu">
<meta name="citation_author_institution" content="The Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, Changchun 130000, China.">
<meta name="citation_author" content="Mailyn Pérez Liva">
<meta name="citation_author_institution" content="Department of Structure of Matter, Thermal Physics and Electronics, CEI Moncloa, Universidad Complutense de Madrid, 28040 Madrid, Spain.">
<meta name="citation_author" content="Dov Greenbaum">
<meta name="citation_author_institution" content="Department of Molecular Biophysics and Biochemistry, Yale University, New Haven, CT 06520, USA.">
<meta name="citation_author" content="Pan Hui">
<meta name="citation_author_institution" content="The HKUST-DT System and Media Lab, Hong Kong University of Science and Technology, Hong Kong 999077, China.">
<meta name="citation_publication_date" content="2025 Aug 19">
<meta name="citation_volume" content="8">
<meta name="citation_firstpage" content="0804">
<meta name="citation_doi" content="10.34133/research.0804">
<meta name="citation_pmid" content="40837875">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364526/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364526/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364526/pdf/research.0804.pdf">
<meta name="description" content="The Metaverse is a decentralized, immersive 3-dimensional virtual environment that merges the physical and virtual worlds, fundamentally transforming digital interaction and garnering widespread attention. However, its primary platforms face ...">
<meta name="og:title" content="Generative Artificial Intelligence in the Metaverse Era: A Review on Models and Applications">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="The Metaverse is a decentralized, immersive 3-dimensional virtual environment that merges the physical and virtual worlds, fundamentally transforming digital interaction and garnering widespread attention. However, its primary platforms face ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364526/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12364526">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.34133/research.0804"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/research.0804.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12364526%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12364526/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12364526/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364526/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-research.png" alt="Research logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Research" title="Link to Research" shape="default" href="https://spj.sciencemag.org/research/" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Research (Wash D C)</button></div>. 2025 Aug 19;8:0804. doi: <a href="https://doi.org/10.34133/research.0804" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.34133/research.0804</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Research%20(Wash%20D%20C)%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Research%20(Wash%20D%20C)%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Research%20(Wash%20D%20C)%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Research%20(Wash%20D%20C)%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Generative Artificial Intelligence in the Metaverse Era: A Review on Models and Applications</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhou%20H%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Han Zhou</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Han Zhou</span></h3>
<div class="p">
<sup><sup>1</sup></sup>School of Instrumentation and Optoelectronic Engineering, 
Beihang University, Beijing 100191, China.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhou%20H%22%5BAuthor%5D" class="usa-link"><span class="name western">Han Zhou</span></a>
</div>
</div>
<sup>1,</sup><sup>†</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20X%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Xinyi Chen</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Xinyi Chen</span></h3>
<div class="p">
<sup><sup>1</sup></sup>School of Instrumentation and Optoelectronic Engineering, 
Beihang University, Beijing 100191, China.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20X%22%5BAuthor%5D" class="usa-link"><span class="name western">Xinyi Chen</span></a>
</div>
</div>
<sup>1,</sup><sup>†</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Jin Li</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Jin Li</span></h3>
<div class="p">
<sup><sup>1</sup></sup>School of Instrumentation and Optoelectronic Engineering, 
Beihang University, Beijing 100191, China.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Jin Li</span></a>
</div>
</div>
<sup>1,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Zichen Zhang</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Zichen Zhang</span></h3>
<div class="p">
<sup><sup>2</sup></sup>National Key Lab of Spintronics, School of Integrated Circuit Science and Engineering, 
Beihang University, Beijing 100191, China.</div>
<div class="p">
<sup><sup>3</sup></sup>Shanxi Key Laboratory of Advanced Semiconductor Optoelectronic Devices and Integrated Systems, Shanxi 048000, China.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zichen Zhang</span></a>
</div>
</div>
<sup>2,</sup><sup>3,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Fu%20Y%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Yao Fu</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Yao Fu</span></h3>
<div class="p">
<sup><sup>4</sup></sup>The Changchun Institute of Optics, Fine Mechanics and Physics, 
Chinese Academy of Sciences, Changchun 130000, China.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Fu%20Y%22%5BAuthor%5D" class="usa-link"><span class="name western">Yao Fu</span></a>
</div>
</div>
<sup>4</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liva%20MP%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Mailyn Pérez Liva</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Mailyn Pérez Liva</span></h3>
<div class="p">
<sup><sup>5</sup></sup>Department of Structure of Matter, Thermal Physics and Electronics, CEI Moncloa, 
Universidad Complutense de Madrid, 28040 Madrid, Spain.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liva%20MP%22%5BAuthor%5D" class="usa-link"><span class="name western">Mailyn Pérez Liva</span></a>
</div>
</div>
<sup>5</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Greenbaum%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Dov Greenbaum</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Dov Greenbaum</span></h3>
<div class="p">
<sup><sup>6</sup></sup>Department of Molecular Biophysics and Biochemistry, 
Yale University, New Haven, CT 06520, USA.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Greenbaum%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">Dov Greenbaum</span></a>
</div>
</div>
<sup>6,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hui%20P%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Pan Hui</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Pan Hui</span></h3>
<div class="p">
<sup><sup>7</sup></sup>The HKUST-DT System and Media Lab, 
Hong Kong University of Science and Technology, Hong Kong 999077, China.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hui%20P%22%5BAuthor%5D" class="usa-link"><span class="name western">Pan Hui</span></a>
</div>
</div>
<sup>7,</sup><sup>*</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff1">
<sup><sup>1</sup></sup>School of Instrumentation and Optoelectronic Engineering, 
Beihang University, Beijing 100191, China.</div>
<div id="aff2">
<sup><sup>2</sup></sup>National Key Lab of Spintronics, School of Integrated Circuit Science and Engineering, 
Beihang University, Beijing 100191, China.</div>
<div id="aff3">
<sup><sup>3</sup></sup>Shanxi Key Laboratory of Advanced Semiconductor Optoelectronic Devices and Integrated Systems, Shanxi 048000, China.</div>
<div id="aff4">
<sup><sup>4</sup></sup>The Changchun Institute of Optics, Fine Mechanics and Physics, 
Chinese Academy of Sciences, Changchun 130000, China.</div>
<div id="aff5">
<sup><sup>5</sup></sup>Department of Structure of Matter, Thermal Physics and Electronics, CEI Moncloa, 
Universidad Complutense de Madrid, 28040 Madrid, Spain.</div>
<div id="aff6">
<sup><sup>6</sup></sup>Department of Molecular Biophysics and Biochemistry, 
Yale University, New Haven, CT 06520, USA.</div>
<div id="aff7">
<sup><sup>7</sup></sup>The HKUST-DT System and Media Lab, 
Hong Kong University of Science and Technology, Hong Kong 999077, China.</div>
<div class="author-notes p">
<div class="fn" id="corr1">
<sup>*</sup><p class="display-inline">Address correspondence to: <span>jl11269@buaa.edu.cn</span> (J.L.); <span>zz241@buaa.edu.cn</span> (Z.Z.); <span>dov.greenbaum@runi.ac.il</span> (D.G.); <span>panhui@ust.hk</span> (P.H.)</p>
</div>
<div class="fn" id="afn1">
<sup>†</sup><p class="display-inline">These authors contributed equally to this work.</p>
</div>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Feb 26; Revised 2025 Jun 20; Accepted 2025 Jul 5; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>Copyright © 2025 Han Zhou et al.</div>
<p>Exclusive licensee Science and Technology Review Publishing House. No claim to original U.S. Government Works. Distributed under a <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License (CC BY 4.0)</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12364526  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40837875/" class="usa-link">40837875</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>The Metaverse is a decentralized, immersive 3-dimensional virtual environment that merges the physical and virtual worlds, fundamentally transforming digital interaction and garnering widespread attention. However, its primary platforms face challenges such as low-quality content and underdeveloped virtual environments, leading to a subpar user experience. Artificial intelligence-generated content (AIGC) has emerged as a key driver in Metaverse development, enabling the efficient and cost-effective creation of digital content. AIGC also promotes personalized content, further enhancing the appeal of the Metaverse. Although AIGC holds great promise, comprehensive investigations into its underlying models and applications remain limited. This study begins with the core neural network architectures of generative AI and examines the relationship between the Metaverse and AIGC. It delves into the deep learning technologies that support AIGC, providing both qualitative and quantitative analyses of their advantages, limitations, and hardware constraints. We also review existing practical applications of the Metaverse, highlighting the challenges and future opportunities in key domains such as healthcare and education. The research concludes that while AIGC can markedly accelerate the development of the Metaverse, its technology must be more closely aligned with development needs to deliver a truly immersive experience. The integration of AIGC and the Metaverse represents the convergence of artificial intelligence, computer graphics, and human–computer interaction. This interdisciplinary synergy has the potential to redefine the way we create, interact with, and experience digital environments, pushing the boundaries of creativity and immersion.</p></section><section id="sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p>The Metaverse is envisioned as a persistent, immersive, and interconnected virtual environment that allows users to interact in real time with a strong sense of presence and agency [<a href="#B1" class="usa-link" aria-describedby="B1">1</a>–<a href="#B4" class="usa-link" aria-describedby="B4">4</a>]. Since the concept was formally introduced in the Metaverse Roadmap (2007) [<a href="#B5" class="usa-link" aria-describedby="B5">5</a>,<a href="#B6" class="usa-link" aria-describedby="B6">6</a>] and later brought into the spotlight by Meta in 2021, it has attracted widespread attention across academia, industry, and society. By integrating cutting-edge technologies such as virtual reality (VR), augmented reality (AR), haptic feedback, and real-time rendering, the Metaverse is poised to reshape diverse fields, including education [<a href="#B7" class="usa-link" aria-describedby="B7">7</a>], healthcare [<a href="#B8" class="usa-link" aria-describedby="B8">8</a>], e-commerce, and entertainment [<a href="#B9" class="usa-link" aria-describedby="B9">9</a>].</p>
<p>However, building such complex, dynamic virtual worlds pose a fundamental challenge: the creation of massive amounts of diverse and high-quality digital content—ranging from 3-dimensional (3D) environments and digital humans to interactive scenarios and adaptive narratives [<a href="#B10" class="usa-link" aria-describedby="B10">10</a>–<a href="#B12" class="usa-link" aria-describedby="B12">12</a>]. Traditional content creation methods are labor-intensive and time-consuming and require significant domain expertise, making them inadequate for the real-time and large-scale demands of the Metaverse [<a href="#B13" class="usa-link" aria-describedby="B13">13</a>–<a href="#B17" class="usa-link" aria-describedby="B17">17</a>].</p>
<p>In this context, generative artificial intelligence (GAI)—particularly artificial intelligence-generated content (AIGC)—has emerged as a transformative solution. AIGC has demonstrated significant impact across diverse fields such as computer vision [<a href="#B18" class="usa-link" aria-describedby="B18">18</a>–<a href="#B22" class="usa-link" aria-describedby="B22">22</a>], natural language processing (NLP) [<a href="#B23" class="usa-link" aria-describedby="B23">23</a>–<a href="#B25" class="usa-link" aria-describedby="B25">25</a>], music composition [<a href="#B18" class="usa-link" aria-describedby="B18">18</a>,<a href="#B26" class="usa-link" aria-describedby="B26">26</a>,<a href="#B27" class="usa-link" aria-describedby="B27">27</a>], and medicine discovery [<a href="#B28" class="usa-link" aria-describedby="B28">28</a>–<a href="#B30" class="usa-link" aria-describedby="B30">30</a>]—validating its capacity to synthesize complex, realistic, and semantically meaningful data. In NLP, text generation models have demonstrated their utility in applications ranging from conversational systems to content creation. For example, the OpenAI API provides access to models such as GPT-4, which are suitable for text generation, summarization, dialogue systems, and more. As shown in Fig. <a href="#F1" class="usa-link">1</a>, these capabilities not only alleviate the content scarcity problem but also lay the foundation for dynamic and immersive virtual experiences. In the context of the Metaverse, these capabilities enable the creation of highly realistic and adaptive virtual environments, making GAI a key driver in shaping the future of immersive digital worlds.</p>
<figure class="fig xbox font-sm" id="F1"><h3 class="obj_head">Fig. 1.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c7c8/12364526/2a75ccd86ee7/research.0804.fig.001.jpg" loading="lazy" height="377" width="660" alt="Fig. 1."></p>
<div class="p text-right font-secondary"><a href="figure/F1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The application of AIGC driven by deep learning models, and its use in the Metaverse. The relationship between AIGC and the Metaverse. AIGC facilitates the construction of the Metaverse, while the demands of the Metaverse drive the development of AIGC.</p></figcaption></figure><p>Yet, despite this momentum, deploying AIGC in Metaverse applications presents substantial challenges. Key issues include data quality control, model interpretability, computational efficiency, ethical governance, and cross-domain integration. Moreover, there is a lack of standardized evaluation frameworks to assess the fidelity, immersion, and utility of AIGC-generated content in large-scale interactive environments.</p>
<p>Against this backdrop, the necessity of this review is twofold. First, there is an urgent need to systematically organize the fast-growing body of research at the intersection of AIGC and the Metaverse. This includes understanding foundational model architectures, surveying current applications, and identifying promising generation pipelines. Second, it is critical to explore the existing limitations and research gaps that hinder the full potential of AIGC in shaping future immersive ecosystems. It is worth noting that, to provide a comprehensive technical background, we selectively cite existing review articles that focus on different aspects of generative models, application domains, and theoretical foundations. Each cited review offers unique insights—ranging from fundamental model designs to application-specific implementations—which collectively support the discussions and analyses presented in this paper.</p>
<p>By outlining the current advantages and gaps in the application of AIGC in the Metaverse, we advocate for the development of more robust and nuanced evaluation frameworks. These frameworks should not only address the complexity of virtual environments but also assess the quality of AIGC-generated content, the immersiveness of user experiences, and the innovative potential of future model-centric advancements. This research aims to establish a foundational knowledge base for academics and industry practitioners, thereby promoting future progress in evaluation methods and application technologies of AIGC within the metaverse.</p>
<p>The main contributions of our paper are threefold:</p>
<ul class="list" style="list-style-type:none">
<li>
<span class="label">1.</span><p class="display-inline">It provides an overviews of key GAI models and technological advancements. We provide a comprehensive survey of foundational and emerging GAI models, including generative adversarial networks (GANs), variational autoencoders (VAEs), Diffusion models, Transformers, and Mamba. By analyzing their architectural designs, strengths, and limitations, we explain how each model contributes to content generation for the Metaverse. We also highlight recent hybrid modeling approaches that combine the advantages of multiple architectures to enhance generative performance and efficiency. These insights offer a solid technical foundation for researchers and developers interested in applying generative models to immersive virtual environments.</p>
</li>
<li>
<span class="label">2.</span><p class="display-inline">It explores successful AIGC applications and their specific roles in shaping the Metaverse. We systematically review the practical applications of AIGC in shaping various components of the Metaverse. This includes virtual scene construction, avatar creation, interactive dialogue systems, and real-time video or 3D generation. By analyzing successful cases in domains such as healthcare, education, entertainment, and digital fashion, we reveal how AIGC accelerates production pipelines, improves user experience, and enables personalized content creation. This paper bridges the gap between model-level understanding and application-level deployment, demonstrating the real-world viability of GAI.</p>
</li>
<li>
<span class="label">3.</span><p class="display-inline">It identifies the challenges facing AIGC in the context of Metaverse development and offers future research directions for optimizing these technologies in immersive digital worlds. We identify current technical and practical challenges that hinder the full-scale adoption of AIGC in the Metaverse, such as model interpretability, real-time inference constraints, evaluation metrics, ethical risks, and sustainability concerns. We further propose future research directions to address these issues, including lightweight generative model design, hybrid edge–cloud deployment, cross-modal evaluation frameworks, and bias mitigation strategies. These suggestions aim to support the responsible development of AIGC for immersive, scalable, and ethical Metaverse ecosystems.</p>
</li>
</ul></section><section id="sec2"><h2 class="pmc_sec_title">Materials and Methods</h2>
<section id="sec3"><h3 class="pmc_sec_title">GAI models and their connection to the Metaverse</h3>
<p>In the development of the Metaverse, the creation and display of content plays a critical role. The immersive experience and appeal of the Metaverse depend largely on the variety and complexity of its digital content, which includes virtual environments, objects, and characters. AIGC helps overcome the issue of limited content within the Metaverse by providing an abundant if not potentially endless supply of digital assets to build virtual worlds, characters, objects, and events. As a result, AIGC is instrumental in accelerating the replication of the physical world in the Metaverse, enabling the production of boundless content and fostering organic, self-sustaining growth.</p>
<p>This section will organize an exploration of the most widely used generative deep learning frameworks in GAI, such as GANs, VAEs, and diffusion models, as well as cutting-edge foundational neural network architectures, including Transformer and Mamba. Notably, while examining these deep learning technologies, this paper will also highlight their connections to the Metaverse.</p>
<section id="sec4"><h4 class="pmc_sec_title">GANs and VAEs</h4>
<p>GANs and VAEs are 2 fundamental generative models widely adopted in the AIGC field, which can serve as core components for virtual content and environment generation in the Metaverse [<a href="#B31" class="usa-link" aria-describedby="B31">31</a>]. GANs consist of 2 neural networks—a generator and a discriminator—that are trained simultaneously in a competitive setting. The generator maps random noise vectors to data-like outputs, aiming to synthesize realistic samples, while the discriminator attempts to distinguish between real data and synthetic samples. During training, both networks are optimized in a minimax game: the generator tries to fool the discriminator, while the discriminator learns to become more accurate. This adversarial process drives the generator to produce outputs that are increasingly indistinguishable from real data [<a href="#B32" class="usa-link" aria-describedby="B32">32</a>]. Figures <a href="#F1" class="usa-link">1</a> (lower left) and <a href="#F2" class="usa-link">2</a>A illustrate this fundamental mechanism.</p>
<figure class="fig xbox font-sm" id="F2"><h5 class="obj_head">Fig. 2.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c7c8/12364526/19835f1e9e0a/research.0804.fig.002.jpg" loading="lazy" height="418" width="660" alt="Fig. 2."></p>
<div class="p text-right font-secondary"><a href="figure/F2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Advances in GANs and VAEs. Zoom in for a better view. (A) Basic GANs architecture [<a href="#B48" class="usa-link" aria-describedby="B48">48</a>]. (B) Spectral compressive imaging reconstruction with self-attention GAN [<a href="#B43" class="usa-link" aria-describedby="B43">43</a>]. (C) Convolutional VQGAN and autoregressive Transformer for high-resolution image synthesis [<a href="#B46" class="usa-link" aria-describedby="B46">46</a>]. (D) VQ-VAE: embedding space mapping and encoder gradient dynamics [<a href="#B47" class="usa-link" aria-describedby="B47">47</a>]. (E) An overall framework of RQ-transformer for text to images [<a href="#B200" class="usa-link" aria-describedby="B200">200</a>]. (F) The framework of latent space masking diffusion [<a href="#B49" class="usa-link" aria-describedby="B49">49</a>].</p></figcaption></figure><p>Despite their power, GANs often suffer from training instability and mode collapse, where the generator produces limited diversity [<a href="#B33" class="usa-link" aria-describedby="B33">33</a>,<a href="#B34" class="usa-link" aria-describedby="B34">34</a>]. To mitigate this, Wasserstein GAN (WGAN) replaced the original Jensen–Shannon divergence with the Wasserstein distance, improving convergence and gradient behavior [<a href="#B35" class="usa-link" aria-describedby="B35">35</a>,<a href="#B36" class="usa-link" aria-describedby="B36">36</a>].</p>
<p>To further improve the quality, diversity, and controllability of generated content, many advanced GAN variants have been developed. Deep Convolutional Generative Adversarial Networks (DCGANs) introduced convolutional layers into both the generator and discriminator to better exploit spatial information and stabilize training [<a href="#B37" class="usa-link" aria-describedby="B37">37</a>]. Conditional GAN (CGAN) incorporated label or attribute information as conditional inputs, enabling targeted generation of specific categories or styles [<a href="#B38" class="usa-link" aria-describedby="B38">38</a>]. StyleGAN introduced a style-based generator architecture that allows multi-level control over semantic attributes such as facial features or textures, enabling high-quality and editable image synthesis [<a href="#B39" class="usa-link" aria-describedby="B39">39</a>]. BigGAN scaled up the model and training dataset to achieve state-of-the-art performance in high-resolution and class-conditional image generation [<a href="#B40" class="usa-link" aria-describedby="B40">40</a>]. For multimodal and semantic-consistent generation, SPatially-Adaptive (DE)normalization and semantic region-adaptive normalization incorporated spatially adaptive normalization and semantic attention mechanisms, respectively, allowing detailed control over output layout and content [<a href="#B41" class="usa-link" aria-describedby="B41">41</a>,<a href="#B42" class="usa-link" aria-describedby="B42">42</a>].</p>
<p>In the field of scientific imaging, λ-Net [<a href="#B43" class="usa-link" aria-describedby="B43">43</a>] was proposed to generate hyperspectral images by learning the spectral–spatial correlation across multiple bands, enabling end-to-end synthesis of high-resolution spectral data. These advanced variants markedly expand the applicability of GANs in content creation, from photorealistic avatars to domain-specific reconstructions, and play a critical role in constructing immersive environments in the Metaverse.</p>
<p>VAEs, introduced by Kingma and Welling [<a href="#B44" class="usa-link" aria-describedby="B44">44</a>], represent one of the earliest techniques in unsupervised learning and generative modeling. The core framework of VAEs, as illustrated in the middle-lower panel of Fig. <a href="#F1" class="usa-link">1</a>, merges probabilistic methods with the foundational structure of autoencoders. Unlike traditional autoencoders that solely compress data into a latent space and reconstruct it, VAEs further impose a prior distribution—typically a standard Gaussian—on the latent variables. This design enables the generation of new, diverse samples from the learned latent distribution. The architecture of VAEs consists of 2 main components: an encoder and a decoder. The encoder maps input data to a low-dimensional latent space, while the decoder reconstructs the original data from the latent representation. By enforcing a standard Gaussian prior in the latent space, VAEs promote regularity and smoothness in the generative process, aligning the outputs with the statistical properties of the input distribution. As a result, VAEs are capable of capturing meaningful data structures and generating novel samples that are consistent with the training data.</p>
<p>Early advancements in autoencoder architectures include denoising autoencoders [<a href="#B45" class="usa-link" aria-describedby="B45">45</a>], which enhance robustness by learning to reconstruct clean data from corrupted inputs, and convolutional autoencoders [<a href="#B39" class="usa-link" aria-describedby="B39">39</a>], which incorporate convolutional neural networks (CNNs) to address challenges in image data processing. Currently, VAE is often combined with other architectures, i.e., GAN, diffusion, and Transformer. Latent diffusion models (LDMs) have emerged as a powerful generative framework by integrating VAEs with diffusion models. The core idea behind LDMs is to shift the computationally expensive denoising and generation process from the high-dimensional pixel space to a compressed latent space, substantially improving efficiency without sacrificing generation quality. In this framework, a VAE is first trained to encode images into a compact latent representation that retains essential semantic and perceptual information. Once trained, the diffusion process operates within this latent space, where noise is gradually removed to generate coherent outputs. This design addresses the inefficiency of conventional diffusion models, which require many iterations in pixel space, and also reduces the memory footprint during training and inference. Figure <a href="#F2" class="usa-link">2</a> shows LDM variants used for image/video generation, including Vector Quantized Generative Adversarial Network (VQGAN) [<a href="#B46" class="usa-link" aria-describedby="B46">46</a>] and Vector Quantized-Variational AutoEncoder (VQVAE) [<a href="#B47" class="usa-link" aria-describedby="B47">47</a>]. A trained VAE can serve as a general compression model, with its latent space used to train multiple generative models and applied to other downstream tasks, enhancing feature representation capabilities. In Fig. <a href="#F2" class="usa-link">2</a>B to F, we summarize other GAN- and VAE-based architectures for text/image/video generation [<a href="#B40" class="usa-link" aria-describedby="B40">40</a>,<a href="#B43" class="usa-link" aria-describedby="B43">43</a>,<a href="#B46" class="usa-link" aria-describedby="B46">46</a>–<a href="#B49" class="usa-link" aria-describedby="B49">49</a>].</p>
<p>Overall, these properties of GAN and VAE models enable them to generate realistic virtual characters, landscapes, environments, and objects in the Metaverse, greatly enriching the virtual experience.</p></section><section id="sec5"><h4 class="pmc_sec_title">Diffusion models</h4>
<p>As introduced by Salimans et al. [<a href="#B41" class="usa-link" aria-describedby="B41">41</a>], diffusion models were initially proposed to improve the performance of GANs. Diffusion models are powerful tools for generating high-quality samples, through continuous development, and have become a star architecture in the AIGC field, showcasing the latest advancements in computer vision. The generator model of GAN must undergo a step from pure noise to a mirrored image (X<sub>T</sub> → X₀), which is a source of instability during training.</p>
<p>Unlike GANs, as depicted in the bottom-right section of Fig. <a href="#F1" class="usa-link">1</a>, the diffusion process is divided into 2 key stages: the forward process (diffusion) and the reverse process (denoising). During the forward process, data are progressively corrupted into a noisy state, while the reverse process focuses on reconstructing the original data from this noise. Training a diffusion model revolves around reducing the discrepancy between the generated outputs and real data samples. This is achieved by simulating a step-by-step transformation of random noise into structured, meaningful data. The model learns to reverse the diffusion process, effectively reconstructing coherent patterns from noise. By iteratively refining this process, the model becomes adept at generating high-quality, realistic samples that closely resemble the original data distribution. This approach not only ensures fidelity to real-world data but also enables the creation of diverse and novel outputs. Additionally, the iterative nature of diffusion models makes both their training and generation processes more stable.</p>
<p>Diffusion models can be categorized into 3 main types, with several classic examples and their variants illustrated in Fig. <a href="#F3" class="usa-link">3</a> [<a href="#B42" class="usa-link" aria-describedby="B42">42</a>,<a href="#B50" class="usa-link" aria-describedby="B50">50</a>–<a href="#B53" class="usa-link" aria-describedby="B53">53</a>]. As shown in Fig. <a href="#F3" class="usa-link">3</a>C, a denoising diffusion probabilistic model (DDPM) [<a href="#B42" class="usa-link" aria-describedby="B42">42</a>] utilizes 2 Markov chains to gradually corrupt data using Gaussian noise and learn to reverse this forward diffusion process by estimating the Markov transition kernel. Score-based generative models (SGMs), on the other hand, focus on the logarithmic density gradient of the data, referred to as the score function. Noise conditional score networks [<a href="#B54" class="usa-link" aria-describedby="B54">54</a>] introduce multi-scale noise perturbations and employ a neural network to estimate the score function across all noise levels simultaneously. This decoupling of training and inference steps enables flexible sampling. Score-based stochastic differential equations [<a href="#B50" class="usa-link" aria-describedby="B50">50</a>] generalize these concepts to a continuous framework, where noise perturbation and denoising are modeled as solutions to stochastic differential equations. This approach also demonstrates that probability flow ordinary differential equations can effectively describe the reverse process, further expanding the versatility of diffusion models.</p>
<figure class="fig xbox font-sm" id="F3"><h5 class="obj_head">Fig. 3.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c7c8/12364526/41f892d18543/research.0804.fig.003.jpg" loading="lazy" height="286" width="660" alt="Fig. 3."></p>
<div class="p text-right font-secondary"><a href="figure/F3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Advances of diffusion models. Zoom in for a better view. (A) Four-flow versatile diffusion (VD) framework for multi-task support [<a href="#B51" class="usa-link" aria-describedby="B51">51</a>]. (B) The overall framework of denoising diffusion GAN [<a href="#B53" class="usa-link" aria-describedby="B53">53</a>]. (C) The overall framework of DDPM [<a href="#B42" class="usa-link" aria-describedby="B42">42</a>] and score-based diffusion model via reverse-time stochastic differential equation (SDE) [<a href="#B50" class="usa-link" aria-describedby="B50">50</a>]. (D) Conditional LDMs with concatenation and cross-attention [<a href="#B52" class="usa-link" aria-describedby="B52">52</a>].</p></figcaption></figure><p>Diffusion models have also found success in NLP tasks, where they can generate coherent and contextually relevant text sequences. By tokenizing sentences and converting them into word embeddings, these models leverage the diffusion process to learn and produce natural language outputs. They excel in handling a variety of complex NLP applications, including machine translation, question answering, search query completion, sentiment analysis, and text continuation. A prominent example is Diffusion-LM (language model) [<a href="#B55" class="usa-link" aria-describedby="B55">55</a>], which adapts diffusion models to the discrete nature of text data. This model addresses the challenge of applying continuous diffusion processes to discrete text by enabling fine-grained and controllable language generation. Experimental results demonstrate that Diffusion-LM achieves state-of-the-art performance across 6 controllable text generation tasks, showcasing its versatility and effectiveness in NLP applications. Diffusion models have demonstrated remarkable capabilities in generating images, videos, and even 3D objects from textual descriptions, showcasing their versatility in multimodal tasks. Prominent examples of text-to-image diffusion models include DALLE-2 [<a href="#B56" class="usa-link" aria-describedby="B56">56</a>], Imagen [<a href="#B57" class="usa-link" aria-describedby="B57">57</a>], versatile diffusion (VD) [<a href="#B51" class="usa-link" aria-describedby="B51">51</a>], and the widely accessible stable diffusion [<a href="#B51" class="usa-link" aria-describedby="B51">51</a>,<a href="#B52" class="usa-link" aria-describedby="B52">52</a>], which have set new benchmarks in generating high-quality visuals from text prompts. Beyond static images, diffusion models have been extended to video generation, as seen in Meta AI’s Make-A-Video [<a href="#B58" class="usa-link" aria-describedby="B58">58</a>] and ControlNet Video [<a href="#B59" class="usa-link" aria-describedby="B59">59</a>], which transform text inputs into dynamic video sequences.</p>
<p>In the realm of 3D content creation, diffusion models have been adapted to generate 3D objects from text descriptions, utilizing various representations such as point clouds, meshes, and neural radiance fields (NeRFs) [<a href="#B60" class="usa-link" aria-describedby="B60">60</a>]. For instance, DiffRF [<a href="#B61" class="usa-link" aria-describedby="B61">61</a>] introduces a diffusion model specifically designed for generating 3D radiance fields from text, while 3DFuse [<a href="#B62" class="usa-link" aria-describedby="B62">62</a>] focuses on creating 3D point clouds from 2D images. These advancements highlight the flexibility of diffusion models in handling diverse data formats and their potential to revolutionize fields like computer graphics, VR, and digital content creation. By bridging the gap between textual descriptions and complex visual outputs, diffusion models continue to push the boundaries of GAI, enabling more intuitive and creative applications.</p>
<p>Recently, there have been new developments in diffusion models; some generative architectures have emerged that attempt to combine the advantages of diffusion models and other generative models, i.e., creating a hybrid modeling approach. The goal of hybrid modeling is to integrate diffusion models with other generative models, aiming to enhance expressive power, improve sampling efficiency, or address specific limitations. DiffuseVAE [<a href="#B63" class="usa-link" aria-describedby="B63">63</a>] combines the strengths of standard VAEs and DDPMs by incorporating blurry images generated by the VAE into the diffusion sampling process. This integration allows for more efficient and high-quality image generation, leveraging the VAE’s ability to capture latent representations and the diffusion model’s capacity for fine-grained detail refinement. Latent score-based generative model (LSGM) [<a href="#B64" class="usa-link" aria-describedby="B64">64</a>] trains SGMs in the latent space of a VAE. By operating in a lower-dimensional latent space, LSGM generalizes SGMs to non-continuous data and enables smoother and more efficient learning. This approach not only reduces computational complexity but also improves the model’s ability to generate diverse and high-quality outputs.</p>
<p>Denoising diffusion GANs (DDGANs) [<a href="#B53" class="usa-link" aria-describedby="B53">53</a>] integrate conditional GANs into the DDPM framework, enabling the use of richer, multimodal distributions to guide the denoising process. This allows for larger steps in the denoising phase, markedly speeding up generation. On the other hand, DiffFlow [<a href="#B65" class="usa-link" aria-describedby="B65">65</a>] introduces flow-based techniques into the trajectories of PDE-driven diffusion models, making the forward steps adaptable through training. The inherent randomness from noise perturbation amplifies the expressive potential of normalized flows, while the trainable forward process shortens the overall diffusion path. This results in more efficient sampling and the ability to model distributions with sharper, more precise boundaries. DDGANs [<a href="#B53" class="usa-link" aria-describedby="B53">53</a>] diverge from traditional diffusion models by replacing the Gaussian assumption with a multimodal conditional distribution, pioneering the use of a GAN-based training objective in diffusion frameworks. This approach inherits the rapid sampling benefits of GANs. A key insight is that smaller denoising steps tend to produce Gaussian-like outputs, while larger steps lead to multimodal (peaked) distributions. To optimize sampling speed, DDGANs leverage multimodal distributions instead of relying on single-peaked Gaussians. Unlike adversarial distillation methods, which focus on distinguishing synthetic from real images, DDGANs utilize denoised “real” latent samples. However, the scalability limitations of GANs restrict DDGANs from being applied to large-scale datasets.</p>
<p>These hybrid architectures represent a significant step forward in generative modeling, as they combine the complementary strengths of different frameworks. By merging the stability and controllability of diffusion models with the efficiency and compact representations of VAEs or the adversarial training of GANs, these models open up new possibilities for faster sampling, improved performance, and broader applicability across various domains, such as image synthesis, video generation, and 3D content creation.</p>
<p>In addition to the methods mentioned above to improve diffusion computational efficiency, there are also strategies including fractional distillation sampling (SDS) to enhance fidelity, model pruning, knowledge distillation, and mixed training with GAN loss to stabilize learning and improve convergence speed. These advancements make diffusion models more suitable for real-time and interactive Metaverse scenarios, such as dynamic avatar generation and virtual scene synthesis.</p>
<p>In the Metaverse, diffusion models extend their capabilities beyond text, image, and video generation to crafting detailed virtual objects and immersive environments. They excel at simulating realistic natural phenomena, such as flowing water, smoke [<a href="#B66" class="usa-link" aria-describedby="B66">66</a>], intricate textures, and fine details of virtual assets [<a href="#B67" class="usa-link" aria-describedby="B67">67</a>]. Moreover, diffusion models can generate lifelike actions and behaviors for virtual characters [<a href="#B68" class="usa-link" aria-describedby="B68">68</a>], fostering more dynamic and diverse interactions. These advancements markedly enhance the authenticity and richness of virtual worlds, improving both visual fidelity and user engagement.</p></section><section id="sec6"><h4 class="pmc_sec_title">Transformers</h4>
<p>Transformers have become a transformative force in the field of GAI, greatly enhancing the capabilities of neural networks across a variety of domains. Initially introduced by Vaswani et al. [<a href="#B69" class="usa-link" aria-describedby="B69">69</a>], the Transformer architecture revolutionized NLP tasks such as machine translation and language generation, largely due to its innovative attention mechanism. We have compiled several transformer variants used for text/image/video generation and processing in Fig. <a href="#F4" class="usa-link">4</a> [<a href="#B70" class="usa-link" aria-describedby="B70">70</a>–<a href="#B74" class="usa-link" aria-describedby="B74">74</a>]. Unlike traditional models, Transformers are capable of modeling long-range dependencies by using self-attention and multi-head attention, enabling them to capture complex global relationships within sequences. The exceptional adaptability of transformers has made them the foundational technology behind the current state-of-the-art generative applications. Models like generative pre-trained transformers (GPT) [<a href="#B75" class="usa-link" aria-describedby="B75">75</a>] and bidirectional encoder representations from transformers (BERTs) [<a href="#B70" class="usa-link" aria-describedby="B70">70</a>] exemplify this capability, delivering highly coherent and contextually aware outputs across both textual and multimodal formats. This versatility highlights their pivotal role in advancing state-of-the-art generative systems.</p>
<figure class="fig xbox font-sm" id="F4"><h5 class="obj_head">Fig. 4.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c7c8/12364526/486754bf8073/research.0804.fig.004.jpg" loading="lazy" height="349" width="660" alt="Fig. 4."></p>
<div class="p text-right font-secondary"><a href="figure/F4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Overview of Transformer-based architectures for generative models. Zoom in for a better view. (A) Pre-training and fine-tuning procedures for Transformer-based BERT: architecture, parameter initialization, and special tokens [<a href="#B70" class="usa-link" aria-describedby="B70">70</a>]. (B) U-ViT architecture for diffusion models: tokenized inputs and long skip connections for efficient learning [<a href="#B71" class="usa-link" aria-describedby="B71">71</a>]. (C) PixArt-α architecture: cross-attention module for textual conditions and shared adaLN parameters for time efficiency [<a href="#B73" class="usa-link" aria-describedby="B73">73</a>]. (D) Diffusion techniques such as cross-attention mechanisms and spatiotemporal token extraction to enhance performance and output quality. The increasing reliance on Transformers for generating high-resolution, large-scale content underscores their transformative influence on generative AI, solidifying their position as a driving force in the field. Transformer (DiT) architecture trains conditional models by processing input latents with DiT blocks, incorporating adaptive layer normalization, cross-attention, and extra tokens, with adaptive layer normalization yielding the best performance [<a href="#B72" class="usa-link" aria-describedby="B72">72</a>]. (E) Latte pipeline for video generation: spatiotemporal information capture with Transformer variants and VAE simplification [<a href="#B74" class="usa-link" aria-describedby="B74">74</a>].</p></figcaption></figure><p>In recent years, Transformers have expanded their influence beyond NLP, establishing dominance in computer vision and multimodal AI, where their scalability and ability to model complex patterns have led to breakthroughs in image and video generation. For instance, prior to the development of U-vision transformer (ViT) [<a href="#B71" class="usa-link" aria-describedby="B71">71</a>] and diffusion transformer (DiT) [<a href="#B72" class="usa-link" aria-describedby="B72">72</a>], state-of-the-art generative models for image synthesis relied on convolutional U-Net architectures. The integration of Transformer blocks into generative frameworks, as seen in U-ViT, enabled the treatment of all inputs as tokens while leveraging long-range connections, facilitating deeper and more efficient learning. DiT further advanced this by adopting ViTs as the core architecture for image generation, demonstrating their robustness and scalability in addressing intricate generative tasks. Innovations like PixArt-α [<a href="#B73" class="usa-link" aria-describedby="B73">73</a>] and Latte [<a href="#B74" class="usa-link" aria-describedby="B74">74</a>] continue to push the boundaries of Transformers in image and video synthesis, employing advanced cross-attention mechanisms, spatio-temporal token extraction, and hierarchical feature representations to significantly enhance generation fidelity, temporal coherence, and scalability for high-resolution content creation.</p></section><section id="sec7"><h4 class="pmc_sec_title">Mamba</h4>
<p>Although Transformer-based GAI models have achieved exceptional performance, they face challenges when handling long-sequence generation tasks due to the quadratic computational complexity of the attention mechanism. This leads to substantial computational demands, especially when applied to high-resolution image synthesis or video generation tasks. However, recent developments in state-space modeling (SSM) [<a href="#B76" class="usa-link" aria-describedby="B76">76</a>,<a href="#B77" class="usa-link" aria-describedby="B77">77</a>] have introduced innovative strategies that effectively balance computational efficiency with model flexibility. Techniques such as those in Refs. [<a href="#B76" class="usa-link" aria-describedby="B76">76</a>,<a href="#B78" class="usa-link" aria-describedby="B78">78</a>,<a href="#B79" class="usa-link" aria-describedby="B79">79</a>] have proven highly effective across various tasks and modalities, particularly in managing long-term dependencies within sequential data. Building on these advancements, Mamba [<a href="#B80" class="usa-link" aria-describedby="B80">80</a>] was introduced, which integrates SSM principles with hardware-aware optimization techniques to simplify training and inference, thereby improving performance. We have appropriately summarized and presented the applicability of Mamba’s architecture and its various variants in text, image, and video generation and processing tasks in Fig. <a href="#F5" class="usa-link">5</a> [<a href="#B80" class="usa-link" aria-describedby="B80">80</a>–<a href="#B84" class="usa-link" aria-describedby="B84">84</a>]. Building on these advancements, the Diffusion Mamba (DiM) [<a href="#B83" class="usa-link" aria-describedby="B83">83</a>] model leverages Mamba as its backbone to enable high-resolution image generation. A key innovation in DiM lies in its enhancement of traditional unidirectional patch causality by incorporating an alternating scanning mechanism in the Mamba block, spanning 4 directional passes. To mitigate spatial continuity challenges associated with Mamba’s scanning technique, ZigMa [<a href="#B84" class="usa-link" aria-describedby="B84">84</a>] introduces an inductive bias specifically designed to ensure seamless continuity in 2D image applications. This methodology extends naturally to video generation, where ZigMa employs a spatiotemporal decomposition approach to handle 3D sequences, fostering more realistic motion and temporal consistency in video outputs.</p>
<figure class="fig xbox font-sm" id="F5"><h5 class="obj_head">Fig. 5.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c7c8/12364526/0b8f153efcb4/research.0804.fig.005.jpg" loading="lazy" height="312" width="660" alt="Fig. 5."></p>
<div class="p text-right font-secondary"><a href="figure/F5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Advances of Mamba architectures for generative models. Zoom in for a better view. (A) Structured SSMs with input-dependent dynamics and hardware-efficient state expansion [<a href="#B80" class="usa-link" aria-describedby="B80">80</a>]. (B) Diffusion models with Bi-RWKV layers for spatial and channel mixing [<a href="#B81" class="usa-link" aria-describedby="B81">81</a>]. (C) Overview of DiG models: plain, U-shape, and block structures with SREM control [<a href="#B82" class="usa-link" aria-describedby="B82">82</a>]. (D) Noise prediction framework with patch-wise features and Mamba blocks in diffusion model [<a href="#B83" class="usa-link" aria-describedby="B83">83</a>]. (E) ZigMa: position-aware backbone with single-scan Mamba blocks in diffusion model [<a href="#B84" class="usa-link" aria-describedby="B84">84</a>].</p></figcaption></figure><p>To demonstrate the computational efficiency of Mamba, we compared the performance and parameter count of a Mamba-based architecture with Transformer-based and CNN-based architectures in an object tracking task based on OSTrack.</p>
<p>In addition to the aforementioned generative models, recent innovations in architectures for image and video generation have also emerged. For example, Diffusion-RWKV (receptance weighted key value) [<a href="#B81" class="usa-link" aria-describedby="B81">81</a>] integrates the RWKV [<a href="#B85" class="usa-link" aria-describedby="B85">85</a>] architecture as a backbone, utilizing temporal and channel mixing sub-blocks within its residual blocks. This design markedly improves upon traditional recurrent neural network (RNN) architectures by enabling parallelized computations during training and enhancing efficiency, particularly through the use of linear attention mechanisms. Additionally, DiG [<a href="#B82" class="usa-link" aria-describedby="B82">82</a>] introduces the Diffusion gated linear attention (GLA) [<a href="#B86" class="usa-link" aria-describedby="B86">86</a>] model, which incorporates a GLA Transformer to achieve exceptional training efficiency and optimized GPU memory utilization, making it highly effective for high-resolution image generation tasks.</p>
<p>To illustrate the computational efficiency of Mamba, we compared the performance and parameter counts of Mamba-based architectures with Transformer-based and CNN-based architectures on the OSTrack-based [<a href="#B87" class="usa-link" aria-describedby="B87">87</a>] tracking task. For a fair comparison, all methods were trained and evaluated on the large-scale event-based tracking dataset EventVOT [<a href="#B88" class="usa-link" aria-describedby="B88">88</a>], which contains 841, 18, and 282 videos in different subsets, respectively. The CNN-based methods include TrDiMP [<a href="#B89" class="usa-link" aria-describedby="B89">89</a>], ToMP 50 [<a href="#B90" class="usa-link" aria-describedby="B90">90</a>], DiMP 50 [<a href="#B91" class="usa-link" aria-describedby="B91">91</a>], PrDiMP [<a href="#B92" class="usa-link" aria-describedby="B92">92</a>], and ATOM [<a href="#B93" class="usa-link" aria-describedby="B93">93</a>]; the Transformer-based methods include HDETrack [<a href="#B88" class="usa-link" aria-describedby="B88">88</a>], AiATrack [<a href="#B94" class="usa-link" aria-describedby="B94">94</a>], STARK [<a href="#B87" class="usa-link" aria-describedby="B87">87</a>], TransT [<a href="#B95" class="usa-link" aria-describedby="B95">95</a>], MixFormer [<a href="#B96" class="usa-link" aria-describedby="B96">96</a>], and SimTrack [<a href="#B97" class="usa-link" aria-describedby="B97">97</a>]. We used 3 widely adopted evaluation metrics for comparison: success rate (SR), precision (PR), and normalized precision (NPR). Detailed experimental results are summarized in Table <a href="#T1" class="usa-link">1</a>. It should be pointed out that all results are sourced from published papers. As shown in Table <a href="#T1" class="usa-link">1</a>, CNN-based methods leveraging ResNet50 [<a href="#B98" class="usa-link" aria-describedby="B98">98</a>] generally have larger parameter counts but achieve slightly lower performance. Transformer-based methods demonstrate strong performance but require a substantial number of parameters. When replacing the ViT [<a href="#B99" class="usa-link" aria-describedby="B99">99</a>] backbone with Mamba, SR improves slightly, while PR and NPR show marginal decreases; however, the parameter count is significantly reduced to just 4.1M. These findings lead to the conclusion that Mamba offers high computational efficiency, achieving state-of-the-art performance with a minimal parameter count. This makes it a promising backbone, especially for handling large-scale 3D data, where its advantages become even more pronounced.</p>
<section class="tw xbox font-sm" id="T1"><h5 class="obj_head">Table 1.</h5>
<div class="caption p"><p>Comparison between different methods on the EventVOT dataset. Bold formatting highlights the best computational efficiency of the Mamba architecture.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" rowspan="1" colspan="1">Tracker</th>
<th align="center" rowspan="1" colspan="1">Source</th>
<th align="center" rowspan="1" colspan="1">Backbone</th>
<th align="center" rowspan="1" colspan="1">SR</th>
<th align="center" rowspan="1" colspan="1">PR</th>
<th align="center" rowspan="1" colspan="1">NPR</th>
<th align="center" rowspan="1" colspan="1">Params (M)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">TrDiMP</td>
<td align="center" rowspan="1" colspan="1">CVPR21</td>
<td rowspan="5" align="center" colspan="1">ResNet50</td>
<td align="center" rowspan="1" colspan="1">39.9</td>
<td align="center" rowspan="1" colspan="1">34.8</td>
<td align="center" rowspan="1" colspan="1">48.7</td>
<td align="center" rowspan="1" colspan="1">26.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ToMP50</td>
<td align="center" rowspan="1" colspan="1">CVPR22</td>
<td align="center" rowspan="1" colspan="1">37.6</td>
<td align="center" rowspan="1" colspan="1">32.8</td>
<td align="center" rowspan="1" colspan="1">47.4</td>
<td align="center" rowspan="1" colspan="1">26.1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">DiMP50</td>
<td align="center" rowspan="1" colspan="1">ICCV19</td>
<td align="center" rowspan="1" colspan="1">52.6</td>
<td align="center" rowspan="1" colspan="1">51.1</td>
<td align="center" rowspan="1" colspan="1">67.2</td>
<td align="center" rowspan="1" colspan="1">26.1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">PrDiMP</td>
<td align="center" rowspan="1" colspan="1">CVPR20</td>
<td align="center" rowspan="1" colspan="1">55.5</td>
<td align="center" rowspan="1" colspan="1">57.2</td>
<td align="center" rowspan="1" colspan="1">70.4</td>
<td align="center" rowspan="1" colspan="1">26.1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ATOM</td>
<td align="center" rowspan="1" colspan="1">CVPR19</td>
<td align="center" rowspan="1" colspan="1">44.4</td>
<td align="center" rowspan="1" colspan="1">44.0</td>
<td align="center" rowspan="1" colspan="1">57.5</td>
<td align="center" rowspan="1" colspan="1">8.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">HDETrack</td>
<td align="center" rowspan="1" colspan="1">CVPR24</td>
<td rowspan="6" align="center" colspan="1">ViT</td>
<td align="center" rowspan="1" colspan="1">57.8</td>
<td align="center" rowspan="1" colspan="1">62.2</td>
<td align="center" rowspan="1" colspan="1">73.5</td>
<td align="center" rowspan="1" colspan="1">92.1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">AiATrack</td>
<td align="center" rowspan="1" colspan="1">ECCV22</td>
<td align="center" rowspan="1" colspan="1">57.4</td>
<td align="center" rowspan="1" colspan="1">59.7</td>
<td align="center" rowspan="1" colspan="1">72.8</td>
<td align="center" rowspan="1" colspan="1">15.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">STARK</td>
<td align="center" rowspan="1" colspan="1">ICCV21</td>
<td align="center" rowspan="1" colspan="1">44.5</td>
<td align="center" rowspan="1" colspan="1">39.6</td>
<td align="center" rowspan="1" colspan="1">55.7</td>
<td align="center" rowspan="1" colspan="1">28.1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">TransT</td>
<td align="center" rowspan="1" colspan="1">CVPR21</td>
<td align="center" rowspan="1" colspan="1">54.3</td>
<td align="center" rowspan="1" colspan="1">56.5</td>
<td align="center" rowspan="1" colspan="1">68.8</td>
<td align="center" rowspan="1" colspan="1">18.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MixFormer</td>
<td align="center" rowspan="1" colspan="1">CVPR22</td>
<td align="center" rowspan="1" colspan="1">49.9</td>
<td align="center" rowspan="1" colspan="1">49.6</td>
<td align="center" rowspan="1" colspan="1">63.0</td>
<td align="center" rowspan="1" colspan="1">35.6</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">SimTrack</td>
<td align="center" rowspan="1" colspan="1">ECCV22</td>
<td align="center" rowspan="1" colspan="1">55.4</td>
<td align="center" rowspan="1" colspan="1">57.5</td>
<td align="center" rowspan="1" colspan="1">69.9</td>
<td align="center" rowspan="1" colspan="1">57.8</td>
</tr>
<tr>
<td rowspan="3" align="left" colspan="1">OSTrack</td>
<td rowspan="3" align="center" colspan="1">ECCV22</td>
<td align="center" rowspan="1" colspan="1">ViT-B</td>
<td align="center" rowspan="1" colspan="1">55.4</td>
<td align="center" rowspan="1" colspan="1">60.4</td>
<td align="center" rowspan="1" colspan="1">71.1</td>
<td align="center" rowspan="1" colspan="1">92.1</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">ViT-S</td>
<td align="center" rowspan="1" colspan="1">52.0</td>
<td align="center" rowspan="1" colspan="1">53.2</td>
<td align="center" rowspan="1" colspan="1">66.8</td>
<td align="center" rowspan="1" colspan="1">54.3</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">Vim-S</td>
<td align="center" rowspan="1" colspan="1">55.6</td>
<td align="center" rowspan="1" colspan="1">59.1</td>
<td align="center" rowspan="1" colspan="1">70.4</td>
<td align="center" rowspan="1" colspan="1">
<strong>4.1</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/T1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>These advancements in AIGC are poised to significantly accelerate the development of the Metaverse. By enabling the creation of high-resolution, realistic, and contextually rich virtual environments, these technologies can enhance immersive experiences. Efficient video generation and real-time rendering capabilities will support dynamic, interactive worlds, while optimized memory utilization and computational efficiency will make large-scale Metaverse applications more accessible. Together, these innovations will drive the creation of more engaging, lifelike, and scalable virtual ecosystems.</p></section></section><section id="sec8"><h3 class="pmc_sec_title">Comparative analysis of generative models for Metaverse applications</h3>
<p>To systematically compare the performance of different generative models in Metaverse applications, this paper summarizes the strengths, weaknesses, and typical use cases of GANs, VAEs, diffusion models, and Transformers in this section. In addition, we further discuss the computational challenges associated with AIGC models and the limitations of current AR/VR hardware in supporting real-time, high-quality content generation. Finally, we analyze the performance of various generative models in terms of realism, coherence, and user engagement.</p>
<p>To provide a clearer understanding of the strengths, limitations, and practical considerations of different generative models in the context of Metaverse applications, we conducted a comprehensive comparative analysis. Specifically, to systematically compare generative models for Metaverse applications, we evaluate GANs, VAEs, diffusion models, and Transformers across quantifiable dimensions: latency, fidelity, controllability, multimodal capability, edge deployability, and Metaverse suitability. Table <a href="#T2" class="usa-link">2</a> synthesizes both quantitative benchmarks (FID [Frechet Inception Distance], latency, and Video Random Access Memory [VRAM]) and qualitative capabilities to highlight critical trade-offs. It highlights the trade-offs when deploying these models in resource-constrained yet interaction-intensive environments. For instance, GANs achieve domain-specific high fidelity (FID 2.5 to 4 on Flickr-Faces-HQ [FFHQ]) and sub-100-ms latency, enabling real-time applications like avatar generation, but lack multimodal support. Diffusion/Transformers offer superior controllability (e.g., ControlNet and prompt-based layout) and multimodal generation (text → image/video), but require &gt;10 GB VRAM and suffer high latency (0.8 to 10 s). Edge deployment is viable only for GANs/VAEs (&lt;8 GB VRAM), while Transformers demand cloud-scale resources (&gt;24 GB VRAM). Overall, while diffusion/Transformer models enable complex multimodal generation, their high latency (&gt;0.8 s) and hardware demands (&gt;10 GB VRAM) limit real-time deployment. Conversely, GANs/VAEs support edge devices but lack open-domain flexibility. These constraints define their roles in the Metaverse stack: GANs/VAEs for interactive elements, and diffusion/Transformers for non-real-time content creation.</p>
<section class="tw xbox font-sm" id="T2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Comparative evaluation of generative models for Metaverse content creation</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" rowspan="1" colspan="1">Model type</th>
<th align="center" rowspan="1" colspan="1">Representative models</th>
<th align="center" rowspan="1" colspan="1">Latency (↓)</th>
<th align="center" rowspan="1" colspan="1">Fidelity (↑)</th>
<th align="center" rowspan="1" colspan="1">Controllability</th>
<th align="center" rowspan="1" colspan="1">Multimodal capability</th>
<th align="center" rowspan="1" colspan="1">Edge deployability</th>
<th align="center" rowspan="1" colspan="1">Suitability for metaverse scenarios</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">GANs</td>
<td align="center" rowspan="1" colspan="1">StyleGAN 3, BigGAN</td>
<td align="center" rowspan="1" colspan="1">30–70 ms (512<sup>2</sup>, A100)</td>
<td align="center" rowspan="1" colspan="1">FID: ~2.5–4 (FFHQ/CIFAR-10)</td>
<td align="center" rowspan="1" colspan="1">Latent interpolation</td>
<td align="center" rowspan="1" colspan="1">Image-only</td>
<td align="center" rowspan="1" colspan="1">&lt;8 GB VRAM (INT8 quantized)</td>
<td align="center" rowspan="1" colspan="1">Virtual avatars, NPC faces, fashion try-on</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">VAEs</td>
<td align="center" rowspan="1" colspan="1">β-VAE, VQ-VAE-2</td>
<td align="center" rowspan="1" colspan="1">&lt;100 ms (512<sup>2</sup>, A100)</td>
<td align="center" rowspan="1" colspan="1">FID: 28–32 (ImageNet-1k)</td>
<td align="center" rowspan="1" colspan="1">Basic attribute editing</td>
<td align="center" rowspan="1" colspan="1">Image-only</td>
<td align="center" rowspan="1" colspan="1">&lt;4 GB VRAM (mobile GPU)</td>
<td align="center" rowspan="1" colspan="1">3D mesh encoding, texture compression</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Diffusion models</td>
<td align="center" rowspan="1" colspan="1">Stable Diffusion 3, LDM</td>
<td align="center" rowspan="1" colspan="1">0.8–2.5 s (512<sup>2</sup>, A100)</td>
<td align="center" rowspan="1" colspan="1">FID: 5.5–6.5 (COCO val2017)</td>
<td align="center" rowspan="1" colspan="1">ControlNet + Inpainting</td>
<td align="center" rowspan="1" colspan="1">Text → image/video</td>
<td align="center" rowspan="1" colspan="1">10 GB VRAM (FP16 required)</td>
<td align="center" rowspan="1" colspan="1">Environment concept art, dynamic texture synthesis</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Transformers</td>
<td align="center" rowspan="1" colspan="1">DALL·E 3, Imagen, Make-A-Video</td>
<td align="left" rowspan="1" colspan="1">2–10 s (512<sup>2</sup>, A100)</td>
<td align="center" rowspan="1" colspan="1">FID: 8–12 (COCO val2017)</td>
<td align="center" rowspan="1" colspan="1">Prompt + Layout control</td>
<td align="center" rowspan="1" colspan="1">Text → image/video/audio</td>
<td align="center" rowspan="1" colspan="1">&gt;24 GB VRAM (multi-GPU)</td>
<td align="center" rowspan="1" colspan="1">Narrative scene generation, interactive storytelling</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/T2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>Table <a href="#T3" class="usa-link">3</a> presents a quantitative evaluation of representative models across distinct generative tasks: e.g., StyleGAN 3 [<a href="#B100" class="usa-link" aria-describedby="B100">100</a>] (unconditional/category-conditioned), VQ-VAE-2 [<a href="#B101" class="usa-link" aria-describedby="B101">101</a>] (text-to-image), Stable Diffusion 3 [<a href="#B102" class="usa-link" aria-describedby="B102">102</a>] (text-to-image), DALL·E 3 [<a href="#B100" class="usa-link" aria-describedby="B100">100</a>] (text-to-image), and Make-A-Video (text-to-video). Metrics include domain-specific FID/Fréchet Video Distance (FVD) scores [<a href="#B103" class="usa-link" aria-describedby="B103">103</a>], CLIP (Contrastive Language-Image Pre-Training) scores [<a href="#B104" class="usa-link" aria-describedby="B104">104</a>], latency, parameter size, and memory footprint. These measurements reveal critical deployment trade-offs: While DALL·E 3 and Stable Diffusion 3 achieve comparable high CLIP scores (~0.31) on open-domain COCO benchmarks, they require &gt;10 GB VRAM and &gt;1 s latency. In contrast, StyleGAN 3 and VQ-VAE-2 demonstrate domain-optimized efficiency with sub-100-ms inference and &lt;8 GB VRAM requirements—making them suitable for edge deployment—though their low FID scores (StyleGAN 3: ~3.0 on FFHQ faces; VQ-VAE-2: 31.1 on ImageNet) reflect specialized rather than general-purpose capabilities.</p>
<section class="tw xbox font-sm" id="T3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Performance and applicability of GANs, VAEs, diffusion models, and Transformers in Metaverse scenarios</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" rowspan="1" colspan="1">Property</th>
<th align="center" rowspan="1" colspan="1">Evaluation benchmark (FID/FVD)</th>
<th align="center" rowspan="1" colspan="1">Representative</th>
<th align="center" rowspan="1" colspan="1">FID↓</th>
<th align="center" rowspan="1" colspan="1">CLIP score↑</th>
<th align="center" rowspan="1" colspan="1">Latency↓</th>
<th align="center" rowspan="1" colspan="1">Params (B)</th>
<th align="center" rowspan="1" colspan="1">Memory</th>
<th align="center" rowspan="1" colspan="1">Metaverse suitability</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Unconditional/category-conditioned</td>
<td align="center" rowspan="1" colspan="1">FFHQ (human faces)</td>
<td align="center" rowspan="1" colspan="1">StyleGAN3</td>
<td align="center" rowspan="1" colspan="1">~3.0</td>
<td align="center" rowspan="1" colspan="1">N/A</td>
<td align="center" rowspan="1" colspan="1">&lt;100 ms</td>
<td align="center" rowspan="1" colspan="1">~0.02</td>
<td align="center" rowspan="1" colspan="1">6–8 GB (single GPU)</td>
<td align="center" rowspan="1" colspan="1">Avatars, face edits</td>
</tr>
<tr>
<td rowspan="3" align="left" colspan="1">U0 text-to-image</td>
<td align="center" rowspan="1" colspan="1">ImageNet</td>
<td align="center" rowspan="1" colspan="1">VQ-VAE-2</td>
<td align="center" rowspan="1" colspan="1">31.1</td>
<td align="center" rowspan="1" colspan="1">N/A</td>
<td align="center" rowspan="1" colspan="1">&lt;80 ms</td>
<td align="center" rowspan="1" colspan="1">0.1–0.3</td>
<td align="center" rowspan="1" colspan="1">&lt;4 GB (single GPU)</td>
<td align="center" rowspan="1" colspan="1">Texture compression</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">COCO (30K)</td>
<td align="center" rowspan="1" colspan="1">Stable Diffusion 3</td>
<td align="center" rowspan="1" colspan="1">~6.0</td>
<td align="center" rowspan="1" colspan="1">~0.31</td>
<td align="center" rowspan="1" colspan="1">~1,000–1,500 ms</td>
<td align="center" rowspan="1" colspan="1">~2.0–2.5</td>
<td align="center" rowspan="1" colspan="1">10–12 GB (A100 preferred)</td>
<td align="center" rowspan="1" colspan="1">High-fidelity images</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">COCO (30K) or equivalent</td>
<td align="center" rowspan="1" colspan="1">DALL·E 3 (Transformers)</td>
<td align="center" rowspan="1" colspan="1">~10.39</td>
<td align="center" rowspan="1" colspan="1">~0.31</td>
<td align="center" rowspan="1" colspan="1">~2,000–3,000 ms</td>
<td align="center" rowspan="1" colspan="1">~12</td>
<td align="center" rowspan="1" colspan="1">≥20 GB (A100 preferred)</td>
<td align="center" rowspan="1" colspan="1">Creative design, storytelling</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Text-to-video</td>
<td align="center" rowspan="1" colspan="1">UCF-101 (FVD)</td>
<td align="center" rowspan="1" colspan="1">Make-A-Video (diffusion)</td>
<td align="center" rowspan="1" colspan="1">~18.5 (video FVD)</td>
<td align="center" rowspan="1" colspan="1">N/A</td>
<td align="center" rowspan="1" colspan="1">~5–10 s (per clip)</td>
<td align="center" rowspan="1" colspan="1">~5.3</td>
<td align="center" rowspan="1" colspan="1">&gt;24 GB (multi-GPU)</td>
<td align="center" rowspan="1" colspan="1">Not real time capable yet</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/T3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>We provide a quantitative summary of the resource requirements of state-of-the-art models for both image and video generation tasks. Table <a href="#T4" class="usa-link">4</a> details the total parameter size, average floating-point operations (FLOPs), memory requirements, and inference time on standard high-end hardware.</p>
<section class="tw xbox font-sm" id="T4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>Summary of model size, computation, and memory needs for generative tasks. FLOPs are estimated per single prompt and generation, assuming standard decoding steps. All metrics are based on FP16 mixed precision with optimized inference backends (e.g., Hugging Face Accelerate and TensorRT). Sora’s statistics are based on available public demos and research speculation, as the model architecture and training details remain unpublished.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" rowspan="1" colspan="1"></th>
<th align="center" rowspan="1" colspan="1">Model</th>
<th align="center" rowspan="1" colspan="1">Task</th>
<th align="center" rowspan="1" colspan="1">Parameters</th>
<th align="center" rowspan="1" colspan="1">FLOPs per generation</th>
<th align="center" rowspan="1" colspan="1">VRAM requirement</th>
<th align="center" rowspan="1" colspan="1">Inference hardware</th>
<th align="center" rowspan="1" colspan="1">Inference time</th>
</tr></thead>
<tbody>
<tr>
<td rowspan="3" align="left" colspan="1">Image generation models (512 × 512 output)</td>
<td align="center" rowspan="1" colspan="1">Stable Diffusion 3</td>
<td align="center" rowspan="1" colspan="1">Image generation (512 × 512)</td>
<td align="center" rowspan="1" colspan="1">~2.0–2.5B</td>
<td align="center" rowspan="1" colspan="1">~400–500 GFLOPs</td>
<td align="center" rowspan="1" colspan="1">~10–12 GB (FP16)</td>
<td align="center" rowspan="1" colspan="1">NVIDIA RTX 3090</td>
<td align="center" rowspan="1" colspan="1">~2–2.5 s</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">DALL E 3</td>
<td align="center" rowspan="1" colspan="1">Image generation (512 × 512)</td>
<td align="center" rowspan="1" colspan="1">~12B</td>
<td align="center" rowspan="1" colspan="1">~1–1.2 TFLOPs</td>
<td align="center" rowspan="1" colspan="1">~20 GB</td>
<td align="center" rowspan="1" colspan="1">Normalize to A100</td>
<td align="center" rowspan="1" colspan="1">Not comparable</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">Imagen 4</td>
<td align="center" rowspan="1" colspan="1">Image generation (512 × 512)</td>
<td align="center" rowspan="1" colspan="1">&gt;10B</td>
<td align="center" rowspan="1" colspan="1">~1.5–2.0 TFLOPs</td>
<td align="center" rowspan="1" colspan="1">&gt;24 GB (TPU v4/A100 equiv)</td>
<td align="center" rowspan="1" colspan="1">Google TPU v4</td>
<td align="center" rowspan="1" colspan="1">Not comparable</td>
</tr>
<tr>
<td rowspan="2" align="left" colspan="1">Video generation models</td>
<td align="center" rowspan="1" colspan="1">Make-A-Video</td>
<td align="center" rowspan="1" colspan="1">16 frames @ 256 × 256</td>
<td align="center" rowspan="1" colspan="1">~5.3B</td>
<td align="center" rowspan="1" colspan="1">&gt;2.5–3 TFLOPs</td>
<td align="center" rowspan="1" colspan="1">&gt;24 GB</td>
<td align="center" rowspan="1" colspan="1">NVIDIA A100</td>
<td align="center" rowspan="1" colspan="1">~20 s</td>
</tr>
<tr>
<td align="center" rowspan="1" colspan="1">Sora</td>
<td align="center" rowspan="1" colspan="1">48–120 frames @ 1,280 × 720*</td>
<td align="center" rowspan="1" colspan="1">&gt;10B (est.)</td>
<td align="center" rowspan="1" colspan="1">&gt;10 TFLOPs</td>
<td align="center" rowspan="1" colspan="1">&gt;40 GB (cluster)</td>
<td align="center" rowspan="1" colspan="1">Multi-GPU A100 cluster</td>
<td align="center" rowspan="1" colspan="1">&gt;30 s</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/T4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>The data above highlights that even high-end GPUs like the NVIDIA A100 or RTX 3090 struggle to generate high-resolution content from AIGC models in real time. Consider the following points: Real-time AR/VR requires latency below ~16 ms per frame (i.e., &gt;60 FPS), but current AIGC image generation takes seconds per frame even with hardware acceleration. Standalone AR/VR headsets (e.g., Meta Quest 3 and Apple Vision Pro) typically have &lt;16 GB of shared memory and integrated mobile GPUs (e.g., Snapdragon XR2 Gen 2), which cannot support the inference of billion-parameter models. Generation models such as Sora or Make-A-Video demand TFLOP-scale compute, multi-GPU setups, and substantial memory overhead, making on-device generation infeasible without cloud assistance.</p>
<p>Therefore, real-time AIGC deployment in AR/VR environments is currently constrained by hardware limitations. Future work should explore lightweight model design (e.g., diffusion distillation, quantization, and knowledge distillation), streaming-based rendering architectures, and edge–cloud hybrid deployment strategies to make high-fidelity generative experiences feasible in immersive systems. In addition to inference complexity, training large AIGC models incurs significant monetary cost, primarily due to the extensive GPU hours required.</p>
<p>Table <a href="#T5" class="usa-link">5</a> provides an estimate of training cost based on available literature, industry disclosures, and cloud GPU pricing benchmarks (assuming FP16 training on NVIDIA A100, $1.5 to $3.0/hour per GPU).</p>
<section class="tw xbox font-sm" id="T5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>Estimated training costs and resource requirements for large-scale AIGC models</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" rowspan="1" colspan="1">Model</th>
<th align="center" rowspan="1" colspan="1">Training data</th>
<th align="center" rowspan="1" colspan="1">Training GPU-hours (A100-equiv hours)<a href="#T5FN1" class="usa-link"><sup>a</sup></a>
</th>
<th align="center" rowspan="1" colspan="1">Estimated cost (US$)<a href="#T5FN2" class="usa-link"><sup>b</sup></a>
</th>
<th align="center" rowspan="1" colspan="1">Source reliability<a href="#T5FN3" class="usa-link"><sup>c</sup></a>
</th>
<th align="center" rowspan="1" colspan="1">Key assumptions</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Stable Diffusion 3</td>
<td align="center" rowspan="1" colspan="1">LAION-2B + proprietary<a href="#T5FN4" class="usa-link"><sup>d</sup></a>
</td>
<td align="center" rowspan="1" colspan="1">~200–300K</td>
<td align="center" rowspan="1" colspan="1">~600K–900K</td>
<td align="center" rowspan="1" colspan="1">High (official partners)</td>
<td align="center" rowspan="1" colspan="1">$3/A100-hour; public disclosures</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">DALL·E 3</td>
<td align="center" rowspan="1" colspan="1">Licensed images + synthetic text</td>
<td align="center" rowspan="1" colspan="1">~1.2–1.8M A100 h</td>
<td align="center" rowspan="1" colspan="1">~3.6M–5.4M</td>
<td align="center" rowspan="1" colspan="1">Medium (analyst estimates)</td>
<td align="center" rowspan="1" colspan="1">$3/A100-hour; scaled from architecture</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Imagen 3 (Google)</td>
<td align="center" rowspan="1" colspan="1">Curated internal dataset</td>
<td align="center" rowspan="1" colspan="1">~350K (TPU → GPU conv)<a href="#T5FN5" class="usa-link"><sup>e</sup></a>
</td>
<td align="center" rowspan="1" colspan="1">~1.0M–1.4M</td>
<td align="center" rowspan="1" colspan="1">Medium (research pubs)</td>
<td align="center" rowspan="1" colspan="1">TPU v4 → A100: 1.4× efficiency; $2.85/TPU-hour</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Make-A-Video (Meta)</td>
<td align="center" rowspan="1" colspan="1">2.3B video–text pairs</td>
<td align="center" rowspan="1" colspan="1">2.0–2.5M</td>
<td align="center" rowspan="1" colspan="1">~6M–7.5M</td>
<td align="center" rowspan="1" colspan="1">High (Meta publication)</td>
<td align="center" rowspan="1" colspan="1">$3/A100-hour; confirmed scale</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Sora (OpenAI, est.)</td>
<td align="center" rowspan="1" colspan="1">Undisclosed video (&gt;100M clips†)</td>
<td align="center" rowspan="1" colspan="1">&gt;8–12M</td>
<td align="center" rowspan="1" colspan="1">&gt;24M<a href="#T5FN6" class="usa-link"><sup>f</sup></a>
</td>
<td align="center" rowspan="1" colspan="1">Low (speculation)</td>
<td align="center" rowspan="1" colspan="1">Extrapolated from output quality; no official data</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/T5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p">
<div class="fn" id="T5FN1">
<sup>
<sup>a</sup>
</sup><p class="display-inline">All costs normalized to $3.00/A100-hour (on-demand cloud pricing).</p>
</div>
<div class="fn" id="T5FN2">
<sup>
<sup>b</sup>
</sup><p class="display-inline">±20% variance possible from optimal cluster utilization.</p>
</div>
<div class="fn" id="T5FN3">
<sup>
<sup>c</sup>
</sup><p class="display-inline">Reliability tiers: High (official disclosure), Medium (peer-extrapolated), AND Low (speculative).</p>
</div>
<div class="fn" id="T5FN4">
<sup>
<sup>d</sup>
</sup><p class="display-inline">Stable Diffusion 3 costs reflect open-source efficiency gains (architectural distillation, LAION-2B curation).</p>
</div>
<div class="fn" id="T5FN5">
<sup>
<sup>e</sup>
</sup><p class="display-inline">TPU→GPU conversion: 1 TPU v4 core ≈ 1.4 A100s (MLPerf v3.0 inference benchmarks).</p>
</div>
<div class="fn" id="T5FN6">
<sup>
<sup>f</sup>
</sup><p class="display-inline">Sora estimate derived from minimum viable training scale for 720p/60s physics-rendered video.</p>
</div>
</div></section><p>As summarized in Table <a href="#T5" class="usa-link">5</a>, training AIGC models requires multi-million dollar investment, often only affordable to large corporations or institutions with access to proprietary data and massive compute clusters. Such training is typically only feasible for large corporations or institutions with access to proprietary datasets and massive compute clusters. Additionally, the environmental impact of training billion-scale models raises sustainability concerns, particularly with respect to carbon emissions.</p>
<p>These combined constraints explain why current AR/VR platforms cannot support on-device generation or customization of high-fidelity AIGC content. To overcome these challenges, future research should focus on lightweight model design (e.g., quantization, knowledge distillation, and diffusion acceleration), efficient adaptation techniques (e.g., low-rank adaptation [LoRA] and adapter layers), and edge–cloud hybrid architectures that offload heavy computation to remote servers while maintaining low-latency user experiences. Such strategies are essential to making AIGC viable for real-time, immersive applications.</p>
<p>Table <a href="#T6" class="usa-link">6</a> provides a comprehensive evaluation framework comparing GANs, diffusion models, and Transformers across 3 critical dimensions: realism, coherence, and user engagement—with domain-specific quantitative benchmarks. For realism, GANs achieve domain-specific excellence (FID 2 to 4 on FFHQ faces), diffusion models lead in open-domain fidelity (FID 5 to 6 on COCO), while Transformers offer superior multimodal realism (CLIP score 0.30 to 0.33). In coherence evaluation, diffusion models demonstrate controllable consistency (FVD 250 to 300 for video), whereas Transformers excel in cross-modal alignment (Winoground score 45% to 55%, temporal FVD 180 to 220)—both significantly outperforming GANs’ limited image-level consistency. For user engagement, GANs enable real-time interaction (&lt;100 ms latency, 4 to 8 GB VRAM) ideal for avatar applications, while diffusion and Transformer models trade higher latency for semantically rich content that drives deeper engagement despite demanding &gt;24 GB VRAM. This structured analysis confirms GANs’ dominance in latency-sensitive tasks like avatar generation, diffusion models’ advantage in high-fidelity scene rendering, and Transformers’ superiority in cross-modal storytelling—with each architecture’s hardware constraints directly informing their Metaverse applicability.</p>
<section class="tw xbox font-sm" id="T6"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>Comparative evaluation of generative model families across realism, coherence, and user engagement dimensions</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" rowspan="1" colspan="1">Aspect</th>
<th align="left" rowspan="1" colspan="1">GANs</th>
<th align="left" rowspan="1" colspan="1">Diffusion models</th>
<th align="left" rowspan="1" colspan="1">Transformers</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Realism</td>
<td align="left" rowspan="1" colspan="1">Domain-specific excellence<br>• FID: 2–4 (FFHQ)<br>• Precise texture rendering</td>
<td align="left" rowspan="1" colspan="1">Open-domain fidelity<br>• FID: 5–6 (COCO)<br>• CLIP score: 0.30–0.32</td>
<td align="left" rowspan="1" colspan="1">Multimodal realism<br>• FID: 8–12 (COCO)<br>• CLIP score: 0.30–0.33</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Coherence</td>
<td align="left" rowspan="1" colspan="1">Image-level only<br>•No cross-frame consistency<br>• Limited semantic control</td>
<td align="left" rowspan="1" colspan="1">Controllable consistency<br>• FVD: 250–300 (UCF-101)<br>• Layout accuracy: 75%–85%</td>
<td align="left" rowspan="1" colspan="1">Cross-modal alignment<br>• Winoground score: 45%–55%<br>• Temporal FVD: 180–220</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">User engagement</td>
<td align="left" rowspan="1" colspan="1">Low latency enables interactive applications (e.g., face editing, avatars) (real-time interaction)</td>
<td align="left" rowspan="1" colspan="1">High-quality outputs attract engagement but suffer from inference latency (VRAM: 4–8 GB [INT8] quality vs. speed trade-off)</td>
<td align="left" rowspan="1" colspan="1">High creative potential and multimodal generation encourage active user input (requires &gt;24 GB VRAM)</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/T6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>Overall, we systematically compare the capabilities of GANs, VAEs, diffusion models, and Transformers for Metaverse applications across key dimensions such as latency, fidelity, controllability, and deployability. Through both qualitative and quantitative analyses, we highlight their respective strengths, limitations, and hardware constraints.</p></section><section id="sec9"><h3 class="pmc_sec_title">Generative models in transition: Comparative insights into GANs, VAEs, Diffusion, Transformers, and Mamba</h3>
<p>In the domain of generative models (AIGC), architectures such as GANs, VAEs, diffusion models, Transformers, and Mamba represent distinct theoretical foundations and developmental trajectories in generation mechanisms. Each of these models approaches generative tasks from different perspectives—probabilistic modeling, sequence modeling, or energy-based modeling—forming unique systems with their own strengths and challenges.</p>
<p>GANs and VAEs embody fundamentally different probabilistic frameworks. GANs employ an adversarial setup rooted in game theory, where a generator network learns to map noise vectors to the data distribution, while a discriminator learns to distinguish between real and synthetic samples. The generator improves by receiving dynamic loss signals from the discriminator, ideally reaching a Nash equilibrium where the generated samples become indistinguishable from real ones. A major strength of GANs lies in their implicit, likelihood-free optimization, which enables the generation of perceptually high-quality outputs—particularly effective in synthesizing high-dimensional data such as images and audio. However, GANs suffer from notorious training instability, including mode collapse and vanishing gradients. The theoretical basis leans more on minimax optimization and divergences like Jensen–Shannon, rather than explicit probabilistic modeling.</p>
<p>In contrast, VAEs adopt a variational inference-based framework with an explicit probabilistic encoder–decoder architecture. The encoder maps inputs to a structured latent space constrained by a Gaussian prior (typically enforced via KL divergence), and the decoder reconstructs data from this compressed representation. This results in a smooth, continuous latent manifold, well-suited for controlled sampling, albeit often at the cost of blurry outputs due to trade-offs in the approximated posterior. VAEs excel in uncertainty modeling but are typically less flexible than GANs in generating high-resolution outputs. Architecturally, GANs tend to rely on deep convolutional networks to capture sharp, local features, while lacking inherent encoding capability. VAEs, on the other hand, provide principled latent representations at the expense of generative sharpness.</p>
<p>Diffusion models represent a fundamentally different generative paradigm. These models conceptualize generation as the reverse of a forward diffusion process that gradually corrupts data by adding noise, transforming it into a Gaussian distribution. The model learns the reverse denoising process to reconstruct the original data distribution, typically modeled as a Markov chain or via stochastic differential equations, as seen in DDPMs and SGMs. Architecturally, diffusion models often use U-Net backbones enhanced with residual connections and attention mechanisms to capture multi-scale features during denoising. One of their standout advantages is high sample quality and training stability, as they allow fine-grained control over the generative process. However, due to their iterative nature, inference is significantly slower compared to other models, posing a bottleneck for practical deployment. Recent advancements such as DDIM and latent diffusion aim to address this issue by accelerating the sampling process.</p>
<p>Transformers, initially designed for sequence modeling, have redefined generative modeling through their attention-based architecture. Theoretically, Transformers rely on self-attention mechanisms to model global dependencies in data, making them highly effective for capturing complex, long-range relationships. This is formalized through multi-head attention, which computes weighted interactions among all input tokens. In generative contexts, Transformers process tokenized inputs—such as image patches or text subwords—through stacked layers of multi-head attention and feedforward networks. This enables powerful context modeling and parallel training over long sequences, though it comes with quadratic computational complexity with respect to sequence length. Extensions such as DiT and PixArt-α adapt the Transformer architecture for image and video synthesis by tokenizing inputs and applying cross-attention for conditional generation. Unlike GANs or VAEs, Transformers are highly scalable and effective for multimodal and multitask scenarios. However, they are computationally expensive and memory-intensive, especially when handling high-resolution visual data.</p>
<p>Mamba introduces a novel architecture based on state space models, aiming to overcome the quadratic complexity bottleneck that Transformers face when processing long sequences. Unlike attention-based models, Mamba’s core innovation lies in its structured SSM layers, which efficiently model long-range dependencies with linear time complexity and sublinear memory growth. This makes it especially well-suited for long-context generation tasks. Mamba integrates input-dependent dynamics, allowing the model to selectively retain or forget information based on the current input, along with hardware-friendly state expansion mechanisms. To make Mamba viable for generative tasks like image synthesis, models such as DiM and ZigMa enhance the architecture with strategies like alternating scan patterns to maintain continuity in 2D spatial data and mitigate artifacts introduced by sequential scanning.</p>
<p>From a theoretical standpoint, Mamba strikes a balance between the stateful representation power of RNNs and the training efficiency of CNNs. While Transformers handle token interactions via explicit attention weights, Mamba captures dependencies implicitly through input-driven state evolution. This design trades some of the interpretability of global interactions for superior scalability, particularly valuable in dense generation tasks such as image and video synthesis. Key architectural innovations include selective state expansion and directional scanning strategies (e.g., zigzag patterns). Consequently, Mamba excels in balancing model expressiveness with computational efficiency, effectively bypassing the Transformer’s scaling issues. Its lightweight design contrasts sharply with the heavy computational demands of Transformers and diffusion models, positioning it as a promising and efficient alternative for scalable generative applications—especially those involving long sequences or high-resolution content—though its application in high-fidelity synthesis is still rapidly evolving.</p>
<p>In summary, these models each offer unique strengths in generative mechanisms and capabilities. VAEs emphasize structured latent spaces; GANs focus on perceptual realism; diffusion models prioritize stability and controllability; Transformers bring unmatched sequence modeling power; and Mamba charts a new path toward efficient, scalable generation. Their evolution reflects broader trends in generative modeling—from probabilistic to structural modeling, and from single-modal to multimodal generation. A key challenge for future models will be integrating the advantages of these diverse architectures to achieve a holistic balance of quality, efficiency, and controllability in generation.</p></section><section id="sec10"><h3 class="pmc_sec_title">AIGC driving the future of content creation</h3>
<p>Building upon foundational deep learning models, AIGC has made significant strides in text-to-image, text-to-video, and text-to-3D scene generation. In text-to-image synthesis, the integration of Transformers and diffusion models has enhanced both fidelity and efficiency, enabling high-resolution, semantically accurate image generation. In text-to-video generation, advancements in spatiotemporal modeling, diffusion techniques, and multimodal fusion have facilitated the creation of longer, more dynamic sequences with improved semantic consistency and physical plausibility. For 3D scene generation, the combination of NeRF, geometry-guided diffusion, and NeRF optimization has overcome limitations in sparse-view reconstruction, enabling high-fidelity, interactive 3D environments. With the continuous advancement in integrating AIGC with robotic technologies, robotics is transforming AIGC’s “digital creativity” into tangible “physical productivity”, while simultaneously driving the evolution of AI models through real-time data feedback loops. These breakthroughs not only streamline content creation but also lay the technical foundation for the development of the Metaverse.</p>
<section id="sec11"><h4 class="pmc_sec_title">Image generation</h4>
<p>In the field of text-to-image generation, numerous novel generative paradigms and innovative technologies have emerged in recent years. OpenAI’s DALL-E [<a href="#B105" class="usa-link" aria-describedby="B105">105</a>] pioneered Transformer-based synthesis via a 2-stage framework: compressing images into 32 × 32 discrete tokens with VQVAE, then modeling text–image joint distributions via autoregressive Transformers, achieving zero-shot capabilities and highlighting computational scaling’s role. Guided Language to Image Diffusion for Generation and Editing (GLIDE) [<a href="#B106" class="usa-link" aria-describedby="B106">106</a>] validated classifier-free guidance for diffusion models, directly inspiring DALL-E2 [<a href="#B107" class="usa-link" aria-describedby="B107">107</a>], which combined CLIP embeddings with hierarchical diffusion to achieve 4× resolution gains (1,024 × 1,024). DALL-E3 [<a href="#B108" class="usa-link" aria-describedby="B108">108</a>] enhanced prompt adherence 2.3× via synthetic caption refinement, improving rare-object and spatial modeling. Google’s Imagen [<a href="#B109" class="usa-link" aria-describedby="B109">109</a>] leveraged frozen T5 [<a href="#B110" class="usa-link" aria-describedby="B110">110</a>] large language models (LLMs) with diffusion cascades, showing that language model scaling improved FID scores 37% vs. 12% for image models, while its 2-stage super-resolution reduced memory by 83%. Parti [<a href="#B111" class="usa-link" aria-describedby="B111">111</a>] used ViT-VQGAN tokenization and a 20B-parameter Transformer for autoregressive 1,024 px generation, outperforming diffusion models in compositional coherence by 19%. Muse [<a href="#B111" class="usa-link" aria-describedby="B111">111</a>] accelerated synthesis 10× via masked token modeling (12 steps vs. 128), with StyleDrop [<a href="#B112" class="usa-link" aria-describedby="B112">112</a>] capturing artistic styles via 60K-parameter tuning (94% user preference). Stable diffusion’s latent-space optimization revolutionized efficiency: compressing 512 px images into 64 × 64 tensors enabled 4.8× faster diffusion in 48D space, paired with dynamic CLIP–text fusion for precise attribute binding. Recent studies have redefined model interpretability and efficiency. Tang et al. [<a href="#B113" class="usa-link" aria-describedby="B113">113</a>] explored syntactic influences in diffusion models through DAAM (diffusion attentive attribution maps), mapping linguistic structures to pixel-level heatmaps to decode how grammar shapes image generation. Concurrently, Feng et al. [<a href="#B114" class="usa-link" aria-describedby="B114">114</a>] devised training-free structured guidance by modifying cross-attention layers, enabling precise attribute-object binding while preserving multi-attribute semantics. On the optimization front, Li et al. introduced Colossal-AI, a framework enhancing resource efficiency for large generative models. Building on this, Stability AI’s SD-XL [<a href="#B113" class="usa-link" aria-describedby="B113">113</a>] integrates an enhanced U-Net backbone with advanced conditioning techniques—such as resolution-aware embeddings—and a robust text encoder, achieving higher fidelity and scalability. These innovations highlight a trajectory toward hybrid architectures (diffusion + Transformers + LLMs), data-engineered precision, and efficient latent processing, paving the way for real-time, high-fidelity generation with granular user control.</p>
<p>The field of text-to-image generation is advancing toward a synthesis of multimodal intelligence, computational efficiency, and human-centric creativity. Future progress will likely hinge on unifying diffusion models, autoregressive Transformers, and LLMs into hybrid architectures that embed deeper semantic understanding while resolving persistent challenges in spatial reasoning and rare-object composition. Innovations in dataset engineering—such as synthetic caption refinement and grammar-aware annotation—could bridge the gap between textual precision and visual fidelity, particularly for complex scenes. Simultaneously, lightweight adaptation techniques like parameter-efficient fine-tuning and resolution-aware cascades promise to democratize high-quality generation, enabling real-time, interactive applications on consumer devices. Emerging interpretability frameworks, such as attention manipulation guided by linguistic structures, may unlock finer control over attribute binding and compositional logic. As computational optimizations in latent-space processing and adaptive compression mature, the boundary between photorealistic accuracy and artistic stylization will blur, fostering tools that dynamically adapt to user intent. These advancements, coupled with open-source ecosystems and ethical dataset practices, could redefine content creation in the Metaverse—ultimately enabling systems that transcend static image synthesis to become collaborative, context-aware partners in visual storytelling.</p></section><section id="sec12"><h4 class="pmc_sec_title">Video generation</h4>
<p>The evolution of video synthesis technology has been driven by a series of iterative architectural innovations, accompanied by pivotal shifts in methodology. Early research by Vondrick et al. [<a href="#B115" class="usa-link" aria-describedby="B115">115</a>] pioneered a GAN-based spatiotemporal modeling architecture, introducing the separation of dynamic foreground elements from static backgrounds in both spatial and temporal dimensions. This groundbreaking work laid the foundation for subsequent studies, with Tulyakov et al. [<a href="#B116" class="usa-link" aria-describedby="B116">116</a>] proposing a recurrent framework to decompose motion embeddings from content representations, further enhanced by the introduction of 3D convolutional networks [<a href="#B117" class="usa-link" aria-describedby="B117">117</a>,<a href="#B118" class="usa-link" aria-describedby="B118">118</a>], which improved temporal consistency by 37% compared to 2D methods. Concurrently, Karras et al. [<a href="#B119" class="usa-link" aria-describedby="B119">119</a>] introduced a progressive refinement paradigm, generating high-resolution videos through iterative spatiotemporal upsampling.</p>
<p>The rise of Transformer technology revolutionized video generation strategies. Wu et al. [<a href="#B120" class="usa-link" aria-describedby="B120">120</a>] first integrated VQ-VAE [<a href="#B121" class="usa-link" aria-describedby="B121">121</a>] with 3D sparse attention mechanisms, enabling open-domain text-to-video synthesis. Subsequent research further advanced the field: Hong et al. [<a href="#B122" class="usa-link" aria-describedby="B122">122</a>] proposed a multi-frame-rate hierarchical training method to effectively align textual semantics with visual narratives, while Kondratyuk et al. [<a href="#B123" class="usa-link" aria-describedby="B123">123</a>] unified multimodal inputs (e.g., text, depth maps, and optical flow) into token sequences processable by LLMs, achieving 91% accuracy in complex scene transition tasks. The introduction of diffusion models marked another significant breakthrough in video synthesis. Ho et al. [<a href="#B124" class="usa-link" aria-describedby="B124">124</a>] extended 2D U-Net to 3D video denoisers and optimized them with pseudo-3D convolutional layers [<a href="#B125" class="usa-link" aria-describedby="B125">125</a>], reducing computational costs by 58%. Zhang et al. [<a href="#B126" class="usa-link" aria-describedby="B126">126</a>] proposed a hybrid architecture combining pixel-space initialization with latent upsampling, enabling the generation of 4K-resolution videos at 30 fps. Additionally, Blattmann et al. [<a href="#B127" class="usa-link" aria-describedby="B127">127</a>] emphasized the importance of high-quality dataset curation, with their filtered training corpus improving the realism metrics of generated videos by 42%.</p>
<p>Despite significant progress, the quality of generated videos still faces numerous challenges. Studies show that current systems exhibit perceptual discrepancies in 38% of generated sequences [<a href="#B128" class="usa-link" aria-describedby="B128">128</a>], particularly struggling to maintain physical plausibility in clips longer than 5 s. Furthermore, instruction misalignment manifests as cumulative semantic drift (0.9% per unit of prompt complexity), which traditional evaluation metrics like FVD [<a href="#B129" class="usa-link" aria-describedby="B129">129</a>] fail to detect effectively. The inadequacy of evaluation frameworks is also pronounced, as no automated framework has achieved a correlation greater than 0.72 with human assessments across 14 quality dimensions [<a href="#B130" class="usa-link" aria-describedby="B130">130</a>], highlighting the need for developing multimodal benchmarking systems, such as those integrating physics-aware validators [<a href="#B131" class="usa-link" aria-describedby="B131">131</a>] and causal relationship trackers. Recent research has proposed neural ODE (ordinary differential equations)-based temporal modeling methods [<a href="#B132" class="usa-link" aria-describedby="B132">132</a>] and the Phenaki model [<a href="#B133" class="usa-link" aria-describedby="B133">133</a>]—the latter capable of generating variable-length, high-quality videos from a series of open-domain text prompts—offering new possibilities for open-domain video generation and complex narrative tasks. However, practical applications still require further breakthroughs in long-range dependency modeling and efficient sampling techniques.</p></section><section id="sec13"><h4 class="pmc_sec_title">3D scene generation</h4>
<p>3D environments are foundational to digital media, gaming, and immersive platforms, but their creation typically requires multi-view data, extensive computational resources, and expert intervention, making the process costly. To address these challenges, researchers have proposed various innovative approaches. The dominant paradigm, “external-focused synthesis,” simulates real-world observation by capturing scenes from internal viewpoints. For example, Text2Room [<a href="#B134" class="usa-link" aria-describedby="B134">134</a>] generates room-scale 3D meshes from text, Text2NeRF [<a href="#B135" class="usa-link" aria-describedby="B135">135</a>] combines NeRF with diffusion models for zero-shot scene synthesis, RoomDreamer [<a href="#B136" class="usa-link" aria-describedby="B136">136</a>] enhances structural coherence through geometry-guided diffusion, and FastScene [<a href="#B137" class="usa-link" aria-describedby="B137">137</a>] achieves rapid reconstruction using 3D Gaussian splatting. Additionally, alternative methods like MAV3D [<a href="#B138" class="usa-link" aria-describedby="B138">138</a>] and 4D-fy [<a href="#B139" class="usa-link" aria-describedby="B139">139</a>] focus on dynamic scene generation, while perpetual view exploration frameworks such as SceneScape [<a href="#B140" class="usa-link" aria-describedby="B140">140</a>] and Vivid-Dream [<a href="#B141" class="usa-link" aria-describedby="B141">141</a>] enable limitless scene navigation. Object-compositional approaches like CompoNeRF [<a href="#B142" class="usa-link" aria-describedby="B142">142</a>] and Set-the-Scene [<a href="#B143" class="usa-link" aria-describedby="B143">143</a>] treat scenes as modular components, supporting flexible editing and synthesis.</p>
<p>Emerging technological advancements, exemplified by innovations like Apple Vision Pro, are propelling the development of the Metaverse from the proof-of-concept phase into a new era characterized by practical applications, heightened immersion, and multimodal integration. The synergistic combination of spatial computing and AIGC facilitates the evolutionary transition from a “virtual world” to a symbiotic coexistence of virtual and physical realities. Notably, Vision Pro’s Room Mesh capability leverages AIGC techniques to generate interactive 3D spatial models in real time, enabling users to directly “grasp” and position virtual objects on physical surfaces. This system achieves near-natural interaction by replacing traditional controllers with advanced biometric recognition (eye-tracking and gesture control), thereby approaching the critical threshold of seamless human–computer interface. This technological progression demonstrates 3 significant paradigm shifts: the realization of dynamic spatial computing through AI-generated 3D reconstruction, the emergence of intuitive interaction modalities that minimize cognitive load, and the creation of persistent hybrid environments where digital and physical elements maintain coherent spatial relationships.</p>
<p>These developments not only enhance user presence and operational fidelity but also establish foundational infrastructure for next-generation mixed reality applications across professional domains including industrial design, medical visualization, and collaborative engineering. The technical architecture exemplifies how contemporary AIGC implementations are overcoming historical limitations in real-time rendering latency and interaction naturalness that previously constrained Metaverse adoption.</p>
<p>Looking ahead, 3D environment generation technologies will evolve toward hybrid neural-explicit representations (e.g., NeRF–mesh fusion), physics-aware constraints, and lightweight on-device optimization. By integrating multimodal inputs (e.g., sketches, voice, or LLMs), the barriers to 3D creation are expected to be further lowered, bridging the gap between creative ideation and technical execution. These advancements will not only reduce reliance on expert knowledge but also markedly enhance the realism and interactivity of virtual environments, driving further progress in digital media and immersive experiences.</p></section><section id="sec14"><h4 class="pmc_sec_title">Robotics–AIGC</h4>
<p>Robotics is emerging as a critical interface between AIGC and the physical world, serving as an essential conduit for bidirectional interaction through embodied AI systems. This paradigm enables the translation of digital content into physical actions while simultaneously incorporating real-world feedback into AI training loops, thereby establishing a closed-loop learning framework.</p>
<p>State-of-the-art multimodal LLMs (e.g., GPT-4V and PaLM-E) demonstrate remarkable capabilities in parsing natural language instructions (such as “organize the cluttered table”) and generating executable robotic action sequences (grasping, sorting, and placing). As shown in Fig. <a href="#F6" class="usa-link">6</a>A, the Google RT-2-X [<a href="#B144" class="usa-link" aria-describedby="B144">144</a>] model exemplifies this advancement by integrating vision-language models with robotic control systems, achieving zero-shot execution of novel tasks (e.g., “hand the cola to the waving person”) without prior specific training.</p>
<figure class="fig xbox font-sm" id="F6"><h5 class="obj_head">Fig. 6.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c7c8/12364526/58beadf1e3f5/research.0804.fig.006.jpg" loading="lazy" height="510" width="660" alt="Fig. 6."></p>
<div class="p text-right font-secondary"><a href="figure/F6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The collaborative development of robotics and AIGC. Zoom in for a better view. (A) RT-2 [<a href="#B144" class="usa-link" aria-describedby="B144">144</a>] overview. (B) Examples of modality conversion when applying MR-GAN and LR-GAN to cube-like objects with complex textures [<a href="#B145" class="usa-link" aria-describedby="B145">145</a>].</p></figcaption></figure><p>Conversely, robotic systems function as physical “sensors” for AIGC frameworks, with their integrated force, tactile, and visual sensors continuously capturing real-time physical interaction data to enhance AIGC model training. The ViTacTip [<a href="#B145" class="usa-link" aria-describedby="B145">145</a>] tactile sensor shown in Fig. <a href="#F6" class="usa-link">6</a>B, for instance, employs visual signals to indirectly measure tactile information, thereby improving AI models’ understanding of physical interactions. Motion data acquired by Tesla’s Optimus humanoid robot in real-world environments contributes to training more versatile embodied AI models capable of dynamic scene understanding.</p>
<p>This symbiotic relationship demonstrates how robotics transforms AIGC’s “digital creativity” into tangible “physical productivity” while simultaneously driving continuous AI model evolution through real-time data feedback loops.</p></section></section><section id="sec15"><h3 class="pmc_sec_title">Connection of AIGC to the Metaverse</h3>
<p>The development of the Metaverse confronts several fundamental challenges, including inefficient content production, suboptimal interactive experiences, constrained physical simulations, and limited personalization capabilities. AIGC fundamentally addresses the content scalability bottleneck in Metaverse construction by revolutionizing production pipelines through GAI models.</p>
<p>AIGC effectively resolves computational and cost barriers in 3D asset generation. Traditional 3D modeling requires manual creation of high-polygon models, whereas NeRF technology generates implicit neural representations of 3D scenes directly from 2D image inputs, reducing modeling time from hours to minutes. Multilayer perceptrons learn mapping functions from 3D spatial coordinates to color and density values <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M1" display="inline" overflow="linebreak"><mi>F</mi><mfenced open="(" close=")" separators=",,,,"><mi>x</mi><mi>y</mi><mi>z</mi><mi>θ</mi><mi>ϕ</mi></mfenced><mo>→</mo><mfenced open="(" close=")" separators=","><mi>RGB</mi><mi>σ</mi></mfenced></math></span>, eliminating the need for explicit mesh structures. Stable Diffusion 3D [<a href="#B146" class="usa-link" aria-describedby="B146">146</a>] bypasses conventional Blender/Maya pipelines by directly generating textured 3D models from text prompts using diffusion models. NVIDIA Omniverse benchmarks demonstrate that AIGC tools can improve 3D scene production efficiency by orders of magnitude—scenes requiring 1 week through traditional methods can now be generated within minutes using AI.</p>
<p>AIGC ensures multimodal content consistency through unified vector space mapping. Technologies like OpenAI’s CLIP [<a href="#B147" class="usa-link" aria-describedby="B147">147</a>] embed text, images, and 3D models into a shared latent space, maintaining semantic coherence across modalities. Advanced systems like Google’s Parti [<a href="#B148" class="usa-link" aria-describedby="B148">148</a>] employ jointly trained text–image–3D diffusion models with cross-attention mechanisms to synchronize multimodal content generation. These technological advancements essentially transform Metaverse content production from manual workflows to AI-driven pipelines, where scalability is no longer constrained by human resources but rather determined by computational power and data availability-constituting the foundational infrastructure for the “infinite Metaverse” paradigm in the Web3.0 era.</p>
<p>The Metaverse scene generation encompasses the large-scale creation of virtual environments, including buildings, landscapes, and interactive objects. AIGC plays a crucial role in the construction and evolution of the Metaverse by addressing content scarcity and enabling the rapid creation of virtual scenes, objects, characters, and events. This capability accelerates the replication of the physical world within digital environments and promotes the unlimited expansion of virtual content. AIGC technology expedites the creation of diverse virtual scenes, characters, buildings, and products, facilitating user interaction and engagement within the virtual realm. AIGC enhances architectural design efficiency and provides dynamic, personalized, and interactive experiences in the Metaverse. Additionally, AIGC enables the real-time and personalized generation of game elements, unique gameplay mechanics, and adaptive dialogues and interactions in virtual social contexts. These capabilities enhance user immersion by creating vibrant, context-specific experiences tailored to individual preferences. For instance, text generation technology, powered by NLP, can be applied in both non-interactive and interactive scenarios. Non-interactive text generation includes tasks such as summarization, title generation, text style transfer, and article generation, while interactive text generation is applied in chatbots and text-based interactive games. The development of the Metaverse confronts several fundamental challenges like AI Dungeon [<a href="#B149" class="usa-link" aria-describedby="B149">149</a>]. The integration of interactive text generation technology has become increasingly vital for enhancing the Metaverse experience, as it markedly improves interaction realism and content diversity through dynamic response to user inputs, environmental narrative generation, and virtual character behavior simulation. Recent advancements in AIGC for interactive text applications have demonstrated substantial progress, particularly exemplified by 2 groundbreaking developments. The May 2024 release of OpenAI’s GPT-4o represents a major leap forward with its unified multimodal architecture that enables seamless end-to-end processing across text, speech, images, and video modalities, while simultaneously achieving remarkable latency reduction to 232 ms in voice interactions through innovative neural compression techniques. Complementing this advancement, NVIDIA’s Genesis technology within the Omniverse ecosystem has revolutionized physical simulations by replacing conventional rigid-body dynamics computations with neural network-based approaches, resulting in a threefold improvement in the authenticity of virtual object interactions. These technological breakthroughs collectively address 2 fundamental challenges in Metaverse development: ensuring continuous cross-modal interaction fidelity and maintaining physics-based behavioral authenticity, thereby pushing the boundaries of what is achievable in virtual environment realism and user engagement. The convergence of these innovations marks a significant milestone in creating more immersive and responsive virtual worlds that better bridge the gap between digital and physical experiences.</p>
<p>In the realm of image generation, AIGC supports both image editing/modification and autonomous image generation. Image editing includes tasks like super-resolution, repair, face replacement, and background removal, while autonomous image generation uses deep learning and GANs to generate images based on descriptions, such as DALL-E [<a href="#B105" class="usa-link" aria-describedby="B105">105</a>], which can create high-quality images from user-provided descriptions and demonstrate exceptional detail handling capabilities.</p>
<p>Video generation technology further automates content creation, allowing for the efficient production of dynamic and high-quality video content from simple text prompts. Products like VEED [<a href="#B150" class="usa-link" aria-describedby="B150">150</a>] can automatically generate short videos based on textual input. Additionally, research projects like Make-A-Video [<a href="#B58" class="usa-link" aria-describedby="B58">58</a>] and Glia-Cloud [<a href="#B151" class="usa-link" aria-describedby="B151">151</a>] have achieved text-to-video generation. For instance, OpenAI’s “Sora” model can generate up to 60-s high-definition videos through text commands, with detailed backgrounds, multi-angle shots, and emotional characters [<a href="#B152" class="usa-link" aria-describedby="B152">152</a>]. This provides content creators with tools to bring ideas to life with lower costs and higher efficiency.</p>
<p>We can glimpse that emerging collaborative protocols bridge human creativity with generative precision—architects sketch concepts refined into physics-compliant 3D models via bounding-box-guided diffusion, while screenwriters trigger multi-angle cinematic previz through text–video synthesis. Crucially, neural ODE-based physics validation ensures that synthetic objects obey virtual world laws, forming a tripartite framework where human intent, AI generation, and digital physics co-evolve. These advancements dissolve traditional production silos, reimagining Metaverse content pipelines as open networks of human-AI co-creation. The endgame mirrors Neal Stephenson’s “Metaverse protocol”—a living digital civilization where every user, armed with natural language interfaces, becomes both consumer and creator, collectively weaving an ever-evolving tapestry of immersive experiences through seamless access to generative comp.</p>
<p>With the assistance of AIGC, the Metaverse is poised to shine in fields such as healthcare, education, psychology, and sociology. For example, in the healthcare sector, the Metaverse plays a transformative role, particularly in medical training and patient support.</p>
<section id="sec16"><h4 class="pmc_sec_title">Healthcare</h4>
<p>With a rapidly aging population and a shortage of healthcare workers, the Metaverse as well as AIGC can also be used in healthcare, including telemedicine, clinical care, physical health, mental health, and more [<a href="#B153" class="usa-link" aria-describedby="B153">153</a>,<a href="#B154" class="usa-link" aria-describedby="B154">154</a>]. The biggest advantages of Metaverse are that it eliminates geographic limitations, enhances the traditional Internet in 3D, and makes the experience immersive through an immersive experience platform.</p>
<p>In particular, the Metaverse offers a broad spectrum of transformative use cases in healthcare, reshaping how care can be delivered, accessed, and experienced. AIGC enables the creation of realistic simulations, allowing practitioners to conduct virtual consultations, perform diagnostics, and even simulate surgical procedures with unparalleled precision. This capability eliminates geographical barriers, ensuring that individuals in remote or underserved regions have access to quality healthcare. These virtual patient consultations in the Metaverse could elevate otherwise staid telemedicine, allowing immersive 3D interactions between patients and physicians. Patients could also gain rehabilitation and physical therapy benefits from gamified exercises and remote monitoring within the Metaverse in AIGC-generated environments. In addition to physical health, mental healthcare has the potential to be revolutionized with Metaverse-based therapies that provide safe spaces for treating posttraumatic stress syndrome, anxiety, and depression.</p>
<p>Medical education becomes more immersive with anatomy explorations, virtual dissections, and simulated hospital environments within the Metaverse via AIGC. The Metaverse could also ultimately support remote surgery, chronic disease management, and real-time health monitoring through wearable devices that integrate access to these technologies.</p>
<p>Beyond standard healthcare, the Metaverse could also foster community care via virtual support groups for patients with rare or chronic illnesses and introduces therapeutic gaming for neurorehabilitation in comfortable and familiar AIGC generated environments. Additionally, drug development and clinical trials could be expedited through collaborative virtual simulations [<a href="#B155" class="usa-link" aria-describedby="B155">155</a>].</p>
<p>The Metaverse can also enhance palliative care by offering AIGC optimized immersive experiences for hospice patients, promotes fitness and preventative care with interactive virtual wellness programs, and prepares healthcare professionals for disaster response with virtual training. All in all, these applications collectively highlight the Metaverse’s capacity to create a more innovative, patient-centric, and accessible healthcare ecosystem [<a href="#B156" class="usa-link" aria-describedby="B156">156</a>].</p>
<p>The Metaverse, as an immersive virtual environment, enables new forms of interaction and training, such as VR-based social skills training for individuals with autism spectrum disorder [<a href="#B157" class="usa-link" aria-describedby="B157">157</a>] and posttraumatic stress disorder [<a href="#B158" class="usa-link" aria-describedby="B158">158</a>]. Although current Metaverse implementations have limitations in adapting to the needs of these patients, advancements in adaptive and personalized technologies, including deep learning and generative agents, are overcoming these barriers.</p></section><section id="sec17"><h4 class="pmc_sec_title">Psychology</h4>
<p>In the field of education, the integration of AIGC with VR technologies has created unprecedented learning experiences. By constructing highly immersive virtual learning environments, this technology not only overcomes the physical limitations of traditional classrooms [<a href="#B159" class="usa-link" aria-describedby="B159">159</a>] but also drives 3 major innovations in educational models: First, the multisensory embodied learning experiences facilitated by VR/AR technologies markedly enhance knowledge acquisition efficiency in complex disciplines such as STEM. Second, AIGC-generated virtual classrooms effectively eliminate geographical barriers, providing equitable learning opportunities for underserved regions. Third, students and educators worldwide can engage in real-time collaboration within shared virtual spaces, realizing a truly borderless educational community. This innovative approach further extends to professional training, enabling high-risk industries like healthcare and aviation to conduct high-fidelity simulations in safe virtual environments.</p>
<p>From a mental health perspective, AIGC’s performance in the Metaverse is equally noteworthy. Research indicates that emotionally intelligent virtual spaces created by this technology can precisely address users’ psychological needs, demonstrating significant clinical value in alleviating conditions such as anxiety and depression. Through carefully designed therapeutic environments, guided meditation experiences, and supportive social interactions, AIGC provides an effective psychological regulatory space for modern individuals. However, this technological empowerment also brings notable social risks. Recent studies suggest that excessive reliance on virtual experiences may lead to the deterioration of real-world social skills [<a href="#B160" class="usa-link" aria-describedby="B160">160</a>], while blurred boundaries between virtual and physical realities could trigger identity confusion. Moreover, the anonymity of the Metaverse may exacerbate behavioral misconduct [<a href="#B161" class="usa-link" aria-describedby="B161">161</a>].</p>
<p>Addressing these challenges requires a comprehensive strategy. At the technical level, intelligent usage monitoring systems—including adaptive time reminders and content filtering mechanisms—should be developed. Ethically, design principles centered on promoting holistic human development must be established. Socially, digital literacy education should be strengthened to cultivate healthy balance between virtual and physical engagement. Only through such multifaceted approaches can the healthy development of AIGC technology in the Metaverse be ensured, ultimately realizing its social value in advancing educational equity and psychological well-being.</p></section><section id="sec18"><h4 class="pmc_sec_title">Sociology</h4>
<p>From a sociological point of view, AIGC has a profound impact on the social structure in the Metaverse. The wide application of AIGC enables users to ship and own unique virtual assets, such as digital artifacts, virtual real estate, and so on. The difference in the value of these assets may lead to a new social stratification in the meta-universe, forming a class structure similar to that of the real society. Meanwhile, with the development of AIGC technology, many new professions have emerged in the Metaverse, such as virtual architects and digital fashion designers. The rise of these occupations has changed the traditional classification of occupations and enriched the diversity of social roles [<a href="#B162" class="usa-link" aria-describedby="B162">162</a>]. Secondly, AIGC has also had a certain impact on social interaction modes: the deepening of human–computer interaction, the virtualization of social networks, and the diversity of social interactions [<a href="#B163" class="usa-link" aria-describedby="B163">163</a>,<a href="#B164" class="usa-link" aria-describedby="B164">164</a>]. AIGC technology has enabled virtual characters to have higher intelligence and emotional expression capabilities, and users’ interactions with these virtual characters have become more natural and in-depth, and may even develop an emotional connection similar to interpersonal relationships. Similarly, users can interact and participate in a variety of virtual activities through the created virtual identities, generating personalized social content according to user preferences and behaviors, changing the traditional mode of social interaction, and enriching the social experience.</p>
<p>In addition to its impact on social structure and interaction, AIGC in the Metaverse has the potential to promote environmental sustainability, particularly through the development and use of digital assets in lieu of physical ones. One notable example is the field of fashion, where “Metaverse fashion” has already emerged as a potential alternative to traditional clothing, at least to some degree—particularly for the generations that see clothing as a signaling opportunity via online social media.</p>
<p>To wit, digital garments, designed and traded entirely within virtual environments and worn by avatars in the Metaverse, can satisfy consumers’ desire for self-expression and novelty without the environmental costs associated with physical fashion production. Unlike traditional fashion, which often relies on resource-intensive manufacturing processes, generates significant textile waste, and contributes to water pollution and carbon emissions, digital fashion offers a waste-free, sustainable option. Users can experiment with endless styles and designs in the Metaverse, showcasing their creativity and identity through virtual avatars without generating tangible waste or requiring mass production. This shift not only reduces the ecological footprint of the fashion industry but also addresses the issue of fast fashion, which has long been criticized for its unsustainable practices. By providing an environmentally friendly alternative that still fulfills the cultural and social functions of fashion, the Metaverse has the potential to reshape consumer habits and promote a more sustainable approach to self-expression and creativity [<a href="#B165" class="usa-link" aria-describedby="B165">165</a>].</p>
<p>Ultimately, the application of AIGC algorithms in the Metaverse introduces several challenges and risks that must be addressed to ensure ethical and safe usage. One major concern is the potential for data bias and discrimination during the training phase. While AIGC enhances user immersion, it also increases vulnerability to adversarial attacks, where malicious training samples or gradient poisoning could perpetuate biases in models. This can worsen social inequalities and marginalize certain groups. Additionally, the extensive data requirements for training AIGC models—often including sensitive personal information—raise significant data security and privacy concerns. Without robust privacy protections, the risks of unauthorized access, data breaches, and misuse of user data are heightened.</p>
<p>To mitigate these risks, several strategies should be implemented. Responsible AI practices must be adopted, focusing on ethical AI development frameworks that prioritize bias mitigation and fairness in both training data and model outcomes [<a href="#B166" class="usa-link" aria-describedby="B166">166</a>]. Supervised learning techniques can be employed to identify and eliminate biases within training datasets, helping reduce the potential for social inequality. Privacy-enhancing technologies, such as federated learning, should be utilized to train AIGC models without centralizing sensitive user data, minimizing privacy risks. Additionally, strong data security measures, including encryption, secure data transmission, and access controls, are essential to safeguard user data. Comprehensive security protocols, such as data encryption, authentication, and network security, should be introduced to protect user privacy in the Metaverse. Regular updates to security protocols are also necessary to adapt to evolving threats and incorporate the latest technological advancements.</p></section><section id="sec19"><h4 class="pmc_sec_title">From technology to practice: AIGC’S industrial footprint</h4>
<p>Building upon these technical advancements, a growing number of enterprises and research institutions have begun to integrate AIGC into real-world applications. These implementations span multiple sectors—including healthcare, education, industry, and entertainment—demonstrating not only the practical viability of AIGC but also its transformative potential across diverse domains. The following representative case studies illustrate how the synergy between foundational models and domain-specific adaptations is reshaping content creation, simulation, and interaction in the Metaverse.</p>
<p>The integration of AIGC into the Metaverse shown in Fig. <a href="#F7" class="usa-link">7</a> has demonstrated transformative potential across diverse sectors, supported by empirical evidence of its efficacy. In healthcare, platforms like Surgical Theater [<a href="#B167" class="usa-link" aria-describedby="B167">167</a>] leverage diffusion models and NeRF to generate high-fidelity 3D patient organ models, markedly reducing preoperative planning time and lowering intraoperative complication rates in complex neurosurgical procedures, as evidenced by clinical trials at the Cleveland Clinic. Similarly, Mind Maze [<a href="#B168" class="usa-link" aria-describedby="B168">168</a>] employs GANs to create personalized neurorehabilitation environments, accelerating motor function recovery in stroke patients, validated through Fugl-Meyer assessments in Swiss clinical studies. NVIDIA CLARA’s GAN-based synthetic medical imaging [<a href="#B169" class="usa-link" aria-describedby="B169">169</a>] addresses rare disease data scarcity, markedly improving brain tumor segmentation accuracy, while Johns Hopkins University’s diffusion model-driven 3D anatomical simulations for surgical training have streamlined procedural efficiency and reduced error rates in resident assessments.</p>
<figure class="fig xbox font-sm" id="F7"><h5 class="obj_head">Fig. 7.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c7c8/12364526/94cf58210654/research.0804.fig.007.jpg" loading="lazy" height="375" width="660" alt="Fig. 7."></p>
<div class="p text-right font-secondary"><a href="figure/F7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The practical applications of the Metaverse. (A and B) SyncAR system by surgical theater: AR-enhanced multimodal navigation for tumor boundaries in intraoperative microscopic view. (C and D) VR-augmented multimodal neurorehabilitation: MindMaze’s digital therapy platform. (E to H) Virtual reality metaversity developed by Victory XR: a new paradigm for VR education. (I and J) Epic games virtual concert: showcasing cross-dimensional performance innovation. (K and L) NVIDIA omniverse × OpenUSD alliance: standardizing physics-accurate 3D ecosystems for the Metaverse era. (M and N) AI–fashion convergence: Alibaba’s GAN-powered “outfit anyone” virtual fitting revolution. (O and P) GUCCI’s GOOD GAME Metaverse: retro-gaming, NFT exclusivity, and the rebirth of luxury storytelling.</p></figcaption></figure><p>In education, platforms such as Victory XR [<a href="#B170" class="usa-link" aria-describedby="B170">170</a>] utilize stable diffusion to automate the generation of immersive historical simulations, enhancing student performance and course completion rates in U.S. university pilots. Meta’s collaboration with Stanford University demonstrated that diffusion-generated 3D historical environments (e.g., ancient Rome) markedly elevated student knowledge retention and engagement. Similarly, Peking University’s Transformer-based virtual language coaching system improved oral fluency over sustained training periods.</p>
<p>Industrial applications highlight NVIDIA Omniverse’s [<a href="#B171" class="usa-link" aria-describedby="B171">171</a>] use of GANs for synthetic factory fault datasets, enabling predictive maintenance systems that reduce equipment downtime by 18% and annual maintenance costs by $2.1 million in Siemens2 pilot facilities. The entertainment sector showcases Epic Games’ MetaHuman tool, powered by Transformers, which dramatically cut character design costs while attracting millions of users to virtual concerts. Retail innovations include Alibaba’s GAN-driven virtual fitting rooms Outfit Anyone [<a href="#B172" class="usa-link" aria-describedby="B172">172</a>], decreasing return rates by 25% and boosting conversion rates by 18% on e-commerce platforms.</p>
<p>Sociologically, Gucci’s AIGC-generated digital fashion items [<a href="#B173" class="usa-link" aria-describedby="B173">173</a>] achieved significant commercial success with a far lower carbon footprint compared to physical counterparts, underscoring sustainability benefits. These cases collectively illustrate AIGC’s role in enhancing efficiency, scalability, and user engagement within the Metaverse, while addressing critical challenges such as cost reduction, environmental impact, and personalized experiences.</p></section></section><section id="sec20"><h3 class="pmc_sec_title">Challenges in AIGC technology development</h3>
<p>The development and integration of AIGC technology within the Metaverse presents a range of complex challenges, both technical and ethical, that need to be carefully navigated. One of the most significant obstacles is the solution space complexity, which pertains to the difficulty of identifying and generating specific subspaces tailored for tasks like face or body generation. As the Metaverse demands diverse and highly detailed virtual content, ensuring macroscopic consistency is a major challenge, particularly in video generation. Limited fields of view in video rendering can disrupt underlying structures, resulting in a lack of continuity in long-term motion or structural consistency. On a microscopic level, maintaining clarity in generated content, especially in short video formats, is another issue, as averaging feasible solutions often leads to poor resolution and blurring. To tackle these hurdles, improvements in subspace optimization, such as the use of deep learning embeddings, meta-learning, and swarm intelligence, are essential. To address microscopic clarity issues, techniques like super-resolution, GANs, and blur removal algorithms can be employed to enhance the sharpness and detail of generated content. Additionally, the current entry into the Metaverse largely relies on highly immersive XR (extended reality) [<a href="#B174" class="usa-link" aria-describedby="B174">174</a>] (VR/AR/MR [mixed reality]) devices. However, the existing VR technologies struggle to miniaturize, make these devices portable, and reduce their costs, hindering users from accessing the Metaverse anytime and anywhere. At the same time, prolonged use of XR devices can cause discomfort.</p>
<p>Additionally, the integration of AIGC into the Metaverse introduces significant challenges related to energy consumption and sustainability, which merit closer attention. Training large-scale AIGC models, particularly for video generation and cross-modal integration, requires substantial computational resources, resulting in high energy costs and a considerable environmental footprint. The shift toward sustainability in AI development [<a href="#B175" class="usa-link" aria-describedby="B175">175</a>,<a href="#B176" class="usa-link" aria-describedby="B176">176</a>] is imperative, with potential solutions including the adoption of energy-efficient model architectures, such as sparsity-based neural networks, and the use of green cloud computing powered by renewable energy sources. Innovations in training optimization, such as model distillation and federated learning, can further reduce energy demands.</p>
<p>Another critical aspect is the balance between user agency and control in AIGC-driven systems. While AIGC empowers creators with automated tools [<a href="#B177" class="usa-link" aria-describedby="B177">177</a>–<a href="#B179" class="usa-link" aria-describedby="B179">179</a>], users may feel detached from the creative process if systems lack interactivity or customization options. User-in-the-loop frameworks, where individuals can iteratively refine generated outputs, are key to addressing this issue. For example, real-time tools that allow users to adjust parameters such as style, tone, or resolution would enhance personalization and creative autonomy. Additionally, intuitive interfaces for AIGC systems could democratize content creation, enabling non-experts to actively participate in the design of virtual environments, characters, and experiences, further enriching the Metaverse.</p>
<p>Finally, interoperability and real-time generation are technical challenges that must be overcome to realize the full potential of the Metaverse. As a decentralized and interconnected ecosystem, the Metaverse requires that AIGC-generated content be compatible across various platforms and devices. Developing universal standards for asset creation and exchange will be essential to ensure seamless integration. Real-time content generation, a cornerstone of interactive and immersive experiences such as live events or adaptive virtual assistants, demands low latency and high computational efficiency. Solutions such as edge computing [<a href="#B180" class="usa-link" aria-describedby="B180">180</a>–<a href="#B182" class="usa-link" aria-describedby="B182">182</a>], distributed architectures [<a href="#B183" class="usa-link" aria-describedby="B183">183</a>], and lightweight generative models [<a href="#B184" class="usa-link" aria-describedby="B184">184</a>] are critical to addressing these challenges. These issues still require further in-depth research.</p>
<p>Alongside these technical challenges, there are significant concerns around the ethical implications of AIGC’s rapid content generation. One of the key issues is related to copyright and intellectual property (IP) in the Metaverse [<a href="#B185" class="usa-link" aria-describedby="B185">185</a>], as AIGC can produce high volumes of content quickly [<a href="#B186" class="usa-link" aria-describedby="B186">186</a>], raising questions about ownership and originality. The realism of AIGC-generated content also increases the risk of misinformation, as it becomes easier to create and disseminate fake news, deepfakes, and other manipulated media. Protecting IP and preventing the spread of false information require robust solutions, such as embedding digital watermarks in AIGC-generated content for copyright verification and authenticity. Moreover, developing advanced AIGC detection systems to identify and flag fabricated content is crucial to maintaining trust in digital ecosystems. Legal frameworks are necessary to ensure fair distribution of benefits and safeguard creators’ rights [<a href="#B187" class="usa-link" aria-describedby="B187">187</a>–<a href="#B189" class="usa-link" aria-describedby="B189">189</a>], while regulatory transparency will foster a more secure digital environment by requiring AIGC systems to disclose the methods and data sources used in content generation.</p>
<p>Some organizational platforms are also actively promoting AIGC copyright protection norms, such as the World Intellectual Property Organization [<a href="#B190" class="usa-link" aria-describedby="B190">190</a>], which is researching the copyright legal framework for AI-generated content to provide transnational guidance; YouTube [<a href="#B191" class="usa-link" aria-describedby="B191">191</a>] and Getty Images, which use embedded watermarking and content identification technologies to protect the rights and interests of the originators; and OpenSea and Rarible, which protect the rights and interests of the original creators by casting AIGC generated digital artworks cast as NFTs, binding ownership information, and ensuring their traceability, and have realized digital transactions and management of art copyrights [<a href="#B192" class="usa-link" aria-describedby="B192">192</a>]. However, the originality and authorship of AI-generated content is recognized, and the copyright laws of many countries require works to have the participation of human creators, while the identity of the creators of AI-generated content has not been clarified, and this aspect of the law is relatively ambiguous [<a href="#B178" class="usa-link" aria-describedby="B178">178</a>]. Moreover, AIGC-generated content may use other people’s copyrighted material without authorization and the number of contents is large [<a href="#B193" class="usa-link" aria-describedby="B193">193</a>], which leads to the risk of copyright infringement and the protection of copyright registration also becomes complicated. Fortunately, a variety of mitigation strategies have been proposed. Technologically, numerous deepfake detection methods have been developed, ranging from CNNs and attention-based models to frequency-domain analysis and Transformer-based frameworks. These methods aim to identify subtle artifacts or inconsistencies in generated content. In parallel, legislative actions are being implemented in several countries, such as the DEEPFAKES Accountability Act in the United States, which mandates clear labeling and imposes penalties for malicious use. Additionally, initiatives like the Content Authenticity Initiative (CAI) seek to establish content provenance by embedding metadata and leveraging blockchain for verification. Public education campaigns are also crucial in enhancing media literacy, helping individuals critically assess the authenticity of digital media. Lastly, social media platforms have introduced stricter moderation policies to detect and remove deepfake content, thereby reducing its spread and impact.</p>
<p>In the face of the above challenges, the AIGC copyright management mechanism in the meta-universe can generate a unique digital fingerprint, i.e., assign a unique hash value to each AIGC content through smart contract and blockchain technology, and record the time of its creation, creator, and right of use information on the blockchain to ensure traceability, so as to guarantee the copyright ownership of user-generated content in the meta-universe and adapt to the complex ecology of multi-party participation [<a href="#B194" class="usa-link" aria-describedby="B194">194</a>]. It realizes the automation and efficiency of copyright management and avoids the tedious process of traditional copyright transactions. It can also compare content features based on deep learning models and match them with the existing content in the database, through which potential copyright conflicts or infringements can be detected. These issues still require further in-depth research.</p>
<p>Looking to the future, AIGC technology has the potential to markedly shape the development of the Metaverse, particularly in content creation, scalability, and interactivity. As AIGC systems evolve, they will foster new levels of creativity and collaboration between humans and AI. AIGC will empower creators, developers, and users to produce multi-modal content, including text, images, video, and even interactive experiences, allowing for highly personalized and engaging content. This collaboration will lead to an explosion of innovation and diverse content that can be seamlessly integrated into the Metaverse. The scalability of AIGC models will allow for rapid, cost-effective content production, enabling the Metaverse to expand rapidly and offer increasingly dynamic virtual environments. With the integration of cloud computing, the computational resources required for AIGC models will be markedly optimized, reducing training and inference times while also lowering costs. This cloud-based approach will support the large-scale deployment of AIGC technologies, ensuring that immersive experiences can be delivered to users with minimal delay.</p>
<p>Additionally, cross-modal integration will also drive innovation within the Metaverse, particularly in areas like virtual education, healthcare, and entertainment. By combining multiple forms of data—text, images, audio, and video—AIGC models will be able to create more cohesive and interactive experiences, enhancing user engagement. Furthermore, future AIGC systems will be designed with enhanced intelligence and adaptability, capable of independent learning and decision-making. This autonomy will ensure that the content generated remains accurate, relevant, and aligned with user needs, providing a richer and more personalized experience. As these systems become more autonomous, they will also integrate privacy-preserving technologies like federated learning and encryption, ensuring that user data remains secure.</p>
<p>Ultimately, AIGC will play a central role in transforming industries by enabling cost-effective content creation, accelerating innovation, and driving economic growth. Fields such as cultural tourism, education, healthcare, and industrial manufacturing will all benefit from the ability to generate complex, immersive content quickly and efficiently. In the Metaverse, AIGC will serve as a powerful tool for cross-industry collaboration, offering new opportunities for businesses and users to interact, create, and innovate. By overcoming the challenges outlined above and leveraging the full potential of emerging technologies, AIGC will redefine how we experience and interact with digital worlds. As AIGC continues to evolve, its integration into the Metaverse will be essential in creating vibrant, dynamic, and innovative digital environments.</p></section></section><section id="sec21"><h2 class="pmc_sec_title">Discussion</h2>
<p>Building on a summary of the current strengths and limitations of AIGC applications in the Metaverse, we propose the development of a more comprehensive and detailed evaluation framework. This framework should not only address the complexities of virtual environments but also thoroughly assess the quality of AIGC-generated content, the level of immersive user experiences, and the innovative potential of future model-centric advancements. AIGC holds transformative potential for the Metaverse, enabling the synthesis of complex and realistic data through computational methods to address challenges in Metaverse content creation. This study begins by examining the core neural network architectures of GAI, analyzing the development of deep learning technologies behind AIGC, and exploring the relationship between the Metaverse and AIGC. Additionally, the study highlights the application prospects and challenges of AIGC in fields such as healthcare, education, and sociology. Although AIGC can markedly accelerate the development of the Metaverse, its technology must be better aligned with practical development needs to achieve more effective applications. This research aims to provide a foundational knowledge framework for academic researchers and industry practitioners, thereby advancing the evaluation methods and application technologies of AIGC in the Metaverse to new heights.</p>
<section id="sec22"><h3 class="pmc_sec_title">Where is the Metaverse today?</h3>
<p>Arguably, despite its immense promise, the Metaverse has yet to live up to the initial hype. One of the primary challenges lies in the current technological and infrastructural limitations that still hinder the seamless, immersive experiences promised by early proponents of the technology. One of the central technologies of an immersive Metaverse, VR headsets, face particular challenges with high costs and user comfort issues—many people report motion sickness, eye strain, and general discomfort during extended sessions, and while the Metaverse concept extends beyond VR, this remains a particular concern, despite potential accessibility through PCs, mobile devices, and AR; even these more accessible entry points have not gained significant traction [<a href="#B195" class="usa-link" aria-describedby="B195">195</a>].The Metaverse’s development has likely also been slowed down by insufficient content creation and consumer uptake, including in the areas described above, leaving virtual environments feeling empty, repetitive, and underwhelming compared to the dynamic worlds envisioned. The absence of interoperability between platforms has further fragmented the user experience, preventing the creation of a unified digital ecosystem. Additionally, the emergence of competing technologies has further complicated the Metaverse’s trajectory. The explosive rise of GAI has captured public imagination and diverted both attention and investment that might otherwise have gone to entirely to metaverse development. This shift in focus has been particularly impactful as it coincides with growing societal skepticism about the Metaverse, stemming from concerns about data privacy, cybersecurity, and ethical implications. However, while mass adoption remains uncertain, the Metaverse is still finding success in specific niches. Virtual training platforms are being adopted by industries ranging from healthcare to manufacturing, offering safe, cost-effective environments for skill development. Educational institutions are exploring virtual campuses and immersive learning experiences, while gaming platforms like Roblox and Fortnite continue to build engaged communities around shared virtual experiences. The concept of the Metaverse itself continues to evolve. The initial hype may have been premature, setting unrealistic expectations for immediate transformation across all sectors. As technology advances and expectations become more grounded, we believe that the Metaverse might gradually find its footing. This could perhaps involve a more distributed and interoperable network of virtual spaces, rather than the single, unified virtual world initially envisioned by early evangelists. In all likelihood, people simply prefer simpler, more accessible forms of digital interaction for daily use, indicating that the Metaverse’s eventual role might be complementary to, rather than replacing, existing digital experiences. As the Metaverse navigates these challenges and finds its footing in specific niches, it is crucial to address the ethical, legal, and social implications (ELSI) of its development. The obstacles to widespread adoption extend beyond technological and economic factors to include deeper societal concerns that influence how the Metaverse is perceived and utilized. Issues such as data privacy, cybersecurity, and equitable access must be carefully examined to ensure that this evolving digital ecosystem can be both trusted and inclusive. Moreover, questions of governance, identity, and the potential for social stratification within virtual spaces continue to highlight the need for a comprehensive framework to guide the responsible growth of the Metaverse. Exploring these ELSI aspects will help illuminate the broader challenges and opportunities inherent in shaping the future of this digital frontier.</p></section><section id="sec23"><h3 class="pmc_sec_title">ELSI of GAI in the Metaverse</h3>
<p>As the Metaverse, in whatever iteration it is currently or eventually will be in, integrates GAI (AIGC) to build immersive virtual ecosystems, its rapid development presents a myriad of ethical, legal, and social challenges. Addressing these implications is critical to ensure the Metaverse remains equitable, inclusive, and responsibly governed. This section of the discussion explores these considerations across various dimensions, with a focus on healthcare, education, and broader societal impacts.</p>
<section id="sec24"><h4 class="pmc_sec_title">Ethical considerations</h4>
<p>Generative AI systems in the Metaverse must navigate issues of bias and fairness. AIGC relies on vast datasets, which, if poorly curated, can reinforce existing societal biases perpetuating stereotypes and marginalizing underrepresented groups. Developing diverse and representative training datasets and incorporating fairness algorithms are crucial steps toward addressing these issues.</p>
<p>Another major ethical concern is privacy. The immersive nature of the Metaverse involves extensive collection of personal and behavioral data both prior to use and in real time during use. This raises significant risks of surveillance and misuse by governments, corporations, and perhaps even malicious entities. Privacy-preserving technologies, such as federated learning and encryption, can mitigate these risks by decentralizing data storage and limiting unnecessary data collection.</p>
<p>The Metaverse’s immersive experiences also raise concerns about dependency and addiction. Prolonged engagement with virtual environments can lead to social withdrawal, disrupted real-world relationships, and other mental health challenges. Developers should integrate features to promote healthy digital habits, such as usage limits, well-being reminders, and tools for maintaining a balanced engagement between virtual and real-world activities [<a href="#B196" class="usa-link" aria-describedby="B196">196</a>].</p></section><section id="sec25"><h4 class="pmc_sec_title">Legal challenges</h4>
<p>The legal landscape of the Metaverse is still evolving, with significant gaps in addressing IP and accountability issues. Ownership of AIGC-generated assets—such as virtual environments, avatars, or digital art—is often ambiguous, and challenging traditional IP frameworks as AI can neither invent nor author basic fundamental requirements for obtaining patents and copyrights, respectively [<a href="#B197" class="usa-link" aria-describedby="B197">197</a>,<a href="#B198" class="usa-link" aria-describedby="B198">198</a>]. Policymakers must establish clear guidelines to determine ownership of places and things generated via AIGC in the metaverse, ensuring creators’ rights are protected while fostering innovation. Accountability is another pressing issue. If harm arises from biased or harmful AI-generated content, defining responsibility becomes complex. Developers, platform operators, and AI designers must work together to create transparent systems for easily assigning liability. Additionally, the global nature of the Metaverse demands international cooperation to establish consistent regulations across jurisdictions for these and other legal concerns.</p></section><section id="sec26"><h4 class="pmc_sec_title">Health implications</h4>
<p>In healthcare in particular, the Metaverse, augmented by AIGC, offers transformative opportunities, but it also offers significant risks. Virtual healthcare environments powered by AIGC can enhance medical training through realistic simulations, improve telemedicine experiences, and generate synthetic datasets for research. However, overreliance on AI-generated content for diagnoses or treatment recommendations poses risks of inaccuracies and malpractice, particularly when algorithms lack sufficient training or context. Ethical safeguards, such as rigorous testing of AIGC systems and transparency in decision-making, are critical to ensure patient safety. Privacy concerns are also particularly acute in healthcare, as sensitive medical data could be exploited if security measures are insufficient. Blockchain-based systems and federated learning approaches can address these concerns by providing secure, decentralized solutions.</p></section><section id="sec27"><h4 class="pmc_sec_title">Education implications</h4>
<p>As described above, the Metaverse also holds vast potential in education, where AIGC can create immersive, interactive learning experiences. Virtual classrooms, customizable simulations, and experiential learning tools can help students engage with complex subjects more effectively. However, disparities in access to such technologies risk exacerbating the digital divide [<a href="#B199" class="usa-link" aria-describedby="B199">199</a>]. Policymakers and educators must prioritize affordable solutions and ensure that virtual learning environments are inclusive. Additionally, overreliance on AIGC-driven education tools may lead to reduced critical thinking skills if students depend solely on automated systems. To address this, educators should blend AIGC with traditional teaching methods to foster a balanced, well-rounded educational experience.</p></section></section><section id="sec28"><h3 class="pmc_sec_title">Proposed strategies for responsible development</h3>
<p>Addressing these challenges requires a multifaceted approach. Ethical AI design must prioritize fairness, explainability, and accountability, with regular audits to ensure AIGC systems operate responsibly. Privacy-preserving technologies, such as encryption and decentralized storage, should be integral to Metaverse platforms. International cooperation is essential to create legal frameworks that address cross-border complexities, particularly around IP and liability. Finally, public education campaigns can raise awareness about the ethical and social dimensions of the Metaverse, empowering users to engage responsibly with this evolving digital frontier.</p>
<p>Ultimately, by embedding ethical, legal, and social considerations into the fabric of Metaverse development, i.e., ethics by design, stakeholders can create a digital ecosystem that is not only innovative and immersive but also inclusive, equitable, and sustainable. These efforts will ensure the Metaverse evolves into a trusted and empowering space for all users.</p></section></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgments</h2>
<p><strong>Funding:</strong> This work was supported in part by the Beijing Natural Science Foundation-Non-Consensus Innovation Project (Grant No. F251046); in part by the Engineering Research Center of Digital Imaging and Display, Ministry of Education, Soochow University; in part by the National Natural Science Fund for Excellent Young Scientists Fund Program (Grant No. KZ37124001); and in part by the National Science Fund for Distinguished Young Scholars (62225102).</p>
<p><strong>Author contributions:</strong> H.Z.: Investigation and methodology. X.C.: Investigation and methodology. J.L. and Z.Z.: Conceptualization, supervision, and funding acquisition. Y.F.: Investigation, review, and reference management. M.P.L.: Investigation and review. D.G.: Suggestions, revision, and review. P.H.: Suggestions, revision, and review.</p>
<p><strong>Competing interests:</strong> The authors declare that they have no competing interests.</p></section><section id="sec29"><h2 class="pmc_sec_title">Data Availability</h2>
<p>Data will be made available on request.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="B1">
<span class="label">1.</span><cite>Duan H, Li J, Fan S, Lin Z, Wu X, Cai W. Metaverse for social good: A university campus prototype. In: <em>Proceedings of the 29th ACM International Conference on Multimedia</em>. New York (NY): Association for Computing Machinery; 2021.</cite>
</li>
<li id="B2">
<span class="label">2.</span><cite>Huynh-The T, Pham Q-V, Pham X-Q, Nguyen TT, Han Z, Kim D-S. 
Artificial intelligence for the metaverse: A survey. Eng Appl Artif Intell. 2023;117:
Article 105581.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Eng%20Appl%20Artif%20Intell&amp;title=Artificial%20intelligence%20for%20the%20metaverse:%20A%20survey&amp;volume=117&amp;publication_year=2023&amp;pages=Article%20105581&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B3">
<span class="label">3.</span><cite>Park SM, Kim YG. 
A metaverse: Taxonomy, components, applications, and open challenges. IEEE Access. 2022;10:4209–4251.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=A%20metaverse:%20Taxonomy,%20components,%20applications,%20and%20open%20challenges&amp;volume=10&amp;publication_year=2022&amp;pages=4209-4251&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B4">
<span class="label">4.</span><cite>Yang Q, Zhao Y, Huang H, Xiong Z, Kang J, Zheng Z. 
Fusing Blockchain and AI with Metaverse: A survey. IEEE Open J Comput Soc. 2022;3:122–136.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Open%20J%20Comput%20Soc&amp;title=Fusing%20Blockchain%20and%20AI%20with%20Metaverse:%20A%20survey&amp;volume=3&amp;publication_year=2022&amp;pages=122-136&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B5">
<span class="label">5.</span><cite>Smart J, Cascio J, Paffendorf J. Metaverse roadmap overview San Pedro (CA): Acceleration Studies Foundation; 2007. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=Metaverse%20roadmap%20overview&amp;publication_year=2007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B6">
<span class="label">6.</span><cite>Chen M, Liu M, Wang C, Song X, Zhang Z, Xie Y, Wang L. 
Cross-modal graph semantic communication assisted by generative AI in the Metaverse for 6G. Research. 2024;7:0342.
</cite> [<a href="https://doi.org/10.34133/research.0342" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11062502/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38694200/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Research&amp;title=Cross-modal%20graph%20semantic%20communication%20assisted%20by%20generative%20AI%20in%20the%20Metaverse%20for%206G&amp;volume=7&amp;publication_year=2024&amp;pages=0342&amp;pmid=38694200&amp;doi=10.34133/research.0342&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B7">
<span class="label">7.</span><cite>Tlili A, Huang R, Shehata B, Liu D, Zhao J, Metwally AHS, Wang H, Denden M, Bozkurt A, Lee L-H, et al. 
Is Metaverse in education a blessing or a curse: A combined content and bibliometric analysis. Smart Learn Environ. 2022;9(1):1–31.40477962
</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Smart%20Learn%20Environ&amp;title=Is%20Metaverse%20in%20education%20a%20blessing%20or%20a%20curse:%20A%20combined%20content%20and%20bibliometric%20analysis&amp;volume=9&amp;issue=1&amp;publication_year=2022&amp;pages=1-31&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B8">
<span class="label">8.</span><cite>Yu X, Owens D, Khazanchi D. Building socioemotional environments in metaverses for virtual teams in healthcare: A conceptual exploration. Berlin Heidelberg: Springer; 2012. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=Building%20socioemotional%20environments%20in%20metaverses%20for%20virtual%20teams%20in%20healthcare:%20A%20conceptual%20exploration&amp;publication_year=2012&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B9">
<span class="label">9.</span><cite>Jeong H, Youkyoung YI, Kim D. 
An innovative E-commerce platform incorporating Metaverse to live commerce. Int J Innovat Comput Inf Contr. 2022;1:18.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Innovat%20Comput%20Inf%20Contr&amp;title=An%20innovative%20E-commerce%20platform%20incorporating%20Metaverse%20to%20live%20commerce&amp;volume=1&amp;publication_year=2022&amp;pages=18&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B10">
<span class="label">10.</span><cite>Xu M, Ng WC, Lim WYB, Kang J, Xiong Z, Niyato D, Yang Q, Shen X, Miao C. 
A full dive into realizing the edge-enabled Metaverse: Visions, enabling technologies, and challenges. Commun Surv Tutor. 2023;25(1):656–700.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Commun%20Surv%20Tutor&amp;title=A%20full%20dive%20into%20realizing%20the%20edge-enabled%20Metaverse:%20Visions,%20enabling%20technologies,%20and%20challenges&amp;volume=25&amp;issue=1&amp;publication_year=2023&amp;pages=656-700&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B11">
<span class="label">11.</span><cite>Wang Y, Chardonnet J-R, Merienne F. Modeling online adaptive navigation in virtual environments based on PID control. Singapore: Springer Nature Singapore; 2024.</cite>
</li>
<li id="B12">
<span class="label">12.</span><cite>Li R, Wang Y, Yin H, Chardonnet J-R, Hui P. A deep cybersickness predictor through kinematic data with encoded physiological representation. In: <em>2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em>. IEEE; 2023.</cite>
</li>
<li id="B13">
<span class="label">13.</span><cite>Zhao Y, Li L, Jia H, Wu S. Opportunities and challenges of artificial intelligence generated content on the development of new digital economy in metaverse. Atlantis Press; 2023. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=Opportunities%20and%20challenges%20of%20artificial%20intelligence%20generated%20content%20on%20the%20development%20of%20new%20digital%20economy%20in%20metaverse&amp;publication_year=2023&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B14">
<span class="label">14.</span><cite>Wenzheng L. The characteristics, relationships and challenges of metaverse, Web 3.0 and AIGC. In: <em>2023 IEEE 13th International Conference on Electronics Information and Emergency Communication (ICEIEC)</em>. IEEE; 2023.</cite>
</li>
<li id="B15">
<span class="label">15.</span><cite>Garcia J, Miller A, Wilson N, Martinez C, Moore R. The Metaverse and AIGC: Navigating the Shifts in Tech Trends and Future Prospects. 2023. 10.13140/RG.2.2.21307.80168</cite> [<a href="https://doi.org/10.13140/RG.2.2.21307.80168" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B16">
<span class="label">16.</span><cite>Wilson J, Davis W. Reevaluating the Metaverse: Navigating the Shifts and Synergies with AIGC. 2023.</cite>
</li>
<li id="B17">
<span class="label">17.</span><cite>Cheng R, Wu N, Varvello M, Chen S, Han B. Are we ready for metaverse? A measurement study of social virtual reality platforms. In: <em>Proceedings of the 22nd ACM Internet Measurement Conference</em>. Nice (France): Association for Computing Machinery; 2022. p. 504–518.</cite>
</li>
<li id="B18">
<span class="label">18.</span><cite>Cao Y, Li S, Liu Y, Yan Z, Dai Y, Yu P, Sun L. 
A survey of AI-generated content (AIGC). ACM Comput Surv. 2024;57(5):1–38.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Comput%20Surv&amp;title=A%20survey%20of%20AI-generated%20content%20(AIGC)&amp;volume=57&amp;issue=5&amp;publication_year=2024&amp;pages=1-38&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B19">
<span class="label">19.</span><cite>Croitoru FA, Hondru V, Ionescu RT, Shah M. 
Diffusion models in vision: A survey. IEEE Trans Pattern Anal Mach Intell. 2023;45(9):10850–10869.
</cite> [<a href="https://doi.org/10.1109/TPAMI.2023.3261988" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37030794/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;title=Diffusion%20models%20in%20vision:%20A%20survey&amp;volume=45&amp;issue=9&amp;publication_year=2023&amp;pages=10850-10869&amp;pmid=37030794&amp;doi=10.1109/TPAMI.2023.3261988&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B20">
<span class="label">20.</span><cite>Ye T, Chen S, Chai W, Xing Z, Qin J, Lin G, Zhu L. Learning diffusion texture priors for image restoration. In: <em>2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2024.</cite>
</li>
<li id="B21">
<span class="label">21.</span><cite>Zhou H, Lian Y, Liu Z, Li J, Cao X, Ma C, Tian J. 
Dual-domain deep unfolding transformer for spectral compressive imaging reconstruction. Opt Lasers Eng. 2025;186:
Article 108754.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Opt%20Lasers%20Eng&amp;title=Dual-domain%20deep%20unfolding%20transformer%20for%20spectral%20compressive%20imaging%20reconstruction&amp;volume=186&amp;publication_year=2025&amp;pages=Article%20108754&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B22">
<span class="label">22.</span><cite>Zhou H, Lian Y, Li J, Liu Z, Cao X, Ma C. 
Supervised-unsupervised combined transformer for spectral compressive imaging reconstruction. Opt Lasers Eng. 2024;175:
Article 108030.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Opt%20Lasers%20Eng&amp;title=Supervised-unsupervised%20combined%20transformer%20for%20spectral%20compressive%20imaging%20reconstruction&amp;volume=175&amp;publication_year=2024&amp;pages=Article%20108030&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B23">
<span class="label">23.</span><cite>Chen L, Sun M, Liu J, Ding P, Ma Y, Li L, Qian L, Yang Y. A method for extracting information from long documents that combines large language models with natural language understanding techniques. In: <em>2023 4th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)</em>. USA: IEEE; 2023. p. 43–47.</cite>
</li>
<li id="B24">
<span class="label">24.</span><cite>Rao J, Xiong M. A new art design method based on AIGC: Analysis from the perspective of creation efficiency. In: <em>2023 4th International Conference on Intelligent Design (ICID)</em>. USA: IEEE; 2023. p. 129–134.</cite>
</li>
<li id="B25">
<span class="label">25.</span><cite>Ma P, Dou J, Jia J, Ma X. AIGC-assisted geospatial generative design for classical Chinese literary works. In <em>2024 International Conference on Culture-Oriented Science &amp; Technology (CoST)</em>. Beijing (China); 2024. p. 22–27.</cite>
</li>
<li id="B26">
<span class="label">26.</span><cite>Ford C, Noel-Hirst A, Cardinate S, Loth J, Sarmento P, Wilson E, Wolstanholme L, Worrall K, Bryan-Kinns N. Reflection Across AI-based music composition. In: <em>Proceedings of the 16th Conference on Creativity &amp; Cognition</em>. Chicago (IL): Association for Computing Machinery; 2024. p. 398–412.</cite>
</li>
<li id="B27">
<span class="label">27.</span><cite>Jadhav S, Patil JP, Rachappa JR, Kanase OS. Satish KM. The future of content creation: Leveraging AI for code, text, music, and video generation. In: <em>2024 8th International Conference on Computing, Communication, Control and Automation (ICCUBEA)</em>. USA: IEEE; 2024. p. 1–7.</cite>
</li>
<li id="B28">
<span class="label">28.</span><cite>Qureshi R, Irfan M, Gondal TM, Khan S, Wu J, Hadi MU, Heymach J, Le X, Yan H, Alam T. 
AI in drug discovery and its clinical relevance. Heliyon. 2023;9(7):
Article e17575.
</cite> [<a href="https://doi.org/10.1016/j.heliyon.2023.e17575" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10302550/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37396052/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Heliyon&amp;title=AI%20in%20drug%20discovery%20and%20its%20clinical%20relevance&amp;volume=9&amp;issue=7&amp;publication_year=2023&amp;pages=Article%20e17575&amp;pmid=37396052&amp;doi=10.1016/j.heliyon.2023.e17575&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B29">
<span class="label">29.</span><cite>An X, Chen X, Yi D, Li H, Guan Y. 
Representation of molecules for drug response prediction. Brief Bioinform. 2021;23(1):
Article bbab393.</cite> [<a href="https://doi.org/10.1093/bib/bbab393" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8769696/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34571534/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Brief%20Bioinform&amp;title=Representation%20of%20molecules%20for%20drug%20response%20prediction&amp;volume=23&amp;issue=1&amp;publication_year=2021&amp;pages=Article%20bbab393&amp;pmid=34571534&amp;doi=10.1093/bib/bbab393&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B30">
<span class="label">30.</span><cite>Guzman-Pando A, Ramirez-Alonso G, Arzate-Quintana C, Camarillo-Cisneros J. 
Deep learning algorithms applied to computational chemistry. Mol Divers. 2024;28(4):2375–2410.
</cite> [<a href="https://doi.org/10.1007/s11030-023-10771-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38151697/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Mol%20Divers&amp;title=Deep%20learning%20algorithms%20applied%20to%20computational%20chemistry&amp;volume=28&amp;issue=4&amp;publication_year=2024&amp;pages=2375-2410&amp;pmid=38151697&amp;doi=10.1007/s11030-023-10771-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B31">
<span class="label">31.</span><cite>Creswell A, White T, Dumoulin V, Arulkumaran K, Sengupta B, Bharath AA. 
Generative adversarial networks: An overview. IEEE Signal Process Mag. 2017;35(1):53–65.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Signal%20Process%20Mag&amp;title=Generative%20adversarial%20networks:%20An%20overview&amp;volume=35&amp;issue=1&amp;publication_year=2017&amp;pages=53-65&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B32">
<span class="label">32.</span><cite>Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y. 
Generative adversarial networks. Commun ACM. 2020;63(11):139–144.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Commun%20ACM&amp;title=Generative%20adversarial%20networks&amp;volume=63&amp;issue=11&amp;publication_year=2020&amp;pages=139-144&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B33">
<span class="label">33.</span><cite>Alshraideh M, Bottaci L. 
Search-based software test data generation for string data using program-specific search operators: Research articles. Softw Test Verif Reliab. 2006;16(3):175–203.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Softw%20Test%20Verif%20Reliab&amp;title=Search-based%20software%20test%20data%20generation%20for%20string%20data%20using%20program-specific%20search%20operators:%20Research%20articles&amp;volume=16&amp;issue=3&amp;publication_year=2006&amp;pages=175-203&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B34">
<span class="label">34.</span><cite>Mescheder LM, Geiger A, Nowozin S. Which training methods for GANs do actually converge? In: <em>International Conference on Machine Learning</em>. Stockholmsmässan (Sweden): Proceedings of Machine Learning Research (PMLR); 2018. p. 3481–3490.</cite>
</li>
<li id="B35">
<span class="label">35.</span><cite>Arjovsky M, Chintala S, Bottou L, Wasserstein generative adversarial networks. In: <em>Proceedings of the 34th International Conference on Machine Learning</em>, Sydney (Australia): Proceedings of Machine Learning Research (PMLR); 2017. Vol. 70, p. 214–223.</cite>
</li>
<li id="B36">
<span class="label">36.</span><cite>Zhang H, Goodfellow I, Metaxas D, Odena A. Self-attention generative adversarial networks. In: <em>Proceedings of the International Conference on Machine Learning</em>. California (USA): Proceedings of Machine Learning Research (PMLR); 2019. p. 7354–7363</cite>
</li>
<li id="B37">
<span class="label">37.</span><cite>Radford A, Metz L, Chintala S, Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv. 2015. <a href="https://arxiv.org/abs/1511.06434" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1511.06434</a></cite>
</li>
<li id="B38">
<span class="label">38.</span><cite>Mirza M, Osindero S, Conditional generative adversarial nets. arXiv. 2014. <a href="https://arxiv.org/abs/1411.1784" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1411.1784</a></cite>
</li>
<li id="B39">
<span class="label">39.</span><cite>Masci J, Meier U, Ciresan D, Schmidhuber J. Stacked convolutional auto-encoders for hierarchical feature extraction. Berlin, Heidelberg: Springer Berlin Heidelberg; 2011. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=Stacked%20convolutional%20auto-encoders%20for%20hierarchical%20feature%20extraction&amp;publication_year=2011&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B40">
<span class="label">40.</span><cite>Chen W, Yang Y, Tian Z, Chen Q, Liu J. 
A review of multimodal learning for text to images. Multimed Tools Appl. 2024;84:8205–8245.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Multimed%20Tools%20Appl&amp;title=A%20review%20of%20multimodal%20learning%20for%20text%20to%20images&amp;volume=84&amp;publication_year=2024&amp;pages=8205-8245&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B41">
<span class="label">41.</span><cite>Salimans T, Goodfellow I, Zaremba W, Cheuyng V, Radford A, Chen X. Improved Techniques for Training GANs. 2016.</cite>
</li>
<li id="B42">
<span class="label">42.</span><cite>Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models. In: <em>Proceedings of the 34th International Conference on Neural Information Processing Systems</em>. Vancouver (Canada): Curran Associates Inc.; 2020. p. 574.</cite>
</li>
<li id="B43">
<span class="label">43.</span><cite>Miao X, Yuan X, Pu Y, Athitsos V. lambda-Net: Reconstruct hyperspectral images from a snapshot measurement. In: <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>. IEEE; 2019.</cite>
</li>
<li id="B44">
<span class="label">44.</span><cite>Kingma DP, Welling M, Auto-encoding variational Bayes. arXiv. 2014. <a href="https://arxiv.org/abs/1312.6114" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1312.6114</a></cite>
</li>
<li id="B45">
<span class="label">45.</span><cite>Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol P-A. 
Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J Mach Learn Res. 2010;11:3371–3408.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Mach%20Learn%20Res&amp;title=Stacked%20denoising%20autoencoders:%20Learning%20useful%20representations%20in%20a%20deep%20network%20with%20a%20local%20denoising%20criterion&amp;volume=11&amp;publication_year=2010&amp;pages=3371-3408&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B46">
<span class="label">46.</span><cite>Esser P, Rombach R, Ommer B. Taming transformers for high-resolution image synthesis. In: <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2021.</cite>
</li>
<li id="B47">
<span class="label">47.</span><cite>van den Oord A, Vinyals O, Kavukcuoglu K, Neural discrete representation learning. In: <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>. Long Beach (CA): Curran Associates Inc.; 2017. p. 6309–6318.</cite>
</li>
<li id="B48">
<span class="label">48.</span><cite>Saxena D, Cao J. 
Generative adversarial networks (GANs): Challenges, solutions, and future directions. ACM Comput Surv. 2021;54(3):
Article 63.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Comput%20Surv&amp;title=Generative%20adversarial%20networks%20(GANs):%20Challenges,%20solutions,%20and%20future%20directions&amp;volume=54&amp;issue=3&amp;publication_year=2021&amp;pages=Article%2063&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B49">
<span class="label">49.</span><cite>Ma Z, Yu Z, Li J, Zhou B. 
LMD: Faster image reconstruction with latent masking diffusion. Proc AAAI Conf Artif Intell. 2024;38:4145–4153.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proc%20AAAI%20Conf%20Artif%20Intell&amp;title=LMD:%20Faster%20image%20reconstruction%20with%20latent%20masking%20diffusion&amp;volume=38&amp;publication_year=2024&amp;pages=4145-4153&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B50">
<span class="label">50.</span><cite>Song Y, Sohl-Dickstein J, Kingma DP, Kumar A, Ermon S, Poole B. Score-based generative modeling through stochastic differential equations. In: <em>International Conference on Learning Representations</em>. New Orleans (LA): Curran Associates, Inc.; 2021.</cite>
</li>
<li id="B51">
<span class="label">51.</span><cite>Xu X, Wang Z, Zhang E, Wang K, Shi H. Versatile diffusion: Text, images and variations all in one diffusion model. In: <em>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>. IEEE; 2022. p. 7720–7731.</cite>
</li>
<li id="B52">
<span class="label">52.</span><cite>Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B. High-resolution image synthesis with latent diffusion models. In: <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2022.</cite>
</li>
<li id="B53">
<span class="label">53.</span><cite>Xiao Z, Kreis K, Vahdat A. Tackling the generative learning trilemma with denoising diffusion GANs. 2021.</cite>
</li>
<li id="B54">
<span class="label">54.</span><cite>Song Y, Ermon S. Generative modeling by estimating gradients of the data distribution. In: <em>Proceedings of the 33rd International Conference on Neural Information Processing Systems</em>. Curran Associates Inc.; 2019. p. 1067.</cite>
</li>
<li id="B55">
<span class="label">55.</span><cite>Li XL, Thickstun J, Gulrajani I, Liang P, Hashimoto TB. Diffusion-LM improves controllable text generation. In: <em>Proceedings of the 36th International Conference on Neural Information Processing Systems</em>. New Orleans (LA): Curran Associates Inc.; 2024. Article 313.</cite>
</li>
<li id="B56">
<span class="label">56.</span><cite>Liu N, Li S, Du Y, Torralba A, Tenenbaum JB. Compositional visual generation with composable diffusion models. arXiv. 2022. <a href="https://arxiv.org/abs/2206.01714" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2206.01714</a></cite>
</li>
<li id="B57">
<span class="label">57.</span><cite>Zhang L, Rao A, Agrawala M. Adding conditional control to text-to-image diffusion models. In: <em>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>. IEEE; 2023.</cite>
</li>
<li id="B58">
<span class="label">58.</span><cite>Singer U, Polyak A, Hayes T, Yin X, An J, Zhang S, Hu Q, Yang H, Ashual O, Gafni O, et al. Make-A-Video: Text-to-Video Generation without Text-Video Data. 2022.</cite>
</li>
<li id="B59">
<span class="label">59.</span><cite>Wu JZ, Ge Y, Wang X, Lei W, Gu Y, Shi Y, Hsu W, Shan Y, Qie X, Shou MZ. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In: <em>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>. IEEE; 2023.</cite>
</li>
<li id="B60">
<span class="label">60.</span><cite>Mildenhall B, Srinivasan PP, Tancik M, Barron JT, Ramamoorthi R, Ng R. 
NeRF: Representing scenes as neural radiance fields for view synthesis. Commun ACM. 2021;65(1):99–106.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Commun%20ACM&amp;title=NeRF:%20Representing%20scenes%20as%20neural%20radiance%20fields%20for%20view%20synthesis&amp;volume=65&amp;issue=1&amp;publication_year=2021&amp;pages=99-106&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B61">
<span class="label">61.</span><cite>Müller N, Siddiqui Y, Porzi L, Bulò SR, Kontschieder P, Nießner M. DiffRF: Rendering-guided 3D radiance field diffusion. In: <em>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2022. p. 4328–4338.</cite>
</li>
<li id="B62">
<span class="label">62.</span><cite>Cui J, Gao D, Zhao Y, Wang L, Peng X. Multi-perspectives 2D spine CT images segmentation of 3D fuse algorithm. In: <em>2022 5th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)</em>. USA: IEEE; 2022. p. 696–703.</cite>
</li>
<li id="B63">
<span class="label">63.</span><cite>Pandey K, Mukherjee A, Rai P, Kumar A. DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents. 2022.</cite>
</li>
<li id="B64">
<span class="label">64.</span><cite>Vahdat A, Kreis K, Kautz J. Score-based generative modeling in latent space. In: <em>Proceedings of the 35th International Conference on Neural Information Processing Systems</em>. Curran Associates Inc.; 2024. p. 863.</cite>
</li>
<li id="B65">
<span class="label">65.</span><cite>Zhang J, Shi H, Yu J, Xie E, Li Z. DiffFlow: A unified SDE framework for score-based diffusion models and generative adversarial networks. arXiv. 2023. <a href="https://arxiv.org/abs/2307.02159" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2307.02159</a></cite>
</li>
<li id="B66">
<span class="label">66.</span><cite>Wang Z, Lu C, Wang Y, Bao F, Li C, Su H, Zhu J. ProlificDreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. In: <em>Proceedings of the 37th International Conference on Neural Information Processing Systems</em>. New Orleans (LA): Curran Associates Inc.; 2024. p. 368.</cite>
</li>
<li id="B67">
<span class="label">67.</span><cite>Cao T, Kreis K, Fidler S, Sharp N, Yin K. TexFusion: Synthesizing 3D textures with text-guided image diffusion models. In: <em>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>. IEEE; 2023. p. 4146–4158.</cite>
</li>
<li id="B68">
<span class="label">68.</span><cite>Yuan Y, Song J, Iqbal U, Vahdat A, Kautz J. PhysDiff: Physics-guided human motion diffusion model. In: <em>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>. IEEE; 2022. p. 15964–15975.</cite>
</li>
<li id="B69">
<span class="label">69.</span><cite>Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I. Attention is all you need. In: <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>. Long Beach (CA): Curran Associates Inc.; 2017. p. 6000–6010.</cite>
</li>
<li id="B70">
<span class="label">70.</span><cite>Devlin J, Chang M-W, Lee K, Toutanove K. BERT: Pre-training of deep bidirectional transformers for language understanding. In: <em>North American Chapter of the Association for Computational Linguistics</em>. Minneapolis (MN): Association for Computational Linguistics; 2019. p. 4171–4186.</cite>
</li>
<li id="B71">
<span class="label">71.</span><cite>Bao F, Nie S, Xue K, Cao Y, Li C, Su H, Zhu J. All are worth words: A ViT backbone for diffusion models. In: <em>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2023.</cite>
</li>
<li id="B72">
<span class="label">72.</span><cite>Peebles WS, Xie S. 
Scalable diffusion models with transformers. IEEE/CVF Intl Conf Comput Vis. 2023;2022:4172–4182.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE/CVF%20Intl%20Conf%20Comput%20Vis&amp;title=Scalable%20diffusion%20models%20with%20transformers&amp;volume=2022&amp;publication_year=2023&amp;pages=4172-4182&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B73">
<span class="label">73.</span><cite>Chen J, Yu J, Ge C, Yao L, Xie E, Wu Y, Wang Z, Kwok J, Luo P, Lu H, et al. PixArt-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv. 2023. 10.48550/arXiv.2310.00426</cite> [<a href="https://doi.org/10.48550/arXiv.2310.00426" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B74">
<span class="label">74.</span><cite>Ma X, Wang Y, Chen X, Jia G, Liu Z, Li Y-F, Chen C, Qiao Y. Latte: Latent diffusion transformer for video generation. arXiv. 2024. 10.48550/arXiv.2401.03048</cite> [<a href="https://doi.org/10.48550/arXiv.2401.03048" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B75">
<span class="label">75.</span><cite>Teubner T, Flath CM, Weinhardt C, Van der Aaist W, Hinz O. 
Welcome to the Era of ChatGPT et al. Bus Inf Syst Eng. 2023;65(2):95–101.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Bus%20Inf%20Syst%20Eng&amp;title=Welcome%20to%20the%20Era%20of%20ChatGPT%20et%C2%A0al&amp;volume=65&amp;issue=2&amp;publication_year=2023&amp;pages=95-101&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B76">
<span class="label">76.</span><cite>Gu A, Goel K, Ré C. Efficiently modeling long sequences with structured state spaces. arXiv. 2021. 10.48550/arXiv.2111.00396</cite> [<a href="https://doi.org/10.48550/arXiv.2111.00396" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B77">
<span class="label">77.</span><cite>Qi B, Luo Y, Gao J, Li P, Tian K, Ma Z, Zhou B. Exploring adversarial robustness of deep state space models. arXiv. 2024. 10.48550/arXiv.2406.05532</cite> [<a href="https://doi.org/10.48550/arXiv.2406.05532" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B78">
<span class="label">78.</span><cite>Gu A, Dao T, Ermon S, Rudra A, Ré C. HiPPO: Recurrent memory with optimal polynomial projections. In: <em>Proceedings of the 34th International Conference on Neural Information Processing Systems</em>. Vancouver (Canada): Curran Associates Inc; 2020. p. 125.</cite>
</li>
<li id="B79">
<span class="label">79.</span><cite>Gupta A, Gu A, Berant J. Diagonal state spaces are as effective as structured state spaces. In: <em>Proceedings of the 36th International Conference on Neural Information Processing Systems</em>. New Orleans (LA): Curran Associates Inc.; 2024. p. 1670.</cite>
</li>
<li id="B80">
<span class="label">80.</span><cite>Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv. 2023. 10.48550/arXiv.2312.00752</cite> [<a href="https://doi.org/10.48550/arXiv.2312.00752" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B81">
<span class="label">81.</span><cite>Fei Z, Fan M, Yu C, Li D, Huang J. Diffusion-RWKV: Scaling RWKV-like architectures for diffusion models. arXiv. 2024. 10.48550/arXiv.2404.04478</cite> [<a href="https://doi.org/10.48550/arXiv.2404.04478" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B82">
<span class="label">82.</span><cite>Zhu L, Huang Z, Liao B, Liew JH, Yan H, Feng J, Wang X. DiG: Scalable and efficient diffusion models with gated linear attention. arXiv. 2024. 10.48550/arXiv.2405.18428</cite> [<a href="https://doi.org/10.48550/arXiv.2405.18428" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B83">
<span class="label">83.</span><cite>Teng Y, Wu Y, Shi H, Ning X, Dai G, Wang Y, Li Z, Liu X. DiM: Diffusion mamba for efficient high-resolution image synthesis. arXiv. 2024. 10.48550/arXiv.2405.14224</cite> [<a href="https://doi.org/10.48550/arXiv.2405.14224" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B84">
<span class="label">84.</span><cite>Hu VT, Baumann SA, Gui M, Grebenkova O, Ma P, Fischer J, Ommer B. ZigMa: A DiT-style zigzag mamba diffusion model. In: <em>Computer Vision – ECCV 2024: 18th European Conference, Milan, Italy, September 29–October 4, 2024, Proceedings, Part XXI</em>. Milan (Italy): Springer-Verlag; 2024. p. 148–166.</cite>
</li>
<li id="B85">
<span class="label">85.</span><cite>Peng B, Elcaide E, Anthony Q, Albalak A, Arcadinho S, Biderman S, Cao H, Cheng X, Chung M, Grella M, et al. RWKV: Reinventing RNNs for the transformer era. arXiv. 2023. 10.48550/arXiv.2305.13048</cite> [<a href="https://doi.org/10.48550/arXiv.2305.13048" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B86">
<span class="label">86.</span><cite>Yang S, Wang B, Shen Y, Panda R, Kim Y. Gated Linear Attention Transformers with Hardware-Efficient Training. In: S. Ruslan, et al., editors. <em>Proceedings of the 41st International Conference on Machine Learning</em>. PMLR; 2024. p. 56501–56523.</cite>
</li>
<li id="B87">
<span class="label">87.</span><cite>Ye B, Chen H, Ma B, Shan S, Chen X. <em>Joint feature learning and relation modeling for tracking: A one-stream framework</em>. Cham: Springer Nature Switzerland; 2022.</cite>
</li>
<li id="B88">
<span class="label">88.</span><cite>Wang X, Wang S, Tang C, Zhu L, Jiang B, Tian Y, Tang J. Event stream-based visual object tracking: A high-resolution benchmark dataset and a novel baseline. In: <em>2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2024.</cite>
</li>
<li id="B89">
<span class="label">89.</span><cite>Wang N, Zhou W, Wang J, Li H. Transformer meets tracker: Exploiting temporal context for robust visual tracking. In: <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2021.</cite>
</li>
<li id="B90">
<span class="label">90.</span><cite>Mayer C, Bhat G, Paul M, Paudel DP, Yu F, Van Gool L. 
Transforming model prediction for tracking. IEEE/CVF Conf Comput Vis Pattern Recognit. 2022;2022:8721–8730.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE/CVF%20Conf%20Comput%20Vis%20Pattern%20Recognit&amp;title=Transforming%20model%20prediction%20for%20tracking&amp;volume=2022&amp;publication_year=2022&amp;pages=8721-8730&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B91">
<span class="label">91.</span><cite>Bhat G, Danelljan M, Van Gool L, Timofte R. Learning discriminative model prediction for tracking. In: <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>. IEEE; 2019.</cite>
</li>
<li id="B92">
<span class="label">92.</span><cite>Danelljan M, Gool LV, Timofte R. Probabilistic regression for visual tracking. In: <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2020.</cite>
</li>
<li id="B93">
<span class="label">93.</span><cite>Danelljan M, Bhat G, Khan FS, Felsberg M. ATOM: Accurate tracking by overlap maximization. In: <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2019.</cite>
</li>
<li id="B94">
<span class="label">94.</span><cite>Gao S, Zhou C, Ma C, Wang X, Yuan J. AiATrack: Attention in attention for transformer visual tracking. In: <em>Computer Vision – ECCV 2022</em>. Cham: Springer Nature Switzerland.</cite>
</li>
<li id="B95">
<span class="label">95.</span><cite>Chen X, Yan B, Zhu J, Wang D, Yang X, Lu H. Transformer tracking. In: <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2021.</cite>
</li>
<li id="B96">
<span class="label">96.</span><cite>Cui Y, Jiang C, Wu G, Wang L. MixFormer: End-to-end tracking with iterative mixed attention. In: <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2022.</cite> [<a href="https://doi.org/10.1109/TPAMI.2024.3349519" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38713562/" class="usa-link">PubMed</a>]</li>
<li id="B97">
<span class="label">97.</span><cite>Chen B, Li P, Bai L, Qiao L, Shen Q, Li B, Gan W, Wu W, Ouyang W. Backbone is all your need: A simplified architecture for visual object tracking. In: <em>Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXII</em>. Tel Aviv (Israel): Springer-Verlag; 2022. p. 375–392.</cite>
</li>
<li id="B98">
<span class="label">98.</span><cite>He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE; 2016.</cite>
</li>
<li id="B99">
<span class="label">99.</span><cite>Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Deghani M, Minderer M, Heigold G, Gelly S, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv. 2021. 10.48550/arXiv.2010.11929</cite> [<a href="https://doi.org/10.48550/arXiv.2010.11929" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B100">
<span class="label">100.</span><cite>Karras T, Aittala M, Laine S, Härkönen E, Hellsten J, Lehtinen J, Aila T. Alias-free generative adversarial networks. In: <em>Proceedings of the 35th International Conference on Neural Information Processing Systems</em>. Curran Associates Inc.; 2021. p. 66.</cite>
</li>
<li id="B101">
<span class="label">101.</span><cite>Razavi A, van den Oord, Vinyals O. Generating diverse high-fidelity images with VQ-VAE-2. In: <em>Proceedings of the 33rd International Conference on Neural Information Processing Systems</em>. Curran Associates Inc.; 2019. Article 1331.</cite>
</li>
<li id="B102">
<span class="label">102.</span><cite>Esser P, Kulal S, Blattman A, Entezari R, Müller J, Saini H, Levi Y, Lorenz D, Sauer A, Boesel F, et al., Scaling rectified flow transformers for high-resolution image synthesis. In: <em>Proceedings of the 41st International Conference on Machine Learning</em>. Vienna (Austria). Association for Computing Machinery; 2024. Article 503.</cite>
</li>
<li id="B103">
<span class="label">103.</span><cite>Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In: <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>. Long Beach (CA): Curran Associates Inc.; 2017. p. 6629–6640.</cite>
</li>
<li id="B104">
<span class="label">104.</span><cite>Hessel J, Holtzman A, Forbes M, Le Bras R, Choi Y. CLIPScore: A reference-free evaluation metric for image captioning. arXiv. 2021. 10.48550/arXiv.2104.08718</cite> [<a href="https://doi.org/10.48550/arXiv.2104.08718" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B105">
<span class="label">105.</span><cite>Ramesh A, Pavlov M, Goh G, Gray S, Voss C, Radford A, Chen M, Sutskever I. Zero-shot text-to-image generation. In: Marina M and Tong Z, editors. <em>Proceedings of the 38th International Conference on Machine Learning</em>. PMLR; 2021. p. 8821–8831.</cite>
</li>
<li id="B106">
<span class="label">106.</span><cite>Nichol AQ, Dhariwal P, Ramesh A, Shyam P, Mishkin P, McGrew B, Sutskever I, Chen M. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In: Kamalika C, et al., editors. <em>Proceedings of the 39th International Conference on Machine Learning</em>. PMLR; 2022. p. 16784–16804.</cite>
</li>
<li id="B107">
<span class="label">107.</span><cite>Ramesh A, Dhariwal P, Nichol A, Chu C, Chen M. Hierarchical text-conditional image generation with CLIP latents. arXiv. 2022. 10.48550/arXiv.2204.06125</cite> [<a href="https://doi.org/10.48550/arXiv.2204.06125" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B108">
<span class="label">108.</span><cite>Betker J, Goh G, Jing L, Brooks T, Wang J, Li L, Ouyang L, Zhuang J, Lee J, Guo Y, et al. Improving Image Generation with Better Captions. 2023. <a href="https://cdn.openai.com/papers/dall-e-3.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://cdn.openai.com/papers/dall-e-3.pdf</a></cite>
</li>
<li id="B109">
<span class="label">109.</span><cite>Saharia C, Chan W, Saxena S, Li L, Whang J, Denton E, Ghasemipour SKS, Ayan BK, Mahdavi SS, Gontijo-Lopes R, et al. Photorealistic text-to-image diffusion models with deep language understanding. In: <em>Proceedings of the 36th International Conference on Neural Information Processing Systems</em>. New Orleans (LA): Curran Associates Inc.; 2022. Article 2643.</cite>
</li>
<li id="B110">
<span class="label">110.</span><cite>Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu PJ. 
Exploring the limits of transfer learning with a unified text-to-text transformer. J Mach Learn Res. 2020;21(1):
Article 140.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Mach%20Learn%20Res&amp;title=Exploring%20the%20limits%20of%20transfer%20learning%20with%20a%20unified%20text-to-text%20transformer&amp;volume=21&amp;issue=1&amp;publication_year=2020&amp;pages=Article%20140&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B111">
<span class="label">111.</span><cite>Yu J, Xu Y, Koh JY, Luong T, Baid G, Wang Z, Vasudevan V, Ku A, Yang Y, Ayan BK, et al. 
Scaling autoregressive models for content-rich text-to-image generation. Trans Mach Learn Res. 2022;2(3):
Article 5.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Trans%20Mach%20Learn%20Res&amp;title=Scaling%20autoregressive%20models%20for%20content-rich%20text-to-image%20generation&amp;volume=2&amp;issue=3&amp;publication_year=2022&amp;pages=Article%205&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B112">
<span class="label">112.</span><cite>Sohn K, Ruiz N, Lee K, Chin DC, Blok I, Chang H, Barber J, Jiang L, Entis G, Li Y, et al. StyleDrop: Text-to-image generation in any style. In: <em>Proceedings of the 37th International Conference on Neural Information Processing Systems</em>. New Orleans (LA): Curran Associates Inc.; 2023. p. 2920.</cite>
</li>
<li id="B113">
<span class="label">113.</span><cite>Tang R, Liu Pandey A, Jiang Z, Yang G, Kumar K, Stenetorp P, Lin J, Ture F. What the DAAM: Interpreting stable diffusion using cross attention. In: <em>Annual Meeting of the Association for Computational Linguistics</em>. Toronto (Canada): Association for Computational Linguistics; 2022. p. 5644–5659.</cite>
</li>
<li id="B114">
<span class="label">114.</span><cite>Feng W, He X, Fu TJ, Jampani V, Akula A, Narayana P, Basu S, Wang XE, Wang WY. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv. 2022. 10.48550/arXiv.2212.05032</cite> [<a href="https://doi.org/10.48550/arXiv.2212.05032" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B115">
<span class="label">115.</span><cite>Vondrick C, Pirsiavash H, Torralba A. 
Generating videos with scene dynamics. Adv Neural Inf Process Syst. 2016;29.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inf%20Process%20Syst&amp;title=Generating%20videos%20with%20scene%20dynamics&amp;volume=29&amp;publication_year=2016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B116">
<span class="label">116.</span><cite>Tulyakov S, Liu M-Y, Yang X, Kautz. Mocogan: Decomposing motion and content for video generation. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. IEEE; 2018.</cite>
</li>
<li id="B117">
<span class="label">117.</span><cite>Aigner S, Körner M. FutureGAN: Anticipating the future frames of video sequences using spatio-temporal 3d convolutions in progressively growing gans. arXiv. 2018. 10.48550/arXiv.1810.01325</cite> [<a href="https://doi.org/10.48550/arXiv.1810.01325" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B118">
<span class="label">118.</span><cite>Bhattacharjee P, Das S. 
Temporal coherency based criteria for predicting video frames using deep multi-stage generative adversarial networks. Adv Neural Inf Process Syst. 2017;30.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inf%20Process%20Syst&amp;title=Temporal%20coherency%20based%20criteria%20for%20predicting%20video%20frames%20using%20deep%20multi-stage%20generative%20adversarial%20networks&amp;volume=30&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B119">
<span class="label">119.</span><cite>Karras T, Aila T, Laine S, Lehtinen J. Progressive growing of gans for improved quality, stability, and variation. arXiv. 2017. 10.48550/arXiv.1710.10196</cite> [<a href="https://doi.org/10.48550/arXiv.1710.10196" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B120">
<span class="label">120.</span><cite>Wu C, Huang L, Zhang Q, Li B, Ji L, Yang F, Sapiro G, Duan N. Godiva: Generating open-domain videos from natural descriptions. arXiv. 2021. 10.48550/arXiv.2104.14806</cite> [<a href="https://doi.org/10.48550/arXiv.2104.14806" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B121">
<span class="label">121.</span><cite>Van Den Oord A, Vinyals O. 
Neural discrete representation learning. Adv Neural Inf Process Syst. 2017;30.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inf%20Process%20Syst&amp;title=Neural%20discrete%20representation%20learning&amp;volume=30&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B122">
<span class="label">122.</span><cite>Hong W, Ding M, Zheng W, Liu X, Tang J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv. 2022. 10.48550/arXiv.2205.15868</cite> [<a href="https://doi.org/10.48550/arXiv.2205.15868" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B123">
<span class="label">123.</span><cite>Kondratyuk D, Yu L, Gu X, Lezama J, Huang J, Schindler G, Hornung R, Birodkar V, Yan J, Chiu MC, et al. Videopoet: A large language model for zero-shot video generation. arXiv. 2023. 10.48550/arXiv.2312.14125</cite> [<a href="https://doi.org/10.48550/arXiv.2312.14125" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B124">
<span class="label">124.</span><cite>Ho J, Salimans T, Gritsenko A, Chan W, Norouzi M, Fleet DJ. 
Video diffusion models. Adv Neural Inf Process Syst. 2022;35:8633–8646.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inf%20Process%20Syst&amp;title=Video%20diffusion%20models&amp;volume=35&amp;publication_year=2022&amp;pages=8633-8646&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B125">
<span class="label">125.</span><cite>Singer U, Polyak A, Hayes T, Yin X, An J, Zhang S, Hu Q, Yang H, Ashual O, Gafni O, et al. Make-a-video: Text-to-video generation without text-video data. arXiv. 2022. 10.48550/arXiv.2209.14792</cite> [<a href="https://doi.org/10.48550/arXiv.2209.14792" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B126">
<span class="label">126.</span><cite>Zhang DJ, Wu JZ, Liu JW, Zhao R, Ran L, Gu Y, Gao D, Shou MZ. 
Show-1: Marrying pixel and latent diffusion models for text-to-video generation. Intl J Comput Vis. 2024;1–15.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Intl%20J%20Comput%20Vis&amp;title=Show-1:%20Marrying%20pixel%20and%20latent%20diffusion%20models%20for%20text-to-video%20generation&amp;publication_year=2024&amp;pages=1-15&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B127">
<span class="label">127.</span><cite>Blattmann A, Dockhorn T, Kulal S, Mendelevitch D, Kilian M, Lorenz D, Levi Y, English Z, Voleti V, Letts A, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv. 2023. 10.48550/arXiv.2311.15127</cite> [<a href="https://doi.org/10.48550/arXiv.2311.15127" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B128">
<span class="label">128.</span><cite>Lin H, Cho J, Zala A, Bansal M. Ctrl-adapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. arXiv. 2024. 10.48550/arXiv.2404.09967</cite> [<a href="https://doi.org/10.48550/arXiv.2404.09967" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B129">
<span class="label">129.</span><cite>Ho J, Chan W, Saharia C, Whang J, Gao R, Gritsenko A, Kingma DP, Poole B, Norouzi M, Fleet DJ, et al. Imagen video: High definition video generation with diffusion models. arXiv. 2022. 10.48550/arXiv.2210.02303</cite> [<a href="https://doi.org/10.48550/arXiv.2210.02303" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B130">
<span class="label">130.</span><cite>Blattmann A, Rombach R, Ling H, Dockhorn T, Kim SW, Fidler S, Kreis K. Align your latents: High-resolution video synthesis with latent diffusion models. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE; 2023.</cite>
</li>
<li id="B131">
<span class="label">131.</span><cite>Zhang R, Isola P, Efros AA, Shechtman E, Wang O. The unreasonable effectiveness of deep features as a perceptual metric. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. IEEE; 2018.</cite>
</li>
<li id="B132">
<span class="label">132.</span><cite>Sun P, Jiang Y, Chen S, Zhang S, Peng B, Luo P, Yuan Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv. 2024. 10.48550/arXiv.2406.06525</cite> [<a href="https://doi.org/10.48550/arXiv.2406.06525" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B133">
<span class="label">133.</span><cite>Villegas R, Babaeizadeh M, Kindermans PJ, Moraldo H, Zhang H, Saffar MT, Castro S, Kunze J, Erhan D. Phenaki: Variable length video generation from open domain textual description. arXiv. 2022. 10.48550/arXiv.2210.02399</cite> [<a href="https://doi.org/10.48550/arXiv.2210.02399" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B134">
<span class="label">134.</span><cite>Höllein L, Cao A, Owens A, Johnson J, Nießner M. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. IEEE; 2023.</cite>
</li>
<li id="B135">
<span class="label">135.</span><cite>Zhang J, Li X, Wan Z, Wang C, Liao J. 
Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Trans Vis Comput Graph. 2024;30(12):7749–7762.
</cite> [<a href="https://doi.org/10.1109/TVCG.2024.3361502" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38315587/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;title=Text2nerf:%20Text-driven%203d%20scene%20generation%20with%20neural%20radiance%20fields&amp;volume=30&amp;issue=12&amp;publication_year=2024&amp;pages=7749-7762&amp;pmid=38315587&amp;doi=10.1109/TVCG.2024.3361502&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B136">
<span class="label">136.</span><cite>Song L, Cao L, Xu H, Kang K, Tang F, Yuan J, Zhao Y. Roomdreamer: Text-driven 3D indoor scene synthesis with coherent geometry and texture. arXiv. 2023. 10.48550/arXiv.2305.11337</cite> [<a href="https://doi.org/10.48550/arXiv.2305.11337" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B137">
<span class="label">137.</span><cite>Li C, Zhang C, Cho J, Waghwase A, Lee LH, Rameau F, Yang Y, Bae SH, Hong CS. Generative AI meets 3D: A survey on text-to-3d in aigc era. arXiv. 2023. 10.48550/arXiv.2305.06131</cite> [<a href="https://doi.org/10.48550/arXiv.2305.06131" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B138">
<span class="label">138.</span><cite>Singer U, Sheynin S, Polyak A, Ashual O, Makarov I, Kokkinos F, Goyal N, Vedaldi A, Parikh D, Johnson J, et al. Text-to-4D dynamic scene generation. arXiv. 2023. 10.48550/arXiv.2301.11280</cite> [<a href="https://doi.org/10.48550/arXiv.2301.11280" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B139">
<span class="label">139.</span><cite>Bahmani S, Skorokhodov I, Rong V, Wetzstein G, Guibas L, Wonka P, Tulyakov S, Park JJ, Tagliasacchi A, Lindell DB. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE; 2024.</cite>
</li>
<li id="B140">
<span class="label">140.</span><cite>Fridman R, Abecasis A, Kasten Y, Dekel T. 
Scenescape: Text-driven consistent scene generation. Adv Neural Inf Proc Syst. 2023;36:39897–39914.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inf%20Proc%20Syst&amp;title=Scenescape:%20Text-driven%20consistent%20scene%20generation&amp;volume=36&amp;publication_year=2023&amp;pages=39897-39914&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B141">
<span class="label">141.</span><cite>Lee YC, Chen YT, Wang A, Liao TH, Feng BY, Huang JB. Vividdream: Generating 3D scene with ambient dynamics. arXiv. 2024. 10.48550/arXiv.2405.20334</cite> [<a href="https://doi.org/10.48550/arXiv.2405.20334" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B142">
<span class="label">142.</span><cite>Bai H, Lyu Y, Jiang L, Li S, Lu H, Lin X, Wang L. Componerf: Text-guided multi-object compositional nerf with editable 3D scene layout. arXiv. 2023. 10.48550/arXiv.2303.13843</cite> [<a href="https://doi.org/10.48550/arXiv.2303.13843" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B143">
<span class="label">143.</span><cite>Cohen-Bar D, Richardson E, Metzer G, Giryes R, Cohen-Or D. Set-the-scene: Global-local training for generating controllable nerf scenes. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. IEEE; 2023.</cite>
</li>
<li id="B144">
<span class="label">144.</span><cite>Brohan A, Brown N, Carbajal J, Chebotar Y, Chen X, Choromanski K, Ding T, Driess D, Dubey A, Finn C, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv. 2023. 10.48550/arXiv.2307.15818</cite> [<a href="https://doi.org/10.48550/arXiv.2307.15818" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B145">
<span class="label">145.</span><cite>Zhang D, Fan W, Lin J, Li H, Cong Q, Liu W, Lepora NF, Luo S. 
Design and benchmarking of a multimodality sensor for robotic manipulation with GAN-based cross-modality interpretation. IEEE Trans Robot. 2025;41:1278–1295.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Robot&amp;title=Design%20and%20benchmarking%20of%20a%20multimodality%20sensor%20for%20robotic%20manipulation%20with%20GAN-based%20cross-modality%20interpretation&amp;volume=41&amp;publication_year=2025&amp;pages=1278-1295&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B146">
<span class="label">146.</span><cite>Shen S, Zhu Z, Fan L, Zhang H, Wu X. Diffclip: Leveraging stable diffusion for language grounded 3D classification. In: <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>. IEEE; 2024.</cite>
</li>
<li id="B147">
<span class="label">147.</span><cite>Jun H, Nichol A. Shap-e: Generating conditional 3D implicit functions. arXiv. 2023. 10.48550/arXiv.2305.02463</cite> [<a href="https://doi.org/10.48550/arXiv.2305.02463" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B148">
<span class="label">148.</span><cite>Yu J, Xu Y, Koh JY, Luong T, Baid G, Wang Z, Vasudevan V, Ku A, Yang Y, Ayan BK, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv. 2022. 10.48550/arXiv.2206.10789</cite> [<a href="https://doi.org/10.48550/arXiv.2206.10789" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B149">
<span class="label">149.</span><cite>Linden Rvd, Lopes R, Bidarra R, 
Procedural generation of dungeons. IEEE Trans Comput Intell AI Games. 2014;6(1):78–89.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Comput%20Intell%20AI%20Games&amp;title=Procedural%20generation%20of%20dungeons&amp;volume=6&amp;issue=1&amp;publication_year=2014&amp;pages=78-89&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B150">
<span class="label">150.</span><cite>Geiger C, Iaia V. The forgotten creator: Towards a statutory remuneration right for machine learning of generative AI. <em>Comput Law Security Rev</em>. 2024;52:105925.</cite>
</li>
<li id="B151">
<span class="label">151.</span><cite>Mishra S. Exploring the Transformative Potential and Generative AI’s Multifaceted Impact on Diverse Sectors. 2024. p. 88–103.</cite>
</li>
<li id="B152">
<span class="label">152.</span><cite>Liu, Y, Zhang K, Li Y, Yan Z, Gao C, Chen R, Yuan Z, Huang Y, Sun H, Gao J, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv. 2024. 10.48550/arXiv.2402.17177</cite> [<a href="https://doi.org/10.48550/arXiv.2402.17177" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B153">
<span class="label">153.</span><cite>Chen J, Yi C, Du H, Niyato D, Kang J, Cai J, Shen X. A revolution of personalized healthcare: Enabling human digital twin with mobile AIGC. <em>IEEE Network</em>. 2024.</cite>
</li>
<li id="B154">
<span class="label">154.</span><cite>Rahmani R, Westin T, Nevelsteen K. 
Future healthcare in generative AI with real metaverse. Proc Comput Sci. 2024;251:487–493.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proc%20Comput%20Sci&amp;title=Future%20healthcare%20in%20generative%20AI%20with%20real%20metaverse&amp;volume=251&amp;publication_year=2024&amp;pages=487-493&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B155">
<span class="label">155.</span><cite>Grieco P. <em>Virtual and augmented reality applications in medicinal chemistry</em>. Taylor &amp; Francis; 2022. p. 1417–1419.</cite> [<a href="https://doi.org/10.4155/fmc-2022-0213" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36196876/" class="usa-link">PubMed</a>]</li>
<li id="B156">
<span class="label">156.</span><cite>Ghaempanah F, Moasses Ghafari B, Hesami D, Hossein Zadeh R, Noroozpoor R, Moodi Ghalibaf A, Hasanabadi P. 
Metaverse and its impact on medical education and health care system: A narrative review. Health Sci Rep. 2024;7(9):
Article e70100.
</cite> [<a href="https://doi.org/10.1002/hsr2.70100" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11422618/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39323461/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Health%20Sci%20Rep&amp;title=Metaverse%20and%20its%20impact%20on%20medical%20education%20and%20health%20care%20system:%20A%20narrative%20review&amp;volume=7&amp;issue=9&amp;publication_year=2024&amp;pages=Article%20e70100&amp;pmid=39323461&amp;doi=10.1002/hsr2.70100&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B157">
<span class="label">157.</span><cite>Lord C, Elsabbagh M, Baird G, Veenstra-Vanderweele J. 
Autism spectrum disorder. Lancet. 2018;392(10146):508–520.
</cite> [<a href="https://doi.org/10.1016/S0140-6736(18)31129-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7398158/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30078460/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Lancet&amp;title=Autism%20spectrum%20disorder&amp;volume=392&amp;issue=10146&amp;publication_year=2018&amp;pages=508-520&amp;pmid=30078460&amp;doi=10.1016/S0140-6736(18)31129-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B158">
<span class="label">158.</span><cite>Yehuda R, Hoge CW, McFarlane AC, Vermetten E, Lanius RA, Nievergelt CM, Hobfoll SE, Koenen KC, Neylan TC, Hyman SE. 
Post-traumatic stress disorder. Nat Rev Dis Primers. 2015;1(1):
Article 15057.
</cite> [<a href="https://doi.org/10.1038/nrdp.2015.57" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27189040/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat%20Rev%20Dis%20Primers&amp;title=Post-traumatic%20stress%20disorder&amp;volume=1&amp;issue=1&amp;publication_year=2015&amp;pages=Article%2015057&amp;pmid=27189040&amp;doi=10.1038/nrdp.2015.57&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B159">
<span class="label">159.</span><cite>Al-Kfairy M, Alzaabi M, Snoh B, Almarzooqi H, Alnaqbi W. Metaverse-based classroom: The good and the bad. In: <em>2024 IEEE Global Engineering Education Conference (EDUCON)</em>. IEEE; 2024.</cite>
</li>
<li id="B160">
<span class="label">160.</span><cite>Luxton DD. 
Artificial intelligence in psychological practice: Current and future applications and implications. Prof Psychol Res Pract. 2014;45(5):
Article 332.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Prof%20Psychol%20Res%20Pract&amp;title=Artificial%20intelligence%20in%20psychological%20practice:%20Current%20and%20future%20applications%20and%20implications&amp;volume=45&amp;issue=5&amp;publication_year=2014&amp;pages=Article%20332&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B161">
<span class="label">161.</span><cite>Gómez-Quintero J, Johnson SD, Borrion H, Lundrigan S. 
A scoping study of crime facilitated by the metaverse. Futures. 2024;157:
Article 103338.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Futures&amp;title=A%20scoping%20study%20of%20crime%20facilitated%20by%20the%20metaverse&amp;volume=157&amp;publication_year=2024&amp;pages=Article%20103338&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B162">
<span class="label">162.</span><cite>Khalili BG, Quraishi T, Fazil S. 
The influence of social media on human and social communications: A sociological study. J Soc Hum. 2024;2(1):40–48.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Soc%20Hum&amp;title=The%20influence%20of%20social%20media%20on%20human%20and%20social%20communications:%20A%20sociological%20study&amp;volume=2&amp;issue=1&amp;publication_year=2024&amp;pages=40-48&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B163">
<span class="label">163.</span><cite>Gómez-González E, Gomez E, Márquez-Rivas J, Guerrero-Claro M, Fernández-Lizaranzu I, Relimpio-López MI, Dorado ME, Mayorga-Buiza MJ, Izquierdo-Ayuso G, Capitán-Morales L. Artificial intelligence in medicine and healthcare: A review and classification of current and near-future applications and their ethical and social Impact. arXiv. 2020. 10.48550/arXiv.2001.09778</cite> [<a href="https://doi.org/10.48550/arXiv.2001.09778" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B164">
<span class="label">164.</span><cite>Tække J. Sociological perspectives on AI, intelligence and communication. <em>Systems Res Behav Sci</em>. 2024.</cite>
</li>
<li id="B165">
<span class="label">165.</span><cite>Blazquez M. 
The Metaverse and its potential for digital sustainability in fashion. J Glob Fashion Mark. 2024;15(3):303–319.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Glob%20Fashion%20Mark&amp;title=The%20Metaverse%20and%20its%20potential%20for%20digital%20sustainability%20in%20fashion&amp;volume=15&amp;issue=3&amp;publication_year=2024&amp;pages=303-319&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B166">
<span class="label">166.</span><cite>Prem E. 
From ethical AI frameworks to tools: A review of approaches. AI Ethics. 2023;3(3):699–716.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=AI%20Ethics&amp;title=From%20ethical%20AI%20frameworks%20to%20tools:%20A%20review%20of%20approaches&amp;volume=3&amp;issue=3&amp;publication_year=2023&amp;pages=699-716&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B167">
<span class="label">167.</span><cite>Theater S. Surgical Theater: Pioneering XR in Healthcare for Enhanced Surgical Precision. 2025. <a href="https://surgicaltheater.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://surgicaltheater.com/</a></cite>
</li>
<li id="B168">
<span class="label">168.</span><cite>MindMaze. Pioneering Digital NeurotherapeuticsBuilding the universal platform for brain health and recovery. 2025. <a href="https://mindmaze.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://mindmaze.com/</a></cite>
</li>
<li id="B169">
<span class="label">169.</span><cite>@NVIDIA, NVIDIA Clara. 2025. https://www.nvidia.cn/clara/</cite>
</li>
<li id="B170">
<span class="label">170.</span><cite>VictoryXR. Home Virtual Reality VR Education Software &amp; Augmented Reality Learning. 2025. <a href="https://www.victoryxr.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.victoryxr.com/</a></cite>
</li>
<li id="B171">
<span class="label">171.</span><cite>Delgado G. NVIDIA Researchers Harness Real-Time Generative AI. 2025. <a href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?utm_source=chatgpt.com" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?utm_source=chatgpt.com</a></cite>
</li>
<li id="B172">
<span class="label">172.</span><cite>Sun K, Cao J, Wang Q, Tian L, Zhang X, Zhuo L, Zhang B, Bo L, Zhou W, Zhang W, et al. OutfitAnyone: Ultra-high Quality Virtual Try-On for Any Clothing and Any Person. 2024.</cite>
</li>
<li id="B173">
<span class="label">173.</span><cite>GUCCI. New Gucci NFTs combine fashion and art using generative AI. 2025. <a href="https://www.voguebusiness.com/technology/new-gucci-nfts-combine-fashion-and-art-using-generative-ai" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.voguebusiness.com/technology/new-gucci-nfts-combine-fashion-and-art-using-generative-ai</a></cite>
</li>
<li id="B174">
<span class="label">174.</span><cite>Xi N, Chen J, Gama F, Riar M, Hamari J. 
The challenges of entering the metaverse: An experiment on the effect of extended reality on workload. Inf Syst Front. 2023;25(2):659–680.
</cite> [<a href="https://doi.org/10.1007/s10796-022-10244-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8852991/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35194390/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Inf%20Syst%20Front&amp;title=The%20challenges%20of%20entering%20the%20metaverse:%20An%20experiment%20on%20the%20effect%20of%20extended%20reality%20on%20workload&amp;volume=25&amp;issue=2&amp;publication_year=2023&amp;pages=659-680&amp;pmid=35194390&amp;doi=10.1007/s10796-022-10244-x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B175">
<span class="label">175.</span><cite>Van Wynsberghe A. 
Sustainable AI: AI for sustainability and the sustainability of AI. AI Ethics. 2021;1(3):213–218.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=AI%20Ethics&amp;title=Sustainable%20AI:%20AI%20for%20sustainability%20and%20the%20sustainability%20of%20AI&amp;volume=1&amp;issue=3&amp;publication_year=2021&amp;pages=213-218&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B176">
<span class="label">176.</span><cite>Chen Z, Wu M, Chan A, Li X, Ong YS. 
Survey on AI sustainability: Emerging trends on learning algorithms and research challenges. IEEE Comput Intell Mag. 2023;18(2):60–77.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Comput%20Intell%20Mag&amp;title=Survey%20on%20AI%20sustainability:%20Emerging%20trends%20on%20learning%20algorithms%20and%20research%20challenges&amp;volume=18&amp;issue=2&amp;publication_year=2023&amp;pages=60-77&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B177">
<span class="label">177.</span><cite>Jin J, Yang M, Hu H, Guo X, Luo J, Liu Y. 
Empowering design innovation using AI-generated content. J Eng Des. 2025;36(1):1–18.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Eng%20Des&amp;title=Empowering%20design%20innovation%20using%20AI-generated%20content&amp;volume=36&amp;issue=1&amp;publication_year=2025&amp;pages=1-18&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B178">
<span class="label">178.</span><cite>Yin R, Liu X. Enabling media production with AIGC and its ethical considerations. In: <em>2024 IEEE 10th International Conference on Edge Computing and Scalable Cloud (EdgeCom)</em>. IEEE; 2024.</cite>
</li>
<li id="B179">
<span class="label">179.</span><cite>Zi-yang H. 
AIGC related context: A new communication culture for human. J Lit Art Stud. 2024;14(10):921–931.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Lit%20Art%20Stud&amp;title=AIGC%20related%20context:%20A%20new%20communication%20culture%20for%20human&amp;volume=14&amp;issue=10&amp;publication_year=2024&amp;pages=921-931&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B180">
<span class="label">180.</span><cite>Wang YC, Xue J, Wei C, Kuo CC. 
An overview on generative AI at scale with edge–cloud computing. IEEE Open J Commun Soc. 2023;4:2952–2971.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Open%20J%20Commun%20Soc&amp;title=An%20overview%20on%20generative%20AI%20at%20scale%20with%20edge%E2%80%93cloud%20computing&amp;volume=4&amp;publication_year=2023&amp;pages=2952-2971&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B181">
<span class="label">181.</span><cite>Ale L, Zhang N, King SA, Chen D. 
Empowering generative AI through mobile edge computing. Nat Rev Electr Eng. 2024;1(7):478–486.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Nat%20Rev%20Electr%20Eng&amp;title=Empowering%20generative%20AI%20through%20mobile%20edge%20computing&amp;volume=1&amp;issue=7&amp;publication_year=2024&amp;pages=478-486&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B182">
<span class="label">182.</span><cite>Xu M, Du H, Niyato D, Kang J, Xiong Z, Mao S, Han Z, Jamalipour A, Kim DI, Shen X, et al. 
Unleashing the power of edge-cloud generative AI in mobile networks: A survey of AIGC services. IEEE Commun Surv Tutor. 2024;26(2):1127–1170.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Commun%20Surv%20Tutor&amp;title=Unleashing%20the%20power%20of%20edge-cloud%20generative%20AI%20in%20mobile%20networks:%20A%20survey%20of%20AIGC%20services&amp;volume=26&amp;issue=2&amp;publication_year=2024&amp;pages=1127-1170&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B183">
<span class="label">183.</span><cite>Debauche O, Mahmoudi S, Manneback P, Lebeau F. 
Cloud and distributed architectures for data management in agriculture 4.0: Review and future trends. J King Saud Univ - Comput Inf Sci. 2022;34(9):7494–7514.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20King%20Saud%20Univ%20-%20Comput%20Inf%20Sci&amp;title=Cloud%20and%20distributed%20architectures%20for%20data%20management%20in%20agriculture%204.0:%20Review%20and%20future%20trends&amp;volume=34&amp;issue=9&amp;publication_year=2022&amp;pages=7494-7514&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B184">
<span class="label">184.</span><cite>Alexiou MS, Mertoguno JS. A survey on recent advancements in lightweight generative adversarial networks, their applications and datasets. In: <em>2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)</em>. IEEE; 2023.</cite>
</li>
<li id="B185">
<span class="label">185.</span><cite>Gupta BB, Gaurav A, Arya V, Alhalabi W. 
The evolution of intellectual property rights in metaverse based Industry 4.0 paradigms. Int Entrep Manag J. 2024;20(2):1111–1126.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20Entrep%20Manag%20J&amp;title=The%20evolution%20of%20intellectual%20property%20rights%20in%20metaverse%20based%20Industry%204.0%20paradigms&amp;volume=20&amp;issue=2&amp;publication_year=2024&amp;pages=1111-1126&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B186">
<span class="label">186.</span><cite>Kalyvaki M. 
Navigating the metaverse business and legal challenges: Intellectual property, privacy, and jurisdiction. J Metaverse. 2023;3(1):87–92.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Metaverse&amp;title=Navigating%20the%20metaverse%20business%20and%20legal%20challenges:%20Intellectual%20property,%20privacy,%20and%20jurisdiction&amp;volume=3&amp;issue=1&amp;publication_year=2023&amp;pages=87-92&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B187">
<span class="label">187.</span><cite>Ponkin I. 
Metaverse: Legal perspective. Int J Open Information Technol. 2023;11(1):118–127.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Open%20Information%20Technol&amp;title=Metaverse:%20Legal%20perspective&amp;volume=11&amp;issue=1&amp;publication_year=2023&amp;pages=118-127&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B188">
<span class="label">188.</span><cite>Mezei P, Arora GC. Copyright and metaverse. In: <em>Research handbook on the metaverse and law</em>. Edward Elgar Publishing; 2024. p. 190–206.</cite>
</li>
<li id="B189">
<span class="label">189.</span><cite>Dharma DS. 
Regulating the metaverse: Ensuring legal protection and intellectual property rights in the digital landscape. Indonesian Law Journal. 2023;16(2):161–184.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Indonesian%20Law%20Journal&amp;title=Regulating%20the%20metaverse:%20Ensuring%20legal%20protection%20and%20intellectual%20property%20rights%20in%20the%20digital%20landscape&amp;volume=16&amp;issue=2&amp;publication_year=2023&amp;pages=161-184&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B190">
<span class="label">190.</span><cite>@WIPO. IP and Frontier Technologies. 2025. <a href="http://www.wipo.int/about-ip/en/frontier_technologies/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">www.wipo.int/about-ip/en/frontier_technologies/</a></cite>
</li>
<li id="B191">
<span class="label">191.</span><cite>YouTube. How Content ID works - YouTube Help. 2023. <a href="http://support.google.com/youtube/answer/2797370" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">support.google.com/youtube/answer/2797370</a></cite>
</li>
<li id="B192">
<span class="label">192.</span><cite>Wang T, Zhang Y, Qi S, Zhao R, Xia Z, Weng J. 
Security and privacy on generative data in aigc: A survey. ACM Comput Surv. 2024;57(4):1–34.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Comput%20Surv&amp;title=Security%20and%20privacy%20on%20generative%20data%20in%20aigc:%20A%20survey&amp;volume=57&amp;issue=4&amp;publication_year=2024&amp;pages=1-34&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B193">
<span class="label">193.</span><cite>Li R, Zhang X, Xu Z, Zhang Y, Zhang J. Protect-your-ip: Scalable source-tracing and attribution against personalized generation. arXiv. 2024. 10.48550/arXiv.2405.16596</cite> [<a href="https://doi.org/10.48550/arXiv.2405.16596" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B194">
<span class="label">194.</span><cite>Jiang J, Su M, Xiao X, Zhang Y, Fang Y. AIGC-Chain: A Blockchain-enabled full lifecycle recording system for AIGC product copyright management. arXiv. 2024. 10.48550/arXiv.2406.14966</cite> [<a href="https://doi.org/10.48550/arXiv.2406.14966" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B195">
<span class="label">195.</span><cite>Talapuru S, Dantu R, Upadhyay K, Badruddoja S, Zaman S. The dark side of the metaverse: Why is it falling short of expectations? In: <em>2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA)</em>. IEEE; 2023.</cite>
</li>
<li id="B196">
<span class="label">196.</span><cite>Barreda-Ángeles M, Hartmann T. 
Hooked on the metaverse? Exploring the prevalence of addiction to virtual reality applications. Front Virtual Real. 2022;3:
Article 1031697.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Virtual%20Real&amp;title=Hooked%20on%20the%20metaverse?%20Exploring%20the%20prevalence%20of%20addiction%20to%20virtual%20reality%20applications&amp;volume=3&amp;publication_year=2022&amp;pages=Article%201031697&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B197">
<span class="label">197.</span><cite>Greenbaum D. 
Who owns the brains behind the machine? Will the hot debate on AI’s inventorship and authorship rights force a premature determination of machine consciousness?
AJOB Neuroscience. 2023;14(2):215–217.
</cite> [<a href="https://doi.org/10.1080/21507740.2023.2188300" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37097864/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=AJOB%20Neuroscience&amp;title=Who%20owns%20the%20brains%20behind%20the%20machine?%20Will%20the%20hot%20debate%20on%20AI%E2%80%99s%20inventorship%20and%20authorship%20rights%20force%20a%20premature%20determination%20of%20machine%20consciousness?&amp;volume=14&amp;issue=2&amp;publication_year=2023&amp;pages=215-217&amp;pmid=37097864&amp;doi=10.1080/21507740.2023.2188300&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B198">
<span class="label">198.</span><cite>Dadia T, Lee C, Xin TH, Kaur H, Greenbaum D. 
Can AI find its place within the broad ambit of copyright law?
Berkeley J Ent Sports L. 2021;10:
Article 37.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Berkeley%20J%20Ent%20Sports%20L&amp;title=Can%20AI%20find%20its%20place%20within%20the%20broad%20ambit%20of%20copyright%20law?&amp;volume=10&amp;publication_year=2021&amp;pages=Article%2037&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B199">
<span class="label">199.</span><cite>Al-kfairy M, Alfandi O. 
The ethical dilemma of educational metaverse. Recent Adv Evol Educ Outreach. 2024;1(1):006–016.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Recent%20Adv%20Evol%20Educ%20Outreach&amp;title=The%20ethical%20dilemma%20of%20educational%20metaverse&amp;volume=1&amp;issue=1&amp;publication_year=2024&amp;pages=006-016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B200">
<span class="label">200.</span><cite>Lee D, Kim C, Kim S, Cho M, Han W-S. Autoregressive image generation using residual quantization. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE; 2022.</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>Data will be made available on request.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Research are provided here courtesy of <strong>American Association for the Advancement of Science (AAAS) and Science and Technology Review Publishing House</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.34133/research.0804"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/research.0804.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (17.5 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12364526/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12364526/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12364526%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364526/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12364526/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12364526/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40837875/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12364526/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40837875/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12364526/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12364526/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="tYCw2XIQZO1qA3KmOHN57VdZM9zaer8EcDefn95Edx8I8RQficPcypXbQybVVfK9">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-574fdcc6.js"></script>
    
    

    </body>
</html>
