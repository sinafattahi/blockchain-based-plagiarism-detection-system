
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Audiovisual angle and voice incongruence do not affect audiovisual verbal short-term memory in virtual reality - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4A0EC8AF1CA9305A0EC00415CCA80.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373245/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="Audiovisual angle and voice incongruence do not affect audiovisual verbal short-term memory in virtual reality">
<meta name="citation_author" content="Cosima A Ermert">
<meta name="citation_author_institution" content="Institute for Hearing Technology and Acoustics, RWTH Aachen University, Aachen, Germany">
<meta name="citation_author" content="Manuj Yadav">
<meta name="citation_author_institution" content="Institute for Hearing Technology and Acoustics, RWTH Aachen University, Aachen, Germany">
<meta name="citation_author" content="Jonathan Ehret">
<meta name="citation_author_institution" content="Visual Computing Institute, RWTH Aachen University, Aachen, Germany">
<meta name="citation_author" content="Chinthusa Mohanathasan">
<meta name="citation_author_institution" content="Work and Engineering Psychology, RWTH Aachen University, Aachen, Germany">
<meta name="citation_author" content="Andrea Bönsch">
<meta name="citation_author_institution" content="Visual Computing Institute, RWTH Aachen University, Aachen, Germany">
<meta name="citation_author" content="Torsten W Kuhlen">
<meta name="citation_author_institution" content="Visual Computing Institute, RWTH Aachen University, Aachen, Germany">
<meta name="citation_author" content="Sabine J Schlittmeier">
<meta name="citation_author_institution" content="Work and Engineering Psychology, RWTH Aachen University, Aachen, Germany">
<meta name="citation_author" content="Janina Fels">
<meta name="citation_author_institution" content="Institute for Hearing Technology and Acoustics, RWTH Aachen University, Aachen, Germany">
<meta name="citation_publication_date" content="2025 Aug 22">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0330693">
<meta name="citation_doi" content="10.1371/journal.pone.0330693">
<meta name="citation_pmid" content="40845029">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373245/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373245/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373245/pdf/pone.0330693.pdf">
<meta name="description" content="Virtual reality (VR) environments are frequently used in auditory and cognitive research to imitate real-life scenarios. The visual component in VR has the potential to affect how auditory information is processed, especially if incongruences ...">
<meta name="og:title" content="Audiovisual angle and voice incongruence do not affect audiovisual verbal short-term memory in virtual reality">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Virtual reality (VR) environments are frequently used in auditory and cognitive research to imitate real-life scenarios. The visual component in VR has the potential to affect how auditory information is processed, especially if incongruences ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373245/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12373245">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0330693"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0330693.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373245%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12373245/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12373245/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373245/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0330693" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 22;20(8):e0330693. doi: <a href="https://doi.org/10.1371/journal.pone.0330693" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330693</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Audiovisual angle and voice incongruence do not affect audiovisual verbal short-term memory in virtual reality</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ermert%20CA%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Cosima A Ermert</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Cosima A Ermert</span></h3>
<div class="p">
<sup>1</sup>Institute for Hearing Technology and Acoustics, RWTH Aachen University, Aachen, Germany</div>
<div>Conceptualization, Data curation, Formal analysis, Methodology, Software, Visualization, Writing – original draft, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ermert%20CA%22%5BAuthor%5D" class="usa-link"><span class="name western">Cosima A Ermert</span></a>
</div>
</div>
<sup>1,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Yadav%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Manuj Yadav</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Manuj Yadav</span></h3>
<div class="p">
<sup>1</sup>Institute for Hearing Technology and Acoustics, RWTH Aachen University, Aachen, Germany</div>
<div>Formal analysis, Methodology, Visualization, Writing – original draft, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Yadav%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Manuj Yadav</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ehret%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Jonathan Ehret</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Jonathan Ehret</span></h3>
<div class="p">
<sup>2</sup>Visual Computing Institute, RWTH Aachen University, Aachen, Germany</div>
<div>Conceptualization, Software, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ehret%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Jonathan Ehret</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mohanathasan%20C%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Chinthusa Mohanathasan</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Chinthusa Mohanathasan</span></h3>
<div class="p">
<sup>3</sup>Work and Engineering Psychology, RWTH Aachen University, Aachen, Germany</div>
<div>Formal analysis, Methodology, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mohanathasan%20C%22%5BAuthor%5D" class="usa-link"><span class="name western">Chinthusa Mohanathasan</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22B%C3%B6nsch%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Andrea Bönsch</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Andrea Bönsch</span></h3>
<div class="p">
<sup>2</sup>Visual Computing Institute, RWTH Aachen University, Aachen, Germany</div>
<div>Conceptualization, Methodology, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22B%C3%B6nsch%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Andrea Bönsch</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kuhlen%20TW%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Torsten W Kuhlen</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Torsten W Kuhlen</span></h3>
<div class="p">
<sup>2</sup>Visual Computing Institute, RWTH Aachen University, Aachen, Germany</div>
<div>Conceptualization, Funding acquisition, Supervision, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kuhlen%20TW%22%5BAuthor%5D" class="usa-link"><span class="name western">Torsten W Kuhlen</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Schlittmeier%20SJ%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Sabine J Schlittmeier</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Sabine J Schlittmeier</span></h3>
<div class="p">
<sup>3</sup>Work and Engineering Psychology, RWTH Aachen University, Aachen, Germany</div>
<div>Conceptualization, Funding acquisition, Supervision, Validation, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Schlittmeier%20SJ%22%5BAuthor%5D" class="usa-link"><span class="name western">Sabine J Schlittmeier</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Fels%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Janina Fels</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Janina Fels</span></h3>
<div class="p">
<sup>1</sup>Institute for Hearing Technology and Acoustics, RWTH Aachen University, Aachen, Germany</div>
<div>Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Supervision, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Fels%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Janina Fels</span></a>
</div>
</div>
<sup>1</sup>
</div>
<div class="cg p">Editor: <span class="name western">Patrick Bruns</span><sup>4</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>Institute for Hearing Technology and Acoustics, RWTH Aachen University, Aachen, Germany</div>
<div id="aff002">
<sup>2</sup>Visual Computing Institute, RWTH Aachen University, Aachen, Germany</div>
<div id="aff003">
<sup>3</sup>Work and Engineering Psychology, RWTH Aachen University, Aachen, Germany</div>
<div id="edit1">
<sup>4</sup>University of Hamburg, GERMANY</div>
<div class="author-notes p">
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>The authors have declared that no competing interests exist.</p></div>
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>cosima.ermert@akustik.rwth-aachen.de</span></p>
</div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Cosima A Ermert</span></strong>: <span class="role">Conceptualization, Data curation, Formal analysis, Methodology, Software, Visualization, Writing – original draft, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Manuj Yadav</span></strong>: <span class="role">Formal analysis, Methodology, Visualization, Writing – original draft, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Jonathan Ehret</span></strong>: <span class="role">Conceptualization, Software, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Chinthusa Mohanathasan</span></strong>: <span class="role">Formal analysis, Methodology, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Andrea Bönsch</span></strong>: <span class="role">Conceptualization, Methodology, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Torsten W Kuhlen</span></strong>: <span class="role">Conceptualization, Funding acquisition, Supervision, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Sabine J Schlittmeier</span></strong>: <span class="role">Conceptualization, Funding acquisition, Supervision, Validation, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Janina Fels</span></strong>: <span class="role">Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Supervision, Writing – review &amp; editing</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Patrick Bruns</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Oct 16; Accepted 2025 Aug 4; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Ermert et al</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12373245  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40845029/" class="usa-link">40845029</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>Virtual reality (VR) environments are frequently used in auditory and cognitive research to imitate real-life scenarios. The visual component in VR has the potential to affect how auditory information is processed, especially if incongruences between the visual and auditory information occur. This study investigated how audiovisual incongruence in VR implemented with a head-mounted display (HMD) affects verbal short-term memory compared to presentation of the same material over traditional computer monitors. Two experiments were conducted with both these display devices and two types of audiovisual incongruences: angle (Exp 1) and voice (Exp 2) incongruence. To quantify short-term memory, an audiovisual verbal serial recall (avVSR) task was developed where an embodied conversational agent (ECA) was animated to speak a digit sequence, which participants had to remember. The results showed no effect of the display devices on the proportion of correctly recalled digits overall, although subjective evaluations showed a higher sense of presence in the HMD condition. For the extreme conditions of angle incongruence in the computer monitor presentation, the proportion of correctly recalled digits increased marginally, presumably due to raised attention, but the effect size was negligible. Response times were not affected by incongruences in either display device across both experiments. These findings suggest that at least for the conditions studied here, the avVSR task is robust against angle and voice audiovisual incongruences in both HMD and computer monitor displays.</p></section><section id="sec001"><h2 class="pmc_sec_title">Introduction</h2>
<p>Experiments in auditory and cognitive sciences research are increasingly conducted in virtual reality (VR) environments [<a href="#pone.0330693.ref001" class="usa-link" aria-describedby="pone.0330693.ref001">1</a>,<a href="#pone.0330693.ref002" class="usa-link" aria-describedby="pone.0330693.ref002">2</a>]. VR can evoke a higher feeling of presence, the feeling of “being in a scene” [<a href="#pone.0330693.ref003" class="usa-link" aria-describedby="pone.0330693.ref003">3</a>], compared to computer monitor presentations [<a href="#pone.0330693.ref004" class="usa-link" aria-describedby="pone.0330693.ref004">4</a>]. Thus, VR is widely employed to create immersive environments that approximate real-life scenarios. Compared to traditional laboratory setups with computer monitors, in most cases, experiments in VR differ in terms of the visual information presented. This offers new possibilities for designing experimental environments and tasks, but also raises the question of how this added visual information affects cognitive performance. Importantly, the effect of VR on participants is not yet fully understood and appears to be highly dependent on the experimental design. Several studies have reported positive effects of VR, such as increased attention [<a href="#pone.0330693.ref005" class="usa-link" aria-describedby="pone.0330693.ref005">5</a>,<a href="#pone.0330693.ref006" class="usa-link" aria-describedby="pone.0330693.ref006">6</a>] and enhanced motivation and engagement [<a href="#pone.0330693.ref007" class="usa-link" aria-describedby="pone.0330693.ref007">7</a>,<a href="#pone.0330693.ref008" class="usa-link" aria-describedby="pone.0330693.ref008">8</a>]. However, studies have also reported disadvantages arising from using VR, since it can cause cybersickness [<a href="#pone.0330693.ref009" class="usa-link" aria-describedby="pone.0330693.ref009">9</a>] and may evoke a higher mental load [<a href="#pone.0330693.ref010" class="usa-link" aria-describedby="pone.0330693.ref010">10</a>,<a href="#pone.0330693.ref011" class="usa-link" aria-describedby="pone.0330693.ref011">11</a>].</p>
<p>When implementing audiovisual tasks in VR, perceived incongruences between the auditory and visual modalities can arise. This is especially relevant when the visual VR environment is not carefully crafted and aligned with the acoustic VR environment. Such an interplay of auditory and visual information has been a key aspect of cognitive research for multiple years, albeit without VR, and being based instead on computer monitor presentations. These studies have shown that coherent auditory and visual information can be integrated into a single multimodal percept [<a href="#pone.0330693.ref012" class="usa-link" aria-describedby="pone.0330693.ref012">12</a>,<a href="#pone.0330693.ref013" class="usa-link" aria-describedby="pone.0330693.ref013">13</a> for a review], Coherence in general is a multidimensional construct, including but not limited to spatial, temporal, and semantic alignment of the auditory and visual stimuli [<a href="#pone.0330693.ref012" class="usa-link" aria-describedby="pone.0330693.ref012">12</a>]. Spatial audiovisual integration is commonly investigated in terms of the <em>ventriloquism effect</em> [<a href="#pone.0330693.ref014" class="usa-link" aria-describedby="pone.0330693.ref014">14</a>,<a href="#pone.0330693.ref015" class="usa-link" aria-describedby="pone.0330693.ref015">15</a>], which refers to the bias in auditory spatial perception toward a synchronous but spatially discrepant visual stimulus. This effect is typically investigated using brief sounds (e.g., clicks) and visual flashes presented with small spatial separations [<a href="#pone.0330693.ref016" class="usa-link" aria-describedby="pone.0330693.ref016">16</a>]. Further, audiovisual stimuli are perceived as temporally incoherent if their stimulus onset asynchrony, i.e., the temporal offset between the onset of the auditory and visual stimuli, is large enough [<a href="#pone.0330693.ref013" class="usa-link" aria-describedby="pone.0330693.ref013">13</a>]. If auditory and visual stimuli are aligned in meaning, semantic congruence is elicited [<a href="#pone.0330693.ref012" class="usa-link" aria-describedby="pone.0330693.ref012">12</a>]. A well-known example of studying semantic incongruence is the <em>Stroop task</em> [<a href="#pone.0330693.ref017" class="usa-link" aria-describedby="pone.0330693.ref017">17</a>], where participants see color words (e.g., “red”) printed in a different color (e.g., green), and are asked to name the color of the ink. Another established audiovisual incongruence effect is the <em>McGurk effect</em> [<a href="#pone.0330693.ref018" class="usa-link" aria-describedby="pone.0330693.ref018">18</a>], which is typically studied using lip movements that are presented together with congruent or incongruent syllables (e.g., “ba”, “ga”). These studies show that lip movement can alter the auditory perceived syllable, if they share the same features (e.g., “pa” and “ba”, but not “pa” and “ta”) [<a href="#pone.0330693.ref018" class="usa-link" aria-describedby="pone.0330693.ref018">18</a>].</p>
<p>While the incongruences listed above have mostly been studied with computer monitor presentations, there is some research investigating audiovisual congruence in VR. For example, the ventriloquism effect has been replicated in VR by Huisman <em>et al</em>. [<a href="#pone.0330693.ref019" class="usa-link" aria-describedby="pone.0330693.ref019">19</a>]. A large angular audiovisual separation has been shown to impede user experience in terms of presence, immersion, and cybersickness, while small separation angles are typically well-tolerated by participants in VR [<a href="#pone.0330693.ref020" class="usa-link" aria-describedby="pone.0330693.ref020">20</a>]. Incongruence in lip movement in terms of the McGurk effect is detected more easily in VR than in computer monitor settings [<a href="#pone.0330693.ref021" class="usa-link" aria-describedby="pone.0330693.ref021">21</a>]. Providing coherent user interaction in VR, e.g., by synchronized audiovisual virtual hand illusions [<a href="#pone.0330693.ref022" class="usa-link" aria-describedby="pone.0330693.ref022">22</a>] or sound feedback according to the user’s movements, e.g., walking [<a href="#pone.0330693.ref023" class="usa-link" aria-describedby="pone.0330693.ref023">23</a>], has been shown to increase immersion.</p>
<p>Notably, these studies on audiovisual incongruences in VR have primarily focused on perception-based effects, such as detectability, or subjectively perceived presence and immersion. As VR is increasingly being employed in cognitive research, it is crucial to understand whether incongruences impact cognitive functions that rely on higher-level processing of information. While it is logical to hypothesize that audiovisual incongruences beyond a certain range could impede cognitive task performance in VR, to the best of our knowledge, this has not been demonstrated yet in experimental settings.</p>
<p>Moreover, within the context of real-world interaction, a particularly relevant cognitive function where auditory and visual information is integrated includes listening to a talker and trying to remember what has been said. In contrast to controlled laboratory settings, everyday face-to-face interactions typically exhibit high audiovisual congruence: when a person speaks, their voice is accompanied by matching visual cues such as spatial location, lip movement, gaze direction, vocal pitch (e.g., gender), and gestures [<a href="#pone.0330693.ref024" class="usa-link" aria-describedby="pone.0330693.ref024">24</a>] - all temporally synchronized and spatially aligned. However, in VR, any of these congruences may be compromised, for instance, when the perceived voice does not match the position or gender appearance of the virtual human, the embodied conversational agent (ECA).</p>
<p>Hence, this study aims to investigate the extent to which certain audiovisual incongruences that are relevant to everyday interaction between two interlocutors affect cognitive task performance and subjective assessments across head-mounted display (HMD) and traditional computer monitor presentations. To examine whether audiovisual incongruences impair memory for spoken content, we implemented an audiovisual version of the verbal serial recall task [<a href="#pone.0330693.ref025" class="usa-link" aria-describedby="pone.0330693.ref025">25</a>]. This implementation includes measuring verbal short-term memory under systematically controlled audio-visual incongruencies: in the first experiment, the effect of audiovisual angle incongruences was studied, whereas the second experiment studied the effect of voice incongruence. In both studies, an HMD and a computer monitor were used as display devices to enable a direct comparison.</p></section><section id="sec002"><h2 class="pmc_sec_title">Experiment 1: Audiovisual spatial incongruence</h2>
<p>As mentioned previously, the starting point for this study is a situation common to everyday life: listening to and remembering spoken information by an interlocutor. Traditionally, the ability to rememorize and remember verbal information has been measured using the verbal serial recall task [<a href="#pone.0330693.ref025" class="usa-link" aria-describedby="pone.0330693.ref025">25</a>]. In this task, participants have to remember and recall a sequence of verbal items, e.g., a sequence of digits or words, for a short period of time. These items are typically presented unimodally, i.e., visually only (as images [<a href="#pone.0330693.ref026" class="usa-link" aria-describedby="pone.0330693.ref026">26</a>], written [<a href="#pone.0330693.ref027" class="usa-link" aria-describedby="pone.0330693.ref027">27</a>,<a href="#pone.0330693.ref028" class="usa-link" aria-describedby="pone.0330693.ref028">28</a>], or as lip-movements [<a href="#pone.0330693.ref029" class="usa-link" aria-describedby="pone.0330693.ref029">29</a>]), or auditorily only (as spoken words [<a href="#pone.0330693.ref027" class="usa-link" aria-describedby="pone.0330693.ref027">27</a>,<a href="#pone.0330693.ref030" class="usa-link" aria-describedby="pone.0330693.ref030">30</a>], like in a listening situation). For a discussion of the differences between visual and auditory digit presentations, please refer to Yadav <em>et al</em>. [<a href="#pone.0330693.ref031" class="usa-link" aria-describedby="pone.0330693.ref031">31</a>].</p>
<p>This task can be extended to the aforementioned scenario of listening to an interlocutor in laboratory settings where a visual representation of the interlocutor - an ECA - speaks the digit sequences in the serial recall task. This involves the digit sequences being presented audiovisually, i.e., as simultaneous lip movements and auditory items. This variation of the classic verbal serial recall is referred to as audiovisual verbal serial recall (avVSR) in the following.</p>
<p>In selecting the audiovisual incongruence for Experiment 1, our objective was to manipulate co-verbal information conveyed by the ECA, so that the participants would naturally focus on it, while simultaneously ensuring that their ability to successfully complete the task was not directly impeded. Thus, we decided to introduce a spatial incongruence in the audiovisual stimulus presentation. More specifically, an angular offset was introduced between the visual position of the ECA and the direction from which the digit sequences were spoken to create a discrepancy between the auditory and visual scenes. The influence of this <em>audiovisual angle incongruence</em> on avVSR performance was evaluated for both HMD and computer monitor presentations as depicted in <a href="#pone.0330693.g001" class="usa-link">Fig 1</a>.</p>
<figure class="fig xbox font-sm" id="pone.0330693.g001"><h3 class="obj_head">Fig 1. Factors in Experiment 1.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/56fe/12373245/552819a2f4ea/pone.0330693.g001.jpg" loading="lazy" height="733" width="733" alt="Fig 1"></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330693.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The between-subject experimental design of the avVSR task with three factors: display device (computer monitor vs. HMD), the audiovisual angle incongruence (congruent vs. incongruent), and the serial position (1 - 8) of the target digit sequence. In the congruent case, the ECA and the virtual sound source are co-located; in the incongruent case, there is an angular offset between the ECA and the virtual sound source.</p></figcaption></figure><section id="sec003"><h3 class="pmc_sec_title">Method</h3>
<section id="sec004"><h4 class="pmc_sec_title">Participants.</h4>
<p>A total of 25 adults were recruited for the experiment via mailing lists and posters in the authors’ institutes between 27th of April and 13th of May 2022. Participants were required to be native German speakers (as the entire experiment was conducted in German), have normal hearing, and (corrected-to-)normal vision. Normal hearing below <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e001"><math id="M1" display="inline" overflow="linebreak"><mrow><mn>25</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">B</mi></mrow></mrow></math></span> HL [<a href="#pone.0330693.ref032" class="usa-link" aria-describedby="pone.0330693.ref032">32</a>] between <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e002"><math id="M2" display="inline" overflow="linebreak"><mrow><mn>250</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">H</mi><mi mathvariant="normal">z</mi></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e003"><math id="M3" display="inline" overflow="linebreak"><mrow><mn>14</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">k</mi><mi mathvariant="normal">H</mi><mi mathvariant="normal">z</mi></mrow></mrow></math></span> was tested with an AURITEC Ear 3.0 audiometer and Sennheiser HDA300 headphones using a pulsed pure-tone ascending audiometry. Two participants failed the audiometry and had to be excluded. <em>N</em> = 23 participants (16 male, 6 female, 1 non-binary, aged between 19 and 36 years, <em>M</em> = 25.74, <em>SD</em> = 4.03) passed the audiometry. Normal or corrected-to-normal vision was validated with a Snellen chart up to (20/30) [<a href="#pone.0330693.ref033" class="usa-link" aria-describedby="pone.0330693.ref033">33</a>]. All participants gave written informed consent before the experiment began. Participants received a 10€ gift voucher for a bookstore. The experiment procedure was pre-approved by the Ethics Committee at the Medical Faculty of RWTH Aachen University (EK396-19). The study was conducted in accordance to the rules of conduct stated in the Declaration of Helsinki.</p></section><section id="sec005"><h4 class="pmc_sec_title">Cognitive task.</h4>
<p>Each experimental trial of the avVSR task was structured as follows (see <a href="#pone.0330693.g002" class="usa-link">Fig 2</a>): the trial started with a visual <em>countdown</em> consisting of three rectangles decreasing in size at a rate of one rectangle per <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e006"><math id="M6" display="inline" overflow="linebreak"><mrow><mn>500</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">s</mi></mrow></mrow></math></span>, rendered on a plane which covered the ECA entirely. Afterwards, in the <em>stimulus presentation phase</em>, participants saw the ECA speak a sequence of eight digits at a rate of one digit per second (digit word duration: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e007"><math id="M7" display="inline" overflow="linebreak"><mrow><mn>600</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">s</mi></mrow></mrow></math></span>, interstimulus interval: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e008"><math id="M8" display="inline" overflow="linebreak"><mrow><mn>400</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">s</mi></mrow></mrow></math></span>). In this phase, the ECA spoke the digits by moving its lips in accordance with the sound. The digit sequences were created from digits between one and nine in such a way that no digit was repeated and no more than two consecutive steps, up or down, occurred (e.g., “1-2-3-4” or “9-8-7-6” would not be allowed). After the stimulus presentation, participants had to wait for a <em>retention interval</em> of three seconds before they were asked to reproduce the order of the digits by clicking on the corresponding fields in a matrix, which was rendered as blocks in front of the ECA (see <a href="#pone.0330693.g002" class="usa-link">Fig 2</a>). During this <em>recall phase</em>, the ECA was partially visible and animated with an idle movement. Only the eight digits present in the sequence were displayed in the matrix. The order of the digits within the matrix was changed randomly for each trial to avoid the use of visuo-spatial recall techniques [<a href="#pone.0330693.ref034" class="usa-link" aria-describedby="pone.0330693.ref034">34</a>]. Once participants clicked on a digit, it disappeared. Corrections were not possible. The dependent variables were the proportion of correctly recalled digits and the response times.</p>
<figure class="fig xbox font-sm" id="pone.0330693.g002"><h5 class="obj_head">Fig 2. One trial of the avVSR task.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373245_pone.0330693.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/56fe/12373245/b0db7ab00aff/pone.0330693.g002.jpg" loading="lazy" height="213" width="750" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330693.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Graphical depiction of the trial phases <em>Countdown</em>, <em>Stimulus presentation</em>, <em>Retention interval</em>, and <em>Recall phase</em> over time. In the <em>Recall phase</em>, the cursor is visible as a red cross.</p></figcaption></figure></section><section id="sec006"><h4 class="pmc_sec_title">Apparatus and materials.</h4>
<p>The avVSR task was configured for both HMD and computer monitor presentations and implemented in Unreal Engine (Epic Games, v4.27) using the following plugins (see section “Supporting Information”): the StudyFramework plugin [<a href="#pone.0330693.ref035" class="usa-link" aria-describedby="pone.0330693.ref035">35</a>] (v4.26), which handles the data logging and experiment procedure, the RWTH VR Toolkit plugin [<a href="#pone.0330693.ref036" class="usa-link" aria-describedby="pone.0330693.ref036">36</a>] (v4.27) for basic VR interaction, and the Character plugin (v4.27) for ECA animation. In the HMD condition, the scene was presented using an HTC Vive Pro Eye (dual AMOLED screens with 3.5″ diagonal display, resolution <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e010"><math id="M10" display="inline" overflow="linebreak"><mrow><mn>1440</mn><mi>×</mi><mn>1600</mn></mrow></math></span> pixels per eye) and one HTC Vive controller. In the computer monitor condition, a Fujitsu B24T-7 (24″ diagonal display, resolution 1920 × 1080 pixels) monitor and a wireless computer mouse (Cherry MW2310) were used. To ensure consistency between the presentation modalities, the actual room in which the experiment took place (VR laboratory [<a href="#pone.0330693.ref037" class="usa-link" aria-describedby="pone.0330693.ref037">37</a>]) was replicated in VR so that the participants were virtually “in the same room” regardless of the display device used (see <a href="#pone.0330693.g003" class="usa-link">Fig 3</a>).</p>
<figure class="fig xbox font-sm" id="pone.0330693.g003"><h5 class="obj_head">Fig 3. Listening experiment setup.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373245_pone.0330693.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/56fe/12373245/fcbc00ccec8c/pone.0330693.g003.jpg" loading="lazy" height="241" width="660" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330693.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The experiment was done with both (left) computer monitor and (right) HMD presentation. <em>Left</em>: computer monitor presentation in which a participant is seen wearing headphones with Optitrack tracking markers. <em>Right</em>: representation of the VR scene created in Unreal Engine. When wearing the HMD, the point of view of the blue camera represents the participants’ point of view.</p></figcaption></figure><p>For the visual stimulus, a female ECA (see <a href="#pone.0330693.g002" class="usa-link">Figs 2</a> and <a href="#pone.0330693.g003" class="usa-link">3</a>) was created with MetaHuman Creator (Epic Games, v0.5.2). Using Oculus Lip Sync (Meta, v20), lip animation was generated from the audio files such that the ECA moved its lips according to the digit sequence heard. In the HMD condition, the ECA sat on a virtual chair at a distance of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e012"><math id="M12" display="inline" overflow="linebreak"><mrow><mi>d</mi><mo>=</mo><mn>2.5</mn><mspace width="0.167em"></mspace><mi>m</mi></mrow></math></span> from the participant, which is a comfortable inter-person distance to an unknown person [<a href="#pone.0330693.ref038" class="usa-link" aria-describedby="pone.0330693.ref038">38</a>,<a href="#pone.0330693.ref039" class="usa-link" aria-describedby="pone.0330693.ref039">39</a>]. In the computer monitor condition, only the upper part of the body was visible on the computer monitor. This image section and the distance of the computer monitor from the participants were chosen so that the size and position of the ECA were consistent between the two display devices. Participants sat on a chair, which was adjusted so that the participants (measured from the top of the head) and the ECA were approximately at the same height. The two main visual differences between display devices were that in the HMD condition the participants could not see themselves and the ECA’s lower body was not visible in the computer monitor condition.</p>
<p>For the acoustic stimulus, German digit words (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e013"><math id="M13" display="inline" overflow="linebreak"><mrow><mn>600</mn><mtext> </mtext><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">s</mi></mrow></mrow></math></span> in duration) were taken from the database by Oberem and Fels [<a href="#pone.0330693.ref040" class="usa-link" aria-describedby="pone.0330693.ref040">40</a>] (voice <em>female b</em>). The original publication did not include a recording of the digit five, but was made available by the authors. No loudspeakers were used to avoid visual fixation to possible source positions. Instead, virtual sound sources for a binaural headphone-based reproduction were auralized with Virtual Acoustics [<a href="#pone.0330693.ref041" class="usa-link" aria-describedby="pone.0330693.ref041">41</a>] (v2020a) in a reflection-free (anechoic) environment using dynamic scene rendering. Communication between Unreal Engine and Virtual Acoustics was established via the Virtual Acoustics Plugin (v4.26) (see section “Supporting Information”). This was achieved by tracking the head movements of the participants and virtually shifting the sound source position accordingly. Head-tracking was done in two different ways depending on the display device: for the HMD, the built-in tracking system was used. This was not possible in the computer monitor presentation. Instead, an OptiTrack system with Motive (OptiTrack, v2.1.1) tracked head movements. To provide plausible localization cues, the head-related transfer function (HRTF) of the Institute for Hearing Technology and Acoustics (IHTA) artificial head [<a href="#pone.0330693.ref042" class="usa-link" aria-describedby="pone.0330693.ref042">42</a>] was employed. As shown by Oberem <em>et al</em>. [<a href="#pone.0330693.ref043" class="usa-link" aria-describedby="pone.0330693.ref043">43</a>], this generic HRTF provides sufficiently accurate localization in dynamic scenes for most listeners.</p>
<p>The auditory stimuli were played back using Sennheiser HD650 open-back headphones with a Behringer ADA8200 sound card for both display devices. All auditory stimuli were calibrated to <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e014"><math id="M14" display="inline" overflow="linebreak"><mrow><mn>60</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">B</mi></mrow></mrow></math></span>(A) as the power sum of both ears using the IHTA artificial head [<a href="#pone.0330693.ref042" class="usa-link" aria-describedby="pone.0330693.ref042">42</a>]. Headphone equalization was performed individually for each participant following Masiero and Fels [<a href="#pone.0330693.ref044" class="usa-link" aria-describedby="pone.0330693.ref044">44</a>].</p></section><section id="sec007"><h4 class="pmc_sec_title">Audiovisual angle incongruence.</h4>
<p>An angle incongruence was introduced to the audiovisual stimulus (audio: spoken digit words, visual: lip movement of ECA) by implementing an angular offset between the auditory source position and the location of the ECA. The position of the ECA as the visual source was kept constant at an azimuth angle of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e015"><math id="M15" display="inline" overflow="linebreak"><mrow><mi>φ</mi><mo>=</mo><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span>, straight ahead of the participant (see <a href="#pone.0330693.g004" class="usa-link">Fig 4</a>). The idea behind the choice of the angular offsets was to include angles small enough to allow the audiovisual stimulus to be perceived as a unit and angles large enough so that an audiovisual angle incongruence is expected to be detectable. Since conversational partners are approximately at the same height in seated settings, we limited our selection to angles on the horizontal plane. Studies on the ventriloquism effect report various threshold angles up to which spatially separated visual and auditory stimuli are integrated. It is suggested that angular offset between auditory and visual source positions should not exceed <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e016"><math id="M16" display="inline" overflow="linebreak"><mrow><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> for audiovisual integration to occur [<a href="#pone.0330693.ref014" class="usa-link" aria-describedby="pone.0330693.ref014">14</a>,<a href="#pone.0330693.ref045" class="usa-link" aria-describedby="pone.0330693.ref045">45</a>], but integration effects up to <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e017"><math id="M17" display="inline" overflow="linebreak"><mrow><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> have been shown on the horizontal plane [<a href="#pone.0330693.ref046" class="usa-link" aria-describedby="pone.0330693.ref046">46</a>]. In their study on audiovisual congruence in VR, Kim and Lee [<a href="#pone.0330693.ref020" class="usa-link" aria-describedby="pone.0330693.ref020">20</a>] found that participants generally tolerate angular offsets of up to <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e018"><math id="M18" display="inline" overflow="linebreak"><mrow><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> on the horizontal plane and detect angle offsets of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e019"><math id="M19" display="inline" overflow="linebreak"><mrow><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> only with a probability of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e020"><math id="M20" display="inline" overflow="linebreak"><mrow><mn>18.8</mn><mi>%</mi></mrow></math></span>. It should be noted that these angular limits are usually determined with simplified audiovisual stimuli such as noise bursts and flashing lights. Thresholds for realistic sounds, such as speech, have hardly been investigated [<a href="#pone.0330693.ref019" class="usa-link" aria-describedby="pone.0330693.ref019">19</a>].</p>
<figure class="fig xbox font-sm" id="pone.0330693.g004"><h5 class="obj_head">Fig 4. Audiovisual angle incongruence in Experiment 1.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373245_pone.0330693.g004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/56fe/12373245/81123c09ac72/pone.0330693.g004.jpg" loading="lazy" height="425" width="675" alt="Fig 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330693.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The female ECA is displayed at an azimuth angle of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e021"><math id="M21" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> on the horizontal plane at a distance of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e022"><math id="M22" display="inline" overflow="linebreak"><mrow><mi>d</mi><mo>=</mo><mn>2.5</mn><mspace width="0.167em"></mspace><mi>m</mi></mrow></math></span> from the listener (blue). The possible virtual sound source positions (green) are at the azimuth angles <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e023"><math id="M23" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup><mo>,</mo><mi>±</mi><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup><mo>,</mo><mi>±</mi><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup><mo>,</mo></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e024"><math id="M24" display="inline" overflow="linebreak"><mrow><mi>±</mi><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> at the distance <em>d</em>.</p></figcaption></figure><p>Since headphone reproduction with a generic HRTF is used here, a localization offset compared to a headphone reproduction with an individual HRTF would be expected. However, Oberem <em>et al</em>. [<a href="#pone.0330693.ref043" class="usa-link" aria-describedby="pone.0330693.ref043">43</a>] showed that this localization offset is non-significant on the horizontal plane with dynamic reproduction with head movements for the given generic HRTF with speech stimuli.</p>
<p>Based on the above, four angular offsets were chosen for the audiovisual angle incongruence. As a baseline condition, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e025"><math id="M25" display="inline" overflow="linebreak"><mrow><mi>Δ</mi><mi>φ</mi><mo>=</mo><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> was included, where the position of the auditory target and visual target are aligned. Furthermore, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e026"><math id="M26" display="inline" overflow="linebreak"><mrow><mi>Δ</mi><mi>φ</mi><mo>=</mo><mi>±</mi><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e027"><math id="M27" display="inline" overflow="linebreak"><mrow><mi>±</mi><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e028"><math id="M28" display="inline" overflow="linebreak"><mrow><mi>±</mi><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> were included with decreasing probability of integration expected for increasing angular differences.</p></section><section id="sec008"><h4 class="pmc_sec_title">Questionnaires.</h4>
<p>To link the behavioral data from the avVSR in the two display devices with subjective impressions and to quantify presence, two distinct questionnaires were used: one questionnaire that had to be completed for each display device and a second general questionnaire. Both questionnaires were implemented using SoSciSurvey in German [<a href="#pone.0330693.ref047" class="usa-link" aria-describedby="pone.0330693.ref047">47</a>].</p>
<p>In the first questionnaire, the perceived presence during the HMD presentation compared to the computer monitor presentation was evaluated with a German translation [<a href="#pone.0330693.ref048" class="usa-link" aria-describedby="pone.0330693.ref048">48</a>] of the Slater, Usoh, and Steed presence questionnaire (SUS) [<a href="#pone.0330693.ref049" class="usa-link" aria-describedby="pone.0330693.ref049">49</a>] on a 7-point Likert scale with the corresponding anchors. Additionally, participants were asked about the appearance of the ECA (following Ehret at al. [<a href="#pone.0330693.ref050" class="usa-link" aria-describedby="pone.0330693.ref050">50</a>]): ( <strong>Q_Nat</strong>: <em>How natural did the interlocutor seem to you?</em>, <strong>Q_Spe</strong>: <em>How natural did the way the interlocutor spoke seem to you?</em>, <strong>Q_Sen</strong>: <em>To which degree did the interlocutor appear to be a sentient being?</em>). Furthermore, the modality participants focused on ( <strong>Q_Vis</strong>: <em>How much did you focus visually on the interlocutor?</em>) and the controller handling ( <strong>Q_Con</strong>: <em>How intuitive was the handling of the controller?</em>) were examined.</p>
<p>At the end of the experiment, participants completed a second questionnaire. This questionnaire focused on the task itself ( <strong>Q_Dif</strong>: <em>How difficult was the task for you?</em>) and on the audiovisual incongruence ( <strong>Q_Inc</strong>: <em>Did you notice any audiovisual incongruence?</em> [Asked on a yes/no basis], <strong>Q_Imp</strong>: <em>If you noticed the audiovisual incongruence, did it impact your performance?</em>, <strong>Q_Dom</strong>: <em>If you noticed the audiovisual incongruence, did you shift your attention towards one domain (audio, visual) as a result?</em>). All these ratings, except for <strong>Q_Inc</strong>, were made on a 7-point Likert scale between 1 (minimum, e.g., “Not at all”) and 7 (maximum, e.g., “Strongly”). A comment field was provided for additional remarks. All questions and their German translation with corresponding anchors can be found in <a href="#pone.0330693.s004" class="usa-link">S4 File</a>.</p></section><section id="sec009"><h4 class="pmc_sec_title">Procedure.</h4>
<p>The experiment was conducted in individual sessions at the IHTA, RWTH Aachen University. The audiometric and visual screening was performed in a sound isolated hearing booth [<a href="#pone.0330693.ref037" class="usa-link" aria-describedby="pone.0330693.ref037">37</a>]. After passing the screening, the main experiment started in the VR laboratory [<a href="#pone.0330693.ref037" class="usa-link" aria-describedby="pone.0330693.ref037">37</a>]. The display device was varied across participants in two blocks, meaning that participants had to complete all trials on one display device before moving to the next one. The starting display device was counterbalanced between participants. At the beginning of the experiment, written instructions explaining the task were given. The participants were instructed to attend to the digit sequences and remember them, without being informed as to whether they were to prioritize the auditory or visual information. Participants were further asked not to vocalize the digits or use their fingers as an aid. The instructions were followed by a training block consisting of eight trials, two for each of the four audiovisual offsets angles (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e029"><math id="M29" display="inline" overflow="linebreak"><mrow><mo stretchy="false">|</mo><mi>Δ</mi><mi>φ</mi><mo stretchy="false">|</mo><mo>=</mo><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup><mo>,</mo><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup><mo>,</mo><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup><mo>,</mo><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></mrow></math></span>) in counterbalanced order. The training was followed by twelve trials for each of the four angles, yielding a total of 12 × 4 = 48 trials per display device in the main experiment, divided into three blocks of 16 trials each. Each offset angle was presented twice in a block, once from the left and once from the right side; the offset angle of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e030"><math id="M30" display="inline" overflow="linebreak"><mrow><mo stretchy="false">|</mo><mi>Δ</mi><mi>φ</mi><mo stretchy="false">|</mo><mo>=</mo><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> was presented twice from the front. The order of angles was counterbalanced across participants to avoid order effects. Between blocks, participants could take a break of no fixed length. After each display device block, participants filled out the first questionnaire. Participants completed the second questionnaire at the end of the experiment. The reason for presenting the second questionnaire at the end rather than between the display device blocks was to avoid participants’ performance being affected by inadvertently disclosing the research questions of the listening experiment. The entire experiment, including screening and questionnaires, took approximately 90 minutes.</p></section><section id="sec010"><h4 class="pmc_sec_title">Statistical analysis.</h4>
<p>All statistical analyses were conducted in the software R (version 4.2.2). The R package <em>tidyverse</em> [<a href="#pone.0330693.ref051" class="usa-link" aria-describedby="pone.0330693.ref051">51</a>] was used for data processing and plots. For the avVSR task, there were two dependent variables (i.e., fixed effects): the <em>Accuracy</em> and the average response times <em>RT_mean</em>. <em>RT_mean</em> is defined as the average time between two button clicks in the recall phase or, for the first digit, the time between the start of the recall phase and the first button click. <em>Accuracy</em> is the proportion of digits recalled at the correct serial position. The independent variables included the serial position, the angle (see <a href="#pone.0330693.g004" class="usa-link">Fig 4</a>), and the display device. Separate Bayesian generalized mixed-effects models were created for each dependent variable using the R package <em>brms</em> (version 2.18) [<a href="#pone.0330693.ref052" class="usa-link" aria-describedby="pone.0330693.ref052">52</a>]. In each model, the repeated-measures experimental design was incorporated as independently varying intercepts (i.e., random effects) for the participants. Since <em>Accuracy</em> includes proportion data with many values close or equal to 1, a zero-one-inflated beta distribution with a logit link function was used as the family for the models with this dependent variable. A gamma distribution with a log link function was used as the family for <em>RT_mean</em> as the dependent variable since such distributions are commonly used for time data.</p>
<p>The modeling process for the avVSR task included starting with an intercept-only baseline model, followed by including an independent variable or an interaction between the independent variables in subsequent models. The prior distributions per independent variable and their interactions were sampled 16000 times, using four independent chains of 5000 samples each and discarding the first 1000 warm-up samples. To avoid dependencies per chain, No-U-Turn Sampling was employed. Weakly-informative prior distributions (in the logit or log scale, depending on the dependent variable) were used, which for the fixed effects included normal distributions each with a mean (<em>M</em>) of 0 and a standard deviation (<em>SD</em>) of 2. The latter is relatively large given the scales, and hence conservative. For the random effects, heavily tailed Student-t distributions with <em>M</em> = 0 and <em>SD</em> = 2 each were used. The performance across models was compared using the leave-one-out cross-validation criteria, based on the difference in values of the expected log point wise predictive density and the standard error of the expected log point wise predictive density of the models [<a href="#pone.0330693.ref053" class="usa-link" aria-describedby="pone.0330693.ref053">53</a>]. Posterior predictive checks were performed for all the models.</p>
<p>Pairwise comparisons across independent variable levels were conducted using the R package <em>emmeans</em> [<a href="#pone.0330693.ref054" class="usa-link" aria-describedby="pone.0330693.ref054">54</a>]. These comparisons are summarized using the median and 95% Credible Intervals (CIs) calculated using the highest density interval of the Posterior Probability Distribution (PPD). The Bayesian 95% CI provides the interval within which 95% of the PPD lies. This interval considers the probability range or the uncertainty/belief regarding the parameter value (e.g., mean), contingent on both the present sample and the prior information. This presents a more direct statement regarding the parameter than the 95% confidence interval used in frequentist statistics. The latter generally means that upon repeated random sampling and calculating the 95% confidence interval per sample, the true parameter value would be contained within approximately 95% of those intervals. This so-called long-run probability takes all values being equally likely a priori and parameter values are then contingent upon the present sample.</p>
<p>To determine whether an effect (comparison across independent variable levels) exists, we consider the probability of direction (PD) of the PPD, which provides the probability of an effect going in either the positive or negative direction. PD further provides a link to frequentist statistics in that the two-sided <em>p</em>-values of .05 and .01 approximately correspond to PD of 97.5% and 99.5%, respectively. PD, however, is not an index of significance. In order to consider whether an effect is meaningful/significant, we use the proportion of the 95% CI inside the region of practical equivalence (ROPE). The latter is the range signifying an effect of negligible magnitude and is conceptually similar to the null hypothesis in frequentist statistics. The range for the ROPE was specified as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e031"><math id="M31" display="inline" overflow="linebreak"><mrow><mi>±</mi><mn>0.1</mn><mi>×</mi><msub><mrow><mi>S</mi><mi>D</mi></mrow><mrow><mi>I</mi><mi>V</mi></mrow></msub></mrow></math></span> [<a href="#pone.0330693.ref055" class="usa-link" aria-describedby="pone.0330693.ref055">55</a>]. PD and ROPE were calculated using the R package <em>bayestestR</em> [<a href="#pone.0330693.ref056" class="usa-link" aria-describedby="pone.0330693.ref056">56</a>].</p>
<p>For each of the questionnaire items, separate Bayesian mixed-effects models were created with the display device as the only fixed effect and independently varying intercepts (random effects) for the participants. For all questions except for Q_Inc, the ratings (the dependent variable) were modelled using the cumulative family with a probit link, since the ratings were on an ordinal scale. For Q_Inc, the rating was modelled using the Bernoulli family with a logit link, since the ratings were binary (yes/no). A weakly informative prior with a mean of 0 and a standard deviation of 5 was used for the dependent variables. Other modeling criteria were the same as used for the avVSR data. The differences in ratings between the display devices are summarized using the standard deviation and 95% CI (calculated using the highest density interval of the PPD) for all questions except Q_Inc, for which the change in log-odds of the response and the associated 95% CI is used.</p></section></section><section id="sec011"><h3 class="pmc_sec_title">Results</h3>
<section id="sec012"><h4 class="pmc_sec_title">Audiovisual verbal serial recall.</h4>
<p>The full result data is given in <a href="#pone.0330693.s001" class="usa-link">S1 File</a>, plots of the data can be found in <a href="#pone.0330693.s003" class="usa-link">S3 File</a>. Based on the leave-one-out criteria with <em>Accuracy</em> as the dependent variable, the baseline (intercept-only) model was the worst model, while the performance across models with the various independent variables and their interactions were not substantially different. Hence, the model with all the independent variables and an interaction term between the audiovisual offset angle and display device was selected as the final model since it encompasses the experimental design most comprehensively. The interaction term with serial position was not considered useful for the research intend and hence not included. The pairwise comparison for the final model is presented in <a href="#pone.0330693.t001" class="usa-link">Table 1</a>. The median differences across the comparisons were generally quite low with PD less than 97.5% except for the difference between the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e032"><math id="M32" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e033"><math id="M33" display="inline" overflow="linebreak"><mrow><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> angles with the computer monitor display device. The latter indicates that there is a higher <em>Accuracy</em> in the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e034"><math id="M34" display="inline" overflow="linebreak"><mrow><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> compared to the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e035"><math id="M35" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> condition, suggesting an effect. However, the high percentage of the PPD within the ROPE suggests that this effect is likely of negligible practical significance (i.e., negligible effect size), as it falls within the range which is generally considered too small to be meaningful.</p>
<section class="tw xbox font-sm" id="pone.0330693.t001"><h5 class="obj_head">Table 1. Summary of pairwise comparisons between the audiovisual Angle offsets (i.e., between the auditory source and the visual representation of the ECA) and the display Device combinations with <em>Accuracy</em> as the dependent variable.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead>
<tr>
<th align="left" colspan="2" rowspan="1">Comparison</th>
<th align="left" rowspan="2" colspan="1">Median</th>
<th align="left" rowspan="2" colspan="1">95% CI</th>
<th align="left" rowspan="2" colspan="1">PD</th>
<th align="left" rowspan="2" colspan="1">% in ROPE</th>
</tr>
<tr>
<th align="left" rowspan="1" colspan="1">Angle</th>
<th align="left" rowspan="1" colspan="1">Device</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e036">
<math id="M36" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Monitor</td>
<td align="left" rowspan="1" colspan="1">-0.01</td>
<td align="left" rowspan="1" colspan="1">[-0.04, 0.01]</td>
<td align="left" rowspan="1" colspan="1">83.88%</td>
<td align="left" rowspan="1" colspan="1">72.15%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e037">
<math id="M37" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Monitor</td>
<td align="left" rowspan="1" colspan="1">-0.02</td>
<td align="left" rowspan="1" colspan="1">[-0.04, 0.01]</td>
<td align="left" rowspan="1" colspan="1">91.64%</td>
<td align="left" rowspan="1" colspan="1">56.24%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e038">
<math id="M38" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Monitor</td>
<td align="left" rowspan="1" colspan="1">-0.03</td>
<td align="left" rowspan="1" colspan="1">[-0.06, -0.01]</td>
<td align="left" rowspan="1" colspan="1">99.69%</td>
<td align="left" rowspan="1" colspan="1">11.01%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e039">
<math id="M39" display="inline" overflow="linebreak"><mrow><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Monitor</td>
<td align="left" rowspan="1" colspan="1">-5.13<em>e</em><sup>−3</sup>
</td>
<td align="left" rowspan="1" colspan="1">[-0.03, 0.02]</td>
<td align="left" rowspan="1" colspan="1">65.78%</td>
<td align="left" rowspan="1" colspan="1">89.24%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e040">
<math id="M40" display="inline" overflow="linebreak"><mrow><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Monitor</td>
<td align="left" rowspan="1" colspan="1">-0.02</td>
<td align="left" rowspan="1" colspan="1">[-0.05, 0.00]</td>
<td align="left" rowspan="1" colspan="1">95.71%</td>
<td align="left" rowspan="1" colspan="1">45.07%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e041">
<math id="M41" display="inline" overflow="linebreak"><mrow><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">Monitor</td>
<td align="left" rowspan="1" colspan="1">-0.02</td>
<td align="left" rowspan="1" colspan="1">[-0.04, 0.01]</td>
<td align="left" rowspan="1" colspan="1">90.73%</td>
<td align="left" rowspan="1" colspan="1">61.86%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e042">
<math id="M42" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">HMD</td>
<td align="left" rowspan="1" colspan="1">9.44<em>e</em><sup>−3</sup>
</td>
<td align="left" rowspan="1" colspan="1">[-0.02, 0.03]</td>
<td align="left" rowspan="1" colspan="1">76.80%</td>
<td align="left" rowspan="1" colspan="1">81.20%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e043">
<math id="M43" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">HMD</td>
<td align="left" rowspan="1" colspan="1">2.77<em>e</em><sup>−3</sup>
</td>
<td align="left" rowspan="1" colspan="1">[-0.02, 0.03]</td>
<td align="left" rowspan="1" colspan="1">50.84%</td>
<td align="left" rowspan="1" colspan="1">93.01%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e044">
<math id="M44" display="inline" overflow="linebreak"><mrow><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">HMD</td>
<td align="left" rowspan="1" colspan="1">-7.33<em>e</em><sup>−3</sup>
</td>
<td align="left" rowspan="1" colspan="1">[-0.03, 0.02]</td>
<td align="left" rowspan="1" colspan="1">72.30%</td>
<td align="left" rowspan="1" colspan="1">86.04%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e045">
<math id="M45" display="inline" overflow="linebreak"><mrow><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">HMD</td>
<td align="left" rowspan="1" colspan="1">-9.14<em>e</em><sup>−3</sup>
</td>
<td align="left" rowspan="1" colspan="1">[-0.03, 0.02]</td>
<td align="left" rowspan="1" colspan="1">77.12%</td>
<td align="left" rowspan="1" colspan="1">82.05%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e046">
<math id="M46" display="inline" overflow="linebreak"><mrow><msup><mn>15</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">HMD</td>
<td align="left" rowspan="1" colspan="1">-0.02</td>
<td align="left" rowspan="1" colspan="1">[-0.04, 0.01]</td>
<td align="left" rowspan="1" colspan="1">91.25%</td>
<td align="left" rowspan="1" colspan="1">61.02%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e047">
<math id="M47" display="inline" overflow="linebreak"><mrow><msup><mn>30</mn><mrow><mo>∘</mo></mrow></msup><mo>−</mo><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math>
</span>
</td>
<td align="left" rowspan="1" colspan="1">HMD</td>
<td align="left" rowspan="1" colspan="1">-7.67<em>e</em><sup>−3</sup>
</td>
<td align="left" rowspan="1" colspan="1">[-0.03, 0.02]</td>
<td align="left" rowspan="1" colspan="1">72.99%</td>
<td align="left" rowspan="1" colspan="1">86.01%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330693.t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="t001fn001"><p>CI = Bayesian credible interval, PD = probability of direction, ROPE = region of practical equivalence</p></div></div></section><p>For <em>RT_mean</em> as the dependent variable, similar to <em>Accuracy</em>, the final model included all the independent variables and an interaction term between the audiovisual offset angle and display device. However, none of the pairwise comparisons were statistically meaningful with PPDs with low PD values and high % in ROPE (see <a href="#pone.0330693.t001" class="usa-link">Table 1</a> in <a href="#pone.0330693.s003" class="usa-link">S3 File</a>), and are not considered further.</p></section><section id="sec013"><h4 class="pmc_sec_title">Questionnaire.</h4>
<p>The full questionnaire data is given in <a href="#pone.0330693.s005" class="usa-link">S5 File</a>. As seen in <a href="#pone.0330693.g005" class="usa-link">Fig 5</a>, for all the questions from the SUS questionnaire, the differences in SD between the HMD and computer monitor display conditions were statistically robust, indicating that the participants reported a higher perceived presence in the HMD condition. This is depicted by the 95% CI of the SD difference between display conditions not crossing the zero line, suggesting a meaningful effect.</p>
<figure class="fig xbox font-sm" id="pone.0330693.g005"><h5 class="obj_head">Fig 5. Questionnaire results of Experiment 1.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373245_pone.0330693.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/56fe/12373245/0511428322c4/pone.0330693.g005.jpg" loading="lazy" height="313" width="675" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330693.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The standard deviation (SD) difference between the HMD and the Monitor display conditions is shown for each question on the y-axis. Error bars indicate the 95% credible intervals. All questions were rated on a scale of 1 to 7, except for Q_Inc, which was asked on a yes/no basis and is thus not displayed here.</p></figcaption></figure><p>For the other questions, the 95% CI crosses the zero line, indicating that the SD differences of the ratings between display devices were not statistically robust. This holds true for the naturalness of the ECA (Q_Nat), the extent to which the ECA was perceived as a sentient being (Q_Sen), the intuitiveness of the controller handling (Q_Con), the perceived difficulty of the task (Q_Dif), and the perceived impact of the incongruence on the participants’ performance (Q_Imp).</p>
<p>Four of the 23 participants reported not noticing the audiovisual angle incongruence (Q_Inc). The change in the log odds of the perceived audiovisual incongruence (Q_Inc) between the monitor and HMD condition (-1.14) was not statistically robust with the 95% CI ([-4.63, 1.74]) crossing zero. This indicates that the perceived noticeability of the incongruence did not differ between the display devices.</p>
<p>Statistically robust SD differences between ratings of the display devices could be detected for the naturalness of speech (Q_Spe), with a higher rating of naturalness in the HMD condition, and in the visual focus (Q_Vis), where participants reported that they focused significantly more on the ECA in the HMD condition.</p></section></section><section id="sec014"><h3 class="pmc_sec_title">Discussion</h3>
<p>The participants’ avVSR performance was evaluated in terms of <em>Accuracy</em> and <em>RT_mean</em> for both HMD and computer monitor presentations. No statistically robust influence of the display device on either metric could be found, which is in accordance with the participants’ self-assessment that the task difficulty was similar for both display devices. This suggests that the avVSR task can be successfully conducted in VR, at least in conditions close to the given setup. However, while the SUS questionnaire data indicated a significantly higher perceived presence in the HMD condition, this increased presence did not translate into changes in cognitive task performance. This finding highlights that higher perceived presence in VR does not necessarily correspond to enhanced performance in cognitive tasks.</p>
<p>Regarding the impact of audiovisual angle incongruence, a marginal increase in recall performance was observed for an offset angle of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e048"><math id="M48" display="inline" overflow="linebreak"><mrow><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> in the computer monitor presentation, albeit with a negligible effect size. This is quite interesting as a contrary effect, i.e., a decrease in performance, could have been expected. However, it has been shown before that more demanding tasks can evoke increased performance [<a href="#pone.0330693.ref057" class="usa-link" aria-describedby="pone.0330693.ref057">57</a>], presumably because participants put in more effort. A similar effect may have occured here, i.e., audiovisual angle incongruence might have had an alerting effect, leading to higher concentration and, thus, improved performance. However, we can only speculate as to why this effect occurred in the computer monitor, but not in the HMD condition. As indicated in the questionnaire (Q_Vis), participants focussed more on the visual signal, the ECA, in the HMD than in the computer monitor condition. It is possible that the visual stimulus dominated over the auditory signal and thus the audiovisual angle incongruence could be ignored better. Furthermore, HMDs influence sound source localization [<a href="#pone.0330693.ref058" class="usa-link" aria-describedby="pone.0330693.ref058">58</a>] and depth perception [<a href="#pone.0330693.ref059" class="usa-link" aria-describedby="pone.0330693.ref059">59</a>]. This could have impacted the perception of the audiovisual angle incongruence as well. Since the effect size was rather small, further investigations are necessary to validate it.</p>
<p>For all other combinations of display device and audiovisual angle incongruence, no effect of incongruence on recall performance and response time was detected. This is consistent with participants’ ratings, which indicated that they felt only marginally influenced by the audiovisual angle incongruence. One possible explanation for the lack of interference is that participants could largely ignore the incongruence, perhaps because the task-relevant information was primarily auditory, with visual information being redundant.</p>
<p>It is worth noting that the audiovisual incongruence could also be interpreted as a visual distractor. There are only very few studies on the impact of visual distractors on verbal serial recall and visual signals have not been shown to impact recall performance. Liebl <em>et al</em>. [<a href="#pone.0330693.ref060" class="usa-link" aria-describedby="pone.0330693.ref060">60</a>] investigated relatively rapid changes in workspace lighting, namely the effects of a traveling beam on a projector screen in the participants’ field of vision, and found no effect on the visual-verbal serial recall performance. In an experiment with flashing color stripes during stimulus presentation, Lange [<a href="#pone.0330693.ref061" class="usa-link" aria-describedby="pone.0330693.ref061">61</a>] found no reduction in performance for visual-verbal serial recall. To the best of the authors’ knowledge, auditory-verbal serial recall has not been investigated with visual distractors yet. However, since the ineffectiveness of visual distractors on verbal serial recall is argued to be due to the separation of auditory-verbal and visual-spatial short-term memory according to the model of Baddeley and Hitch [<a href="#pone.0330693.ref062" class="usa-link" aria-describedby="pone.0330693.ref062">62</a>], it is reasonable to assume that the audiovisual incongruence did not act as a performance-relevant distractor in the present study with auditory reproduction either. In summary, our results suggest that participants are able to ignore audiovisual angle incongruence in avVSR tasks, at least in experimental settings resembling the current.</p>
<p>The ratings regarding the general appearance of the ECA did not differ between display devices in a statistically robust way. Similar effects regarding ECAs can be found in literature. McKeown <em>et al</em>. [<a href="#pone.0330693.ref063" class="usa-link" aria-describedby="pone.0330693.ref063">63</a>] conducted a study where participants rated social interactions presented either on an HMD or a computer monitor. These ratings were not influenced by the display device. In a study on human-ECA interactions, Zojaji <em>et al</em>. [<a href="#pone.0330693.ref064" class="usa-link" aria-describedby="pone.0330693.ref064">64</a>] found no influence on the perceived persuasiveness and offensiveness between display devices. This suggests that the perception of an ECA may not generally be influenced by the display device. Interestingly, in the present study, the ECA’s speech was perceived as more natural in the HMD condition compared to the computer monitor condition, although the exact same material (and in the case of the speech even audio reproduction) was used. This may be linked to the higher presence overall and higher visual focus reported in the HMD condition.</p>
<p>The experiment had several limitations for both display devices. Firstly, with the employed animation technique (computer-generated lip movement, no blinking, and only idle movement), the subjective evaluation of the naturalness of the ECA was generally low. Based on the general comments, the participants found the ECA’s eye movement, or rather the lack thereof, particularly unnatural, which provided an opportunity for improvement. Secondly, regarding the choice of incongruence, the behavioral results indicate that participants were able to ignore it. According to the questionnaire, a total of 13% of participants did not notice the audiovisual angle incongruence at all. It is possible that the selected angle differences were insufficiently large, and that only the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e049"><math id="M49" display="inline" overflow="linebreak"><mrow><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> condition entered the range of disruptive incongruences. We derived the selection of angles from ventriloquism experiments, in which participants focus on the spatial characteristics of the stimuli since their task is to localize them. In contrast, the perception of spatial direction was incidental in our study, as subjects were instructed to focus on the content, specifically the to-be-remembered digits. This could have shifted the threshold for detection, and/or the need to attend to incongruences. Also, participants were likely experienced in integrating audiovisual angle incongruences. For example, when watching a movie, no directional discrepancy between audio and video is perceived actively, even though loudspeakers are usually placed beside the screen. It remains unclear if the participants were able to ignore the incongruence due to the small angle range and the familiarity of the situation - or if incongruent audiovisual stimuli can be ignored in general if they are not task-relevant.</p></section></section><section id="sec015"><h2 class="pmc_sec_title">Experiment 2: Audiovisual semantic incongruence</h2>
<p>A limitation of Experiment 1 was that the spatial incongruence went unnoticed by many participants, potentially confounding the findings on audiovisual incongruences in VR. Moreover, the chosen angle incongruence made it challenging to distinguish between the transition from congruent to incongruent cases, as the angle is a continuous variable with no clear threshold for noticeability. To address this issue, a clearly noticeable audiovisual incongruence was considered in Experiment 2. One potential approach to ensure perceptibility is a pre-test with each participant to determine individual angular thresholds of detectibility, similar to the study by Kim and Lee [<a href="#pone.0330693.ref020" class="usa-link" aria-describedby="pone.0330693.ref020">20</a>]. However, even with this refined approach, we could not be sure that the participants would interpret the spatial incongruence as such. As discussed previously, most people naturally integrate spatial incongruent audiovisual stimuli, especially in entertainment applications. Therefore, we wanted to use a binary and more easily distinguishable incongruence which allows for clearly comparing congruent with incongruent cases.</p>
<p>For this purpose, in Experiment 2, a voice swap, i.e., two ECAs speaking with each others’ voices, was introduced. To allow a clear distinction between the voices, a swap between a male and female ECA was chosen. A voice swap can be considered as a special case of semantic incongruence because it disrupts the expected auditory-visual correspondence, leading to a mismatch between the perceived voice and the visual identity or context. This <em>audiovisual voice incongruence</em> was investigated in both HMD and computer monitor presentations. An overview over the experimental design can be found in <a href="#pone.0330693.g006" class="usa-link">Fig 6</a>. Compared to Experiment 1, the position of the ECA was not changed in this experiment, and hence no spatial incongruence was included.</p>
<figure class="fig xbox font-sm" id="pone.0330693.g006"><h3 class="obj_head">Fig 6. Factors in Experiment 2.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/56fe/12373245/56c38daa598d/pone.0330693.g006.jpg" loading="lazy" height="733" width="733" alt="Fig 6"></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330693.g006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The between-subject experimental design of the avVSR task with three factors: display device (computer monitor vs. HMD), the audiovisual voice incongruence (congruent vs. incongruent), and the serial position (1 – 8) of the target digit sequence. In the congruent case, the ECA speaks with their own voice; in the incongruent case the female ECA speaks with the male ECAs’ voice and vice versa (here shown for the female ECA).</p></figcaption></figure><section id="sec016"><h3 class="pmc_sec_title">Method</h3>
<section id="sec017"><h4 class="pmc_sec_title">Participants.</h4>
<p>The recruitment procedure, screening, ethics approval, consent form, and compensation were the same as in Experiment 1. Participants were recruited between 30th of November and 23rd of December 2022. <em>N</em> = 26 adults (16 male, 10 female) aged between 21 and 39 years (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e050"><math id="M50" display="inline" overflow="linebreak"><mrow><mi>M</mi><mo>=</mo><mn>28.08</mn><mo>,</mo><mspace width="0.167em"></mspace><mi>S</mi><mi>D</mi><mo>=</mo><mn>4.39</mn></mrow></math></span>) participated in Experiment 2. It was not an exclusion criterion if participants had already participated in the first study.</p></section><section id="sec018"><h4 class="pmc_sec_title">Cognitive task.</h4>
<p>The paradigm for the cognitive task was the same as in Experiment 1: an avVSR task in which digit sequences were played back auditorily and an ECA was animated to speak the sentences (see <a href="#pone.0330693.g002" class="usa-link">Fig 2</a>).</p></section><section id="sec019"><h4 class="pmc_sec_title">Audiovisual voice incongruence.</h4>
<p>A voice swap was implemented as the audiovisual incongruence. At the beginning of the experiment, two ECAs one female and one male (see <a href="#pone.0330693.g007" class="usa-link">Fig 7</a>), were introduced with their respective voices. In the experiment, they unexpectedly spoke with the voice of the other ECA, thus, swapping voices for an audiovisual incongruence (male-female, female-male). Voice swaps have been used in cognitive research before. Walker-Andres <em>et al</em>. [<a href="#pone.0330693.ref065" class="usa-link" aria-describedby="pone.0330693.ref065">65</a>] found that infants as young as six months prefer to look at audiovisual stimuli with congruent gender compared to incongruent gender. In an experiment of just noticeable onset asynchronies of audiovisual stimuli, Vataksi and Spence [<a href="#pone.0330693.ref066" class="usa-link" aria-describedby="pone.0330693.ref066">66</a>] found a higher sensitivity for gender-incongruent stimuli. They suggested that a incongruence between the auditory and visual stimuli in terms of gender leads to them not being perceived as a unit. A study by Szerszen [<a href="#pone.0330693.ref067" class="usa-link" aria-describedby="pone.0330693.ref067">67</a>] indicated that a gender incongruence in the voice can lead to an uncanny valley effect. In this experiment, the position of both the auditory and visual sources was kept constant at an azimuth angle of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e051"><math id="M51" display="inline" overflow="linebreak"><mrow><mi>φ</mi><mo>=</mo><msup><mn>0</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> on the horizontal plane to avoid the additional effect of spatial incongruence.</p>
<figure class="fig xbox font-sm" id="pone.0330693.g007"><h5 class="obj_head">Fig 7. ECAs used in Experiment 2.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373245_pone.0330693.g007.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/56fe/12373245/6d82dd863d57/pone.0330693.g007.jpg" loading="lazy" height="403" width="776" alt="Fig 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330693.g007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><em>Left:</em> The female ECA was the same as in Experiment 1. <em>Right:</em> A male ECA was created for Experiment 2.</p></figcaption></figure></section><section id="sec020"><h4 class="pmc_sec_title">Apparatus and materials.</h4>
<p>The same equipment and software as in Experiment 1 were used. A second, male, ECA was created. Since participants reported that the gaze of the ECA seemed unnatural in Experiment 1, an idle eye movement was added for both ECAs using the VHGazeComponent of the Character Plugin in Speech Mode (for more information see [<a href="#pone.0330693.ref068" class="usa-link" aria-describedby="pone.0330693.ref068">68</a>]). The male voice stimuli were taken from [<a href="#pone.0330693.ref040" class="usa-link" aria-describedby="pone.0330693.ref040">40</a>] (voice <em>male a</em>).</p></section><section id="sec021"><h4 class="pmc_sec_title">Questionnaires.</h4>
<p>The questionnaires remained unchanged from Experiment 1.</p></section><section id="sec022"><h4 class="pmc_sec_title">Procedure.</h4>
<p>The procedure was mainly the same as in Experiment 1. The training block consisted of six trials (three with the female ECA, three with the male ECA) with matching voices. This way, participants could familiarize themselves with the task and the voice matches. In the main experiment, twelve trials were presented for all four possible combinations of the ECAs and their voices, resulting in <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e052"><math id="M52" display="inline" overflow="linebreak"><mrow><mn>12</mn><mspace width="0.167em"></mspace><mrow><mi>×</mi></mrow><mspace width="0.167em"></mspace><mn>4</mn><mo>=</mo><mn>48</mn></mrow></math></span> trials per display device, divided into four blocks of twelve trials each. The order of the visual ECA and voice combinations was counterbalanced across participants in each block.</p></section><section id="sec023"><h4 class="pmc_sec_title">Statistical analysis.</h4>
<p>The procedure for the statistical analysis was identical to that in Experiment 1 except for two changes. First, audiovisual voice incongruence was introduced as a binary independent variable. The other two independent variables were the same as in Experiment 1 (display device and serial position), with the dependent variables also being the same (<em>Accuracy</em> and <em>RT_mean</em>). Secondly, the priors for display device and serial position were set based on the respective PPD from models in Experiment 1.</p></section></section><section id="sec024"><h3 class="pmc_sec_title">Results</h3>
<section id="sec025"><h4 class="pmc_sec_title">Audiovisual verbal serial recall.</h4>
<p>The full result data of Experiment 2 is given in <a href="#pone.0330693.s002" class="usa-link">S2 File</a>, plots of the data can be found in <a href="#pone.0330693.s003" class="usa-link">S3 File</a>. Similar to Experiment 1, the leave-one-out criteria was used to determine the final models. For <em>Accuracy</em> as the dependent variable, the model with all the IVs and an interaction term between the audiovisual voice incongruence and display device was selected as the final model since it encompasses the experimental design most comprehensively. The pairwise comparisons for this model are presented in <a href="#pone.0330693.t002" class="usa-link">Table 2</a>. While the PD for the Monitor condition was greater than 97.5% (equivalent to a <em>p</em>-value of less than .05), suggesting better performance for incongruent compared to congruent trials, neither of the comparisons here was statistically meaningful with the % of the PPD within ROPE being very high.</p>
<section class="tw xbox font-sm" id="pone.0330693.t002"><h5 class="obj_head">Table 2. Summary of pairwise comparisons between the voice incongruence (Match) and display Device combinations with <em>Accuracy</em> as the dependent variable.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead>
<tr>
<th align="left" colspan="2" rowspan="1">Comparison</th>
<th align="left" rowspan="2" colspan="1">Median</th>
<th align="left" rowspan="2" colspan="1">95% CI</th>
<th align="left" rowspan="2" colspan="1">PD</th>
<th align="left" rowspan="2" colspan="1">% in ROPE</th>
</tr>
<tr>
<th align="left" rowspan="1" colspan="1">Match</th>
<th align="left" rowspan="1" colspan="1">Device</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Incong.–Cong.</td>
<td align="left" rowspan="1" colspan="1">Monitor</td>
<td align="left" rowspan="1" colspan="1">0.02</td>
<td align="left" rowspan="1" colspan="1">[ 0.00, 0.04]</td>
<td align="left" rowspan="1" colspan="1">97.71%</td>
<td align="left" rowspan="1" colspan="1">58.72%</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Incong.–Cong.</td>
<td align="left" rowspan="1" colspan="1">HMD</td>
<td align="left" rowspan="1" colspan="1">0.01</td>
<td align="left" rowspan="1" colspan="1">[ 0.00, 0.03]</td>
<td align="left" rowspan="1" colspan="1">94.80%</td>
<td align="left" rowspan="1" colspan="1">72.24%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330693.t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="t002fn001"><p>CI = Bayesian credible interval, PD = probability of direction, ROPE = Region of practical equivalence, Incong. = Incongruent, Cong. = Congruent</p></div></div></section><p>For <em>RT_mean</em> as the DV, the final model also included all the independent variables and an interaction term between audiovisual voice incongruence and display device. However, as in Experiment 1 and for <em>Accuracy</em> as the dependent variable in this experiment, none of the pairwise comparisons were statistically meaningful (see Table 2 in <a href="#pone.0330693.s003" class="usa-link">S3 File</a>), and are not considered further.</p></section><section id="sec026"><h4 class="pmc_sec_title">Questionnaire.</h4>
<p>All questionnaire data is given in <a href="#pone.0330693.s005" class="usa-link">S5 File</a>. The results of the questionnaire analysis are displayed in <a href="#pone.0330693.g008" class="usa-link">Fig 8</a>. As in Experiment 1, the SD differences between the ratings on the SUS questionnaire for the two display conditions were statistically robust (95% CI does not include zero), with the participants reporting higher presence in the HMD vs. the computer monitor condition. The ECAs’ naturalness (Q_Nat) was rated higher in the HMD condition.</p>
<figure class="fig xbox font-sm" id="pone.0330693.g008"><h5 class="obj_head">Fig 8. Questionnaire results of Experiment 2.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373245_pone.0330693.g008.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/56fe/12373245/9955fb2ef239/pone.0330693.g008.jpg" loading="lazy" height="337" width="675" alt="Fig 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330693.g008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The standard deviation (SD) difference between the HMD and the Monitor display conditions is shown for each question on the y-axis. Error bars indicate the 95% credible intervals. All questions were rated on a scale of 1 to 7, excluding Q_Inc, which was asked on a yes/no basis and is thus not displayed here.</p></figcaption></figure><p>For the remaining questions, the SD differences between ratings for the conditions were not significantly robust. Only one participant reported not noticing the voice incongruence (Q_Inc). However, the change in the log odds of the perceivability of the voice incongruence (Q_Inc) between the monitor and HMD condition (–0.01) was not statistically robust with the 95% CI ([–4.01, 3.99]) crossing zero. This suggests that the perception of incongruence did not vary across the devices.</p>
<p>Finally, the questionnaire results from Experiment 1 were compared with the results from Experiment 2 to see if the changes in the experimental design (changing the type of audiovisual incongruence, adding eye movements to the ECA animation) had any effect. To that end, the subjective impact of the two types of audiovisual incongruences (Q_Imp) and the three questions related to the ECA (Q_Nat, Q_Spe, Q_Sen) were compared between Experiment 1 and Experiment 2 using the same analysis procedure as for the individual questionnaires with the experiment as the dependent variable. The results are displayed in <a href="#pone.0330693.g009" class="usa-link">Fig 9</a>. Indeed, statistically relevant differences in terms of perceived naturalness (Q_Nat) and sentience (Q_Sen) were found between Experiments 1 and 2, with higher ratings in Experiment 2. Differences in the naturalness of speech (Q_Spe) and the perceived impact of the incongruence (Q_Imp) were not statistically robust.</p>
<figure class="fig xbox font-sm" id="pone.0330693.g009"><h5 class="obj_head">Fig 9. Standard deviation (SD) difference of questionnaire data between Experiment 1 and 2.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373245_pone.0330693.g009.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/56fe/12373245/161851d9c0df/pone.0330693.g009.jpg" loading="lazy" height="132" width="675" alt="Fig 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330693.g009/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Results are shown for the questions Q_Nat, Q_Sen, Q_Spe, and Q_Imp (on the y-axis). Error bars indicate the 95% credible intervals. All questions (see section <em>Questionnaire</em>) were rated on a scale of 1 to 7.</p></figcaption></figure></section></section><section id="sec027"><h3 class="pmc_sec_title">Discussion</h3>
<p>In Experiment 2, task performance in terms of <em>Accuracy</em> and <em>RT_mean</em> in the serial recall were examined both for HMD and computer monitor presentations. As in Experiment 1, no meaningful influence of the display device on either performance metrics could be found. This again suggests that the avVSR task can be successfully conducted in VR, at least in conditions close to the given setup.</p>
<p>In this experiment, we used a more obvious and binary audiovisual distractor by introducing an audiovisual voice incongruence as a type of semantic incongruence. This reduced the number of participants who failed to notice the incongruence from 13 In Experiment 1, where audiovisual angle incongruence was investigated, a small increase in recall performance could be found for the audiovisual offset angle of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330693.e053"><math id="M53" display="inline" overflow="linebreak"><mrow><msup><mn>60</mn><mrow><mo>∘</mo></mrow></msup></mrow></math></span> in the computer monitor presentation. We hypothesized this to be due to the alerting effect of the incongruence. In the current experiment employing the audiovisual voice incongruence, again, the <em>Accuracy</em> was slightly increased in trials with incongruent voice and the computer monitor as display device. However, this finding was not statistically robust. Even though the type of incongruence was fundamentally different between the experiments, it is reasonable to argue that <em>Accuracy</em> is not affected by these incongruences in a statistically relevant way. In other words, the cognitive performance in the avVSR task seems robust against the two incongruences that were tested. While our findings contribute to the understanding of the complex mechanisms underlying the processing of audiovisual stimuli, further research is needed to comprehensively explore the intricate interactions and neural pathways involved.</p>
<p>As in Experiment 1, increased presence could be detected in Experiment 2 for the HMD presentation in the questionnaires. In Experiment 1, the naturalness of speech of the ECA was rated higher in the HMD condition and the ratings of the general naturalness of the ECA was not affected by the display device. This was the other way around in Experiment 2, where the ratings of the naturalness of speech were not affected by the display device, but the general naturalness of the ECA was rated higher in the HMD condition, even though the same lip animation algorithm and speech material were used. This indicates that while the display device does influence the perception of an ECA, the specific nature and nuances of this influence remain unclear. The improved animation of the ECA led to better ratings with regard to naturalness and the degree to which the ECA is perceived as a sentient being compared to Experiment 1. Adding even more animations to the ECA, like gestures and plausible mimics, could further increase perceived naturalness. It should be noted that participants’ ratings were based on their perception of the scene while performing the memory task, which may not be generalizable to other contexts. Still, since differences in the display device became apparent only in the questionnaire and not in the performance data, our study highlights the need for a multidimensional evaluation framework when assessing VR environments.</p></section></section><section id="sec028"><h2 class="pmc_sec_title">Limitations</h2>
<p>It is important to note that both the experiments were conducted under relatively simple conditions - specifically, a minimalistic and static laboratory scene featuring only the conversing ECAs as an additional element. Coherent audiovisual cues are likely to play a more critical role when focusing on a target source becomes more difficult in more complex and busier scenes. To gain further insights into how incongruences affect participants, future research should consider increasing the complexity of the scenes by introducing acoustic distractors (such as background noise) and visual distractors (like additional embodied conversational agents or moving scene objects) that bear increasing resemblance to real-life situations. In a busier scene with multiple distractions, participants may find it increasingly challenging to extract and attend to the voice of an ECA if it does not match its visual representation (cp. Experiment 2) or if it is perceived from a different direction (cp. Experiment 1). Another limitation was that no a-priori power calculations were conducted, and the sample size may be considered relatively small. The Bayesian analysis used here can be robust against smaller sample sizes when the priors are informative and the model is specified correctly [<a href="#pone.0330693.ref069" class="usa-link" aria-describedby="pone.0330693.ref069">69</a>]. The priors used here were, however, weakly/mildly informative since effect sizes from previous studies (not compatible enough with the current experimental design) could not be incorporated within informative priors. Hence, arguably larger sample sizes may be necessary in future studies that can at least benefit from the findings here.</p></section><section id="sec029"><h2 class="pmc_sec_title">Conclusion</h2>
<p>Two experiments were conducted to investigate the influence of audiovisual spatial (angle) and semantical (voice) incongruence, simulated over two display devices (HMD and computer monitor), on avVSR performance and subjective ratings. With regard to audiovisual incongruences, no clear effect on the porportion of correctly recalled items (<em>Accuracy</em>) or response times (<em>RT_mean</em>) could be detected. Some tendencies of increased performance were observed within the computer monitor presentation for the more extreme angle incongruences. The only difference between display devices could be found in the SUS questionnaire, which consistently revealed higher presence when using an HMD compared to a computer monitor in both experiments. Ratings of the naturalness and sentience of the ECAs did not change with the display device.</p>
<p>So far, research on audiovisual incongruence has mostly focused on the perception level, whereas this paper investigated its relevance for verbal short-term memory, which has not been studied previously. Generally speaking, participants were able to ignore the types of audiovisual incongruences presented in the avVSR task. Further research is needed to explore the relation between perceptibility and task impact in more complex scenes, which could provide further insights into the role of audiovisual congruence in cognitive processing and VR design.</p></section><section id="sec030"><h2 class="pmc_sec_title">Supporting information</h2>
<section class="sm xbox font-sm" id="pone.0330693.s001"><div class="caption p">
<span>S1 File. Data of Experiment 1.</span><p><em>Accuracy</em> and <em>RT_mean</em> for each condition and serial position in Experiment 1 (audiovisual angle incongruence).</p>
<p>(CSV)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s001.CSV" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s001.CSV</a><sup> (54.6KB, CSV) </sup>
</div></div></section><section class="sm xbox font-sm" id="pone.0330693.s002"><div class="caption p">
<span>S2 File. Data of Experiment 2.</span><p><em>Accuracy</em> and <em>RT_mean</em> for each condition and serial position in Experiment 2 (audiovisual voice incongruence).</p>
<p>(CSV)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s002.CSV" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s002.CSV</a><sup> (36.5KB, CSV) </sup>
</div></div></section><section class="sm xbox font-sm" id="pone.0330693.s003"><div class="caption p">
<span>S3 File. Data plots and analysis of RT_mean Experiment 1 and 2.</span><p>Plots of <em>Accuracy</em> and <em>RT_mean</em> as well as results of the statistical analysis of <em>RT_mean</em> for Experiment 1 and 2.</p>
<p>(PDF)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s003.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s003.pdf</a><sup> (1.2MB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="pone.0330693.s004"><div class="caption p">
<span>S4 File. Questionnaire.</span><p>Questions asked in the questionnaires (except for SUS questions) with corresponding anchors.</p>
<p>(TXT)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s004.txt" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s004.txt</a><sup> (2.9KB, txt) </sup>
</div></div></section><section class="sm xbox font-sm" id="pone.0330693.s005"><div class="caption p">
<span>S5 File. Questionnaire Data Experiment 1 and 2.</span><p>Ratings for all questions of the questionnaire for Experiments 1 and 2.</p>
<p>(CSV)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s005.CSV" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s005.CSV</a><sup> (6.9KB, CSV) </sup>
</div></div></section></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgments</h2>
<p>The authors would like to thank Jamilla Balint, Carolin Breuer, Karin Loh, Lukas Vollmer, and Julia Seitz for the fruitful discussions, and Alissa Wenzel for assisting with the experiment conduction. The following plugins by the Virtual Reality &amp; Immersive Visualization Group at RWTH Aachen University were used to implement this project:</p>
<ul class="list" style="list-style-type:disc">
<li><p>RWTH VR Toolkit (for basic VR interaction) <a href="https://git-ce.rwth-aachen.de/vr-vis/VR-Group/unreal-development/plugins/rwth-vr-toolkit" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://git-ce.rwth-aachen.de/vr-vis/VR-Group/unreal-development/plugins/rwth-vr-toolkit</a>, see also Gilbert <em>et al</em>. [<a href="#pone.0330693.ref036" class="usa-link" aria-describedby="pone.0330693.ref036">36</a>]</p></li>
<li><p>Character Plugin (Animation of MetaHuman Lip Movement and Eye Gazing) <a href="https://git-ce.rwth-aachen.de/vr-vis/VR-Group/unreal-development/plugins/character-plugin" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://git-ce.rwth-aachen.de/vr-vis/VR-Group/unreal-development/plugins/character-plugin</a></p></li>
<li><p>Study Framework Plugin (Control of listening experiment, logging of data) <a href="https://git-ce.rwth-aachen.de/vr-vis/VR-Group/unreal-development/plugins/unreal-study-framework" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://git-ce.rwth-aachen.de/vr-vis/VR-Group/unreal-development/plugins/unreal-study-framework</a>, see also Ehret <em>et al</em>. [<a href="#pone.0330693.ref035" class="usa-link" aria-describedby="pone.0330693.ref035">35</a>]</p></li>
<li><p>Virtual Acoustics Plugin (Communication with the Virtual Acoustics server) <a href="https://git-ce.rwth-aachen.de/vr-vis/VR-Group/unreal-development/plugins/unreal-va-plugin" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://git-ce.rwth-aachen.de/vr-vis/VR-Group/unreal-development/plugins/unreal-va-plugin</a>, see also Schäfer <em>et al</em>. [<a href="#pone.0330693.ref041" class="usa-link" aria-describedby="pone.0330693.ref041">41</a>]</p></li>
</ul></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>All relevant data are within the manuscript and its <a href="#sec030" class="usa-link">Supporting information</a> files.</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation, <a href="https://www.dfg.de/de" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.dfg.de/de</a>): SPP2236 - Project number 444724862. Manuj Yadav was supported by the DFG: Project number 503914237. Sabine Schlittmeier’s contribution to fundraising was supported by a grant from the HEAD-Genuit-Stiftung (Head-Genuit Foundation, <a href="https://head-genuit-stiftung.de/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://head-genuit-stiftung.de/</a>; P-16/10-W). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0330693.ref001">
<span class="label">1.</span><cite>Parsons TD. Virtual reality for enhanced ecological validity and experimental control in the clinical, affective and social neurosciences. Front Hum Neurosci. 2015;9:660. doi: 10.3389/fnhum.2015.00660

</cite> [<a href="https://doi.org/10.3389/fnhum.2015.00660" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4675850/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26696869/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Hum%20Neurosci&amp;title=Virtual%20reality%20for%20enhanced%20ecological%20validity%20and%20experimental%20control%20in%20the%20clinical,%20affective%20and%20social%20neurosciences&amp;author=TD%20Parsons&amp;volume=9&amp;publication_year=2015&amp;pages=660&amp;pmid=26696869&amp;doi=10.3389/fnhum.2015.00660&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref002">
<span class="label">2.</span><cite>Pieri L, Tosi G, Romano D. Virtual reality technology in neuropsychological testing: a systematic review. J Neuropsychol. 2023;17(2):382–99. doi: 10.1111/jnp.12304

</cite> [<a href="https://doi.org/10.1111/jnp.12304" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36624041/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Neuropsychol&amp;title=Virtual%20reality%20technology%20in%20neuropsychological%20testing:%20a%20systematic%20review&amp;author=L%20Pieri&amp;author=G%20Tosi&amp;author=D%20Romano&amp;volume=17&amp;issue=2&amp;publication_year=2023&amp;pages=382-99&amp;pmid=36624041&amp;doi=10.1111/jnp.12304&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref003">
<span class="label">3.</span><cite>Wilkinson M, Brantley S, Feng J. A mini review of presence and immersion in virtual reality. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2021;65(1):1099–103. 10.1177/1071181321651148</cite> [<a href="https://doi.org/10.1177/1071181321651148" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref004">
<span class="label">4.</span><cite>Gorini A, Capideville CS, De Leo G, Mantovani F, Riva G. The role of immersion and narrative in mediated presence: the virtual hospital experience. Cyberpsychol Behav Soc Netw. 2011;14(3):99–105. doi: 10.1089/cyber.2010.0100

</cite> [<a href="https://doi.org/10.1089/cyber.2010.0100" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20649451/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cyberpsychol%20Behav%20Soc%20Netw&amp;title=The%20role%20of%20immersion%20and%20narrative%20in%20mediated%20presence:%20the%20virtual%20hospital%20experience&amp;author=A%20Gorini&amp;author=CS%20Capideville&amp;author=G%20De%20Leo&amp;author=F%20Mantovani&amp;author=G%20Riva&amp;volume=14&amp;issue=3&amp;publication_year=2011&amp;pages=99-105&amp;pmid=20649451&amp;doi=10.1089/cyber.2010.0100&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref005">
<span class="label">5.</span><cite>Li G, Anguera JA, Javed SV, Khan MA, Wang G, Gazzaley A. Enhanced attention using head-mounted virtual reality. J Cogn Neurosci. 2020;32(8):1438–54. doi: 10.1162/jocn_a_01560

</cite> [<a href="https://doi.org/10.1162/jocn_a_01560" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32286132/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Cogn%20Neurosci&amp;title=Enhanced%20attention%20using%20head-mounted%20virtual%20reality&amp;author=G%20Li&amp;author=JA%20Anguera&amp;author=SV%20Javed&amp;author=MA%20Khan&amp;author=G%20Wang&amp;volume=32&amp;issue=8&amp;publication_year=2020&amp;pages=1438-54&amp;pmid=32286132&amp;doi=10.1162/jocn_a_01560&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref006">
<span class="label">6.</span><cite>Breuer C, Loh K, Leist L, Fremerey S, Raake A, Klatte M, et al. Examining the auditory selective attention switch in a child-suited virtual reality classroom environment. Int J Environ Res Public Health. 2022;19(24):16569. doi: 10.3390/ijerph192416569

</cite> [<a href="https://doi.org/10.3390/ijerph192416569" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9779209/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36554463/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Environ%20Res%20Public%20Health&amp;title=Examining%20the%20auditory%20selective%20attention%20switch%20in%20a%20child-suited%20virtual%20reality%20classroom%20environment&amp;author=C%20Breuer&amp;author=K%20Loh&amp;author=L%20Leist&amp;author=S%20Fremerey&amp;author=A%20Raake&amp;volume=19&amp;issue=24&amp;publication_year=2022&amp;pages=16569&amp;pmid=36554463&amp;doi=10.3390/ijerph192416569&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref007">
<span class="label">7.</span><cite>Wenk N, Penalver-Andres J, Buetler KA, Nef T, Müri RM, Marchal-Crespo L. Effect of immersive visualization technologies on cognitive load, motivation, usability, and embodiment. Virtual Real. 2023;27(1):307–31. doi: 10.1007/s10055-021-00565-8

</cite> [<a href="https://doi.org/10.1007/s10055-021-00565-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9998603/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36915633/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Virtual%20Real&amp;title=Effect%20of%20immersive%20visualization%20technologies%20on%20cognitive%20load,%20motivation,%20usability,%20and%20embodiment&amp;author=N%20Wenk&amp;author=J%20Penalver-Andres&amp;author=KA%20Buetler&amp;author=T%20Nef&amp;author=RM%20M%C3%BCri&amp;volume=27&amp;issue=1&amp;publication_year=2023&amp;pages=307-31&amp;pmid=36915633&amp;doi=10.1007/s10055-021-00565-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref008">
<span class="label">8.</span><cite>Huang W, Roscoe RD, Johnson-Glenberg MC, Craig SD. Motivation, engagement, and performance across multiple virtual reality sessions and levels of immersion. Computer Assisted Learning. 2020;37(3):745–58. doi: 10.1111/jcal.12520</cite> [<a href="https://doi.org/10.1111/jcal.12520" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Computer%20Assisted%20Learning&amp;title=Motivation,%20engagement,%20and%20performance%20across%20multiple%20virtual%20reality%20sessions%20and%20levels%20of%20immersion&amp;author=W%20Huang&amp;author=RD%20Roscoe&amp;author=MC%20Johnson-Glenberg&amp;author=SD%20Craig&amp;volume=37&amp;issue=3&amp;publication_year=2020&amp;pages=745-58&amp;doi=10.1111/jcal.12520&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref009">
<span class="label">9.</span><cite>Weech S, Kenny S, Barnett-Cowan M. Presence and cybersickness in virtual reality are negatively related: a review. Front Psychol. 2019;10:158. doi: 10.3389/fpsyg.2019.00158

</cite> [<a href="https://doi.org/10.3389/fpsyg.2019.00158" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6369189/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30778320/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Psychol&amp;title=Presence%20and%20cybersickness%20in%20virtual%20reality%20are%20negatively%20related:%20a%20review&amp;author=S%20Weech&amp;author=S%20Kenny&amp;author=M%20Barnett-Cowan&amp;volume=10&amp;publication_year=2019&amp;pages=158&amp;pmid=30778320&amp;doi=10.3389/fpsyg.2019.00158&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref010">
<span class="label">10.</span><cite>Magosso E, De Crescenzio F, Ricci G, Piastra S, Ursino M. EEG alpha power is modulated by attentional changes during cognitive tasks and virtual reality immersion. Comput Intell Neurosci. 2019;2019:7051079. doi: 10.1155/2019/7051079

</cite> [<a href="https://doi.org/10.1155/2019/7051079" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6614966/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31341468/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Intell%20Neurosci&amp;title=EEG%20alpha%20power%20is%20modulated%20by%20attentional%20changes%20during%20cognitive%20tasks%20and%20virtual%20reality%20immersion&amp;author=E%20Magosso&amp;author=F%20De%20Crescenzio&amp;author=G%20Ricci&amp;author=S%20Piastra&amp;author=M%20Ursino&amp;volume=2019&amp;publication_year=2019&amp;pages=7051079&amp;pmid=31341468&amp;doi=10.1155/2019/7051079&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref011">
<span class="label">11.</span><cite>Redlinger E, Glas B, Rong Y. Enhanced cognitive training using virtual reality: examining a memory task modified for use in virtual environments. In: 2021 5th International Conference on Artificial Intelligence and Virtual Reality (AIVR). 2021. p. 1–8. 10.1145/3480433.3480435</cite> [<a href="https://doi.org/10.1145/3480433.3480435" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref012">
<span class="label">12.</span><cite>Spence C. Audiovisual multisensory integration. Acoust Sci &amp; Tech. 2007;28(2):61–70. doi: 10.1250/ast.28.61</cite> [<a href="https://doi.org/10.1250/ast.28.61" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Acoust%20Sci%20&amp;%20Tech&amp;title=Audiovisual%20multisensory%20integration&amp;author=C%20Spence&amp;volume=28&amp;issue=2&amp;publication_year=2007&amp;pages=61-70&amp;doi=10.1250/ast.28.61&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref013">
<span class="label">13.</span><cite>Boyce WP, Lindsay A, Zgonnikov A, Rañó I, Wong-Lin K. Optimality and limitations of audio-visual integration for cognitive systems. Front Robot AI. 2020;7:94. doi: 10.3389/frobt.2020.00094

</cite> [<a href="https://doi.org/10.3389/frobt.2020.00094" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7805627/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33501261/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Robot%20AI&amp;title=Optimality%20and%20limitations%20of%20audio-visual%20integration%20for%20cognitive%20systems&amp;author=WP%20Boyce&amp;author=A%20Lindsay&amp;author=A%20Zgonnikov&amp;author=I%20Ra%C3%B1%C3%B3&amp;author=K%20Wong-Lin&amp;volume=7&amp;publication_year=2020&amp;pages=94&amp;pmid=33501261&amp;doi=10.3389/frobt.2020.00094&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref014">
<span class="label">14.</span><cite>Chen L, Vroomen J. Intersensory binding across space and time: a tutorial review. Atten Percept Psychophys. 2013;75(5):790–811. doi: 10.3758/s13414-013-0475-4

</cite> [<a href="https://doi.org/10.3758/s13414-013-0475-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23709064/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Atten%20Percept%20Psychophys&amp;title=Intersensory%20binding%20across%20space%20and%20time:%20a%20tutorial%20review&amp;author=L%20Chen&amp;author=J%20Vroomen&amp;volume=75&amp;issue=5&amp;publication_year=2013&amp;pages=790-811&amp;pmid=23709064&amp;doi=10.3758/s13414-013-0475-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref015">
<span class="label">15.</span><cite>Thurlow WR, Jack CE. Certain determinants of the “ventriloquism effect”. Percept Mot Skills. 1973;36(3):1171–84. doi: 10.2466/pms.1973.36.3c.1171

</cite> [<a href="https://doi.org/10.2466/pms.1973.36.3c.1171" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/4711968/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Percept%20Mot%20Skills&amp;title=Certain%20determinants%20of%20the%20%E2%80%9Cventriloquism%20effect%E2%80%9D&amp;author=WR%20Thurlow&amp;author=CE%20Jack&amp;volume=36&amp;issue=3&amp;publication_year=1973&amp;pages=1171-84&amp;pmid=4711968&amp;doi=10.2466/pms.1973.36.3c.1171&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref016">
<span class="label">16.</span><cite>Alais D, Burr D. The ventriloquist effect results from near-optimal bimodal integration. Curr Biol. 2004;14(3):257–62. doi: 10.1016/j.cub.2004.01.029

</cite> [<a href="https://doi.org/10.1016/j.cub.2004.01.029" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/14761661/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Curr%20Biol&amp;title=The%20ventriloquist%20effect%20results%20from%20near-optimal%20bimodal%20integration&amp;author=D%20Alais&amp;author=D%20Burr&amp;volume=14&amp;issue=3&amp;publication_year=2004&amp;pages=257-62&amp;pmid=14761661&amp;doi=10.1016/j.cub.2004.01.029&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref017">
<span class="label">17.</span><cite>Stroop JR. Studies of interference in serial verbal reactions. Journal of Experimental Psychology. 1935;18(6):643–62. doi: 10.1037/h0054651</cite> [<a href="https://doi.org/10.1037/h0054651" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Experimental%20Psychology&amp;title=Studies%20of%20interference%20in%20serial%20verbal%20reactions&amp;author=JR%20Stroop&amp;volume=18&amp;issue=6&amp;publication_year=1935&amp;pages=643-62&amp;doi=10.1037/h0054651&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref018">
<span class="label">18.</span><cite>McGurk H, MacDonald J. Hearing lips and seeing voices. Nature. 1976;264(5588):746–8. doi: 10.1038/264746a0

</cite> [<a href="https://doi.org/10.1038/264746a0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/1012311/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nature&amp;title=Hearing%20lips%20and%20seeing%20voices&amp;author=H%20McGurk&amp;author=J%20MacDonald&amp;volume=264&amp;issue=5588&amp;publication_year=1976&amp;pages=746-8&amp;pmid=1012311&amp;doi=10.1038/264746a0&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref019">
<span class="label">19.</span><cite>Huisman T, Dau T, Piechowiak T, MacDonald E. The ventriloquist effect is not consistently affected by stimulus realism. JPI. 2022;5(0):000404.doi: 10.2352/j.percept.imaging.2022.5.000404</cite> [<a href="https://doi.org/10.2352/j.percept.imaging.2022.5.000404" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=JPI&amp;title=The%20ventriloquist%20effect%20is%20not%20consistently%20affected%20by%20stimulus%20realism&amp;author=T%20Huisman&amp;author=T%20Dau&amp;author=T%20Piechowiak&amp;author=E%20MacDonald&amp;volume=5&amp;issue=0&amp;publication_year=2022&amp;pages=000404&amp;doi=10.2352/j.percept.imaging.2022.5.000404&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref020">
<span class="label">20.</span><cite>Kim H, Lee I-K. Studying the effects of congruence of auditory and visual stimuli on virtual reality experiences. IEEE Trans Vis Comput Graph. 2022;28(5):2080–90. doi: 10.1109/TVCG.2022.3150514

</cite> [<a href="https://doi.org/10.1109/TVCG.2022.3150514" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35167477/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;title=Studying%20the%20effects%20of%20congruence%20of%20auditory%20and%20visual%20stimuli%20on%20virtual%20reality%20experiences&amp;author=H%20Kim&amp;author=I-K%20Lee&amp;volume=28&amp;issue=5&amp;publication_year=2022&amp;pages=2080-90&amp;pmid=35167477&amp;doi=10.1109/TVCG.2022.3150514&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref021">
<span class="label">21.</span><cite>Siddig A, Sun PW, Parker M, Hines A. Perception deception: audio-visual mismatch in virtual reality using the McGurk effect. In: Proceedings of the 27th Irish Conference on Artificial Intelligence and Cognitive Science. 2019.</cite>
</li>
<li id="pone.0330693.ref022">
<span class="label">22.</span><cite>Choi W, Li L, Satoh S, Hachimura K. Multisensory integration in the virtual hand illusion with active movement. Biomed Res Int. 2016;2016:8163098. doi: 10.1155/2016/8163098

</cite> [<a href="https://doi.org/10.1155/2016/8163098" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5099483/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27847822/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Biomed%20Res%20Int&amp;title=Multisensory%20integration%20in%20the%20virtual%20hand%20illusion%20with%20active%20movement&amp;author=W%20Choi&amp;author=L%20Li&amp;author=S%20Satoh&amp;author=K%20Hachimura&amp;volume=2016&amp;publication_year=2016&amp;pages=8163098&amp;pmid=27847822&amp;doi=10.1155/2016/8163098&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref023">
<span class="label">23.</span><cite>Hoppe M, Karolus J, Dietz F, Woźniak PW, Schmidt A, Machulla T-K. VRsneaky. In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 2019. p. 1–9. 10.1145/3290605.3300776</cite> [<a href="https://doi.org/10.1145/3290605.3300776" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref024">
<span class="label">24.</span><cite>Thézé R, Gadiri MA, Albert L, Provost A, Giraud A-L, Mégevand P. Animated virtual characters to explore audio-visual speech in controlled and naturalistic environments. Sci Rep. 2020;10(1):15540. doi: 10.1038/s41598-020-72375-y

</cite> [<a href="https://doi.org/10.1038/s41598-020-72375-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7511320/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32968127/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Rep&amp;title=Animated%20virtual%20characters%20to%20explore%20audio-visual%20speech%20in%20controlled%20and%20naturalistic%20environments&amp;author=R%20Th%C3%A9z%C3%A9&amp;author=MA%20Gadiri&amp;author=L%20Albert&amp;author=A%20Provost&amp;author=A-L%20Giraud&amp;volume=10&amp;issue=1&amp;publication_year=2020&amp;pages=15540&amp;pmid=32968127&amp;doi=10.1038/s41598-020-72375-y&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref025">
<span class="label">25.</span><cite>Hurlstone MJ, Hitch GJ, Baddeley AD. Memory for serial order across domains: an overview of the literature and directions for future research. Psychol Bull. 2014;140(2):339–73. doi: 10.1037/a0034221

</cite> [<a href="https://doi.org/10.1037/a0034221" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24079725/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol%20Bull&amp;title=Memory%20for%20serial%20order%20across%20domains:%20an%20overview%20of%20the%20literature%20and%20directions%20for%20future%20research&amp;author=MJ%20Hurlstone&amp;author=GJ%20Hitch&amp;author=AD%20Baddeley&amp;volume=140&amp;issue=2&amp;publication_year=2014&amp;pages=339-73&amp;pmid=24079725&amp;doi=10.1037/a0034221&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref026">
<span class="label">26.</span><cite>Delogu F, Raffone A, Belardinelli MO. Semantic encoding in working memory: is there a (multi)modality effect?. Memory. 2009;17(6):655–63. doi: 10.1080/09658210902998054

</cite> [<a href="https://doi.org/10.1080/09658210902998054" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19536688/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Memory&amp;title=Semantic%20encoding%20in%20working%20memory:%20is%20there%20a%20(multi)modality%20effect?&amp;author=F%20Delogu&amp;author=A%20Raffone&amp;author=MO%20Belardinelli&amp;volume=17&amp;issue=6&amp;publication_year=2009&amp;pages=655-63&amp;pmid=19536688&amp;doi=10.1080/09658210902998054&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref027">
<span class="label">27.</span><cite>Surprenant AM, Neath I, LeCompte DC. irrelevant speech, phonological similarity, and presentation modality. Memory. 1999;7(4):405–20. doi: 10.1080/741944920</cite> [<a href="https://doi.org/10.1080/741944920" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Memory&amp;title=irrelevant%20speech,%20phonological%20similarity,%20and%20presentation%20modality&amp;author=AM%20Surprenant&amp;author=I%20Neath&amp;author=DC%20LeCompte&amp;volume=7&amp;issue=4&amp;publication_year=1999&amp;pages=405-20&amp;doi=10.1080/741944920&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref028">
<span class="label">28.</span><cite>Harvey AJ, Beaman CP. Input and output modality effects in immediate serial recall. Memory. 2007;15(7):693–700. doi: 10.1080/09658210701644677

</cite> [<a href="https://doi.org/10.1080/09658210701644677" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17924278/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Memory&amp;title=Input%20and%20output%20modality%20effects%20in%20immediate%20serial%20recall&amp;author=AJ%20Harvey&amp;author=CP%20Beaman&amp;volume=15&amp;issue=7&amp;publication_year=2007&amp;pages=693-700&amp;pmid=17924278&amp;doi=10.1080/09658210701644677&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref029">
<span class="label">29.</span><cite>Divin W, Coyle K, James DT. The effects of irrelevant speech and articulatory suppression on the serial recall of silently presented lipread digits. Br J Psychol. 2001;92(Pt 4):593–616. doi: 10.1348/000712601162365

</cite> [<a href="https://doi.org/10.1348/000712601162365" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11762863/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Br%20J%20Psychol&amp;title=The%20effects%20of%20irrelevant%20speech%20and%20articulatory%20suppression%20on%20the%20serial%20recall%20of%20silently%20presented%20lipread%20digits&amp;author=W%20Divin&amp;author=K%20Coyle&amp;author=DT%20James&amp;volume=92&amp;publication_year=2001&amp;pages=593-616&amp;pmid=11762863&amp;doi=10.1348/000712601162365&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref030">
<span class="label">30.</span><cite>Schlittmeier SJ, Hellbrück J, Klatte M. Does irrelevant music cause an irrelevant sound effect for auditory items?
Eur J Cognit Psychol. 2008;20(2):252–71. doi: 10.1080/09541440701427838</cite> [<a href="https://doi.org/10.1080/09541440701427838" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Eur%20J%20Cognit%20Psychol&amp;title=Does%20irrelevant%20music%20cause%20an%20irrelevant%20sound%20effect%20for%20auditory%20items?&amp;author=SJ%20Schlittmeier&amp;author=J%20Hellbr%C3%BCck&amp;author=M%20Klatte&amp;volume=20&amp;issue=2&amp;publication_year=2008&amp;pages=252-71&amp;doi=10.1080/09541440701427838&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref031">
<span class="label">31.</span><cite>Yadav M, Georgi M, Leist L, Klatte M, Schlittmeier SJ, Fels J. Cognitive performance in open-plan office acoustic simulations: effects of room acoustics and semantics but not spatial separation of sound sources. Applied Acoustics. 2023;211:109559. doi: 10.1016/j.apacoust.2023.109559</cite> [<a href="https://doi.org/10.1016/j.apacoust.2023.109559" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Applied%20Acoustics&amp;title=Cognitive%20performance%20in%20open-plan%20office%20acoustic%20simulations:%20effects%20of%20room%20acoustics%20and%20semantics%20but%20not%20spatial%20separation%20of%20sound%20sources&amp;author=M%20Yadav&amp;author=M%20Georgi&amp;author=L%20Leist&amp;author=M%20Klatte&amp;author=SJ%20Schlittmeier&amp;volume=211&amp;publication_year=2023&amp;pages=109559&amp;doi=10.1016/j.apacoust.2023.109559&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref032">
<span class="label">32.</span><cite>World Health Organization. Report of the informal working group on prevention of deafness and hearing impairment programme planning. World Health Organization; 1991.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Report%20of%20the%20informal%20working%20group%20on%20prevention%20of%20deafness%20and%20hearing%20impairment%20programme%20planning&amp;publication_year=1991&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref033">
<span class="label">33.</span><cite>Snellen H. Probebuchstaben zur Bestimmung der Sehschärfe. H. Peters; 1873.</cite>
</li>
<li id="pone.0330693.ref034">
<span class="label">34.</span><cite>Healy AF. Short-term memory for order information. Psychology of Learning and Motivation. Elsevier; 1982. p. 191–238. 10.1016/s0079-7421(08)60550-2</cite> [<a href="https://doi.org/10.1016/s0079-7421(08)60550-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Psychology%20of%20Learning%20and%20Motivation&amp;author=AF%20Healy&amp;publication_year=1982&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref035">
<span class="label">35.</span><cite>Ehret J, Bönsch A, Fels J, Schlittmeier SJ, Kuhlen TW. StudyFramework: comfortably setting up and conducting factorial-design studies using the unreal engine. In: 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW). 2024. p. 442–9. 10.1109/vrw62533.2024.00087</cite> [<a href="https://doi.org/10.1109/vrw62533.2024.00087" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref036">
<span class="label">36.</span><cite>Gilbert D, Rupp D, Krüger M, Ehret J, Helwig K, Römer T, et al. RWTH VR Group Unreal Engine Toolkit (UE5.3-2023.1-rc2). Zenodo; 2024. 10.5281/zenodo.10817754</cite> [<a href="https://doi.org/10.5281/zenodo.10817754" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref037">
<span class="label">37.</span><cite>Pausch F. Documentation of the experimental environments and hardware used in the dissertation “Spatial audio reproduction for hearing aid research: system design, evaluation and application.” RWTH Aachen University; 2022. 10.18154/RWTH-2022-01536</cite> [<a href="https://doi.org/10.18154/RWTH-2022-01536" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref038">
<span class="label">38.</span><cite>Hall ET. The hidden dimension: Man’s use of space in public and private. Bodley Head; 1969.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20hidden%20dimension:%20Man%E2%80%99s%20use%20of%20space%20in%20public%20and%20private&amp;author=ET%20Hall&amp;publication_year=1969&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref039">
<span class="label">39.</span><cite>Bonsch A, Radke S, Overath H, Asche LM, Wendt J, Vierjahn T, et al. Social VR: how personal space is affected by virtual agents’ emotions. In: 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). 2018. p. 199–206. 10.1109/vr.2018.8446480</cite> [<a href="https://doi.org/10.1109/vr.2018.8446480" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref040">
<span class="label">40.</span><cite>Oberem J, Fels J. Speech material for a paradigm on the intentional switching of auditory selective attention. RWTH Aachen University; 2020.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Speech%20material%20for%20a%20paradigm%20on%20the%20intentional%20switching%20of%20auditory%20selective%20attention&amp;author=J%20Oberem&amp;author=J%20Fels&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref041">
<span class="label">41.</span><cite>Schäfer P, Palenda P, Aspöck L, Vorländer M. Virtual acoustics - a real-time auralization framework for scientific research. Zenodo; 2024. 10.5281/zenodo.13788752</cite> [<a href="https://doi.org/10.5281/zenodo.13788752" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref042">
<span class="label">42.</span><cite>Schmitz A. Ein neues digitales Kunstkopfmeßsystem. Acta Acustica united with Acustica. 1995;81(4):416–20.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Acta%20Acustica%20united%20with%20Acustica&amp;title=Ein%20neues%20digitales%20Kunstkopfme%C3%9Fsystem&amp;author=A%20Schmitz&amp;volume=81&amp;issue=4&amp;publication_year=1995&amp;pages=416-20&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref043">
<span class="label">43.</span><cite>Oberem J, Richter J-G, Setzer D, Seibold J, Koch I, Fels J. Experiments on localization accuracy with non-individual and individual HRTFs comparing static and dynamic reproduction methods. Cold Spring Harbor Laboratory; 2020. 10.1101/2020.03.31.011650</cite> [<a href="https://doi.org/10.1101/2020.03.31.011650" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Experiments%20on%20localization%20accuracy%20with%20non-individual%20and%20individual%20HRTFs%20comparing%20static%20and%20dynamic%20reproduction%20methods&amp;author=J%20Oberem&amp;author=J-G%20Richter&amp;author=D%20Setzer&amp;author=J%20Seibold&amp;author=I%20Koch&amp;publication_year=2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref044">
<span class="label">44.</span><cite>Masiero B, Fels J. Perceptually robust headphone equalization for binaural reproduction. Journal of the Audio Engineering Society. 2011;8388.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20the%20Audio%20Engineering%20Society&amp;title=Perceptually%20robust%20headphone%20equalization%20for%20binaural%20reproduction&amp;author=B%20Masiero&amp;author=J%20Fels&amp;publication_year=2011&amp;pages=8388&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref045">
<span class="label">45.</span><cite>Slutsky DA, Recanzone GH. Temporal and spatial dependency of the ventriloquism effect. Neuroreport. 2001;12(1):7–10. doi: 10.1097/00001756-200101220-00009

</cite> [<a href="https://doi.org/10.1097/00001756-200101220-00009" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/11201094/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neuroreport&amp;title=Temporal%20and%20spatial%20dependency%20of%20the%20ventriloquism%20effect&amp;author=DA%20Slutsky&amp;author=GH%20Recanzone&amp;volume=12&amp;issue=1&amp;publication_year=2001&amp;pages=7-10&amp;pmid=11201094&amp;doi=10.1097/00001756-200101220-00009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref046">
<span class="label">46.</span><cite>Jack CE, Thurlow WR. Effects of degree of visual association and angle of displacement on the “ventriloquism” effect. Perceptual and Motor Skills. 1973;37(3):967–79. doi: 10.2466/pms.1973.37.3.967
</cite> [<a href="https://doi.org/10.2466/pms.1973.37.3.967" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/4764534/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Perceptual%20and%20Motor%20Skills&amp;title=Effects%20of%20degree%20of%20visual%20association%20and%20angle%20of%20displacement%20on%20the%20%E2%80%9Cventriloquism%E2%80%9D%20effect&amp;author=CE%20Jack&amp;author=WR%20Thurlow&amp;volume=37&amp;issue=3&amp;publication_year=1973&amp;pages=967-79&amp;pmid=4764534&amp;doi=10.2466/pms.1973.37.3.967&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref047">
<span class="label">47.</span><cite>Leiner DJ. SoSci Survey. 2019. <a href="https://www.soscisurvey.de/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.soscisurvey.de/</a></cite>
</li>
<li id="pone.0330693.ref048">
<span class="label">48.</span><cite>Bönsch A. Social wayfinding strategies to explore immersive virtual environments. RWTH Aachen University; 2024.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Social%20wayfinding%20strategies%20to%20explore%20immersive%20virtual%20environments&amp;author=A%20B%C3%B6nsch&amp;publication_year=2024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref049">
<span class="label">49.</span><cite>Usoh M, Catena E, Arman S, Slater M. Using presence questionnaires in reality. Presence: Teleoperators &amp; Virtual Environments. 2000;9(5):497–503. doi: 10.1162/105474600566989</cite> [<a href="https://doi.org/10.1162/105474600566989" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Presence:%20Teleoperators%20&amp;%20Virtual%20Environments&amp;title=Using%20presence%20questionnaires%20in%20reality&amp;author=M%20Usoh&amp;author=E%20Catena&amp;author=S%20Arman&amp;author=M%20Slater&amp;volume=9&amp;issue=5&amp;publication_year=2000&amp;pages=497-503&amp;doi=10.1162/105474600566989&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref050">
<span class="label">50.</span><cite>Ehret J, Bönsch A, Aspöck L, Röhr CT, Baumann S, Grice M, et al. Do Prosody and embodiment influence the perceived naturalness of conversational agents’ speech?. ACM Trans Appl Percept. 2021;18(4):1–15. doi: 10.1145/3486580</cite> [<a href="https://doi.org/10.1145/3486580" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Trans%20Appl%20Percept&amp;title=Do%20Prosody%20and%20embodiment%20influence%20the%20perceived%20naturalness%20of%20conversational%20agents%E2%80%99%20speech?&amp;author=J%20Ehret&amp;author=A%20B%C3%B6nsch&amp;author=L%20Asp%C3%B6ck&amp;author=CT%20R%C3%B6hr&amp;author=S%20Baumann&amp;volume=18&amp;issue=4&amp;publication_year=2021&amp;pages=1-15&amp;doi=10.1145/3486580&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref051">
<span class="label">51.</span><cite>Wickham H, Averick M, Bryan J, Chang W, McGowan L, François R, et al. Welcome to the Tidyverse. JOSS. 2019;4(43):1686. doi: 10.21105/joss.01686</cite> [<a href="https://doi.org/10.21105/joss.01686" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=JOSS&amp;title=Welcome%20to%20the%20Tidyverse&amp;author=H%20Wickham&amp;author=M%20Averick&amp;author=J%20Bryan&amp;author=W%20Chang&amp;author=L%20McGowan&amp;volume=4&amp;issue=43&amp;publication_year=2019&amp;pages=1686&amp;doi=10.21105/joss.01686&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref052">
<span class="label">52.</span><cite>Bürkner P-C. brms: an R package for bayesian multilevel models using stan. J Stat Soft. 2017;80(1):1–12. doi: 10.18637/jss.v080.i01</cite> [<a href="https://doi.org/10.18637/jss.v080.i01" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Stat%20Soft&amp;title=brms:%20an%20R%20package%20for%20bayesian%20multilevel%20models%20using%20stan&amp;author=P-C%20B%C3%BCrkner&amp;volume=80&amp;issue=1&amp;publication_year=2017&amp;pages=1-12&amp;doi=10.18637/jss.v080.i01&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref053">
<span class="label">53.</span><cite>Sivula T, Magnusson M, Matamoros AA, Vehtari A. Uncertainty in Bayesian leave-one-out cross-validation based model comparison. arXiv preprint
2023. doi: arXiv:2008.10296</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=Uncertainty%20in%20Bayesian%20leave-one-out%20cross-validation%20based%20model%20comparison&amp;author=T%20Sivula&amp;author=M%20Magnusson&amp;author=AA%20Matamoros&amp;author=A%20Vehtari&amp;publication_year=2023&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref054">
<span class="label">54.</span><cite>emmeans: Estimated Marginal Means, aka Least-Squares Means. R package; 2022. <a href="https://CRAN.R-project.org/package=emmeans" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://CRAN.R-project.org/package=emmeans</a>.</cite>
</li>
<li id="pone.0330693.ref055">
<span class="label">55.</span><cite>Kruschke JK, Liddell TM. The Bayesian New Statistics: hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychon Bull Rev. 2018;25(1):178–206. doi: 10.3758/s13423-016-1221-4

</cite> [<a href="https://doi.org/10.3758/s13423-016-1221-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28176294/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychon%20Bull%20Rev&amp;title=The%20Bayesian%20New%20Statistics:%20hypothesis%20testing,%20estimation,%20meta-analysis,%20and%20power%20analysis%20from%20a%20Bayesian%20perspective&amp;author=JK%20Kruschke&amp;author=TM%20Liddell&amp;volume=25&amp;issue=1&amp;publication_year=2018&amp;pages=178-206&amp;pmid=28176294&amp;doi=10.3758/s13423-016-1221-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref056">
<span class="label">56.</span><cite>Makowski D, Ben-Shachar M, Lüdecke D. bayestestR: describing effects and their uncertainty, existence and significance within the Bayesian framework. JOSS. 2019;4(40):1541. doi: 10.21105/joss.01541</cite> [<a href="https://doi.org/10.21105/joss.01541" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=JOSS&amp;title=bayestestR:%20describing%20effects%20and%20their%20uncertainty,%20existence%20and%20significance%20within%20the%20Bayesian%20framework&amp;author=D%20Makowski&amp;author=M%20Ben-Shachar&amp;author=D%20L%C3%BCdecke&amp;volume=4&amp;issue=40&amp;publication_year=2019&amp;pages=1541&amp;doi=10.21105/joss.01541&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref057">
<span class="label">57.</span><cite>Marsh JE, Ljung R, Jahncke H, MacCutcheon D, Pausch F, Ball LJ, et al. Why are background telephone conversations distracting?
J Exp Psychol Appl. 2018;24(2):222–35. doi: 10.1037/xap0000170

</cite> [<a href="https://doi.org/10.1037/xap0000170" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29878842/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Exp%20Psychol%20Appl&amp;title=Why%20are%20background%20telephone%20conversations%20distracting?&amp;author=JE%20Marsh&amp;author=R%20Ljung&amp;author=H%20Jahncke&amp;author=D%20MacCutcheon&amp;author=F%20Pausch&amp;volume=24&amp;issue=2&amp;publication_year=2018&amp;pages=222-35&amp;pmid=29878842&amp;doi=10.1037/xap0000170&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref058">
<span class="label">58.</span><cite>Huisman T, Ahrens A, MacDonald E. Ambisonics sound source localization with varying amount of visual information in virtual reality. Front Virtual Real. 2021;2:722321.doi: 10.3389/frvir.2021.722321</cite> [<a href="https://doi.org/10.3389/frvir.2021.722321" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front%20Virtual%20Real&amp;title=Ambisonics%20sound%20source%20localization%20with%20varying%20amount%20of%20visual%20information%20in%20virtual%20reality&amp;author=T%20Huisman&amp;author=A%20Ahrens&amp;author=E%20MacDonald&amp;volume=2&amp;publication_year=2021&amp;pages=722321&amp;doi=10.3389/frvir.2021.722321&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref059">
<span class="label">59.</span><cite>Kroczek LOH, Roßkopf S, Stärz F, Blau M, van de Par S, Mühlberger A. The influence of affective voice on sound distance perception. J Exp Psychol Hum Percept Perform. 2024;50(9):918–33. doi: 10.1037/xhp0001222

</cite> [<a href="https://doi.org/10.1037/xhp0001222" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39101929/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Exp%20Psychol%20Hum%20Percept%20Perform&amp;title=The%20influence%20of%20affective%20voice%20on%20sound%20distance%20perception&amp;author=LOH%20Kroczek&amp;author=S%20Ro%C3%9Fkopf&amp;author=F%20St%C3%A4rz&amp;author=M%20Blau&amp;author=S%20van%20de%20Par&amp;volume=50&amp;issue=9&amp;publication_year=2024&amp;pages=918-33&amp;pmid=39101929&amp;doi=10.1037/xhp0001222&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref060">
<span class="label">60.</span><cite>Liebl A, Haller J, Jödicke B, Baumgartner H, Schlittmeier S, Hellbrück J. Combined effects of acoustic and visual distraction on cognitive performance and well-being. Appl Ergon. 2012;43(2):424–34. doi: 10.1016/j.apergo.2011.06.017

</cite> [<a href="https://doi.org/10.1016/j.apergo.2011.06.017" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21802069/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Appl%20Ergon&amp;title=Combined%20effects%20of%20acoustic%20and%20visual%20distraction%20on%20cognitive%20performance%20and%20well-being&amp;author=A%20Liebl&amp;author=J%20Haller&amp;author=B%20J%C3%B6dicke&amp;author=H%20Baumgartner&amp;author=S%20Schlittmeier&amp;volume=43&amp;issue=2&amp;publication_year=2012&amp;pages=424-34&amp;pmid=21802069&amp;doi=10.1016/j.apergo.2011.06.017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref061">
<span class="label">61.</span><cite>Lange EB. Disruption of attention by irrelevant stimuli in serial recall. Journal of Memory and Language. 2005;53(4):513–31. doi: 10.1016/j.jml.2005.07.002</cite> [<a href="https://doi.org/10.1016/j.jml.2005.07.002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Memory%20and%20Language&amp;title=Disruption%20of%20attention%20by%20irrelevant%20stimuli%20in%20serial%20recall&amp;author=EB%20Lange&amp;volume=53&amp;issue=4&amp;publication_year=2005&amp;pages=513-31&amp;doi=10.1016/j.jml.2005.07.002&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref062">
<span class="label">62.</span><cite>Baddeley AD, Hitch G. Working memory. Psychology of learning and motivation. Elsevier; 1974. p. 47–89. 10.1016/s0079-7421(08)60452-1</cite> [<a href="https://doi.org/10.1016/s0079-7421(08)60452-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Psychology%20of%20learning%20and%20motivation&amp;author=AD%20Baddeley&amp;author=G%20Hitch&amp;publication_year=1974&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref063">
<span class="label">63.</span><cite>McKeown G, Spencer C, Patterson A, Creaney T, Dupré D. Comparing virtual reality with computer monitors as rating environments for affective dimensions in social interactions. In: Proceedings of the 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII); 2017. San Antonio, TX, USA: IEEE; 2017. p. 464–69. 10.1109/ACII.2017.8273640</cite> [<a href="https://doi.org/10.1109/ACII.2017.8273640" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref064">
<span class="label">64.</span><cite>Zojaji S, Steed A, Peters C. Impact of immersiveness on persuasiveness, politeness and social adherence in human-agent interactions within small groups. In: ICAT-EGVE 2023 - International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments, 2023. 10.2312/egve.20231315</cite> [<a href="https://doi.org/10.2312/egve.20231315" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref065">
<span class="label">65.</span><cite>Walker-Andrews AS, Bahrick LE, Raglioni SS, Diaz I. Infants’ bimodal perception of gender. Ecological Psychology. 1991;3(2):55–75. doi: 10.1207/s15326969eco0302_1</cite> [<a href="https://doi.org/10.1207/s15326969eco0302_1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Ecological%20Psychology&amp;title=Infants%E2%80%99%20bimodal%20perception%20of%20gender&amp;author=AS%20Walker-Andrews&amp;author=LE%20Bahrick&amp;author=SS%20Raglioni&amp;author=I%20Diaz&amp;volume=3&amp;issue=2&amp;publication_year=1991&amp;pages=55-75&amp;doi=10.1207/s15326969eco0302_1&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref066">
<span class="label">66.</span><cite>Vatakis A, Spence C. Crossmodal binding: evaluating the “unity assumption” using audiovisual speech stimuli. Percept Psychophys. 2007;69(5):744–56. doi: 10.3758/bf03193776

</cite> [<a href="https://doi.org/10.3758/bf03193776" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17929697/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Percept%20Psychophys&amp;title=Crossmodal%20binding:%20evaluating%20the%20%E2%80%9Cunity%20assumption%E2%80%9D%20using%20audiovisual%20speech%20stimuli&amp;author=A%20Vatakis&amp;author=C%20Spence&amp;volume=69&amp;issue=5&amp;publication_year=2007&amp;pages=744-56&amp;pmid=17929697&amp;doi=10.3758/bf03193776&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330693.ref067">
<span class="label">67.</span><cite>Szerszen KA. The audio/visual mismatch and the uncanny valley: an investigation using a mismatch in the human realism of facial and vocal aspects of stimuli. Indiana University-Purdue University Indianapolis (IUPUI); 2011.</cite>
</li>
<li id="pone.0330693.ref068">
<span class="label">68.</span><cite>Ehret J, Bönsch A, Nossol P, Ermert CA, Mohanathasan C, Schlittmeier SJ, et al. Who’s next?: Integrating non-verbal turn-taking cues for embodied conversational agents. In: Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents, 2023. p. 1–8. 10.1145/3570945.3607312</cite> [<a href="https://doi.org/10.1145/3570945.3607312" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330693.ref069">
<span class="label">69.</span><cite>McNeish D. On using Bayesian methods to address small sample problems. Structural Equation Modeling: A Multidisciplinary Journal. 2016;23(5):750–73. doi: 10.1080/10705511.2016.1186549</cite> [<a href="https://doi.org/10.1080/10705511.2016.1186549" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Structural%20Equation%20Modeling:%20A%20Multidisciplinary%20Journal&amp;title=On%20using%20Bayesian%20methods%20to%20address%20small%20sample%20problems&amp;author=D%20McNeish&amp;volume=23&amp;issue=5&amp;publication_year=2016&amp;pages=750-73&amp;doi=10.1080/10705511.2016.1186549&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adsm93_" lang="en" class="supplementary-materials"><h3 class="pmc_sec_title">Supplementary Materials</h3>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="caption p">
<span>S1 File. Data of Experiment 1.</span><p><em>Accuracy</em> and <em>RT_mean</em> for each condition and serial position in Experiment 1 (audiovisual angle incongruence).</p>
<p>(CSV)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s001.CSV" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s001.CSV</a><sup> (54.6KB, CSV) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="caption p">
<span>S2 File. Data of Experiment 2.</span><p><em>Accuracy</em> and <em>RT_mean</em> for each condition and serial position in Experiment 2 (audiovisual voice incongruence).</p>
<p>(CSV)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s002.CSV" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s002.CSV</a><sup> (36.5KB, CSV) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material3_reqid_"><div class="caption p">
<span>S3 File. Data plots and analysis of RT_mean Experiment 1 and 2.</span><p>Plots of <em>Accuracy</em> and <em>RT_mean</em> as well as results of the statistical analysis of <em>RT_mean</em> for Experiment 1 and 2.</p>
<p>(PDF)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s003.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s003.pdf</a><sup> (1.2MB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material4_reqid_"><div class="caption p">
<span>S4 File. Questionnaire.</span><p>Questions asked in the questionnaires (except for SUS questions) with corresponding anchors.</p>
<p>(TXT)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s004.txt" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s004.txt</a><sup> (2.9KB, txt) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material5_reqid_"><div class="caption p">
<span>S5 File. Questionnaire Data Experiment 1 and 2.</span><p>Ratings for all questions of the questionnaire for Experiments 1 and 2.</p>
<p>(CSV)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12373245/bin/pone.0330693.s005.CSV" data-ga-action="click_feat_suppl" class="usa-link">pone.0330693.s005.CSV</a><sup> (6.9KB, CSV) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>All relevant data are within the manuscript and its <a href="#sec030" class="usa-link">Supporting information</a> files.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0330693"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0330693.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (7.9 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12373245/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12373245/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373245%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373245/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12373245/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12373245/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40845029/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12373245/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40845029/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12373245/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12373245/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="Gq18Fw5Ih1b9YzViouSWsgdoFreZR7lEPSGJJurmgVeb8nrmkzPypxkqc5I7vVMr">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-574fdcc6.js"></script>
    
    

    </body>
</html>
