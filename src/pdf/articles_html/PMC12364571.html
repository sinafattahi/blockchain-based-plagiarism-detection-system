
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Characterizing internal models of the visual environment - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4AE5A8AF308F305AE5A0050403D55.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="procb">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364571/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Proceedings of the Royal Society B: Biological Sciences">
<meta name="citation_title" content="Characterizing internal models of the visual environment">
<meta name="citation_author" content="Micha Engeser">
<meta name="citation_author_institution" content="Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany">
<meta name="citation_author_institution" content="Center for Mind, Brain and Behavior, Universities of Giessen, Marburg, and Darmstadt, Marburg, HE, Germany">
<meta name="citation_author" content="Susan Ajith">
<meta name="citation_author_institution" content="Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany">
<meta name="citation_author" content="Ilker Duymaz">
<meta name="citation_author_institution" content="Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany">
<meta name="citation_author" content="Gongting Wang">
<meta name="citation_author_institution" content="Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany">
<meta name="citation_author_institution" content="Department of Education and Psychology, Freie Universität Berlin, Berlin, Germany">
<meta name="citation_author" content="Matthew J Foxwell">
<meta name="citation_author_institution" content="Department of Psychology, University of York, York, UK">
<meta name="citation_author" content="Radoslaw M Cichy">
<meta name="citation_author_institution" content="Department of Education and Psychology, Freie Universität Berlin, Berlin, Germany">
<meta name="citation_author" content="David Pitcher">
<meta name="citation_author_institution" content="Department of Psychology, University of York, York, UK">
<meta name="citation_author" content="Daniel Kaiser">
<meta name="citation_author_institution" content="Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany">
<meta name="citation_author_institution" content="Center for Mind, Brain and Behavior, Universities of Giessen, Marburg, and Darmstadt, Marburg, HE, Germany">
<meta name="citation_author_institution" content="Cluster of Excellence &quot;The Adaptive Mind&quot;, Universities of Giessen, Marburg, and Darmstadt, Giessen, HE, Germany">
<meta name="citation_publication_date" content="2025 Aug 20">
<meta name="citation_volume" content="292">
<meta name="citation_issue" content="2053">
<meta name="citation_firstpage" content="20250602">
<meta name="citation_doi" content="10.1098/rspb.2025.0602">
<meta name="citation_pmid" content="40829663">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364571/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364571/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364571/pdf/rspb.2025.0602.pdf">
<meta name="description" content="Despite the complexity of real-world environments, natural vision is seamlessly efficient. To explain this efficiency, researchers often use predictive processing frameworks, in which perceptual efficiency is determined by the match between the ...">
<meta name="og:title" content="Characterizing internal models of the visual environment">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Despite the complexity of real-world environments, natural vision is seamlessly efficient. To explain this efficiency, researchers often use predictive processing frameworks, in which perceptual efficiency is determined by the match between the ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364571/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12364571">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1098/rspb.2025.0602"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/rspb.2025.0602.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12364571%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12364571/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12364571/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364571/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-procb.gif" alt="Proceedings of the Royal Society B: Biological Sciences logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Proceedings of the Royal Society B: Biological Sciences" title="Link to Proceedings of the Royal Society B: Biological Sciences" shape="default" href="http://rspb.royalsocietypublishing.org/" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Proc Biol Sci</button></div>. 2025 Aug 20;292(2053):20250602. doi: <a href="https://doi.org/10.1098/rspb.2025.0602" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1098/rspb.2025.0602</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Proc%20Biol%20Sci%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Proc%20Biol%20Sci%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Proc%20Biol%20Sci%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Proc%20Biol%20Sci%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Characterizing internal models of the visual environment</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Engeser%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Micha Engeser</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Micha Engeser</span></h3>
<div class="p">
<sup><sup>1</sup></sup>Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany</div>
<div class="p">
<sup><sup>2</sup></sup>Center for Mind, Brain and Behavior, Universities of Giessen, Marburg, and Darmstadt, Marburg, HE, Germany</div>
<div>Conceptualization, Writing – original draft, Writing – review and editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Engeser%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Micha Engeser</span></a>
</div>
</div>
<sup>1,</sup><sup>2,</sup><sup>†</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ajith%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Susan Ajith</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Susan Ajith</span></h3>
<div class="p">
<sup><sup>1</sup></sup>Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany</div>
<div>Conceptualization, Writing – original draft, Writing – review and editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ajith%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Susan Ajith</span></a>
</div>
</div>
<sup>1,</sup><sup>†</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Duymaz%20I%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Ilker Duymaz</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Ilker Duymaz</span></h3>
<div class="p">
<sup><sup>1</sup></sup>Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany</div>
<div>Conceptualization, Writing – original draft, Writing – review and editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Duymaz%20I%22%5BAuthor%5D" class="usa-link"><span class="name western">Ilker Duymaz</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20G%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Gongting Wang</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Gongting Wang</span></h3>
<div class="p">
<sup><sup>1</sup></sup>Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany</div>
<div class="p">
<sup><sup>3</sup></sup>Department of Education and Psychology, Freie Universität Berlin, Berlin, Germany</div>
<div>Writing – original draft, Writing – review and editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20G%22%5BAuthor%5D" class="usa-link"><span class="name western">Gongting Wang</span></a>
</div>
</div>
<sup>1,</sup><sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Foxwell%20MJ%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Matthew J Foxwell</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Matthew J Foxwell</span></h3>
<div class="p">
<sup><sup>4</sup></sup>Department of Psychology, University of York, York, UK</div>
<div>Conceptualization, Writing – original draft, Writing – review and editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Foxwell%20MJ%22%5BAuthor%5D" class="usa-link"><span class="name western">Matthew J Foxwell</span></a>
</div>
</div>
<sup>4</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cichy%20RM%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Radoslaw M Cichy</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Radoslaw M Cichy</span></h3>
<div class="p">
<sup><sup>3</sup></sup>Department of Education and Psychology, Freie Universität Berlin, Berlin, Germany</div>
<div>Supervision, Writing – review and editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Cichy%20RM%22%5BAuthor%5D" class="usa-link"><span class="name western">Radoslaw M Cichy</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Pitcher%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">David Pitcher</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">David Pitcher</span></h3>
<div class="p">
<sup><sup>4</sup></sup>Department of Psychology, University of York, York, UK</div>
<div>Supervision, Writing – review and editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Pitcher%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">David Pitcher</span></a>
</div>
</div>
<sup>4</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kaiser%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Daniel Kaiser</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Daniel Kaiser</span></h3>
<div class="p">
<sup><sup>1</sup></sup>Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany</div>
<div class="p">
<sup><sup>2</sup></sup>Center for Mind, Brain and Behavior, Universities of Giessen, Marburg, and Darmstadt, Marburg, HE, Germany</div>
<div class="p">
<sup><sup>5</sup></sup>Cluster of Excellence "The Adaptive Mind", Universities of Giessen, Marburg, and Darmstadt, Giessen, HE, Germany</div>
<div>Conceptualization, Funding acquisition, Project administration, Supervision, Visualization, Writing – original draft, Writing – review and editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kaiser%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">Daniel Kaiser</span></a>
</div>
</div>
<sup>1,</sup><sup>2,</sup><sup>5,</sup><sup>✉</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff1">
<sup><sup>1</sup></sup>Department of Mathematics and Computer Science, Physics, Geography, Justus Liebig University Giessen, Giessen, HE, Germany</div>
<div id="aff2">
<sup><sup>2</sup></sup>Center for Mind, Brain and Behavior, Universities of Giessen, Marburg, and Darmstadt, Marburg, HE, Germany</div>
<div id="aff3">
<sup><sup>3</sup></sup>Department of Education and Psychology, Freie Universität Berlin, Berlin, Germany</div>
<div id="aff4">
<sup><sup>4</sup></sup>Department of Psychology, University of York, York, UK</div>
<div id="aff5">
<sup><sup>5</sup></sup>Cluster of Excellence "The Adaptive Mind", Universities of Giessen, Marburg, and Darmstadt, Giessen, HE, Germany</div>
<div class="author-notes p">
<div class="fn" id="equal-contrib1">
<sup>
<sup>†</sup>
</sup><p class="display-inline">These authors contributed equally to the study.</p>
</div>
<div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Micha Engeser</span></strong>: <span class="role">Conceptualization, Writing – original draft, Writing – review and editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Susan Ajith</span></strong>: <span class="role">Conceptualization, Writing – original draft, Writing – review and editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Ilker Duymaz</span></strong>: <span class="role">Conceptualization, Writing – original draft, Writing – review and editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Gongting Wang</span></strong>: <span class="role">Writing – original draft, Writing – review and editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Matthew J Foxwell</span></strong>: <span class="role">Conceptualization, Writing – original draft, Writing – review and editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Radoslaw M Cichy</span></strong>: <span class="role">Supervision, Writing – review and editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">David Pitcher</span></strong>: <span class="role">Supervision, Writing – review and editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Daniel Kaiser</span></strong>: <span class="role">Conceptualization, Funding acquisition, Project administration, Supervision, Visualization, Writing – original draft, Writing – review and editing</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Mar 3; Revised 2025 Jun 16; Accepted 2025 Jul 25; Collection date 2025 Aug.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 The Authors.</div>
<p>Published by the Royal Society under the terms of the Creative Commons Attribution License <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>, which permits unrestricted use, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12364571  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40829663/" class="usa-link">40829663</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>Despite the complexity of real-world environments, natural vision is seamlessly efficient. To explain this efficiency, researchers often use predictive processing frameworks, in which perceptual efficiency is determined by the match between the visual input and internal models of what the world should look like. In scene vision, predictions derived from our internal models of a scene should play a particularly important role, given the highly reliable statistical structure of our environment. Despite their importance for scene perception, we still do not fully understand what is contained in our internal models of the environment. Here, we highlight that the current literature disproportionately focuses on an experimental approach that tries to infer the contents of internal models from arbitrary, experimenter-driven manipulations in stimulus characteristics. To make progress, additional participant-driven approaches are needed, focusing on participants’ descriptions of what constitutes a typical scene. We discuss how recent studies on memory and perception used methods like line drawings to characterize internal representations in unconstrained ways and on the level of individual participants. These emerging methods show that it is now time to also study natural scene perception from a different angle—starting with a characterization of an individual’s expectations about the world.</p>
<section id="kwd-group1" class="kwd-group"><p><strong>Keywords:</strong> visual perception, scene representation, predictive processing, internal models, individual differences, drawings</p></section></section><section id="s1"><h2 class="pmc_sec_title">1.  Natural vision and internal models of the world</h2>
<p>Perceptual efficiency is often understood through the lens of predictive processing [<a href="#B1" class="usa-link" aria-describedby="B1">1</a>,<a href="#B2" class="usa-link" aria-describedby="B2">2</a>]. In this framework, visual inputs are routinely compared against internal models, which are based on our expectations of what the world should look like. In the processing of natural environments, internal models should play a particularly helpful role [<a href="#B3" class="usa-link" aria-describedby="B3">3</a>,<a href="#B4" class="usa-link" aria-describedby="B4">4</a>]: natural scenes are reliably structured, with a global structure that is stable across instances of a category and objects placed in statistically predictable locations [<a href="#B5" class="usa-link" aria-describedby="B5">5</a>–<a href="#B10" class="usa-link" aria-describedby="B10">10</a>]. The reliable structure of natural scenes should give rise to rich internal models that capture what a specific scene (e.g. a kitchen) should typically look like.</p>
<p>The study of vision as an inverse inference problem has its origins in Helmholtz’s idea of perception [<a href="#B11" class="usa-link" aria-describedby="B11">11</a>]. Within this framework, the perceptual system uses prior knowledge about the world, obtained through experience, to infer the causes of proximal stimulus patterns. In this view, internal models of the world, which contain this prior knowledge, are thus critical determinants for further efficient natural perception. This was later highlighted by schema theory, which postulated that inputs are referenced against internal models (schemata) that reflect the structure of the world (e.g. the likely object arrangements found in a scene; [<a href="#B12" class="usa-link" aria-describedby="B12">12</a>,<a href="#B13" class="usa-link" aria-describedby="B13">13</a>]). This concept has influenced early research on scene perception [<a href="#B14" class="usa-link" aria-describedby="B14">14</a>,<a href="#B15" class="usa-link" aria-describedby="B15">15</a>] and memory [<a href="#B16" class="usa-link" aria-describedby="B16">16</a>,<a href="#B17" class="usa-link" aria-describedby="B17">17</a>]. In contemporary research, the idea reverberates in the use of predictive processing frameworks for explaining how we perceive [<a href="#B2" class="usa-link" aria-describedby="B2">2</a>,<a href="#B18" class="usa-link" aria-describedby="B18">18</a>,<a href="#B19" class="usa-link" aria-describedby="B19">19</a>] and explore natural scenes [<a href="#B20" class="usa-link" aria-describedby="B20">20</a>] as well as how they are analysed in the brain [<a href="#B21" class="usa-link" aria-describedby="B21">21</a>,<a href="#B22" class="usa-link" aria-describedby="B22">22</a>].</p>
<p>Together, the currently favoured theoretical frameworks converge towards a view in which the contents of our internal models shape how we perceive the world. Yet, critical questions remain unsolved: what exactly are the contents of the internal models that guide natural vision? And, given the variability in visual experiences, how do these models differ across individuals? Here, we discuss how internal models have classically been characterized in the domain of natural scene perception. Reviewing this literature, we distil a need for complementary methods that enable individual participants to report the characteristics of their internal model of a scene. We highlight emerging methods suitable for this purpose, including drawings, scene arrangements, linguistic descriptions and neuroimaging-based methods.</p></section><section id="s2"><h2 class="pmc_sec_title">2.  The classical approach to characterizing internal models</h2>
<section id="s2-1"><h3 class="pmc_sec_title">(a). Probing internal models of scenes by manipulating input characteristics</h3>
<p>Previous work was built on the assumption that the contents of internal models can be studied by varying the level of typicality (i.e. schema-congruence) in the input. Under this assumption, internal models can be characterized by studying responses to stimuli that are in accordance with or in conflict with typical real-world experience, and thus with internal models (<a href="#F1" class="usa-link">figure 1</a>). Through such manipulations, researchers were able to isolate various aspects of typical scene structure that facilitate perception and cortical processing. These include the positioning of individual objects across space [<a href="#B23" class="usa-link" aria-describedby="B23">23</a>,<a href="#B27" class="usa-link" aria-describedby="B27">27</a>,<a href="#B28" class="usa-link" aria-describedby="B28">28</a>], spatial relationships between objects [<a href="#B24" class="usa-link" aria-describedby="B24">24</a>,<a href="#B29" class="usa-link" aria-describedby="B29">29</a>–<a href="#B32" class="usa-link" aria-describedby="B32">32</a>], contextual relationships between scenes and objects [<a href="#B33" class="usa-link" aria-describedby="B33">33</a>–<a href="#B36" class="usa-link" aria-describedby="B36">36</a>] and the spatial configuration of the scene as a whole [<a href="#B14" class="usa-link" aria-describedby="B14">14</a>,<a href="#B37" class="usa-link" aria-describedby="B37">37</a>–<a href="#B39" class="usa-link" aria-describedby="B39">39</a>]. Together, these studies show that typical scene structure at multiple levels of description contributes to the efficient perception and neural representation of scenes (for reviews, see [<a href="#B5" class="usa-link" aria-describedby="B5">5</a>–<a href="#B10" class="usa-link" aria-describedby="B10">10</a>,<a href="#B19" class="usa-link" aria-describedby="B19">19</a>]), suggesting that internal models contain rich information about the typical properties of natural scenes.</p>
<figure class="fig xbox font-sm" id="F1" title="Understanding internal models through stimulus manipulation."><h4 class="obj_head">Figure 1. </h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12364571_rspb.2025.0602.f001.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5256/12364571/56884d1bd879/rspb.2025.0602.f001.jpg" loading="lazy" height="216" width="730" alt="Understanding internal models through stimulus manipulation."></a></p>
<div class="p text-right font-secondary"><a href="figure/F1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Understanding internal models through stimulus manipulation. To infer the contents of internal models of the world, researchers have manipulated the real-world typicality of visual inputs on different levels. From left to right: manipulations in the typical positioning of individual objects across visual space [<a href="https://www.zotero.org/google-docs/?3bpT1y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">23</a>], the typical composition of multiple objects across space (figure is reproduced from [<a href="https://www.zotero.org/google-docs/?x36x4U" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">24</a>]), the semantic consistency between scenes and the objects they contain [<a href="https://www.zotero.org/google-docs/?i0VcpH" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">25</a>] and the structural coherence of the scene [<a href="https://www.zotero.org/google-docs/?ZPLXal" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">26</a>]. By comparing typically arranged stimuli with atypically arranged stimuli, such studies show that the visual system preferentially processes stimuli that are in accordance with our priors.</p></figcaption></figure></section><section id="s2-2"><h3 class="pmc_sec_title">(b). Challenges for the stimulus manipulation approach</h3>
<p>While the approach of manipulating stimulus characteristics has led to significant advances in our understanding of the contents of our internal models of the world, this ‘classical’ approach has several critical downsides.</p>
<p>First, it largely rests on the experimenter’s intuition of what a typical scene looks like and which factors construe its typicality. Although researchers have started using computational analyses to determine typical scene properties more objectively, for example, by extracting object distributions across large scene databases [<a href="#B6" class="usa-link" aria-describedby="B6">6</a>,<a href="#B40" class="usa-link" aria-describedby="B40">40</a>,<a href="#B41" class="usa-link" aria-describedby="B41">41</a>], such approaches are still centred on the idea that the property selected by the researcher plays an important role. Properties that intuitively should be featured in our internal models because they are visually prominent (e.g. the colour of the objects contained in a bathroom versus living room) can, in principle, be relatively uncritical for scene processing, while others that are harder to grasp intuitively (e.g. the relative orientation of objects towards each other) may be exploited by the brain more strongly.</p>
<p>Second, stimulus manipulation approaches only allow for independently manipulating particular stimulus dimensions at a time. Natural scenes, however, cannot be easily decomposed into a few orthogonal dimensions. Furthermore, the relevant dimensions probably interact with each other: in a kitchen scene, objects like an oven or sink are typically aligned along the walls, whereas smaller objects like utensils or cups are placed on horizontal surfaces. Studies separately investigating object distributions or scene geometry may therefore miss critical interactions between these properties. On a practical end, studies that look at such different factors tend to employ different experimental paradigms, making it hard to compare their relative contributions.</p>
<p>Third, many studies use artificial or unusual stimuli to create ‘atypical’ scenes. For instance, in studies of scene–object congruence [<a href="#B25" class="usa-link" aria-describedby="B25">25</a>,<a href="#B33" class="usa-link" aria-describedby="B33">33</a>,<a href="#B34" class="usa-link" aria-describedby="B34">34</a>], researchers need to create incongruent conditions, in which the objects are positioned atypically: a living room lamp is shown on a street, and the streetlight in a living room. Recent behavioural work suggests that differences between congruent and incongruent conditions may indeed be driven by a ‘congruency cost’, where the unexpected, incongruent objects gain a relative processing advantage [<a href="#B42" class="usa-link" aria-describedby="B42">42</a>]. Moreover, on the cortical level, spatially distinct regions code for congruent and incongruent conditions [<a href="#B43" class="usa-link" aria-describedby="B43">43</a>]. The problem is further aggravated when the atypical conditions violate the laws of physics [<a href="#B24" class="usa-link" aria-describedby="B24">24</a>,<a href="#B31" class="usa-link" aria-describedby="B31">31</a>] or produce unnaturalistic inputs [<a href="#B7" class="usa-link" aria-describedby="B7">7</a>].</p>
<p>Finally, the stimulus manipulation approach neglects inter-individual differences in internal models. Yet, internal models probably differ across individuals: although a typical kitchen will probably look somewhat similar for two people, there may be critical differences, for instance, in the placement of objects. Such individual differences may relate to idiosyncratic visual diets, but also to cultural, linguistic or socioeconomic factors [<a href="#B44" class="usa-link" aria-describedby="B44">44</a>,<a href="#B45" class="usa-link" aria-describedby="B45">45</a>]. If we could harness this variability, we may find that there are characteristic differences in the way each of us perceives the world, based on our idiosyncratic priors of what the world looks like.</p>
<p>To overcome these critical differences, an approach focused on obtaining descriptions of internal models directly from the participants bears enormous potential. Moving on, it is important to note that the following approaches are not intended to replace the classical approach discussed above—they rather constitute an addition to the existing toolkit for characterizing natural vision.</p></section></section><section id="s3"><h2 class="pmc_sec_title">3.  A complementary approach for characterizing internal models of the world</h2>
<section id="s3-1"><h3 class="pmc_sec_title">(a). Describing internal models</h3>
<p>Here, we highlight a novel, complementary approach that characterizes individual participants’ internal models of scenes in more unconstrained ways, without prior assumptions about their properties (<a href="#F2" class="usa-link">figure 2</a>). The key idea is to ask participants to provide ‘descriptions’ about the contents of their internal model that can then inform further investigation. By obtaining such descriptions of internal models, we can start to understand how individuals converge and diverge in their conceptions of scene typicality. Moreover, we can in turn use descriptions of internal models to make targeted predictions about processing efficiency for individual scenes in individual observers.</p>
<figure class="fig xbox font-sm" id="F2" title="A complementary approach for studying internal models of the world."><h4 class="obj_head">Figure 2. </h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12364571_rspb.2025.0602.f002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5256/12364571/c0709e52955f/rspb.2025.0602.f002.jpg" loading="lazy" height="327" width="733" alt="A complementary approach for studying internal models of the world."></a></p>
<div class="p text-right font-secondary"><a href="figure/F2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>A complementary approach for studying internal models of the world. The classical approach aims at discovering properties of internal models through stimulus manipulation, for instance by manipulating a scene’s global structure. Here, we highlight a complementary approach, in which the contents of internal models are described by observers, for instance through line drawing (where people draw typical versions of scenes) or scene arrangement methods (where people arrange physical or virtual scenes in typical ways). These descriptions can, in turn, be used to derive targeted predictions about processing efficiency for a set of inputs.</p></figcaption></figure><p>This approach circumvents critical limitations of the classical stimulus manipulation approach. First, researchers do not need to use their intuitions about which features constrain internal models of natural scenes, as they can rather rely on the features emerging from participants’ descriptions. For instance, rather than manipulating specific features based on prior knowledge, such as the object position or overall scene context (<a href="#F1" class="usa-link">figure 1</a>), individual descriptions provide direct access to the features prioritized across participants. Second, multiple interacting feature dimensions can be studied at once, as these will inherently be present in the descriptions. Third, there is no need to artificially create atypical stimuli: the individual typicality of a range of stimuli can be quantified relative to the descriptors. Finally, by focusing on descriptions of internal models in individuals, we can understand to what extent our priors are general or idiosyncratic.</p>
<p>How can we experimentally obtain such descriptions of internal models? We will discuss several methods that enable individual participants to convey their idea of a typical scene exemplar. One strong candidate is drawing, which has proven to be a versatile tool for transforming mental representations into visible descriptions [<a href="#B46" class="usa-link" aria-describedby="B46">46</a>]. Beyond that, we discuss scene arrangement, linguistic descriptions and neuroimaging-based techniques. We illustrate these techniques with a range of studies, primarily focusing on the memory and perception literature.</p></section><section id="s3-2"><h3 class="pmc_sec_title">(b). Line drawings as descriptors of internal models</h3>
<p>Line drawings can be seen as functional abstractions of the ways in which we see the world in the sense that they ‘exploit the underlying neural codes of vision’ [<a href="#B47" class="usa-link" aria-describedby="B47">47</a>]: when we draw an object or a scene, our drawing tends to focus on what conveys the most essential details of visual images in a form that abstracts away from irrelevant detail. Line drawings of scenes are recognized with virtually identical efficiency as scene photographs [<a href="#B48" class="usa-link" aria-describedby="B48">48</a>], probably because they preserve critical information about the curvature and intersection of contours [<a href="#B49" class="usa-link" aria-describedby="B49">49</a>,<a href="#B50" class="usa-link" aria-describedby="B50">50</a>]. Neuroimaging work has shown that line drawings yield characteristic category-specific neural activation patterns in the high-level visual cortex [<a href="#B50" class="usa-link" aria-describedby="B50">50</a>,<a href="#B51" class="usa-link" aria-describedby="B51">51</a>]. Beyond theoretical reasons, drawings also offer practical advantages: they are easy to generate and enable the creation of rich scenes in an (almost) unconstrained manner. These qualities render drawings an ideal candidate for the description of internal models. In the following, we highlight how these advantages have been leveraged in clinical and developmental settings, before turning towards research in memory and perception.</p>
<section id="s3-2-1"><h4 class="pmc_sec_title">(i). Line drawings in clinical assessment and developmental research</h4>
<p>The use of drawings has a long history in clinical assessment to diagnose and classify visual impairments such as agnosia [<a href="#B52" class="usa-link" aria-describedby="B52">52</a>], spatial neglect [<a href="#B53" class="usa-link" aria-describedby="B53">53</a>] or different types of neurodegenerative disease [<a href="#B54" class="usa-link" aria-describedby="B54">54</a>,<a href="#B55" class="usa-link" aria-describedby="B55">55</a>]. Furthermore, systematic differences in drawing tasks have been described for individuals with psychiatric disorders, including autism spectrum disorder [<a href="#B56" class="usa-link" aria-describedby="B56">56</a>] and schizophrenia [<a href="#B57" class="usa-link" aria-describedby="B57">57</a>]. These two conditions are associated with compromised predictive processing [<a href="#B58" class="usa-link" aria-describedby="B58">58</a>,<a href="#B59" class="usa-link" aria-describedby="B59">59</a>], suggesting that alterations to drawings in these disorders may be linked to alterations in internal models of the world.</p>
<p>In a similar vein, drawings have proven extremely useful for studying internal models across development. For instance, drawings have been used to assess the emergence of detail in visual representations of objects [<a href="#B60" class="usa-link" aria-describedby="B60">60</a>–<a href="#B62" class="usa-link" aria-describedby="B62">62</a>]. In recent work, Long <em>et al</em>. [<a href="#B61" class="usa-link" aria-describedby="B61">61</a>] asked children across different ages to draw various everyday objects, revealing characteristic changes in drawings across development (<a href="#F3" class="usa-link">figure 3a</a>). Interestingly, the content of children’s drawings predicted their ability to recognize visual objects [<a href="#B62" class="usa-link" aria-describedby="B62">62</a>], suggesting a link between the mental representation revealed by drawings and the ability of the visual system to process critical object details. Together, these findings indicate that drawings mirror the visual system’s ability to efficiently represent visual inputs.</p>
<figure class="fig xbox font-sm" id="F3" title="Using drawings to describe representations in development, memory, and perception."><h5 class="obj_head">Figure 3. </h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12364571_rspb.2025.0602.f003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5256/12364571/e0a5297b4bec/rspb.2025.0602.f003.jpg" loading="lazy" height="692" width="735" alt="Using drawings to describe representations in development, memory, and perception."></a></p>
<div class="p text-right font-secondary"><a href="figure/F3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Using drawings to describe representations in development, memory and perception. (a) In developmental research, the use of drawings allows researchers to gain insights into the emergence of detailed visual object representations (figure is reproduced from [<a href="#B62" class="usa-link" aria-describedby="B62">62</a>] under a CC BY 4.0 license). (b) In memory research, drawings can be used to quantify memory precision in free recall paradigms. For instance, in scenes with inconsistent objects, more detail about the inconsistent object is recalled, at the expense of recalling details of the scene (figure reproduced from [<a href="#B63" class="usa-link" aria-describedby="B63">63</a>]). (c) Using a similar free recall paradigm, Bainbridge &amp; Baker [<a href="#B64" class="usa-link" aria-describedby="B64">64</a>] showed that scene boundaries are extended or compressed in memory, depending on the viewpoint and geometry of the original scene. (d) In perception research, drawings were used to probe the cortical filling-in of missing information. Participants’ drawings of what should be present in an occluded quadrant predict neural activation: response patterns in areas of primary visual cortex (V1) that respond to the occluded quadrant are well explained by visual low-level visual features of these drawings [<a href="#B65" class="usa-link" aria-describedby="B65">65</a>].</p></figcaption></figure></section><section id="s3-2-2"><h4 class="pmc_sec_title">(ii). Line drawings in studying memory</h4>
<p>The use of drawings for studying internal representations in healthy adults dates back around a century. Metzger [<a href="#B66" class="usa-link" aria-describedby="B66">66</a>] described how participants made systematic drawing errors when copying line drawings under unfavourable conditions such as low contrast, miniature size or peripheral presentation. These reproductions tended to appear more unitary, regular and tightly structured than the originals, suggesting a normalization process towards internal perceptual templates.</p>
<p>More recently, line drawings received renewed attention in the memory literature, enabling unconstrained free-recall tasks on complex scenes [<a href="#B46" class="usa-link" aria-describedby="B46">46</a>]. For instance, Bainbridge <em>et al.</em> [<a href="#B67" class="usa-link" aria-describedby="B67">67</a>] uncovered a surprising amount of detail in memory drawings of scenes and objects. The precision of memory drawings varies as a function of scene typicality: semantically inconsistent objects are remembered more vividly than semantically consistent objects—at the expense of weaker memory for other scene characteristics in the incongruent images [<a href="#B63" class="usa-link" aria-describedby="B63">63</a>] (<a href="#F3" class="usa-link">figure 3b</a>). Memory drawings also reveal the degree of boundary extension in natural scene images. In a large-scale analysis, Bainbridge &amp; Baker [<a href="#B64" class="usa-link" aria-describedby="B64">64</a>] demonstrated that this effect is ultimately stimulus-dependent, with the tendency for boundaries to be extended for scenes perceived as near and compressed for those perceived as distant (<a href="#F4" class="usa-link">figure 4c</a>), interpreted as a normalization towards a typical viewing distance. In this trade-off, the depth of field plays a critical role, such that a naturalistic depth of field leads to a larger boundary extension than a non-naturalistic depth of field [<a href="#B69" class="usa-link" aria-describedby="B69">69</a>]. Together, these findings further illustrate how drawings can help to understand the intricacies of human memory representations.</p>
<figure class="fig xbox font-sm" id="F4" title="Using drawings to link individual differences in internal models to idiosyncrasies in perception"><h5 class="obj_head">Figure 4. </h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12364571_rspb.2025.0602.f004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5256/12364571/3560ba725a43/rspb.2025.0602.f004.jpg" loading="lazy" height="396" width="760" alt="Using drawings to link individual differences in internal models to idiosyncrasies in perception"></a></p>
<div class="p text-right font-secondary"><a href="figure/F4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Using drawings to link individual differences in internal models to idiosyncrasies in perception. (a) To assess the contents of internal models for real-world scenes, participants drew typical versions of scene categories (here: living rooms). (b) These drawings were converted to three-dimensional renders to control for visual differences. (c) During the subsequent categorization task, participants categorized briefly presented renders. Critically, they viewed renders based on their own drawings (‘own’ condition), other participants’ drawings (‘other’ condition) or renders created from scenes participants previously copied from a photograph (‘control’ condition, designed to control for drawing-related familiarity effects). Across two experiments with two (left) or six (right) scene categories, participants more accurately categorized renders from the ‘own’ condition than renders from the ‘other’ or ‘control’ conditions, suggesting that similarity to internal models on the individual level modulates scene processing in idiosyncratic ways [<a href="https://www.zotero.org/google-docs/?wEejOz" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">68</a>].</p></figcaption></figure></section><section id="s3-2-3"><h4 class="pmc_sec_title">(iii). Line drawings in visual perception</h4>
<p>In the study of perception, drawings have been used more sparingly. Yet, they are a promising method for studying predictive processes. In predictive processing frameworks, perception is often described as a generative model in which a percept is constructed by employing the internal world model to infer the most likely cause of sensory input [<a href="#B1" class="usa-link" aria-describedby="B1">1</a>]. Drawings offer researchers a tool to access these hypotheses by serving as an extension of the generative process into a visible format.</p>
<p>In line with this idea, drawings have been used to capture representations of invisible but predictable visual content in the visual cortex [<a href="#B65" class="usa-link" aria-describedby="B65">65</a>]. In this study, participants viewed natural scene images, in which one quadrant was occluded while functional magnetic resonance imaging (fMRI) activity was recorded from areas of the early visual cortex that exclusively respond to input from the occluded quadrant. Outside the scanner, participants were asked to draw the likely content of the occluded quadrant. Results showed that basic visual features of the drawings predicted scene-specific activations in the unstimulated area of the primary visual cortex (<a href="#F4" class="usa-link">figure 4d</a>). This finding highlights that drawings can be used to capture the content of neural predictions.</p>
<p>More recently, Wang <em>et al.</em> [<a href="#B68" class="usa-link" aria-describedby="B68">68</a>] used drawings as a readout of individual participants’ internal models of visual scenes (<a href="#F4" class="usa-link">figure 4</a>). Here, participants were asked to draw typical versions of a set of natural scene categories (e.g. kitchens or living rooms). These drawings were converted into standardized three-dimensional renders to control for different drawing abilities and styles. In a subsequent categorization task, participants were more accurate in categorizing renders that were constructed from their <em>own</em> drawings (and were thus more similar to their <em>own</em> internal models) than in categorizing renders based on <em>other</em> participants’ drawings (which were more dissimilar to their <em>own</em> internal models). The authors further showed that the similarity to the scene renders based on participants’ own drawings (measured by a deep neural network model) predicted categorization accuracy on other rendered scenes. This result demonstrates how drawings can be used to make personalized predictions about the efficiency of perception—derived from only a single drawing of a typical scene. Complementary EEG work [<a href="#B70" class="usa-link" aria-describedby="B70">70</a>] showed that neural representations of scenes that are similar to participants’ drawings (and thus their internal models) are enhanced during perceptual processing unfolding in the initial 250 ms of visual analysis. This suggests a rapid interaction between the visual input and an observer’s internal model during natural vision, underscoring the importance of idiosyncratic representations in scene perception. Although the current results are based on a recognition task, we believe that other behaviours are also shaped by an individual’s internal models. This opens new avenues for research into inter-individual variability in other tasks, for example, how individuals navigate through a scene or search for an object.</p>
<p>However, drawing methods also come with limitations, such as the challenge of objectively quantifying the contents of drawings and handling the substantial inter-subject variability in drawing abilities and style. Variation in drawing expertise has been associated with inter-individual differences in cognitive and perceptual abilities such as visual imagery, shape encoding and detection, as well as the allocation of visual attention and working memory [<a href="#B71" class="usa-link" aria-describedby="B71">71</a>–<a href="#B74" class="usa-link" aria-describedby="B74">74</a>]. Different approaches to minimizing such potentially confounding factors have been put forward: sufficient sample sizes and suitable control conditions can mitigate variance related to drawing abilities. Further, innovative methods like pen-tracking, computer vision or online crowdsourcing provide objective and reproducible tools for quantifying drawings [<a href="#B46" class="usa-link" aria-describedby="B46">46</a>,<a href="#B75" class="usa-link" aria-describedby="B75">75</a>].</p>
<p>In sum, drawings have proven a powerful tool for describing internal representations and have advanced our understanding of the precision of visual memory, the development of visual object representation and the perception and neural representations of scenes. Nevertheless, the methodology of drawings comes with limitations. In the following, we discuss three alternative methods for characterizing the contents of internal models, which offer complementary strengths.</p></section></section><section id="s3-3"><h3 class="pmc_sec_title">(c). Alternative descriptors of internal models</h3>
<p>We highlight three alternative ways of assessing the contents of internal models. The first two approaches (scene arrangement and linguistic descriptions) build on a similar principle as drawings by enabling participants to describe their internal model using physical or virtual objects, as well as language. Finally, we highlight an approach that attempts to directly infer characteristics of subjects’ internal models from brain recordings without relying on overt reports.</p>
<section id="s3-3-1"><h4 class="pmc_sec_title">(i). Scene arrangement</h4>
<p>In scene arrangement paradigms, participants create a scene by arranging a set of candidate objects provided by the experimenter. Although such methods often limit participants’ degrees of freedom in describing their internal models (e.g. because of fixed object exemplars available for arrangement), they mitigate the issue of inter-individual variability in drawings.</p>
<p>Scene arrangement tasks can be realized with real physical objects. For example, Öhlschläger &amp; Võ [<a href="#B76" class="usa-link" aria-describedby="B76">76</a>] used an arrangement task in a doll house to test how children across different age groups honour semantic relationships (e.g. chairs and tables appear together in a dining room) or spatial regularities (e.g. chairs face the dining table) between objects (<a href="#F5" class="usa-link">figure 5a</a>). Results showed that children as young as 3 years respected semantic relationships but not spatial regularities among multiple related objects, while children over 4 years arranged the objects in semantically and spatially congruent ways. Using the same task, Bahn <em>et al.</em> [<a href="#B79" class="usa-link" aria-describedby="B79">79</a>] showed that the performance of scene arrangement tasks covaries with language development, suggesting a link between the linguistic concept organization and the visual rules that structure natural scenes.</p>
<figure class="fig xbox font-sm" id="F5" title="Using explicit scene arrangement to describe internal models."><h5 class="obj_head">Figure 5. </h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12364571_rspb.2025.0602.f005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/5256/12364571/f039a0753da4/rspb.2025.0602.f005.jpg" loading="lazy" height="450" width="727" alt="Using explicit scene arrangement to describe internal models."></a></p>
<div class="p text-right font-secondary"><a href="figure/F5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Using explicit scene arrangement to describe internal models. (a) Children of different age groups were asked to arrange a set of miniature objects across a dollhouse. Object arrangements showed that children first appreciate semantic object similarities and only later incorporate the typical spatial organization across groups of objects [<a href="#B77" class="usa-link" aria-describedby="B77">77</a>]. (b) Participants arranged objects in a VR environment into typical or atypical configurations. In subsequent search and memory tasks, participants performed better when the task was situated in the scenes constructed in a typical fashion (figure reproduced from [<a href="#B78" class="usa-link" aria-describedby="B78">78</a>] under a CC BY 4.0 license).</p></figcaption></figure><p>Such real-world scene arrangement tasks, however, require real objects that need to be moved around in physical space. As an alternative, virtual reality (VR) allows for highly controlled, easily manipulable and interactive environments [<a href="#B80" class="usa-link" aria-describedby="B80">80</a>]. Showcasing this potential, Draschkow &amp; Võ [<a href="#B78" class="usa-link" aria-describedby="B78">78</a>] asked participants to construct scenes that concurred with their internal models of typical scenes (e.g. placing the objects in a kitchen in a typical fashion) or that violated them (e.g. placing the same objects in an atypical fashion) (<a href="#F5" class="usa-link">figure 5b</a>). In subsequent experiments, participants more successfully searched and memorized scenes arranged in typical ways, compared with scenes arranged in atypical ways. These results show how descriptors of internal models, as captured by explicit scene arrangement, can in turn be used to test perception and memory in environments that are specifically tailored to individual participants’ internal models.</p>
<p>Another promising variant of scene arrangement methods is adjustment procedures, where participants linearly vary scenes along one or more dimensions to match their internal model. Such approaches have, for instance, been used for matching colour expectations [<a href="#B81" class="usa-link" aria-describedby="B81">81</a>] or peripheral appearance [<a href="#B82" class="usa-link" aria-describedby="B82">82</a>]. For scenes, such adjustments can easily be realized for low-level properties such as contrast or colour. Yet, high-level properties can also be adjusted through continuous scene spaces derived from generative computational models, as for instance implemented in the ‘scene wheel’ [<a href="#B83" class="usa-link" aria-describedby="B83">83</a>], which provides seamless continua between individual scene exemplars. Such adjustment methods, however, depend on the experimenter’s choice of dimensions, but may strike a balance between experimental control and participant-driven insights into internal model properties.</p></section><section id="s3-3-2"><h4 class="pmc_sec_title">(ii). Linguistic descriptions</h4>
<p>Focusing more directly on conceptual rather than visual attributes of scenes, linguistic descriptions of scenes offer another effective tool that is independent of sensory-motor demands. Such reports are comparably easy to obtain, and they can carry rich semantic detail. Yet, they may be less precise in capturing some of the aspects that drawings convey (e.g. spatial organization).</p>
<p>When asked to describe scene images, our linguistic descriptions are tightly linked to our perception of scenes. For example, atypical scenes are harder to describe after brief exposure compared with typical scenes [<a href="#B84" class="usa-link" aria-describedby="B84">84</a>]. Furthermore, linguistic descriptions are constrained by individual differences in the ways we explore scenes [<a href="#B85" class="usa-link" aria-describedby="B85">85</a>]: inter-subject similarities in fixation patterns during free viewing can be predicted by inter-subject similarities in subsequent scene descriptions. For instance, participants who mentioned people more often in their descriptions also looked at people more prominently during exploration, and participants who mentioned text more often spent more time looking at text. This finding highlights the potential of linguistic descriptors to capture information about scene representations on the individual level.</p>
<p>Future studies could employ linguistic descriptions to gauge the contents of internal models directly, revealing the conceptual factors that organize our priors. Specifically, similar to the paradigm used by Wang <em>et al.</em> [<a href="#B68" class="usa-link" aria-describedby="B68">68</a>] wherein participants were asked to draw typical versions of a set of natural scene categories and then tested on categorization for scenes similar or dissimilar to these drawings, participants could alternatively provide linguistic descriptions of what they think a typical exemplar of a specific scene category should look like. The similarity of any given scene image with the linguistic descriptions provided by individual participants could, in turn, be used to predict perceptual efficiency or neural responses for these scenes on the individual level. Alternatively, generative text-to-image models could be used to generate stimulus materials that are in accordance with, or deviate from, individual participants’ linguistic descriptions. However, the outputs of generative models are influenced by the model’s own priors acquired throughout training. The model’s priors may be difficult to disentangle from the participant’s internal model.</p></section><section id="s3-3-3"><h4 class="pmc_sec_title">(iii). Neural quantification of internal models</h4>
<p>As an alternative to behavioural reports, the content of participants’ internal models could, in principle, be read out from neural responses. This is an exciting future prospect because brain activity recorded during simple visual tasks or passive fixation is potentially less prone to subjective biases and task-specific demand characteristics.</p>
<p>Studies linking scene typicality to neural responses are the foundation for this idea. Scenes that align with internal models strongly produce more diagnostic, ‘sharpened’ neural responses [<a href="#B86" class="usa-link" aria-describedby="B86">86</a>]. This variation in visual responses can be used to derive ‘neural prototypes’ of a given category in individual participants, for instance, by averaging responses to a larger set of exemplars into a prototypical response [<a href="#B87" class="usa-link" aria-describedby="B87">87</a>] or by comparing an exemplar’s activity pattern to all other members of that category, following the premise that a typical exemplar shares more attributes with other members of its category than atypical exemplars [<a href="#B88" class="usa-link" aria-describedby="B88">88</a>]. However, more work is needed to characterize how typicality is processed in the brain before neuroimaging-based methods can be used to read out a person’s internal model with confidence.</p>
<p>Moreover, neuroimaging-based approaches remain inherently constrained by the experimenter’s selection of stimulus materials, which may not cover natural visual distributions sufficiently and in unbiased ways. To mitigate this constraint, large and diverse stimulus sets that feature thousands of scene instances, such as the Natural Scenes Dataset (NSD; [<a href="#B89" class="usa-link" aria-describedby="B89">89</a>]), provide useful benchmarks. A promising direction for future research is to combine neural measures of typicality with free production approaches, such as drawings or scene arrangement, thereby leveraging their complementary strengths.</p></section></section></section><section id="s4"><h2 class="pmc_sec_title">4.  Challenges in describing internal models</h2>
<p>The methods described above enable researchers to infer the content of internal models with fewer constraints and prior assumptions about their properties than classical approaches. However, these methods also come with their own challenges. We highlight three challenges below and outline what we gain from solving them.</p>
<p>First, behavioural descriptions of internal models, such as drawings or linguistic reports, are subjective reports. Can we assume that such introspective insights are reliable? Introspection is often disregarded as inherently problematic because observers may not be able to reliably characterize their internal representations [<a href="#B90" class="usa-link" aria-describedby="B90">90</a>,<a href="#B91" class="usa-link" aria-describedby="B91">91</a>]. However, this view has been challenged, most prominently by Gestalt psychologists [<a href="#B92" class="usa-link" aria-describedby="B92">92</a>], but also more recently [<a href="#B93" class="usa-link" aria-describedby="B93">93</a>,<a href="#B94" class="usa-link" aria-describedby="B94">94</a>], with proponents arguing that introspection offers converging information compared with analytic approaches. Further, the quality of introspective insight can be addressed empirically by quantifying whether introspective insights about internal models indeed predict the efficiency of perception. Our review, therefore, does not make the case to replace or overcome classical approaches to studying scene vision—we rather need to combine stimulus manipulation approaches with approaches for describing internal models.</p>
<p>Second, some of the outlined methods require separating informative differences from incidental variance across individuals. This is particularly relevant for drawings, where different drawing styles and abilities [<a href="#B95" class="usa-link" aria-describedby="B95">95</a>,<a href="#B96" class="usa-link" aria-describedby="B96">96</a>] introduce substantial variation that is not directly related to our internal representations. Moreover, methods like scene arrangement may bias the provided descriptions by offering a limited number of available objects. A careful choice of methods, control conditions, sample size and analysis approach is required to minimize these shortcomings and unfold the complementary strengths of these techniques. Additionally, participants may prioritize some objects over others during production. Reasons for this include biases towards larger and more salient objects (e.g. a sink is more often drawn than a toothbrush) or more scene-diagnostic objects (e.g. a socket is rarely drawn, given it is not diagnostic for a room category). Such biases may yield imperfect descriptors of internal models.</p>
<p>Third, although subjective reports such as drawings offer rich insights into internal models, they may not be ideal to clarify the complexity of features in the internal model. Recent work shows that individual differences in internal models are expressed in complex high-level features coded in late layers of deep neural network models [<a href="#B70" class="usa-link" aria-describedby="B70">70</a>], suggesting that complex object- and scene-related features structure our internal models. Yet, low-level properties may prominently feature in internal models, too, and drawings can capture some of such low-level regularities, such as visual density, texture continuity or edge alignment. Other low-level properties, like colour distributions or texture properties, are harder to capture with drawings and may elude investigation with drawing methods.</p>
<p>In addition, most of the methods highlighted here yield single descriptions of participants’ internal models, implying that there is a single, stable internal model for a given scene category. However, internal models may encompass multiple typical scene configurations (e.g. private versus public bathrooms). They may also be shaped by context (e.g. in the form of precision weighting [<a href="#B1" class="usa-link" aria-describedby="B1">1</a>]), recently formed associations (e.g. through serial dependence effects [<a href="#B97" class="usa-link" aria-describedby="B97">97</a>]) and behavioural goals [<a href="#B98" class="usa-link" aria-describedby="B98">98</a>]. This requires experiments that repeatedly quantify the contents of internal models within the same participants, thereby characterizing how internal models change across time and context.</p></section><section id="s5"><h2 class="pmc_sec_title">5.  Conclusion</h2>
<p>We showed that classical approaches to characterizing internal models of natural scenes through stimulus manipulation have notable limitations, including an over-reliance on <em>a priori</em> assumptions about scene typicality, restricted possibilities for stimulus manipulation, the use of artificial control stimuli and insensitivity to inter-individual differences. We therefore highlight a complementary methodological framework that allows for more unconstrained descriptors of participants’ internal models. One promising method is the use of line drawings, which has recently opened new avenues in the study of visual memory and perception. Overall, we believe that natural vision research greatly benefits from methods with fewer constraints and prior assumptions about the nature of internal models, complementing traditional approaches. Although we focused on characterizing internal models for natural scenes, we believe that approaches targeting the contents of internal models can be very informative in other visual domains. Indeed, drawings have been used for understanding mental generalization in object recognition [<a href="#B99" class="usa-link" aria-describedby="B99">99</a>,<a href="#B100" class="usa-link" aria-describedby="B100">100</a>], perceptual distortions in peripheral vision [<a href="#B101" class="usa-link" aria-describedby="B101">101</a>] and individual differences in mental imagery [<a href="#B102" class="usa-link" aria-describedby="B102">102</a>]. Embracing this approach could yield novel insights into how internal models differently shape perception across individuals and across cultural and linguistic contexts, and how alterations of internal models drive changes in visual processing across the lifespan and from health to disease.</p></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>We thank Melissa Võ for providing materials for <a href="#F4" class="usa-link">figure 4</a>.</p></section><section id="_ci93_" lang="en" class="contrib-info"><h2 class="pmc_sec_title">Contributor Information</h2>
<p>Micha Engeser, Email: michaengeser@gmail.com.</p>
<p>Susan Ajith, Email: susan.ajith@psychiat.med.uni-giessen.de.</p>
<p>Ilker Duymaz, Email: ilkrdymz@gmail.com.</p>
<p>Gongting Wang, Email: generalwgt@gmail.com.</p>
<p>Matthew J Foxwell, Email: matt.foxwell@york.ac.uk.</p>
<p>Radoslaw M Cichy, Email: rmcichy@googlemail.com.</p>
<p>David Pitcher, Email: david.pitcher@york.ac.uk.</p>
<p>Daniel Kaiser, Email: danielkaiser.net@gmail.com.</p></section><section id="s6"><h2 class="pmc_sec_title">Ethics</h2>
<p>This work did not require ethical approval from a human subject or animal welfare committee.</p></section><section id="s7"><h2 class="pmc_sec_title">Declaration of AI use</h2>
<p>We have not used AI-assisted technologies in creating this article.</p></section><section id="s8"><h2 class="pmc_sec_title">Authors’ contributions</h2>
<p>M.E.: conceptualization, writing—original draft, writing—review and editing; S.A.: conceptualization, writing—original draft, writing—review and editing; I.D.: conceptualization, writing—original draft, writing—review and editing; G.W.: writing—original draft, writing—review and editing; M.J.F.: conceptualization, writing—original draft, writing—review and editing; R.M.C.: supervision, writing—review and editing; D.P.: supervision, writing—review and editing; D.K.: conceptualization, funding acquisition, project administration, supervision, visualization, writing—original draft, writing—review and editing.</p>
<p>All authors gave final approval for publication and agreed to be held accountable for the work performed therein.</p></section><section id="s9"><h2 class="pmc_sec_title">Conflict of interest declaration</h2>
<p>We declare we have no competing interests.</p></section><section id="s10"><h2 class="pmc_sec_title">Funding</h2>
<p>This work was supported by the Deutsche Forschungsgemeinschaft (German Research Foundation, DFG) under Germany’s Excellence Strategy (EXC 3066/1 'The Adaptive Mind', project no. 533717223) and by an ERC Starting grant (PEP, ERC-2022-STG 101076057; awarded to D.K.). Views and opinions expressed are those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="B1">
<span class="label">1.</span><cite>
Clark A. 2013. 
Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behav. Brain Sci.
36, 181–204. ( 10.1017/S0140525X12000477)
</cite> [<a href="https://doi.org/10.1017/S0140525X12000477" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23663408/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Behav.%20Brain%20Sci.&amp;title=Whatever%20next?%20Predictive%20brains,%20situated%20agents,%20and%20the%20future%20of%20cognitive%20science&amp;volume=36&amp;publication_year=2013&amp;pages=181-204&amp;pmid=23663408&amp;doi=10.1017/S0140525X12000477&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B2">
<span class="label">2.</span><cite>
de Lange FP, Heilbron M, Kok P. 2018. 
How do expectations shape perception?
Trends Cogn. Sci.
22, 764–779. ( 10.1016/j.tics.2018.06.002)
</cite> [<a href="https://doi.org/10.1016/j.tics.2018.06.002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30122170/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trends%20Cogn.%20Sci.&amp;title=How%20do%20expectations%20shape%20perception?&amp;volume=22&amp;publication_year=2018&amp;pages=764-779&amp;pmid=30122170&amp;doi=10.1016/j.tics.2018.06.002&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B3">
<span class="label">3.</span><cite>
Kayser C, Körding KP, König P. 2004. 
Processing of complex stimuli and natural scenes in the visual cortex. Curr. Opin. Neurobiol.
14, 468–473. ( 10.1016/j.conb.2004.06.002)
</cite> [<a href="https://doi.org/10.1016/j.conb.2004.06.002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/15302353/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Curr.%20Opin.%20Neurobiol.&amp;title=Processing%20of%20complex%20stimuli%20and%20natural%20scenes%20in%20the%20visual%20cortex&amp;volume=14&amp;publication_year=2004&amp;pages=468-473&amp;pmid=15302353&amp;doi=10.1016/j.conb.2004.06.002&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B4">
<span class="label">4.</span><cite>
Mirza MB, Adams RA, Mathys CD, Friston KJ. 2016. 
Scene construction, visual foraging, and active inference. Front. Comput. Neurosci.
10, 56. ( 10.3389/fncom.2016.00056)
</cite> [<a href="https://doi.org/10.3389/fncom.2016.00056" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4906014/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27378899/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Comput.%20Neurosci.&amp;title=Scene%20construction,%20visual%20foraging,%20and%20active%20inference&amp;volume=10&amp;publication_year=2016&amp;pages=56&amp;pmid=27378899&amp;doi=10.3389/fncom.2016.00056&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B5">
<span class="label">5.</span><cite>
Bar M. 2004. 
Visual objects in context. Nat. Rev. Neurosci.
5, 617–629. ( 10.1038/nrn1476)
</cite> [<a href="https://doi.org/10.1038/nrn1476" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/15263892/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Rev.%20Neurosci.&amp;title=Visual%20objects%20in%20context&amp;volume=5&amp;publication_year=2004&amp;pages=617-629&amp;pmid=15263892&amp;doi=10.1038/nrn1476&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B6">
<span class="label">6.</span><cite>
Kaiser D, Quek GL, Cichy RM, Peelen MV. 2019. 
Object vision in a structured world. Trends Cogn. Sci.
23, 672–685. ( 10.1016/j.tics.2019.04.013)
</cite> [<a href="https://doi.org/10.1016/j.tics.2019.04.013" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7612023/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31147151/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trends%20Cogn.%20Sci.&amp;title=Object%20vision%20in%20a%20structured%20world&amp;volume=23&amp;publication_year=2019&amp;pages=672-685&amp;pmid=31147151&amp;doi=10.1016/j.tics.2019.04.013&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B7">
<span class="label">7.</span><cite>
Kaiser D, Cichy RM. 2021. 
Parts and wholes in scene processing. J. Cogn. Neurosci.
34, 4–15. ( 10.1162/jocn_a_01788)
</cite> [<a href="https://doi.org/10.1162/jocn_a_01788" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34705031/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Cogn.%20Neurosci.&amp;title=Parts%20and%20wholes%20in%20scene%20processing&amp;volume=34&amp;publication_year=2021&amp;pages=4-15&amp;pmid=34705031&amp;doi=10.1162/jocn_a_01788&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B8">
<span class="label">8.</span><cite>
Oliva A, Torralba A. 2007. 
The role of context in object recognition. Trends Cogn. Sci.
11, 520–527. ( 10.1016/j.tics.2007.09.009)
</cite> [<a href="https://doi.org/10.1016/j.tics.2007.09.009" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/18024143/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trends%20Cogn.%20Sci.&amp;title=The%20role%20of%20context%20in%20object%20recognition&amp;volume=11&amp;publication_year=2007&amp;pages=520-527&amp;pmid=18024143&amp;doi=10.1016/j.tics.2007.09.009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B9">
<span class="label">9.</span><cite>
Võ MH. 2021. 
The meaning and structure of scenes. Vision Res.
181, 10–20. ( 10.1016/j.visres.2020.11.003)
</cite> [<a href="https://doi.org/10.1016/j.visres.2020.11.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33429218/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Vision%20Res.&amp;title=The%20meaning%20and%20structure%20of%20scenes&amp;volume=181&amp;publication_year=2021&amp;pages=10-20&amp;pmid=33429218&amp;doi=10.1016/j.visres.2020.11.003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B10">
<span class="label">10.</span><cite>
Võ MH, Boettcher SE, Draschkow D. 2019. 
Reading scenes: how scene grammar guides attention and aids perception in real-world environments. Curr. Opin. Psychol.
29, 205–210. ( 10.1016/j.copsyc.2019.03.009)
</cite> [<a href="https://doi.org/10.1016/j.copsyc.2019.03.009" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31051430/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Curr.%20Opin.%20Psychol.&amp;title=Reading%20scenes:%20how%20scene%20grammar%20guides%20attention%20and%20aids%20perception%20in%20real-world%20environments&amp;volume=29&amp;publication_year=2019&amp;pages=205-210&amp;pmid=31051430&amp;doi=10.1016/j.copsyc.2019.03.009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B11">
<span class="label">11.</span><cite>
Von Helmholtz H. 1867. 
Treatise on physiological optics. vol. III. Garden City, New York, USA: Dover Publications.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Treatise%20on%20physiological%20optics&amp;publication_year=1867&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B12">
<span class="label">12.</span><cite>
Mandler JM. 1984. 
Stories, scripts, and scenes: aspects of schema theory. New York, NY, USA: Psychology Press. ( 10.4324/9781315802459)</cite> [<a href="https://doi.org/10.4324/9781315802459" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Stories,%20scripts,%20and%20scenes:%20aspects%20of%20schema%20theory&amp;publication_year=1984&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B13">
<span class="label">13.</span><cite>
Minsky M. 1974. 
A framework for representing knowledge. Cambridge, MA, USA: MIT. See
<a href="https://dspace.mit.edu/bitstream/handle/1721.1/6089/AIM-306.pdf?%2520sequence%3D2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://dspace.mit.edu/bitstream/handle/1721.1/6089/AIM-306.pdf?%2520sequence%3D2</a>.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=A%20framework%20for%20representing%20knowledge&amp;publication_year=1974&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B14">
<span class="label">14.</span><cite>
Biederman I. 1972. 
Perceiving real-world scenes. Science
177, 77–80. ( 10.1126/science.177.4043.77)
</cite> [<a href="https://doi.org/10.1126/science.177.4043.77" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/5041781/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Science&amp;title=Perceiving%20real-world%20scenes&amp;volume=177&amp;publication_year=1972&amp;pages=77-80&amp;pmid=5041781&amp;doi=10.1126/science.177.4043.77&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B15">
<span class="label">15.</span><cite>
Biederman I, Mezzanotte RJ, Rabinowitz JC. 1982. 
Scene perception: detecting and judging objects undergoing relational violations. Cognit. Psychol.
14, 143–177. ( 10.1016/0010-0285(82)90007-X)
</cite> [<a href="https://doi.org/10.1016/0010-0285(82)90007-X" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/7083801/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cognit.%20Psychol.&amp;title=Scene%20perception:%20detecting%20and%20judging%20objects%20undergoing%20relational%20violations&amp;volume=14&amp;publication_year=1982&amp;pages=143-177&amp;pmid=7083801&amp;doi=10.1016/0010-0285(82)90007-X&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B16">
<span class="label">16.</span><cite>
Brewer WF, Treyens JC. 1981. 
Role of schemata in memory for places. Cognit. Psychol.
13, 207–230. ( 10.1016/0010-0285(81)90008-6)</cite> [<a href="https://doi.org/10.1016/0010-0285(81)90008-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cognit.%20Psychol.&amp;title=Role%20of%20schemata%20in%20memory%20for%20places&amp;volume=13&amp;publication_year=1981&amp;pages=207-230&amp;doi=10.1016/0010-0285(81)90008-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B17">
<span class="label">17.</span><cite>
Mandler JM, Parker RE. 1976. 
Memory for descriptive and spatial information in complex pictures. J. Exp. Psychol.
38–48. ( 10.1037/0278-7393.2.1.38)</cite> [<a href="https://doi.org/10.1037/0278-7393.2.1.38" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/1249532/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Exp.%20Psychol.&amp;title=Memory%20for%20descriptive%20and%20spatial%20information%20in%20complex%20pictures&amp;publication_year=1976&amp;pages=38-48&amp;pmid=1249532&amp;doi=10.1037/0278-7393.2.1.38&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B18">
<span class="label">18.</span><cite>
Bar M. 2009. 
The proactive brain: memory for predictions. Phil. Trans. R. Soc. B
364, 1235–1243. ( 10.1098/rstb.2008.0310)
</cite> [<a href="https://doi.org/10.1098/rstb.2008.0310" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC2666710/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19528004/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Phil.%20Trans.%20R.%20Soc.%20B&amp;title=The%20proactive%20brain:%20memory%20for%20predictions&amp;volume=364&amp;publication_year=2009&amp;pages=1235-1243&amp;pmid=19528004&amp;doi=10.1098/rstb.2008.0310&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B19">
<span class="label">19.</span><cite>
Peelen MV, Berlot E, de Lange FP. 2024. 
Predictive processing of scenes and objects. Nat. Rev. Psychol.
3, 13–26. ( 10.1038/s44159-023-00254-0)
</cite> [<a href="https://doi.org/10.1038/s44159-023-00254-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7616164/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38989004/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Rev.%20Psychol.&amp;title=Predictive%20processing%20of%20scenes%20and%20objects&amp;volume=3&amp;publication_year=2024&amp;pages=13-26&amp;pmid=38989004&amp;doi=10.1038/s44159-023-00254-0&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B20">
<span class="label">20.</span><cite>
Henderson JM. 2017. 
Gaze control as prediction. Trends Cogn. Sci.
21, 15–23. ( 10.1016/j.tics.2016.11.003)
</cite> [<a href="https://doi.org/10.1016/j.tics.2016.11.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27931846/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trends%20Cogn.%20Sci.&amp;title=Gaze%20control%20as%20prediction&amp;volume=21&amp;publication_year=2017&amp;pages=15-23&amp;pmid=27931846&amp;doi=10.1016/j.tics.2016.11.003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B21">
<span class="label">21.</span><cite>
Kaiser D, Jacopo T, Cichy RM. 2019. 
A neural mechanism for contextualizing fragmented inputs during naturalistic vision. eLife
8. ( 10.7554/eLife.48182)</cite> [<a href="https://doi.org/10.7554/eLife.48182" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6802952/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31596234/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=eLife&amp;title=A%20neural%20mechanism%20for%20contextualizing%20fragmented%20inputs%20during%20naturalistic%20vision&amp;volume=8&amp;publication_year=2019&amp;pmid=31596234&amp;doi=10.7554/eLife.48182&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B22">
<span class="label">22.</span><cite>
Muckli L, De Martino F, Vizioli L, Petro LS, Smith FW, Ugurbil K, Goebel R, Yacoub E. 2015. 
Contextual feedback to superficial layers of V1. Curr. Biol.
25, 2690–2695. ( 10.1016/j.cub.2015.08.057)
</cite> [<a href="https://doi.org/10.1016/j.cub.2015.08.057" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4612466/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26441356/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Curr.%20Biol.&amp;title=Contextual%20feedback%20to%20superficial%20layers%20of%20V1&amp;volume=25&amp;publication_year=2015&amp;pages=2690-2695&amp;pmid=26441356&amp;doi=10.1016/j.cub.2015.08.057&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B23">
<span class="label">23.</span><cite>
Kaiser D, Cichy RM. 2018. 
Typical visual-field locations enhance processing in object-selective channels of human occipital cortex. J. Neurophysiol.
120, 848–853. ( 10.1152/jn.00229.2018)
</cite> [<a href="https://doi.org/10.1152/jn.00229.2018" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29766762/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Neurophysiol.&amp;title=Typical%20visual-field%20locations%20enhance%20processing%20in%20object-selective%20channels%20of%20human%20occipital%20cortex&amp;volume=120&amp;publication_year=2018&amp;pages=848-853&amp;pmid=29766762&amp;doi=10.1152/jn.00229.2018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B24">
<span class="label">24.</span><cite>
Bilalić M, Lindig T, Turella L. 2019. 
Parsing rooms: the role of the PPA and RSC in perceiving object relations and spatial layout. Brain Struct. Funct.
224, 2505–2524.
</cite> [<a href="https://doi.org/10.1007/s00429-019-01901-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6698272/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31317256/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Brain%20Struct.%20Funct.&amp;title=Parsing%20rooms:%20the%20role%20of%20the%20PPA%20and%20RSC%20in%20perceiving%20object%20relations%20and%20spatial%20layout&amp;volume=224&amp;publication_year=2019&amp;pages=2505-2524&amp;pmid=31317256&amp;doi=10.1007/s00429-019-01901-0&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B25">
<span class="label">25.</span><cite>
Munneke J, Brentari V, Peelen M. 2013. 
The influence of scene context on object recognition is independent of attentional focus. Front. Psychol
4, 1–10. ( 10.3389/fpsyg.2013.00552)
</cite> [<a href="https://doi.org/10.3389/fpsyg.2013.00552" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3748376/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23970878/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Psychol&amp;title=The%20influence%20of%20scene%20context%20on%20object%20recognition%20is%20independent%20of%20attentional%20focus&amp;volume=4&amp;publication_year=2013&amp;pages=1-10&amp;pmid=23970878&amp;doi=10.3389/fpsyg.2013.00552&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B26">
<span class="label">26.</span><cite>
Kaiser D, Häberle G, Cichy RM. 2021. 
Coherent natural scene structure facilitates the extraction of task-relevant object information in visual cortex. Neuroimage
240, 118365. ( 10.1016/j.neuroimage.2021.118365)
</cite> [<a href="https://doi.org/10.1016/j.neuroimage.2021.118365" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8456750/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34233220/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neuroimage&amp;title=Coherent%20natural%20scene%20structure%20facilitates%20the%20extraction%20of%20task-relevant%20object%20information%20in%20visual%20cortex&amp;volume=240&amp;publication_year=2021&amp;pages=118365&amp;pmid=34233220&amp;doi=10.1016/j.neuroimage.2021.118365&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B27">
<span class="label">27.</span><cite>
Kaiser D, Moeskops MM, Cichy RM. 2018. 
Typical retinotopic locations impact the time course of object coding. Neuroimage
176, 372–379. ( 10.1016/j.neuroimage.2018.05.006)
</cite> [<a href="https://doi.org/10.1016/j.neuroimage.2018.05.006" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29733954/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neuroimage&amp;title=Typical%20retinotopic%20locations%20impact%20the%20time%20course%20of%20object%20coding&amp;volume=176&amp;publication_year=2018&amp;pages=372-379&amp;pmid=29733954&amp;doi=10.1016/j.neuroimage.2018.05.006&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B28">
<span class="label">28.</span><cite>
Kaiser D, Cichy RM. 2018. 
Typical visual-field locations facilitate access to awareness for everyday objects. Cognition
180, 118–122. ( 10.1016/j.cognition.2018.07.009)
</cite> [<a href="https://doi.org/10.1016/j.cognition.2018.07.009" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30029067/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cognition&amp;title=Typical%20visual-field%20locations%20facilitate%20access%20to%20awareness%20for%20everyday%20objects&amp;volume=180&amp;publication_year=2018&amp;pages=118-122&amp;pmid=30029067&amp;doi=10.1016/j.cognition.2018.07.009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B29">
<span class="label">29.</span><cite>
Gronau N, Shachar M. 2014. 
Contextual integration of visual objects necessitates attention. Atten. Percept. Psychophys.
76, 695–714. ( 10.3758/s13414-013-0617-8)
</cite> [<a href="https://doi.org/10.3758/s13414-013-0617-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24448697/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Atten.%20Percept.%20Psychophys.&amp;title=Contextual%20integration%20of%20visual%20objects%20necessitates%20attention&amp;volume=76&amp;publication_year=2014&amp;pages=695-714&amp;pmid=24448697&amp;doi=10.3758/s13414-013-0617-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B30">
<span class="label">30.</span><cite>
Kaiser D, Stein T, Peelen MV. 2014. 
Object grouping based on real-world regularities facilitates perception by reducing competitive interactions in visual cortex. Proc. Natl Acad. Sci. USA
111, 11217–11222. ( 10.1073/pnas.1400559111)
</cite> [<a href="https://doi.org/10.1073/pnas.1400559111" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4121846/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25024190/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;title=Object%20grouping%20based%20on%20real-world%20regularities%20facilitates%20perception%20by%20reducing%20competitive%20interactions%20in%20visual%20cortex&amp;volume=111&amp;publication_year=2014&amp;pages=11217-11222&amp;pmid=25024190&amp;doi=10.1073/pnas.1400559111&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B31">
<span class="label">31.</span><cite>
Kaiser D, Peelen MV. 2018. 
Transformation from independent to integrative coding of multi-object arrangements in human visual cortex. Neuroimage
169, 334–341. ( 10.1016/j.neuroimage.2017.12.065)
</cite> [<a href="https://doi.org/10.1016/j.neuroimage.2017.12.065" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5857358/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29277645/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neuroimage&amp;title=Transformation%20from%20independent%20to%20integrative%20coding%20of%20multi-object%20arrangements%20in%20human%20visual%20cortex&amp;volume=169&amp;publication_year=2018&amp;pages=334-341&amp;pmid=29277645&amp;doi=10.1016/j.neuroimage.2017.12.065&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B32">
<span class="label">32.</span><cite>
Kim JG, Biederman I. 2011. 
Where do objects become scenes?
Cereb. Cortex
21, 1738–1746. ( 10.1093/cercor/bhq240)
</cite> [<a href="https://doi.org/10.1093/cercor/bhq240" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3138508/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21148087/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cereb.%20Cortex&amp;title=Where%20do%20objects%20become%20scenes?&amp;volume=21&amp;publication_year=2011&amp;pages=1738-1746&amp;pmid=21148087&amp;doi=10.1093/cercor/bhq240&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B33">
<span class="label">33.</span><cite>
Chen L, Cichy RM, Kaiser D. 2022. 
Semantic scene-object consistency modulates N300/400 EEG components, but does not automatically facilitate object representations. Cereb. Cortex
32, 3553–3567. ( 10.1093/cercor/bhab433)
</cite> [<a href="https://doi.org/10.1093/cercor/bhab433" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34891169/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cereb.%20Cortex&amp;title=Semantic%20scene-object%20consistency%20modulates%20N300/400%20EEG%20components,%20but%20does%20not%20automatically%20facilitate%20object%20representations&amp;volume=32&amp;publication_year=2022&amp;pages=3553-3567&amp;pmid=34891169&amp;doi=10.1093/cercor/bhab433&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B34">
<span class="label">34.</span><cite>
Davenport JL, Potter MC. 2004. 
Scene consistency in object and background perception. Psychol. Sci.
15, 559–564. ( 10.1111/j.0956-7976.2004.00719.x)
</cite> [<a href="https://doi.org/10.1111/j.0956-7976.2004.00719.x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/15271002/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol.%20Sci.&amp;title=Scene%20consistency%20in%20object%20and%20background%20perception&amp;volume=15&amp;publication_year=2004&amp;pages=559-564&amp;pmid=15271002&amp;doi=10.1111/j.0956-7976.2004.00719.x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B35">
<span class="label">35.</span><cite>
Mudrik L, Lamy D, Deouell LY. 2010. 
ERP evidence for context congruity effects during simultaneous object–scene processing. Neuropsychologia
48, 507–517. ( 10.1016/j.neuropsychologia.2009.10.011)
</cite> [<a href="https://doi.org/10.1016/j.neuropsychologia.2009.10.011" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19837103/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neuropsychologia&amp;title=ERP%20evidence%20for%20context%20congruity%20effects%20during%20simultaneous%20object%E2%80%93scene%20processing&amp;volume=48&amp;publication_year=2010&amp;pages=507-517&amp;pmid=19837103&amp;doi=10.1016/j.neuropsychologia.2009.10.011&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B36">
<span class="label">36.</span><cite>
Võ MH, Wolfe JM. 2013. 
Differential electrophysiological signatures of semantic and syntactic scene processing. Psychol. Sci.
24, 1816–1823. ( 10.1177/0956797613476955)
</cite> [<a href="https://doi.org/10.1177/0956797613476955" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4838599/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23842954/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol.%20Sci.&amp;title=Differential%20electrophysiological%20signatures%20of%20semantic%20and%20syntactic%20scene%20processing&amp;volume=24&amp;publication_year=2013&amp;pages=1816-1823&amp;pmid=23842954&amp;doi=10.1177/0956797613476955&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B37">
<span class="label">37.</span><cite>
Biederman I, Rabinowitz JC, Glass AL, Stacy EW. 1974. 
On the information extracted from a glance at a scene. J. Exp. Psychol.
103, 597–600. ( 10.1037/h0037158)
</cite> [<a href="https://doi.org/10.1037/h0037158" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/4448962/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Exp.%20Psychol.&amp;title=On%20the%20information%20extracted%20from%20a%20glance%20at%20a%20scene&amp;volume=103&amp;publication_year=1974&amp;pages=597-600&amp;pmid=4448962&amp;doi=10.1037/h0037158&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B38">
<span class="label">38.</span><cite>
Kaiser D, Häberle G, Cichy RM. 2020. 
Cortical sensitivity to natural scene structure. Hum. Brain Mapp.
41, 1286–1295. ( 10.1002/hbm.24875)
</cite> [<a href="https://doi.org/10.1002/hbm.24875" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7267931/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31758632/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Hum.%20Brain%20Mapp.&amp;title=Cortical%20sensitivity%20to%20natural%20scene%20structure&amp;volume=41&amp;publication_year=2020&amp;pages=1286-1295&amp;pmid=31758632&amp;doi=10.1002/hbm.24875&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B39">
<span class="label">39.</span><cite>
Kaiser D, Häberle G, Cichy RM. 2020. 
Real-world structure facilitates the rapid emergence of scene category information in visual brain signals. J. Neurophysiol.
124, 145–151. ( 10.1152/jn.00164.2020)
</cite> [<a href="https://doi.org/10.1152/jn.00164.2020" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7474449/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32519577/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Neurophysiol.&amp;title=Real-world%20structure%20facilitates%20the%20rapid%20emergence%20of%20scene%20category%20information%20in%20visual%20brain%20signals&amp;volume=124&amp;publication_year=2020&amp;pages=145-151&amp;pmid=32519577&amp;doi=10.1152/jn.00164.2020&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B40">
<span class="label">40.</span><cite>
Bonner MF, Epstein RA. 2021. 
Object representations in the human brain reflect the co-occurrence statistics of vision and language. Nat. Commun.
12, 4081. ( 10.1038/s41467-021-24368-2)
</cite> [<a href="https://doi.org/10.1038/s41467-021-24368-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8253839/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34215754/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Commun.&amp;title=Object%20representations%20in%20the%20human%20brain%20reflect%20the%20co-occurrence%20statistics%20of%20vision%20and%20language&amp;volume=12&amp;publication_year=2021&amp;pages=4081&amp;pmid=34215754&amp;doi=10.1038/s41467-021-24368-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B41">
<span class="label">41.</span><cite>
Gregorová K, Turini J, Gagl B, Võ MH. 2023. 
Access to meaning from visual input: object and word frequency effects in categorization behavior. J. Exp. Psychol. Gen.
152, 2861–2881. ( 10.1037/xge0001342)
</cite> [<a href="https://doi.org/10.1037/xge0001342" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37155283/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Exp.%20Psychol.%20Gen.&amp;title=Access%20to%20meaning%20from%20visual%20input:%20object%20and%20word%20frequency%20effects%20in%20categorization%20behavior&amp;volume=152&amp;publication_year=2023&amp;pages=2861-2881&amp;pmid=37155283&amp;doi=10.1037/xge0001342&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B42">
<span class="label">42.</span><cite>
Spaak E, Peelen MV, De Lange FP. 2022. 
Scene context impairs perception of semantically congruent objects. Psychol. Sci.
33, 299–313. ( 10.1177/09567976211032676)
</cite> [<a href="https://doi.org/10.1177/09567976211032676" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35020519/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol.%20Sci.&amp;title=Scene%20context%20impairs%20perception%20of%20semantically%20congruent%20objects&amp;volume=33&amp;publication_year=2022&amp;pages=299-313&amp;pmid=35020519&amp;doi=10.1177/09567976211032676&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B43">
<span class="label">43.</span><cite>
Faivre N, Dubois J, Schwartz N, Mudrik L. 2019. 
Imaging object-scene relations processing in visible and invisible natural scenes. Sci. Rep.
9, 4567. ( 10.1038/s41598-019-38654-z)
</cite> [<a href="https://doi.org/10.1038/s41598-019-38654-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6418099/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30872607/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Imaging%20object-scene%20relations%20processing%20in%20visible%20and%20invisible%20natural%20scenes&amp;volume=9&amp;publication_year=2019&amp;pages=4567&amp;pmid=30872607&amp;doi=10.1038/s41598-019-38654-z&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B44">
<span class="label">44.</span><cite>
Barrett HC. 2020. 
Towards a cognitive science of the human: cross-cultural approaches and their urgency. Trends Cogn. Sci.
24, 620–638. ( 10.1016/j.tics.2020.05.007)
</cite> [<a href="https://doi.org/10.1016/j.tics.2020.05.007" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32534836/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trends%20Cogn.%20Sci.&amp;title=Towards%20a%20cognitive%20science%20of%20the%20human:%20cross-cultural%20approaches%20and%20their%20urgency&amp;volume=24&amp;publication_year=2020&amp;pages=620-638&amp;pmid=32534836&amp;doi=10.1016/j.tics.2020.05.007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B45">
<span class="label">45.</span><cite>
Hartley CA. 2022. 
How do natural environments shape adaptive cognition across the lifespan?
Trends Cogn. Sci.
26, 1029–1030. ( 10.1016/j.tics.2022.10.002)
</cite> [<a href="https://doi.org/10.1016/j.tics.2022.10.002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36272935/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trends%20Cogn.%20Sci.&amp;title=How%20do%20natural%20environments%20shape%20adaptive%20cognition%20across%20the%20lifespan?&amp;volume=26&amp;publication_year=2022&amp;pages=1029-1030&amp;pmid=36272935&amp;doi=10.1016/j.tics.2022.10.002&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B46">
<span class="label">46.</span><cite>
Fan JE, Bainbridge WA, Chamberlain R, Wammes JD. 2023. 
Drawing as a versatile cognitive tool. Nat. Rev. Psychol.
2, 556–568. ( 10.1038/s44159-023-00212-w)
</cite> [<a href="https://doi.org/10.1038/s44159-023-00212-w" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11377027/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39239312/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Rev.%20Psychol.&amp;title=Drawing%20as%20a%20versatile%20cognitive%20tool&amp;volume=2&amp;publication_year=2023&amp;pages=556-568&amp;pmid=39239312&amp;doi=10.1038/s44159-023-00212-w&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B47">
<span class="label">47.</span><cite>
Sayim B, Cavanagh P. 2011. 
What line drawings reveal about the visual brain. Front. Hum. Neurosci.
5, 118. ( 10.3389/fnhum.2011.00118)
</cite> [<a href="https://doi.org/10.3389/fnhum.2011.00118" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3203412/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22065509/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Front.%20Hum.%20Neurosci.&amp;title=What%20line%20drawings%20reveal%20about%20the%20visual%20brain&amp;volume=5&amp;publication_year=2011&amp;pages=118&amp;pmid=22065509&amp;doi=10.3389/fnhum.2011.00118&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B48">
<span class="label">48.</span><cite>
Biederman I, Ju G. 1988. 
Surface versus edge-based determinants of visual recognition. Cognit. Psychol.
20, 38–64. ( 10.1016/0010-0285(88)90024-2)
</cite> [<a href="https://doi.org/10.1016/0010-0285(88)90024-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/3338267/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cognit.%20Psychol.&amp;title=Surface%20versus%20edge-based%20determinants%20of%20visual%20recognition&amp;volume=20&amp;publication_year=1988&amp;pages=38-64&amp;pmid=3338267&amp;doi=10.1016/0010-0285(88)90024-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B49">
<span class="label">49.</span><cite>
Barrow HG, Tenenbaum JM. 1981. 
Interpreting line drawings as three-dimensional surfaces. Artif. Intell.
17, 75–116. ( 10.1016/0004-3702(81)90021-7)</cite> [<a href="https://doi.org/10.1016/0004-3702(81)90021-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Artif.%20Intell.&amp;title=Interpreting%20line%20drawings%20as%20three-dimensional%20surfaces&amp;volume=17&amp;publication_year=1981&amp;pages=75-116&amp;doi=10.1016/0004-3702(81)90021-7&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B50">
<span class="label">50.</span><cite>
Walther DB, Chai B, Caddigan E, Beck DM, Fei-Fei L. 2011. 
Simple line drawings suffice for functional MRI decoding of natural scene categories. Proc. Natl Acad. Sci. USA
108, 9661–9666. ( 10.1073/pnas.1015666108)
</cite> [<a href="https://doi.org/10.1073/pnas.1015666108" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3111263/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21593417/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;title=Simple%20line%20drawings%20suffice%20for%20functional%20MRI%20decoding%20of%20natural%20scene%20categories&amp;volume=108&amp;publication_year=2011&amp;pages=9661-9666&amp;pmid=21593417&amp;doi=10.1073/pnas.1015666108&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B51">
<span class="label">51.</span><cite>
Singer JJD, Cichy RM, Hebart MN. 2023. 
The spatiotemporal neural dynamics of object recognition for natural images and line drawings. J. Neurosci.
43, 484–500. ( 10.1523/JNEUROSCI.1546-22.2022)
</cite> [<a href="https://doi.org/10.1523/JNEUROSCI.1546-22.2022" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9864561/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36535769/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Neurosci.&amp;title=The%20spatiotemporal%20neural%20dynamics%20of%20object%20recognition%20for%20natural%20images%20and%20line%20drawings&amp;volume=43&amp;publication_year=2023&amp;pages=484-500&amp;pmid=36535769&amp;doi=10.1523/JNEUROSCI.1546-22.2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B52">
<span class="label">52.</span><cite>
Bauer RM. 2006. 
The agnosias. In Clinical neuropsychology: a pocket handbook for assessment (eds Snyder PJ, Nussbaum PD, Robins DL), pp. 508–533, 2nd
edn. Washington, DC, USA: American Psychological Association. ( 10.1037/11299-020)</cite> [<a href="https://doi.org/10.1037/11299-020" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Clinical%20neuropsychology:%20a%20pocket%20handbook%20for%20assessment&amp;publication_year=2006&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B53">
<span class="label">53.</span><cite>
Agrell B, Dehlin O. 1998. 
The clock-drawing test. Age Ageing
27, 399–403. ( 10.1093/ageing/27.3.399)</cite> [<a href="https://doi.org/10.1093/ageing/27.3.399" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23144287/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Age%20Ageing&amp;title=The%20clock-drawing%20test&amp;volume=27&amp;publication_year=1998&amp;pages=399-403&amp;pmid=23144287&amp;doi=10.1093/ageing/27.3.399&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B54">
<span class="label">54.</span><cite>
Cahn DA, Salmon DP, Monsch AU, Butters N, Wiederholt WC, Corey-Bloom J, Barrett-Connor E. 1996. 
Screening for dementia of the Alzheimer type in the community: the utility of the Clock Drawing Test. Arch. Clin. Neuropsychol. Off. J. Natl Acad. Neuropsychol.
11, 529–539.</cite> [<a href="https://pubmed.ncbi.nlm.nih.gov/14588458/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Arch.%20Clin.%20Neuropsychol.%20Off.%20J.%20Natl%20Acad.%20Neuropsychol.&amp;title=Screening%20for%20dementia%20of%20the%20Alzheimer%20type%20in%20the%20community:%20the%20utility%20of%20the%20Clock%20Drawing%20Test&amp;volume=11&amp;publication_year=1996&amp;pages=529-539&amp;pmid=14588458&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B55">
<span class="label">55.</span><cite>
Wechsler D. 2009. 
Wechsler memory scale, 4th
edn. London, UK: Pearson. ( 10.1037/t15175-000)</cite> [<a href="https://doi.org/10.1037/t15175-000" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?title=Wechsler%20memory%20scale&amp;publication_year=2009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B56">
<span class="label">56.</span><cite>
Shi F, Sun W, Duan H, Liu X, Hu M, Wang W, Zhai G. 2021. 
Drawing reveals hallmarks of children with autism. Displays
67, 102000. ( 10.1016/j.displa.2021.102000)</cite> [<a href="https://doi.org/10.1016/j.displa.2021.102000" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Displays&amp;title=Drawing%20reveals%20hallmarks%20of%20children%20with%20autism&amp;volume=67&amp;publication_year=2021&amp;pages=102000&amp;doi=10.1016/j.displa.2021.102000&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B57">
<span class="label">57.</span><cite>
Bozikas VP, Kosmidis MH, Gamvrula K, Hatzigeorgiadou M, Kourtis A, Karavatos A. 2004. 
Clock drawing test in patients with schizophrenia. Psychiatry Res.
121, 229–238. ( 10.1016/j.psychres.2003.07.003)
</cite> [<a href="https://doi.org/10.1016/j.psychres.2003.07.003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/14675742/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychiatry%20Res.&amp;title=Clock%20drawing%20test%20in%20patients%20with%20schizophrenia&amp;volume=121&amp;publication_year=2004&amp;pages=229-238&amp;pmid=14675742&amp;doi=10.1016/j.psychres.2003.07.003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B58">
<span class="label">58.</span><cite>
Fletcher PC, Frith CD. 2009. 
Perceiving is believing: a Bayesian approach to explaining the positive symptoms of schizophrenia. Nat. Rev. Neurosci.
10, 48–58. ( 10.1038/nrn2536)
</cite> [<a href="https://doi.org/10.1038/nrn2536" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/19050712/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Rev.%20Neurosci.&amp;title=Perceiving%20is%20believing:%20a%20Bayesian%20approach%20to%20explaining%20the%20positive%20symptoms%20of%20schizophrenia&amp;volume=10&amp;publication_year=2009&amp;pages=48-58&amp;pmid=19050712&amp;doi=10.1038/nrn2536&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B59">
<span class="label">59.</span><cite>
Pellicano E, Burr D. 2012. 
When the world becomes ‘too real’: a Bayesian explanation of autistic perception. Trends Cogn. Sci.
16, 504–510. ( 10.1016/j.tics.2012.08.009)
</cite> [<a href="https://doi.org/10.1016/j.tics.2012.08.009" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/22959875/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trends%20Cogn.%20Sci.&amp;title=When%20the%20world%20becomes%20%E2%80%98too%20real%E2%80%99:%20a%20Bayesian%20explanation%20of%20autistic%20perception&amp;volume=16&amp;publication_year=2012&amp;pages=504-510&amp;pmid=22959875&amp;doi=10.1016/j.tics.2012.08.009&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B60">
<span class="label">60.</span><cite>
Karmiloff-Smith A. 1990. 
Constraints on representational change: evidence from children’s drawing. Cognition
34, 57–83. ( 10.1016/0010-0277(90)90031-E)
</cite> [<a href="https://doi.org/10.1016/0010-0277(90)90031-E" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/1689233/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cognition&amp;title=Constraints%20on%20representational%20change:%20evidence%20from%20children%E2%80%99s%20drawing&amp;volume=34&amp;publication_year=1990&amp;pages=57-83&amp;pmid=1689233&amp;doi=10.1016/0010-0277(90)90031-E&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B61">
<span class="label">61.</span><cite>
Long B, Fan J, Chai Z, Frank MC. 2019. 
Developmental changes in the ability to draw distinctive features of object categories. Seattle, WA, USA: Cognitive Science Society. ( 10.31234/osf.io/8rzku). <a href="https://escholarship.org/uc/item/1gx7k7p7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://escholarship.org/uc/item/1gx7k7p7</a>.</cite> [<a href="https://doi.org/10.31234/osf.io/8rzku" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?Long%20B,%20Fan%20J,%20Chai%20Z,%20Frank%20MC.%202019.%20Developmental%20changes%20in%20the%20ability%20to%20draw%20distinctive%20features%20of%20object%20categories.%20Seattle,%20WA,%20USA:%20Cognitive%20Science%20Society.%20(%2010.31234/osf.io/8rzku).%20https://escholarship.org/uc/item/1gx7k7p7." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B62">
<span class="label">62.</span><cite>
Long B, Fan JE, Huey H, Chai Z, Frank MC. 2024. 
Parallel developmental changes in children’s production and recognition of line drawings of visual concepts. Nat. Commun.
15, 1191. ( 10.1038/s41467-023-44529-9)
</cite> [<a href="https://doi.org/10.1038/s41467-023-44529-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10853520/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38331850/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Commun.&amp;title=Parallel%20developmental%20changes%20in%20children%E2%80%99s%20production%20and%20recognition%20of%20line%20drawings%20of%20visual%20concepts&amp;volume=15&amp;publication_year=2024&amp;pages=1191&amp;pmid=38331850&amp;doi=10.1038/s41467-023-44529-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B63">
<span class="label">63.</span><cite>
Bainbridge WA, Kwok WY, Baker CI. 2021. 
Disrupted object-scene semantics boost scene recall but diminish object recall in drawings from memory. Mem. Cognit.
49, 1568–1582. ( 10.3758/s13421-021-01180-3)</cite> [<a href="https://doi.org/10.3758/s13421-021-01180-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8568627/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34031795/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Mem.%20Cognit.&amp;title=Disrupted%20object-scene%20semantics%20boost%20scene%20recall%20but%20diminish%20object%20recall%20in%20drawings%20from%20memory&amp;volume=49&amp;publication_year=2021&amp;pages=1568-1582&amp;pmid=34031795&amp;doi=10.3758/s13421-021-01180-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B64">
<span class="label">64.</span><cite>
Bainbridge WA, Baker CI. 2020. 
Boundaries extend and contract in scene memory depending on image properties. Curr. Biol.
30, 537–543. ( 10.1016/j.cub.2019.12.004)
</cite> [<a href="https://doi.org/10.1016/j.cub.2019.12.004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7187786/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31983637/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Curr.%20Biol.&amp;title=Boundaries%20extend%20and%20contract%20in%20scene%20memory%20depending%20on%20image%20properties&amp;volume=30&amp;publication_year=2020&amp;pages=537-543&amp;pmid=31983637&amp;doi=10.1016/j.cub.2019.12.004&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B65">
<span class="label">65.</span><cite>
Morgan AT, Petro LS, Muckli L. 2019. 
Scene representations conveyed by cortical feedback to early visual cortex can be described by line drawings. J. Neurosci.
39, 9410–9423. ( 10.1523/JNEUROSCI.0852-19.2019)
</cite> [<a href="https://doi.org/10.1523/JNEUROSCI.0852-19.2019" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6867807/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31611306/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Neurosci.&amp;title=Scene%20representations%20conveyed%20by%20cortical%20feedback%20to%20early%20visual%20cortex%20can%20be%20described%20by%20line%20drawings&amp;volume=39&amp;publication_year=2019&amp;pages=9410-9423&amp;pmid=31611306&amp;doi=10.1523/JNEUROSCI.0852-19.2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B66">
<span class="label">66.</span><cite>
Metzger W. 1936. 
Gesetze des Sehens. [Laws of vision]. Frankfurt a.M., Germany: Kramer.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Gesetze%20des%20Sehens.%20%5BLaws%20of%20vision%5D&amp;publication_year=1936&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B67">
<span class="label">67.</span><cite>
Bainbridge WA, Hall EH, Baker CI. 2019. 
Drawings of real-world scenes during free recall reveal detailed object and spatial information in memory. Nat. Commun.
10, 5. ( 10.1038/s41467-018-07830-6)
</cite> [<a href="https://doi.org/10.1038/s41467-018-07830-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6315028/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30602785/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Commun.&amp;title=Drawings%20of%20real-world%20scenes%20during%20free%20recall%20reveal%20detailed%20object%20and%20spatial%20information%20in%20memory&amp;volume=10&amp;publication_year=2019&amp;pages=5&amp;pmid=30602785&amp;doi=10.1038/s41467-018-07830-6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B68">
<span class="label">68.</span><cite>
Wang G, Foxwell MJ, Cichy RM, Pitcher D, Kaiser D. 2024. 
Individual differences in internal models explain idiosyncrasies in scene perception. Cognition
245, 105723. ( 10.1016/j.cognition.2024.105723)
</cite> [<a href="https://doi.org/10.1016/j.cognition.2024.105723" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38262271/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cognition&amp;title=Individual%20differences%20in%20internal%20models%20explain%20idiosyncrasies%20in%20scene%20perception&amp;volume=245&amp;publication_year=2024&amp;pages=105723&amp;pmid=38262271&amp;doi=10.1016/j.cognition.2024.105723&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B69">
<span class="label">69.</span><cite>
Gandolfo M, Nägele H, Peelen MV. 2023. 
Predictive processing of scene layout depends on naturalistic depth of field. Psychol. Sci.
34, 394–405. ( 10.1177/09567976221140341)
</cite> [<a href="https://doi.org/10.1177/09567976221140341" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36608172/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol.%20Sci.&amp;title=Predictive%20processing%20of%20scene%20layout%20depends%20on%20naturalistic%20depth%20of%20field&amp;volume=34&amp;publication_year=2023&amp;pages=394-405&amp;pmid=36608172&amp;doi=10.1177/09567976221140341&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B70">
<span class="label">70.</span><cite>
Wang G, Chen L, Cichy RM, Kaiser D. 2025. 
Enhanced and idiosyncratic neural representations of personally typical scenes. Proc. R. Soc. B
292, 20250272. ( 10.1098/rspb.2025.0272)</cite> [<a href="https://doi.org/10.1098/rspb.2025.0272" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11936675/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40132631/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proc.%20R.%20Soc.%20B&amp;title=Enhanced%20and%20idiosyncratic%20neural%20representations%20of%20personally%20typical%20scenes&amp;volume=292&amp;publication_year=2025&amp;pages=20250272&amp;pmid=40132631&amp;doi=10.1098/rspb.2025.0272&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B71">
<span class="label">71.</span><cite>
Calabrese L, Marucci FS. 2006. 
The influence of expertise level on the visuo-spatial ability: differences between experts and novices in imagery and drawing abilities. Cogn. Process
7, 118–120. ( 10.1007/s10339-006-0094-2)</cite> [<a href="https://doi.org/10.1007/s10339-006-0094-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cogn.%20Process&amp;title=The%20influence%20of%20expertise%20level%20on%20the%20visuo-spatial%20ability:%20differences%20between%20experts%20and%20novices%20in%20imagery%20and%20drawing%20abilities&amp;volume=7&amp;publication_year=2006&amp;pages=118-120&amp;doi=10.1007/s10339-006-0094-2&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B72">
<span class="label">72.</span><cite>
Chamberlain R, Drake JE, Kozbelt A, Hickman R, Siev J, Wagemans J. 2019. 
Artists as experts in visual cognition: an update. Psychol. Aesthet. Creat. Arts
13, 58–73. ( 10.1037/aca0000156)</cite> [<a href="https://doi.org/10.1037/aca0000156" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol.%20Aesthet.%20Creat.%20Arts&amp;title=Artists%20as%20experts%20in%20visual%20cognition:%20an%20update&amp;volume=13&amp;publication_year=2019&amp;pages=58-73&amp;doi=10.1037/aca0000156&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B73">
<span class="label">73.</span><cite>
Kozbelt A. 2001. 
Artists as experts in visual cognition. Vis. Cogn.
8, 705–723. ( 10.1080/13506280042000090)</cite> [<a href="https://doi.org/10.1080/13506280042000090" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Vis.%20Cogn.&amp;title=Artists%20as%20experts%20in%20visual%20cognition&amp;volume=8&amp;publication_year=2001&amp;pages=705-723&amp;doi=10.1080/13506280042000090&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B74">
<span class="label">74.</span><cite>
Perdreau F, Cavanagh P. 2015. 
Drawing experts have better visual memory while drawing. J. Vis.
15, 5. ( 10.1167/15.5.5)</cite> [<a href="https://doi.org/10.1167/15.5.5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26067523/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Vis.&amp;title=Drawing%20experts%20have%20better%20visual%20memory%20while%20drawing&amp;volume=15&amp;publication_year=2015&amp;pages=5&amp;pmid=26067523&amp;doi=10.1167/15.5.5&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B75">
<span class="label">75.</span><cite>
Bainbridge WA. 2022. 
A tutorial on capturing mental representations through drawing and crowd-sourced scoring. Behav. Res. Methods
54, 663–675. ( 10.3758/s13428-021-01672-9)
</cite> [<a href="https://doi.org/10.3758/s13428-021-01672-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9046317/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34341961/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Behav.%20Res.%20Methods&amp;title=A%20tutorial%20on%20capturing%20mental%20representations%20through%20drawing%20and%20crowd-sourced%20scoring&amp;volume=54&amp;publication_year=2022&amp;pages=663-675&amp;pmid=34341961&amp;doi=10.3758/s13428-021-01672-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B76">
<span class="label">76.</span><cite>
Öhlschläger S, Võ MH. 2020. 
Development of scene knowledge: evidence from explicit and implicit scene knowledge measures. J. Exp. Child Psychol.
194, 104782. ( 10.1016/j.jecp.2019.104782)
</cite> [<a href="https://doi.org/10.1016/j.jecp.2019.104782" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32179293/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Exp.%20Child%20Psychol.&amp;title=Development%20of%20scene%20knowledge:%20evidence%20from%20explicit%20and%20implicit%20scene%20knowledge%20measures&amp;volume=194&amp;publication_year=2020&amp;pages=104782&amp;pmid=32179293&amp;doi=10.1016/j.jecp.2019.104782&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B77">
<span class="label">77.</span><cite>
Öhlschläger S, Võ MH. 2017. 
SCEGRAM: an image database for semantic and syntactic inconsistencies in scenes. Behav. Res. Methods
49. ( 10.3758/s13428-016-0820-3)</cite> [<a href="https://doi.org/10.3758/s13428-016-0820-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27800578/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Behav.%20Res.%20Methods&amp;title=SCEGRAM:%20an%20image%20database%20for%20semantic%20and%20syntactic%20inconsistencies%20in%20scenes&amp;volume=49&amp;publication_year=2017&amp;pmid=27800578&amp;doi=10.3758/s13428-016-0820-3&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B78">
<span class="label">78.</span><cite>
Draschkow D, Võ MH. 2017. 
Scene grammar shapes the way we interact with objects, strengthens memories, and speeds search. Sci. Rep.
7, 16471. ( 10.1038/s41598-017-16739-x)
</cite> [<a href="https://doi.org/10.1038/s41598-017-16739-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5705766/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29184115/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Scene%20grammar%20shapes%20the%20way%20we%20interact%20with%20objects,%20strengthens%20memories,%20and%20speeds%20search&amp;volume=7&amp;publication_year=2017&amp;pages=16471&amp;pmid=29184115&amp;doi=10.1038/s41598-017-16739-x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B79">
<span class="label">79.</span><cite>
Bahn D, Türk DD, Tsenkova N, Schwarzer G, Võ MH, Kauschke C. 2025. 
Processing of scene-grammar inconsistencies in children with developmental language disorder—insights from implicit and explicit measures. Brain Sci.
15, 139. ( 10.3390/brainsci15020139)
</cite> [<a href="https://doi.org/10.3390/brainsci15020139" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11852763/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40002472/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Brain%20Sci.&amp;title=Processing%20of%20scene-grammar%20inconsistencies%20in%20children%20with%20developmental%20language%20disorder%E2%80%94insights%20from%20implicit%20and%20explicit%20measures&amp;volume=15&amp;publication_year=2025&amp;pages=139&amp;pmid=40002472&amp;doi=10.3390/brainsci15020139&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B80">
<span class="label">80.</span><cite>
Wilson CJ, Soranzo A. 2015. 
The use of virtual reality in psychology: a case study in visual perception. Comput. Math. Methods Med.
151702. ( 10.1155/2015/151702)
</cite> [<a href="https://doi.org/10.1155/2015/151702" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4538594/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26339281/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Comput.%20Math.%20Methods%20Med.&amp;title=The%20use%20of%20virtual%20reality%20in%20psychology:%20a%20case%20study%20in%20visual%20perception&amp;publication_year=2015&amp;pages=151702&amp;pmid=26339281&amp;doi=10.1155/2015/151702&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B81">
<span class="label">81.</span><cite>
Hansen T, Olkkonen M, Walter S, Gegenfurtner KR. 2006. 
Memory modulates color appearance. Nat. Neurosci.
9, 1367–1368. ( 10.1038/nn1794)
</cite> [<a href="https://doi.org/10.1038/nn1794" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17041591/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Neurosci.&amp;title=Memory%20modulates%20color%20appearance&amp;volume=9&amp;publication_year=2006&amp;pages=1367-1368&amp;pmid=17041591&amp;doi=10.1038/nn1794&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B82">
<span class="label">82.</span><cite>
Valsecchi M, Koenderink J, van Doorn A, Gegenfurtner KR. 2018. 
Prediction shapes peripheral appearance. J. Vis.
18, 21. ( 10.1167/18.13.21)</cite> [<a href="https://doi.org/10.1167/18.13.21" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30593064/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=J.%20Vis.&amp;title=Prediction%20shapes%20peripheral%20appearance&amp;volume=18&amp;publication_year=2018&amp;pages=21&amp;pmid=30593064&amp;doi=10.1167/18.13.21&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B83">
<span class="label">83.</span><cite>
Son G, Walther DB, Mack ML. 2021. 
Scene wheels: measuring perception and memory of real-world scenes with a continuous stimulus space. Behav. Res. Methods
54, 444–456. ( 10.3758/s13428-021-01630-5)
</cite> [<a href="https://doi.org/10.3758/s13428-021-01630-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34244986/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Behav.%20Res.%20Methods&amp;title=Scene%20wheels:%20measuring%20perception%20and%20memory%20of%20real-world%20scenes%20with%20a%20continuous%20stimulus%20space&amp;volume=54&amp;publication_year=2021&amp;pages=444-456&amp;pmid=34244986&amp;doi=10.3758/s13428-021-01630-5&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B84">
<span class="label">84.</span><cite>
Greene MR, Botros AP, Beck DM, Fei-Fei L. 2015. 
What you see is what you expect: rapid scene understanding benefits from prior experience. Atten. Percept. Psychophys.
77, 1239–1251. ( 10.3758/s13414-015-0859-8)
</cite> [<a href="https://doi.org/10.3758/s13414-015-0859-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/25776799/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Atten.%20Percept.%20Psychophys.&amp;title=What%20you%20see%20is%20what%20you%20expect:%20rapid%20scene%20understanding%20benefits%20from%20prior%20experience&amp;volume=77&amp;publication_year=2015&amp;pages=1239-1251&amp;pmid=25776799&amp;doi=10.3758/s13414-015-0859-8&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B85">
<span class="label">85.</span><cite>
Kollenda D, Reher AS, de Haas B. 2025. 
Individual gaze predicts individual scene descriptions. Sci. Rep.
15, 9443. ( 10.1038/s41598-025-94056-4)
</cite> [<a href="https://doi.org/10.1038/s41598-025-94056-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11923161/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40108359/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci.%20Rep.&amp;title=Individual%20gaze%20predicts%20individual%20scene%20descriptions&amp;volume=15&amp;publication_year=2025&amp;pages=9443&amp;pmid=40108359&amp;doi=10.1038/s41598-025-94056-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B86">
<span class="label">86.</span><cite>
Torralbo A, Walther DB, Chai B, Caddigan E, Fei-Fei L, Beck DM. 2013. 
Good exemplars of natural scene categories elicit clearer patterns than bad exemplars but not greater BOLD activity. PLoS One
8, e58594. ( 10.1371/journal.pone.0058594)
</cite> [<a href="https://doi.org/10.1371/journal.pone.0058594" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC3608650/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23555588/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Good%20exemplars%20of%20natural%20scene%20categories%20elicit%20clearer%20patterns%20than%20bad%20exemplars%20but%20not%20greater%20BOLD%20activity&amp;volume=8&amp;publication_year=2013&amp;pages=e58594&amp;pmid=23555588&amp;doi=10.1371/journal.pone.0058594&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B87">
<span class="label">87.</span><cite>
Iordan MC, Greene MR, Beck DM, Fei-Fei L. 2016. 
Typicality sharpens category representations in object-selective cortex. Neuroimage
134, 170–179. ( 10.1016/j.neuroimage.2016.04.012)
</cite> [<a href="https://doi.org/10.1016/j.neuroimage.2016.04.012" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4912889/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27079531/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neuroimage&amp;title=Typicality%20sharpens%20category%20representations%20in%20object-selective%20cortex&amp;volume=134&amp;publication_year=2016&amp;pages=170-179&amp;pmid=27079531&amp;doi=10.1016/j.neuroimage.2016.04.012&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B88">
<span class="label">88.</span><cite>
Davis T, Poldrack RA. 2014. 
Quantifying the internal structure of categories using a neural typicality measure. Cereb. Cortex
24, 1720–1737. ( 10.1093/cercor/bht014)
</cite> [<a href="https://doi.org/10.1093/cercor/bht014" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23442348/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cereb.%20Cortex&amp;title=Quantifying%20the%20internal%20structure%20of%20categories%20using%20a%20neural%20typicality%20measure&amp;volume=24&amp;publication_year=2014&amp;pages=1720-1737&amp;pmid=23442348&amp;doi=10.1093/cercor/bht014&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B89">
<span class="label">89.</span><cite>
Allen EJ, et al. 2022. 
A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nat. Neurosci.
25, 116–126. ( 10.1038/s41593-021-00962-x)
</cite> [<a href="https://doi.org/10.1038/s41593-021-00962-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34916659/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Neurosci.&amp;title=A%20massive%207T%20fMRI%20dataset%20to%20bridge%20cognitive%20neuroscience%20and%20artificial%20intelligence&amp;volume=25&amp;publication_year=2022&amp;pages=116-126&amp;pmid=34916659&amp;doi=10.1038/s41593-021-00962-x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B90">
<span class="label">90.</span><cite>
Engelbert M, Carruthers P. 2010. 
Introspection. WIREs Cogn. Sci.
1, 245–253. ( 10.1002/wcs.4)</cite> [<a href="https://doi.org/10.1002/wcs.4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26271238/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=WIREs%20Cogn.%20Sci.&amp;title=Introspection&amp;volume=1&amp;publication_year=2010&amp;pages=245-253&amp;pmid=26271238&amp;doi=10.1002/wcs.4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B91">
<span class="label">91.</span><cite>
Schwitzgebel E. 2008. 
The unreliability of naive introspection. Philos. Rev.
117, 245–273.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Philos.%20Rev.&amp;title=The%20unreliability%20of%20naive%20introspection&amp;volume=117&amp;publication_year=2008&amp;pages=245-273&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B92">
<span class="label">92.</span><cite>
Koffka K. 1924. 
Introspection and the method of psychology. Br. J. Psychol.
15, 149–161.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Br.%20J.%20Psychol.&amp;title=Introspection%20and%20the%20method%20of%20psychology&amp;volume=15&amp;publication_year=1924&amp;pages=149-161&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B93">
<span class="label">93.</span><cite>
Jack AI, Roepstorff A. 2002. 
Introspection and cognitive brain mapping: from stimulus-response to script-report. Trends Cogn. Sci.
6, 333–339. ( 10.1016/s1364-6613(02)01941-1)
</cite> [<a href="https://doi.org/10.1016/s1364-6613(02)01941-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/12140083/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Trends%20Cogn.%20Sci.&amp;title=Introspection%20and%20cognitive%20brain%20mapping:%20from%20stimulus-response%20to%20script-report&amp;volume=6&amp;publication_year=2002&amp;pages=333-339&amp;pmid=12140083&amp;doi=10.1016/s1364-6613(02)01941-1&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B94">
<span class="label">94.</span><cite>
Locke EA. 2009. 
It’s time we brought introspection out of the closet. Perspect. Psychol. Sci. J. Assoc. Psychol. Sci.
4, 24–25. ( 10.1111/j.1745-6924.2009.01090.x)</cite> [<a href="https://doi.org/10.1111/j.1745-6924.2009.01090.x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26158826/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Perspect.%20Psychol.%20Sci.%20J.%20Assoc.%20Psychol.%20Sci.&amp;title=It%E2%80%99s%20time%20we%20brought%20introspection%20out%20of%20the%20closet&amp;volume=4&amp;publication_year=2009&amp;pages=24-25&amp;pmid=26158826&amp;doi=10.1111/j.1745-6924.2009.01090.x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B95">
<span class="label">95.</span><cite>
Chamberlain R. 2018. 
Drawing as a window onto expertise. Curr. Dir. Psychol. Sci.
27, 501–507. ( 10.1177/0963721418797301)</cite> [<a href="https://doi.org/10.1177/0963721418797301" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Curr.%20Dir.%20Psychol.%20Sci.&amp;title=Drawing%20as%20a%20window%20onto%20expertise&amp;volume=27&amp;publication_year=2018&amp;pages=501-507&amp;doi=10.1177/0963721418797301&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B96">
<span class="label">96.</span><cite>
Chamberlain R, McManus C, Riley H, Rankin Q, Brunswick N. 2014. 
Cain’s house task revisited and revived: extending theory and methodology for quantifying drawing accuracy. Psychol. Aesthet. Creat. Arts
8, 152–167. ( 10.1037/a0035635)</cite> [<a href="https://doi.org/10.1037/a0035635" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Psychol.%20Aesthet.%20Creat.%20Arts&amp;title=Cain%E2%80%99s%20house%20task%20revisited%20and%20revived:%20extending%20theory%20and%20methodology%20for%20quantifying%20drawing%20accuracy&amp;volume=8&amp;publication_year=2014&amp;pages=152-167&amp;doi=10.1037/a0035635&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B97">
<span class="label">97.</span><cite>
Fischer J, Whitney D. 2014. 
Serial dependence in visual perception. Nat. Neurosci.
17, 738–743. ( 10.1038/nn.3689)
</cite> [<a href="https://doi.org/10.1038/nn.3689" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4012025/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24686785/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nat.%20Neurosci.&amp;title=Serial%20dependence%20in%20visual%20perception&amp;volume=17&amp;publication_year=2014&amp;pages=738-743&amp;pmid=24686785&amp;doi=10.1038/nn.3689&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B98">
<span class="label">98.</span><cite>
Bracci S, op de Beeck HP. 2023. 
Understanding human object vision: a picture is worth a thousand representations. Annu. Rev. Psychol.
74, 113–135. ( 10.1146/annurev-psych-032720-041031)
</cite> [<a href="https://doi.org/10.1146/annurev-psych-032720-041031" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36378917/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Annu.%20Rev.%20Psychol.&amp;title=Understanding%20human%20object%20vision:%20a%20picture%20is%20worth%20a%20thousand%20representations&amp;volume=74&amp;publication_year=2023&amp;pages=113-135&amp;pmid=36378917&amp;doi=10.1146/annurev-psych-032720-041031&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B99">
<span class="label">99.</span><cite>
Tiedemann H, Morgenstern Y, Schmidt F, Fleming RW. 2022. 
One-shot generalization in humans revealed through a drawing task. eLife
11, e75485. ( 10.7554/eLife.75485)
</cite> [<a href="https://doi.org/10.7554/eLife.75485" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9090327/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35536739/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=eLife&amp;title=One-shot%20generalization%20in%20humans%20revealed%20through%20a%20drawing%20task&amp;volume=11&amp;publication_year=2022&amp;pages=e75485&amp;pmid=35536739&amp;doi=10.7554/eLife.75485&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B100">
<span class="label">100.</span><cite>
Schmidt F, Tiedemann H, Fleming RW, Morgenstern Y. 2025. 
Inferring shape transformations in a drawing task. Mem. Cognit.
53, 189–199. ( 10.3758/s13421-023-01452-0)</cite> [<a href="https://doi.org/10.3758/s13421-023-01452-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11779755/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37668880/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Mem.%20Cognit.&amp;title=Inferring%20shape%20transformations%20in%20a%20drawing%20task&amp;volume=53&amp;publication_year=2025&amp;pages=189-199&amp;pmid=37668880&amp;doi=10.3758/s13421-023-01452-0&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B101">
<span class="label">101.</span><cite>
Baldwin J, Burleigh A, Pepperell R, Ruta N. 2016. 
The perceived size and shape of objects in peripheral vision. Percept
7, 2041669516661900. ( 10.1177/2041669516661900)</cite> [<a href="https://doi.org/10.1177/2041669516661900" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5030758/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27698981/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Percept&amp;title=The%20perceived%20size%20and%20shape%20of%20objects%20in%20peripheral%20vision&amp;volume=7&amp;publication_year=2016&amp;pages=2041669516661900&amp;pmid=27698981&amp;doi=10.1177/2041669516661900&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B102">
<span class="label">102.</span><cite>
Bainbridge WA, Pounder Z, Eardley AF, Baker CI. 2021. 
Quantifying aphantasia through drawing: those without visual imagery show deficits in object but not spatial memory. Cortex
135, 159–172. ( 10.1016/j.cortex.2020.11.014)
</cite> [<a href="https://doi.org/10.1016/j.cortex.2020.11.014" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7856239/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33383478/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Cortex&amp;title=Quantifying%20aphantasia%20through%20drawing:%20those%20without%20visual%20imagery%20show%20deficits%20in%20object%20but%20not%20spatial%20memory&amp;volume=135&amp;publication_year=2021&amp;pages=159-172&amp;pmid=33383478&amp;doi=10.1016/j.cortex.2020.11.014&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Proceedings of the Royal Society B: Biological Sciences are provided here courtesy of <strong>The Royal Society</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1098/rspb.2025.0602"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/rspb.2025.0602.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (2.9 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12364571/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12364571/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12364571%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12364571/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12364571/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12364571/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40829663/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12364571/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40829663/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12364571/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12364571/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="wTFnmklPhQERNQsU3HkEpEIFrL4LZbrElS4Wt650FddrlmxBc20UTHKLCEsn9bfn">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
