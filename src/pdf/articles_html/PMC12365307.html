
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Multi-stage framework using transformer models, feature fusion and ensemble learning for enhancing eye disease classification - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="AE5338128AF30593053812004F598F20.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365307/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Multi-stage framework using transformer models, feature fusion and ensemble learning for enhancing eye disease classification">
<meta name="citation_author" content="Abdulaziz AlMohimeed">
<meta name="citation_author_institution" content="College of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia">
<meta name="citation_publication_date" content="2025 Aug 19">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30469">
<meta name="citation_doi" content="10.1038/s41598-025-16415-5">
<meta name="citation_pmid" content="40830659">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365307/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365307/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365307/pdf/41598_2025_Article_16415.pdf">
<meta name="description" content="Eye diseases can affect vision and well-being, so early, accurate diagnosis is crucial to prevent serious impairment. Deep learning models have shown promise for automating the diagnosis of eye diseases from images. However, current methods mostly ...">
<meta name="og:title" content="Multi-stage framework using transformer models, feature fusion and ensemble learning for enhancing eye disease classification">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Eye diseases can affect vision and well-being, so early, accurate diagnosis is crucial to prevent serious impairment. Deep learning models have shown promise for automating the diagnosis of eye diseases from images. However, current methods mostly ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365307/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12365307">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-16415-5"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_16415.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12365307%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12365307/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12365307/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365307/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 19;15:30469. doi: <a href="https://doi.org/10.1038/s41598-025-16415-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-16415-5</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Multi-stage framework using transformer models, feature fusion and ensemble learning for enhancing eye disease classification</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22AlMohimeed%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Abdulaziz AlMohimeed</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Abdulaziz AlMohimeed</span></h3>
<div class="p">
<sup>1</sup>College of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22AlMohimeed%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Abdulaziz AlMohimeed</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>College of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Apr 7; Accepted 2025 Aug 14; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12365307  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40830659/" class="usa-link">40830659</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Eye diseases can affect vision and well-being, so early, accurate diagnosis is crucial to prevent serious impairment. Deep learning models have shown promise for automating the diagnosis of eye diseases from images. However, current methods mostly use single-model architectures, including convolutional neural networks (CNNs), which might not adequately capture the long-range spatial correlations and local fine-grained features required for classification. To address these limitations, this study proposes a multi-stage framework for eye diseases (MST-EDS), including two stages: hybrid and stacking models in the categorization of eye illnesses across four classes: normal, diabetic_retinopathy, glaucoma, and cataract, utilizing a benchmark dataset from Kaggle. Hybrid models are developed based on Transformer models: Vision Transformer (ViT), Data-efficient Image Transformer (DeiT), and Swin Transformer are used to extract deep features from images, Principal Component Analysis (PCA) is used to reduce the complexity of extracted features, and Machine Learning (ML) models are used as classifiers to enhance performance. In the stacking model, the outputs of the best hybrid models are stacked, and they are used to train and evaluate meta-learners to improve classification performance. The experimental results show that the MST-EDS-RF model recorded the best performance compared to individual Transformer and hybrid models, with 97.163% accuracy.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Image processing, Eye diseases, Diagnostic model, Vision transformer (ViT), Data-efficient image transformer (DeiT), Swin transformer, MST-EDS</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Eye diseases, Diagnosis</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Early and precise diagnosis of eye diseases can save patients from vision loss and help in preventing blindness through taking timely treatment for irreversible vision loss caused by factors such as diabetic retinopathy, glaucoma, and age-related macular degeneration<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>. However, many eye conditions have few initial symptoms and can often go unnoticed until extensive screening is carried out<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>, revealing the need for an accurate examination. Unfortunately, diagnosis usually depends on complex imaging modalities such as OCT and fundus photography by highly trained professionals, which can result in delay and variability<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a>,<a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>.</p>
<p id="Par3">In addition, techniques such as measuring intraocular pressure (IOP) are uncomfortable or invasive, making them unsuitable for large-scale screening where patient cooperation is critical, especially in glaucoma detection<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup>. Furthermore, glaucoma detection does not address other eye diseases, which leads to incomplete screening without employing multiple methods<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>. Moreover, many traditional methods, such as optical coherence tomography (OCT) and fundus photography, require expensive and specialized equipment that may not be accessible in all healthcare settings. For this reason, the accurate evaluation of the optic nerve and interpretation of results requires trained ophthalmologists or optometrists, which is especially limiting in rural or underserved areas<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>.</p>
<p id="Par4">Artificial Intelligence (AI) technology’s intervention in diagnosing eye diseases<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a>–<a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>, CNNs were used, but over time, many of its weaknesses became apparent. CNNs are affected by changes in imaging conditions like lighting, angle, and resolution<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup> and still cannot synthesize a high-order global context in images, which is conducive to analyzing complex relations in eye diseases<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>, making it challenging to incorporate the contextual information of patient leading to inconsistency in performance on various datasets<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>. While CNNs effectively capture spatial features, they struggle to model temporal changes in imaging data. In contrast, Transformers models use self-attention, hierarchical structure, and a shifted windowing mechanism that can capture long-range dependencies, contextual information, and local and global feature representations to significantly improve diagnostic performance in eye disease detection<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>.</p>
<p id="Par5">In addition, transformer models include different types, such as ViT, DeiT, and Swin. ViT divides images into patches, processes them as sequences, and uses self-attention mechanisms to capture long-range dependencies<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>. Swin is built upon ViT architecture but introduces a hierarchical structure and shifted windowing mechanism to improve efficiency significantly. Swin enables better spatial modeling by capturing both local and global feature representations through the integration of non-overlapping and shifted windows. This design reduces computational complexity and memory consumption<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>. DeiT is designed to enhance the efficiency of ViT Transformer, particularly on smaller datasets, by incorporating knowledge distillation techniques, making it more accessible. Deit relies on self-attention to capture relationships between different parts of the image, allowing it to learn complex patterns<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. In our model, MST-EDS, we used three types together to benefit from each one’s advantages by applying the stacking model. These transformer models serve as diverse and complementary feature extractors within the framework. Their integration enriches the feature space and improves the robustness and accuracy of eye disease classification.</p>
<p id="Par6">The stacked model facilitates the integration of heterogeneous architectures and learning paradigms, enabling each constituent model to extract complementary patterns and insights from the data. By leveraging this ensemble approach, the overall system benefits from improved predictive performance and enhanced generalizability across diverse datasets<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a>,<a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>.</p>
<p id="Par7">Existing research often focuses on applying pre-trained CNN, single transformer models, or hybrid models to classify eye disease; they do not apply different types of transformer or stacking models to make generalizations and enhance performance. For example, Aslam et al.<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup> applied five different pre-trained models, including VGG-16, VGG-19, Resnet-50, Resnet-152, and DenseNet-121. Wang et al.<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup> presented ViT based on the self-attention mechanism to enhance performance in medical image analysis. Abbas et al.<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup> introduced a hybrid ensemble model consisting of the AlexNet model, ReliefF as a feature selector, and ML as a classifier. This study aims to bridge this gap by proposing that MST-EDS be developed based on hybrid and stacking models to make generalizations and enhance performance. Advantages of this design: (1) Diversity in models: Different transformer models, such as ViT, DeiT, and Swin, can be applied to ensure varied feature representations. (2) To minimize dimensionality while retaining the most informative aspects of the data, we apply PCA after extracting feature vectors from multiple models. PCA effectively captures the most significant variance within the dataset, enabling us to reduce redundant and irrelevant features<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. This alleviates the risk of overfitting and improves the model’s computational efficiency and overall predictive performance. (3) Stacking Ensemble: Improves generalization by combining the strengths of individual classifiers.</p>
<p id="Par8">The study’s findings and insights can be distilled into the following key contributions:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par9">Novel multi-stage framework for eye diseases: Developing MST-EDS consists of hybrid and stacking models. Hybrid models were developed based on transformer architecture (Swin, ViT, and DeiT), feature selection, and ML models as classifiers. The stacking model is trained and evaluated using the outputs of the best hybrid models in stacking training and stacking testing to enhance classification performance.</p></li>
<li><p id="Par10">Evaluating performance across benchmark datasets: The proposed model is evaluated using a benchmark eye disease classification images dataset. MST-EDS achieves an exceptional accuracy of 97.163%, surpassing existing transformer and hybrid models in precision, recall, and F1-score.</p></li>
<li><p id="Par11">Applying transformer model in medical image: Applying Swin transformer with framework records the significant performance; its architecture uses a hierarchical self-attention mechanism that can capture long and short contextual patterns.</p></li>
<li><p id="Par12">Addressing computational efficiency of model: Through PCA feature selection, our approach maximizes the computational efficiency of transformer models, lowering processing overhead without sacrificing accuracy.</p></li>
<li><p id="Par13">We demonstrate the effectiveness of using transformers in classifying eye illnesses by going beyond the techniques currently used in the literature.</p></li>
</ul>
<p>This paper is structured as follows: “<a href="#Sec2" class="usa-link">Literature reviews</a>” provides an overview of existing research on eye diseases. “<a href="#Sec3" class="usa-link">Materials and method</a>” introduces our proposed framework, presenting its design and methodology. The experimental results are presented in “<a href="#Sec14" class="usa-link">Experiments results</a>”. “<a href="#Sec21" class="usa-link">Limitation and future work</a>” presents limitations and future work. Finally, “<a href="#Sec22" class="usa-link">Conclusion</a>” presents the essential findings and contributions of the study.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Literature reviews</h2>
<p id="Par14">Early fund screening can efficiently and cost-effectively reduce the risk of blindness from ophthalmic diseases. A manual diagnosis may delay the diagnosis due to a lack of medical resources. Researchers have achieved good results in eye diseases using deep learning and machine learning. Many researchers addressed the development models for classifying normal glaucoma, diabetic retinopathy, and contract eye diseases.</p>
<p id="Par15">Aslam et al.<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup> applied five different pre-trained models, including VGG-16, VGG-19, Resnet-50, Resnet-152, and DenseNet-121, for classifying eye disease. According to the findings, VGG-19 had the best results. Albelaihi et al.<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup> proposed a model that integrates ResNet152V2 + Bidirectional GRU (Bi-GRU) to classify four classes of eye disease. Their proposed model recorded the best performance compared to other models, EfficientNetB0, VGG16, ResNet152V2, and ResNet152V2. They employed online and offline geometric augmentation methods to assess the accuracy of models. Wang et al.<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup> presented ViT based on the self-attention mechanism to enhance performance in medical image analysis. The results showed that ViT performed the best compared to other models, such as ResNet, VGG, DenseNet, and MobileNet, in classifying eye disease. In<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>, the authors proposed an R-CNN+LSTM model based on DL models (R-CNN and LSTM) to extract features, NCAR to select features, and SVM as a classifier to classify eight different ophthalmologic diseases using the ODIR dataset.</p>
<p id="Par16">The authors applied models to classify four classes: cataract, diabetic retinopathy, and glaucoma, using the eye_diseases_classification (EDC) dataset collected from Kaggle. Babaqi<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup> et al. identified eye illnesses using CNN models and transfer learning. The results proved that transfer learning for multi-class classification recorded the highest accuracy compared to traditional CNN. Using the same dataset, Tasnim et al.<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup> proposed BayeSVM500 based on different stages. Firstly, Deep features were extracted from pre-trained CNN models, VGG16, VGG19, ResNet50, EfficientNet, and DenseNet, to extract deep features. Principal Component Analysis (PCA) was used to reduce feature dimensionality. Then, a Support Vector Machine classifier (SVM) was used as a classifier. They conducted different experiments to select the best-extracted features and recorded the best performance. Abdullah et al.<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup> proposed a weighted ensemble DL based on feature selection and a pre-trained CNN. Both models of Efficientb6 and Densnet169 were employed for the extracted features. PCA and Two-Dimensional Discrete Wavelet Transform (2D DWT) to optimize extracted features. Jessica Ryan et al.<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup> explored various pre-trained-CNN: VGG-16, VGG-19, ResNet-50, and ResNet-152v2 to identify the best model for detecting different types of eye diseases using ocular diseases. The results showed that ResNet-152v2 performed well compared to other models. Wahab Sait et al.<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> proposed a model based on the DL technique to classify eye disease using advanced image pre-processing methods. Denoising autoencoders were used to remove noise from image datasets. The essential features are produced via the single-shot detection (SSD) method. The features are chosen using the whale optimization algorithm (WOA) and the Levy Flight and Wavelet search strategy. Babaqi et al.<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup> applied a custom CNN and EfficientNet to detect three eye diseases, i.e., cataracts, diabetic retinopathy, and glaucoma. The results showed that performance significantly increased with the CNN-pretrained model. The results showed that the proposed model recorded the highest performance. Abbas et al.<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup> introduced a hybrid ensemble model consisting of the AlexNet model as a feature extractor, ReliefF as a feature selector, and XgBoost as a classifier. Image feature extraction was conducted using the AlexNet model. Subsequently, the ReliefF method was employed to select the most crucial features. The XgBoost classifier was applied to the selected features for class identification.</p>
<p id="Par17">The authors conducted experiments based on OCT images. Hemalakshmi et al.<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup> proposed a hybrid model (SViT) that combined the strengths of SqueezeNet and ViT to capture local and global features of images. SViT compared CNN-based and standalone transformer models and recorded the highest accuracy. Said et al.<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> proposed Tokens-To-Token Vision Transformer (T2T-ViT) and Mobile Vision Transformer (Mobile-ViT). According to experimental results using ViT techniques, Mobile-ViT performs better than the others in terms of classification accuracy.</p>
<p id="Par18">Table <a href="#Tab1" class="usa-link">1</a> compares the research studies related to the areas discussed. Some authors developed a model using the ODIR dataset, a subset of EDC; therefore, we conducted an experiment based on the EDC dataset in our research. Existing research employs single transformer models, hybrid models, or pre-trained CNNs to categorize eye diseases; it does not use several transformer models or stacking models to improve performance and make generalizations.</p>
<section class="tw xbox font-sm" id="Tab1"><h3 class="obj_head">Table 1.</h3>
<div class="caption p"><p>Comparing literature studies based on highlights and limitations.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Papers</th>
<th align="left" colspan="1" rowspan="1">Datasets</th>
<th align="left" colspan="1" rowspan="1">Highlights</th>
<th align="left" colspan="1" rowspan="1">Limitations</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Aslam et al.<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">ODIR</td>
<td align="left" colspan="1" rowspan="1">Applying different pre-trained CNN models</td>
<td align="left" colspan="1" rowspan="1">
<p>It did not use transformer models</p>
<p>It did not apply hybrid models</p>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Albelaihi et al.<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">ODIR</td>
<td align="left" colspan="1" rowspan="1">Developing an integrated model ResNet152V2 + Bi-GRU</td>
<td align="left" colspan="1" rowspan="1">It did not use transformer models</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Wang et al.<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">ODIR</td>
<td align="left" colspan="1" rowspan="1">
<p>Applying ViT transformer</p>
<p>Comparing ViT with pretrained models</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>It did not apply hybrid model</p>
<p>It did not apply ensemble learning</p>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Hemalakshmi et al.<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">OCT</td>
<td align="left" colspan="1" rowspan="1">Developing a hybrid model (SViT) that combined the strengths of SqueezeNet and ViT</td>
<td align="left" colspan="1" rowspan="1">
<p>It did not apply hybrid model</p>
<p>It did not apply ensemble learning</p>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Said et al.<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">OCT</td>
<td align="left" colspan="1" rowspan="1">
<p>Developing hybrid model Mobile-ViT</p>
<p>Applying ViT transformer</p>
</td>
<td align="left" colspan="1" rowspan="1">It did not apply ensemble learning</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Babaqi<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">Applying different pre-trained CNN models</td>
<td align="left" colspan="1" rowspan="1">
<p>It did not use transformer models</p>
<p>It did not apply hybrid models</p>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Tasnim et al.<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">Proposing BayeSVM500 based on pretrained CNN model, PCA, and SVM</td>
<td align="left" colspan="1" rowspan="1">
<p>It did not use transformer models</p>
<p>It did not apply ensemble learning</p>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Abdullah et al.<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">ODIR</td>
<td align="left" colspan="1" rowspan="1">Proposing a weighted ensemble DL based on pretrained CNN models and PCA</td>
<td align="left" colspan="1" rowspan="1">It did not use transformer models</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Jessica Ryan et al.<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">ODIR</td>
<td align="left" colspan="1" rowspan="1">Applying different pre-trained CNN models</td>
<td align="left" colspan="1" rowspan="1">
<p>It did not use transformer models</p>
<p>It did not apply ensemble learning</p>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Wahab Sait et al.<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">
<p>Applying DL models</p>
<p>Applying denoising autoencoders to remove noise from an image</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>It did not use transformer models</p>
<p>It did not apply stacking model</p>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Babaqi et al.<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">ODIR</td>
<td align="left" colspan="1" rowspan="1">
<p>Applying customize CNN model</p>
<p>Applying pre-trained CNN model</p>
</td>
<td align="left" colspan="1" rowspan="1">
<p>It did not use transformer models</p>
<p>It did not apply ensemble learning</p>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Abbas et al.<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">ODIR</td>
<td align="left" colspan="1" rowspan="1">Developing a hybrid ensemble model based on AlexNet, ReliefF, and XgBoost</td>
<td align="left" colspan="1" rowspan="1">
<p>It did not use transformer models</p>
<p>It did not apply stacking model</p>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec3"><h2 class="pmc_sec_title">Materials and method</h2>
<p id="Par19">The primary steps involved in classifying eye diseases are as follows: image data description, data augmentation, training models, and evaluation models, as shown in Fig. <a href="#Fig1" class="usa-link">1</a>. Each step is described in the following.</p>
<figure class="fig xbox font-sm" id="Fig1"><h3 class="obj_head">Fig. 1.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365307_41598_2025_16415_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/d0d3261f892a/41598_2025_16415_Fig1_HTML.jpg" loading="lazy" id="MO1" height="702" width="701" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The main steps of classifying eye diseases.</p></figcaption></figure><section id="Sec4"><h3 class="pmc_sec_title">Image data descriptions</h3>
<p id="Par20">The performance of the models was assessed using a publicly available dataset obtained from Kaggle: Eye diseases classification images dataset (EDC)<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>. The EDC dataset was collected from various sources such as IDRiD, Ocular recognition, HRF, retinal_dataset, and DRIVE. The dataset is balanced, comprising 4217 images distributed across four classes: 1074 normal, 1098 diabetic retinopathy, 1007 glaucoma, and 1038 cataract cases as shown in Fig. <a href="#Fig2" class="usa-link">2</a>.</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par21">In cataracts, the eye’s lens becomes cloudy and blurry, causing impaired vision. A cloudy lens can be replaced with an artificial one surgically to restore clear vision and quality of life.</p></li>
<li><p id="Par22">Diabetic retinopathy is a complication of diabetes that affects the blood vessels in the retina. In severe cases, it may lead to blindness because of blurred or distorted vision. Preventing and managing diabetes requires early detection, regular eye examinations, and proper management.</p></li>
<li><p id="Par23">A glaucoma is an eye disease characterized by damage to the optic nerve caused by increased fluid pressure in the eye. It gradually leads to vision loss, starting with peripheral vision and potentially progressing to complete blindness. Timely diagnosis, treatment, and ongoing monitoring are vital for preserving vision and preventing irreversible damage.</p></li>
</ul>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365307_41598_2025_16415_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/0eb0616a7dfa/41598_2025_16415_Fig2_HTML.jpg" loading="lazy" id="MO2" height="185" width="708" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Eye-related diseases.</p></figcaption></figure></section><section id="Sec5"><h3 class="pmc_sec_title">Image preprocessing and augmentation techniques</h3>
<p id="Par24">Image preprocessing is a crucial phase in numerous applications for computer vision since it boosts the performance and reliability of models<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>. Augmenting image data involves changes in original imaging to increase the diversity and variability of the training data artificially. These augmentations help enhance models’ generalization robustness and accuracy. The typical image preprocessing/augmentation techniques are:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par25">Flipping is the process of rotating an image horizontally, reversing the left and right sides of the image, or vertically, reversing the top and bottom of the image<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>. Flipping may assist in expanding the array of the training data and improve the model’s generalizability by introducing novel viewpoints and mirrored copy images of the input images.</p></li>
<li><p id="Par26">Resizing is frequently required since deep learning models usually need fixed-size inputs. Resizing can be accomplished via several interpolation approaches, notably nearest-neighbor, bilinear, and bicubic interpolation<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>.</p></li>
<li><p id="Par27">Normalization is a method of scaling input data to guarantee that all features hold the same scale, thereby promoting the stability and convergence of the training process. Image data is commonly normalized using mean and standard deviation<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>.</p></li>
</ul>
<p>Table <a href="#Tab2" class="usa-link">2</a> presents the values of augmentation techniques. Figure <a href="#Fig3" class="usa-link">3</a> presents the impact of each augmentation technique in the image.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>The values of augmentation techniques.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Augmentation techniques</th>
<th align="left" colspan="1" rowspan="1">Value</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">HorizontalFlip</td>
<td align="left" colspan="1" rowspan="1">0.5</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VerticalFlip</td>
<td align="left" colspan="1" rowspan="1">0.5</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Resize</td>
<td align="left" colspan="1" rowspan="1">224x224</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normlization</td>
<td align="left" colspan="1" rowspan="1">Mean and standardization</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365307_41598_2025_16415_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/aa291aea060a/41598_2025_16415_Fig3_HTML.jpg" loading="lazy" id="MO3" height="485" width="707" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The effect of each augmentation technique.</p></figcaption></figure></section><section id="Sec6"><h3 class="pmc_sec_title">Transformer models</h3>
<p id="Par29">We used three Transformer models: Vision Transformer (ViT), Data-efficient Image Transformer (DeiT) and Swin as standalone to compare with proposed models.</p>
<section id="Sec7"><h4 class="pmc_sec_title">Vision transformer (ViT)</h4>
<p id="Par30">ViT is a deep learning architecture that extends the transformer architecture from natural language processing (NLP) to computer vision<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>. This model got attention and performed well in image recognition<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>. ViT treats an image as a sequence of patches and uses self-attention to record their interactions. The model relies on transformer encoder and patch embedding<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>. ViT is a transformer-based architecture that interprets an image as a series of patches and processes them through the transformer architecture<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup>. The source image is split into a grid of non-overlapping patches of 16 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/db3c1d08ff04/d33e709.gif" loading="lazy" id="d33e709" alt="Inline graphic"></span> 16 or 32 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/db3c1d08ff04/d33e716.gif" loading="lazy" id="d33e716" alt="Inline graphic"></span> 32 pixels and an embedding dimension (D) of 768<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup>. The embedding dimension sets the size of the patch embeddings, which are vector representations of image patches. Expanding the embedding dimension can lead to a more expressive and powerful representation of input patches but also increases computational and memory requirements. The patch embedding sequence and positional encodings are included in the transformer encoder, which comprises 12 transformer blocks, each with a multi-head attention mechanism based on feedforward neural network FFN with 12 attention heads. FFN controls the number of parallel attention computations conducted in the Multi-Head Attention module. The FFN module applies a simple feedforward neural network of dimension 3072 for every patch separately in the ViT transformer blocks, offering more modeling capacity to learn deeper representations<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup>. To prevent overfitting in neural networks, a dropout rate 0.1 controls the potential of activations dropping out with a weight decay of 0.1 during the training phase. To improve the model’s generalization efficiency, Attention Dropout with 0.1 was utilized to regularize the attention mechanism<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>. A unique classification token is added after the transformer encoder sequence to record the input image’s global representation while being processed by a fully connected layer that generates the final classification output<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>. Figure <a href="#Fig4" class="usa-link">4</a> shows the general architecture of ViT transformer.</p>
<figure class="fig xbox font-sm" id="Fig4"><h5 class="obj_head">Fig. 4.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365307_41598_2025_16415_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/e9272be85ac7/41598_2025_16415_Fig4_HTML.jpg" loading="lazy" id="MO4" height="457" width="708" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>General architecture of ViT transformer.</p></figcaption></figure></section><section id="Sec8"><h4 class="pmc_sec_title">Data-efficient image transformer (DeiT)</h4>
<p id="Par31">DeiT is a version of the ViT model that aims to achieve high accuracy on image classification tasks while being trained with far less training data than the original ViT. Distillation token from DeiT takes a different approach, which is capable of fast learning without relying heavily on large, labeled datasets that hinder the traditional transfer learning methods. This approach enables the possibility of fast convergence and enhanced performance. DeiT is built to be trained on smaller datasets without requiring large-scale pretraining<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>. DeiT uses optimization via a distillation token and a teacher model. It learns from a teacher’s predictions to allow the model to achieve superior results with less data. Similar to ViT, DeiT employs a pure transformer architecture for image classification. It divides an image into fixed-size patches, flattens them, and applies self-attention mechanisms to them. DeiT enhances performance by incorporating optimized training strategies, including data augmentation, regularization, and learning rate schedules<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>.</p></section><section id="Sec9"><h4 class="pmc_sec_title">Swin transformer</h4>
<p id="Par32">The Swin transformer architecture introduces a hierarchical structure to model local and global dependencies efficiently, distinguishing it from the traditional Transformer. Swin Tiny is used to classify eye disease. The Swin tiny transformer applies the self-attention method to the entire image; the Swin Transformer separates the 4*4 input images onto non-overlapping 7-size windows and calculates self-attention within each window<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup>. This minimizes the computational complexity and increases model effectiveness. The Transformer possesses a hierarchical architecture, increasing the number of layers as one proceeds further into the model<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup>. A multi-head attention mechanism with heads was additionally employed, in which the self-attention computation is distributed between many attention heads to assist the model in capturing more diverse and complicated relationships in the input data<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup>. Yet, it increases the model’s complexity and processing complexity.</p>
<p id="Par33">The Multi-Layer Perceptron (MLP) Ratio is typically set to 4, implying that the hidden dimension in the MLP layer is four times the embedding dimension. To prevent overfitting, standard values for dropout rates in Swin Transformer models are 0.1 and weight decay is 0.05, while the learning rate, which primarily regulates the step size during the optimization process used to train the Swin Transformer model, should be 0.001<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup>. Swin Transformer alters the window partitioning in the following layer to record interactions between neighboring windows, allowing the self-attention mechanism to function across window boundaries<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup>. The Swin Transformer has a hierarchical architecture in which the number of windows and window sizes is gradually reduced in deeper layers, permitting the model to capture knowledge at various scales. The embedding dimension plays a vital role in establishing the representative capacity and performance of the Swin Transformer model<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>. The Embedding Dimension describes the number of channels or features within each transformer block’s output. Applying the Swin transform with an embedding size of 96 is preferable since higher embedding dimensions empower the model to capture more complex and detailed properties. However, they additionally increase the model’s computational and memory needs<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>. Figure <a href="#Fig5" class="usa-link">5</a> shows the general architecture of Swin transformer.</p>
<figure class="fig xbox font-sm" id="Fig5"><h5 class="obj_head">Fig. 5.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365307_41598_2025_16415_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/a1edc6a44291/41598_2025_16415_Fig5_HTML.jpg" loading="lazy" id="MO5" height="286" width="669" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>General architecture of Swin transformer.</p></figcaption></figure></section></section><section id="Sec10"><h3 class="pmc_sec_title">The proposed model (MST-EDS)</h3>
<p id="Par34">Figure <a href="#Fig6" class="usa-link">6</a> illustrates a two-stage of the proposed model (MST-EDS) for eye disease classification using hybrid models and stacking models. Advantages of this design: (1) Diversity in models: Different transformer models, such as ViT, DeiT, and Swin, can be applied to ensure varied feature representations. (2) To minimize dimensionality while retaining the most informative aspects of the data, we apply PCA after extracting feature vectors from multiple models. PCA effectively captures the most significant variance within the dataset, enabling us to reduce redundant and irrelevant features<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. This not only alleviates the risk of overfitting but also improves the model’s computational efficiency and overall predictive performance<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. (3) Stacking ensemble: Improves generalization by combining the strengths of individual classifiers.</p>
<figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365307_41598_2025_16415_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/dc9170d4dff4/41598_2025_16415_Fig6_HTML.jpg" loading="lazy" id="MO6" height="273" width="702" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The main stages of the proposed model.</p></figcaption></figure><section id="Sec11"><h4 class="pmc_sec_title">Stage 1: Hybrid models</h4>
<p id="Par35">This stage includes three parallel pipelines, each combining Transformer models, PCA for dimensionality reduction, and an ML classifier. Feature selection (FS) is crucial to transforming high-dimensional data into low-dimensional data<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a>,<a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup>.</p>
<ul class="list" style="list-style-type:disc">
<li><div class="p" id="Par36">Hybrid model 1 pipeline<ul class="list" style="list-style-type:disc">
<li><div class="p" id="Par37">ViT model extracts high-dimensional features and global context from images using self-attention mechanisms.</div></li>
<li><div class="p" id="Par38">PCA reduces the high-dimensional features output by the ViT model. This minimizes redundancy and computational complexity while preserving variance.</div></li>
<li><div class="p" id="Par39">Classifier: The reduced feature set is fed into SVM, RF, and LR classifiers to generate the prediction output (P1).</div></li>
</ul>
</div></li>
<li><div class="p" id="Par40">Hybrid model 2 pipeline<ul class="list" style="list-style-type:disc">
<li><div class="p" id="Par41">DeiT is designed to enhance performance by incorporating knowledge distillation techniques, making it more accessible. DeiT relies on self-attention to capture relationships between different parts of the image, allowing it to learn complex patterns.</div></li>
<li><div class="p" id="Par42">PCA reduces dimensionality of the DeiT-extracted features.</div></li>
<li><div class="p" id="Par43">The reduced feature vectors are subsequently input to SVM, RF, LR classifiers to produce the prediction output, denoted as P2</div></li>
</ul>
</div></li>
<li><div class="p" id="Par44">Hybrid model 3 pipeline<ul class="list" style="list-style-type:disc">
<li><div class="p" id="Par45">Swin Transformers extracts local and global features using hierarchical structure and shifted windows.</div></li>
<li><div class="p" id="Par46">PCA as feature reduction is applied to Swin outputs.</div></li>
<li><div class="p" id="Par47">The reduced feature vectors are subsequently input to produces prediction P3.</div></li>
</ul>
</div></li>
</ul>
<p>Each pipeline outputs a prediction vector (P1, P2, P3) representing class probabilities.</p></section><section id="Sec12"><h4 class="pmc_sec_title">Stage 2: Stacking model</h4>
<p id="Par48">This stage aggregates the individual predictions from the hybrid models to make a final classification. The stacking model is called stacked generalization, a type of ensemble learning that integrates various base models to enhance model performance<sup><a href="#CR49" class="usa-link" aria-describedby="CR49">49</a>,<a href="#CR50" class="usa-link" aria-describedby="CR50">50</a></sup>. Stacking consists of three hybrid models used as base models and meta-learning, including RF, SVM and LR. First, the outputs of the base models for the training set are combined into the stacking train, and the predictions of the base models for the testing set are incorporated into the stacking test. Second, the stacking training set is used to train the meta-model, while the stacking testing is used to evaluate the meta-model.</p></section></section><section id="Sec13"><h3 class="pmc_sec_title">Evaluation models</h3>
<p id="Par49">The evaluation of classification models in machine learning is critically hinged on a suite of key metrics. It is defined as the ratio of true positives (TP) and true negatives (TN) to the total number of instances, including false positives (FP) and false negatives (FN)<sup><a href="#CR51" class="usa-link" aria-describedby="CR51">51</a>,<a href="#CR52" class="usa-link" aria-describedby="CR52">52</a></sup>.</p>
<ul class="list" style="list-style-type:disc">
<li><div class="p" id="Par50">Accuracy: Represents the proportion of correctly classified instances. <table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/0dcbc029e56d/d33e913.gif" loading="lazy" id="d33e913" alt="graphic file with name d33e913.gif"></td>
<td class="label">1</td>
</tr></table>
</div></li>
<li><div class="p" id="Par51">Recall: Reflects the ability of the algorithm to correctly identify positive instances out of all actual positives. <table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/4b41bdd3c2ae/d33e922.gif" loading="lazy" id="d33e922" alt="graphic file with name d33e922.gif"></td>
<td class="label">2</td>
</tr></table>
</div></li>
<li><div class="p" id="Par52">Precision: Indicates the proportion of correctly identified positive instances out of all instances predicted as positive. <table class="disp-formula p" id="Equ3"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/45aa8a64311a/d33e931.gif" loading="lazy" id="d33e931" alt="graphic file with name d33e931.gif"></td>
<td class="label">3</td>
</tr></table>
</div></li>
<li><div class="p" id="Par53">F1-score: Presents the harmonic mean of precision and recall, providing a balanced measure of a model’s performance. <table class="disp-formula p" id="Equ4"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/135383b220cf/d33e940.gif" loading="lazy" id="d33e940" alt="graphic file with name d33e940.gif"></td>
<td class="label">4</td>
</tr></table>
</div></li>
</ul></section></section><section id="Sec14"><h2 class="pmc_sec_title">Experiments results</h2>
<section id="Sec15"><h3 class="pmc_sec_title">Experimental setup</h3>
<p id="Par54">This study implemented models using Python, an NVIDIA RTX-3090 GPU, Windows 10 Professional, and an Intel i7 processor with 3.2 GHz. Swin, ViT, and DeiT were implemented using Monai, PyTorch, and Sklearn.</p>
<p id="Par55">We conducted different experiments based on various approaches. The first approach is standalone transformer models (Swin, ViT, and DeiT). The second approach integrates transformer models, Swin, ViT, and Diet, as feature extraction with ML as a classifier. The second approach is based on Swin, ViT, and DieT, which are used for feature extraction; PCA is used for feature reduction to reduce the complexity of features; and ML models, including RF, SVM, and LR, are used as classifiers. The third approach is hybrid models based on Swin, ViT, and Diet, which are used for feature extraction; PCA is used for feature reduction to reduce the complexity of features; and ML models, including RF, SVM, and LR, are used as classifiers. The fourth approach is the proposed model MST-EDS, which is trained by stacking training that is stacked by the output predictions from the training set based on the best hybrid models, and evaluated by stacking testing that is stacked from the testing set based on the best hybrid models.</p>
<p id="Par56">The PCA method is employed with a preserved variance of 95% to reduce the data’s dimensionality. It is applied to training features, and then the number of components is used to testing features, resulting in DeiT reducing from (2949, 37824) to (2949, 2757) with 2757 principal components, Swin reducing from (2949, 768) to (2949, 500) with 500 principal components (500), and ViT reducing from (2949, 768) to (2949, 580) with 580 principal components.</p>
<p id="Par57">For the setting of ML, we stated that ML classifiers were trained using a 5-fold cross-validation strategy to ensure generalizability and avoid overfitting with key hyperparameters of RF (number of estimators = 100, max depth = 10, criterion = gini), LR (C = 1.0, max_iter = 100), and SVM (kernel = rbf, C = 1). The hyperparameters of Transformer models as shown in Table <a href="#Tab3" class="usa-link">3</a>.</p>
<section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Setting of model parameters.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Models</th>
<th align="left" colspan="1" rowspan="1">Parameters</th>
<th align="left" colspan="1" rowspan="1">Values</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="10" colspan="1">Swin-tiny</td>
<td align="left" colspan="1" rowspan="1">Embedding dimension</td>
<td align="left" colspan="1" rowspan="1">96</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Number of layers</td>
<td align="left" colspan="1" rowspan="1">4 Stages</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Window size</td>
<td align="left" colspan="1" rowspan="1">7 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/db3c1d08ff04/d33e997.gif" loading="lazy" id="d33e997" alt="Inline graphic"></span> 7</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Number of heads</td>
<td align="left" colspan="1" rowspan="1">(3, 6, 12, 24)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MLP hidden dim</td>
<td align="left" colspan="1" rowspan="1">4 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/db3c1d08ff04/d33e1014.gif" loading="lazy" id="d33e1014" alt="Inline graphic"></span> Embedding dim</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Number of blocks</td>
<td align="left" colspan="1" rowspan="1">(2, 2, 6, 2)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Input size</td>
<td align="left" colspan="1" rowspan="1">224 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq5"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/db3c1d08ff04/d33e1031.gif" loading="lazy" id="d33e1031" alt="Inline graphic"></span> 224</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Epoch</td>
<td align="left" colspan="1" rowspan="1">70 with early stopping</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Batch size</td>
<td align="left" colspan="1" rowspan="1">32</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Optimizer</td>
<td align="left" colspan="1" rowspan="1">AdamW</td>
</tr>
<tr>
<td align="left" rowspan="8" colspan="1">ViT-base</td>
<td align="left" colspan="1" rowspan="1">Embedding dimension (D)</td>
<td align="left" colspan="1" rowspan="1">768</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Number of layers (L)</td>
<td align="left" colspan="1" rowspan="1">12</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Number of attention heads (H)</td>
<td align="left" colspan="1" rowspan="1">12</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MLP dimension (D MLP)</td>
<td align="left" colspan="1" rowspan="1">3072</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Input size</td>
<td align="left" colspan="1" rowspan="1">224 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq6"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/db3c1d08ff04/d33e1081.gif" loading="lazy" id="d33e1081" alt="Inline graphic"></span> 224</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Epoch</td>
<td align="left" colspan="1" rowspan="1">70 with early stopping</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Batch size</td>
<td align="left" colspan="1" rowspan="1">32</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Optimizer</td>
<td align="left" colspan="1" rowspan="1">AdamW</td>
</tr>
<tr>
<td align="left" rowspan="7" colspan="1">DeiT-Small</td>
<td align="left" colspan="1" rowspan="1">Embedding dimension</td>
<td align="left" colspan="1" rowspan="1">384</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Number of layers</td>
<td align="left" colspan="1" rowspan="1">12</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Number of attention heads</td>
<td align="left" colspan="1" rowspan="1">6</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Input size</td>
<td align="left" colspan="1" rowspan="1">224 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq7"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/db3c1d08ff04/d33e1125.gif" loading="lazy" id="d33e1125" alt="Inline graphic"></span> 224</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Epoch</td>
<td align="left" colspan="1" rowspan="1">70 with early stopping</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Batch size</td>
<td align="left" colspan="1" rowspan="1">32</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Optimizer</td>
<td align="left" colspan="1" rowspan="1">AdamW</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section id="Sec16"><h4 class="pmc_sec_title">Data splitting</h4>
<p id="Par58">The models were trained using 70% of the total number of images, validated using 10%, and tested with 20%. The dataset is balanced and divided using stratified methods, which means the number of classes in training, testing, and validation sets is approximately nearby. Table <a href="#Tab4" class="usa-link">4</a> shows the number of images in each class.</p>
<section class="tw xbox font-sm" id="Tab4"><h5 class="obj_head">Table 4.</h5>
<div class="caption p"><p>The number of images in training, validation, and testing sets.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Classes</th>
<th align="left" colspan="1" rowspan="1">Training</th>
<th align="left" colspan="1" rowspan="1">Validation</th>
<th align="left" colspan="1" rowspan="1">Testing</th>
<th align="left" colspan="1" rowspan="1"></th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">751</td>
<td align="left" colspan="1" rowspan="1">107</td>
<td align="left" colspan="1" rowspan="1">216</td>
<td align="left" colspan="1" rowspan="1">1074</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">768</td>
<td align="left" colspan="1" rowspan="1">110</td>
<td align="left" colspan="1" rowspan="1">220</td>
<td align="left" colspan="1" rowspan="1">1098</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">704</td>
<td align="left" colspan="1" rowspan="1">101</td>
<td align="left" colspan="1" rowspan="1">202</td>
<td align="left" colspan="1" rowspan="1">1007</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">726</td>
<td align="left" colspan="1" rowspan="1">104</td>
<td align="left" colspan="1" rowspan="1">208</td>
<td align="left" colspan="1" rowspan="1">1038</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Total</td>
<td align="left" colspan="1" rowspan="1">2949</td>
<td align="left" colspan="1" rowspan="1">422</td>
<td align="left" colspan="1" rowspan="1">846</td>
<td align="left" colspan="1" rowspan="1">4217</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section><section id="Sec17"><h3 class="pmc_sec_title">The performance of models across all classes of eye disease</h3>
<p id="Par59">Tables <a href="#Tab5" class="usa-link">5</a> and <a href="#Tab6" class="usa-link">6</a> present the performance of different models for each class across three approaches: standalone models, hybrid models, and MST-EDS based on various evaluation matrices precision, recall, and F1-score. As shown in Tables, all models based on diabetic_retinopathy classes recorded the highest performance compared to glaucoma or cataracts because the features in diabetic_retinopathy, such as microaneurysms, hemorrhages, and exudates, are easy to detect by models. MST-EDS recorded the best performance across all classes, especially with RF, because Swin captures local and global features using hierarchical representation and attention mechanisms. RF provides insights into the importance of different features, which is useful in understanding the underlying relationships in the data.</p>
<section class="tw xbox font-sm" id="Tab5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>The performance of models across all classes of eye disease.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Approaches</th>
<th align="left" colspan="1" rowspan="1">Models</th>
<th align="left" colspan="1" rowspan="1">Classes</th>
<th align="left" colspan="1" rowspan="1">Precision</th>
<th align="left" colspan="1" rowspan="1">Recall</th>
<th align="left" colspan="1" rowspan="1">F1-score</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="12" colspan="1">Standalone models</td>
<td align="left" rowspan="4" colspan="1">ViT</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">90.640</td>
<td align="left" colspan="1" rowspan="1">88.462</td>
<td align="left" colspan="1" rowspan="1">89.538</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">94.787</td>
<td align="left" colspan="1" rowspan="1">90.909</td>
<td align="left" colspan="1" rowspan="1">92.807</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">85.561</td>
<td align="left" colspan="1" rowspan="1">79.208</td>
<td align="left" colspan="1" rowspan="1">82.262</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">75.102</td>
<td align="left" colspan="1" rowspan="1">85.185</td>
<td align="left" colspan="1" rowspan="1">79.826</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">DeiT</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">92.500</td>
<td align="left" colspan="1" rowspan="1">88.942</td>
<td align="left" colspan="1" rowspan="1">90.686</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">94.860</td>
<td align="left" colspan="1" rowspan="1">92.273</td>
<td align="left" colspan="1" rowspan="1">93.548</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">85.864</td>
<td align="left" colspan="1" rowspan="1">81.188</td>
<td align="left" colspan="1" rowspan="1">83.461</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">76.763</td>
<td align="left" colspan="1" rowspan="1">85.648</td>
<td align="left" colspan="1" rowspan="1">80.963</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">Swin</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">94.175</td>
<td align="left" colspan="1" rowspan="1">93.269</td>
<td align="left" colspan="1" rowspan="1">93.720</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">99.099</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">99.548</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">83.945</td>
<td align="left" colspan="1" rowspan="1">90.594</td>
<td align="left" colspan="1" rowspan="1">87.143</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">90.500</td>
<td align="left" colspan="1" rowspan="1">83.796</td>
<td align="left" colspan="1" rowspan="1">87.019</td>
</tr>
<tr>
<td align="left" rowspan="36" colspan="1">Models-ML</td>
<td align="left" rowspan="4" colspan="1">ViT-RF</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">92.893</td>
<td align="left" colspan="1" rowspan="1">87.981</td>
<td align="left" colspan="1" rowspan="1">90.370</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">96.875</td>
<td align="left" colspan="1" rowspan="1">98.636</td>
<td align="left" colspan="1" rowspan="1">97.748</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">82.524</td>
<td align="left" colspan="1" rowspan="1">84.158</td>
<td align="left" colspan="1" rowspan="1">83.333</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">86.301</td>
<td align="left" colspan="1" rowspan="1">87.500</td>
<td align="left" colspan="1" rowspan="1">86.897</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">ViT-LR</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">93.401</td>
<td align="left" colspan="1" rowspan="1">88.462</td>
<td align="left" colspan="1" rowspan="1">90.864</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">96.429</td>
<td align="left" colspan="1" rowspan="1">98.182</td>
<td align="left" colspan="1" rowspan="1">97.297</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">80.583</td>
<td align="left" colspan="1" rowspan="1">82.178</td>
<td align="left" colspan="1" rowspan="1">81.373</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">85.388</td>
<td align="left" colspan="1" rowspan="1">86.574</td>
<td align="left" colspan="1" rowspan="1">85.977</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">ViT-SVM</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">91.327</td>
<td align="left" colspan="1" rowspan="1">86.058</td>
<td align="left" colspan="1" rowspan="1">88.614</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">95.536</td>
<td align="left" colspan="1" rowspan="1">97.273</td>
<td align="left" colspan="1" rowspan="1">96.396</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">79.126</td>
<td align="left" colspan="1" rowspan="1">80.693</td>
<td align="left" colspan="1" rowspan="1">79.902</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">84.545</td>
<td align="left" colspan="1" rowspan="1">86.111</td>
<td align="left" colspan="1" rowspan="1">85.321</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">DieT-RF</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">94.872</td>
<td align="left" colspan="1" rowspan="1">88.942</td>
<td align="left" colspan="1" rowspan="1">91.811</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">97.738</td>
<td align="left" colspan="1" rowspan="1">98.182</td>
<td align="left" colspan="1" rowspan="1">97.959</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">84.466</td>
<td align="left" colspan="1" rowspan="1">86.139</td>
<td align="left" colspan="1" rowspan="1">85.294</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">87.500</td>
<td align="left" colspan="1" rowspan="1">90.741</td>
<td align="left" colspan="1" rowspan="1">89.091</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">DieT-LR</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">93.000</td>
<td align="left" colspan="1" rowspan="1">89.423</td>
<td align="left" colspan="1" rowspan="1">91.176</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">95.982</td>
<td align="left" colspan="1" rowspan="1">97.727</td>
<td align="left" colspan="1" rowspan="1">96.847</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">83.333</td>
<td align="left" colspan="1" rowspan="1">84.158</td>
<td align="left" colspan="1" rowspan="1">83.744</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">86.239</td>
<td align="left" colspan="1" rowspan="1">87.037</td>
<td align="left" colspan="1" rowspan="1">86.636</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">DeiT-SVM</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">92.040</td>
<td align="left" colspan="1" rowspan="1">88.942</td>
<td align="left" colspan="1" rowspan="1">90.465</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">95.982</td>
<td align="left" colspan="1" rowspan="1">97.727</td>
<td align="left" colspan="1" rowspan="1">96.847</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">81.818</td>
<td align="left" colspan="1" rowspan="1">80.198</td>
<td align="left" colspan="1" rowspan="1">81.000</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">84.305</td>
<td align="left" colspan="1" rowspan="1">87.037</td>
<td align="left" colspan="1" rowspan="1">85.649</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">Swin-RF</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">94.231</td>
<td align="left" colspan="1" rowspan="1">94.231</td>
<td align="left" colspan="1" rowspan="1">94.231</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">99.087</td>
<td align="left" colspan="1" rowspan="1">98.636</td>
<td align="left" colspan="1" rowspan="1">98.861</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">88.095</td>
<td align="left" colspan="1" rowspan="1">91.584</td>
<td align="left" colspan="1" rowspan="1">89.806</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">90.909</td>
<td align="left" colspan="1" rowspan="1">87.963</td>
<td align="left" colspan="1" rowspan="1">89.412</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">Swin-LR</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">94.634</td>
<td align="left" colspan="1" rowspan="1">93.269</td>
<td align="left" colspan="1" rowspan="1">93.947</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">98.636</td>
<td align="left" colspan="1" rowspan="1">98.636</td>
<td align="left" colspan="1" rowspan="1">98.636</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">87.500</td>
<td align="left" colspan="1" rowspan="1">88.099</td>
<td align="left" colspan="1" rowspan="1">88.780</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">89.202</td>
<td align="left" colspan="1" rowspan="1">87.963</td>
<td align="left" colspan="1" rowspan="1">88.578</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">Swim-SVM</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">91.943</td>
<td align="left" colspan="1" rowspan="1">93.269</td>
<td align="left" colspan="1" rowspan="1">92.601</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">98.074</td>
<td align="left" colspan="1" rowspan="1">97.273</td>
<td align="left" colspan="1" rowspan="1">98.165</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">84.332</td>
<td align="left" colspan="1" rowspan="1">90.594</td>
<td align="left" colspan="1" rowspan="1">87.351</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">91.089</td>
<td align="left" colspan="1" rowspan="1">85.185</td>
<td align="left" colspan="1" rowspan="1">88.038</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section class="tw xbox font-sm" id="Tab6"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>Continued the performance of models across all classes of eye disease.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Approaches</th>
<th align="left" colspan="1" rowspan="1">Models</th>
<th align="left" colspan="1" rowspan="1">Classes</th>
<th align="left" colspan="1" rowspan="1">Precision</th>
<th align="left" colspan="1" rowspan="1">Recall</th>
<th align="left" colspan="1" rowspan="1">F1-score</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="36" colspan="1">Models-PCA-ML</td>
<td align="left" rowspan="4" colspan="1">ViT-PCA-RF</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">94.898</td>
<td align="left" colspan="1" rowspan="1">89.423</td>
<td align="left" colspan="1" rowspan="1">92.079</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">98.206</td>
<td align="left" colspan="1" rowspan="1">99.545</td>
<td align="left" colspan="1" rowspan="1">98.871</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">83.575</td>
<td align="left" colspan="1" rowspan="1">85.644</td>
<td align="left" colspan="1" rowspan="1">84.597</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">87.273</td>
<td align="left" colspan="1" rowspan="1">88.889</td>
<td align="left" colspan="1" rowspan="1">88.073</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">ViT-PCA-LR</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">93.939</td>
<td align="left" colspan="1" rowspan="1">89.423</td>
<td align="left" colspan="1" rowspan="1">91.626</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">96.889</td>
<td align="left" colspan="1" rowspan="1">99.091</td>
<td align="left" colspan="1" rowspan="1">97.978</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">82.843</td>
<td align="left" colspan="1" rowspan="1">83.663</td>
<td align="left" colspan="1" rowspan="1">83.251</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">86.301</td>
<td align="left" colspan="1" rowspan="1">87.500</td>
<td align="left" colspan="1" rowspan="1">86.897</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">ViT-PCA-SVM</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">92.386</td>
<td align="left" colspan="1" rowspan="1">87.500</td>
<td align="left" colspan="1" rowspan="1">89.877</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">96.444</td>
<td align="left" colspan="1" rowspan="1">98.636</td>
<td align="left" colspan="1" rowspan="1">97.528</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">80.882</td>
<td align="left" colspan="1" rowspan="1">81.683</td>
<td align="left" colspan="1" rowspan="1">81.281</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">85.455</td>
<td align="left" colspan="1" rowspan="1">87.037</td>
<td align="left" colspan="1" rowspan="1">86.239</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">DeiT-PCA-RF</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">95.918</td>
<td align="left" colspan="1" rowspan="1">90.385</td>
<td align="left" colspan="1" rowspan="1">93.069</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">98.649</td>
<td align="left" colspan="1" rowspan="1">99.545</td>
<td align="left" colspan="1" rowspan="1">99.095</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">85.507</td>
<td align="left" colspan="1" rowspan="1">87.624</td>
<td align="left" colspan="1" rowspan="1">86.553</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">88.688</td>
<td align="left" colspan="1" rowspan="1">90.741</td>
<td align="left" colspan="1" rowspan="1">89.703</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">DeiT-PCA-LR</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">94.500</td>
<td align="left" colspan="1" rowspan="1">90.865</td>
<td align="left" colspan="1" rowspan="1">92.647</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">97.321</td>
<td align="left" colspan="1" rowspan="1">99.091</td>
<td align="left" colspan="1" rowspan="1">98.198</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">84.804</td>
<td align="left" colspan="1" rowspan="1">85.644</td>
<td align="left" colspan="1" rowspan="1">85.222</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">87.615</td>
<td align="left" colspan="1" rowspan="1">88.426</td>
<td align="left" colspan="1" rowspan="1">88.018</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">DeiT-PCA-SVM</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">93.035</td>
<td align="left" colspan="1" rowspan="1">89.904</td>
<td align="left" colspan="1" rowspan="1">91.443</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">96.875</td>
<td align="left" colspan="1" rowspan="1">98.636</td>
<td align="left" colspan="1" rowspan="1">97.748</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">83.333</td>
<td align="left" colspan="1" rowspan="1">81.683</td>
<td align="left" colspan="1" rowspan="1">82.500</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">85.650</td>
<td align="left" colspan="1" rowspan="1">88.426</td>
<td align="left" colspan="1" rowspan="1">87.016</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">Swin-PCA-RF</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">95.673</td>
<td align="left" colspan="1" rowspan="1">95.673</td>
<td align="left" colspan="1" rowspan="1">95.673</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">89.474</td>
<td align="left" colspan="1" rowspan="1">92.574</td>
<td align="left" colspan="1" rowspan="1">90.998</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">91.866</td>
<td align="left" colspan="1" rowspan="1">88.889</td>
<td align="left" colspan="1" rowspan="1">90.353</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">Swin-PCA-LR</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">95.631</td>
<td align="left" colspan="1" rowspan="1">94.712</td>
<td align="left" colspan="1" rowspan="1">95.169</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">99.548</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">99.773</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">88.942</td>
<td align="left" colspan="1" rowspan="1">91.584</td>
<td align="left" colspan="1" rowspan="1">90.244</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">90.995</td>
<td align="left" colspan="1" rowspan="1">88.889</td>
<td align="left" colspan="1" rowspan="1">89.930</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">Swin-PCA-SVM</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">93.810</td>
<td align="left" colspan="1" rowspan="1">94.712</td>
<td align="left" colspan="1" rowspan="1">94.258</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">99.548</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">99.773</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">86.667</td>
<td align="left" colspan="1" rowspan="1">90.099</td>
<td align="left" colspan="1" rowspan="1">88.350</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">91.220</td>
<td align="left" colspan="1" rowspan="1">86.574</td>
<td align="left" colspan="1" rowspan="1">88.836</td>
</tr>
<tr>
<td align="left" rowspan="12" colspan="1">The proposed model</td>
<td align="left" rowspan="4" colspan="1">MST-EDS-RF</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">97.573</td>
<td align="left" colspan="1" rowspan="1">96.635</td>
<td align="left" colspan="1" rowspan="1">97.101</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">95.588</td>
<td align="left" colspan="1" rowspan="1">96.535</td>
<td align="left" colspan="1" rowspan="1">96.059</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">95.370</td>
<td align="left" colspan="1" rowspan="1">95.370</td>
<td align="left" colspan="1" rowspan="1">95.370</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">MST-EDS-LR</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">97.087</td>
<td align="left" colspan="1" rowspan="1">96.154</td>
<td align="left" colspan="1" rowspan="1">96.618</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">92.271</td>
<td align="left" colspan="1" rowspan="1">94.554</td>
<td align="left" colspan="1" rowspan="1">93.399</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">93.427</td>
<td align="left" colspan="1" rowspan="1">92.130</td>
<td align="left" colspan="1" rowspan="1">92.774</td>
</tr>
<tr>
<td align="left" rowspan="4" colspan="1">MST-EDS-SVM</td>
<td align="left" colspan="1" rowspan="1">Cataract</td>
<td align="left" colspan="1" rowspan="1">94.787</td>
<td align="left" colspan="1" rowspan="1">96.154</td>
<td align="left" colspan="1" rowspan="1">95.465</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Diabetic_retinopathy</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
<td align="left" colspan="1" rowspan="1">100.000</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Glaucoma</td>
<td align="left" colspan="1" rowspan="1">88.995</td>
<td align="left" colspan="1" rowspan="1">92.079</td>
<td align="left" colspan="1" rowspan="1">90.511</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Normal</td>
<td align="left" colspan="1" rowspan="1">93.204</td>
<td align="left" colspan="1" rowspan="1">88.889</td>
<td align="left" colspan="1" rowspan="1">90.995</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par60">In standalone models, Swin models achieved the highest performance across all classes with 99.099 precision and 93.548 F1-score for diabetic_retinopathy. ViT scored the lowest across all classes with 75.102 precision and 79.826 F1-score for normal classes.</p>
<p id="Par61">For integrating transformer models with ML, for ViT-ML models, ViT-RF achieved the highest recall, with a score of 98.636 for diabetic_retinopathy. Meanwhile, ViT-SVM with glaucoma had the lowest precision of 79.126. For DieT-ML, DeiT-RF had the highest recall for diabetic_retinopathy, scoring 98.182. and DeiT-SVM recorded the lowest recall with 80.198 for glaucoma. Swin-RF had the highest precision, recall, and F1 Score for Swin-PCA-ML models in every class, with a score of 99.087 for diabetic_retinopathy.</p>
<p id="Par62">Similarly, combining transformer models with PCA and ML yields better results than combining transformer models with ML since PCA chooses the best features from the feature representation matrix. The integration of Swin-PCA-RF scored the highest because Swin captures local and global features. RF provides insights into the importance of different features, which is useful in understanding the underlying relationships in the data.</p>
<p id="Par63">For ViT-PCA-ML models, ViT-PCA-RF scored the highest recall, with 99.545 for diabetic_retinopathy. It also had the highest precision and recall for glaucoma and normal, with 83.575 and 87.273, respectively. ViT-PCA-SVM scored the lowest recall, with 81.683 for glaucoma. It had the same recall for cataracts and normal, around 87. For DieT-PCA-ML, DeiT-PCA-RF had the highest recall for diabetic retinopathy, scoring 99.545. At 87.624 and 90.741, respectively, it likewise had the highest recall for glaucoma and normal. For Swin-PCA-ML models, Swin-PCA-RF recorded the highest precision, recall and F1 Score across all classes, with 100 for diabetic retinopathy and 95.673 for cataract. Swin-PCA-SVM scored the worst precision, with 86.667 for glaucoma and the same F1-score, round 88, for normal and glaucoma.</p>
<p id="Par64">The proposed model (MST-EDS), MST-EDS-RF enhanced results and scored the best performance across all classes, with 100 precision for diabetic_retinopathy and 97.573 precision for cataract. MST-EDS-SVM recorded the lowest precision, at 88.995 and 90.511 F1-score for glaucoma.</p></section><section id="Sec18"><h3 class="pmc_sec_title">Results of the average performance of the models</h3>
<p id="Par65">Table <a href="#Tab7" class="usa-link">7</a> presents the average of accuracy, precision, recall, and F1-score of different models for each class across approaches: standalone models, hybrid models, and MST-EDS.</p>
<section class="tw xbox font-sm" id="Tab7"><h4 class="obj_head">Table 7.</h4>
<div class="caption p"><p>Results of the average performance of the models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Approaches</th>
<th align="left" colspan="1" rowspan="1">Models</th>
<th align="left" colspan="1" rowspan="1">Accuracy</th>
<th align="left" colspan="1" rowspan="1">Precision</th>
<th align="left" colspan="1" rowspan="1">Recall</th>
<th align="left" colspan="1" rowspan="1">F1-score</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="3" colspan="1">Standalone models</td>
<td align="left" colspan="1" rowspan="1">Swin</td>
<td align="left" colspan="1" rowspan="1">91.962</td>
<td align="left" colspan="1" rowspan="1">92.075</td>
<td align="left" colspan="1" rowspan="1">91.962</td>
<td align="left" colspan="1" rowspan="1">91.954</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT</td>
<td align="left" colspan="1" rowspan="1">86.052</td>
<td align="left" colspan="1" rowspan="1">86.539</td>
<td align="left" colspan="1" rowspan="1">86.052</td>
<td align="left" colspan="1" rowspan="1">86.171</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeiT</td>
<td align="left" colspan="1" rowspan="1">87.116</td>
<td align="left" colspan="1" rowspan="1">87.511</td>
<td align="left" colspan="1" rowspan="1">87.116</td>
<td align="left" colspan="1" rowspan="1">87.223</td>
</tr>
<tr>
<td align="left" rowspan="9" colspan="1">Model-ML</td>
<td align="left" colspan="1" rowspan="1">ViT-RF</td>
<td align="left" colspan="1" rowspan="1">89.716</td>
<td align="left" colspan="1" rowspan="1">89.770</td>
<td align="left" colspan="1" rowspan="1">89.716</td>
<td align="left" colspan="1" rowspan="1">89.722</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT-LR</td>
<td align="left" colspan="1" rowspan="1">89.007</td>
<td align="left" colspan="1" rowspan="1">89.082</td>
<td align="left" colspan="1" rowspan="1">89.007</td>
<td align="left" colspan="1" rowspan="1">89.023</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT- SVM</td>
<td align="left" colspan="1" rowspan="1">87.707</td>
<td align="left" colspan="1" rowspan="1">87.777</td>
<td align="left" colspan="1" rowspan="1">87.707</td>
<td align="left" colspan="1" rowspan="1">87.717</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeiT-RF</td>
<td align="left" colspan="1" rowspan="1">91.135</td>
<td align="left" colspan="1" rowspan="1">91.250</td>
<td align="left" colspan="1" rowspan="1">91.135</td>
<td align="left" colspan="1" rowspan="1">91.159</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeiT-LR</td>
<td align="left" colspan="1" rowspan="1">89.716</td>
<td align="left" colspan="1" rowspan="1">89.741</td>
<td align="left" colspan="1" rowspan="1">89.716</td>
<td align="left" colspan="1" rowspan="1">89.717</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeiT-SVM</td>
<td align="left" colspan="1" rowspan="1">88.652</td>
<td align="left" colspan="1" rowspan="1">88.650</td>
<td align="left" colspan="1" rowspan="1">88.652</td>
<td align="left" colspan="1" rowspan="1">88.635</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Swin-RF</td>
<td align="left" colspan="1" rowspan="1">93.144</td>
<td align="left" colspan="1" rowspan="1">93.180</td>
<td align="left" colspan="1" rowspan="1">93.144</td>
<td align="left" colspan="1" rowspan="1">93.148</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Swin-LR</td>
<td align="left" colspan="1" rowspan="1">92.553</td>
<td align="left" colspan="1" rowspan="1">92.585</td>
<td align="left" colspan="1" rowspan="1">92.553</td>
<td align="left" colspan="1" rowspan="1">92.562</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Swin-SVM</td>
<td align="left" colspan="1" rowspan="1">91.608</td>
<td align="left" colspan="1" rowspan="1">91.762</td>
<td align="left" colspan="1" rowspan="1">91.608</td>
<td align="left" colspan="1" rowspan="1">91.630</td>
</tr>
<tr>
<td align="left" rowspan="9" colspan="1">Model-PCA-ML</td>
<td align="left" colspan="1" rowspan="1">ViT-PCA-RF</td>
<td align="left" colspan="1" rowspan="1">91.017</td>
<td align="left" colspan="1" rowspan="1">91.108</td>
<td align="left" colspan="1" rowspan="1">91.017</td>
<td align="left" colspan="1" rowspan="1">91.036</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT-PCA-LR</td>
<td align="left" colspan="1" rowspan="1">90.071</td>
<td align="left" colspan="1" rowspan="1">90.107</td>
<td align="left" colspan="1" rowspan="1">90.071</td>
<td align="left" colspan="1" rowspan="1">90.070</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT-PCA-SVM</td>
<td align="left" colspan="1" rowspan="1">88.889</td>
<td align="left" colspan="1" rowspan="1">88.925</td>
<td align="left" colspan="1" rowspan="1">88.889</td>
<td align="left" colspan="1" rowspan="1">88.885</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeiT-PCA-RF</td>
<td align="left" colspan="1" rowspan="1">92.199</td>
<td align="left" colspan="1" rowspan="1">92.296</td>
<td align="left" colspan="1" rowspan="1">92.199</td>
<td align="left" colspan="1" rowspan="1">92.221</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeiT-PCA-LR</td>
<td align="left" colspan="1" rowspan="1">91.135</td>
<td align="left" colspan="1" rowspan="1">91.161</td>
<td align="left" colspan="1" rowspan="1">91.135</td>
<td align="left" colspan="1" rowspan="1">91.136</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeiT-PCA-SVM</td>
<td align="left" colspan="1" rowspan="1">89.835</td>
<td align="left" colspan="1" rowspan="1">89.832</td>
<td align="left" colspan="1" rowspan="1">89.835</td>
<td align="left" colspan="1" rowspan="1">89.817</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Swin-PCA-RF</td>
<td align="left" colspan="1" rowspan="1">94.799</td>
<td align="left" colspan="1" rowspan="1">94.797</td>
<td align="left" colspan="1" rowspan="1">94.799</td>
<td align="left" colspan="1" rowspan="1">94.793</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Swin-PCA-LR</td>
<td align="left" colspan="1" rowspan="1">94.326</td>
<td align="left" colspan="1" rowspan="1">94.346</td>
<td align="left" colspan="1" rowspan="1">94.326</td>
<td align="left" colspan="1" rowspan="1">94.324</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Swin-PCA-SVM</td>
<td align="left" colspan="1" rowspan="1">92.908</td>
<td align="left" colspan="1" rowspan="1">92.935</td>
<td align="left" colspan="1" rowspan="1">92.908</td>
<td align="left" colspan="1" rowspan="1">92.897</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">The proposed model</td>
<td align="left" colspan="1" rowspan="1">MST-EDS-RF</td>
<td align="left" colspan="1" rowspan="1">97.163</td>
<td align="left" colspan="1" rowspan="1">97.168</td>
<td align="left" colspan="1" rowspan="1">97.163</td>
<td align="left" colspan="1" rowspan="1">97.164</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MST-EDS-LR</td>
<td align="left" colspan="1" rowspan="1">95.745</td>
<td align="left" colspan="1" rowspan="1">95.760</td>
<td align="left" colspan="1" rowspan="1">95.745</td>
<td align="left" colspan="1" rowspan="1">95.747</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MST-EDS-SVM</td>
<td align="left" colspan="1" rowspan="1">94.326</td>
<td align="left" colspan="1" rowspan="1">94.355</td>
<td align="left" colspan="1" rowspan="1">94.326</td>
<td align="left" colspan="1" rowspan="1">94.320</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par66">As shown in Table <a href="#Tab7" class="usa-link">7</a>, standalone transformer models were evaluated, where Swin achieved the highest performance with 91.962 accuracy, 92.075 precision, 91.962 recall, and 91.954 F1-score because Swin captures local and global features using hierarchical representation and attention mechanisms. While, ViT presented poorly with 87.116 accuracy, 87.511 precision, 87.116 recall, and 87.223 F1-score.</p>
<p id="Par67">For integrating transformer models with ML, transformer models integrating with RF recorded the best performance because RF provides insights into the importance of different features, which helps understand the underlying relationships in the data. As a result, Swin-RF recorded the best performance at 93.144 accuracy, 93.180 precision, 93.144 recall, and 93.148 F1-score. Meanwhile, ViT-SVM recorded the lowest performance, with 87.707 accuracy and 87.717 F1-score.</p>
<p id="Par68">In the same way, integrating transformer models with PCA and ML results in the best performance compared to integrating transformer models with ML because PCA selects the best features from the feature representation matrix, as shown in the Table <a href="#Tab7" class="usa-link">7</a>. As a result, Swin-PCA-RF performed scientifically with 94.799 accuracy, 94.797 precision, 94.799 recall, and 94.793 F1-score, resulting in Swin using attention processes and hierarchical representation to capture both local and global features. While transformer models integrating with SVM scored the worst, due to SVM’s limitations with high-dimensional embeddings and difficulty handling large datasets, as a result, ViT-PCA-SVM registered the worst results with 88.889 accuracy, 88.925 precision, 88.889 recall, and 88.885 F1-score.</p>
<p id="Par69">The proposed model (MST-EDS) enhanced performance by 2% compared to Swin-RF because stacking the outputs of Swin-PCA-RF, ViT-PCA-RF, and DeiT-PCA-RF with RF as a meta-learner effectively learns optimal feature combinations, enhancing generalization and performance. MST-EDS-RF records the best performances with 97.163 accuracy, 97.168 recall, 97.163 precision, and 97.164 F1-score compared to other models.</p>
<p id="Par70">The experimental results showed that ML models improve the performance of the MST-EDS system by effectively leveraging deep, high-dimensional feature representations created by transformer models: ViT, Swin, and DeiT. Unlike the traditional SoftMax classifier, which applies a shallow, linear decision layer, ML models such as RF and SVM can capture complex, non-linear patterns within the reduced feature space by PCA, which reduces redundancy and highlights the most informative features. These models offer greater robustness and improved classification accuracy.</p>
<p id="Par71">Overall, the combination of transformer model, feature fusion, and ensemble learning in the multi-stage framework provides a more complicated and effective method for eye disease classification, which can overcome the shortcomings of the traditional SoftMax classification methods.</p></section><section id="Sec19"><h3 class="pmc_sec_title">Discussion</h3>
<p id="Par72">Looking at Fig. <a href="#Fig7" class="usa-link">7</a>, it is clear that MST-EDS-RF recorded the highest performance because, by stacking the outputs of hybrid models with RF, optimal feature combinations are effectively learned, and generalization and performance are enhanced. This suggests that adopting sophisticated AI tools in the clinic might improve diagnostic accuracy in eye diseases. The experimental results of MST-EDS, which integrates advanced transformer-based models (Swin, ViT, and DeiT) along with dimensionality reduction (PCA) and ML classifiers, have several implications for the future development of AI tools in medical diagnostics, particularly for eye disease detection: (1) Multimodal and hybrid design potential: By integrating deep transformer architectures with ML, MST-EDS proves that the hybrid approach can improve performance and generalizability, especially when dealing with image medical data. (2) Scalable architecture: MST-EDS’s modularity promotes the method’s transferability by enabling future researchers and developers to modify the pipeline to other imaging modalities or medical areas outside of ophthalmology. (3) Clinical Value and decision support: MST-EDS has the potential to be a dependable decision support system that can help ophthalmologists detect diseases early and accurately, especially in areas with limited access to experts, thanks to its strong diagnostic performance and capacity to incorporate multi-input feature representation. These implications emphasize that MST-EDS advances technical performance and aligns with the practical needs of scalable, interpretable, and clinically deployable AI solutions.</p>
<figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365307_41598_2025_16415_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/27f03c95d265/41598_2025_16415_Fig7_HTML.jpg" loading="lazy" id="MO7" height="400" width="684" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Performance comparison of standalone, hybrid, and proposed models using accuracy, precision, recall, and F1-Score.</p></figcaption></figure><p id="Par73">Figure <a href="#Fig8" class="usa-link">8</a> shows three confusion matrices for MST-EDS-RF, MST-EDS-LR, MST-EDS-SVM. Each confusion matrix evaluates the performance of a classification task involving four classes: cataract, diabetic retinopathy, glaucoma, and normal. All three models show high classification accuracy for diabetic retinopathy. Glaucoma appears to be the most challenging to classify accurately, with a higher number of misclassifications. The MST-EDS-RF model seems to have slightly better performance compared to LR and SVM in terms of lower misclassification rates.</p>
<figure class="fig xbox font-sm" id="Fig8"><h4 class="obj_head">Fig. 8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365307_41598_2025_16415_Fig8_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa1e/12365307/64ab2bd70541/41598_2025_16415_Fig8_HTML.jpg" loading="lazy" id="MO8" height="639" width="793" alt="Fig. 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Confusion matrixs of MST-EDS-RF, MST-EDS-LR and MST-EDS-SVM transformers for four classes blue color presents the percentage of True Positive (TP) and percentage of True Negative (TN) and white color presents percentage of False Positive (FP) and percentage of False Negatives.</p></figcaption></figure></section><section id="Sec20"><h3 class="pmc_sec_title">Comparison with literature studies</h3>
<p id="Par74">Table <a href="#Tab8" class="usa-link">8</a> compares the proposed model and literature studies using EDC. The EDC dataset was collected from various sources such as IDRiD, Ocular recognition, HRF, retinal_dataset, and DRIVE. The MST-EDS-RF model improves performance by integrating the hybrid models as base models with a meta-model (RF), achieving the highest accuracy at 97.163. Advantages of this design: (1) Diversity in models: Different transformer models, such as ViT, DeiT, and Swin, can be applied to ensure varied feature representations. (2) PCA is applied to minimize dimensionality by selecting the most informative aspects of the data. (3) Stacking Ensemble to improve performance and make generalizations. While the proposed model offers high accuracy and reliability, its reliance on large labeled datasets and computational complexity poses challenges for real-time deployment. The table shows that MST-EDS-RF recorded the best performance compared to other studies using the EDC dataset. In<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>, the authors applied the EfficientNet and CNN models and achieved an accuracy of 94 and 84, respectively. In<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>, the authors applied BayeSVM500 and achieved an accuracy 95.33. An ensemble approach was deployed in the study by<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>, leading to an accuracy of 96.1, and they compared it with EfficientNetB6 and DenseNet169, which recorded 88.3 and 93.9 of accuracy, respectively. In<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>, EfficientNetB3 was utilized, yielding an accuracy 93.8.</p>
<section class="tw xbox font-sm" id="Tab8"><h4 class="obj_head">Table 8.</h4>
<div class="caption p"><p>Comparison with literature studies.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Papers</th>
<th align="left" colspan="1" rowspan="1">Models</th>
<th align="left" colspan="1" rowspan="1">Datasets</th>
<th align="left" colspan="1" rowspan="1">Accuracy</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="2" colspan="1">Babaqi<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">EfficientNet</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">94</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CNN model</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">84</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Tasnim et al.<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">BayeSVM500</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">95.33</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">Abdullah et al.<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">Ensemble approach</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">96.1</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">EfficientNetB6</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">88.5</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DenseNet169</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">93.9</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Soni et al.<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">EfficientNetB3</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">93.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Our work</td>
<td align="left" colspan="1" rowspan="1">MST-EDS-RF</td>
<td align="left" colspan="1" rowspan="1">EDC</td>
<td align="left" colspan="1" rowspan="1">97.163</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section><section id="Sec21"><h2 class="pmc_sec_title">Limitation and future work</h2>
<p id="Par75">The proposed model recorded the best performance in eye disease classification, several limitations must be noted. First, the model was tested and evaluated using an annotated large dataset, which may not be readily available in all clinical settings. Second, further validation on diverse and external datasets is required to evaluate the generalizability and robustness of the proposed model. Third, the transformer-ensemble architecture’s computational complexity makes it difficult to deploy in situations with limited resources, especially for real-time applications. Lastly, our framework does not integrate explainability mechanisms, which are essential for supporting clinical decision-making and the trust of healthcare professionals. Future work may focus on reducing computational demands through model optimization techniques such as pruning and quantization. In addition, approaches like self-supervised learning and domain adaptation could improve performance in low-data scenarios. To support clinical adoption, integrating explainable AI methods may enhance transparency and foster trust among healthcare professionals.</p></section><section id="Sec22"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par76">This study proposed a multi-stage framework for eye diseases (MST-EDS) to classify eye illnesses across four classes: normal, diabetic, glaucoma, and cataract, utilizing a benchmark dataset from Kaggle. Our approach leverages transformer models, hybrid models, feature selection methods, and ML models, leading to robust decision-making. It aims to improve accuracy and generalization in complex medical image classification tasks compared to existing methods.</p>
<p id="Par77">It is developed in two stages: hybrid and stacking models. In hybrid models, transformer models ViT, DeiT, and Swin extract deep features from images. PCA is used to reduce the complexity of the extracted features and select the best features. The resulting optimized features are then classified using ML models (RF, SVM, and LR). In the stacking stage, the best hybrid models are selected based on their performance and used to generate prediction outputs, which are then stacked into a stacking training set and a stacking testing set. Stacking training is used to train meta-learners (RF, SVM, and LR), and the stacking testing set is used to evaluate further and enhance overall classification performance. In addition, we conducted different experiments based on various approaches, including standalone transformers, hybrid models, and the proposed model MST-EDS. The experimental results indicated that the MST-EDS-RF model recorded the best results compared to individual transformer and hybrid models. It achieves 97.163 accuracy, 97.168 precision, 97.163 recall, and 97.164 F1-Score.</p>
<p id="Par78">The results demonstrate the potential of integrating transformer-based models with ensemble learning techniques to enhance the classification of eye diseases. This approach may contribute to the development of advanced AI-assisted tools in medical diagnostics.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>A.A. done conceptualization; data curation, formal analysis, funding acquisition, investigation, methodology, software, H.S.; Writing—original draft; Writing—review and editing. All authors have read and agreed to the published version of the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Funding</h2>
<p>No funding was received to conduct this study.</p></section><section id="notes3"><h2 class="pmc_sec_title">Data availability</h2>
<p>The data that support the findings of this study are publicly available at the following URL: <a href="https://www.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification</a></p></section><section id="notes4"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par83">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Dar, M. A., Maqbool, M., Ara, I. &amp; Qadrie, Z. Preserving sight: Managing and preventing diabetic retinopathy. <em>Open Health</em><strong>4</strong>, 20230019 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Dar,%20M.%20A.,%20Maqbool,%20M.,%20Ara,%20I.%20&amp;%20Qadrie,%20Z.%20Preserving%20sight:%20Managing%20and%20preventing%20diabetic%20retinopathy.%20Open%20Health4,%2020230019%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Cassel, G. H. <em>The Eye Book: A Complete Guide to Eye Disorders and Health</em> (JHU Press, 2021).</cite>
</li>
<li id="CR3">
<span class="label">3.</span><cite>Saleh, G. A. et al. The role of medical image modalities and ai in the early detection, diagnosis and grading of retinal diseases: a survey. <em>Bioengineering</em><strong>9</strong>, 366 (2022).
</cite> [<a href="https://doi.org/10.3390/bioengineering9080366" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9405367/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36004891/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Saleh,%20G.%20A.%20et%20al.%20The%20role%20of%20medical%20image%20modalities%20and%20ai%20in%20the%20early%20detection,%20diagnosis%20and%20grading%20of%20retinal%20diseases:%20a%20survey.%20Bioengineering9,%20366%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR4">
<span class="label">4.</span><cite>Yang, L., Li, J., Zhou, B. &amp; Wang, Y. An injectable copolymer for in situ lubrication effectively relieves dry eye disease. <em>ACS Mater. Lett.</em><strong>7</strong>, 884–890 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yang,%20L.,%20Li,%20J.,%20Zhou,%20B.%20&amp;%20Wang,%20Y.%20An%20injectable%20copolymer%20for%20in%20situ%20lubrication%20effectively%20relieves%20dry%20eye%20disease.%20ACS%20Mater.%20Lett.7,%20884%E2%80%93890%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Kelly, S. <em>Using large-scale visual field data to gain insights into management of patients with glaucoma</em>. Ph.D. thesis, City, University of London (2019).</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>Thompson, A. C., Jammal, A. A. &amp; Medeiros, F. A. A review of deep learning for screening, diagnosis, and detection of glaucoma progression. <em>Transl. Vis. Sci. Technol.</em><strong>9</strong>, 42–42 (2020).
</cite> [<a href="https://doi.org/10.1167/tvst.9.2.42" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7424906/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32855846/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Thompson,%20A.%20C.,%20Jammal,%20A.%20A.%20&amp;%20Medeiros,%20F.%20A.%20A%20review%20of%20deep%20learning%20for%20screening,%20diagnosis,%20and%20detection%20of%20glaucoma%20progression.%20Transl.%20Vis.%20Sci.%20Technol.9,%2042%E2%80%9342%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<span class="label">7.</span><cite>Dolar-Szczasny, J., Barańska, A. &amp; Rejdak, R. Evaluating the efficacy of teleophthalmology in delivering ophthalmic care to underserved populations: a literature review. <em>J. Clin. Med.</em><strong>12</strong>, 3161 (2023).
</cite> [<a href="https://doi.org/10.3390/jcm12093161" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10179149/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37176602/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Dolar-Szczasny,%20J.,%20Bara%C5%84ska,%20A.%20&amp;%20Rejdak,%20R.%20Evaluating%20the%20efficacy%20of%20teleophthalmology%20in%20delivering%20ophthalmic%20care%20to%20underserved%20populations:%20a%20literature%20review.%20J.%20Clin.%20Med.12,%203161%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>Singh, L. K. et al. An artificial intelligence-based smart system for early glaucoma recognition using oct images. <em>Int. J. E-Health Med. Commun. IJEHMC</em><strong>12</strong>, 32–59 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Singh,%20L.%20K.%20et%20al.%20An%20artificial%20intelligence-based%20smart%20system%20for%20early%20glaucoma%20recognition%20using%20oct%20images.%20Int.%20J.%20E-Health%20Med.%20Commun.%20IJEHMC12,%2032%E2%80%9359%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>Singh, L. K., Garg, H. <em>et al.</em> Detection of glaucoma in retinal fundus images using fast fuzzy c means clustering approach. In <em>2019 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)</em>, 397–403 (IEEE, 2019).</cite>
</li>
<li id="CR10">
<span class="label">10.</span><cite>Zeng, Y. <em>et al.</em> Gccnet: A novel network leveraging gated cross-correlation for multi-view classification. <em>IEEE Trans. Multimed.</em> (2024).</cite>
</li>
<li id="CR11">
<span class="label">11.</span><cite>Hu, C., Sapkota, B. B., Thomasson, J. A. &amp; Bagavathiannan, M. V. Influence of image quality and light consistency on the performance of convolutional neural networks for weed mapping. <em>Remote Sens.</em><strong>13</strong>, 2140 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Hu,%20C.,%20Sapkota,%20B.%20B.,%20Thomasson,%20J.%20A.%20&amp;%20Bagavathiannan,%20M.%20V.%20Influence%20of%20image%20quality%20and%20light%20consistency%20on%20the%20performance%20of%20convolutional%20neural%20networks%20for%20weed%20mapping.%20Remote%20Sens.13,%202140%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Kumar, Y., Koul, A., Singla, R. &amp; Ijaz, M. F. Artificial intelligence in disease diagnosis: a systematic literature review, synthesizing framework and future research agenda. <em>J. Ambient. Intell. Humaniz. Comput.</em><strong>14</strong>, 8459–8486 (2023).
</cite> [<a href="https://doi.org/10.1007/s12652-021-03612-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8754556/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35039756/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Kumar,%20Y.,%20Koul,%20A.,%20Singla,%20R.%20&amp;%20Ijaz,%20M.%20F.%20Artificial%20intelligence%20in%20disease%20diagnosis:%20a%20systematic%20literature%20review,%20synthesizing%20framework%20and%20future%20research%20agenda.%20J.%20Ambient.%20Intell.%20Humaniz.%20Comput.14,%208459%E2%80%938486%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>Santos, C. F. G. D. &amp; Papa, J. P. Avoiding overfitting: A survey on regularization methods for convolutional neural networks. <em>ACM Comput. Surv. Csur</em><strong>54</strong>, 1–25 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Santos,%20C.%20F.%20G.%20D.%20&amp;%20Papa,%20J.%20P.%20Avoiding%20overfitting:%20A%20survey%20on%20regularization%20methods%20for%20convolutional%20neural%20networks.%20ACM%20Comput.%20Surv.%20Csur54,%201%E2%80%9325%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Nerella, S. <em>et al.</em> Transformers in healthcare: A survey. <em>arXiv preprint</em><a href="http://arxiv.org/abs/2307.00067" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2307.00067</a> (2023).</cite>
</li>
<li id="CR15">
<span class="label">15.</span><cite>Chitty-Venkata, K. T., Mittal, S., Emani, M., Vishwanath, V. &amp; Somani, A. K. A survey of techniques for optimizing transformer inference. <em>J. Syst. Architect.</em><strong>144</strong>, 102990 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chitty-Venkata,%20K.%20T.,%20Mittal,%20S.,%20Emani,%20M.,%20Vishwanath,%20V.%20&amp;%20Somani,%20A.%20K.%20A%20survey%20of%20techniques%20for%20optimizing%20transformer%20inference.%20J.%20Syst.%20Architect.144,%20102990%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>Liu, Z. <em>et al.</em> Swin transformer: Hierarchical vision transformer using shifted windows. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 10012–10022 (2021).</cite>
</li>
<li id="CR17">
<span class="label">17.</span><cite>Cord, M. Going deeper with image transformers. In <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em> (2021).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>Ganaie, M. A., Hu, M., Malik, A. K., Tanveer, M. &amp; Suganthan, P. N. Ensemble deep learning: A review. <em>Eng. Appl. Artif. Intell.</em><strong>115</strong>, 105151 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ganaie,%20M.%20A.,%20Hu,%20M.,%20Malik,%20A.%20K.,%20Tanveer,%20M.%20&amp;%20Suganthan,%20P.%20N.%20Ensemble%20deep%20learning:%20A%20review.%20Eng.%20Appl.%20Artif.%20Intell.115,%20105151%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19">
<span class="label">19.</span><cite>Matlock, K., De Niz, C., Rahman, R., Ghosh, S. &amp; Pal, R. Investigation of model stacking for drug sensitivity prediction. <em>BMC Bioinform.</em><strong>19</strong>, 21–33 (2018).</cite> [<a href="https://doi.org/10.1186/s12859-018-2060-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5872495/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29589559/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Matlock,%20K.,%20De%20Niz,%20C.,%20Rahman,%20R.,%20Ghosh,%20S.%20&amp;%20Pal,%20R.%20Investigation%20of%20model%20stacking%20for%20drug%20sensitivity%20prediction.%20BMC%20Bioinform.19,%2021%E2%80%9333%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Aslam, J., Arshed, M. A., Iqbal, S. &amp; Hasnain, H. M. Deep learning based multi-class eye disease classification: Enhancing vision health diagnosis. <em>Tech. J.</em><strong>29</strong>, 7–12 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Aslam,%20J.,%20Arshed,%20M.%20A.,%20Iqbal,%20S.%20&amp;%20Hasnain,%20H.%20M.%20Deep%20learning%20based%20multi-class%20eye%20disease%20classification:%20Enhancing%20vision%20health%20diagnosis.%20Tech.%20J.29,%207%E2%80%9312%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR21">
<span class="label">21.</span><cite>Wang, D., Lian, J. &amp; Jiao, W. Multi-label classification of retinal disease via a novel vision transformer model. <em>Front. Neurosci.</em><strong>17</strong>, 1290803 (2024).
</cite> [<a href="https://doi.org/10.3389/fnins.2023.1290803" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10800810/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38260025/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wang,%20D.,%20Lian,%20J.%20&amp;%20Jiao,%20W.%20Multi-label%20classification%20of%20retinal%20disease%20via%20a%20novel%20vision%20transformer%20model.%20Front.%20Neurosci.17,%201290803%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Abbas, Q., Albathan, M., Altameem, A., Almakki, R. S. &amp; Hussain, A. Deep-ocular: Improved transfer learning architecture using self-attention and dense layers for recognition of ocular diseases. <em>Diagnostics</em><strong>13</strong>, 3165 (2023).
</cite> [<a href="https://doi.org/10.3390/diagnostics13203165" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10605427/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37891986/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Abbas,%20Q.,%20Albathan,%20M.,%20Altameem,%20A.,%20Almakki,%20R.%20S.%20&amp;%20Hussain,%20A.%20Deep-ocular:%20Improved%20transfer%20learning%20architecture%20using%20self-attention%20and%20dense%20layers%20for%20recognition%20of%20ocular%20diseases.%20Diagnostics13,%203165%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR23">
<span class="label">23.</span><cite>Omuya, E. O., Okeyo, G. O. &amp; Kimwele, M. W. Feature selection for classification using principal component analysis and information gain. <em>Expert Syst. Appl.</em><strong>174</strong>, 114765 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Omuya,%20E.%20O.,%20Okeyo,%20G.%20O.%20&amp;%20Kimwele,%20M.%20W.%20Feature%20selection%20for%20classification%20using%20principal%20component%20analysis%20and%20information%20gain.%20Expert%20Syst.%20Appl.174,%20114765%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Albelaihi, A. &amp; Ibrahim, D. M. Deepdiabetic: An identification system of diabetic eye diseases using deep neural networks. <em>IEEE Access</em>. (2024).</cite>
</li>
<li id="CR25">
<span class="label">25.</span><cite>Demir, F. &amp; Taşcı, B. An effective and robust approach based on r-cnn+ lstm model and ncar feature selection for ophthalmological disease detection from fundus images. <em>J. Pers. Med.</em><strong>11</strong>, 1276 (2021).
</cite> [<a href="https://doi.org/10.3390/jpm11121276" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8709012/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34945747/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Demir,%20F.%20&amp;%20Ta%C5%9Fc%C4%B1,%20B.%20An%20effective%20and%20robust%20approach%20based%20on%20r-cnn+%20lstm%20model%20and%20ncar%20feature%20selection%20for%20ophthalmological%20disease%20detection%20from%20fundus%20images.%20J.%20Pers.%20Med.11,%201276%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<span class="label">26.</span><cite>Babaqi, T., Jaradat, M., Yildirim, A. E., Al-Nimer, S. H. &amp; Won, D. Eye disease classification using deep learning techniques. <em>arXiv preprint</em><a href="http://arxiv.org/abs/2307.10501" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2307.10501</a> (2023).</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Zannah, T. B. et al. Bayesian optimized machine learning model for automated eye disease classification from fundus images. <em>Computation</em><strong>12</strong>, 190 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zannah,%20T.%20B.%20et%20al.%20Bayesian%20optimized%20machine%20learning%20model%20for%20automated%20eye%20disease%20classification%20from%20fundus%20images.%20Computation12,%20190%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Abdullah, A. A., Aldhahab, A. &amp; Al Abboodi, H. M. Deep-ensemble learning models for the detection and classification of eye diseases based on engineering feature extraction with efficientb6 and densnet169. <em>Int. J. Intell. Eng. Syst.</em><strong>17</strong> (2024).</cite>
</li>
<li id="CR29">
<span class="label">29.</span><cite>Ryan, J., Nathaniel, D. A., Purwanto, E. S. &amp; Ario, M. K. Harnessing deep learning for ocular disease diagnosis. <em>Proc. Comput. Sci.</em><strong>245</strong>, 914–923 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ryan,%20J.,%20Nathaniel,%20D.%20A.,%20Purwanto,%20E.%20S.%20&amp;%20Ario,%20M.%20K.%20Harnessing%20deep%20learning%20for%20ocular%20disease%20diagnosis.%20Proc.%20Comput.%20Sci.245,%20914%E2%80%93923%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR30">
<span class="label">30.</span><cite>Wahab Sait, A. R. Artificial intelligence-driven eye disease classification model. <em>Appl. Sci.</em><strong>13</strong>, 11437 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wahab%20Sait,%20A.%20R.%20Artificial%20intelligence-driven%20eye%20disease%20classification%20model.%20Appl.%20Sci.13,%2011437%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Hemalakshmi, G., Murugappan, M., Sikkandar, M. Y., Begum, S. S. &amp; Prakash, N. Automated retinal disease classification using hybrid transformer model (svit) using optical coherence tomography images. <em>Neural Comput. Appl.</em> 1–18 (2024).</cite>
</li>
<li id="CR32">
<span class="label">32.</span><cite>Akça, S., Garip, Z., Ekinci, E. &amp; Atban, F. Automated classification of choroidal neovascularization, diabetic macular edema, and drusen from retinal oct images using vision transformers: a comparative study. <em>Lasers Med. Sci.</em><strong>39</strong>, 140 (2024).
</cite> [<a href="https://doi.org/10.1007/s10103-024-04089-w" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11128386/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38797751/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ak%C3%A7a,%20S.,%20Garip,%20Z.,%20Ekinci,%20E.%20&amp;%20Atban,%20F.%20Automated%20classification%20of%20choroidal%20neovascularization,%20diabetic%20macular%20edema,%20and%20drusen%20from%20retinal%20oct%20images%20using%20vision%20transformers:%20a%20comparative%20study.%20Lasers%20Med.%20Sci.39,%20140%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33">
<span class="label">33.</span><cite>Eye diseases classification. <a href="https://www.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification</a> (accessed 2024).</cite>
</li>
<li id="CR34">
<span class="label">34.</span><cite>Vidal, M. &amp; Amigo, J. M. Pre-processing of hyperspectral images. essential steps before image analysis. <em>Chemom. Intell. Lab. Syst.</em><strong>117</strong>, 138–148 (2012).</cite> [<a href="https://scholar.google.com/scholar_lookup?Vidal,%20M.%20&amp;%20Amigo,%20J.%20M.%20Pre-processing%20of%20hyperspectral%20images.%20essential%20steps%20before%20image%20analysis.%20Chemom.%20Intell.%20Lab.%20Syst.117,%20138%E2%80%93148%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Maharana, K., Mondal, S. &amp; Nemade, B. A review: Data pre-processing and data augmentation techniques. <em>Glob. Transit. Proc.</em><strong>3</strong>, 91–99 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Maharana,%20K.,%20Mondal,%20S.%20&amp;%20Nemade,%20B.%20A%20review:%20Data%20pre-processing%20and%20data%20augmentation%20techniques.%20Glob.%20Transit.%20Proc.3,%2091%E2%80%9399%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<span class="label">36.</span><cite>Maitra, I. K., Nag, S. &amp; Bandyopadhyay, S. K. Technique for preprocessing of digital mammogram. <em>Comput. Methods Programs Biomed.</em><strong>107</strong>, 175–188 (2012).
</cite> [<a href="https://doi.org/10.1016/j.cmpb.2011.05.007" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21669471/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Maitra,%20I.%20K.,%20Nag,%20S.%20&amp;%20Bandyopadhyay,%20S.%20K.%20Technique%20for%20preprocessing%20of%20digital%20mammogram.%20Comput.%20Methods%20Programs%20Biomed.107,%20175%E2%80%93188%20(2012)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR37">
<span class="label">37.</span><cite>Huang, L. et al. Normalization techniques in training dnns: Methodology, analysis and application. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em><strong>45</strong>, 10173–10196 (2023).
</cite> [<a href="https://doi.org/10.1109/TPAMI.2023.3250241" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37027763/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Huang,%20L.%20et%20al.%20Normalization%20techniques%20in%20training%20dnns:%20Methodology,%20analysis%20and%20application.%20IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.45,%2010173%E2%80%9310196%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR38">
<span class="label">38.</span><cite>Dosovitskiy, A. <em>et al.</em> An image is worth 16x16 words: Transformers for image recognition at scale. <em>arXiv preprint</em><a href="http://arxiv.org/abs/2010.11929" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2010.11929</a> (2020).</cite>
</li>
<li id="CR39">
<span class="label">39.</span><cite>Chen, C.-F. R., Fan, Q. &amp; Panda, R. Crossvit: Cross-attention multi-scale vision transformer for image classification. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 357–366 (2021).</cite>
</li>
<li id="CR40">
<span class="label">40.</span><cite>Khan, S. et al. Transformers in vision: A survey. <em>ACM Comput. Surv. CSUR.</em><strong>54</strong>, 1–41 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Khan,%20S.%20et%20al.%20Transformers%20in%20vision:%20A%20survey.%20ACM%20Comput.%20Surv.%20CSUR.54,%201%E2%80%9341%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR41">
<span class="label">41.</span><cite>Chen, M., Peng, H., Fu, J. &amp; Ling, H. Autoformer: Searching transformers for visual recognition. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 12270–12280 (2021).</cite>
</li>
<li id="CR42">
<span class="label">42.</span><cite>Jumphoo, T. et al. Exploiting data-efficient image transformer-based transfer learning for valvular heart diseases detection. <em>IEEE Access</em><strong>12</strong>, 15845–15855 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Jumphoo,%20T.%20et%20al.%20Exploiting%20data-efficient%20image%20transformer-based%20transfer%20learning%20for%20valvular%20heart%20diseases%20detection.%20IEEE%20Access12,%2015845%E2%80%9315855%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR43">
<span class="label">43.</span><cite>Yoo, D. &amp; Yoo, J. Fswin transformer: Feature-space window attention vision transformer for image classification. <em>IEEE Access</em>. (2024).</cite>
</li>
<li id="CR44">
<span class="label">44.</span><cite>Liu, Y. <em>et al.</em> Vision transformers with hierarchical attention. <em>Mach. Intell. Res.</em> 1–14 (2024).</cite>
</li>
<li id="CR45">
<span class="label">45.</span><cite>Park, N. &amp; Kim, S. How do vision transformers work? <em>arXiv preprint</em><a href="http://arxiv.org/abs/2202.06709" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2202.06709</a> (2022).</cite>
</li>
<li id="CR46">
<span class="label">46.</span><cite>Liu, Z. <em>et al.</em> Video swin transformer. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 3202–3211 (2022).</cite>
</li>
<li id="CR47">
<span class="label">47.</span><cite>Singh, L. K., Khanna, M., Thawkar, S. &amp; Singh, R. A novel hybridized feature selection strategy for the effective prediction of glaucoma in retinal fundus images. <em>Multimed. Tools Appl.</em><strong>83</strong>, 46087–46159 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Singh,%20L.%20K.,%20Khanna,%20M.,%20Thawkar,%20S.%20&amp;%20Singh,%20R.%20A%20novel%20hybridized%20feature%20selection%20strategy%20for%20the%20effective%20prediction%20of%20glaucoma%20in%20retinal%20fundus%20images.%20Multimed.%20Tools%20Appl.83,%2046087%E2%80%9346159%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR48">
<span class="label">48.</span><cite>Singh, L. K., Khanna, M. &amp; Singh, R. Feature subset selection through nature inspired computing for efficient glaucoma classification from fundus images. <em>Multimed. Tools Appl.</em><strong>83</strong>, 77873–77944 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Singh,%20L.%20K.,%20Khanna,%20M.%20&amp;%20Singh,%20R.%20Feature%20subset%20selection%20through%20nature%20inspired%20computing%20for%20efficient%20glaucoma%20classification%20from%20fundus%20images.%20Multimed.%20Tools%20Appl.83,%2077873%E2%80%9377944%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR49">
<span class="label">49.</span><cite>Yin, R., Tran, V. H., Zhou, X., Zheng, J. &amp; Kwoh, C. K. Predicting antigenic variants of h1n1 influenza virus based on epidemics and pandemics using a stacking model. <em>PLoS One</em><strong>13</strong>, e0207777 (2018).
</cite> [<a href="https://doi.org/10.1371/journal.pone.0207777" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6303045/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30576319/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Yin,%20R.,%20Tran,%20V.%20H.,%20Zhou,%20X.,%20Zheng,%20J.%20&amp;%20Kwoh,%20C.%20K.%20Predicting%20antigenic%20variants%20of%20h1n1%20influenza%20virus%20based%20on%20epidemics%20and%20pandemics%20using%20a%20stacking%20model.%20PLoS%20One13,%20e0207777%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR50">
<span class="label">50.</span><cite>Zhang, R. et al. Mvmrl: a multi-view molecular representation learning method for molecular property prediction. <em>Brief. Bioinform.</em><strong>25</strong>, bbae298 (2024).
</cite> [<a href="https://doi.org/10.1093/bib/bbae298" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11200189/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38920342/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20R.%20et%20al.%20Mvmrl:%20a%20multi-view%20molecular%20representation%20learning%20method%20for%20molecular%20property%20prediction.%20Brief.%20Bioinform.25,%20bbae298%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR51">
<span class="label">51.</span><cite>Başaran, E. <em>et al.</em> Chronic tympanic membrane diagnosis based on deep convolutional neural network. In <em>2019 4th International Conference on Computer Science and Engineering (UBMK)</em>, 1–4 (IEEE, 2019).</cite>
</li>
<li id="CR52">
<span class="label">52.</span><cite>Sertkaya, M. E., Ergen, B. &amp; Togacar, M. Diagnosis of eye retinal diseases based on convolutional neural networks using optical coherence images. In <em>2019 23rd International Conference Electronics</em>, 1–5 (IEEE, 2019).</cite>
</li>
<li id="CR53">
<span class="label">53.</span><cite>Soni, T. Advanced eye disease classification using the efficientnetb3 deep learning model. In <em>2024 3rd International Conference on Automation, Computing and Renewable Systems (ICACRS)</em>, 875–879 (IEEE, 2024).</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The data that support the findings of this study are publicly available at the following URL: <a href="https://www.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification</a></p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-16415-5"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_16415.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (2.9 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12365307/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12365307/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12365307%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365307/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12365307/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12365307/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40830659/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12365307/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40830659/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12365307/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12365307/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="mV9udtRbHm7hoTDk7xSpwOARSt2rxg8cVhhRMaBpfKAdRTX60tK9Of2VlU2PD4HI">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
