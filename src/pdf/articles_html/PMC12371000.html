
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Detection of weeds in teff crops using deep learning and UAV imagery for precision herbicide application - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="AE532FBF8AF23153052FBF0033E70433.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371000/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Detection of weeds in teff crops using deep learning and UAV imagery for precision herbicide application">
<meta name="citation_author" content="Alemu Setargew Kebede">
<meta name="citation_author_institution" content="Department of Information Technology, University of Gondar, Gondar, Ethiopia">
<meta name="citation_author" content="Tsehay Wasihun Muluneh">
<meta name="citation_author_institution" content="Department of Information Technology, University of Gondar, Gondar, Ethiopia">
<meta name="citation_author" content="Abebe Belay Adege">
<meta name="citation_author_institution" content="Department of Agricultural and Biological Engineering, Institution of Food and Agricultural Science, University of Florida, Gainesville, Florida USA">
<meta name="citation_author_institution" content="Department of Information Technology, Institute of Technology, Debre Markos University, Debre Markos, Amhara Ethiopia">
<meta name="citation_publication_date" content="2025 Aug 21">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30708">
<meta name="citation_doi" content="10.1038/s41598-025-15380-3">
<meta name="citation_pmid" content="40841734">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371000/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371000/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371000/pdf/41598_2025_Article_15380.pdf">
<meta name="description" content="In Ethiopia, Teff is a vital staple crop, yet its productivity is significantly challenges due to inefficient weed and fertilizer management, threatening food security. Traditional weed control methods rely on manual labor and the indiscriminate ...">
<meta name="og:title" content="Detection of weeds in teff crops using deep learning and UAV imagery for precision herbicide application">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="In Ethiopia, Teff is a vital staple crop, yet its productivity is significantly challenges due to inefficient weed and fertilizer management, threatening food security. Traditional weed control methods rely on manual labor and the indiscriminate ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371000/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12371000">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-15380-3"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_15380.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12371000%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12371000/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12371000/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371000/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 21;15:30708. doi: <a href="https://doi.org/10.1038/s41598-025-15380-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-15380-3</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Detection of weeds in teff crops using deep learning and UAV imagery for precision herbicide application</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kebede%20AS%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Alemu Setargew Kebede</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Alemu Setargew Kebede</span></h3>
<div class="p">
<sup>1</sup>Department of Information Technology, University of Gondar, Gondar, Ethiopia </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kebede%20AS%22%5BAuthor%5D" class="usa-link"><span class="name western">Alemu Setargew Kebede</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Muluneh%20TW%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Tsehay Wasihun Muluneh</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Tsehay Wasihun Muluneh</span></h3>
<div class="p">
<sup>1</sup>Department of Information Technology, University of Gondar, Gondar, Ethiopia </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Muluneh%20TW%22%5BAuthor%5D" class="usa-link"><span class="name western">Tsehay Wasihun Muluneh</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Adege%20AB%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Abebe Belay Adege</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Abebe Belay Adege</span></h3>
<div class="p">
<sup>2</sup>Department of Agricultural and Biological Engineering, Institution of Food and Agricultural Science, University of Florida, Gainesville, Florida USA </div>
<div class="p">
<sup>3</sup>Department of Information Technology, Institute of Technology, Debre Markos University, Debre Markos, Amhara Ethiopia </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Adege%20AB%22%5BAuthor%5D" class="usa-link"><span class="name western">Abebe Belay Adege</span></a>
</div>
</div>
<sup>2,</sup><sup>3,</sup><sup>✉</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Department of Information Technology, University of Gondar, Gondar, Ethiopia </div>
<div id="Aff2">
<sup>2</sup>Department of Agricultural and Biological Engineering, Institution of Food and Agricultural Science, University of Florida, Gainesville, Florida USA </div>
<div id="Aff3">
<sup>3</sup>Department of Information Technology, Institute of Technology, Debre Markos University, Debre Markos, Amhara Ethiopia </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Nov 14; Accepted 2025 Aug 7; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12371000  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40841734/" class="usa-link">40841734</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">In Ethiopia, Teff is a vital staple crop, yet its productivity is significantly challenges due to inefficient weed and fertilizer management, threatening food security. Traditional weed control methods rely on manual labor and the indiscriminate application of herbicides, resulting in inaccurate targeting, inefficient distribution, excessive labor, and reduced yields. Non-selective weed management often leads to herbicide misuse, compounded by the difficulty in distinguishing Teff from visually similar weeds, particularly on large farms. This study introduces an optimized deep learning model for weed detection in Teff fields, enabling selective and efficient herbicide application through unmanned aerial vehicles (UAVs). A dataset of 1308 high-resolution drone-captured images was collected across various growth stages and weather conditions from the University of Gondar Agricultural Research Farm and surrounding farms. Key shape-based features such as aspect ratio (AR) and solidity were utilized to enhance model performance. Further, we applied data augmentations at different ratios of the original dataset and experimented with various optimizers to enhance the model’s adaptabliy to different data characteristics and minimize overfitting problems. Deep learning models, such as MobileNetV2, InceptionResNetV2, DenseNet201, VGG16, Resnet50, Fast R-CNN, and YOLOv8, were evaluated with and without fine-tuning. Among the models, fine-tuned MobileNetV2 achieved the highest accuracy (96.40%), demonstrating its potential for practical implementation in UAV-assisted precision agriculture. This work highlights the transformative role of AI-driven solutions in enhancing weed management and improving Teff crop productivity.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Deep learning, Weed detection, Precision agriculture, Drone technology</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Computational biology and bioinformatics, Engineering, Mathematics and computing</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Teff, a staple crop of immense cultural and economic importance in Ethiopia, sustaining over 57.2 million people, accounting for more than 64% of the country’s population<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>. It plays a crucial role in Ethiopian traditions and food security. Despite its resilience to diseases and insect pests compared to other Ethiopian crops, Teff productivity is significantly threatened by weeds, particularly during the V3–V4 growth stages. The V3 stage refers to the phase where the Teff plant has developed three fully formed leaves, while the V4 stage is characterized by the presence of four fully formed leaves. These stages are critical for weed management, as weeds compete most aggressively with Teff for nutrients and moisture during this time, leading to potential yield resuction<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>. Uncontrolled weed growth remains a primary challenge, causing major yield losses. According to Gebretsadik, Haile, and Yamoah<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>, Teffee crops can suffer biomass reductions of up to 36% and grain yield losses of up to 38% due to infestations from weed species such as Cyperus esculentus, Setaria pumila, Avena abyssinica, and Galinsoga parviflora.</p>
<p id="Par3">Ethiopian farmers traditionally rely on manual labor and, more recently, chemical herbicides like glyphosate for weed control. These methods, while commonly used, are labor-intensive, environmentally harmful, and costly, particularly for larger farms <sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>. Current herbicide application techniques are non-selective, posing risks to both the environment and human health<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup>. In<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>, the study emphasize the limitations of conventional spraying methods, advocating for more precise and cost-effective solutions, such as UAV-based technologies. Gebretsadik, Haile, and Yamoah<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup> argue that traditional approaches fail to effectively manage weed populations, resulting in substantial yield losses. Alzubaidi et al.<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup> highlight that these challenges can be mitigated through the development of effective deep learning models for weed detection, which are critical for advancing modern, automated solutions. However, Murad et al.<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup> identify a significant research gap in addressing weed detection in Teff crops and developing precise weed control strategies. Moreover, Yared, Yibekal, and Kiya<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup> stress the difficulty in distinguishing Teff leaves from weed leaves due to their similar morphology, which presents a key challenge in accurately detecting and quantifying weed infestations.</p>
<p id="Par4">Recent advancements in computer vision and deep learning, particularly Convolutional Neural Networks (CNNs), offer promising solutions for weed detection and selective herbicide spraying using Unmanned Aerial Vehicles (UAVs). High-performance drones equipped with high-resolution cameras can capture detailed visual data, enabling CNNs to effectively distinguish between weeds and crops<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. Once weeds are identified, this technology can be integrated with UAVs to facilitate precise herbicide application, a process that will be further explored in the second phase of this study. However, challenges persist in accurately detecting weeds amidst the complex backgrounds and densely overlapping Teff leaves<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>. The dense growth of Teff crops often obscures ground conditions, making accurate weed identification more difficult. These challenges highlight the critical need for advanced technological solutions to improve weed management in Teff cultivation, ensuring greater productivity and sustainability.</p>
<p id="Par5">The existing works focus on weed detection for commonly cultivated crops, such as maize<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>, cabbage<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>, beans<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>, rice<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>, and fruit crops<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>. Most of these studies often rely on publicly available or satellite imagery datasets rather than high-resolution drone imagery. Moreover, none of the aforementioned studies evaluated weed detection in Teff crops using combined optimization strategies. The detection of weeds in Teff is particularly challenging due to its dense growth pattern, the morphological similarity between the crop and weeds, and the narrowness of the leaves, as a result there is no any studied. Despite the fact that uncontrolled weed infestations can cause yield losses of 5–39% in Teff cultivation <sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>, current weed control practices remain largely manual, mechanical, or dependent on backpack sprayers. This gap highlights the need for targeted research into automated Teff weed detection and the application of state-of-the-art technologies such as drone-based spraying systems. Focusing on Teff weed detection gives a significant opportunity to enhance precision agriculture for those highly producing of Teff crops countries, such as Ethiopia and USA.</p>
<p id="Par6">This study addresses the challenge of weed detection in Teff crops by introducing a novel approach that leverages a fine-tuned CNN model, trained on datasets of drone-captured images of Teff plants and various weed species. The research aims to enhance detection accuracy and efficiency in herbicide spraying through drone technology. By utilizing a refined MobileNetV2 model, the system achieves improved precision in identifying weeds, enabling targeted herbicide application that spares Teff crops while minimizing the use of chemicals. This approach not only reduces environmental impact but also promotes sustainable farming practices. The contributions of this work are summarized as follows:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par7"><em>Development of a fine-tuned deep learning model</em> Designed a refined deep learning model tailored for accurate weed detection in Teff crops, achieving superior performance in challenging agricultural scenarios.</p></li>
<li><p id="Par8"><em>Comprehensive image dataset collection and preprocessing</em> Assembled and preprocessed a diverse dataset of Teff crops and weeds, captured under varying environmental conditions using drone cameras, ensuring robust training and testing of the model.</p></li>
<li><p id="Par9"><em>Innovative integration of shape-based features for enhanced detection</em> Incorporated shape-based features, such as aspect ratio (AR) and solidity, to improve the model’s ability to differentiate between Teff plants and weeds, significantly boosting detection accuracy and overall performance.</p></li>
</ul>
<p id="Par10">The remaining sections of this paper are organized as follows: Sect. “<a href="#Sec2" class="usa-link">Related works</a>”reviews related literature. Sect. “<a href="#Sec3" class="usa-link">Methodology</a>”describes the proposed method and data collection approach. Sect. “<a href="#Sec13" class="usa-link">Experimental setups, results and discussion</a>” details the experimental setup, results, and discussion. Finally, Sect. “<a href="#Sec17" class="usa-link">Conclusion</a>” concludes the paper and suggests directions for future research.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Related works</h2>
<p id="Par11">This section reviews recent studies that have advanced weed detection and UAV-based herbicide application using deep learning techniques. Sa et al.<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup> employed multispectral image datasets to explore dense semantic classification for vegetation detection in crops and weeds. Their research utilized 465 data points divided into three categories: 132 images of crops, 243 images of weeds, and 90 images containing both crops and weeds. This well-structured dataset facilitated the fine-tuning of deep learning models for precise vegetation classification. The study also emphasized the importance of sensor calibration prior to data acquisition, which significantly enhanced model performance.</p>
<p id="Par12">Giselsson et al.<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup> focused on identifying plant species and detecting weeds during the early growth stages of 12 different weed and crop species. Their dataset featured high-resolution imagery (5184 × 3456 pixels), including full images, segmented plants, and unsegmented plants, each tagged with species-specific identifiers. This research highlighted the utility of segmentation techniques and high-resolution imagery in detecting weeds early, enabling timely crop management interventions. Similarly, dos Santos Ferreira et al.<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup> targeted weed detection using CNNs to differentiate between soil, soybean, broadleaf, and grass weeds. Their dataset, captured via UAVs, consisted of 15,336 images with a resolution of 4000 × 3000 pixels, which were manually segmented and annotated. The model achieved over 98% accuracy in detecting broadleaf and grass weeds and an average accuracy of 99%, demonstrating the efficacy of CNNs in weed detection. These findings underscore the potential for developing UAV-based herbicide spraying systems with high precision and efficiency.</p>
<p id="Par13">Yu et al.<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup> explored weed detection for species such as dandelion, ground ivy, spotted spurge, and ryegrass, utilizing a dataset comprising 33,086 images with a resolution of 1920 × 1080 pixels, including 17,600 positive and 15,486 negative samples. The study employed VGGNet, achieving a recall value of 0.9952, which highlights the model’s effectiveness in accurately identifying weeds. The researchers proposed integrating Deep CNN, VGGNet, and DetectNet to develop machine vision-based smart sprayers for precision weed control. This approach demonstrates the potential of high-recall deep learning models for selective herbicide spraying, offering a path toward more efficient and accurate weed management in agriculture. Ma et al.<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup> focused on image segmentation of rice seedlings and weeds, analyzing 224 data points collected from paddy fields. Their research targeted weeds during early growth stages, training six models under varying conditions. The study achieved an 8% improvement in AUC classification metrics, emphasizing the importance of fine-tuning deep learning models for precise vegetation classification.</p>
<p id="Par14">Olsen et al.<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup> focused on classifying multiple weed species using deep learning, targeting eight nationally significant species from eight locations across northern Australia. Their dataset comprised 17,509 images with a resolution of 256 × 256 pixels, with each class containing between 1009 and 1125 images. Using benchmark models Inception-v3 and ResNet-50, they achieved classification accuracies of 95.1% and 95.7%, respectively, establishing a strong baseline for weed species classification. Madsen et al.<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup> developed detection and classification algorithms for 47 common weed species in Denmark. Their dataset of 7590 records included both monocotyledonous and dicotyledonous weeds grown in semi-field settings to mimic natural growth conditions. This work highlighted the importance of training robust deep learning models on diverse datasets to ensure accurate weed detection and classification across various agricultural environments, thereby supporting more precise weed management strategies. Sudars et al.<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup> proposed identifying six food crops and eight weed species, using a total of 1,118 manually annotated images captured under controlled and field conditions at various growth stages. The study employed three RGB digital cameras, demonstrating the potential of multi-camera systems to enhance detection accuracy. This innovative approach underscores the value of integrating advanced imaging techniques with deep learning for improved crop and weed classification.</p>
<p id="Par15">Previous studies have highlighted the effectiveness of shape-based features, such as AR and Solidity, in various agricultural applications, particularly for weed detection and classification. For example, Kanda et al.<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup> applied AR and Solidity into a machine learning framework for weed detection in wheat fields, demonstrating improved performance compared to models relying solely on color features. In remote sensing, Wang et al.<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup> emphasized the robustness of shape features under diverse environmental conditions, showcasing their adaptability. Furthermore, Li, Zhang, and Wang<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup> successfully integrated AR and Solidity into a deep learning model for plant disease detection, significantly enhancing the model’s discriminative capabilities.</p>
<p id="Par16">More recently, researchers have focused using the application of drone technology in enhancing smart agriculture, such as weed detection and herbsie spraying<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a>,<a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>. Agriculture spraying through the use of drone technology has been reported to be capable of enhancing efficiency by as much as 60 times<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>. Shahi et al.<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>, for instance, utilized the application of drone imagery for cotton field weed detection through a U-Net model on 201 images. The model achieved 88.20% precision rate, 88.97% recall rate, 88.24% F1-score, and 56.21% Intersection over Union (IoU) mean. This research didn’t focus on thin leave images and used small datasets. In<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>, UAV images were applied in maize weed detection, where CNN recorded an F1-score of 82%, 75% recall rate, and 90% precision rate. The main purpose of the research was to compare the performance of various multispectral sensors rather than applying enhancing techniques. In<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>, they employed the YOLOv5 algorithm to resolve the conflicting requirements of high accuracy and low computational overhead in weed detection in grass fields. The model recorded a mean Average Precision (mAP) of 86.80%. Ong, Teo, and Sia<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> employed a CNN to detect weeds in Chinese cabbage fields from UAV images and compared the performance with that of a Random Forest (RF) model. They revieled that CNN and RF achived an accuracy of 92.41% and 86.18%, respectly. This research showed that the deep learning can achieve more accurately than traditional machine learning (RF). Similarly, Lu et al.<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup> utilized UAV images for potato weed detection using YOLOv8 and Mask R-CNN with mAP of 0.902 and 0.920, respectively.</p>
<p id="Par17">In the above studies, including<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a>–<a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>, and<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>, provided a solid foundation for advancing weed detection and herbicide spraying technologies. However. none of these studies didn’t show different optimization techniques to find accurate results for Teff crops. Besides, the fieldwork data were not used from various climate, locations and lighting levels. In addition, UAV-based data were not collected under varying climatic conditions, lighting levels, which factors that are essential for generating diverse datasets.</p>
<p id="Par18">Following this, the current research aims to fill the noted gap in research by developing sophisticated deep learning models with data augmentation combine with learning techniques (k-fold cross-validation, early stopping, regularization, and dropout) and optimizers (Adam, SGD, RMSprop, and Nadam). Teff’s distinctive morphological characteristics, including its compact growth and resemblance of leaves to numerous weeds, require the generation of demanding models based on carefully segmented images and optimized with state-of-the-art techniques hadn’t studied yet. As Teff crop is a widely producing crop in Ethiopia and partially in USA, the integration of state-of-the-art deep learning models and drone-based real-time herbicide spraying can introduce tremendous advances in agricultural productivity.</p></section><section id="Sec3"><h2 class="pmc_sec_title">Methodology</h2>
<p id="Par19">This study employed Design Science Research (DSR) to develop and evaluate a fine-tuned deep learning model for Teff-weed detection using datasets collected by drones. DSR, a research methodology focused on creating and evaluating IT artifacts to address organizational challenges, aims to enhance understanding while delivering practical solutions. This approach is particularly suitable to the development of innovative technologies, such as UAV-based herbicide applications, which demand both robust theoretical foundations and practical validation<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>. the study not only designs an advanced deep learning model but also rigorously tests and refines it to ensure its applicability in real-world agricultural scenarios for Teff weed identification. This methodology bridges the gap between theoretical research and its practical implementation, contributing to more efficient and sustainable agricultural practices in Ethiopia.</p>
<section id="Sec4"><h3 class="pmc_sec_title">Proposed system architecture</h3>
<p id="Par20">Figure <a href="#Fig1" class="usa-link">1</a> illustrates the proposed system architecture for detecting and classifying weeds in Teff crops using deep learning techniques. The process begins with image acquisition, where images of Teff crops and weeds are captured using a high-resolution drone camera. These images then undergo a preprocessing phase, which includes resizing, enhancement, filtering, normalization, and annotation to ensure the data is suitable for model training. After preprocessing, the dataset is divided into training and testing subsets with varying ratios to identify the optimal split. During the training phase, feature extraction and pre-designed models are utilized to extract relevant features and leverage pre-trained networks, which are then refined for the specific task of weed detection. These extracted features and pre-designed models are integrated into a backbone architecture that supports both bounding box detection and class label classification. The model undergoes performance evaluation, and if it achieves the desired accuracy, it is exported as an. H5 file for deployment. If the performance falls short, the model enters a fine-tuning phase, where hyperparameters and architectural adjustments are made to enhance its accuracy and robustness. This iterative process of evaluation and fine-tuning continues until the model meets the optimal performance criteria, ensuring its readiness for real-world deployment.</p>
<figure class="fig xbox font-sm" id="Fig1"><h4 class="obj_head">Fig. 1.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371000_41598_2025_15380_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/d793a073efb5/41598_2025_15380_Fig1_HTML.jpg" loading="lazy" id="d33e420" height="642" width="630" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Proposed system architecture.</p></figcaption></figure></section><section id="Sec5"><h3 class="pmc_sec_title">Image acquisition</h3>
<p id="Par21">Images of Teff and weeds were collected at various growth stages using a drone, both before and after rainy and sunny periods. The aerial perspective enabled effective monitoring of crop progress, identification of key leaf characteristics, and detecting weed density. To introduce variability, images were captured under different lighting conditions and at various altitudes, as detailed in Table <a href="#Tab1" class="usa-link">1</a>. This comprehensive dataset strengthens the deep learning model by ensuring its robustness across diverse environmental conditions, thereby enhancing its efficiency and accuracy in real-world applications.</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>Drone flight terms and conditions.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Drone flight sessions</th>
<th align="left" colspan="1" rowspan="1">Timing around</th>
<th align="left" colspan="1" rowspan="1">Soil condition</th>
<th align="left" colspan="1" rowspan="1">Sunlight condition</th>
<th align="left" colspan="1" rowspan="1">Date captured</th>
<th align="left" colspan="1" rowspan="1">Teff fields</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">1st</td>
<td align="left" colspan="1" rowspan="1">3:00</td>
<td align="left" colspan="1" rowspan="1">Dry</td>
<td align="left" colspan="1" rowspan="1">Sunny</td>
<td align="left" colspan="1" rowspan="1">July 12, 2023</td>
<td align="left" colspan="1" rowspan="1">UoG agro lab</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">2nd</td>
<td align="left" colspan="1" rowspan="1">3:00</td>
<td align="left" colspan="1" rowspan="1">Dry</td>
<td align="left" colspan="1" rowspan="1">Cloudy</td>
<td align="left" colspan="1" rowspan="1">August 1, 2023</td>
<td align="left" colspan="1" rowspan="1">Farmer’s 1st Teff field</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">3rd</td>
<td align="left" colspan="1" rowspan="1">10:00</td>
<td align="left" colspan="1" rowspan="1">Dry</td>
<td align="left" colspan="1" rowspan="1">Near sunset</td>
<td align="left" colspan="1" rowspan="1">August 1, 2023</td>
<td align="left" colspan="1" rowspan="1">Farmer’s 2nd Teff field</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">4th</td>
<td align="left" colspan="1" rowspan="1">8:00</td>
<td align="left" colspan="1" rowspan="1">Wet</td>
<td align="left" colspan="1" rowspan="1">Sunny+cloudy</td>
<td align="left" colspan="1" rowspan="1">July 15, 2023</td>
<td align="left" colspan="1" rowspan="1">UoG agro lab</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par22">For this study, Teff crops were planted during the summer of 2023 at the University of Gondar (UoG) Agricultural Research Farm, located at latitude 12.580178 and longitude 37.441197. Four experimental fields, each measuring 4 × 4 m, were established, as illustrated in Fig. <a href="#Fig2" class="usa-link">2</a>. The Teff was sown manually, maintaining a standard row distance of 20 cm and intra-row spacing, as recommended by Meseret Gezahegn and Tamiru<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>. Planting was conducted on a level surface to mitigate the influence of topographical variations on the results. By the time of image acquisition in Field 1 (right side), the Teff crops had matured sufficiently, facilitating an accurate evaluation of the model’s performance.</p>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371000_41598_2025_15380_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/19e961ebcfd5/41598_2025_15380_Fig2_HTML.jpg" loading="lazy" id="d33e522" height="270" width="790" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Planting teff field 1 experiment.</p></figcaption></figure><p id="Par23">Additionally, an extensive dataset of field images was gathered from two farmlands in Northern Amhara, Azezo Kebele, located at latitude 12.539057 and longitude 37.385636. These images were captured using a DJI Air 2S UAV equipped with high-resolution RGB cameras, as shown in Fig. <a href="#Fig3" class="usa-link">3</a>. The images, taken at a resolution of 3056 × 3056 pixels, were collected during flights conducted at altitudes of 1–2 m above the ground. Captures were performed under varying surface and lighting conditions, including sunny and post-rain scenarios. The resulting dataset comprises up to 3,250 RGB images, some containing noise, and is formatted in JPG. This diverse dataset facilitates a thorough evaluation of the model’s performance under different field conditions and enhances its potential for generalizability across various agricultural environments.</p>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371000_41598_2025_15380_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/917218fa6f8d/41598_2025_15380_Fig3_HTML.jpg" loading="lazy" id="d33e534" height="325" width="660" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Image acquisition using drones.</p></figcaption></figure><p id="Par24">As detailed in Table <a href="#Tab1" class="usa-link">1</a>, four drone flight sessions conducted over three Teff fields (Terms 1–4) captured a diverse range of natural conditions. These included varying sunlight levels, wet and dry soil conditions, crop shadows in different orientations, and Teff crops at multiple growth stages. Such heterogeneous conditions form an optimal dataset for deep learning applications, promoting the development of robust models capable of generalizing across diverse real-world scenarios. Supporting this, Murad et al.<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup> emphasize that acquiring image data at different growth stages not only aids in tracking crop development but also facilitates the identification of critical leaf features, offering valuable insights for precision agriculture solutions.</p>
<p id="Par25">Figure <a href="#Fig4" class="usa-link">4</a> illustrates the various levels of Teff crop and weed growth, as captured during experimental data collection with a drone for weed detection. The images depict varying soil conditions and crop growth stages, offering a comprehensive dataset for developing and validating weed detection algorithms across diverse agricultural settings. Additionally, a dataset of weed species was compiled through the manual classification of individual weeds in the images, assisted by experts. This dataset included images of various weed species categorized as one group and Teff crops as another.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371000_41598_2025_15380_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/da9eb6c6aa5b/41598_2025_15380_Fig4_HTML.jpg" loading="lazy" id="d33e555" height="180" width="712" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Different environment conditions (rainy and sunny), and growth level.</p></figcaption></figure></section><section id="Sec6"><h3 class="pmc_sec_title">Data preprocessing</h3>
<section id="Sec7"><h4 class="pmc_sec_title">Image annotation and image filtering</h4>
<p id="Par26">From the originally collected dataset of 3250 images, many defective images were discarded, resulting in a total of 1308 usable images that were annotated using makesense.ai. Annotations involved drawing rectangles around instances of ‘Teff’ and ‘Weed,’ which were then saved in text files. The rectangles were confined to smaller regions within the high-resolution raw images, as defined by bounding boxes in the text files. The dataset was created by cropping the raw images according to the bounding boxes that defined the annotated regions. Data for each image was extracted by projecting a rectangle from the raw image space onto the space delineated by its corresponding bounding box. To reduce computational requirements and enhance model performance, the following preprocessing steps were implemented. First, the original images were resized to 512 × 512 pixels from their original size of 3056 × 3056 pixels. Second, the pixel intensities of the images were scaled to the range [0, 1] to normalize the dynamic range and enhance model performance. Additionally, the mask filter, as discussed by Al-Ameen, Muttar, and Al-Badrani<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>, was employed to adjust the radius and achieve the appropriate sharpness for images with varying levels of motion blur. This non-destructive editing feature allowed fine-tuning adjustments without permanently altering the original image data, as illustrated in Fig. <a href="#Fig5" class="usa-link">5</a>. It has been adjusted the radius to achieve the appropriate level of sharpness for images with varying degrees of motion blur. Moreover, the non-destructive editing feature ensures that adjustments can be fine-tuned without permanently altering the original image data.</p>
<figure class="fig xbox font-sm" id="Fig5"><h5 class="obj_head">Fig. 5.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371000_41598_2025_15380_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/7824afb2545d/41598_2025_15380_Fig5_HTML.jpg" loading="lazy" id="d33e575" height="332" width="710" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Drone motion blur and filtered by unsharp mask.</p></figcaption></figure></section><section id="Sec8"><h4 class="pmc_sec_title">Image preprocessing</h4>
<p id="Par27">The process of obtaining more suitable datasets for Teff weed detection involves several steps, each utilizing mathematical formulations to extract meaningful information from images. This is essential because Teff leaves are extremely narrow and require extensive preprocessing. Key preprocessing steps include resizing, noise filtering, and color normalization. For instance, Eq. (<a href="#Equ1" class="usa-link">1</a>) is used to resize images to a standard dimension:</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/3ed331ee5811/d33e583.gif" loading="lazy" id="d33e583" alt="graphic file with name d33e583.gif"></td>
<td class="label">1</td>
</tr></table>
<p id="Par28">Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/42e3f9ef48cc/d33e591.gif" loading="lazy" id="d33e591" alt="Inline graphic"></span> represents the input image, which is resized to a new standardized width <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/96c8f3fdf208/d33e597.gif" loading="lazy" id="d33e597" alt="Inline graphic"></span> and height <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/eab8a4e955a3/d33e603.gif" loading="lazy" id="d33e603" alt="Inline graphic"></span>. This resizing ensures uniform input dimensions for the deep learning models, facilitating consistent feature extraction. I <sub>resized</sub> refers to the newly resized image derived from the original image. For this work, Gaussian filtering was applied to reduce noise and minimize overfitting problems in model development<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>, as illustrated in Eq. (<a href="#Equ2" class="usa-link">2</a>).</p>
<table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/2f5e02f63d2f/d33e619.gif" loading="lazy" id="d33e619" alt="graphic file with name d33e619.gif"></td>
<td class="label">2</td>
</tr></table>
<p id="Par29">Here,<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/6d18c03cf10c/d33e627.gif" loading="lazy" id="d33e627" alt="Inline graphic"></span> (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq5"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/eaff0cd2a300/d33e633.gif" loading="lazy" id="d33e633" alt="Inline graphic"></span> represents a Gaussian blur applied to reduce noise in the image, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq6"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/3f64dd268fd7/d33e639.gif" loading="lazy" id="d33e639" alt="Inline graphic"></span> is the standard deviation of the Gaussian distribution, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq7"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/fdfff813a087/d33e645.gif" loading="lazy" id="d33e645" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq8"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/efcc4ad69ea7/d33e651.gif" loading="lazy" id="d33e651" alt="Inline graphic"></span> are pixel coordinates, and “<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq9"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/fee187df0f98/d33e658.gif" loading="lazy" id="d33e658" alt="Inline graphic"></span>” denotes convolution. In Eq. (<a href="#Equ3" class="usa-link">3</a>), we apply color normalization outlined in the reports by Nair <sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>as:</p>
<table class="disp-formula p" id="Equ3"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/ed102feeed29/d33e671.gif" loading="lazy" id="d33e671" alt="graphic file with name d33e671.gif"></td>
<td class="label">3</td>
</tr></table>
<p id="Par30">Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq10"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/53ccaf3bc2df/d33e679.gif" loading="lazy" id="d33e679" alt="Inline graphic"></span>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq11"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/3de6c2073d9f/d33e685.gif" loading="lazy" id="d33e685" alt="Inline graphic"></span> normalizes the color channels (Red, Green, Blue) by subtracting the mean (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq12"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/b43cf175bce2/d33e691.gif" loading="lazy" id="d33e691" alt="Inline graphic"></span> and dividing by the standard deviation (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq13"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/73f32c75cc4f/d33e697.gif" loading="lazy" id="d33e697" alt="Inline graphic"></span>) of the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq14"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/a8d7c5331cd7/d33e703.gif" loading="lazy" id="d33e703" alt="Inline graphic"></span> color channel. This process standardizes the pixel values across different images. By applying these mathematical formulations and techniques, relevant features for weed detection in Teff crops can be effectively extracted, facilitating accurate and efficient herbicide application using UAVs.</p></section><section id="Sec9"><h4 class="pmc_sec_title">Feature extraction</h4>
<p id="Par31">In this study, feature extraction was performed on images of Teff and weeds. However, prior to feature extraction, the images captured by UAVs underwent preprocessing to enhance quality and emphasize relevant information. This preprocessing included steps such as image normalization (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq15"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/ed8b6cd48fad/d33e713.gif" loading="lazy" id="d33e713" alt="Inline graphic"></span>) and image segmentation (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq16"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/e5dd694d8688/d33e719.gif" loading="lazy" id="d33e719" alt="Inline graphic"></span>).</p>
<table class="disp-formula p" id="Equ4"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/9498192da6c5/d33e725.gif" loading="lazy" id="d33e725" alt="graphic file with name d33e725.gif"></td>
<td class="label">4</td>
</tr></table>
<p id="Par32">where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq17"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/4f3e3b441977/d33e733.gif" loading="lazy" id="d33e733" alt="Inline graphic"></span> is the original image, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq18"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/522dcb2e5063/d33e739.gif" loading="lazy" id="d33e739" alt="Inline graphic"></span> is the mean intensity, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq19"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/2aabd379ce4a/d33e745.gif" loading="lazy" id="d33e745" alt="Inline graphic"></span> is the standard deviation.</p>
<table class="disp-formula p" id="Equ5"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/7da0ebbf84c9/d33e751.gif" loading="lazy" id="d33e751" alt="graphic file with name d33e751.gif"></td>
<td class="label">5</td>
</tr></table>
<p id="Par33">where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq20"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/75a72dbb8086/d33e759.gif" loading="lazy" id="d33e759" alt="Inline graphic"></span> is a function for segmentation applied to the normalized image to separate teff and weed from the background.</p>
<p id="Par34">Using the normalized images, feature extraction involves deriving important features from segmented images to distinguish weeds from Teff. In a deep learning context, features can be automatically learned through convolutional layers via the convolution operation, as illustrated in Eq. (<a href="#Equ6" class="usa-link">6</a>):</p>
<table class="disp-formula p" id="Equ6"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/91b6f3183afd/d33e770.gif" loading="lazy" id="d33e770" alt="graphic file with name d33e770.gif"></td>
<td class="label">6</td>
</tr></table>
<p id="Par35">where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq21"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/4f3e3b441977/d33e778.gif" loading="lazy" id="d33e778" alt="Inline graphic"></span> represents the input image, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq22"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/2aad2085578d/d33e784.gif" loading="lazy" id="d33e784" alt="Inline graphic"></span> denotes the convolution kernel, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq23"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/6678622e98b1/d33e790.gif" loading="lazy" id="d33e790" alt="Inline graphic"></span> are the spatial coordinates. Convolutional layers in a deep learning model learn spatial hierarchies of features, such as edges, textures, and shapes, which are critical for detecting weeds and assessing plant health. During feature extraction, the pooling operation is applied, where max-pooling layers reduce the dimensionality of the features while preserving the most significant ones, thereby enhancing the model’s ability to generalize. It is shown in Eq. (<a href="#Equ7" class="usa-link">7</a>):</p>
<table class="disp-formula p" id="Equ7"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/078fe3de27b0/d33e799.gif" loading="lazy" id="d33e799" alt="graphic file with name d33e799.gif"></td>
<td class="label">7</td>
</tr></table>
<p id="Par36">Convolution and pooling operations were applied for feature extraction across all the proposed algorithms: MobileNetV2, InceptionResNetV2, DenseNet201, and VGG16. The dataset was partitioned into training and testing sets using the train_test_split function, with 80% of the data allocated for training and 20% for testing, as summarized in Table <a href="#Tab2" class="usa-link">2</a>.</p>
<section class="tw xbox font-sm" id="Tab2"><h5 class="obj_head">Table 2.</h5>
<div class="caption p"><p>Augmented data splitting for model training.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Property</th>
<th align="left" colspan="1" rowspan="1">Type</th>
<th align="left" colspan="1" rowspan="1">Shape</th>
<th align="left" colspan="1" rowspan="1">Data size</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="3" colspan="1">Data size</td>
<td align="left" colspan="1" rowspan="1">Train</td>
<td align="left" colspan="1" rowspan="1">1831</td>
<td align="left" colspan="1" rowspan="1">70% of the total dataset</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Test</td>
<td align="left" colspan="1" rowspan="1">523</td>
<td align="left" colspan="1" rowspan="1">20% of the total dataset</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Validation</td>
<td align="left" colspan="1" rowspan="1">262</td>
<td align="left" colspan="1" rowspan="1">10% of the training dataset</td>
</tr>
<tr>
<td align="left" colspan="2" rowspan="1">Total dataset(augmented)</td>
<td align="left" colspan="1" rowspan="1">2616</td>
<td align="left" colspan="1" rowspan="1">Class size</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">Data class</td>
<td align="left" colspan="1" rowspan="1">Labels (class)</td>
<td align="left" colspan="1" rowspan="1">2</td>
<td align="left" colspan="1" rowspan="1">Teff and weed</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Bounding box</td>
<td align="left" colspan="1" rowspan="1">4</td>
<td align="left" colspan="1" rowspan="1">2 (width x height)</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section><section id="Sec10"><h3 class="pmc_sec_title">Modeling usages</h3>
<p id="Par37">In this paper, we use pre-trained models with transfer learning techniques (MobileNetV2, InceptionResNetV2, DenseNet201, VGG16, Resnet50, Fast R-CNN, YOLOv8), then fine-tune and train these networks to develop a new model. Our approach leverages the non-uniform visual characteristics of Teff and weed species across different fields by using different CNN backbone architectures—connected to model ‘heads’ for detecting class labels and bounding boxes. Key components include fine-tuned Keras model backbones for detection, the ReLU activation function, a Sigmoid output layer as a classifier, and two iterations for class label and bounding box detection. The different hyperparameters and their values used for the pre-trained models in transfer learning are shown in Table <a href="#Tab3" class="usa-link">3</a>. These values were selected after exhaustive testing. Besides to the CNN architectures, the YOLOv8 was used for comparing to show robustness of the proposed method<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>.</p>
<section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Hyper-parameter description.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Hyperparameters</th>
<th align="left" colspan="1" rowspan="1">Values</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Optimizer</td>
<td align="left" colspan="1" rowspan="1">Nadam</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Activation</td>
<td align="left" colspan="1" rowspan="1">ReLU, sigmoid</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Batch size</td>
<td align="left" colspan="1" rowspan="1">32</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Epoch</td>
<td align="left" colspan="1" rowspan="1">100</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Learning rate</td>
<td align="left" colspan="1" rowspan="1">0.0001</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Loss function</td>
<td align="left" colspan="1" rowspan="1">
<p>Class label = binary cross entropy</p>
<p>Bounding box = mean squared error</p>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec11"><h3 class="pmc_sec_title">Model optimizations</h3>
<p id="Par38">Aspect Ratio and Solidity were employed to differentiate the uniform shapes of Teff plants from the irregular shapes of weeds. These shape-based features provide significant advantages in accurately identifying and classifying vegetation, thereby enhancing the efficiency of weed detection systems. The robustness of AR and Solidity to size variations ensures consistent performance across different growth stages and plant sizes, making these features particularly effective in real-world applications<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup>. In Teff plants, this ratio tends to be more uniform, whereas weeds often exhibit greater variability. The mathematical formula of AR is shown in Eq. (<a href="#Equ8" class="usa-link">8</a>):</p>
<table class="disp-formula p" id="Equ8"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/f3e71c33a3ae/d33e945.gif" loading="lazy" id="d33e945" alt="graphic file with name d33e945.gif"></td>
<td class="label">8</td>
</tr></table>
<p id="Par39">We also use solidity (S) which is the ratio of the area of the object to the area of the convex hull-area of the object<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>, as shown in Eq. (<a href="#Equ9" class="usa-link">9</a>).</p>
<table class="disp-formula p" id="Equ9"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/96984584aec3/d33e960.gif" loading="lazy" id="d33e960" alt="graphic file with name d33e960.gif"></td>
<td class="label">9</td>
</tr></table>
<p id="Par40">The convex hull of Teff crops typically exhibits higher solidity values due to their uniform and compact shape, whereas the majority of weeds have lower solidity due to their irregular and more dispersed shapes. Our methodology involves preprocessing the images to segment the plants from the background and then identifying the contours of the segmented plants. We then compute the bounding box and convex hull for each plant to extract the AR and Solidity features. These features are combined with other visual characteristics to form a comprehensive feature vector that is used to train a machine-learning model for detecting and classifying Teff crops and weeds. The effectiveness of our approach is validated through extensive experiments, demonstrating the potential of AR and Solidity in improving weed detection and enabling precise herbicide application in agricultural fields.</p>
<p id="Par41">Once the datasets had been normalized, data augmentation techniques in different ratios of the original data, such as 25%, 50%, 75%, and 100%, were applied. As a result, the 1308 images increased by 327 for 25%, by 654 for 50%, by 981images for 75%, and by 1308 images for 100%. Then, the performance of the MobileNetv2 model in each datasize was computed, and select the optimum result. The 100% augmentation (2616 images) was used for this study in different optimizatioin techniques, as shown in Table <a href="#Tab4" class="usa-link">4</a>. Different optimizers, such as Adam, SGD, RMSprop, and Nadam, combined with different learning techniques, such as k-fold (k = 5), dropout regularization, early stoping and lassio regularization techniques, were used. The optimum results are found in the combinations of K-fold and dropout regularization with Nadam optimizer, and gives 96.40 of accuracies. Since the dropout optimizer is more efficient than k-fold technique, it has been used for this study. The overall result shows that using optimizers with overfitting minimizing technique can improves the accuracy of the model performances, as illustrated in Tables <a href="#Tab4" class="usa-link">4</a> and <a href="#Tab5" class="usa-link">5</a>.</p>
<section class="tw xbox font-sm" id="Tab4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>MobileNetv2 accuracies in different optimizers and learning techniques.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">Optiomizers</th>
<th align="left" colspan="5" rowspan="1">Accuracy evaluation in different learning techniques</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">K-fold</th>
<th align="left" colspan="1" rowspan="1">Dropout (%)</th>
<th align="left" colspan="1" rowspan="1">Early stopping (%)</th>
<th align="left" colspan="1" rowspan="1">Regularization (%)</th>
<th align="left" colspan="1" rowspan="1">Mixed (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Adam</td>
<td align="center" colspan="1" rowspan="1">95.74%</td>
<td align="center" colspan="1" rowspan="1">94.62</td>
<td align="center" colspan="1" rowspan="1">93.33</td>
<td align="center" colspan="1" rowspan="1">95.11</td>
<td align="center" colspan="1" rowspan="1">92.12</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SGD</td>
<td align="center" colspan="1" rowspan="1">95.42%</td>
<td align="center" colspan="1" rowspan="1">92.32</td>
<td align="center" colspan="1" rowspan="1">90.21</td>
<td align="center" colspan="1" rowspan="1">95.34</td>
<td align="center" colspan="1" rowspan="1">88.90</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RMSprop</td>
<td align="center" colspan="1" rowspan="1">95.86</td>
<td align="center" colspan="1" rowspan="1">95.01</td>
<td align="center" colspan="1" rowspan="1">91.91</td>
<td align="center" colspan="1" rowspan="1">92.11</td>
<td align="center" colspan="1" rowspan="1">85.12</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Nadam</td>
<td align="center" colspan="1" rowspan="1">96.40</td>
<td align="center" colspan="1" rowspan="1">96.40</td>
<td align="center" colspan="1" rowspan="1">90.00</td>
<td align="center" colspan="1" rowspan="1">94.88</td>
<td align="center" colspan="1" rowspan="1">87.57</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section class="tw xbox font-sm" id="Tab5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>Training, validation and testing stage comparisons before fine-tuning.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">Model</th>
<th align="left" colspan="1" rowspan="1">Training</th>
<th align="left" colspan="1" rowspan="1">Validation</th>
<th align="left" colspan="1" rowspan="1">Testing</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">Acc</th>
<th align="left" colspan="1" rowspan="1">Acc</th>
<th align="left" colspan="1" rowspan="1">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">MobileNetv2</td>
<td align="center" colspan="1" rowspan="1">88.5</td>
<td align="center" colspan="1" rowspan="1">88.1</td>
<td align="center" colspan="1" rowspan="1">87.7</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">InceptionresNetv2</td>
<td align="center" colspan="1" rowspan="1">81.2</td>
<td align="center" colspan="1" rowspan="1">81.2</td>
<td align="center" colspan="1" rowspan="1">65.6</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DenseNet201</td>
<td align="center" colspan="1" rowspan="1">86.6</td>
<td align="center" colspan="1" rowspan="1">80.4</td>
<td align="center" colspan="1" rowspan="1">79.1</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VGG16</td>
<td align="center" colspan="1" rowspan="1">77.1</td>
<td align="center" colspan="1" rowspan="1">66.2</td>
<td align="center" colspan="1" rowspan="1">65.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Resnet50</td>
<td align="center" colspan="1" rowspan="1">99.0</td>
<td align="center" colspan="1" rowspan="1">89.4</td>
<td align="center" colspan="1" rowspan="1">81.4</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Fast R-CNN</td>
<td align="center" colspan="1" rowspan="1">89.2</td>
<td align="center" colspan="1" rowspan="1">65.3</td>
<td align="center" colspan="1" rowspan="1">56.9</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">YOLOv8</td>
<td align="center" colspan="1" rowspan="1">98.6</td>
<td align="center" colspan="1" rowspan="1">93.0</td>
<td align="center" colspan="1" rowspan="1">90.4</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec12"><h3 class="pmc_sec_title">Model evaluations</h3>
<p id="Par42">The models are evaluated using different metrics such as accuracy, recall, precision, F1-score, mAP and COCO mAP score. We use COCO mAP to show how much the metrics indicates the accuracy across multiple IoU values, ranging from 0.5 to 0.95, rather than using only 0.5. The COCO mAP metric measures the accuracy of the model in detecting bounding boxes of objects<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>. A common threshold for positive detection of one instance is setting IoU with a threshold value of 0.5:</p>
<table class="disp-formula p" id="Equ10"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/722a5d8bc389/d33e1160.gif" loading="lazy" id="d33e1160" alt="graphic file with name d33e1160.gif"></td>
<td class="label">10</td>
</tr></table>
<table class="disp-formula p" id="Equ11"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/c991b0f7f897/d33e1166.gif" loading="lazy" id="d33e1166" alt="graphic file with name d33e1166.gif"></td>
<td class="label">11</td>
</tr></table>
<p id="Par43">The proposed evaluation technique, COCO mAP, is the average of APs calculated across all IoU thresholds. In object detection, average precision (AP or mAP) has become a standard metric to represent detection, as shown in Eq. (<a href="#Equ12" class="usa-link">12</a>)<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>.</p>
<table class="disp-formula p" id="Equ12"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/f34a83cce43f/d33e1181.gif" loading="lazy" id="d33e1181" alt="graphic file with name d33e1181.gif"></td>
<td class="label">12</td>
</tr></table>
<p id="Par44">where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq24"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/2608698f928f/d33e1189.gif" loading="lazy" id="d33e1189" alt="Inline graphic"></span> is the number of IoU thresholds (in COCO mAP), <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq25"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/73bf1af2851c/d33e1195.gif" loading="lazy" id="d33e1195" alt="Inline graphic"></span> is the number of classes, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq26"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/f1591f1c124a/d33e1201.gif" loading="lazy" id="d33e1201" alt="Inline graphic"></span> is the average precision for class <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq27"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/906e271ec85e/d33e1207.gif" loading="lazy" id="d33e1207" alt="Inline graphic"></span> at the threshold <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq28"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/7a2c3b82a31f/d33e1213.gif" loading="lazy" id="d33e1213" alt="Inline graphic"></span></p>
<p id="Par45">Once our model achieved the desired accuracy, we exported it to the “.h5” file format, allowing for seamless integration into the drone’s onboard systems for further operations, ensuring real-time decision-making, and enhancing the overall efficiency of the drone’s operations.</p></section></section><section id="Sec13"><h2 class="pmc_sec_title">Experimental setups, results and discussion</h2>
<section id="Sec14"><h3 class="pmc_sec_title">Experimental setup</h3>
<p id="Par46">In this work, experiments were conducted using the free cloud service Google Colab, which provides 78.2 GB of disk space, Python 3.11, and a Google Compute Engine backend (GPU) with 12.7 GB of system RAM and 15 GB of GPU RAM. Additionally, experiments were also performed on an HP laptop with an Intel Core i5-5200 CPU, 8 GB of RAM, and a 64-bit Windows operating system. Programming tasks were completed using Python and OpenCV, and the detection of Teff and weeds based on their leaves was developed using four Keras deep learning models.</p>
<p id="Par47">As indicated in Table <a href="#Tab4" class="usa-link">4</a>, the optimum results were found when the Nadam optimizer is combined with the dropout learning technique. Therefore, for this section, each report is based on the combinations of the specified learning technique and optimizer type. This combination is evaluated in different CNN architectures before and after the data augmentation technique is applied. At the end of the experiment, different models that have been applied for weed detection other than Teff were shown. Moreover, YOLOv8 has been evaluated to show the performance of MobileNetv2 compared to other state-of-the-art algorithms.</p></section><section id="Sec15"><h3 class="pmc_sec_title">Evaluating the model before fine-tuned application</h3>
<p id="Par48">Table <a href="#Tab5" class="usa-link">5</a> summarizes the results of an experiment evaluating the performance of seven deep learning models prior to fine-tuning. The models were assessed on their ability to classify and detect Teff crops and weeds during the training, validation, and testing phases. During the training phase, YOLOv8 exhibited the best performance, achieving the highest classification accuracy of 98.6%, significantly outperforming the other models. It also achieved more than 90% of accuracy duing validation and testing phase while the accuracies in training and testing phases have bigger gaps. MobileNetV2 achieved an accuracy of 87.7%, which is the highest among all models except YOLOv8. In addition, MobileNetV2 demonstrates lower variability across training, validation, and testing accuracies. Resnet50 and Fasr R-CNN had higher accuruacies than MobileNetv2 during the training phase. However, their performances was lower than MobileNetv2 during testing, indicating an inablity to control overfitting problems in Teff weed detection. DenseNet201, in comparison, achieved an accuracy of 86.6%, 80.4%, and 79.1% during the training, validation and testing phases, indicating lower performances than YOLOv8, MobileNetv2, and Resnet50. InceptionResNetV2 demonstrated an accuracy of 81.2% for the traing and validation phases. However, its performance lower to 65.6% during the testing phase. VGG16 showed the weakest performance during the training, validation and testing phases.</p>
<p id="Par49">Figure <a href="#Fig6" class="usa-link">6</a> is a loss curve, which shows the training and validation loss trends, consests of a comparative analysis of four deep learning models—MobileNetV2, DenseNet201, InceptionResNetV2, and VGG16. MobileNetv2 demonistrates stable learns convergence throughout the epochs, both training and validation losses decreasing steadily and closely tracking each other. This indicates a good generalization and minimal overfitting compard to other figures. VGG16 in Figure shows its training loss graph declining steadily. However, after a few epochs, the validation line shows significant variability, indicating a high level of overfitting and poor generalization. InceptionNetV2 has relatively high final loss values compared to MobileNetV2, indicating slower convergence. However, the training and the loss functions have steady declines with lower variability between epochs. DenseNet201 shows a rapid drop in both training and validation losses early in training, with the two curves remaining close throughout, suggesting efficient learning and moderate generalization. Overall, MobileNetV2 and DenseNet201 appear to be the most stable and generalizable models prior to fine-tuning, while VGG16 exhibits signs of overfitting and instability.</p>
<figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371000_41598_2025_15380_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/295bfc79f736/41598_2025_15380_Fig6_HTML.jpg" loading="lazy" id="d33e1249" height="461" width="668" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Loss graph of sample models before fine-tuned applications.</p></figcaption></figure><p id="Par50">Figure <a href="#Fig7" class="usa-link">7</a> depicts the bounding box detection performance of four deep learning models in weed detection. Among these models, MobileNetv2 (Fig. <a href="#Fig7" class="usa-link">7</a>A) achieved the highest precision, effectively adapting its bounding boxes to the size and shape of the weeds, thereby demonstrating superior detection capabilities. DenseNet201 (Fig. <a href="#Fig7" class="usa-link">7</a>B) performed reasonably well, although its bounding boxes were slightly less accurate compared to MobileNetV2. In contrast, VGG16 (Fig. <a href="#Fig7" class="usa-link">7</a>D) exhibited the poorest performance, struggling to accurately identify and bound weeds, which highlights their limitations in this detection task.</p>
<figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371000_41598_2025_15380_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/021948f61855/41598_2025_15380_Fig7_HTML.jpg" loading="lazy" id="d33e1283" height="180" width="744" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Bounding box base model for weed detections (<strong>A</strong> = MobileNetV2, <strong>B</strong> = DenseNet201, <strong>C</strong> = InceptionResNetV2, <strong>D</strong> = VGG16).</p></figcaption></figure><p id="Par51">Table <a href="#Tab6" class="usa-link">6</a> summarizes the testing accuracies of MobileNetV2, DenseNet201, InceptionResNetV2, VGG16, Resnet50, Fast R-CNN and YOLOv8 prior to the application of fine-tuning techniques. YOLOv8 and MobileNetv2 achieved the highest baseline accuracy at 90.4 and 87.4%, respectively, highlighting their superior performances during the initial testing phase. Its superior performance, particularly in maintaining low loss values and high accuracy, underscores its effectiveness in both learning and generalizing across different stages of model evaluation. InceptionResNetV2 and DenseNet201 recorded accuracies of 80.6% and 79.1%, respectively, while VGG16 exhibited the lowest accuracy at 65.3%. This performance decline is likely attributable to overfitting, where models excel on training and validation data but fail to generalize to unseen test data. Furthermore, the challenging nature of Teff crop and weed detection likely contributed to the observed performance gap. These challenges include occluded objects, small object sizes, and instances where objects occupy only a minimal portion of the image frame, all of which make accurate detection more difficult.</p>
<section class="tw xbox font-sm" id="Tab6"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>Different model performances in various metrics before fine-tuning.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Algorithms</th>
<th align="left" colspan="1" rowspan="1">Precision</th>
<th align="left" colspan="1" rowspan="1">Recall</th>
<th align="left" colspan="1" rowspan="1">F1-score</th>
<th align="left" colspan="1" rowspan="1">Accuracy %</th>
<th align="left" colspan="1" rowspan="1">Infer</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">MobileNetv2</td>
<td align="center" colspan="1" rowspan="1">84.12</td>
<td align="center" colspan="1" rowspan="1">87.4</td>
<td align="center" colspan="1" rowspan="1">87.4</td>
<td align="center" colspan="1" rowspan="1">87.7</td>
<td align="center" colspan="1" rowspan="1">0.09</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">InceptionresNetv2</td>
<td align="center" colspan="1" rowspan="1">65.6</td>
<td align="center" colspan="1" rowspan="1">65.6</td>
<td align="center" colspan="1" rowspan="1">65.6</td>
<td align="center" colspan="1" rowspan="1">65.6</td>
<td align="center" colspan="1" rowspan="1">0.07</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DenseNet201</td>
<td align="center" colspan="1" rowspan="1">65.0</td>
<td align="center" colspan="1" rowspan="1">83.8</td>
<td align="center" colspan="1" rowspan="1">75.6</td>
<td align="center" colspan="1" rowspan="1">79.1</td>
<td align="center" colspan="1" rowspan="1">0.17</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VGG16</td>
<td align="center" colspan="1" rowspan="1">58.0</td>
<td align="center" colspan="1" rowspan="1">65.3</td>
<td align="center" colspan="1" rowspan="1">65.3</td>
<td align="center" colspan="1" rowspan="1">65.3</td>
<td align="center" colspan="1" rowspan="1">0.08</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Resnet50</td>
<td align="center" colspan="1" rowspan="1">77.3</td>
<td align="center" colspan="1" rowspan="1">81.4</td>
<td align="center" colspan="1" rowspan="1">81.4</td>
<td align="center" colspan="1" rowspan="1">81.4</td>
<td align="center" colspan="1" rowspan="1">0.25</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Fast R-CNN</td>
<td align="center" colspan="1" rowspan="1">25.8</td>
<td align="center" colspan="1" rowspan="1">50.8</td>
<td align="center" colspan="1" rowspan="1">34.2</td>
<td align="center" colspan="1" rowspan="1">56.9</td>
<td align="center" colspan="1" rowspan="1">7.30</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">YOLOv8</td>
<td align="center" colspan="1" rowspan="1">86.1</td>
<td align="center" colspan="1" rowspan="1">93.4</td>
<td align="center" colspan="1" rowspan="1">87.0</td>
<td align="center" colspan="1" rowspan="1">90.4</td>
<td align="center" colspan="1" rowspan="1">0.12</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par52">The performance of Fast R-CNN is lower compared to other algorithms in precision, recall, F1-score, and accuracy. Besides, the inference time is slower than other algorithms. The overall learning capacity and efficiency are the poorest. In the inference column, VGG16 has a shorter computing time than other models. MobileNetv2 also has a smaller inference time compared to YOLOv8, Fast R-CNN, DenseNet201, and ResNet50. The slowest performance was achieved in Fast R-CNN, which was 5.30 s.</p></section><section id="Sec16"><h3 class="pmc_sec_title">Evaluating the models after fine-tuned application</h3>
<p id="Par53">Table <a href="#Tab7" class="usa-link">7</a> shows a detailed comparison of varios deep learning models after fine-tuning, using the same metrics and phases as those evaluated before the fine-tuning application. In the training phase, VGG16, Densnet201, MobileNetV2, and InceptionresNetv2 achieved extraordinary accuracy, achieving 99.1%, 98.9, 98.7 and 96.8% of training accuracies, respectively. These results indicate the models’ effectiveness in learning from training data with minimal errors. However, the accuracies in validation phase dropped to 97.3%, 96.5% and 95.2% for MobileNetv2, DenseNet201, and VGG16, respectively. The accuracy gaps between training and validation phases for InceptionresNetv2 decreased by more than 6%, indicating high overfitting. In the testing phase, mobileNetv2 performed better compared to other six models. Furthermore, its testing accuracy is close to both training and validation accuracy, suggesting that MobileNetV2 exhibits stronger generalization capability than the other models in Teff weed detection.</p>
<section class="tw xbox font-sm" id="Tab7"><h4 class="obj_head">Table 7.</h4>
<div class="caption p"><p>Training, validation and testing stage comparisons after fine-tuning.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">Model</th>
<th align="left" colspan="1" rowspan="1">Training</th>
<th align="left" colspan="1" rowspan="1">Validation</th>
<th align="left" colspan="1" rowspan="1">Testing</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">Accuracy</th>
<th align="left" colspan="1" rowspan="1">Accuracy</th>
<th align="left" colspan="1" rowspan="1">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">MobileNetv2</td>
<td align="center" colspan="1" rowspan="1">98.7</td>
<td align="center" colspan="1" rowspan="1">97.3</td>
<td align="center" colspan="1" rowspan="1">96.4</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DenseNet201</td>
<td align="center" colspan="1" rowspan="1">98.9</td>
<td align="center" colspan="1" rowspan="1">96.5</td>
<td align="center" colspan="1" rowspan="1">95.4</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">InceptionresNetv2</td>
<td align="center" colspan="1" rowspan="1">96.8</td>
<td align="center" colspan="1" rowspan="1">89.5</td>
<td align="center" colspan="1" rowspan="1"><em>95.8</em></td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VGG16</td>
<td align="center" colspan="1" rowspan="1">99.1</td>
<td align="center" colspan="1" rowspan="1">95.2</td>
<td align="center" colspan="1" rowspan="1">94.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Resnet50</td>
<td align="center" colspan="1" rowspan="1">88.0</td>
<td align="center" colspan="1" rowspan="1">79.2</td>
<td align="center" colspan="1" rowspan="1">87.7</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Fast R-CNN</td>
<td align="center" colspan="1" rowspan="1">57.4</td>
<td align="center" colspan="1" rowspan="1">51.0</td>
<td align="center" colspan="1" rowspan="1">50.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">YOLOv8</td>
<td align="center" colspan="1" rowspan="1">91.7</td>
<td align="center" colspan="1" rowspan="1">77.9</td>
<td align="center" colspan="1" rowspan="1">83.0</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par54">After fine-tuning, YOLOv8 showed reduced performance compared to the training phase before fine-tuning. Additionally, the accuracies in the validation and testing phases were not as strong as those of MobileNetV2. InceptionResNetV2 also achieved a training accuracy of 99.1%; however, its accuracy dropped to 89.5% during validation and 95.8% during testing, after applying dropout and fine-tuning. This represents a significant improvement compared to using only the optimization technique. DenseNet20, which has lower variations between training, validation and testing phases’ accuracy, showed more stable performances than InceptionResNetV2. However, the ovrall results of the seven models show that they have more improved generalization capabilities due to proper combination of different optimizers and fine-tuning techniques.</p>
<p id="Par55">Figure <a href="#Fig8" class="usa-link">8</a> illustrates the object detection performance of the models after fine-tuning. MobileNetV2 (Fig. <a href="#Fig8" class="usa-link">8</a>a) achieved the highest detection accuracy, with well-aligned bounding boxes that closely match the actual objects, highlighting the model’s superior precision following fine-tuning. InceptionResNetV2 (Fig. <a href="#Fig8" class="usa-link">8</a>b) also performed effectively, though its bounding box alignment was slightly less precise than MobileNetV2. While it demonstrated strong detection accuracy, it faced some challenges with exact localization. DenseNet201 (Fig. <a href="#Fig8" class="usa-link">8</a>c) exhibited moderate performance, with bounding boxes that were less tightly fitted. This suggests that although it achieved general accuracy in detection, it lacked the precision of the leading models. VGG16 (Fig. <a href="#Fig8" class="usa-link">8</a>d) struggled significantly, producing poorly aligned bounding boxes that reflect considerable difficulties in object detection and localization, even after fine-tuning. This comparison underscores the effectiveness of MobileNetV2 in object detection tasks post-fine-tuning, with InceptionResNetV2 and DenseNet201 showing reasonable performance, while VGG16 remained the least effective model.</p>
<figure class="fig xbox font-sm" id="Fig8"><h4 class="obj_head">Fig. 8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371000_41598_2025_15380_Fig8_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/45d6/12371000/b48d19292e13/41598_2025_15380_Fig8_HTML.jpg" loading="lazy" id="d33e1548" height="183" width="741" alt="Fig. 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The object detection after fine-tuning application: (<strong>a</strong>) MobileNetv2, (<strong>b</strong>) InceptionResNetV2, (<strong>c</strong>) DenseNet201, and (<strong>d</strong>) VGG16 models.</p></figcaption></figure><p id="Par56">Table <a href="#Tab8" class="usa-link">8</a> outlines a comparative analysis of seven deep learning models—MobileNetV2, DenseNet201, InceptionResNetV2, VGG16, ResNet50, Fast R-CNN, and YOLOv8—evaluated based on precision, recall, F1-score, accuracy, and inference time after employing fine-tuning, dropout, and data augmentation techniques. Of the models evaluated, MobileNetV2 demonstrated the best performance across all evaluation metrics, achieving precision, recall, F1-score, and accuracy values of 96.2%, 97.5%, 96.2%, and 96.4%, respectively, while also logging the lowest inference time (0.07 s) in comparison all models except VGG-16, thus representing both exemplary predictive performance and computational efficiency. DenseNet201 came in a close second, achieving 95.8% on all classification metrics; however, it showed a higher inference time (0.38 s), reflecting excellent performance with a concomitant moderate sacrifice in processing speed. InceptionResNetv2 showed slightly lower values (95.4%) across all metrics, with a reasonable inference time (0.27 s), thus making it a viable choice for applications where time sensitivity is a moderate concern.</p>
<section class="tw xbox font-sm" id="Tab8"><h4 class="obj_head">Table 8.</h4>
<div class="caption p"><p>Comparision of different models in different metrics after applying fine-tuning, dropout and data augmentation<em>.</em></p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Algorithms</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">F1-score (%)</th>
<th align="left" colspan="1" rowspan="1">Accuracy (%)</th>
<th align="left" colspan="1" rowspan="1">Infer</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1"><em>MobileNetv2</em></td>
<td align="center" colspan="1" rowspan="1">96.2</td>
<td align="center" colspan="1" rowspan="1">97.5</td>
<td align="center" colspan="1" rowspan="1">96.2</td>
<td align="center" colspan="1" rowspan="1">96.4</td>
<td align="center" colspan="1" rowspan="1">0.07</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DenseNet201</td>
<td align="center" colspan="1" rowspan="1"><em>95.8</em></td>
<td align="center" colspan="1" rowspan="1"><em>95.8</em></td>
<td align="center" colspan="1" rowspan="1"><em>95.8</em></td>
<td align="center" colspan="1" rowspan="1">95.4</td>
<td align="center" colspan="1" rowspan="1">0.38</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">InceptionresNetv2</td>
<td align="center" colspan="1" rowspan="1">95.4</td>
<td align="center" colspan="1" rowspan="1">95.4</td>
<td align="center" colspan="1" rowspan="1">95.4</td>
<td align="center" colspan="1" rowspan="1"><em>95.8</em></td>
<td align="center" colspan="1" rowspan="1">0.27</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VGG16</td>
<td align="center" colspan="1" rowspan="1">94.4</td>
<td align="center" colspan="1" rowspan="1">94.3</td>
<td align="center" colspan="1" rowspan="1">94.3</td>
<td align="center" colspan="1" rowspan="1">94.3</td>
<td align="center" colspan="1" rowspan="1">0.05</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Resnet50</td>
<td align="center" colspan="1" rowspan="1">88.1</td>
<td align="center" colspan="1" rowspan="1">87.9</td>
<td align="center" colspan="1" rowspan="1">87.7</td>
<td align="center" colspan="1" rowspan="1">87.7</td>
<td align="center" colspan="1" rowspan="1">0.81</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Fast R-CNN</td>
<td align="center" colspan="1" rowspan="1">25.8</td>
<td align="center" colspan="1" rowspan="1">50.8</td>
<td align="center" colspan="1" rowspan="1">34.1</td>
<td align="center" colspan="1" rowspan="1">50.8</td>
<td align="center" colspan="1" rowspan="1">4.22</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">YOLOv8</td>
<td align="center" colspan="1" rowspan="1">86.12</td>
<td align="center" colspan="1" rowspan="1">69.2</td>
<td align="center" colspan="1" rowspan="1">83.0</td>
<td align="center" colspan="1" rowspan="1">83.0</td>
<td align="center" colspan="1" rowspan="1">0.33</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par57">VGG16 achieved 94.4% of precision, 94.3% of recall, F1-score, and accuracy, with the shortest inference time (0.05 s) of all models, although its classification performance was weaker than the top three models: MobileNetv2, DenseNet201 and InceptionresNetv2. ResNet50, at 87.7% accuracy, was considerably weaker than most of the model performances, indicating lower effectiveness in this fine-tuned scenario. YOLOv8, although generally powerful in object detection tasks, obtained a comparatively lower accuracy of 83.0%, with lower recall (69.2%) likely being a factor in this result. Lastly, Fast R-CNN performed poorly in terms of classification (F1-score of 34.1%, accuracy 50.8%) and had the longest inference time (4.22 s), reflecting inefficiency and limited applicability to this task.</p>
<p id="Par58">These findings highlight the better generalization and efficiency of MobileNetV2 following fine-tuning and optimization, rendering it the best and most deployable model for Teff weed detection. DenseNet201 and InceptionResNetV2 also demonstrate high potential, albeit at a modest computational overhead. VGG16 is computationally light but provides relatively lower accuracy, while Fast R-CNN is still the worst performing in both accuracy and speed.</p>
<p id="Par59">Table <a href="#Tab9" class="usa-link">9</a> presents a comparative performance differences of mAP of YOLOv8 and MobileNetV2 on different IoU thresholds—namely, COCO mAP (calculated at a few IoU levels in the range of 0.5 and 0.95) and fixed-threshold mAP—before and after fine-tuning. Before fine-tuning, YOLOv8 recorded higher COCO mAP (58.00%) and standard mAP (90.50%) than MobileNetV2, with a value of 44.15% and 87.12%, respectively. But after fine-tuning, MobileNetV2 became much better with its COCO mAP rising to 61.50% and mAP to 95.80%, which is superior to that of YOLOv8 in both measurements. YOLOv8 decreased in COCO mAP to 51.13% and in mAP to 80.81%. This shows that MobileNetV2 responds better to fine-tuning and optimization techniques, particularly for Teff weed detection. The improvement in both COCO mAP and fixed-threshold mAP for MobileNetV2 indicates more accurate detection and localization precision at all IoU levels. Conversely, the decline in performance for YOLOv8 following fine-tuning may indicate overfitting, reduced generalization, or being sensitive to the optimization strategies employed.</p>
<section class="tw xbox font-sm" id="Tab9"><h4 class="obj_head">Table 9.</h4>
<div class="caption p"><p>mAP comparisons in various IoU ranges.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">Model</th>
<th align="left" colspan="2" rowspan="1">Before fine-tuning</th>
<th align="left" colspan="2" rowspan="1">After fine-tuning</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">COCO mAP (%)</th>
<th align="left" colspan="1" rowspan="1">mAP (%)</th>
<th align="left" colspan="1" rowspan="1">COCO mAP (%)</th>
<th align="left" colspan="1" rowspan="1">mAP (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">MobileNetV2</td>
<td align="center" colspan="1" rowspan="1">44.15</td>
<td align="center" colspan="1" rowspan="1">87.12</td>
<td align="center" colspan="1" rowspan="1">61.50</td>
<td align="center" colspan="1" rowspan="1">95.80</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">YOLOv8</td>
<td align="center" colspan="1" rowspan="1">58.00</td>
<td align="center" colspan="1" rowspan="1">90.50</td>
<td align="center" colspan="1" rowspan="1">51.13</td>
<td align="center" colspan="1" rowspan="1">80.81</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par60">Table <a href="#Tab10" class="usa-link">10</a> presents a comparative analysis of the proposed model’s performance against several state-of-the-art weed detection approaches applied to crops other than Teff. The results demonstrate that while weed detection in Teff crops remains extremely challenging—due to the high planting density and the narrow, thin morphology of Teff leaves—the proposed model based on MobileNetV2 still achieved a high accuracy of 96.4%. This performance is particularly remarkable with respect to models applied in wider-leaf crops, such as soybean, maize, or sugar beet, which tend to be easier for weed detection techniques to analyze. The precision of the proposed model is comparable to or better than approaches applied in less complex crop types, such as SLIC (98%) in soybean, SegNet (97%) in food crops, and ResNet-50 (95%) in multiweed classification. These results highlight the strength and adaptability of the proposed model and demonstrate its validity even in the more adverse visual environments involved in Teff cultivation.</p>
<section class="tw xbox font-sm" id="Tab10"><h4 class="obj_head">Table 10.</h4>
<div class="caption p"><p>Proposed model performances comparison with different weed detection models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Authors and year</th>
<th align="left" colspan="1" rowspan="1">Aims</th>
<th align="left" colspan="1" rowspan="1">Accuracy (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1"><sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup></td>
<td align="left" colspan="1" rowspan="1">Weed detection from Soybean using SLIC</td>
<td align="left" colspan="1" rowspan="1">98</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup></td>
<td align="left" colspan="1" rowspan="1">Classification, and localizing sugar beets and Weeds using SegNet</td>
<td align="left" colspan="1" rowspan="1">89</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup></td>
<td align="left" colspan="1" rowspan="1">Segmentation of rice Seedlings and weeds using SegNet</td>
<td align="left" colspan="1" rowspan="1">80</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup></td>
<td align="left" colspan="1" rowspan="1">Classification of multiple weeds using ResNet-50</td>
<td align="left" colspan="1" rowspan="1">95</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup></td>
<td align="left" colspan="1" rowspan="1">Detection and classification common weed species using R-CNN</td>
<td align="left" colspan="1" rowspan="1">77</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup></td>
<td align="left" colspan="1" rowspan="1">weeds and soybean crops detections from drone imagery through the combinations of ResNet101 and DSASPP</td>
<td align="left" colspan="1" rowspan="1">97.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Proposed work</td>
<td align="left" colspan="1" rowspan="1">weed Detection based on drone imagery of Teff and Weeds using optimized MobileNetv2</td>
<td align="left" colspan="1" rowspan="1">96.40</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab10/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section><section id="Sec17"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par61">In this study, we investigated the application of fine-tuned deep learning models for classifying and detecting Teff crops and weeds using drone-collected datasets for UAV-assisted precision herbicide application. The performance of MobileNetV2, InceptionResNetV2, DenseNet201, VGG16, Resnet-50, Fasr R-CNN, and YOLOv8 were compared both before and after fine-tuning. The results demonstrated that MobileNetV2 outperformed the other models in terms of accuracy, mAP, and inference time, establishing it as the most suitable model for real-time object detection in precision agriculture. MobileNetV2’s ability to accurately distinguish between Teff crops and weeds with minimal errors suggests that it can significantly enhance the efficiency and precision of herbicide application, reducing chemical usage and minimizing environmental impact. Although YOLOv9, InceptionResNetV2 and DenseNet201 showed promising results, their longer inference times compared to MobileNetV2 indicate the need for further optimization. VGG16, however, struggled with detection accuracy, rendering it less viable for this application. In conclusion, integrating deep learning models such as MobileNetV2 with UAV technology presents significant potential for advancing precision agriculture, particularly in the targeted application of herbicides. This approach not only improves crop management practices but also promotes sustainable and environmentally friendly farming operations. Future research will focus on optimizing these models further, applying them to diverse agricultural settings, and integrating fine-tuned models with drones for real-time herbicide application.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>A.B.A. and A.S.K. wrote the manuscript, code, analysis, investigation, access resources, data curation, A.B.A and T.M.T revised, edited, and validated the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Funding</h2>
<p>This research is not supported by any financial sources.</p></section><section id="notes3"><h2 class="pmc_sec_title">Data availability</h2>
<p>The datasets used and/or analyzed during the current study are available from the corresponding author upon reasonable request.</p></section><section id="notes4"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par62">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Yared, T., Yibekal, A. &amp; Kiya, A. Effect of blended NPS and nitrogen fertilizers rates on yield components and yield of tef. <em>Acad. Res. J. Agric. Sci. Res.</em><strong>8</strong>(4), 361–377. 10.14662/ARJASR2020.018 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yared,%20T.,%20Yibekal,%20A.%20&amp;%20Kiya,%20A.%20Effect%20of%20blended%20NPS%20and%20nitrogen%20fertilizers%20rates%20on%20yield%20components%20and%20yield%20of%20tef.%20Acad.%20Res.%20J.%20Agric.%20Sci.%20Res.8(4),%20361%E2%80%93377.%2010.14662/ARJASR2020.018%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Vickneswaran, M., Carolan, J. C., Saunders, M. &amp; White, B. Establishing the extent of pesticide contamination in Irish agricultural soils. <em>Heliyon.</em>10.1016/j.heliyon.2023.e19416 (2023).
</cite> [<a href="https://doi.org/10.1016/j.heliyon.2023.e19416" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10478240/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37674820/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Vickneswaran,%20M.,%20Carolan,%20J.%20C.,%20Saunders,%20M.%20&amp;%20White,%20B.%20Establishing%20the%20extent%20of%20pesticide%20contamination%20in%20Irish%20agricultural%20soils.%20Heliyon.10.1016/j.heliyon.2023.e19416%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>H. Gebretsadik, M. Haile, and C. F. Yamoah, <em>Tillage Frequency, Soil Compaction and N-Fertilizer Rate Effects on Yield of Teff (Eragrostis Tef (Zucc) Trotter) in Central Zone of Tigray, Northern Ethiopia</em>. 2009.</cite>
</li>
<li id="CR4">
<span class="label">4.</span><cite>Degaga, J. Review on coffee production and marketing in Ethiopia. <em>J. Mark. Consum. Res.</em>10.7176/jmcr/67-02 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Degaga,%20J.%20Review%20on%20coffee%20production%20and%20marketing%20in%20Ethiopia.%20J.%20Mark.%20Consum.%20Res.10.7176/jmcr/67-02%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Parven, A., Meftaul, I. M., Venkateswarlu, K. &amp; Megharaj, M. Herbicides in modern sustainable agriculture: Environmental fate, ecological implications, and human health concerns. <em>Springer Nat.</em>10.1007/s13762-024-05818-y (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Parven,%20A.,%20Meftaul,%20I.%20M.,%20Venkateswarlu,%20K.%20&amp;%20Megharaj,%20M.%20Herbicides%20in%20modern%20sustainable%20agriculture:%20Environmental%20fate,%20ecological%20implications,%20and%20human%20health%20concerns.%20Springer%20Nat.10.1007/s13762-024-05818-y%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR6">
<span class="label">6.</span><cite>Kubiak, A., Wolna-Maruwka, A., Niewiadomska, A. &amp; Pilarska, A. A. The problem of weed infestation of agricultural plantations vs. The assumptions of the European biodiversity strategy. <em>Agronomy</em>10.3390/agronomy12081808 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Kubiak,%20A.,%20Wolna-Maruwka,%20A.,%20Niewiadomska,%20A.%20&amp;%20Pilarska,%20A.%20A.%20The%20problem%20of%20weed%20infestation%20of%20agricultural%20plantations%20vs.%20The%20assumptions%20of%20the%20European%20biodiversity%20strategy.%20Agronomy10.3390/agronomy12081808%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<span class="label">7.</span><cite>Alzubaidi, L. et al. Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions. <em>J. Big. Data.</em>10.1186/s40537-021-00444-8 (2021).
</cite> [<a href="https://doi.org/10.1186/s40537-021-00444-8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8010506/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33816053/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Alzubaidi,%20L.%20et%20al.%20Review%20of%20deep%20learning:%20Concepts,%20CNN%20architectures,%20challenges,%20applications,%20future%20directions.%20J.%20Big.%20Data.10.1186/s40537-021-00444-8%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>Murad, N. Y. et al. Weed detection using deep learning: A systematic literature review. <em>Sensors.</em>10.3390/s23073670 (2023).
</cite> [<a href="https://doi.org/10.3390/s23073670" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10098587/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37050730/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Murad,%20N.%20Y.%20et%20al.%20Weed%20detection%20using%20deep%20learning:%20A%20systematic%20literature%20review.%20Sensors.10.3390/s23073670%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>Zheng, Y. Y. et al. Cropdeep: The crop vision dataset for deep-learning-based classification and detection in precision agriculture. <em>Sensors.</em>10.3390/s19051058 (2019).
</cite> [<a href="https://doi.org/10.3390/s19051058" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6427818/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30832283/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Zheng,%20Y.%20Y.%20et%20al.%20Cropdeep:%20The%20crop%20vision%20dataset%20for%20deep-learning-based%20classification%20and%20detection%20in%20precision%20agriculture.%20Sensors.10.3390/s19051058%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR10">
<span class="label">10.</span><cite>Chebrolu, N. et al. Agricultural robot dataset for plant classification, localization and mapping on sugar beet fields. <em>Int. J. Robot. Res.</em><strong>36</strong>(10), 1045–1052. 10.1177/0278364917720510 (2017).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chebrolu,%20N.%20et%20al.%20Agricultural%20robot%20dataset%20for%20plant%20classification,%20localization%20and%20mapping%20on%20sugar%20beet%20fields.%20Int.%20J.%20Robot.%20Res.36(10),%201045%E2%80%931052.%2010.1177/0278364917720510%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>Chen, P., Xia, T. &amp; Yang, G. A new strategy for weed detection in maize fields. <em>Eur. J. Agron.</em>10.1016/j.eja.2024.127289 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20P.,%20Xia,%20T.%20&amp;%20Yang,%20G.%20A%20new%20strategy%20for%20weed%20detection%20in%20maize%20fields.%20Eur.%20J.%20Agron.10.1016/j.eja.2024.127289%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Kong, X. et al. Lightweight cabbage segmentation network and improved weed detection method. <em>Comput. Electron Agric.</em>10.1016/j.compag.2024.109403 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Kong,%20X.%20et%20al.%20Lightweight%20cabbage%20segmentation%20network%20and%20improved%20weed%20detection%20method.%20Comput.%20Electron%20Agric.10.1016/j.compag.2024.109403%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>Mishra, A. M., Harnal, S., Gautam, V., Tiwari, R. &amp; Upadhyay, S. Weed density estimation in soya bean crop using deep convolutional neural networks in smart agriculture. <em>J. Plant Dis. Prot.</em><strong>129</strong>(3), 593–604. 10.1007/s41348-022-00595-7 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Mishra,%20A.%20M.,%20Harnal,%20S.,%20Gautam,%20V.,%20Tiwari,%20R.%20&amp;%20Upadhyay,%20S.%20Weed%20density%20estimation%20in%20soya%20bean%20crop%20using%20deep%20convolutional%20neural%20networks%20in%20smart%20agriculture.%20J.%20Plant%20Dis.%20Prot.129(3),%20593%E2%80%93604.%2010.1007/s41348-022-00595-7%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Ali, M. S. et al. A comprehensive dataset of rice field weed detection from Bangladesh. <em>Data Brief.</em>10.1016/j.dib.2024.110981 (2024).
</cite> [<a href="https://doi.org/10.1016/j.dib.2024.110981" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11827074/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39957731/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ali,%20M.%20S.%20et%20al.%20A%20comprehensive%20dataset%20of%20rice%20field%20weed%20detection%20from%20Bangladesh.%20Data%20Brief.10.1016/j.dib.2024.110981%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR15">
<span class="label">15.</span><cite>Xiao, F., Wang, H., Xu, Y. &amp; Zhang, R. Fruit detection and recognition based on deep learning for automatic harvesting: An overview and review. <em>Agronomy</em>10.3390/agronomy13061625 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Xiao,%20F.,%20Wang,%20H.,%20Xu,%20Y.%20&amp;%20Zhang,%20R.%20Fruit%20detection%20and%20recognition%20based%20on%20deep%20learning%20for%20automatic%20harvesting:%20An%20overview%20and%20review.%20Agronomy10.3390/agronomy13061625%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>M. Berhan and D. Bekele, “<em>Review of Major Cereal Crops Production Losses, Quality Deterioration of Grains by Weeds and Its Prevention in Ethiopia</em>,” 2021. Available: <a href="https://www.researchgate.net/publication/356189011" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.researchgate.net/publication/356189011</a></cite>
</li>
<li id="CR17">
<span class="label">17.</span><cite>Sa, I. et al. WeedNet: Dense semantic weed classification using multispectral images and MAV for smart farming. <em>IEEE Robot Autom. Lett.</em><strong>3</strong>(1), 588–595. 10.1109/LRA.2017.2774979 (2018).</cite> [<a href="https://scholar.google.com/scholar_lookup?Sa,%20I.%20et%20al.%20WeedNet:%20Dense%20semantic%20weed%20classification%20using%20multispectral%20images%20and%20MAV%20for%20smart%20farming.%20IEEE%20Robot%20Autom.%20Lett.3(1),%20588%E2%80%93595.%2010.1109/LRA.2017.2774979%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR18">
<span class="label">18.</span><cite>T. M. Giselsson, R. N. Jørgensen, P. K. Jensen, M. Dyrmann, and H. S. Midtiby, “<em>A Public Image Database for Benchmark of Plant Seedling Classification Algorithms</em>,” 2017. Available: <a href="http://arxiv.org/abs/1711.05458" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1711.05458</a></cite>
</li>
<li id="CR19">
<span class="label">19.</span><cite>dos Santos Ferreira, A., Matte Freitas, D., Gonçalves da Silva, G., Pistori, H. &amp; Theophilo Folhes, M. Weed detection in soybean crops using ConvNets. <em>Comput. Electron Agric.</em><strong>143</strong>, 314–324. 10.1016/j.compag.2017.10.027 (2017).</cite> [<a href="https://scholar.google.com/scholar_lookup?dos%20Santos%20Ferreira,%20A.,%20Matte%20Freitas,%20D.,%20Gon%C3%A7alves%20da%20Silva,%20G.,%20Pistori,%20H.%20&amp;%20Theophilo%20Folhes,%20M.%20Weed%20detection%20in%20soybean%20crops%20using%20ConvNets.%20Comput.%20Electron%20Agric.143,%20314%E2%80%93324.%2010.1016/j.compag.2017.10.027%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Yu, J., Schumann, A. W., Cao, Z., Sharpe, S. M. &amp; Boyd, N. S. Weed detection in perennial ryegrass with deep learning convolutional neural network. <em>Front. Plant Sci.</em>10.3389/fpls.2019.01422 (2019).
</cite> [<a href="https://doi.org/10.3389/fpls.2019.01422" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6836412/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31737026/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Yu,%20J.,%20Schumann,%20A.%20W.,%20Cao,%20Z.,%20Sharpe,%20S.%20M.%20&amp;%20Boyd,%20N.%20S.%20Weed%20detection%20in%20perennial%20ryegrass%20with%20deep%20learning%20convolutional%20neural%20network.%20Front.%20Plant%20Sci.10.3389/fpls.2019.01422%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR21">
<span class="label">21.</span><cite>Ma, X. et al. Fully convolutional network for rice seedling and weed image segmentation at the seedling stage in paddy fields. <em>PLoS ONE</em>10.1371/journal.pone.0215676 (2019).
</cite> [<a href="https://doi.org/10.1371/journal.pone.0215676" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6472823/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30998770/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ma,%20X.%20et%20al.%20Fully%20convolutional%20network%20for%20rice%20seedling%20and%20weed%20image%20segmentation%20at%20the%20seedling%20stage%20in%20paddy%20fields.%20PLoS%20ONE10.1371/journal.pone.0215676%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Olsen, A. et al. Deepweeds: A multiclass weed species image dataset for deep learning. <em>Sci. Rep.</em>10.1038/s41598-018-38343-3 (2019).
</cite> [<a href="https://doi.org/10.1038/s41598-018-38343-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6375952/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30765729/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Olsen,%20A.%20et%20al.%20Deepweeds:%20A%20multiclass%20weed%20species%20image%20dataset%20for%20deep%20learning.%20Sci.%20Rep.10.1038/s41598-018-38343-3%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR23">
<span class="label">23.</span><cite>Madsen, S. L. et al. Open plant phenotype database of common weeds in Denmark. <em>Remote Sens.</em>10.3390/RS12081246 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Madsen,%20S.%20L.%20et%20al.%20Open%20plant%20phenotype%20database%20of%20common%20weeds%20in%20Denmark.%20Remote%20Sens.10.3390/RS12081246%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Sudars, K., Jasko, J., Namatevs, I., Ozola, L. &amp; Badaukis, N. Dataset of annotated food crops and weed images for robotic computer vision control. <em>Data Brief.</em>10.1016/j.dib.2020.105833 (2020).
</cite> [<a href="https://doi.org/10.1016/j.dib.2020.105833" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7305380/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32577458/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Sudars,%20K.,%20Jasko,%20J.,%20Namatevs,%20I.,%20Ozola,%20L.%20&amp;%20Badaukis,%20N.%20Dataset%20of%20annotated%20food%20crops%20and%20weed%20images%20for%20robotic%20computer%20vision%20control.%20Data%20Brief.10.1016/j.dib.2020.105833%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR25">
<span class="label">25.</span><cite>Kanda, P. S., Xia, K., Kyslytysna, A. &amp; Owoola, E. O. Tomato leaf disease recognition on leaf images based on fine-tuned residual neural networks. <em>Plants</em>10.3390/plants11212935 (2022).
</cite> [<a href="https://doi.org/10.3390/plants11212935" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9653987/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36365386/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Kanda,%20P.%20S.,%20Xia,%20K.,%20Kyslytysna,%20A.%20&amp;%20Owoola,%20E.%20O.%20Tomato%20leaf%20disease%20recognition%20on%20leaf%20images%20based%20on%20fine-tuned%20residual%20neural%20networks.%20Plants10.3390/plants11212935%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<span class="label">26.</span><cite>Wang, D. et al. A review of deep learning in multiscale agricultural sensing. <em>Comput. Electron. Agric.</em>10.3390/rs14030559 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20D.%20et%20al.%20A%20review%20of%20deep%20learning%20in%20multiscale%20agricultural%20sensing.%20Comput.%20Electron.%20Agric.10.3390/rs14030559%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR27">
<span class="label">27.</span><cite>L. Li, S. Zhang, and B. Wang, <em>Plant disease detection and classification by deep learning—A review</em>. Institute of Electrical and Electronics Engineers Inc. 10.1109/ACCESS.2021.3069646. (2021).</cite>
</li>
<li id="CR28">
<span class="label">28.</span><cite>Patil, A. S., Mailapalli, D. R. &amp; Singh, P. K. <em>Drone Technology Reshaping Agriculture: A Meta-Review and Bibliometric Analysis on Fertilizer and Pesticide Deployment</em> (Springer Science and Business Media, Cham, 2024). 10.1007/s42853-024-00240-1.</cite> [<a href="https://scholar.google.com/scholar_lookup?Patil,%20A.%20S.,%20Mailapalli,%20D.%20R.%20&amp;%20Singh,%20P.%20K.%20Drone%20Technology%20Reshaping%20Agriculture:%20A%20Meta-Review%20and%20Bibliometric%20Analysis%20on%20Fertilizer%20and%20Pesticide%20Deployment%20(Springer%20Science%20and%20Business%20Media,%20Cham,%202024).%2010.1007/s42853-024-00240-1." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29">
<span class="label">29.</span><cite>Meesaragandla, S., Jagtap, M. P., Khatri, N., Madan, H. &amp; Vadduri, A. A. Herbicide spraying and weed identification using drone technology in modern farms: A comprehensive review. <em>Results Eng.</em>10.1016/j.rineng.2024.101870 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Meesaragandla,%20S.,%20Jagtap,%20M.%20P.,%20Khatri,%20N.,%20Madan,%20H.%20&amp;%20Vadduri,%20A.%20A.%20Herbicide%20spraying%20and%20weed%20identification%20using%20drone%20technology%20in%20modern%20farms:%20A%20comprehensive%20review.%20Results%20Eng.10.1016/j.rineng.2024.101870%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR30">
<span class="label">30.</span><cite>Shahi, T. B., Dahal, S., Sitaula, C., Neupane, A. &amp; Guo, W. Deep learning-based weed detection using UAV images: A comparative study. <em>Drones.</em>10.3390/drones7100624 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Shahi,%20T.%20B.,%20Dahal,%20S.,%20Sitaula,%20C.,%20Neupane,%20A.%20&amp;%20Guo,%20W.%20Deep%20learning-based%20weed%20detection%20using%20UAV%20images:%20A%20comparative%20study.%20Drones.10.3390/drones7100624%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Liu, B. An automated weed detection approach using deep learning and UAV imagery in smart agriculture system. <em>J. Optics</em><strong>53</strong>(3), 2183–2191. 10.1007/s12596-023-01445-x (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20B.%20An%20automated%20weed%20detection%20approach%20using%20deep%20learning%20and%20UAV%20imagery%20in%20smart%20agriculture%20system.%20J.%20Optics53(3),%202183%E2%80%932191.%2010.1007/s12596-023-01445-x%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Ong, P., Teo, K. S. &amp; Sia, C. K. UAV-based weed detection in Chinese cabbage using deep learning. <em>Smart Agric. Technol.</em>10.1016/j.atech.2023.100181 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ong,%20P.,%20Teo,%20K.%20S.%20&amp;%20Sia,%20C.%20K.%20UAV-based%20weed%20detection%20in%20Chinese%20cabbage%20using%20deep%20learning.%20Smart%20Agric.%20Technol.10.1016/j.atech.2023.100181%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33">
<span class="label">33.</span><cite>Lu, C. et al. Weed instance segmentation from UAV orthomosaic images based on deep learning. <em>Smart Agric. Technol.</em>10.1016/j.atech.2025.100966 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Lu,%20C.%20et%20al.%20Weed%20instance%20segmentation%20from%20UAV%20orthomosaic%20images%20based%20on%20deep%20learning.%20Smart%20Agric.%20Technol.10.1016/j.atech.2025.100966%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR34">
<span class="label">34.</span><cite>Peffers, K., Tuunanen, T., Rothenberger, M. A. &amp; Chatterjee, S. A design science research methodology for information systems research. <em>J. Manag. Inf. Syst.</em><strong>24</strong>(3), 45–77. 10.2753/MIS0742-1222240302 (2007).</cite> [<a href="https://scholar.google.com/scholar_lookup?Peffers,%20K.,%20Tuunanen,%20T.,%20Rothenberger,%20M.%20A.%20&amp;%20Chatterjee,%20S.%20A%20design%20science%20research%20methodology%20for%20information%20systems%20research.%20J.%20Manag.%20Inf.%20Syst.24(3),%2045%E2%80%9377.%2010.2753/MIS0742-1222240302%20(2007)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Meseret Gezahegn, A. &amp; Tamiru, S. Effect of seed rate and row spacing on (Tef Eragrostis tef (Zucc) Trotter) production at central highlands of Ethiopia. <em>J. Plant Sci.</em><strong>9</strong>(3), 71. 10.11648/j.jps.20210903.11 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Meseret%20Gezahegn,%20A.%20&amp;%20Tamiru,%20S.%20Effect%20of%20seed%20rate%20and%20row%20spacing%20on%20(Tef%20Eragrostis%20tef%20(Zucc)%20Trotter)%20production%20at%20central%20highlands%20of%20Ethiopia.%20J.%20Plant%20Sci.9(3),%2071.%2010.11648/j.jps.20210903.11%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<span class="label">36.</span><cite>Al-Ameen, Z., Muttar, A. &amp; Al-Badrani, G. Improving the sharpness of digital image using an amended unsharp mask filter. <em>Int. J. Image Graph. Signal Process.</em><strong>11</strong>(3), 1–9. 10.5815/ijigsp.2019.03.01 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Al-Ameen,%20Z.,%20Muttar,%20A.%20&amp;%20Al-Badrani,%20G.%20Improving%20the%20sharpness%20of%20digital%20image%20using%20an%20amended%20unsharp%20mask%20filter.%20Int.%20J.%20Image%20Graph.%20Signal%20Process.11(3),%201%E2%80%939.%2010.5815/ijigsp.2019.03.01%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR37">
<span class="label">37.</span><cite>Zufria, I. &amp; Syahnan, M. Improved digital image quality using the Gaussian filter method. <em>Int. J. Inf. Syst. Technol. Akreditasi</em><strong>5</strong>(158), 556–563 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zufria,%20I.%20&amp;%20Syahnan,%20M.%20Improved%20digital%20image%20quality%20using%20the%20Gaussian%20filter%20method.%20Int.%20J.%20Inf.%20Syst.%20Technol.%20Akreditasi5(158),%20556%E2%80%93563%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR38">
<span class="label">38.</span><cite>B. Nair B J and A. S. Nair, Ancient horoscopic palm leaf binarization using a deep binarization model–RESNET. In: <em>Proc. 5th International Conference on Computing Methodologies and Communication, ICCMC </em>2021, Institute of Electrical and Electronics Engineers Inc., pp. 1524–1529. 10.1109/ICCMC51019.2021.9418461. (2021).</cite>
</li>
<li id="CR39">
<span class="label">39.</span><cite>Ma, C., Chi, G., Ju, X., Zhang, J. &amp; Yan, C. YOLO-CWD: A novel model for crop and weed detection based on improved YOLOv8. <em>Crop Protect.</em>10.1016/j.cropro.2025.107169 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ma,%20C.,%20Chi,%20G.,%20Ju,%20X.,%20Zhang,%20J.%20&amp;%20Yan,%20C.%20YOLO-CWD:%20A%20novel%20model%20for%20crop%20and%20weed%20detection%20based%20on%20improved%20YOLOv8.%20Crop%20Protect.10.1016/j.cropro.2025.107169%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Olorunfemi, B. O., Nwulu, N. I., Adebo, O. A. &amp; Kavadias, K. A. <em>Advancements in Machine Visions for Fruit Sorting and Grading: A Bibliometric Analysis, Systematic Review, and Future Research Directions</em> (Elsevier B.V, 2024). 10.1016/j.jafr.2024.101154.</cite> [<a href="https://scholar.google.com/scholar_lookup?Olorunfemi,%20B.%20O.,%20Nwulu,%20N.%20I.,%20Adebo,%20O.%20A.%20&amp;%20Kavadias,%20K.%20A.%20Advancements%20in%20Machine%20Visions%20for%20Fruit%20Sorting%20and%20Grading:%20A%20Bibliometric%20Analysis,%20Systematic%20Review,%20and%20Future%20Research%20Directions%20(Elsevier%20B.V,%202024).%2010.1016/j.jafr.2024.101154." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR41">
<span class="label">41.</span><cite>Qiankun, K. et al. A method for measuring geometric information content of area cartographic objects based on discrepancy degree of shape points. <em>Geocarto Int.</em>10.1080/10106049.2023.2275685 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Qiankun,%20K.%20et%20al.%20A%20method%20for%20measuring%20geometric%20information%20content%20of%20area%20cartographic%20objects%20based%20on%20discrepancy%20degree%20of%20shape%20points.%20Geocarto%20Int.10.1080/10106049.2023.2275685%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR42">
<span class="label">42.</span><cite>Rutili de Lima, C., Khan, S. G., Shah, S. H. &amp; Ferri, L. Mask region-based CNNs for cervical cancer progression diagnosis on pap smear examinations. <em>Heliyon.</em>10.1016/j.heliyon.2023.e21388 (2023).
</cite> [<a href="https://doi.org/10.1016/j.heliyon.2023.e21388" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10641213/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37964829/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Rutili%20de%20Lima,%20C.,%20Khan,%20S.%20G.,%20Shah,%20S.%20H.%20&amp;%20Ferri,%20L.%20Mask%20region-based%20CNNs%20for%20cervical%20cancer%20progression%20diagnosis%20on%20pap%20smear%20examinations.%20Heliyon.10.1016/j.heliyon.2023.e21388%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets used and/or analyzed during the current study are available from the corresponding author upon reasonable request.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-15380-3"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_15380.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (2.6 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12371000/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12371000/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12371000%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371000/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12371000/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12371000/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40841734/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12371000/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40841734/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12371000/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12371000/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="z0DpmrnX4SOI79mm4nhmAt7okeDKei7CXWDZcO0wsuqRY4sYZSpOS5gIeHhOktvi">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
