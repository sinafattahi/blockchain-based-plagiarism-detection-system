
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Neonatal pose estimation in the unaltered clinical environment with fusion of RGB, depth and IR images - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E486998AEF4D33058699002CFD01F0.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="npjdigitmed">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373853/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="NPJ Digital Medicine">
<meta name="citation_title" content="Neonatal pose estimation in the unaltered clinical environment with fusion of RGB, depth and IR images">
<meta name="citation_author" content="Alex Grafton">
<meta name="citation_author_institution" content="Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK">
<meta name="citation_author" content="Joana M Warnecke">
<meta name="citation_author_institution" content="Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK">
<meta name="citation_author" content="Maxwell Li">
<meta name="citation_author_institution" content="Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK">
<meta name="citation_author" content="Eric He">
<meta name="citation_author_institution" content="Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK">
<meta name="citation_author" content="Lynn Thomson">
<meta name="citation_author_institution" content="Department of Paediatrics, University of Cambridge, Cambridge, CB2 0QQ UK">
<meta name="citation_author" content="Kathryn Beardsall">
<meta name="citation_author_institution" content="Department of Paediatrics, University of Cambridge, Cambridge, CB2 0QQ UK">
<meta name="citation_author" content="Joan Lasenby">
<meta name="citation_author_institution" content="Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK">
<meta name="citation_publication_date" content="2025 Aug 22">
<meta name="citation_volume" content="8">
<meta name="citation_firstpage" content="539">
<meta name="citation_doi" content="10.1038/s41746-025-01929-z">
<meta name="citation_pmid" content="40847126">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373853/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373853/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373853/pdf/41746_2025_Article_1929.pdf">
<meta name="description" content="Visual monitoring of pre-term infants in intensive care is critical to ensuring proper development and treatment. Camera systems have been explored for this purpose, with human pose estimation having applications in monitoring position, motion, ...">
<meta name="og:title" content="Neonatal pose estimation in the unaltered clinical environment with fusion of RGB, depth and IR images">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Visual monitoring of pre-term infants in intensive care is critical to ensuring proper development and treatment. Camera systems have been explored for this purpose, with human pose estimation having applications in monitoring position, motion, ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373853/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12373853">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41746-025-01929-z"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41746_2025_Article_1929.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373853%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12373853/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12373853/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373853/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-npjdigitmed.jpg" alt="NPJ Digital Medicine logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to NPJ Digital Medicine" title="Link to NPJ Digital Medicine" shape="default" href="https://www.nature.com/npjdigitalmed/" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">NPJ Digit Med</button></div>. 2025 Aug 22;8:539. doi: <a href="https://doi.org/10.1038/s41746-025-01929-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41746-025-01929-z</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22NPJ%20Digit%20Med%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22NPJ%20Digit%20Med%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22NPJ%20Digit%20Med%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22NPJ%20Digit%20Med%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Neonatal pose estimation in the unaltered clinical environment with fusion of RGB, depth and IR images</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Grafton%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Alex Grafton</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Alex Grafton</span></h3>
<div class="p">
<sup>1</sup>Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Grafton%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Alex Grafton</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Warnecke%20JM%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Joana M Warnecke</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Joana M Warnecke</span></h3>
<div class="p">
<sup>1</sup>Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Warnecke%20JM%22%5BAuthor%5D" class="usa-link"><span class="name western">Joana M Warnecke</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Maxwell Li</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Maxwell Li</span></h3>
<div class="p">
<sup>1</sup>Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Maxwell Li</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22He%20E%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Eric He</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Eric He</span></h3>
<div class="p">
<sup>1</sup>Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22He%20E%22%5BAuthor%5D" class="usa-link"><span class="name western">Eric He</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Thomson%20L%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Lynn Thomson</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Lynn Thomson</span></h3>
<div class="p">
<sup>2</sup>Department of Paediatrics, University of Cambridge, Cambridge, CB2 0QQ UK </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Thomson%20L%22%5BAuthor%5D" class="usa-link"><span class="name western">Lynn Thomson</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Beardsall%20K%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Kathryn Beardsall</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Kathryn Beardsall</span></h3>
<div class="p">
<sup>2</sup>Department of Paediatrics, University of Cambridge, Cambridge, CB2 0QQ UK </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Beardsall%20K%22%5BAuthor%5D" class="usa-link"><span class="name western">Kathryn Beardsall</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Lasenby%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Joan Lasenby</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Joan Lasenby</span></h3>
<div class="p">
<sup>1</sup>Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Lasenby%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Joan Lasenby</span></a>
</div>
</div>
<sup>1</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ UK </div>
<div id="Aff2">
<sup>2</sup>Department of Paediatrics, University of Cambridge, Cambridge, CB2 0QQ UK </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Jun 27; Accepted 2025 Aug 3; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12373853  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40847126/" class="usa-link">40847126</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Visual monitoring of pre-term infants in intensive care is critical to ensuring proper development and treatment. Camera systems have been explored for this purpose, with human pose estimation having applications in monitoring position, motion, behaviour and vital signs. Validation in the full range of clinical visual scenarios is necessary to prove real-life utility. We conducted a clinical study to collect RGB, depth and infra-red video from 24 participants with no modifications to clinical care. We propose and train image fusion pose estimation algorithms for locating the torso key-points. Our best-performing approach, a late fusion method, achieves an average precision score of 0.811. Chest covering or side lying decrease the object key-point similarity score by 0.15 and 0.1 respectively, while accounting for 50% and 44% of the time. The baby’s positioning and covering supports their development and comfort, and these scenarios should therefore be considered when validating visual monitoring algorithms.</p>
<section id="kwd-group1" class="kwd-group"><p><strong>Subject terms:</strong> Computational biology and bioinformatics, Engineering, Health care, Mathematics and computing, Medical research</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">The neonatal period—the first 28 days of life<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>—is among the most critical in human development. Pre-term infants experience increased risk of short- and long-term complications<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a>–<a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>, while neonatal mortality accounts for almost half of all under-5 mortality<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup>. In the UK, around 100,000 babies—approximately one out of every seven—will spend time in neonatal care every year<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>. In the neonatal intensive care unit (NICU), the highest dependency care, clinical staff are responsible for multiple babies, supported by patient monitors with continuous vital sign monitoring. However, they are unable to provide continuous visual monitoring—each nurse’s attention is divided between patients, while spending time writing notes, preparing feeds and carrying out their other clinical responsibilities. To address this monitoring gap, cameras, coupled with image processing and intelligent decision tools can be employed to identify critical situations<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>.</p>
<p id="Par3">Continuous pose estimation has a range of applications in supporting neonatal care. Low frame-rate analysis, (e.g. once per second) could monitor the baby’s position and ensure that positioning guidelines<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a>,<a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup> are followed. It can also provide a reference point for vital sign monitoring algorithms<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a>,<a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>, especially those that rely on a manually selected region of interest<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a>–<a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>, or for motion detection algorithms to distinguish localized and gross body movement<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a>–<a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. High frame-rate analysis can provide more detailed information about a baby’s movements, suitable for automatic General Movements Assessment (GMA)<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a>,<a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>, cerebral dysfunction, or monitoring sedation<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>.</p>
<p id="Par4">Existing work in this area is predominantly focussed on controlled environments—well-lit, with babies uncovered and clearly visible, which is ideal for image analysis. In reality, the neonatal incubator can be a visually busy and varied environment, with monitoring equipment, blankets, clothing and nurses’ hands cluttering the scene, making pose estimation difficult. Understanding the performance of computer vision algorithms in this environment is a gap in existing literature. In this paper, we explore several approaches for pose estimation in the clinical environment combining RGB, depth and infra-red imaging. We investigate the covering and position of the babies during the recordings, and evaluate the pose estimation approaches across the different imaging scenarios. Our data includes multiple 24-hour recordings, during which nurses were instructed to continue delivering care as though the camera was not present.</p>
<p id="Par5">Adult pose estimation is an extensively studied field, summarized by Munea et al. in a review for 2D<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup> and by Wang et al. for 3D pose estimation<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>, and further by Gamra et al. in 2021<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>, and Zheng et al. in 2023<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>. The two primary datasets for adult pose estimation are MPII Human Pose<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>, assessed using the percentage of correct keypoints (PCKh) metric, and MS COCO<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>, assessed using the average precision (AP) metric. The HRNet<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a>,<a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup> backbone yields promising results for the MPII dataset<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>, with PCKh@0.5=92.3. In addition, many subsequent state-of-the-art models are based on an HRNet backbone with additional enhancements<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a>–<a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>. The vision transformer, specifically ViTPose ViTAE-G,<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> achieves state-of-the-art performance on the COCO dataset, albeit with vastly more parameters than comparable models. Smaller ViTPose-based models also achieve promising results. Other transformers, such as Swin<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a>,<a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup> and HRFormer<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup> are also successful. In our work, we use transfer learning from pose estimation models designed for adults and older infants, which are available in the MMPose model zoo<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>. The MMPose model zoo provides pre-trained models models and training configurations, with readily comparable scores. We use the HRNet model (up to AP=0.767), HRFormer (up to AP=0.774), and ViTPose (sizes S ‘small’ and B ‘base’, AP=0.739 and 0.757 respectively).</p>
<p id="Par6">Pre-trained models from adults have been frequently used for infant pose estimation and deliver promising results. Huang et al.<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup> achieved an AP of 93.6 using the publicly available infant image dataset MINI-RGBD<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>. This dataset uses synthetic images of infants above term age superimposed on patterned backgrounds. Gross et al.<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup> used 1424 videos from an infant pose dataset and OpenPose<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup> pre-trained on MPII, achieving 99.61% PCKh@0.5, including a mixture of hospital and home recordings. These recordings were also of post-term age infants on plain backgrounds. Moccia et al. use a depth camera in clinical conditions, achieving a median root mean square (RMS) distance of limbs (limbs being a group of three joints) - of 10.2 pixels, across all limbs<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>, and subsequently depth video with temporal processing from 16 preterm infants with a median RMS distance of 9.06 pixels<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>. Further work<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a>–<a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup> continues improve neonatal pose estimation, though it is found that generalizability across neonatal datasets is poor. This behaviour is also found by Jahn et al.<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>, who show that retraining on one’s own infant dataset improves performance on that particular dataset, it fails to generalize to other infant datasets where it is outperformed by a generic adult model.</p>
<p id="Par7">These datasets differ in terms of both the ages of the subjects, ranging from pre-term newborns to old babies, and the recording environment. The BabyPose dataset<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup> contains depth images of babies in incubators, our target demographic, but babies are uncovered and their limbs are clearly visible. The MINI-RGBD dataset<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup> is a synthetic dataset with generated images imposed over real baby poses. Each has a plain background. No existing dataset includes images of babies in the presence of blanket covering or clinical interventions. Understanding the performance of these algorithms in this environment is needed to demonstrate feasibility for continuous clinical use. Datasets used in the literature are summarized in Table <a href="#Tab1" class="usa-link">1</a>. We distinguish between an <em>infant</em>—a baby who is post-term age, and may be able to move and crawl unaided, and a <em>neonate</em>—a new-born baby, who has yet to be discharged from hospital. Only the BabyPose dataset contains neonates in their NICU cots. The work in this paper focusses on <em>pre-term neonates</em>.</p>
<section class="tw xbox font-sm" id="Tab1"><h3 class="obj_head">Table 1.</h3>
<div class="caption p"><p>Summary of existing infant/neonate image datasets</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th colspan="1" rowspan="1">Dataset</th>
<th colspan="1" rowspan="1">Type</th>
<th colspan="1" rowspan="1">Imaging</th>
<th colspan="1" rowspan="1">Environment</th>
<th colspan="1" rowspan="1">Subjects</th>
<th colspan="1" rowspan="1">Scene</th>
</tr></thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">MINI-RGBD<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>
</td>
<td colspan="1" rowspan="1">S</td>
<td colspan="1" rowspan="1">RGBD</td>
<td colspan="1" rowspan="1">Flat Surface</td>
<td colspan="1" rowspan="1">Infant</td>
<td colspan="1" rowspan="1">Unclothed, Supine</td>
</tr>
<tr>
<td colspan="1" rowspan="1">SyRIP<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>
</td>
<td colspan="1" rowspan="1">R+S</td>
<td colspan="1" rowspan="1">RGB</td>
<td colspan="1" rowspan="1">Home</td>
<td colspan="1" rowspan="1">Infant</td>
<td colspan="1" rowspan="1">Varied Poses</td>
</tr>
<tr>
<td colspan="1" rowspan="1">BabyPose<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup>
</td>
<td colspan="1" rowspan="1">R</td>
<td colspan="1" rowspan="1">D</td>
<td colspan="1" rowspan="1">NICU</td>
<td colspan="1" rowspan="1">Neonate</td>
<td colspan="1" rowspan="1">Unclothed, Supine</td>
</tr>
<tr>
<td colspan="1" rowspan="1">AGMA<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup>
</td>
<td colspan="1" rowspan="1">R</td>
<td colspan="1" rowspan="1">Stereo RGB</td>
<td colspan="1" rowspan="1">GMA Clinic</td>
<td colspan="1" rowspan="1">Neonate</td>
<td colspan="1" rowspan="1">Unclothed, Supine</td>
</tr>
<tr>
<td colspan="1" rowspan="1">AggPose<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup>
</td>
<td colspan="1" rowspan="1">R</td>
<td colspan="1" rowspan="1">RGB</td>
<td colspan="1" rowspan="1">GMA Clinic</td>
<td colspan="1" rowspan="1">Infant</td>
<td colspan="1" rowspan="1">Unclothed, Supine</td>
</tr>
<tr>
<td colspan="1" rowspan="1">CII-RGBD<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup>
</td>
<td colspan="1" rowspan="1">R</td>
<td colspan="1" rowspan="1">Stereo RGB</td>
<td colspan="1" rowspan="1">GMA Clinic</td>
<td colspan="1" rowspan="1">Infant</td>
<td colspan="1" rowspan="1">Supine, Varied Clothing</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Jahn et al.<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>
</td>
<td colspan="1" rowspan="1">R</td>
<td colspan="1" rowspan="1">RGB</td>
<td colspan="1" rowspan="1">GMA Clinic</td>
<td colspan="1" rowspan="1">Infant</td>
<td colspan="1" rowspan="1">Supine, Varied Clothing</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p9"><p><em>R</em> Real, <em>S</em> Synthetic.</p></div></div></section><p id="Par8">To overcome this challenge, the fusion of different camera types may increase reliability<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup> because each delivers different information which could not be extracted from a single signal<sup><a href="#CR49" class="usa-link" aria-describedby="CR49">49</a></sup>. For example, although depth imaging does not see patterns and similar features, it is unaffected by poor lighting conditions. Although existing work has included either RGB or depth images, to our knowledge no work has attempted to combine multiple image types within the same framework.</p>
<p id="Par9">We recorded 24 babies using a calibrated RGB, depth and IR camera system during 1-hour and 24-hour sessions in the neonatal intensive care unit at Addenbrooke’s Hospital, Cambridge, UK. These recordings were in the normal clinical settings, and the 24-hour recordings included real interventions, light changes, movement and interactions with parents. This paper investigates the performance of signal fusion approaches for combining the three imaging methods in the unaltered clinical environment. In summary, we make the following contributions:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par10">An assessment of the types of scenes found in the normal NICU clinical environment, and the performance of pose estimation algorithms.</p></li>
<li><p id="Par11">Proposal and comparison of methods for combining RGB, depth and IR imaging for pose estimation, where earlier stages are separate (image-specific) and later stages are shared.</p></li>
<li><p id="Par12">Comparison of neonatal and infant datasets, finding that models trained on our dataset show good generalization to other NICU data, but poor generalization to older infant and adult data.</p></li>
</ul></section><section id="Sec2"><h2 class="pmc_sec_title">Results</h2>
<section id="Sec3"><h3 class="pmc_sec_title">Dataset</h3>
<p id="Par13">The data collection study consists of our 1-hour recordings, testing the clinical set-up, and 24-hour recordings, which we use to analyse the clinical environment. Each recording contains RGB, depth and infra-red images. The distribution of the baby’s position across the 24-hour recordings is shown in Fig. <a href="#Fig1" class="usa-link">1</a>, arranged by position and covering. We find that babies are typically less covered during interventions and that the uncovered/half covered period, when the baby is easily visible, accounts for approximately half of the time. Babies are typically more covered when on their side. The supine, uncovered scenario is around 15% of the dataset (excluding interventions) and around 10% (including interventions). The complete distribution for the 1-hour dataset and the 1-hour plus 24-hour dataset is provided in the <a href="#MOESM1" class="usa-link">Supplementary Information</a> (Supplementary Section <a href="#MOESM1" class="usa-link">1</a>), Supplementary Figs. <a href="#MOESM1" class="usa-link">1</a>-<a href="#MOESM1" class="usa-link">3</a> and Supplementary Tables <a href="#MOESM1" class="usa-link">1</a> and <a href="#MOESM1" class="usa-link">2</a>.</p>
<figure class="fig xbox font-sm" id="Fig1"><h4 class="obj_head">Fig. 1. Distribution of the 24-hour data collection, divided into position and covering.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/71209a4a18a9/41746_2025_1929_Fig1_HTML.jpg" loading="lazy" id="d33e620" height="326" width="700" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> Each column represents a given position. <strong>b</strong> Each column represents a level of covering. Int. intervention, Unc. uncovered. Percentages indicate the overall time in that position or covering.</p></figcaption></figure></section><section id="Sec4"><h3 class="pmc_sec_title">Model evaluation</h3>
<p id="Par14">In addition to <em>Single-Image models</em>, which use only one image type (RGB, depth, or IR), we analyse three fusion methods. Early Image Fusion models (EIF) stack the image along the channel dimension at the input, with up to five color channels (RGB-D-IR), or a combination thereof. Intermediate Image Fusion models (IIF-X) have <em>X</em> separate stages that are specific to each image type, after which the feature maps are summed and input to a single set of shared layers. Late Image Fusion models (LIF-X) also have <em>X</em> separate stages followed by the remaining shared stages, but the heatmaps from each image are only combined after the key-point head. IIF and LIF fusion methods are tested from <em>X</em> = 1 to 4, representing separate/shared division after each of the four blocks of HRNet and HRFormer. All models are trained and tested using five-fold cross-validation. More detailed explanation of the models is provided in Section “Pose Estimation Models”.</p>
<p id="Par15">Table <a href="#Tab2" class="usa-link">2</a> shows the AP scores for the models using the HRNet backbone. Scores are presented as mean ± standard deviation across the five folds. For 10 of the 15 models, models with input size 384 × 384 outperformed those with input size 256 × 256. The HRNet W48 model outperforms the W32 model in 11 of the 15 models. Table <a href="#Tab3" class="usa-link">3</a> shows the scores for the HRFormer and vision transformer backbones. The HRFormer and HRNet models achieve similar scores, though the ViTPose-based models are substantially worse, so several additional experiments are conducted for the depth-only ViTPose-B model to understand if this is due to the experimental configuration. For the depth-only model (AP=0.741), we find that freezing half of the stages reduces performance (AP=0.721) and freezing only the multi-head self-attention module as suggested by the original authors<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> offers only marginal improvement (AP=0.742). This improved score does not exceed other backbones. Training curves over the 50 epochs are shown in Figure <a href="#Fig2" class="usa-link">2</a> for the HRNet-W48 LIF-3 and HRFormer-B IIF-2 model, with no noticeable improvement on the test sets after 30 epochs.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>AP Scores across model types using the HRNet backbone</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th colspan="1" rowspan="1">Model</th>
<th colspan="1" rowspan="1">W32-256</th>
<th colspan="1" rowspan="1">W32-384</th>
<th colspan="1" rowspan="1">W48-384</th>
</tr></thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">RGB</td>
<td colspan="1" rowspan="1">0.753 ± 0.102</td>
<td colspan="1" rowspan="1">0.762 ± 0.095</td>
<td colspan="1" rowspan="1">0.779 ± 0.093</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Depth</td>
<td colspan="1" rowspan="1">0.778 ± 0.082</td>
<td colspan="1" rowspan="1">0.769 ± 0.090</td>
<td colspan="1" rowspan="1">0.765 ± 0.091</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IR</td>
<td colspan="1" rowspan="1">0.739 ± 0.099</td>
<td colspan="1" rowspan="1">0.747 ± 0.095</td>
<td colspan="1" rowspan="1">0.758 ± 0.096</td>
</tr>
<tr>
<td colspan="1" rowspan="1">EIF-RGB-D</td>
<td colspan="1" rowspan="1">0.761 ± 0.086</td>
<td colspan="1" rowspan="1">0.765 ± 0.090</td>
<td colspan="1" rowspan="1">0.768 ± 0.093</td>
</tr>
<tr>
<td colspan="1" rowspan="1">EIF-RGB-IR</td>
<td colspan="1" rowspan="1">0.747 ± 0.092</td>
<td colspan="1" rowspan="1">0.765 ± 0.089</td>
<td colspan="1" rowspan="1">0.715 ± 0.121</td>
</tr>
<tr>
<td colspan="1" rowspan="1">EIF-D-IR</td>
<td colspan="1" rowspan="1">0.780 ± 0.087</td>
<td colspan="1" rowspan="1">0.760 ± 0.094</td>
<td colspan="1" rowspan="1">0.785 ± 0.083</td>
</tr>
<tr>
<td colspan="1" rowspan="1">EIF-RGB-D-IR</td>
<td colspan="1" rowspan="1">0.763 ± 0.088</td>
<td colspan="1" rowspan="1">0.753 ± 0.092</td>
<td colspan="1" rowspan="1">0.773 ± 0.083</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-1</td>
<td colspan="1" rowspan="1">0.785 ± 0.083</td>
<td colspan="1" rowspan="1">0.788 ± 0.083</td>
<td colspan="1" rowspan="1">0.793 ± 0.086</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">
<strong>0.800</strong> ± <strong>0.076</strong>
</td>
<td colspan="1" rowspan="1">0.784 ± 0.090</td>
<td colspan="1" rowspan="1">0.774 ± 0.094</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-3</td>
<td colspan="1" rowspan="1">0.792 ± 0.077</td>
<td colspan="1" rowspan="1">0.790 ± 0.080</td>
<td colspan="1" rowspan="1">0.776 ± 0.094</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-4</td>
<td colspan="1" rowspan="1">0.753 ± 0.099</td>
<td colspan="1" rowspan="1">0.761 ± 0.100</td>
<td colspan="1" rowspan="1">0.771 ± 0.097</td>
</tr>
<tr>
<td colspan="1" rowspan="1">LIF-1</td>
<td colspan="1" rowspan="1">0.769 ± 0.088</td>
<td colspan="1" rowspan="1">0.775 ± 0.091</td>
<td colspan="1" rowspan="1">0.798 ± 0.078</td>
</tr>
<tr>
<td colspan="1" rowspan="1">LIF-2</td>
<td colspan="1" rowspan="1">0.757 ± 0.101</td>
<td colspan="1" rowspan="1">0.775 ± 0.089</td>
<td colspan="1" rowspan="1">0.771 ± 0.095</td>
</tr>
<tr>
<td colspan="1" rowspan="1">LIF-3</td>
<td colspan="1" rowspan="1">0.775 ± 0.089</td>
<td colspan="1" rowspan="1">
<strong>0.802</strong> ± <strong>0.077</strong>
</td>
<td colspan="1" rowspan="1">
<strong>0.811</strong> ± <strong>0.069</strong>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">LIF-4</td>
<td colspan="1" rowspan="1">0.749 ± 0.099</td>
<td colspan="1" rowspan="1">0.765 ± 0.100</td>
<td colspan="1" rowspan="1">0.772 ± 0.089</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p20">
<p>Scores in bold are the best scores for each backbone.</p>
<p><em>E/I/LIF</em> Early/Intermediate/Late Image Fusion.</p>
</div></div></section><section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>AP Scores using the HRFormer and ViTPose backbones</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th colspan="1" rowspan="1">Model</th>
<th colspan="1" rowspan="1">HRFormer-S</th>
<th colspan="1" rowspan="1">HRFormer-B</th>
<th colspan="1" rowspan="1">ViTPose-S</th>
<th colspan="1" rowspan="1">ViTPose-B</th>
</tr></thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">RGB</td>
<td colspan="1" rowspan="1">0.747 ± 0.112</td>
<td colspan="1" rowspan="1">0.760 ± 0.099</td>
<td colspan="1" rowspan="1">0.714 ± 0.098</td>
<td colspan="1" rowspan="1">0.693 ± 0.129</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Depth</td>
<td colspan="1" rowspan="1">0.788 ± 0.097</td>
<td colspan="1" rowspan="1">0.795 ± 0.091</td>
<td colspan="1" rowspan="1">0.657 ± 0.143</td>
<td colspan="1" rowspan="1">0.671 ± 0.139</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IR</td>
<td colspan="1" rowspan="1">0.764 ± 0.093</td>
<td colspan="1" rowspan="1">0.776 ± 0.088</td>
<td colspan="1" rowspan="1">0.657 ± 0.131</td>
<td colspan="1" rowspan="1">0.711 ± 0.108</td>
</tr>
<tr>
<td colspan="1" rowspan="1">EIF-RGB-D</td>
<td colspan="1" rowspan="1">0.764 ± 0.086</td>
<td colspan="1" rowspan="1">0.783 ± 0.077</td>
<td colspan="1" rowspan="1">0.663 ± 0.076</td>
<td colspan="1" rowspan="1">0.698 ± 0.073</td>
</tr>
<tr>
<td colspan="1" rowspan="1">EIF-RGB-IR</td>
<td colspan="1" rowspan="1">0.770 ± 0.089</td>
<td colspan="1" rowspan="1">0.795 ± 0.077</td>
<td colspan="1" rowspan="1">0.645 ± 0.096</td>
<td colspan="1" rowspan="1">0.682 ± 0.077</td>
</tr>
<tr>
<td colspan="1" rowspan="1">EIF-D-IR</td>
<td colspan="1" rowspan="1">0.788 ± 0.087</td>
<td colspan="1" rowspan="1">0.795 ± 0.081</td>
<td colspan="1" rowspan="1">0.662 ± 0.14</td>
<td colspan="1" rowspan="1">0.671 ± 0.14</td>
</tr>
<tr>
<td colspan="1" rowspan="1">EIF-RGB-D-IR</td>
<td colspan="1" rowspan="1">0.780 ± 0.090</td>
<td colspan="1" rowspan="1">0.777 ± 0.079</td>
<td colspan="1" rowspan="1">0.653 ± 0.082</td>
<td colspan="1" rowspan="1">0.690 ± 0.079</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-1</td>
<td colspan="1" rowspan="1">0.785 ± 0.096</td>
<td colspan="1" rowspan="1">0.794 ± 0.094</td>
<td colspan="1" rowspan="1">
<strong>0.751</strong> ± <strong>0.105</strong>
</td>
<td colspan="1" rowspan="1">0.714 ± 0.100</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">
<strong>0.800</strong> ± <strong>0.096</strong>
</td>
<td colspan="1" rowspan="1">
<strong>0.809</strong> ± <strong>0.085</strong>
</td>
<td colspan="1" rowspan="1">0.715 ± 0.096</td>
<td colspan="1" rowspan="1">
<strong>0.753</strong> ± <strong>0.121</strong>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-3</td>
<td colspan="1" rowspan="1">0.785 ± 0.098</td>
<td colspan="1" rowspan="1">0.804 ± 0.088</td>
<td colspan="1" rowspan="1">0.710 ± 0.110</td>
<td colspan="1" rowspan="1">0.746 ± 0.118</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-4</td>
<td colspan="1" rowspan="1">0.753 ± 0.103</td>
<td colspan="1" rowspan="1">0.759 ± 0.095</td>
<td colspan="1" rowspan="1">0.721 ± 0.093</td>
<td colspan="1" rowspan="1">0.742 ± 0.091</td>
</tr>
<tr>
<td colspan="1" rowspan="1">LIF-1</td>
<td colspan="1" rowspan="1">0.752 ± 0.103</td>
<td colspan="1" rowspan="1">0.752 ± 0.103</td>
<td colspan="1" rowspan="1">0.671 ± 0.096</td>
<td colspan="1" rowspan="1">0.676 ± 0.110</td>
</tr>
<tr>
<td colspan="1" rowspan="1">LIF-2</td>
<td colspan="1" rowspan="1">0.772 ± 0.097</td>
<td colspan="1" rowspan="1">0.772 ± 0.097</td>
<td colspan="1" rowspan="1">0.661 ± 0.092</td>
<td colspan="1" rowspan="1">0.674 ± 0.103</td>
</tr>
<tr>
<td colspan="1" rowspan="1">LIF-3</td>
<td colspan="1" rowspan="1">0.764 ± 0.105</td>
<td colspan="1" rowspan="1">0.761 ± 0.104</td>
<td colspan="1" rowspan="1">0.625 ± 0.106</td>
<td colspan="1" rowspan="1">0.643 ± 0.106</td>
</tr>
<tr>
<td colspan="1" rowspan="1">LIF-4</td>
<td colspan="1" rowspan="1">0.741 ± 0.107</td>
<td colspan="1" rowspan="1">0.744 ± 0.096</td>
<td colspan="1" rowspan="1">0.655 ± 0.093</td>
<td colspan="1" rowspan="1">0.679 ± 0.101</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p23"><p>S indicates small configuration, B indicates base configuration. The best performance for each backbone is given in bold.</p></div></div></section><figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2. Model training curves showing training loss and test set AP.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/361ae638a52a/41746_2025_1929_Fig2_HTML.jpg" loading="lazy" id="d33e1056" height="224" width="683" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> Training curves for HRNet-W48 LIF-3. <strong>b</strong> Training curves for HRFormer-B IIF-2. The horizontal line shows the final score. In each case, there is minimal difference after 30 epochs.</p></figcaption></figure></section><section id="Sec5"><h3 class="pmc_sec_title">Inference time</h3>
<p id="Par16">The inference cost and number of model parameters are given in Fig. <a href="#Fig3" class="usa-link">3</a>. Table <a href="#Tab4" class="usa-link">4</a> shows the inference latency in milliseconds for each model across three hardware specifications, reflecting different possible applications: MacBook Air (Apple M2 processor, CPU), for processing on a cot-side iPad or mini PC; a desktop computer with a modest GPU (NVIDIA RTX 3060Ti, CUDA), acting as a local computer away from the cot; a high-performance server (NVIDIA A100 80GB, CUDA) for off-site analysis. Models suitable for real-time (for continuous motion analysis) and 1 frame-per-second inference (for position summaries or vital sign monitoring) are highlighted, thought it should be noted that an off-site server would likely process data from multiple cameras at once time. Results are given for multiple batch sizes where applicable. The HRNet-W48 LIF-3 model achieves the highest performance, at the cost of the highest number of parameters and alongside the HRFormer-B IIF-2 model, the highest inference complexity. The ViTPose models, whose performance is worse in each case than the other models, are not included in these plots. The HRFormer-S and HRNet-W32 IIF-2 models offer high performance at lower complexity.</p>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3. Model performance and computation requirements for selected models.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/a100c67495b7/41746_2025_1929_Fig3_HTML.jpg" loading="lazy" id="d33e1080" height="179" width="683" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> Score against model inference cost. <strong>b</strong> Score against number of parameters. HRF-S/B HRFormer Small/Base, HR32/48 HRNet-W32/48, D Depth.</p></figcaption></figure><section class="tw xbox font-sm" id="Tab4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>Inference latency in milliseconds for selected models in three hardware configurations</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th colspan="1" rowspan="1"></th>
<th colspan="1" rowspan="1"></th>
<th colspan="1" rowspan="1">Apple M2</th>
<th colspan="2" rowspan="1">RTX 3060Ti</th>
<th colspan="3" rowspan="1">NVIDIA A100</th>
</tr>
<tr>
<th colspan="1" rowspan="1">Backbone</th>
<th colspan="1" rowspan="1">Model</th>
<th colspan="1" rowspan="1">B=1</th>
<th colspan="1" rowspan="1">B=1</th>
<th colspan="1" rowspan="1">B=10</th>
<th colspan="1" rowspan="1">B=1</th>
<th colspan="1" rowspan="1">B=10</th>
<th colspan="1" rowspan="1">B=100</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-256</td>
<td colspan="1" rowspan="1">Depth</td>
<td colspan="1" rowspan="1"><strong>129</strong></td>
<td colspan="1" rowspan="1">
<strong>11.4</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>3.79</strong><sup>a</sup>
</td>
<td colspan="1" rowspan="1">
<strong>29.4</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>2.94</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>0.956</strong><sup><strong>a</strong></sup>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-256</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1"><strong>316</strong></td>
<td colspan="1" rowspan="1">
<strong>18.2</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>14.7</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>29.6</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>3.96</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>3.20</strong><sup><strong>a</strong></sup>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-384</td>
<td colspan="1" rowspan="1">LIF-3</td>
<td colspan="1" rowspan="1"><strong>208</strong></td>
<td colspan="1" rowspan="1">
<strong>12.5</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>5.87</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1"><strong>36.2</strong></td>
<td colspan="1" rowspan="1">
<strong>3.64</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>1.52</strong><sup><strong>a</strong></sup>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W48-384</td>
<td colspan="1" rowspan="1">RGB</td>
<td colspan="1" rowspan="1"><strong>629</strong></td>
<td colspan="1" rowspan="1">
<strong>31.7</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>24.6</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1"><strong>91.8</strong></td>
<td colspan="1" rowspan="1">
<strong>8.87</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>6.53</strong><sup><strong>a</strong></sup>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W48-384</td>
<td colspan="1" rowspan="1">LIF-3</td>
<td colspan="1" rowspan="1">1070</td>
<td colspan="1" rowspan="1"><strong>51.7</strong></td>
<td colspan="1" rowspan="1"><strong>44.8</strong></td>
<td colspan="1" rowspan="1"><strong>89.9</strong></td>
<td colspan="1" rowspan="1">
<strong>11.9</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>9.59</strong><sup><strong>a</strong></sup>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRFormer-S</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1"><strong>432</strong></td>
<td colspan="1" rowspan="1">
<strong>23.3</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>16.5</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1"><strong>55.9</strong></td>
<td colspan="1" rowspan="1">
<strong>7.60</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>6.49</strong><sup><strong>a</strong></sup>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRFormer-B</td>
<td colspan="1" rowspan="1">Depth</td>
<td colspan="1" rowspan="1"><strong>784</strong></td>
<td colspan="1" rowspan="1"><strong>52.4</strong></td>
<td colspan="1" rowspan="1"><strong>40.0</strong></td>
<td colspan="1" rowspan="1"><strong>47.0</strong></td>
<td colspan="1" rowspan="1">
<strong>16.6</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>15.0</strong><sup><strong>a</strong></sup>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRFormer-B</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">1090</td>
<td colspan="1" rowspan="1"><strong>59.3</strong></td>
<td colspan="1" rowspan="1"><strong>52.0</strong></td>
<td colspan="1" rowspan="1"><strong>57.0</strong></td>
<td colspan="1" rowspan="1">
<strong>21.2</strong><sup><strong>a</strong></sup>
</td>
<td colspan="1" rowspan="1">
<strong>19.2</strong><sup><strong>a</strong></sup>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p28">
<p>The three hardware configurations represent a cot-side small PC, on-site desktop PC, or off-site server.</p>
<p>Bold indicates suitability for 1fps inference.</p>
<p><em>B</em> batch size.</p>
<p><sup>a</sup>30fps inference.</p>
</div></div></section></section><section id="Sec6"><h3 class="pmc_sec_title">Effect of position and covering</h3>
<p id="Par17">Next, we consider variations in the level of covering and presence of intervention in the image for the best performing models. The OKS scores for a selection of the best performing model backbones and architectures are presented in Table <a href="#Tab5" class="usa-link">5</a> divided by position and covering. For every model, including those not presented in this table, increasing the covering decreases the mean OKS score, most notably when the baby is fully covered. The differences in performance between every pair of coverings or positions for each model are significant at <em>p</em> = 1<em>e</em> − 5, except for prone/supine. The prone and supine positions are similar in performance and neither position is better overall. However, performance is lower in all models during interventions, and even lower when the baby is on their side. The scores with and without intervention are presented for the HRNet-W48-384 LIF-3 and HRFormer-B IIF-2 models in Fig. <a href="#Fig4" class="usa-link">4</a>. Interventions marginally decrease the score on average, and noticeably reduce the fifth percentile score for the HRFormer-B model. The most substantial difference caused by interventions is for babies that are at least 3/4 covered. This may be expected given that interventions effectively increase the level of covering by occluding parts of the baby, and moving from 3/4 covering to full covering (during non-intervention) drastically reduces performance. All differences between covering/intervention are significant at <em>p</em> = 1<em>e</em> − 5 except uncovered/half-covered with interventions, 3/4-covered/fully covered with interventions, and half-covered with/without interventions. The range of OKS scores is presented, rather than the average precision, as these scores are taken across each fold of cross-validation (and hence with different models). The complete statistical analysis is provided in the <a href="#MOESM1" class="usa-link">Supplementary Information</a> (Supplementary Section <a href="#MOESM1" class="usa-link">2</a>), comparing covering and interventions in Supplementary Tables <a href="#MOESM1" class="usa-link">3</a>-<a href="#MOESM1" class="usa-link">5</a>, position in Supplementary Table <a href="#MOESM1" class="usa-link">6</a> and covering in Supplementary Table <a href="#MOESM1" class="usa-link">7</a>.</p>
<section class="tw xbox font-sm" id="Tab5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>Mean OKS score for selected best-performing models</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th colspan="1" rowspan="1">Backbone</th>
<th colspan="1" rowspan="1">Model</th>
<th colspan="1" rowspan="1">Uncovered</th>
<th colspan="1" rowspan="1">Half-Covered</th>
<th colspan="1" rowspan="1">3/4 Covered</th>
<th colspan="1" rowspan="1">Fully Covered</th>
</tr></thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-256</td>
<td colspan="1" rowspan="1">Depth</td>
<td colspan="1" rowspan="1">0.925</td>
<td colspan="1" rowspan="1">0.898</td>
<td colspan="1" rowspan="1">0.880</td>
<td colspan="1" rowspan="1">0.753</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-256</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.933</td>
<td colspan="1" rowspan="1">0.915</td>
<td colspan="1" rowspan="1">0.892</td>
<td colspan="1" rowspan="1">0.772</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-384</td>
<td colspan="1" rowspan="1">LIF-3</td>
<td colspan="1" rowspan="1">0.938</td>
<td colspan="1" rowspan="1">0.908</td>
<td colspan="1" rowspan="1">0.882</td>
<td colspan="1" rowspan="1">0.805</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W48-384</td>
<td colspan="1" rowspan="1">RGB</td>
<td colspan="1" rowspan="1">0.923</td>
<td colspan="1" rowspan="1">0.919</td>
<td colspan="1" rowspan="1">0.877</td>
<td colspan="1" rowspan="1">0.695</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W48-384</td>
<td colspan="1" rowspan="1">LIF-3</td>
<td colspan="1" rowspan="1">0.941</td>
<td colspan="1" rowspan="1">0.916</td>
<td colspan="1" rowspan="1">0.882</td>
<td colspan="1" rowspan="1">0.794</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRFormer-S</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.934</td>
<td colspan="1" rowspan="1">0.910</td>
<td colspan="1" rowspan="1">0.892</td>
<td colspan="1" rowspan="1">0.752</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRFormer-B</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.933</td>
<td colspan="1" rowspan="1">0.918</td>
<td colspan="1" rowspan="1">0.891</td>
<td colspan="1" rowspan="1">0.780</td>
</tr>
</tbody>
</table></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th colspan="1" rowspan="1">Backbone</th>
<th colspan="1" rowspan="1">Model</th>
<th colspan="1" rowspan="1">Prone</th>
<th colspan="1" rowspan="1">Supine</th>
<th colspan="1" rowspan="1">Side</th>
<th colspan="1" rowspan="1">Intervention</th>
</tr></thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-256</td>
<td colspan="1" rowspan="1">Depth</td>
<td colspan="1" rowspan="1">0.935</td>
<td colspan="1" rowspan="1">0.934</td>
<td colspan="1" rowspan="1">0.823</td>
<td colspan="1" rowspan="1">0.837</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-256</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.952</td>
<td colspan="1" rowspan="1">0.929</td>
<td colspan="1" rowspan="1">0.836</td>
<td colspan="1" rowspan="1">0.861</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-384</td>
<td colspan="1" rowspan="1">LIF-3</td>
<td colspan="1" rowspan="1">0.932</td>
<td colspan="1" rowspan="1">0.946</td>
<td colspan="1" rowspan="1">0.850</td>
<td colspan="1" rowspan="1">0.877</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-384</td>
<td colspan="1" rowspan="1">RGB</td>
<td colspan="1" rowspan="1">0.933</td>
<td colspan="1" rowspan="1">0.919</td>
<td colspan="1" rowspan="1">0.805</td>
<td colspan="1" rowspan="1">0.846</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W48-384</td>
<td colspan="1" rowspan="1">LIF-3</td>
<td colspan="1" rowspan="1">0.940</td>
<td colspan="1" rowspan="1">0.942</td>
<td colspan="1" rowspan="1">0.843</td>
<td colspan="1" rowspan="1">0.889</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRFormer-S</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.938</td>
<td colspan="1" rowspan="1">0.930</td>
<td colspan="1" rowspan="1">0.824</td>
<td colspan="1" rowspan="1">0.866</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRFormer-B</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.944</td>
<td colspan="1" rowspan="1">0.940</td>
<td colspan="1" rowspan="1">0.835</td>
<td colspan="1" rowspan="1">0.866</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p34"><p>Each row shows the score for each level of covering or position. For every model, the score decreases as the covering increases. There is no general trend between prone and supine, but every model scores lower during interventions than prone/supine and lower still when in the side position. Note that mean OKS scores are typically higher than the AP value.</p></div></div></section><figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4. Box-whisker plots of object key-point similarity (OKS) scores, divided by level of covering and intervention.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/c4da8214eb91/41746_2025_1929_Fig4_HTML.jpg" loading="lazy" id="d33e1731" height="212" width="683" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> Scores are shown for the HRNet-W48 LIF-3 model. <strong>b</strong> Scores are shown for the HRFormer-B IIF-2 model. I Intervention, N/I Not intervention. Quartiles and 5/95 percentile are shown.</p></figcaption></figure><p id="Par18">Table <a href="#Tab6" class="usa-link">6</a> shows the comparative performance of the models with and without pre-training on the COCO adult dataset. The models with pre-training achieve higher scores in each case.</p>
<section class="tw xbox font-sm" id="Tab6"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>AP Scores for selected HRNet-W32-256 IIF with and without pre-training</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th colspan="1" rowspan="1">Model Type</th>
<th colspan="1" rowspan="1">With Pre-training</th>
<th colspan="1" rowspan="1">Without Pre-training</th>
</tr></thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">IIF-1</td>
<td colspan="1" rowspan="1">0.772</td>
<td colspan="1" rowspan="1">0.647</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.781</td>
<td colspan="1" rowspan="1">0.614</td>
</tr>
<tr>
<td colspan="1" rowspan="1">IIF-3</td>
<td colspan="1" rowspan="1">0.772</td>
<td colspan="1" rowspan="1">0.645</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p38"><p>Each model is trained using either the weights from the COCO dataset, or randomly initialised.</p></div></div></section></section><section id="Sec7"><h3 class="pmc_sec_title">Cross-dataset comparison</h3>
<p id="Par19">Next, we compare results across other datasets, summarised in Table <a href="#Tab7" class="usa-link">7</a>. To create models suitable for grayscale inputs, the first convolutional layer is summed along the input channel dimension. Models trained on COCO perform well on the SyRIP dataset, but poorly on our dataset, indicating that are images are out-of-domain for the adult model. When evaluating models trained on COCO on depth images, both BabyPose and our dataset perform poorly; this is expected as the model is expecting grayscale images which would contain different features. We use the HRNet-W48-384 model to compare performance on other datasets. We find that our model performs well on BabyPose, outperforming the original work, while performing poorly on SyRIP; this can be expected due to SyRIP’s images being of older babies, and noting the high score for COCO → SyRIP models. We also see that training on our dataset has greatly degraded performance on the COCO dataset (AP=0.698 to 0.351 for RGB and AP=0.682 to 0.081 for depth).</p>
<section class="tw xbox font-sm" id="Tab7"><h4 class="obj_head">Table 7.</h4>
<div class="caption p"><p>Cross-domain model performance</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th colspan="1" rowspan="1">Dataset</th>
<th colspan="1" rowspan="1">AP</th>
<th colspan="1" rowspan="1">AP50</th>
<th colspan="1" rowspan="1">AP75</th>
<th colspan="1" rowspan="1">RMSE</th>
</tr></thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">COCO (RGB)</td>
<td colspan="1" rowspan="1">0.698</td>
<td colspan="1" rowspan="1">0.832</td>
<td colspan="1" rowspan="1">0.738</td>
<td colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td colspan="1" rowspan="1">COCO (RGB) → SyRIP</td>
<td colspan="1" rowspan="1">0.727</td>
<td colspan="1" rowspan="1">0.931</td>
<td colspan="1" rowspan="1">0.810</td>
<td colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td colspan="1" rowspan="1">COCO (RGB) → Ours (RGB)</td>
<td colspan="1" rowspan="1">0.223</td>
<td colspan="1" rowspan="1">0.426</td>
<td colspan="1" rowspan="1">0.197</td>
<td colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td colspan="1" rowspan="1">COCO (Gray)</td>
<td colspan="1" rowspan="1">0.682</td>
<td colspan="1" rowspan="1">0.800</td>
<td colspan="1" rowspan="1">0.729</td>
<td colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td colspan="1" rowspan="1">COCO (Gray) → BabyPose</td>
<td colspan="1" rowspan="1">0.201</td>
<td colspan="1" rowspan="1">0.377</td>
<td colspan="1" rowspan="1">0.202</td>
<td colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td colspan="1" rowspan="1">COCO (Gray) → Ours (Depth)</td>
<td colspan="1" rowspan="1">0.118</td>
<td colspan="1" rowspan="1">0.230</td>
<td colspan="1" rowspan="1">0.106</td>
<td colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Ours (Depth)</td>
<td colspan="1" rowspan="1">0.765</td>
<td colspan="1" rowspan="1">0.944</td>
<td colspan="1" rowspan="1">0.850</td>
<td colspan="1" rowspan="1">7.11</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Ours (Depth) → BabyPose</td>
<td colspan="1" rowspan="1">0.824</td>
<td colspan="1" rowspan="1">0.979</td>
<td colspan="1" rowspan="1">0.951</td>
<td colspan="1" rowspan="1">5.68</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Ours (Depth) → COCO (Grayscale)</td>
<td colspan="1" rowspan="1">0.081</td>
<td colspan="1" rowspan="1">0.214</td>
<td colspan="1" rowspan="1">0.060</td>
<td colspan="1" rowspan="1">50.84</td>
</tr>
<tr>
<td colspan="1" rowspan="1">BabyPose*</td>
<td colspan="1" rowspan="1">–</td>
<td colspan="1" rowspan="1">–</td>
<td colspan="1" rowspan="1">–</td>
<td colspan="1" rowspan="1">9.06</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Ours (RGB)</td>
<td colspan="1" rowspan="1">0.779</td>
<td colspan="1" rowspan="1">0.929</td>
<td colspan="1" rowspan="1">0.837</td>
<td colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Ours (RGB) → SyRIP</td>
<td colspan="1" rowspan="1">0.383</td>
<td colspan="1" rowspan="1">0.639</td>
<td colspan="1" rowspan="1">0.395</td>
<td colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Ours (RGB) → COCO (RGB)</td>
<td colspan="1" rowspan="1">0.351</td>
<td colspan="1" rowspan="1">0.588</td>
<td colspan="1" rowspan="1">0.360</td>
<td colspan="1" rowspan="1">–</td>
</tr>
<tr>
<td colspan="1" rowspan="1">SyRIP*</td>
<td colspan="1" rowspan="1">0.911</td>
<td colspan="1" rowspan="1">0.985</td>
<td colspan="1" rowspan="1">0.985</td>
<td colspan="1" rowspan="1">–</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p41"><p>A → B indicates a model is trained on A and tested on B. Models labelled * have scores taken directly from the respective authors’ original work. The RMSE is only included for depth models to allow comparison with BabyPose authors’ results.</p></div></div></section><p id="Par20">Finally, we present some examples of images in our dataset, generated using the HRNet-W32-256 IIF-2 model. Figure <a href="#Fig5" class="usa-link">5</a> shows examples of infants in different poses with key-point detections. Figure <a href="#Fig6" class="usa-link">6</a> shows two examples, where only one image type is provided with the others set to zero. In the first example, the image is very dark and the model only works well when the IR image is given. In the second example, the image is well-lit and the model works best when the RGB image is available, although the IR image works well, but providing only the depth image to the fusion model does not allow good detection. This approach is tested across the entire dataset, presented in Table <a href="#Tab8" class="usa-link">8</a>. The IIF-2 models lose performance when any images are missing, though the performance loss is lowest for the depth image. For the LIF-3 model, the performance loss is much smaller when images are missing and removing the depth image marginally increases performance. Poor visibility is the most common scenario affecting image quality. Some examples of motion artefacts are provided in Supplementary Fig. <a href="#MOESM1" class="usa-link">4</a> and discussed in the <a href="#MOESM1" class="usa-link">Supplementary Information</a> (Supplementary Section <a href="#MOESM1" class="usa-link">3</a>).</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5. Example images, one from each of the five dataset folds.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/0b9ceba1aad3/41746_2025_1929_Fig5_HTML.jpg" loading="lazy" id="d33e1994" height="195" width="700" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Each image is shown with the detections overlaid. In the fourth image, the model is inaccurate when locating the hips due to the baby being on its side.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6. Detections for example images when only one image type is available.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/59e5e1f314fc/41746_2025_1929_Fig6_HTML.jpg" loading="lazy" id="d33e2002" height="177" width="700" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>In each case, the other images are set to zero. The input image and summed heatmaps are shown. The top example is for a dark image (covered incubator). The bottom row shows a well-lit image.</p></figcaption></figure><section class="tw xbox font-sm" id="Tab8"><h4 class="obj_head">Table 8.</h4>
<div class="caption p"><p>AP scores for selected fusion models when an image is not provided, or when a single image is provided</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th colspan="1" rowspan="1"></th>
<th colspan="1" rowspan="1"></th>
<th colspan="1" rowspan="1">All</th>
<th colspan="3" rowspan="1">Missing Image</th>
<th colspan="3" rowspan="1">Single Image</th>
</tr>
<tr>
<th colspan="1" rowspan="1">Backbone</th>
<th colspan="1" rowspan="1">Model</th>
<th colspan="1" rowspan="1">Images</th>
<th colspan="1" rowspan="1">RGB</th>
<th colspan="1" rowspan="1">Depth</th>
<th colspan="1" rowspan="1">IR</th>
<th colspan="1" rowspan="1">RGB</th>
<th colspan="1" rowspan="1">Depth</th>
<th colspan="1" rowspan="1">IR</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">HRNet-W32-256</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.800</td>
<td colspan="1" rowspan="1">0.675</td>
<td colspan="1" rowspan="1">0.734</td>
<td colspan="1" rowspan="1">0.716</td>
<td colspan="1" rowspan="1">0.494</td>
<td colspan="1" rowspan="1">0.088</td>
<td colspan="1" rowspan="1">0.164</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRNet-W48-384</td>
<td colspan="1" rowspan="1">LIF-3</td>
<td colspan="1" rowspan="1">0.811</td>
<td colspan="1" rowspan="1">0.741</td>
<td colspan="1" rowspan="1">0.813</td>
<td colspan="1" rowspan="1">0.744</td>
<td colspan="1" rowspan="1">0.749</td>
<td colspan="1" rowspan="1">0.064</td>
<td colspan="1" rowspan="1">0.746</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRFormer-S</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.800</td>
<td colspan="1" rowspan="1">0.662</td>
<td colspan="1" rowspan="1">0.753</td>
<td colspan="1" rowspan="1">0.704</td>
<td colspan="1" rowspan="1">0.500</td>
<td colspan="1" rowspan="1">0.015</td>
<td colspan="1" rowspan="1">0.151</td>
</tr>
<tr>
<td colspan="1" rowspan="1">HRFormer-B</td>
<td colspan="1" rowspan="1">IIF-2</td>
<td colspan="1" rowspan="1">0.809</td>
<td colspan="1" rowspan="1">0.608</td>
<td colspan="1" rowspan="1">0.771</td>
<td colspan="1" rowspan="1">0.724</td>
<td colspan="1" rowspan="1">0.496</td>
<td colspan="1" rowspan="1">0.004</td>
<td colspan="1" rowspan="1">0.104</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section><section id="Sec8"><h2 class="pmc_sec_title">Discussion</h2>
<p id="Par21">The challenges of the neonatal clinical setting necessitate an alternative approach from typical RGB pose estimation. This paper has proposed early, intermediate and late fusion methods for combining RGB, depth and infra-red images for neonatal pose estimation, based on retraining from models originally trained for pose estimation in adults. Four model structures were explored; the use of a single image type only, or combining images at the input stage (EIF), summing features within the model (IIF) or summing output heatmaps (LIF). The possibility of sharing weights for each image type was also considered.</p>
<p id="Par22">The dataset contained 14 one-hour and ten 24-hour recordings. The 24-hour recordings were analysed to understand the percentage of time that babies spend in different positions and coverings. It was found that babies spent approximately half of their time in the uncovered (no blanket cover) or half-covered (blanket covering the lower half of the body) state. In the remaining half of the data, the majority of the baby’s chest is covered. A subset of frames were annotated for training, with increased frequency where the depth variation was greater. While this approach somewhat biases the dataset towards selecting images with interventions, the number of frames selected in a fifteen-minute period is fixed; babies’ positions and coverings change much less frequently. Excluding interventions, 31% of images contained babies on their back (supine), 25% on their front (prone) and 44% on their side. These results have implications for future neonatal camera-based approaches: existing work includes babies which are predominantly unclothed and supine, accounting for around 15% of the non-intervention data in our dataset. This would inflate performance scores as the baby is (a) more visible and (b) always known to be in the same position, so the model does not have to identify if the baby is prone or supine.</p>
<p id="Par23">We only annotate the hips and shoulders as these joints are visible in most frames. Including other key-points would either unbalance the dataset (joints such as ankles and knees are often not visible) or not represent the NICU environment, as certain types of images would become oversampled. When comparing to other work, only these four joints are evaluated. Five-fold cross validation was used for training and testing. This method ensures that models are tested on every image and baby in the dataset<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>. It also ensures that each baby is tested entirely unseen. This should provide greater confidence that the model can perform well on additional data.</p>
<p id="Par24">We believe that existing methods for scoring detections require some adaptation for neonatal data. The RMS error does not account for the size of the image or subject. The PCKh metric has not been adjusted for the proportionally larger head of neonates, compared to adults. We calculated the median torso-to-head-size ratio for the MPII (adult) dataset (1.28) and for ours (0.83), indicating that the head is larger than the torso for our neonates. The AP metric is chosen as it uses the area of the subject, and contains per-key-point constants to account for the relative difficulty of each joint type. However, the per-key-point constants are based variance of human annotation in images of adults. An appropriate set of constants is needed for neonatal data; for exampe, Jahn et al.<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup> report greater error between human annotators and pose estimators for the hip joints than shoulders. This would require a larger dataset with all key-points annotated, which is a limitation of our work.</p>
<p id="Par25">Comparison with other work is difficult due to lack of availability of images and models for privacy reasons. In addition, datasets contain images of infants in different environments and at different ages, and no available dataset contains multiple imaging modalities. We are therefore limited to comparing our single-image models with other work. We found that an adult COCO HRNet model with AP=0.682 scores only 0.201 on the BabyPose dataset, and 0.118 on our dataset, while scoring 0.727 on the SyRIP dataset. Conversely, our HRNet models score highly on our dataset (0.779) and the BabyPose dataset (0.824), but lower on the SyRIP (0.351) and COCO (0.383) datasets. The AP scores for our models evaluated on BabyPose (0.824) and ours (0.765) for the depth-only model also suggest that our depth dataset is similar to BabyPose, albeit with more challenging images due to the lower score and inclusion of multiple poses, interventions and covering. In summary, we find generalizabilty between adult/older infant datasets (COCO and SyRIP), and between neonatal datasets (ours and BabyPose), with retraining required between the two types.</p>
<p id="Par26">Three backbones are tested - HRNet, HRFormer, and the Vision Transformer (labelled ViTPose). For single images, the best performance was achieved for RGB or depth models. A selection of fusion models (EIF-D-IR, IIF-1,2,3 and LIF-3) also performed well, outperforming the single-image models. The best-performing single-image model used the depth image; however, providing only the depth image to the fusion models greatly reduced performance. As the depth and IR images come from the same sensor, it is likely that either only the RGB image is available, or only depth and IR images are available, and in these cases, an RGB-specific or EIF-D-IR model is superior. Despite attempts to modify the training, the vision transformer models exhibited worse performance than the HRFormer and HRNet models.</p>
<p id="Par27">For the HRFormer-based models and HRNet-W32-256, we find that IIF-X outperforms LIF-X for all X, and that IIF-2 outperforms all EIF models. This is also true for vision transformer models, but the lower overall performance of these models should be addressed before drawing a conclusion. The HRNet W32/48-384 models show different behaviour; LIF-3 outperforms all other models, and also has a marginally lower standard deviation across folds. This difference cannot be attributed purely to the resolution of the image (possibly requiring more modality-specific stages) as the HRFormer models use the same resolution. The LIF-3 model behaves differently to IIF-2 when one or two images are missing; removing the depth image marginally increases the score (0.811 to 0.813) and removing the IR or RGB image does not cause such drastic performance loss. The IIF-2 model’s deterioration can be explained by the latter stages of the model being trained to assume that all images are available, while removing images from the LIF-3 model would simply reduce the heatmap intensity. For example, when the RGB image is dark, the IIF-2 model should rely on the depth and IR images - but when these are removed, performance is poor. Further analysis is provided in the <a href="#MOESM1" class="usa-link">Supplementary Information</a> (Supplementary Section <a href="#MOESM1" class="usa-link">4</a>) as a test image is made artificially darker (Supplementary Fig. <a href="#MOESM1" class="usa-link">5</a>). In all cases, only providing the depth image yields extremely poor performance (AP &lt; 0.1), suggesting that fusion models trained on all three modalities learn to rely on the RGB and IR images and not depth.</p>
<p id="Par28">These models have considerably different inference times - HRNet-W32-256 IIF-2 can be run at 30fps for 8 babies simultaneously on a desktop-class GPU, while HRNet-W48-384 LIF-3 would require a server-class GPU for just three babies. In scenarios where depth or infra-red imaging is unavailable, the LIF-3 model does not show as much performance degradation as the IIF-2 model, and its inference time could be greatly reduced by only evaluating branches for the available images. Further training with missing images may allow the model to become more robust and be used regardless of the imaging configuration. The IIF-2 and LIF-3 models therefore fill different use cases and we would suggest continuing to investigate both.</p>
<p id="Par29">Using fusion models increases the model complexity, number of parameters and inference time; the later the fusion stage, the greater the effect. The best performing model - HRNet-W48 LIF-3, achieved an AP score of 0.811 but requires over 140 GFLOPS for inference and contains almost 100M parameters. The need for patient privacy and data security may necessitate smaller models for cot-side inference. The HRFormer-S, IIF-2 model requires 18 GFLOPS and only 9.1M parameters, while achieving AP 0.809, which may present a worthwhile trade-off. The best performing single-image model, HRFormer-B with depth images, achieves an AP of 0.801 with 43.3 GFLOPS and 43.6M parameters. We find that using a smaller backbone in a fusion architecture outperforms or matches a larger backbone in a single-image architecture with faster inference and fewer parameters. The HRFormer-S IIF-2 model is suitable for 1 frame-per-second inference on a small, cot-side PC, or real-time inference on a desktop or server PC. It should be noted that including full-body key-point annotations for 24-hour real data is not yet explored and may require the use of larger backbones. This remains a key limitation of this work.</p>
<p id="Par30">The detection scores are shown to be marginally lower during interventions, and decrease as the baby is more covered. The best performance, with 95% of OKS scores above 0.9, is achieved for uncovered babies without intervention. For fully covered babies, the median OKS score decreases to around 0.86, though the range of scores, as shown in Fig. <a href="#Fig4" class="usa-link">4</a>, increases greatly. We find an overall trend of performance decrease as the baby is more covered, regardless of the backbone or fusion architecture.</p>
<p id="Par31">No conclusion can be drawn on the difference between prone and supine positions; the difference in performance for a given model is not always statistically significant, and the difference is not uniformly positive or negative across models. However, better performance is always seen for prone/supine compared to intervention, and again for intervention compared to the side position. The side position is the most challenging infant position. Despite the side position being more common than the front/back positions, and hence appearing more often in the dataset, it has not caused the models to perform better for babies in this position.</p>
<p id="Par32">In our 24-hour dataset, a baby spends 44% (excluding interventions) of their time on the side, over 50% at least three-quarters covered and 24% fully covered. We have shown these images to be a more challenging, but real NICU scenario. We also see that for non-intervention images, the range of OKS scores increases as the baby is more covered, indicating that a completely failed detection becomes more likely. In summary, the side position and covered infants are common clinical environments that should be included in test datasets for future neonatal computer vision methods.</p>
<p id="Par33">The fusion approaches used here simply sum the features from each separate part of the model. Each part’s weight is effectively the amplitude of the feature, so we rely on the separate branches not having large responses to irrelevant features such as a patterned blanket. Other strategies could use a separate part of the model to weight the branches, either on a per-image or per-feature basis. A model like this would require careful training, as plain gradient calculations would give whichever modality is currently favoured by the weighting scheme an effectively higher learning rate. The current fusion schemes are also symmetric. We could use the depth image, currently the best-performing single-image model, which is unaffected by patterns, within an attention mechanism for the RGB or IR images. Image dropout could also be used to make the LIF-3 model into a single model suitable for scenarios where certain image types are unavailable.</p>
<p id="Par34">We have focussed on a wide range of model architectures and backbones, rather than deep hyperparameter analysis. This would ultimately require a separate validation fold, reducing the available training data. Further work will include more data (from more recordings), annotated with limbs as well as the torso, carefully divided into folds that match the overall distribution of positions and coverings. Once limb key-points have been added, we can also investigate the possibility of reducing the inference time and number of parameters by pruning the model asymmetrically.</p>
<p id="Par35">This work has focussed exclusively on pose estimation through camera imaging from outside the incubator. The pose estimation accuracy decreases as the infant is more covered and when the infant is on their side. One possible avenue for enhancement involves incorporating additional sensors. Other research has already been conducted to derive movements based on an accelerometer<sup><a href="#CR50" class="usa-link" aria-describedby="CR50">50</a></sup> or electromagnetic tracking systems (EMTS)<sup><a href="#CR51" class="usa-link" aria-describedby="CR51">51</a></sup>. It is also possible to use a pressure mat<sup><a href="#CR52" class="usa-link" aria-describedby="CR52">52</a>,<a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>, which can provide additional information when the baby is covered, and can confirm or reject spurious pose estimation results.</p>
<p id="Par36">Improved pose estimation accuracy has significant potential clinical implications for enhancing neonatal care such as providing critical information about an infant when not directly observed by clinical staff. Accurate pose estimation can contribute to the early detection of developmental abnormalities and ongoing monitoring of infant health. These advancements offer valuable insights that can inform clinical decisions and interventions, ultimately improving the standard of care for neonates. This must be considered in the context of the real-world NICU environment; babies are covered for their own comfort, and the supine position is not always ideal for development<sup><a href="#CR54" class="usa-link" aria-describedby="CR54">54</a>,<a href="#CR55" class="usa-link" aria-describedby="CR55">55</a></sup>.</p></section><section id="Sec9"><h2 class="pmc_sec_title">Methods</h2>
<section id="Sec10"><h3 class="pmc_sec_title">Clinical study design</h3>
<p id="Par37">The study was conducted in the Neonatal Intensive Care Unit (NICU) at Addenbrooke’s Hospital, Cambridge, UK, involving 24 preterm neonates in incubators in a single-center study. The study received approval from the North West Preston Research Ethics Committee (reference number 21/NW/0194, IRAS ID 285615) and followed the ethical guidelines of the 1964 Helsinki Declaration. Informed consent was received from each participant’s parent/guardian. Each neonate was recorded for 1 or 24 hours in normal clinical conditions. The incubator environment remained unaltered during the recording. As such, the dataset includes variability in lighting, infant position, and environmental factors. The camera was mounted to the side of the incubator using a flexible arm and placed on top of, and outside, the incubator (Fig. <a href="#Fig7" class="usa-link">7</a>). The camera is kept within 1cm of the surface to prevent reflections from the camera’s infra-red emitter, or ceiling lights.</p>
<figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7. Study recording setup.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/bc80e6197337/41746_2025_1929_Fig7_HTML.jpg" loading="lazy" id="d33e2223" height="338" width="666" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> Experimental setup in the NICU, showing a covered incubator with the camera visible through the hole in the cover. <strong>b</strong> Schematic diagram of the recording setup.</p></figcaption></figure><p id="Par38">A laptop was used to record and synchronize data from an Azure Kinect camera (Microsoft Corporation, USA) and the neonate’s patient monitor (GE Healthcare Technologies Inc., USA). The Azure Kinect camera captured time-synchronized RGB, depth and infrared (IR) video at 30 frames per second. The RGB resolution was 1280 × 720 pixels, and the depth/IR frames were upsampled to match this resolution from a native 640 × 576 pixels during the RGB/IR alignment. The spatial camera alignment uses the camera’s factory calibration parameters to account for the physical separation between the RGB and IR sensors and their image distortion.</p></section><section id="Sec11"><h3 class="pmc_sec_title">Dataset</h3>
<p id="Par39">The dataset contains 10 24-hour recordings and 14 1-hour recordings. The 1-hour recordings were during daylight hours and there were minimal interventions and disruptions during the recording. The 24-hour recordings contained the normal range of lighting, interventions and activity, as nurses and parents were instructed to behave as they normally would. The demographic information for the two datasets are given in Table <a href="#Tab9" class="usa-link">9</a>, including the mean, standard deviation and range for the gestational age (GA) at birth and recording, weight at birth and recording, and sex/ethnicity. The dataset encompasses a wide range of baby sizes, (485-1950g), GA at birth (23+1 to 31+1 wk+d), and GA at recording (24+6 to 34+6 wk+d).</p>
<section class="tw xbox font-sm" id="Tab9"><h4 class="obj_head">Table 9.</h4>
<div class="caption p"><p>Demographic data for neonates included in the study</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th colspan="1" rowspan="1">Description</th>
<th colspan="1" rowspan="1">1-Hour</th>
<th colspan="1" rowspan="1">24-Hour</th>
<th colspan="1" rowspan="1">All</th>
</tr></thead>
<tbody>
<tr>
<td colspan="1" rowspan="1">
<p>GA (birth)</p>
<p>(wk+d)</p>
</td>
<td colspan="1" rowspan="1">
<p>29+0 ± 19d</p>
<p>(24+6,31+1)</p>
</td>
<td colspan="1" rowspan="1">
<p>25+4 ± 15d</p>
<p>(23+1,29+2)</p>
</td>
<td colspan="1" rowspan="1">
<p>27+4 ± 20d</p>
<p>(23+1,31+1)</p>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">
<p>GA (recording)</p>
<p>(wk+d)</p>
</td>
<td colspan="1" rowspan="1">
<p>31+5 ± 15d</p>
<p>(27+2,34+5)</p>
</td>
<td colspan="1" rowspan="1">
<p>29+6 ± 18d</p>
<p>(24+6,34+6)</p>
</td>
<td colspan="1" rowspan="1">
<p>31+0 ± 18d</p>
<p>(24+6,34+6)</p>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Weight (birth) (g)</td>
<td colspan="1" rowspan="1">
<p>1068 ± 497</p>
<p>(485,1950)</p>
</td>
<td colspan="1" rowspan="1">
<p>750 ± 262</p>
<p>(500,1420)</p>
</td>
<td colspan="1" rowspan="1">
<p>936 ± 438</p>
<p>(485,1950)</p>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">Weight (recording) (g)</td>
<td colspan="1" rowspan="1">
<p>1289 ± 463</p>
<p>(610,2130)</p>
</td>
<td colspan="1" rowspan="1">
<p>1088 ± 323</p>
<p>(655,1630)</p>
</td>
<td colspan="1" rowspan="1">
<p>1205 ± 415</p>
<p>(610,2130)</p>
</td>
</tr>
<tr>
<td colspan="1" rowspan="1">
<p>Sex</p>
<p>Ethnicity</p>
</td>
<td colspan="1" rowspan="1">
<p>12M / 2F</p>
<p>10W / 4NW</p>
</td>
<td colspan="1" rowspan="1">
<p>7M / 3F</p>
<p>5W / 5NW</p>
</td>
<td colspan="1" rowspan="1">
<p>19M / 5F</p>
<p>15W / 9NW</p>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p103">
<p>Statistics are divided into the 1-hour study, 24-hour study, and combined statistics. Where appropriate, values are given as mean ± standard deviation, (minimum - maximum). Ethnicity is given as white (W) or non-white (NW).</p>
<p><em>GA</em> gestational age.</p>
</div></div></section></section><section id="Sec12"><h3 class="pmc_sec_title">Ground truth</h3>
<p id="Par40">Our pose estimation algorithm is designed to locate the torso key-points (shoulders and hips) only. This is because the torso position is visible, or its position can be inferred, in almost all images. Face key-points are often not visible, and limbs are often covered by blankets. By using key-points which can be localized in all images, we have a larger dataset and we can compare the performance in different clinical conditions, such as the level of covering of the infant.</p>
<p id="Par41">Due to the similarity of consecutive frames, images were annotated manually in 15-30 second intervals using a self-developed annotation tool. In the 24-hour recordings, the mean absolute difference between depth pixels was taken for each fifteen-minute window, and 30 frames were selected so as to be evenly spaced in cumulative depth difference. As such, frames featuring greater activity were prioritised. Annotators selected the position of the shoulders and hips, where they are visible or where they can be inferred. Inferred positions might indicate that although the hip is covered by a blanket, the video may be skipped to a point where the blanket is removed, provided that there is no motion of the baby in that period. Key-points were labelled as visible if they were either clearly visible or under a blanket, occluded if covered by any other object (such as a nurse’s hand), and not visible otherwise. Annotators included clinicians and staff with clinical experience, and multiple annotators reviewed each annotation.</p>
<p id="Par42">Frames were also annotated with the infant’s position (prone, supine, or on their side), intervention (yes, no), and level of covering (none, half, three-quarter, full). Half covering indicates that the lower body was covered (or a similar amount, for example across the stomach). Three quarter covering indicates that the lower body and a significant part of the torso was covered, but the shoulders are clearly visible. Full covering indicates that the shoulders are only just visible, if at all.</p></section><section id="Sec13"><h3 class="pmc_sec_title">Pose estimation models</h3>
<p id="Par43">For pose estimation, we use a top-down heatmap approach. As we know each frame contains exactly one baby, we do not need a joint connection method such as associative embedding. The model is built using an HRNet, HRFormer or ViTPose backbone, followed by a key-point head. These models are chosen due their recent state-of-the-art results in adult data, specifically ViTPose and derivatives of HRNet. The models described subsequently are designed to process different image types for the same task. We therefore anticipate that there will be merit to having separate, image type-specific early layers and shared later layers, as we are looking for the same features in each image but the appearance of the features in each image type is different. The HRNet and HRFormer backbones contain input layers followed by four stages. We consider possible separate/shared divisions after each stage; if the division is after the fourth stage, then only the key-point head is shared. The division point is denoted <em>X</em>, which can be between 1 and 4. For the Vision Transformer, we determine each stage to be a quarter of the transformer layers, with the patch embedding included in the first stage. We denote the image-specific early stages as <em>Part 1</em> of the model and the shared later stages as <em>Part 2</em>. We implement four categories of models to process the different image types.</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par44"><em>Single-Image models</em> trained and evaluated using a single image type, providing reference performance scores.</p></li>
<li><p id="Par45"><em>Early Image Fusion Models (EIF)</em>. These models concatentate the input images into a single image with up to five color channels (RGB-D-IR), or a combination thereof. This architecture is illustrated in Fig. <a href="#Fig8" class="usa-link">8</a>.</p></li>
<li><p id="Par46"><em>Intermediate Image Fusion Models (IIF-X)</em>. These models have <em>X</em> separate stages, at which point the features are summed before being input to a single set of later stages. If the separate/shared division is after the fourth stage (IIF-4), then the summation is <em>before</em> the key-point head. This architecture is illustrated in Fig. <a href="#Fig9" class="usa-link">9</a>.</p></li>
<li><p id="Par47"><em>Late Image Fusion Models (LIF-X)</em>. These models have <em>X</em> separate stages, followed by 4 − <em>X</em> shared stages to produce a set of heatmaps for each image. The heatmaps are summed (after the key-point head) to produce the overall prediction. These models are the most computationally expensive as they effectively require three models to be evaluated for each frame. This architecture is illustrated in Fig. <a href="#Fig10" class="usa-link">10</a>.</p></li>
</ul>
<figure class="fig xbox font-sm" id="Fig8"><h4 class="obj_head">Fig. 8. Illustration of the early image fusion EIF-RGB-D-IR model.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig8_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/f23168c6a052/41746_2025_1929_Fig8_HTML.jpg" loading="lazy" id="d33e2365" height="166" width="683" alt="Fig. 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Images are concatenated along the channel dimension, producing a 5-channel image in the example shown. The very first layer of the backbone is modified to accommodate the different number of channels, and the model proceeds as normal. For other EIF variants (e.g. RGB-D), the number of channels produced would be different.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig9"><h4 class="obj_head">Fig. 9. Illustration of intermediate image fusion IIF-X models.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig9_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/31793ae540a9/41746_2025_1929_Fig9_HTML.jpg" loading="lazy" id="d33e2387" height="159" width="683" alt="Fig. 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Each image is processed through an image type-specific Part 1 with X stages. The resulting feature maps are summed, and processed through Part 2 to give a single prediction.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig10"><h4 class="obj_head">Fig. 10. Illustration of late image fusion LIF-X models.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12373853_41746_2025_1929_Fig10_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/6a88/12373853/cf9af55feac5/41746_2025_1929_Fig10_HTML.jpg" loading="lazy" id="d33e2409" height="170" width="683" alt="Fig. 10"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig10/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Each image is processed through an image-type specific Part 1, followed by a shared Part 2 and the resulting heatmaps are summed.</p></figcaption></figure><p id="Par48">The models are evaluated for <em>X</em> from 1 to 4. We use HRNet backbone configurations W32 and W48 with input image sizes 256 × 256 and 384 × 384. The HRFormer backbone is evaluated at input size 384 × 384 in small and base configurations. The Vision Transformer backbone is evaluated using small and base (S, B) configurations with 256 × 192 image size.</p></section><section id="Sec14"><h3 class="pmc_sec_title">Evaluation metrics</h3>
<p id="Par49">The COCO average precision (AP)<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>, percentage of correct key-points based on head size (PCKh)<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a>,<a href="#CR56" class="usa-link" aria-describedby="CR56">56</a></sup>, and RMS position error are commonly used in the literature for evaluating pose estimation algorithms. We choose to primarily use the AP metric in this work, for the following reasons: we only use four key-points and the AP metric is suitable as it includes a per-key-point constant that accounts for each key-point’s different localisation difficulty; the RMS error is dependent on the image size, which varies across datasets; PCKh is based on the head size of the subject, which does not account for differences in neonatal body proportions, and the difficulty in detecting the different key-points. We also compute the AP metric with/without interventions and different blanket coverage. In the AP metric, each frame <em>k</em> is given an object key-point similarity score (<em>O</em><em>K</em><em>S</em><sub><em>k</em></sub>), defined in Equation (<a href="#Equ1" class="usa-link">1</a>) where <em>δ</em><sub><em>i</em><em>k</em></sub> is the visibility of key-point <em>i</em> in frame <em>k</em> (1 if visible, 0 otherwise), <em>d</em><sub><em>i</em><em>k</em></sub> is the distance of detected key-point <em>i</em> in frame <em>k</em> from its ground truth, <em>κ</em> is a per-key-point constant reflecting the detection difficulty of each key-point type, and <em>s</em><sub><em>k</em></sub> is a scale parameter for frame <em>k</em>.</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><math id="d33e2495" display="block"><mrow><mi>O</mi><mi>K</mi><msub><mrow><mi>S</mi></mrow><mrow><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mrow><msubsup><mrow><mo>∑</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mn>4</mn></mrow></msubsup><msub><mrow><mi>δ</mi></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mi>exp</mi><mrow><mo>(</mo><mrow><mo>−</mo><msubsup><mrow><mi>d</mi></mrow><mrow><mi>i</mi><mi>k</mi></mrow><mrow><mn>2</mn></mrow></msubsup><mo>/</mo><mrow><mo>(</mo><mrow><mn>2</mn><msubsup><mrow><mi>κ</mi></mrow><mrow><mi>i</mi></mrow><mrow><mn>2</mn></mrow></msubsup><msubsup><mrow><mi>s</mi></mrow><mrow><mi>k</mi></mrow><mrow><mn>2</mn></mrow></msubsup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mrow><mo>∑</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mn>4</mn></mrow></msubsup><msub><mrow><mi>δ</mi></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></mfrac></mrow></math></td>
<td class="label">1</td>
</tr></table>
<p>Note that detection <em>k</em> is equivalent to frame <em>k</em> in this work as each frame contains exactly one person. The scale parameter in the COCO dataset is given by the square root of the person’s area. In the absence of the area parameter, the xtcocotools Python<sup><a href="#CR57" class="usa-link" aria-describedby="CR57">57</a></sup> package uses 0.53 × the area of the rectangular bounding box around the person - this factor is approximately the median ratio of person area to bounding box in the dataset. As our dataset does not contain a labelled outline of the baby - this would be impossible due to the blanket - we use the bounding box around the visible key-points to determine the scale parameter, then apply multiplication factors based on the median ratio in the COCO dataset, as follows, depending on key-point visibility (each item’s percentage of our dataset in parenthesis):</p>
<ol class="list" style="list-style-type:decimal">
<li><p id="Par50">3-4 key-points, or 2 opposing corners: use <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><math id="d33e2595"><mrow><msubsup><mrow><mi>s</mi></mrow><mrow><mi>k</mi></mrow><mrow><mn>2</mn></mrow></msubsup><mo>=</mo><mn>3.28</mn><mo>×</mo></mrow></math></span> the torso bounding box area (95.4%).</p></li>
<li><p id="Par51">Only left, or only right key-points: use <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><math id="d33e2619"><mrow><msubsup><mrow><mi>s</mi></mrow><mrow><mi>k</mi></mrow><mrow><mn>2</mn></mrow></msubsup><mo>=</mo><mn>2.0</mn><mo>×</mo></mrow></math></span> torso height squared (0.9%).</p></li>
<li><p id="Par52">Only shoulders: use <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><math id="d33e2643"><mrow><msubsup><mrow><mi>s</mi></mrow><mrow><mi>k</mi></mrow><mrow><mn>2</mn></mrow></msubsup><mo>=</mo><mn>6.16</mn><mo>×</mo></mrow></math></span> shoulder width squared (3.0%).</p></li>
<li><p id="Par53">Only hips: use <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><math id="d33e2667"><mrow><msubsup><mrow><mi>s</mi></mrow><mrow><mi>k</mi></mrow><mrow><mn>2</mn></mrow></msubsup><mo>=</mo><mn>13.64</mn><mo>×</mo></mrow></math></span> hip width squared (0.7%).</p></li>
<li><p id="Par54">If one or zero key-points are visible, the scale cannot be found and the image is not evaluated.</p></li>
</ol></section><section id="Sec15"><h3 class="pmc_sec_title">Training</h3>
<p id="Par55">We employ transfer learning due to the small size of our dataset in comparison with COCO. Pre-trained models are obtained from the MMPose model zoo<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>. Certain weights are adjusted for this application: only indices 5, 6, 11 and 12, corresponding to the shoulders and hips, are used in the key-point head to produce 4 heatmaps instead of 17. For single channel (depth/IR) images, the weights of the initial convolutional layer are summed along the image channel dimension. For EIF models, these weights are stacked along the image channel dimension so that the number of input channels matches the input image. In models with separate stages for each image type, each is initialized with the same initial weights. Before training on our dataset, each pre-trained model was confirmed to produce correct outputs when provided with an image of an adult, with a grayscale version used as a substitute for depth/IR images.</p>
<p id="Par56">Models are trained for up to 50 epochs using the Adam optimizer method with weight decay<sup><a href="#CR58" class="usa-link" aria-describedby="CR58">58</a></sup> and learning rate 0.0005. The dataset is divided into five folds for cross-validation, and each model is tested on each held-out fold. Each baby’s recording is assigned to a particular fold. Our experiments are implemented in Python (version 3.10) using libraries PyTorch (version 2.3), CUDA (version 12.1), and MMPose (forked from version 1.3.1).</p></section><section id="Sec16"><h3 class="pmc_sec_title">Cross-dataset evaluation</h3>
<p id="Par57">We compare our results to existing work, namely the SyRIP<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup> and BabyPose<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup> datasets. However, lack of availability of trained models or images, due to participant privacy, makes direct comparison impossible. No other dataset includes all three image types, hence we cannot compare our fusion models. In addition, SyRIP contains images of babies significantly older than those in our dataset, making them not directly comparable. Instead we analyse cross-dataset performance, using models trained on either COCO (adult) or our dataset, and tested on the SyRIP and BabyPose datasets.</p>
<p id="Par58">Models trained on COCO and their respective evaluation scores are obtained from the MMPose model zoo<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>. These models are converted to grayscale by summing the first convolution kernel along the channel axis, and retaining the axis in the tensor. This is equivalent to converting the image to three-channel grayscale inference. We convert the SyRIP and BabyPose datasets to the COCO format, with only the torso annotated. Images where the torso is not at all visible are excluded. We then evaluate each model on each other dataset. For comparison with the BabyPose dataset, we also evaluate the root-mean-square error (RMSE). We scale the errors to be that of a 128 × 96 image, as used in the original work.</p></section></section><section id="Sec17"><h2 class="pmc_sec_title">Supplementary information</h2>
<section class="sm xbox font-sm" id="MOESM1"><div class="media p"><div class="caption">
<a href="/articles/instance/12373853/bin/41746_2025_1929_MOESM1_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Supplementary information</a><sup> (1.5MB, pdf) </sup>
</div></div></section></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>We are grateful to all participants, and their parents, for taking part in the study. We also thank the staff on the neonatal unit for their support in undertaking the study in the busy NICU. This work is funded by the Rosetrees Trust, Stoneygate Trust, and Isaac Newton Trust, with funding support from the Cambridge Centre for Data-Driven Discovery and Accelerate Programme for Scientific Discovery, made possible by a donation from Schmidt Futures, and Trinity College, University of Cambridge. This work was performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (<a href="http://www.csd3.cam.ac.uk" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">www.csd3.cam.ac.uk</a>), provided by Dell EMC and Intel using Tier-2 funding from the Engineering and Physical Sciences Research Council (capital grant EP/T022159/1), and DiRAC funding from the Science and Technology Facilities Council (<a href="http://www.dirac.ac.uk" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">www.dirac.ac.uk</a>).</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>A.G. developed the software for clinical data collection. A.G. and J.M.W. conceptualised and implemented the fusion models and analysed the clinical data. M.L. and E.H. conducted the testing and training of the models and analysed the results. A.G., J.M.W., M.L., E.H., L.T. and K.B. conducted the data annotation, labelling and review. The clinical study was designed by A.G., L.T., K.B. and J.L. L.T. recruited participants. A.G. and L.T. conducted the data collection. The overall project conceptualisation, design and supervision was by J.L. (Engineering) and K.B. (paediatrics). J.M.W. and A.G. wrote the main manuscript text. All authors reviewed the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The datasets generated and/or analysed during the current study are not publicly available due to the sensitive and identifiable nature of the data. The data may be made available for academic purposes under a controlled data sharing agreement.</p></section><section id="notes3"><h2 class="pmc_sec_title">Code availability</h2>
<p>The underlying code for this study is not publicly available but may be made available to qualified researchers on reasonable request from the corresponding author.</p></section><section id="FPar1"><h2 class="pmc_sec_title">Competing interests</h2>
<p id="Par59">The authors declare no competing interests.</p></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1"><p><strong>Publisher’s note</strong> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section id="sec18"><h2 class="pmc_sec_title">Supplementary information</h2>
<p>The online version contains supplementary material available at 10.1038/s41746-025-01929-z.</p></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Lee, H. C., Bardach, N. S., Maselli, J. H. &amp; Gonzales, R. Emergency department visits in the neonatal period in the united states. <em>Pediatr. Emerg. Care</em><strong>30</strong>, 315–318 (2014).
</cite> [<a href="https://doi.org/10.1097/PEC.0000000000000120" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4160308/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24759490/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Lee,%20H.%20C.,%20Bardach,%20N.%20S.,%20Maselli,%20J.%20H.%20&amp;%20Gonzales,%20R.%20Emergency%20department%20visits%20in%20the%20neonatal%20period%20in%20the%20united%20states.%20Pediatr.%20Emerg.%20Care30,%20315%E2%80%93318%20(2014)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Hannan, K. E., Hwang, S. S. &amp; Bourque, S. L. Readmissions among nicu graduates: Who, when and why? <em>Semin. Perinatol.</em><strong>44</strong>, 151245 (2020).
</cite> [<a href="https://doi.org/10.1016/j.semperi.2020.151245" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32253024/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Hannan,%20K.%20E.,%20Hwang,%20S.%20S.%20&amp;%20Bourque,%20S.%20L.%20Readmissions%20among%20nicu%20graduates:%20Who,%20when%20and%20why?%20Semin.%20Perinatol.44,%20151245%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Ray, K. N. &amp; Lorch, S. A. Hospitalization of early preterm, late preterm, and term infants during the first year of life by gestational age. <em>Hospital Pediatrics</em><strong>3</strong>, 194–203 (2013).
</cite> [<a href="https://doi.org/10.1542/hpeds.2012-0063" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24313087/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ray,%20K.%20N.%20&amp;%20Lorch,%20S.%20A.%20Hospitalization%20of%20early%20preterm,%20late%20preterm,%20and%20term%20infants%20during%20the%20first%20year%20of%20life%20by%20gestational%20age.%20Hospital%20Pediatrics3,%20194%E2%80%93203%20(2013)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR4">
<span class="label">4.</span><cite>Wang, M. L., Dorer, D. J., Fleming, M. P. &amp; Catlin, E. A. Clinical outcomes of near-term infants. <em>Pediatrics</em><strong>114</strong>, 372–376 (2004).
</cite> [<a href="https://doi.org/10.1542/peds.114.2.372" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/15286219/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wang,%20M.%20L.,%20Dorer,%20D.%20J.,%20Fleming,%20M.%20P.%20&amp;%20Catlin,%20E.%20A.%20Clinical%20outcomes%20of%20near-term%20infants.%20Pediatrics114,%20372%E2%80%93376%20(2004)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Organization, W. H. Newborn mortality [Internet]. Genève: World Health Organization [accessed 14 May 2024]; Available from: <a href="https://www.who.int/news-room/fact-sheets/detail/newborn-mortality" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.who.int/news-room/fact-sheets/detail/newborn-mortality</a>.</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>Newborn mortality [Internet]. London: National Institute for Health and Care Excellence [cited 2024 May 14]; [about screen 1]. Available from: <a href="https://www.nice.org.uk/about/what-we-do/into-practice/measuring-the-use-of-nice-guidance/impact-of-our-guidance/niceimpact-maternity-neonatal-care" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.nice.org.uk/about/what-we-do/into-practice/measuring-the-use-of-nice-guidance/impact-of-our-guidance/niceimpact-maternity-neonatal-care</a>.</cite>
</li>
<li id="CR7">
<span class="label">7.</span><cite>Keles, E. &amp; Bagci, U. The past, current, and future of neonatal intensive care units with artificial intelligence: a systematic review. <em>npj Digital Med.</em><strong>6</strong>, 220 (2023).</cite> [<a href="https://doi.org/10.1038/s41746-023-00941-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10682088/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38012349/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Keles,%20E.%20&amp;%20Bagci,%20U.%20The%20past,%20current,%20and%20future%20of%20neonatal%20intensive%20care%20units%20with%20artificial%20intelligence:%20a%20systematic%20review.%20npj%20Digital%20Med.6,%20220%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>Parry, S., Ranson, P. &amp; Tandy, S. Positioning and handling guideline <a href="https://www.neonatalnetwork.co.uk/nwnodn/wp-content/uploads/2023/11/Positioning-and-Handling-Guideline-2023-Final-1.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.neonatalnetwork.co.uk/nwnodn/wp-content/uploads/2023/11/Positioning-and-Handling-Guideline-2023-Final-1.pdf</a> (2023).</cite>
</li>
<li id="CR9">
<span class="label">9.</span><cite>Trust, R. C. H. N. Developmental positioning on the neonatal unit clinical guideline <a href="https://doclibrary-rcht.cornwall.nhs.uk/GET/d10366683&amp;ved=2ahUKEwii7pfAmsSOAxUgQUEAHQ7zA-4QFnoECBgQAQ&amp;usg=AOvVaw3mfhvvYSXyrEcv5ghWAXq8" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://doclibrary-rcht.cornwall.nhs.uk/GET/d10366683&amp;ved=2ahUKEwii7pfAmsSOAxUgQUEAHQ7zA-4QFnoECBgQAQ&amp;usg=AOvVaw3mfhvvYSXyrEcv5ghWAXq8</a> (2024).</cite>
</li>
<li id="CR10">
<span class="label">10.</span><cite>Villarroel, M. et al. Non-contact physiological monitoring of preterm infants in the neonatal intensive care unit. <em>NPJ Digital Med icine</em><strong>2</strong>, 128 (2019).</cite> [<a href="https://doi.org/10.1038/s41746-019-0199-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6908711/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31872068/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Villarroel,%20M.%20et%20al.%20Non-contact%20physiological%20monitoring%20of%20preterm%20infants%20in%20the%20neonatal%20intensive%20care%20unit.%20NPJ%20Digital%20Med%20icine2,%20128%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>Ruhrberg Estévez, S. et al. Continuous non-contact vital sign monitoring of neonates in intensive care units using rgb-d cameras. <em>Sci. Rep.</em><strong>15</strong>, 16863 (2025).
</cite> [<a href="https://doi.org/10.1038/s41598-025-00539-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC12081704/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40374723/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ruhrberg%20Est%C3%A9vez,%20S.%20et%20al.%20Continuous%20non-contact%20vital%20sign%20monitoring%20of%20neonates%20in%20intensive%20care%20units%20using%20rgb-d%20cameras.%20Sci.%20Rep.15,%2016863%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Cenci, A., Liciotti, D., Frontoni, E. &amp; Mancini, A. Non-contact monitoring of preterm infants using rgb-d camera (2015).</cite>
</li>
<li id="CR13">
<span class="label">13.</span><cite>Gibson, K. et al. Non-contact heart and respiratory rate monitoring of preterm infants based on a computer vision system: a method comparison study. <em>Pediatr. Res.</em><strong>86</strong>, 738–741 (2019).
</cite> [<a href="https://doi.org/10.1038/s41390-019-0506-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31351437/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Gibson,%20K.%20et%20al.%20Non-contact%20heart%20and%20respiratory%20rate%20monitoring%20of%20preterm%20infants%20based%20on%20a%20computer%20vision%20system:%20a%20method%20comparison%20study.%20Pediatr.%20Res.86,%20738%E2%80%93741%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Cobos-Torres, J.-C., Abderrahim, M. &amp; Martínez-Orgado, J. Non-contact, simple neonatal monitoring by photoplethysmography. <em>Sensors</em><strong>18</strong><a href="https://www.mdpi.com/1424-8220/18/12/4362" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.mdpi.com/1424-8220/18/12/4362</a> (2018).</cite> [<a href="https://doi.org/10.3390/s18124362" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6308706/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30544689/" class="usa-link">PubMed</a>]</li>
<li id="CR15">
<span class="label">15.</span><cite>Cattani, L. et al. Monitoring infants by automatic video processing: A unified approach to motion analysis. <em>Computers Biol. Med.</em><strong>80</strong>, 158–165 (2017).</cite> [<a href="https://doi.org/10.1016/j.compbiomed.2016.11.010" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27940321/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Cattani,%20L.%20et%20al.%20Monitoring%20infants%20by%20automatic%20video%20processing:%20A%20unified%20approach%20to%20motion%20analysis.%20Computers%20Biol.%20Med.80,%20158%E2%80%93165%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>Olmi, B., Frassineti, L., Lanata, A. &amp; Manfredi, C. Automatic detection of epileptic seizures in neonatal intensive care units through eeg, ecg and video recordings: A survey. <em>IEEE Access</em><strong>9</strong>, 138174–138191 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Olmi,%20B.,%20Frassineti,%20L.,%20Lanata,%20A.%20&amp;%20Manfredi,%20C.%20Automatic%20detection%20of%20epileptic%20seizures%20in%20neonatal%20intensive%20care%20units%20through%20eeg,%20ecg%20and%20video%20recordings:%20A%20survey.%20IEEE%20Access9,%20138174%E2%80%93138191%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR17">
<span class="label">17.</span><cite>Plouin, P. &amp; Kaminska, A. Chapter 51 - neonatal seizures. In Dulac, O., Lassonde, M. &amp; Sarnat, H. B. (eds.) <em>Pediatric Neurology Part I</em>, vol. 111 of <em>Handbook of Clinical Neurology</em>, 467–476 (Elsevier, Amsterdam, 2013).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>Schroeder, A. S. et al. General movement assessment from videos of computed 3d infant body models is equally effective compared to conventional rgb video rating. <em>Early Hum. Dev.</em><strong>144</strong>, 104967 (2020).
</cite> [<a href="https://doi.org/10.1016/j.earlhumdev.2020.104967" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32304982/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Schroeder,%20A.%20S.%20et%20al.%20General%20movement%20assessment%20from%20videos%20of%20computed%203d%20infant%20body%20models%20is%20equally%20effective%20compared%20to%20conventional%20rgb%20video%20rating.%20Early%20Hum.%20Dev.144,%20104967%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19">
<span class="label">19.</span><cite>Marchi, V. et al. Automated pose estimation captures key aspects of general movements at eight to 17 weeks from conventional videos. <em>Acta Paediatrica</em><strong>108</strong>, 1817–1824 (2019).
</cite> [<a href="https://doi.org/10.1111/apa.14781" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30883894/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Marchi,%20V.%20et%20al.%20Automated%20pose%20estimation%20captures%20key%20aspects%20of%20general%20movements%20at%20eight%20to%2017%20weeks%20from%20conventional%20videos.%20Acta%20Paediatrica108,%201817%E2%80%931824%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Gleason, A. et al. Accurate prediction of neurologic changes in critically ill infants using pose ai. <em>medRxiv</em><a href="https://www.medrxiv.org/content/early/2024/06/10/2024.04.17.24305953.full.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.medrxiv.org/content/early/2024/06/10/2024.04.17.24305953.full.pdf</a> (2024).</cite>
</li>
<li id="CR21">
<span class="label">21.</span><cite>Munea, T. L. et al. The progress of human pose estimation: A survey and taxonomy of models applied in 2d human pose estimation. <em>IEEE Access</em><strong>8</strong>, 133330–133348 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Munea,%20T.%20L.%20et%20al.%20The%20progress%20of%20human%20pose%20estimation:%20A%20survey%20and%20taxonomy%20of%20models%20applied%20in%202d%20human%20pose%20estimation.%20IEEE%20Access8,%20133330%E2%80%93133348%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Wang, J. et al. Deep 3d human pose estimation: A review. <em>Computer Vis. Image Underst.</em><strong>210</strong>, 103225 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20J.%20et%20al.%20Deep%203d%20human%20pose%20estimation:%20A%20review.%20Computer%20Vis.%20Image%20Underst.210,%20103225%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR23">
<span class="label">23.</span><cite>Gamra, M. B. &amp; Akhloufi, M. A. A review of deep learning techniques for 2d and 3d human pose estimation. <em>Image Vis. Comput.</em><strong>114</strong>, 104282 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Gamra,%20M.%20B.%20&amp;%20Akhloufi,%20M.%20A.%20A%20review%20of%20deep%20learning%20techniques%20for%202d%20and%203d%20human%20pose%20estimation.%20Image%20Vis.%20Comput.114,%20104282%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Zheng, C. et al. Deep learning-based human pose estimation: A survey. <em>ACM Comput. Surv.</em><strong>56</strong>, 1–37 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zheng,%20C.%20et%20al.%20Deep%20learning-based%20human%20pose%20estimation:%20A%20survey.%20ACM%20Comput.%20Surv.56,%201%E2%80%9337%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR25">
<span class="label">25.</span><cite>Andriluka, M., Pishchulin, L., Gehler, P. &amp; Schiele, B. 2d human pose estimation: New benchmark and state of the art analysis. In <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> (2014).</cite>
</li>
<li id="CR26">
<span class="label">26.</span><cite>Lin, T.-Y. et al. Microsoft coco: Common objects in context. In Fleet, D., Pajdla, T., Schiele, B. &amp; Tuytelaars, T. (eds.) <em>Computer Vision – ECCV 2014</em>, 740–755 (Springer International Publishing, Cham, 2014).</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Wang, J. et al. Deep High-Resolution Representation Learning for Visual Recognition. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em><strong>43</strong>, 3349–3364 (2021).
</cite> [<a href="https://doi.org/10.1109/TPAMI.2020.2983686" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32248092/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wang,%20J.%20et%20al.%20Deep%20High-Resolution%20Representation%20Learning%20for%20Visual%20Recognition.%20IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.43,%203349%E2%80%933364%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Sun, K., Xiao, B., Liu, D. &amp; Wang, J. Deep high-resolution representation learning for human pose estimation. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 5693–5703 (2019).</cite>
</li>
<li id="CR29">
<span class="label">29.</span><cite>Zhang, F., Zhu, X., Dai, H., Ye, M. &amp; Zhu, C. Distribution-aware coordinate representation for human pose estimation. In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 7091–7100 (2020).</cite>
</li>
<li id="CR30">
<span class="label">30.</span><cite>Liu, H., Liu, F., Fan, X. &amp; Huang, D. Polarized self-attention: Towards high-quality pixel-wise mapping. <em>Neurocomput</em><strong>506</strong>, 158–167 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20H.,%20Liu,%20F.,%20Fan,%20X.%20&amp;%20Huang,%20D.%20Polarized%20self-attention:%20Towards%20high-quality%20pixel-wise%20mapping.%20Neurocomput506,%20158%E2%80%93167%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Zhang, J., Chen, Z. &amp; Tao, D. Towards high performance human keypoint detection. <em>Int. J. Comput. Vis.</em><strong>129</strong>, 2639–2662 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20J.,%20Chen,%20Z.%20&amp;%20Tao,%20D.%20Towards%20high%20performance%20human%20keypoint%20detection.%20Int.%20J.%20Comput.%20Vis.129,%202639%E2%80%932662%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Xu, Y., Zhang, J., Zhang, Q. &amp; Tao, D. Vitpose: simple vision transformer baselines for human pose estimation. In <em>Proceedings of the 36th International Conference on Neural Information Processing Systems</em>, NIPS ’22 (Curran Associates Inc., Red Hook, NY, USA, 2024).</cite>
</li>
<li id="CR33">
<span class="label">33.</span><cite>Liu, Z. et al. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In <em>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 9992–10002 (IEEE Computer Society, Los Alamitos, CA, USA, 2021). 10.1109/ICCV48922.2021.00986.</cite>
</li>
<li id="CR34">
<span class="label">34.</span><cite>Xiong, Z., Wang, C., Li, Y., Luo, Y. &amp; Cao, Y. Swin-Pose: Swin Transformer Based Human Pose Estimation. In <em>2022 IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR)</em>, 228–233 (IEEE Computer Society, Los Alamitos, CA, USA, 2022). 10.1109/MIPR54900.2022.00048.</cite>
</li>
<li id="CR35">
<span class="label">35.</span><cite>Yuan, Y. et al. Hrformer: high-resolution transformer for dense prediction. In <em>Proceedings of the 35th International Conference on Neural Information Processing Systems</em>, NIPS ’21 (Curran Associates Inc., Red Hook, NY, USA, 2021).</cite>
</li>
<li id="CR36">
<span class="label">36.</span><cite>Contributors, M. Openmmlab pose estimation toolbox and benchmark. <a href="https://github.com/open-mmlab/mmpose" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/open-mmlab/mmpose</a> (2020).</cite>
</li>
<li id="CR37">
<span class="label">37.</span><cite>Huang, X., Fu, N., Liu, S. &amp; Ostadabbas, S. Invariant representation learning for infant pose estimation with small data. In <em>2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</em>, 1–8 (IEEE, 2021).</cite>
</li>
<li id="CR38">
<span class="label">38.</span><cite>Hesse, N. et al. Computer vision for medical infant motion analysis: State of the art and rgb-d data set. In <em>Proceedings of the European Conference on Computer Vision (ECCV) Workshops</em>, 0–0 (2018).</cite>
</li>
<li id="CR39">
<span class="label">39.</span><cite>Groos, D., Adde, L., Støen, R., Ramampiaro, H. &amp; Ihlen, E. A. Towards human-level performance on automatic pose estimation of infant spontaneous movements. <em>Computerized Med. Imaging Graph.</em><strong>95</strong>, 102012 (2022).</cite> [<a href="https://doi.org/10.1016/j.compmedimag.2021.102012" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34864580/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Groos,%20D.,%20Adde,%20L.,%20St%C3%B8en,%20R.,%20Ramampiaro,%20H.%20&amp;%20Ihlen,%20E.%20A.%20Towards%20human-level%20performance%20on%20automatic%20pose%20estimation%20of%20infant%20spontaneous%20movements.%20Computerized%20Med.%20Imaging%20Graph.95,%20102012%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Cao, Z., Simon, T., Wei, S.-E. &amp; Sheikh, Y. Realtime multi-person 2d pose estimation using part affinity fields. In <em>CVPR</em> (2017).</cite> [<a href="https://doi.org/10.1109/TPAMI.2019.2929257" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31331883/" class="usa-link">PubMed</a>]</li>
<li id="CR41">
<span class="label">41.</span><cite>Moccia, S., Migliorelli, L., Pietrini, R. &amp; Frontoni, E. Preterm infants’ limb-pose estimation from depth images using convolutional neural networks. In <em>2019 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)</em>, 1–7 (2019).</cite>
</li>
<li id="CR42">
<span class="label">42.</span><cite>Moccia, S., Migliorelli, L., Carnielli, V. &amp; Frontoni, E. Preterm infants’ pose estimation with spatio-temporal features. <em>IEEE Trans. Biomed. Eng.</em><strong>67</strong>, 2370–2380 (2020).
</cite> [<a href="https://doi.org/10.1109/TBME.2019.2961448" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31870974/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Moccia,%20S.,%20Migliorelli,%20L.,%20Carnielli,%20V.%20&amp;%20Frontoni,%20E.%20Preterm%20infants%E2%80%99%20pose%20estimation%20with%20spatio-temporal%20features.%20IEEE%20Trans.%20Biomed.%20Eng.67,%202370%E2%80%932380%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR43">
<span class="label">43.</span><cite>Cao, X. et al. Aggpose: Deep aggregation vision transformer for infant pose estimation. In Raedt, L. D. (ed.) Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, 5045–5051 (International Joint Conferences on Artificial Intelligence Organization, Vienna, Austria, 2022). 10.24963/ijcai.2022/700.</cite>
</li>
<li id="CR44">
<span class="label">44.</span><cite>Soualmi, A., Ducottet, C., Patural, H., Giraud, A. &amp; Alata, O. A 3D pose estimation framework for preterm infants hospitalized in the Neonatal Unit. <em>Multimed. Tools Appl.</em><strong>83</strong>, 24383–24400 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Soualmi,%20A.,%20Ducottet,%20C.,%20Patural,%20H.,%20Giraud,%20A.%20&amp;%20Alata,%20O.%20A%203D%20pose%20estimation%20framework%20for%20preterm%20infants%20hospitalized%20in%20the%20Neonatal%20Unit.%20Multimed.%20Tools%20Appl.83,%2024383%E2%80%9324400%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR45">
<span class="label">45.</span><cite>Yin, W. et al. A self-supervised spatio-temporal attention network for video-based 3d infant pose estimation. <em>Med. Image Anal.</em><strong>96</strong>, 103208 (2024).
</cite> [<a href="https://doi.org/10.1016/j.media.2024.103208" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38788327/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Yin,%20W.%20et%20al.%20A%20self-supervised%20spatio-temporal%20attention%20network%20for%20video-based%203d%20infant%20pose%20estimation.%20Med.%20Image%20Anal.96,%20103208%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR46">
<span class="label">46.</span><cite>Jahn, L. et al. Comparison of marker-less 2d image-based methods for infant pose estimation <a href="https://arxiv.org/abs/2410.04980" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2410.04980</a> (2024).</cite> [<a href="https://doi.org/10.1038/s41598-025-96206-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11982382/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40204781/" class="usa-link">PubMed</a>]</li>
<li id="CR47">
<span class="label">47.</span><cite>Migliorelli, L., Moccia, S., Pietrini, R., Carnielli, V. P. &amp; Frontoni, E. The babypose dataset. <em>Data Brief.</em><strong>33</strong>, 106329 (2020).
</cite> [<a href="https://doi.org/10.1016/j.dib.2020.106329" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7551984/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33083503/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Migliorelli,%20L.,%20Moccia,%20S.,%20Pietrini,%20R.,%20Carnielli,%20V.%20P.%20&amp;%20Frontoni,%20E.%20The%20babypose%20dataset.%20Data%20Brief.33,%20106329%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR48">
<span class="label">48.</span><cite>Assa, A. &amp; Janabi-Sharifi, F. A robust vision-based sensor fusion approach for real-time pose estimation. <em>IEEE Trans. Cybern.</em><strong>44</strong>, 217–227 (2013).</cite> [<a href="https://doi.org/10.1109/TCYB.2013.2252339" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/23757545/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Assa,%20A.%20&amp;%20Janabi-Sharifi,%20F.%20A%20robust%20vision-based%20sensor%20fusion%20approach%20for%20real-time%20pose%20estimation.%20IEEE%20Trans.%20Cybern.44,%20217%E2%80%93227%20(2013)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR49">
<span class="label">49.</span><cite>Lyra, S. et al. Camera fusion for real-time temperature monitoring of neonates using deep learning. <em>Med. Biol. Eng. Comput.</em><strong>60</strong>, 1787–1800 (2022).
</cite> [<a href="https://doi.org/10.1007/s11517-022-02561-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9079037/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35505175/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Lyra,%20S.%20et%20al.%20Camera%20fusion%20for%20real-time%20temperature%20monitoring%20of%20neonates%20using%20deep%20learning.%20Med.%20Biol.%20Eng.%20Comput.60,%201787%E2%80%931800%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR50">
<span class="label">50.</span><cite>Gravem, D. et al. Assessment of infant movement with a compact wireless accelerometer system (2012).</cite>
</li>
<li id="CR51">
<span class="label">51.</span><cite>Karch, D. et al. Quantitative score for the evaluation of kinematic recordings in neuropediatric diagnostics. <em>Methods Inf. Med.</em><strong>49</strong>, 526–530 (2010).
</cite> [<a href="https://doi.org/10.3414/ME09-02-0034" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/20526521/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Karch,%20D.%20et%20al.%20Quantitative%20score%20for%20the%20evaluation%20of%20kinematic%20recordings%20in%20neuropediatric%20diagnostics.%20Methods%20Inf.%20Med.49,%20526%E2%80%93530%20(2010)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR52">
<span class="label">52.</span><cite>Kulvicius, T. et al. Infant movement classification through pressure distribution analysis. <em>Communications Medicine</em><strong>3</strong> (2023).</cite> [<a href="https://doi.org/10.1038/s43856-023-00342-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10432534/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37587165/" class="usa-link">PubMed</a>]</li>
<li id="CR53">
<span class="label">53.</span><cite>Aziz, S. et al. Detection of neonatal patient motion using a pressure-sensitive mat. In <em>2020 IEEE International Symposium on Medical Measurements and Applications (MeMeA)</em>, 1–6 (2020).</cite>
</li>
<li id="CR54">
<span class="label">54.</span><cite>Waitzman, K. A. The importance of positioning the near-term infant for sleep, play, and development. <em>Newborn Infant Nurs. Rev.</em><strong>7</strong>, 76–81 (2007).</cite> [<a href="https://scholar.google.com/scholar_lookup?Waitzman,%20K.%20A.%20The%20importance%20of%20positioning%20the%20near-term%20infant%20for%20sleep,%20play,%20and%20development.%20Newborn%20Infant%20Nurs.%20Rev.7,%2076%E2%80%9381%20(2007)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR55">
<span class="label">55.</span><cite>Raczyńska, A., Gulczyńska, E. &amp; Talar, T. Advantages of side-lying position. a comparative study of positioning during bottle-feeding in preterm infants (&lt;34 weeks ga). <em>J. Mother Child</em><strong>25</strong>, 269–276 (2021).
</cite> [<a href="https://doi.org/10.34763/jmotherandchild.20212504.d-22-00008" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9444194/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35675828/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Raczy%C5%84ska,%20A.,%20Gulczy%C5%84ska,%20E.%20&amp;%20Talar,%20T.%20Advantages%20of%20side-lying%20position.%20a%20comparative%20study%20of%20positioning%20during%20bottle-feeding%20in%20preterm%20infants%20(&lt;34%20weeks%20ga).%20J.%20Mother%20Child25,%20269%E2%80%93276%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR56">
<span class="label">56.</span><cite>Yang, Y. &amp; Ramanan, D. Articulated human detection with flexible mixtures of parts. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em><strong>35</strong>, 2878–2890 (2013).
</cite> [<a href="https://doi.org/10.1109/TPAMI.2012.261" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/24136428/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Yang,%20Y.%20&amp;%20Ramanan,%20D.%20Articulated%20human%20detection%20with%20flexible%20mixtures%20of%20parts.%20IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.35,%202878%E2%80%932890%20(2013)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR57">
<span class="label">57.</span><cite>Extended coco api (xtcocotools).<a href="https://github.com/jin-s13/xtcocoapi" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/jin-s13/xtcocoapi</a>.</cite>
</li>
<li id="CR58">
<span class="label">58.</span><cite>Loshchilov, I. &amp; Hutter, F. Decoupled weight decay regularization. In <em>International Conference on Learning Representations</em><a href="https://api.semanticscholar.org/CorpusID:53592270" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://api.semanticscholar.org/CorpusID:53592270</a> (2017).</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adsm93_" lang="en" class="supplementary-materials"><h3 class="pmc_sec_title">Supplementary Materials</h3>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="media p"><div class="caption">
<a href="/articles/instance/12373853/bin/41746_2025_1929_MOESM1_ESM.pdf" data-ga-action="click_feat_suppl" class="usa-link">Supplementary information</a><sup> (1.5MB, pdf) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets generated and/or analysed during the current study are not publicly available due to the sensitive and identifiable nature of the data. The data may be made available for academic purposes under a controlled data sharing agreement.</p>
<p>The underlying code for this study is not publicly available but may be made available to qualified researchers on reasonable request from the corresponding author.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from NPJ Digital Medicine are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41746-025-01929-z"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41746_2025_Article_1929.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (2.1 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12373853/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12373853/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12373853%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12373853/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12373853/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12373853/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40847126/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12373853/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40847126/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12373853/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12373853/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="euzsqtm7E3dDuspWU98DM6YCBYHnm4PNjCPAUdh3AYbEVVDvK0q9V7cNU6GaZc5l">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-574fdcc6.js"></script>
    
    

    </body>
</html>
