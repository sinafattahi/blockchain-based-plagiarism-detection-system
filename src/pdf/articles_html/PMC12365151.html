
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Lung adenocarcinoma subtype classification based on contrastive learning model with multimodal integration - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="152547A28AF34FB30547A200316B020D.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365151/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Lung adenocarcinoma subtype classification based on contrastive learning model with multimodal integration">
<meta name="citation_author" content="Changmiao Wang">
<meta name="citation_author_institution" content="Shenzhen Research Institute of Big Data, Shenzhen, China">
<meta name="citation_author" content="Lijian Liu">
<meta name="citation_author_institution" content="National Cancer Center, Chinese Academy of Medical Sciences and Peking Union Medical College, National Clinical Research Center for Cancer, Cancer Hospital and Shenzhen Hospital, Shenzhen, China">
<meta name="citation_author" content="Chenchen Fan">
<meta name="citation_author_institution" content="Zhejiang University of Finance and Economics, Hangzhou, China">
<meta name="citation_author" content="Yongquan Zhang">
<meta name="citation_author_institution" content="Zhejiang University of Finance and Economics, Hangzhou, China">
<meta name="citation_author" content="Zhijun Mai">
<meta name="citation_author_institution" content="National Cancer Center, Chinese Academy of Medical Sciences and Peking Union Medical College, National Clinical Research Center for Cancer, Cancer Hospital and Shenzhen Hospital, Shenzhen, China">
<meta name="citation_author" content="Li Li">
<meta name="citation_author_institution" content="National Cancer Center, Chinese Academy of Medical Sciences and Peking Union Medical College, National Clinical Research Center for Cancer, Cancer Hospital and Shenzhen Hospital, Shenzhen, China">
<meta name="citation_author" content="Zhou Liu">
<meta name="citation_author_institution" content="National Cancer Center, Chinese Academy of Medical Sciences and Peking Union Medical College, National Clinical Research Center for Cancer, Cancer Hospital and Shenzhen Hospital, Shenzhen, China">
<meta name="citation_author" content="Yuan Tian">
<meta name="citation_author_institution" content="The Second Affiliated Hospital, School of Medicine, The Chinese University of Hong Kong, Shenzhen, China">
<meta name="citation_author" content="Jiahang Hu">
<meta name="citation_author_institution" content="Hongqi Hospital Affiliated to Mudanjiang Medical University, Mudanjiang, China">
<meta name="citation_author" content="Ahmed Elazab">
<meta name="citation_author_institution" content="School of Biomedical Engineering, Shenzhen University, Shenzhen, China">
<meta name="citation_publication_date" content="2025 Aug 19">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30404">
<meta name="citation_doi" content="10.1038/s41598-025-13818-2">
<meta name="citation_pmid" content="40830161">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365151/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365151/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365151/pdf/41598_2025_Article_13818.pdf">
<meta name="description" content="Accurately identifying the stages of lung adenocarcinoma is essential for selecting the most appropriate treatment plans. Nonetheless, this task is complicated due to challenges such as integrating diverse data, similarities among subtypes, and the ...">
<meta name="og:title" content="Lung adenocarcinoma subtype classification based on contrastive learning model with multimodal integration">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Accurately identifying the stages of lung adenocarcinoma is essential for selecting the most appropriate treatment plans. Nonetheless, this task is complicated due to challenges such as integrating diverse data, similarities among subtypes, and the ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365151/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12365151">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-13818-2"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_13818.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12365151%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12365151/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12365151/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365151/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 19;15:30404. doi: <a href="https://doi.org/10.1038/s41598-025-13818-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-13818-2</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Lung adenocarcinoma subtype classification based on contrastive learning model with multimodal integration</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20C%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Changmiao Wang</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Changmiao Wang</span></h3>
<div class="p">
<sup>1</sup>Shenzhen Research Institute of Big Data, Shenzhen, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20C%22%5BAuthor%5D" class="usa-link"><span class="name western">Changmiao Wang</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20L%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Lijian Liu</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Lijian Liu</span></h3>
<div class="p">
<sup>2</sup>National Cancer Center, Chinese Academy of Medical Sciences and Peking Union Medical College, National Clinical Research Center for Cancer, Cancer Hospital and Shenzhen Hospital, Shenzhen, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20L%22%5BAuthor%5D" class="usa-link"><span class="name western">Lijian Liu</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Fan%20C%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Chenchen Fan</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Chenchen Fan</span></h3>
<div class="p">
<sup>3</sup>Zhejiang University of Finance and Economics, Hangzhou, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Fan%20C%22%5BAuthor%5D" class="usa-link"><span class="name western">Chenchen Fan</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20Y%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Yongquan Zhang</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Yongquan Zhang</span></h3>
<div class="p">
<sup>3</sup>Zhejiang University of Finance and Economics, Hangzhou, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20Y%22%5BAuthor%5D" class="usa-link"><span class="name western">Yongquan Zhang</span></a>
</div>
</div>
<sup>3,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mai%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Zhijun Mai</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Zhijun Mai</span></h3>
<div class="p">
<sup>2</sup>National Cancer Center, Chinese Academy of Medical Sciences and Peking Union Medical College, National Clinical Research Center for Cancer, Cancer Hospital and Shenzhen Hospital, Shenzhen, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Mai%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zhijun Mai</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20L%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Li Li</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Li Li</span></h3>
<div class="p">
<sup>2</sup>National Cancer Center, Chinese Academy of Medical Sciences and Peking Union Medical College, National Clinical Research Center for Cancer, Cancer Hospital and Shenzhen Hospital, Shenzhen, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Li%20L%22%5BAuthor%5D" class="usa-link"><span class="name western">Li Li</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Zhou Liu</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Zhou Liu</span></h3>
<div class="p">
<sup>2</sup>National Cancer Center, Chinese Academy of Medical Sciences and Peking Union Medical College, National Clinical Research Center for Cancer, Cancer Hospital and Shenzhen Hospital, Shenzhen, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zhou Liu</span></a>
</div>
</div>
<sup>2,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Tian%20Y%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Yuan Tian</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Yuan Tian</span></h3>
<div class="p">
<sup>4</sup>The Second Affiliated Hospital, School of Medicine, The Chinese University of Hong Kong, Shenzhen, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Tian%20Y%22%5BAuthor%5D" class="usa-link"><span class="name western">Yuan Tian</span></a>
</div>
</div>
<sup>4</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hu%20J%22%5BAuthor%5D" class="usa-link" aria-describedby="id9"><span class="name western">Jiahang Hu</span></a><div hidden="hidden" id="id9">
<h3><span class="name western">Jiahang Hu</span></h3>
<div class="p">
<sup>5</sup>Hongqi Hospital Affiliated to Mudanjiang Medical University, Mudanjiang, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hu%20J%22%5BAuthor%5D" class="usa-link"><span class="name western">Jiahang Hu</span></a>
</div>
</div>
<sup>5</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Elazab%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id10"><span class="name western">Ahmed Elazab</span></a><div hidden="hidden" id="id10">
<h3><span class="name western">Ahmed Elazab</span></h3>
<div class="p">
<sup>6</sup>School of Biomedical Engineering, Shenzhen University, Shenzhen, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Elazab%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Ahmed Elazab</span></a>
</div>
</div>
<sup>6</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Shenzhen Research Institute of Big Data, Shenzhen, China </div>
<div id="Aff2">
<sup>2</sup>National Cancer Center, Chinese Academy of Medical Sciences and Peking Union Medical College, National Clinical Research Center for Cancer, Cancer Hospital and Shenzhen Hospital, Shenzhen, China </div>
<div id="Aff3">
<sup>3</sup>Zhejiang University of Finance and Economics, Hangzhou, China </div>
<div id="Aff4">
<sup>4</sup>The Second Affiliated Hospital, School of Medicine, The Chinese University of Hong Kong, Shenzhen, China </div>
<div id="Aff5">
<sup>5</sup>Hongqi Hospital Affiliated to Mudanjiang Medical University, Mudanjiang, China </div>
<div id="Aff6">
<sup>6</sup>School of Biomedical Engineering, Shenzhen University, Shenzhen, China </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Dec 16; Accepted 2025 Jul 28; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12365151  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40830161/" class="usa-link">40830161</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Accurately identifying the stages of lung adenocarcinoma is essential for selecting the most appropriate treatment plans. Nonetheless, this task is complicated due to challenges such as integrating diverse data, similarities among subtypes, and the need to capture contextual features, making precise differentiation difficult. We address these challenges and propose a multimodal deep neural network that integrates computed tomography (CT) images, annotated lesion bounding boxes, and electronic health records. Our model first combines bounding boxes with precise lesion location data and CT scans, generating a richer semantic representation through feature extraction from regions of interest to enhance localization accuracy using a vision transformer module. Beyond imaging data, the model also incorporates clinical information encoded using a fully connected encoder. Features extracted from both CT and clinical data are optimized for cosine similarity using a contrastive language-image pre-training module, ensuring they are cohesively integrated. In addition, we introduce an attention-based feature fusion module that harmonizes these features into a unified representation to fuse features from different modalities further. This integrated feature set is then fed into a classifier that effectively distinguishes among the three types of adenocarcinomas. Finally, we employ focal loss to mitigate the effects of unbalanced classes and contrastive learning loss to enhance feature representation and improve the model’s performance. Our experiments on public and proprietary datasets demonstrate the efficiency of our model, achieving a superior validation accuracy of 81.42% and an area under the curve of 0.9120. These results significantly outperform recent multimodal classification approaches. The code is available at <a href="https://github.com/fancccc/LungCancerDC" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/fancccc/LungCancerDC</a>.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Lung Adenocarcinoma, Clinical Information, Multimodal Learning</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Breast cancer, Cancer imaging</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Lung cancer stands as one of the most prevalent cancers worldwide and is the leading cause of cancer-related deaths among men. It ranks as the second leading cause of cancer deaths among women, with its incidence rising each year<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a>,<a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>. Lung cancer is primarily categorized into two types: small cell lung cancer (SCLC) and non-small cell lung cancer (NSCLC), with NSCLC accounting for about 85% of all cases<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>. Within NSCLC, lung adenocarcinoma (LUAD) emerges as the most common subtype<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>. LUAD is further divided based on histological characteristics and the degree of tumor invasion into three categories: adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive adenocarcinoma (IA)<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a>,<a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>. AIS is localized within the alveoli, and early detection often leads to successful surgical intervention, resulting in a high cure rate and a positive prognosis. Likewise, recognizing MIA at an early stage enables timely treatment, which can halt its progression to more aggressive invasive adenocarcinoma, thus enhancing survival rates. Even though IA is already invasive, early detection significantly improves patient outcomes by reducing the risk of recurrence through surgical and adjuvant therapies. Therefore, accurately differentiating among these types of adenocarcinoma is essential for healthcare professionals to ensure effective early diagnosis and treatment.</p>
<p id="Par3">Low-dose computed tomography (CT) is the main method for early screening and diagnosis of lung cancer<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>. It enables doctors to assess vital tumor characteristics such as size, shape, location, metastasis, and heterogeneity, which are crucial for identifying the type of lung cancer. However, this process depends heavily on the expertise of experienced physicians, which can lead to variability in diagnostic opinions. Furthermore, the subtle histological differences among adenocarcinoma subtypes make distinguishing them based solely on visual CT features difficult. To address these challenges, diagnosing adenocarcinoma subtypes frequently requires integrating clinical information, including patients’ electronic health records (EHR). Physicians often combine this clinical data with CT findings to enhance diagnostic accuracy<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>. While this approach improves the precision of diagnoses, it also significantly increases the workload for healthcare professionals and reduces efficiency.</p>
<p id="Par4">The advancement of machine learning and radiomics has facilitated automated diagnosis using CT images<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>. Radiomics involves extracting histological and morphological features from these images based on established knowledge and specific rules, with these features often demonstrating strong associations with clinical outcomes. However, raw images need pre-processing to identify regions of interest (ROI), such as cancerous areas, which typically involves manual annotation by skilled radiologists or segmentation algorithms. Once features are extracted, they usually undergo further selection to retain only the most relevant ones, often employing statistical methods like LASSO and principal component analysis<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. Subsequently, machine learning models, including supervised methods like random forest<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>, support vector machine<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>, and generalized linear models<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>, or unsupervised methods like k-nearest neighbors<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup> are applied<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. Nonetheless, radiomics-based approaches sometimes fail to fully utilize information from CT images because significant data is lost during feature extraction, including regions adjacent to lesions that might correlate with the disease type. Additionally, the feature selection process can lead to further information loss, and traditional machine learning models, with their limited capacity for parameters, often face challenges in achieving optimal model fitting and performance.</p>
<p id="Par5">In the past decade, developments in deep learning technologies have helped address challenges in the early detection of lung cancer<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>. Convolutional neural networks (CNNs), such as the ResNet series<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>, have greatly enhanced the accuracy of recognizing features in CT scans. ResNet addresses the vanishing gradient problem in deep network training by using residual connections, which allows for the creation of deeper networks with improved accuracy and performance. More recently, the Transformer architecture<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>, especially the Vision Transformer (ViT)<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>, has outperformed CNN-based models while also reducing computational demands. Additionally, the contrastive language-image pre-training (CLIP) model<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup> has shown significant promise as a vision-language tool. Deep learning automates the extraction of visual features from CT scans, serving as a valuable quantification tool. This automation facilitates the identification of various lesions, offering faster and more precise diagnostic support for physicians<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a>,<a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>, and alleviates the workload on radiologists<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>. In multi-view, Zhou et al.<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup> introduced an ensemble multi-view 3D CNN model that excels in risk stratification of lung adenocarcinoma, Luo et al.<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup> proposed cross-aligned representation learning, even surpassing experienced doctors in evaluating invasive adenocarcinoma. Despite these advancements, accurately differentiating cancer subtypes remains challenging. Wang et al.<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup> combined different models, which enhanced preoperative diagnosis. Recent neurological studies have significantly advanced the development of Spiking Neural Networks (SNNs). Despite these advancements, the learning methods for SNNs are not yet fully understood <sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>. In addressing this gap, Agarwal et al. introduced a dual Encoder-Decoder framework specifically designed for processing CT images <sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>. Building on this work, they also developed a multi-scale dual-channel feature embedding decoder aimed at improving biomedical image segmentation <sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>. Furthermore, Mandal et al. explored optimization techniques by implementing a Real Coded Genetic Algorithm to effectively reduce errors <sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>. Existing models often struggle with classification accuracy because lesions are confined to small areas within CT scans, which contain considerable redundant information. Furthermore, effectively distinguishing cancer subtypes necessitates integrating both visual data and patient clinical information, a task that traditional network models find difficult to manage.</p>
<p id="Par6">The development of multimodal approaches has also significantly enhanced the capabilities of deep learning in the medical field. Yang et al.<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> examined the CT features of lung adenocarcinoma across different demographic groups and found that these features vary between genders and age groups. Studies by Yu et al.<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup> and Guo et al.<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> have demonstrated notable advancements. Wang et al.<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup> proposed an approach that integrates multiple imaging modalities but only visional modal. Integrating features from multiple sources often yields superior results compared to relying on a single feature set<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>. Vale-Silva et al.<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup> introduced the MultiSurv, a model applied across 33 different cancer types, which extracts features from visual data using CNN models like ResNeXt-50<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup> while processing clinical data through fully connected networks. The model employs a max-pooling method to combine these feature representations. However, this straightforward fusion might overlook important information within each modality and their interactions. Similarly, TMSS<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup> utilizes a ViT encoder to process multimodal data by integrating CT, positron emission tomography (PET) scans, and EHR for tasks such as segmentation and prognosis. Another approach, CLIP-Lung<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>, uses a textual knowledge-guided framework to predict lung nodule malignancy. The LLM-guided model<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup> employs large language models to analyze clinical notes and align them with image data. Although this model effectively integrates multimodal data and examines the relationships between different features, it struggles with information redundancy in large CT datasets, potentially overlooking critical histological features in lesion areas.</p>
<p id="Par7">This paper introduces a novel approach, a CLIP-Enhanced Multimodal Fusion Network (CMMFNet), designed to distinguish between lung adenocarcinoma subtypes effectively. Our method integrates multi-sized CT scans, corresponding lesion boundary boxes, and EHR. We utilize the CLIP module<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup> to extract features and calculate cosine distances between modalities, thereby enhancing contrastive loss optimization and improving the alignment of visual and textual information. In extracting features from CT images, bounding boxes are used to delineate lesion areas, enhancing the image data’s semantic representation. This enables the acquisition of ROI-based features, which are focused on the ROIs that contain the lesion, allowing the model to capture more meaningful and context-relevant information. To further refine this process, we introduce a hybrid attention mechanism for deep feature fusion in two stages. The first stage employs a multi-head self-attention mechanism to capture feature representations both within and between modalities, enhancing information coupling and interconnectivity. The second stage applies a Channel attention mechanism to emphasize the relative importance of different modality features, optimizing the feature fusion process with greater precision. This two-stage attention mechanism significantly enhances the completeness and semantic depth of multimodal feature representations, thereby improving the model’s ability to differentiate among various cancer subtypes. The main contributions of our work are outlined as follows:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par8">We enhance the semantic representation of CT images by leveraging bounding boxes around lesions to focus feature extraction on the ROI, which improves localization accuracy and ensures more precise lesion characterization.</p></li>
<li><p id="Par9">We propose a two-stage attention mechanism that integrates multi-head self-attention and channel attention, effectively coupling features across different modalities and assigning appropriate weights to each one for improved model performance.</p></li>
<li><p id="Par10">Our model generates a rich contextual feature representation by incorporating CT scans of varying resolutions, enabling more accurate differentiation of cancer subtypes.</p></li>
<li><p id="Par11">We evaluate the proposed CMMFNet model on public and proprietary datasets to verify its superiority over state-of-the-art methods.</p></li>
</ul></section><section id="Sec2"><h2 class="pmc_sec_title">Method</h2>
<p id="Par13">This study introduces the CMMFNet, a multimodal fusion network designed to diagnose lung adenocarcinoma by integrating CT images, positional data, and clinical information. As depicted in Fig. <a href="#Fig1" class="usa-link">1</a>, the process begins with the merger of three key inputs: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/eecf2d4c77da/d33e520.gif" loading="lazy" id="d33e520" alt="Inline graphic"></span>. Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2a7144b41b88/d33e526.gif" loading="lazy" id="d33e526" alt="Inline graphic"></span> represents the small-sized images cropped from the original CT scans using lesion bounding boxes <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/17ca751b81e7/d33e532.gif" loading="lazy" id="d33e532" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2a49eddd3af4/d33e538.gif" loading="lazy" id="d33e538" alt="Inline graphic"></span> are the corresponding large-sized CT images, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq5"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8d72f6d0a0ca/d33e545.gif" loading="lazy" id="d33e545" alt="Inline graphic"></span> represents the clinical data. These inputs are processed through the CLIP module, which extracts features from these diverse modalities. To enhance feature alignment, we calculate the cosine similarity between the extracted features, which informs the contrastive loss calculation. The resulting fused features, referred to as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq6"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0903214e80c9/d33e551.gif" loading="lazy" id="d33e551" alt="Inline graphic"></span>, are further refined through the Deep Fusion Framework (DFF), an attention-based module that facilitates deeper integration of the features. Ultimately, the integrated features are passed through a classification head to produce the diagnostic outcome. In the following subsection, we discuss each component of the CMMFNet in detail.</p>
<figure class="fig xbox font-sm" id="Fig1"><h3 class="obj_head">Fig. 1.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365151_41598_2025_13818_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/98580bbe83bb/41598_2025_13818_Fig1_HTML.jpg" loading="lazy" id="MO1" height="391" width="669" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The overall architecture of the proposed <strong>CMMFNet</strong>. It contains <strong>(a) CLIP</strong> and <strong>(b) DFF</strong> module. CT with bounding boxes are processed by an encoder based on the ViT module, <strong>(c) CTE</strong>, to capture and integrate multi-scale image information. EHR data is encoded using an FC-Base module, <strong>(d) CLE</strong>, to extract and integrate clinical text features.</p></figcaption></figure><section id="Sec3"><h3 class="pmc_sec_title">Multimodal feature extraction module</h3>
<p id="Par14">We employ a CLIP-based architecture to extract features from CT scans and clinical data, effectively. This architecture processes two sizes of CT images, each associated with lesion-bounding boxes, alongside structured clinical information. This process results in the extraction of features denoted as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq7"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8f0d65157259/d33e561.gif" loading="lazy" id="d33e561" alt="Inline graphic"></span>, representing each modality, respectively. The primary components of this model are as follows.</p>
<p id="Par15"><strong>The CT Encoder (CTE)</strong> module, depicted in Fig. <a href="#Fig1" class="usa-link">1</a>(c), is built on the ViT framework. Initially, a CT image <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq8"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0fd53e864e7d/d33e574.gif" loading="lazy" id="d33e574" alt="Inline graphic"></span> is processed through a 3D patch embedding technique. For an image with dimensions <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq9"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/61e4e8f845dc/d33e580.gif" loading="lazy" id="d33e580" alt="Inline graphic"></span>, it is divided into patches using a patch size of 8, resulting in <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq10"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/5221e3c01765/d33e586.gif" loading="lazy" id="d33e586" alt="Inline graphic"></span> patches. Each patch is embedded in a vector space of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq11"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/eab8a4e955a3/d33e592.gif" loading="lazy" id="d33e592" alt="Inline graphic"></span> dimensional, forming a matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq12"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/68d8e31dca4e/d33e599.gif" loading="lazy" id="d33e599" alt="Inline graphic"></span>, where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq13"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/27b90e81086a/d33e605.gif" loading="lazy" id="d33e605" alt="Inline graphic"></span>.</p>
<p id="Par16">The bounding box <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq14"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/03b204412e02/d33e613.gif" loading="lazy" id="d33e613" alt="Inline graphic"></span>, centered at <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq15"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/eb6391f23137/d33e619.gif" loading="lazy" id="d33e619" alt="Inline graphic"></span> with dimensions <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq16"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/7620ca734c5f/d33e625.gif" loading="lazy" id="d33e625" alt="Inline graphic"></span>, is adjusted through a fully connected layer to match this vector space, resulting in a matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq17"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/bb83014a98ba/d33e631.gif" loading="lazy" id="d33e631" alt="Inline graphic"></span>. The matrices <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq18"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/a086866e2160/d33e637.gif" loading="lazy" id="d33e637" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq19"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/07f08451fa63/d33e644.gif" loading="lazy" id="d33e644" alt="Inline graphic"></span> are concatenated into <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq20"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/623941c1d68d/d33e650.gif" loading="lazy" id="d33e650" alt="Inline graphic"></span>. Positional encoding with learnable parameters is then applied to this concatenated matrix. The matrix is subsequently processed through a standard 12-layer ViT module for encoding. The output from the final layer of this module is averaged across the channel dimension to produce the final output.</p>
<p id="Par17"><strong>The Clinical Encoder (CLE)</strong> module, shown in Fig. <a href="#Fig1" class="usa-link">1</a>(d), utilizes a fully connected network to encode clinical information. This process starts with EHR data, represented as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq21"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/826fa992cbb6/d33e663.gif" loading="lazy" id="d33e663" alt="Inline graphic"></span>. The EHR data is initially transformed through a fully connected layer to reshape it to <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq22"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/10be48561b5e/d33e669.gif" loading="lazy" id="d33e669" alt="Inline graphic"></span>. Positional encoding with learnable parameters is then applied to enhance the model’s comprehension of the data’s structural relationships and uncover latent interactions among features. The transformed vector is subsequently processed through another fully connected layer, adjusting its dimensions to produce <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq23"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0729c5a1f9c6/d33e675.gif" loading="lazy" id="d33e675" alt="Inline graphic"></span>. This final transformation aligns the clinical feature length with that of the visual modality feature.</p>
<p id="Par18"><strong>Contrastive learning:</strong> The encoded feature vectors <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq24"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/69d7c06a5244/d33e685.gif" loading="lazy" id="d33e685" alt="Inline graphic"></span> are processed through two distinct pathways. In the first pathway, these vectors are directly concatenated and passed to the next stages of the network, represented as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq25"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/07f65d7ea0f2/d33e691.gif" loading="lazy" id="d33e691" alt="Inline graphic"></span>. In the second pathway, features from each modality are normalized using the following equations:</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/80519b3b30a1/d33e697.gif" loading="lazy" id="d33e697" alt="graphic file with name d33e697.gif"></td>
<td class="label">1</td>
</tr></table>
<p>This normalization results in dimensionally consistent feature vectors <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq26"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/de9b105d8f8a/d33e704.gif" loading="lazy" id="d33e704" alt="Inline graphic"></span>. To compute the contrastive loss, the cosine similarities between these features are calculated and scaled by a temperature parameter: <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq27"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/081f5a3d9c1f/d33e710.gif" loading="lazy" id="d33e710" alt="Inline graphic"></span>. The cosine similarities between image and clinical features are given by:</p>
<table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/01a44a09d425/d33e717.gif" loading="lazy" id="d33e717" alt="graphic file with name d33e717.gif"></td>
<td class="label">2</td>
</tr></table>
<p>The logits <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq28"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/019a8ca1d548/d33e724.gif" loading="lazy" id="d33e724" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq29"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/ef1c4a75638e/d33e730.gif" loading="lazy" id="d33e730" alt="Inline graphic"></span> derived from both image modalities and clinical data are then used to calculate the contrastive loss <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq30"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/4547a4c8888e/d33e736.gif" loading="lazy" id="d33e736" alt="Inline graphic"></span>, as detailed in Section <a href="#Sec5" class="usa-link">Dynamic loss regulation</a>.</p></section><section id="Sec4"><h3 class="pmc_sec_title">Deep features fusion module</h3>
<p id="Par19">To achieve feature fusion, we develop an attention-based module, referred to as <strong>DFF</strong>, as shown in Fig. <a href="#Fig1" class="usa-link">1</a>(b). Initially, the feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq31"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/b1c216e7c794/d33e755.gif" loading="lazy" id="d33e755" alt="Inline graphic"></span> undergoes a two-step sequential process using the attention-based fusion modules. This approach effectively integrates the features. The fused features are then passed to the classification head, which produces the final category output.</p>
<p id="Par20"><strong>Phase 1:</strong> To integrate the concatenated multimodal features, we treat them as a unified entity and utilize an <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq32"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8c6f618fcae1/d33e765.gif" loading="lazy" id="d33e765" alt="Inline graphic"></span>-layer Transformer block to identify intrinsic correlations among these features. Each Transformer block is composed of multi-head self-attention and a feed-forward network. The input features pass through multiple attention layers, enabling the model to capture global dependencies within the data. After processing through the final Transformer block, layer normalization is applied to the output to ensure stable training.</p>
<p id="Par21">The forward process of this module is structured as follows: The input feature tensor <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq33"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/d2b355283c52/d33e773.gif" loading="lazy" id="d33e773" alt="Inline graphic"></span> is passed through a series of Transformer blocks. Within each block, multi-head self-attention is employed to extract contextual representations from the input. After processing through each block, the hidden state <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq34"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/d954518720e8/d33e779.gif" loading="lazy" id="d33e779" alt="Inline graphic"></span> is preserved for further analysis. This sequence of operations can be summarized as follows:</p>
<table class="disp-formula p" id="Equ3"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0436945b5884/d33e785.gif" loading="lazy" id="d33e785" alt="graphic file with name d33e785.gif"></td>
<td class="label">3</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq35"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8c1cda0127c0/d33e792.gif" loading="lazy" id="d33e792" alt="Inline graphic"></span> represents the comprehensive transformation within a Transformer block. This transformation is detailed as follows:</p>
<table class="disp-formula p" id="Equ4"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2d4b657709b0/d33e798.gif" loading="lazy" id="d33e798" alt="graphic file with name d33e798.gif"></td>
<td class="label">4</td>
</tr></table>
<table class="disp-formula p" id="Equ5"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/504abec9758a/d33e804.gif" loading="lazy" id="d33e804" alt="graphic file with name d33e804.gif"></td>
<td class="label">5</td>
</tr></table>
<table class="disp-formula p" id="Equ6"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2565457bd046/d33e811.gif" loading="lazy" id="d33e811" alt="graphic file with name d33e811.gif"></td>
<td class="label">6</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq36"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/418bdd9c74d0/d33e818.gif" loading="lazy" id="d33e818" alt="Inline graphic"></span> are the projection matrices, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq37"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/6cf492552b3f/d33e824.gif" loading="lazy" id="d33e824" alt="Inline graphic"></span> is the dimension of each attention head. Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq38"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/52961d9ed27a/d33e830.gif" loading="lazy" id="d33e830" alt="Inline graphic"></span> refers to one of the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq39"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1366073af62f/d33e836.gif" loading="lazy" id="d33e836" alt="Inline graphic"></span> attention heads, and the relation <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq40"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/d740bb4bf374/d33e842.gif" loading="lazy" id="d33e842" alt="Inline graphic"></span> holds. The matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq41"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/dcd8d971a441/d33e849.gif" loading="lazy" id="d33e849" alt="Inline graphic"></span> is the output projection matrix.</p>
<p id="Par22">The final output of the Transformer block, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq42"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8a710ccebcfb/d33e857.gif" loading="lazy" id="d33e857" alt="Inline graphic"></span>, is normalized to ensure consistency:</p>
<table class="disp-formula p" id="Equ7"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2b8463ced547/d33e863.gif" loading="lazy" id="d33e863" alt="graphic file with name d33e863.gif"></td>
<td class="label">7</td>
</tr></table>
<p>This normalized output, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq43"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/cdcf6bcfdeff/d33e870.gif" loading="lazy" id="d33e870" alt="Inline graphic"></span>, is then prepared for the subsequent phase, Phase 2.</p>
<p id="Par23"><strong>Phase 2:</strong> To improve feature representation, we incorporate a channel attention mechanism that selectively highlights important channels. This approach starts by performing pooling operations to gather information from each channel, which is then processed through a fully connected layer to determine the attention weights. During pooling, we use both average pooling and max pooling to capture spatial information across channels, offering two different perspectives on channel significance. These pooled features are then passed into a two-layer fully connected network to compute the attention weights. A reduction ratio is applied to manage the dimensionality of the intermediate layer, and the attention weights are normalized using a sigmoid function to maintain values between 0 and 1. The operations are defined as follows:</p>
<table class="disp-formula p" id="Equ8"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/40004833311c/d33e880.gif" loading="lazy" id="d33e880" alt="graphic file with name d33e880.gif"></td>
<td class="label">8</td>
</tr></table>
<table class="disp-formula p" id="Equ9"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/752fa6cf2a56/d33e886.gif" loading="lazy" id="d33e886" alt="graphic file with name d33e886.gif"></td>
<td class="label">9</td>
</tr></table>
<table class="disp-formula p" id="Equ10"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/12dc435a2e36/d33e892.gif" loading="lazy" id="d33e892" alt="graphic file with name d33e892.gif"></td>
<td class="label">10</td>
</tr></table>
<table class="disp-formula p" id="Equ11"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/250fd50c17bc/d33e898.gif" loading="lazy" id="d33e898" alt="graphic file with name d33e898.gif"></td>
<td class="label">11</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq44"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/a5d518980938/d33e905.gif" loading="lazy" id="d33e905" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq45"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8bfe477558a5/d33e911.gif" loading="lazy" id="d33e911" alt="Inline graphic"></span> represent the average and max pooling processes, respectively, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq46"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/dc3eae884ab8/d33e918.gif" loading="lazy" id="d33e918" alt="Inline graphic"></span> specifies the pooling region indices. The weights <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq47"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8786c1cde556/d33e924.gif" loading="lazy" id="d33e924" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq48"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/c322ede23ee7/d33e930.gif" loading="lazy" id="d33e930" alt="Inline graphic"></span> are learnable parameters of the fully connected layers, with <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq49"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0a33f142b04d/d33e936.gif" loading="lazy" id="d33e936" alt="Inline graphic"></span> as the reduction ratio. The sigmoid function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq50"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8db2ad445597/d33e942.gif" loading="lazy" id="d33e942" alt="Inline graphic"></span> is used for normalization.</p>
<p id="Par24">The final channel attention weights are computed by summing the weights from both pooling operations and applying them to the input feature map, resulting in the weighted feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq51"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/d8808bd6fbd1/d33e950.gif" loading="lazy" id="d33e950" alt="Inline graphic"></span>:</p>
<table class="disp-formula p" id="Equ12"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1e5c6ef3ab94/d33e956.gif" loading="lazy" id="d33e956" alt="graphic file with name d33e956.gif"></td>
<td class="label">12</td>
</tr></table>
<p>This method enables the network to focus on the most informative channels, thereby enhancing overall feature representation by adjusting the input feature map based on the calculated attention weights. We specifically configured the model with 12 Transformer layers and an embedding size of 768. This ViT model was adapted based on the 3D version of MONAI. <sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup></p></section><section id="Sec5"><h3 class="pmc_sec_title">Dynamic loss regulation</h3>
<p id="Par25">The total loss function utilized in this study consists of two contrastive loss components, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq52"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/5c66988cf7d4/d33e970.gif" loading="lazy" id="d33e970" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq53"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/ec4368029d1e/d33e976.gif" loading="lazy" id="d33e976" alt="Inline graphic"></span>, along with a focal loss<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq54"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2fd71c623f54/d33e986.gif" loading="lazy" id="d33e986" alt="Inline graphic"></span>. The contrastive loss is designed to optimize the cosine distance between different modalities, ensuring that similar representations of image and clinical information are closely aligned. Meanwhile, the focal loss addresses class imbalance by assigning greater weight to difficult-to-classify examples, thereby improving classification performance across imbalanced categories. The combined loss function is expressed as:</p>
<table class="disp-formula p" id="Equ13"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/31aded32771d/d33e992.gif" loading="lazy" id="d33e992" alt="graphic file with name d33e992.gif"></td>
<td class="label">13</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq55"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/5c66988cf7d4/d33e1000.gif" loading="lazy" id="d33e1000" alt="Inline graphic"></span> optimizes the cosine similarity between small-sized CT images and clinical data, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq56"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/ec4368029d1e/d33e1006.gif" loading="lazy" id="d33e1006" alt="Inline graphic"></span> pertains to large-sized CT images and clinical data. The focal loss <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq57"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2fd71c623f54/d33e1012.gif" loading="lazy" id="d33e1012" alt="Inline graphic"></span> handles class imbalance by computing class-specific losses. The hyperparameter <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq58"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0fa42dae2c49/d33e1018.gif" loading="lazy" id="d33e1018" alt="Inline graphic"></span> balances the contrastive and focal losses. To dynamically adjust <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq59"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0fa42dae2c49/d33e1024.gif" loading="lazy" id="d33e1024" alt="Inline graphic"></span>, we apply the dynamic loss scaling principle:</p>
<table class="disp-formula p" id="Equ14"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/cba4ddb5fbe5/d33e1030.gif" loading="lazy" id="d33e1030" alt="graphic file with name d33e1030.gif"></td>
<td class="label">14</td>
</tr></table>
<p>This dynamic adjustment ensures that if one loss component becomes larger, its weight is reduced, balancing the two loss values.</p>
<p id="Par26">The contrastive loss is derived from the cross-entropy loss applied to image and text logits. For each image-text logit pair, the cross-entropy loss is independently computed and averaged across the batch. The loss for a batch of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq60"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/e1842c352784/d33e1039.gif" loading="lazy" id="d33e1039" alt="Inline graphic"></span> image-text pairs is calculated as follows:</p>
<table class="disp-formula p" id="Equ15"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/4d61f2198503/d33e1045.gif" loading="lazy" id="d33e1045" alt="graphic file with name d33e1045.gif"></td>
<td class="label">15</td>
</tr></table>
<table class="disp-formula p" id="Equ16"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/539690e77be5/d33e1051.gif" loading="lazy" id="d33e1051" alt="graphic file with name d33e1051.gif"></td>
<td class="label">16</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq61"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/f3b1d0a97a48/d33e1058.gif" loading="lazy" id="d33e1058" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq62"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2af789020499/d33e1064.gif" loading="lazy" id="d33e1064" alt="Inline graphic"></span> are the logits from the image and text models, respectively, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq63"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/4ad6ea062580/d33e1070.gif" loading="lazy" id="d33e1070" alt="Inline graphic"></span> is the true label for the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq64"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/a086866e2160/d33e1077.gif" loading="lazy" id="d33e1077" alt="Inline graphic"></span>-th image-text pair. The function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq65"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1279689218a2/d33e1083.gif" loading="lazy" id="d33e1083" alt="Inline graphic"></span> represents the cross-entropy loss between logits <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq66"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0c895e9af837/d33e1089.gif" loading="lazy" id="d33e1089" alt="Inline graphic"></span> and label <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq67"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/efcc4ad69ea7/d33e1095.gif" loading="lazy" id="d33e1095" alt="Inline graphic"></span>. This approach ensures equal consideration of image-to-text and text-to-image relationships during training, with the final loss averaged over all pairs, thereby improving alignment between corresponding representations.</p>
<p id="Par27">To address class imbalance in the dataset, the focal loss extends traditional cross-entropy loss by focusing less on easily classified examples and more on challenging ones. The focal loss is formulated as:</p>
<table class="disp-formula p" id="Equ17"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2b0f4312a434/d33e1103.gif" loading="lazy" id="d33e1103" alt="graphic file with name d33e1103.gif"></td>
<td class="label">17</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq68"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/631dd6d2de12/d33e1110.gif" loading="lazy" id="d33e1110" alt="Inline graphic"></span> represents the predicted probability for the true class label, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq69"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/f28c1c61734a/d33e1116.gif" loading="lazy" id="d33e1116" alt="Inline graphic"></span> acts as a balancing factor for the class, and the parameter <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq70"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/020274b3a072/d33e1122.gif" loading="lazy" id="d33e1122" alt="Inline graphic"></span> is the focusing parameter. A higher <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq71"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/020274b3a072/d33e1128.gif" loading="lazy" id="d33e1128" alt="Inline graphic"></span> value increases the emphasis on difficult examples, improving performance in the presence of class imbalance. We set <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq72"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/f28c1c61734a/d33e1135.gif" loading="lazy" id="d33e1135" alt="Inline graphic"></span> based on the class distribution (e.g., inversely proportional to class frequencies) to help address the imbalance between classes.</p></section></section><section id="Sec6"><h2 class="pmc_sec_title">Experiments</h2>
<section id="Sec7"><h3 class="pmc_sec_title">Datasets and implementation</h3>
<p id="Par29"><strong>The open-source LPCD dataset</strong> <sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>, comprises DICOM images from CT and PET-CT scans, featuring bounding boxes that mark tumor locations and include associated clinical data. This dataset consists of 342 lung cancer instances, which are classified into 246 cases of adenocarcinoma, 55 cases of small cell carcinoma, 36 cases of large cell carcinoma, and 5 cases of squamous cell carcinoma. Given the limited number of squamous cell carcinoma instances, the large cell and squamous cell carcinoma categories are combined, resulting in a three-class dataset with the following distributions: 70.7% adenocarcinoma, 17.3% small cell carcinoma, and 12.0% for the merged category.</p>
<p id="Par30"><strong>The private LUNA-M dataset</strong>, sourced from the Cancer Hospital &amp; Shenzhen Hospital in Shenzhen, China, includes 1,614 cases of lung adenocarcinoma from 1,430 anonymized patients, comprising 545 males and 885 females. Each case contains CT scans, clinical data, and bounding boxes marking tumor locations. Patients’ ages range from 19 to 86, with half between 46 and 64. The dataset is divided into 63 cases of AAH, 299 of AIS, 389 of MIA, and 863 of IA. Due to the low occurrence and pathological similarity of AAH to adenocarcinoma, AAH and AIS are merged into a single category, creating a three-class dataset with 53.5% IA, 24.1% MA, and 22.4% for the combined AAH and AIS. Initial bounding boxes, which provide approximate 2D annotations of the largest lesion cross-section, are refined to accurately define the lesion’s physical location and depth. This refinement results in a 3D bounding box that fully encapsulates the lesion.</p>
<p id="Par31"><strong>Dataset Processing:</strong> All CT scans initially have dimensions of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq73"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/40c251322750/d33e1161.gif" loading="lazy" id="d33e1161" alt="Inline graphic"></span> in a 3D volume, where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq74"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/f91c962886e8/d33e1167.gif" loading="lazy" id="d33e1167" alt="Inline graphic"></span> indicates the number of scans. The slice intervals range from 0.625 mm to 5 mm, but all scans are resampled to a uniform resolution of 1 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq75"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/6b410ed72ba1/d33e1173.gif" loading="lazy" id="d33e1173" alt="Inline graphic"></span>. To ensure the images are within a relevant intensity range, windowing and truncation are applied to the pixel values, restricting them to the range <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq76"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8795c2ff9809/d33e1179.gif" loading="lazy" id="d33e1179" alt="Inline graphic"></span> HU, a common practice in medical imaging to focus on the most relevant tissue densities. Each lesion is then center-cropped into 3D blocks of sizes <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq77"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/00a6e97dc96a/d33e1185.gif" loading="lazy" id="d33e1185" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq78"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/9441d23d4da2/d33e1192.gif" loading="lazy" id="d33e1192" alt="Inline graphic"></span>, with bounding boxes adjusted to relative positions. The accuracy of this cropped data is validated using ITK-SNAP<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup>. For data augmentation, random flips are applied to the CT scans along each axis (i.e., axial, sagittal, and coronal), enhancing the model’s robustness and generalization capabilities. Corresponding adjustments are made to the bounding boxes to maintain accurate lesion localization after the flip. Additionally, the EHR data undergoes preprocessing by standardizing numerical variables and applying one-hot encoding to categorical variables.</p>
<p id="Par32"><strong>Implementation Details:</strong> Our experiments were conducted using a 48G NVIDIA A40 GPU with CUDA Version 12.2. The computational setup included Python 3.11 and PyTorch 2.4. To train the model, we employed five-fold cross-validation. This process involved dividing the dataset into five subsets using stratified sampling, where each subset was used once as a validation set while the remaining four subsets were used for training. This approach ensured that class distributions remained consistent across all folds by employing stratified splitting. We employed the Adam optimizer, starting with a learning rate of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq79"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/9282db406ceb/d33e1206.gif" loading="lazy" id="d33e1206" alt="Inline graphic"></span>, a weight decay of 0.001, and beta set to (0.9, 0.99) to handle momentum and adjust the learning rate efficiently. Furthermore, the ExponentialLR scheduler was applied to gradually reduce the learning rate during training, using a gamma value of 0.99. During the training phase, we configured the number of epochs to 500 and used a batch size of 24. For the loss function parameters, we set the initial weight of the contrastive loss to 0.5, as specified in Eq. (<a href="#Equ13" class="usa-link">13</a>). Additionally, in Eq. (<a href="#Equ17" class="usa-link">17</a>), the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq80"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/f28c1c61734a/d33e1225.gif" loading="lazy" id="d33e1225" alt="Inline graphic"></span> hyperparameter for focal loss was adjusted to be inversely proportional to class frequencies, while the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq81"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/020274b3a072/d33e1231.gif" loading="lazy" id="d33e1231" alt="Inline graphic"></span> value was fixed at 0.25.</p>
<p id="Par33"><strong>Evaluation criteria:</strong> The evaluation of all methods was conducted using widely recognized metrics to ensure a fair and comprehensive comparison. These metrics include Accuracy, Precision, Recall, F1-Score, and Area Under the ROC Curve (AUC). These metrics collectively offer a holistic view of the models’ classification performance, ensuring that improvements are not biased towards a single metric but reflect an overall enhancement in prediction quality.</p></section><section id="Sec8"><h3 class="pmc_sec_title">Comparison with other methods</h3>
<p id="Par35">In our experiments with the LUNA-M and LPCD datasets, as indicated in Table <a href="#Tab1" class="usa-link">1</a>, our method consistently surpassed other approaches. The baseline experiments, drawing from previous studies<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a>,<a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>, focused on performance analysis, particularly with the LUNA-M dataset. This focus was necessary due to the highly imbalanced class distribution in the LPCD dataset, which led to predictions overly favoring a single dominant class across all samples. ResNet18 showed superior accuracy compared to the ViT model, achieving improvements of 1.24% and 13.00%. We believe this is due to the ability of CNNs to effectively capture subtle lesion regions. In contrast, Transformer-based models like ViT tend to focus on the overall structure of the image, which can limit their effectiveness in highlighting small, localized areas. Further experiments were conducted using images of varying resolutions. Results from the LUNA-M dataset indicated that CT32 images outperformed CT128 images. This finding underscores the negative impact of redundant information outside the lesion areas on network performance. Specifically, using CT32 images resulted in accuracy improvements of 4.03% and 15.79%. These outcomes highlight the critical importance of concentrating on the region of interest (ROI) to enhance the performance of the network.</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>Comparison of the proposed method with baseline methods and state-of-the-art approaches on the LPCD and LUNA-M datasets. <strong>Bold</strong> text highlights the best indicator, while <span class="text-underline">underlined</span> text represents the second-best. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq82"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1257.gif" loading="lazy" id="d33e1257" alt="Inline graphic"></span> indicates an increase, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq83"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/3ad61e754407/d33e1263.gif" loading="lazy" id="d33e1263" alt="Inline graphic"></span> indicates a decrease.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Method</th>
<th align="left" colspan="1" rowspan="1">Data</th>
<th align="left" colspan="1" rowspan="1">Accuracy (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">F1-Score (%)</th>
<th align="left" colspan="1" rowspan="1">AUC (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq84"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2543fae3df6f/d33e1294.gif" loading="lazy" id="d33e1294" alt="Inline graphic"></span>)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">ResNet18 (CT32)<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>
</td>
<td align="left" rowspan="9" colspan="1">LPCD</td>
<td align="left" colspan="1" rowspan="1">71.01</td>
<td align="left" colspan="1" rowspan="1">23.67</td>
<td align="left" colspan="1" rowspan="1">33.33</td>
<td align="left" colspan="1" rowspan="1">27.68</td>
<td align="left" colspan="1" rowspan="1">50.00</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ResNet18 (CT128)<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">73.91</td>
<td align="left" colspan="1" rowspan="1">57.71</td>
<td align="left" colspan="1" rowspan="1">38.46</td>
<td align="left" colspan="1" rowspan="1">37.05</td>
<td align="left" colspan="1" rowspan="1">82.53</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT (CT32)<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">71.01</td>
<td align="left" colspan="1" rowspan="1">23.67</td>
<td align="left" colspan="1" rowspan="1">33.33</td>
<td align="left" colspan="1" rowspan="1">27.68</td>
<td align="left" colspan="1" rowspan="1">50.00</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT (CT128)<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">71.01</td>
<td align="left" colspan="1" rowspan="1">23.67</td>
<td align="left" colspan="1" rowspan="1">33.33</td>
<td align="left" colspan="1" rowspan="1">27.68</td>
<td align="left" colspan="1" rowspan="1">64.87</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CLIP-Lung<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">72.46</td>
<td align="left" colspan="1" rowspan="1">42.04</td>
<td align="left" colspan="1" rowspan="1">49.08</td>
<td align="left" colspan="1" rowspan="1">44.95</td>
<td align="left" colspan="1" rowspan="1">76.73</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">TMSS<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">79.71</span>
</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">76.67</span>
</td>
<td align="left" colspan="1" rowspan="1">61.59</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">63.33</span>
</td>
<td align="left" colspan="1" rowspan="1">79.62</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MultiSurv<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">78.26</td>
<td align="left" colspan="1" rowspan="1">49.89</td>
<td align="left" colspan="1" rowspan="1">
<strong>65.31</strong>
</td>
<td align="left" colspan="1" rowspan="1">56.46</td>
<td align="left" colspan="1" rowspan="1">78.87</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">LLM-guided<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">75.36</td>
<td align="left" colspan="1" rowspan="1">49.49</td>
<td align="left" colspan="1" rowspan="1">55.78</td>
<td align="left" colspan="1" rowspan="1">52.29</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">85.31</span>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<strong>CMMFNet (Ours)</strong>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>81.16</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq85"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1471.gif" loading="lazy" id="d33e1471" alt="Inline graphic"></span><em>1.45</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>87.89</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq86"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1483.gif" loading="lazy" id="d33e1483" alt="Inline graphic"></span><em>11.22</em>
</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">62.27</span>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq87"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/3ad61e754407/d33e1495.gif" loading="lazy" id="d33e1495" alt="Inline graphic"></span>
<strong><em>3.04</em></strong>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>64.23</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq88"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1508.gif" loading="lazy" id="d33e1508" alt="Inline graphic"></span><em>0.90</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>85.67</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq89"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1520.gif" loading="lazy" id="d33e1520" alt="Inline graphic"></span><em>0.36</em>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ResNet18 (CT32)<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>
</td>
<td align="left" rowspan="9" colspan="1">LUNA-M</td>
<td align="left" colspan="1" rowspan="1">70.59</td>
<td align="left" colspan="1" rowspan="1">63.26</td>
<td align="left" colspan="1" rowspan="1">63.48</td>
<td align="left" colspan="1" rowspan="1">62.81</td>
<td align="left" colspan="1" rowspan="1">85.84</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ResNet18 (CT128)<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">66.56</td>
<td align="left" colspan="1" rowspan="1">63.04</td>
<td align="left" colspan="1" rowspan="1">59.75</td>
<td align="left" colspan="1" rowspan="1">59.70</td>
<td align="left" colspan="1" rowspan="1">83.86</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT (CT32)<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">69.35</td>
<td align="left" colspan="1" rowspan="1">61.88</td>
<td align="left" colspan="1" rowspan="1">61.81</td>
<td align="left" colspan="1" rowspan="1">61.84</td>
<td align="left" colspan="1" rowspan="1">80.97</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT (CT128)<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">53.56</td>
<td align="left" colspan="1" rowspan="1">17.85</td>
<td align="left" colspan="1" rowspan="1">33.33</td>
<td align="left" colspan="1" rowspan="1">23.25</td>
<td align="left" colspan="1" rowspan="1">52.41</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CLIP-Lung<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">71.52</td>
<td align="left" colspan="1" rowspan="1">65.05</td>
<td align="left" colspan="1" rowspan="1">65.26</td>
<td align="left" colspan="1" rowspan="1">64.64</td>
<td align="left" colspan="1" rowspan="1">85.60</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">TMSS<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">78.95</span>
</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">74.79</span>
</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">76.58</span>
</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">75.45</span>
</td>
<td align="left" colspan="1" rowspan="1">88.96</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MultiSurv<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">74.92</td>
<td align="left" colspan="1" rowspan="1">70.28</td>
<td align="left" colspan="1" rowspan="1">68.46</td>
<td align="left" colspan="1" rowspan="1">68.72</td>
<td align="left" colspan="1" rowspan="1">88.45</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">LLM-guided<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">77.09</td>
<td align="left" colspan="1" rowspan="1">72.75</td>
<td align="left" colspan="1" rowspan="1">73.50</td>
<td align="left" colspan="1" rowspan="1">73.05</td>
<td align="left" colspan="1" rowspan="1">
<span class="text-underline">90.47</span>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<strong>CMMFNet (Ours)</strong>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>81.42</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq90"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1698.gif" loading="lazy" id="d33e1698" alt="Inline graphic"></span><em>2.47</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>78.12</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq91"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1710.gif" loading="lazy" id="d33e1710" alt="Inline graphic"></span><em>3.33</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>79.39</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq92"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1722.gif" loading="lazy" id="d33e1722" alt="Inline graphic"></span><em>2.81</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>78.63</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq93"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1734.gif" loading="lazy" id="d33e1734" alt="Inline graphic"></span><em>3.18</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>91.20</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq94"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1746.gif" loading="lazy" id="d33e1746" alt="Inline graphic"></span><em>0.73</em>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par36">We compared our proposed method to several state-of-the-art multimodal approaches<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a>,<a href="#CR37" class="usa-link" aria-describedby="CR37">37</a>–<a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>. While these methods typically use single-size CT images (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq95"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/9441d23d4da2/d33e1778.gif" loading="lazy" id="d33e1778" alt="Inline graphic"></span>) and EHR as inputs, our approach integrates a more comprehensive set of inputs, including dual-size CT images, BBOX, and EHR. On the LUNA-M dataset, our method achieved the highest performance metrics: Accuracy at 81.42%, Precision at 78.12%, Recall at 79.39%, F1-Score at 78.63%, and AUC at 0.9120. These results represent significant improvements over the second-best scores, with increases of 2.47% in Accuracy, 3.33% in Precision, 2.81% in Recall, and 3.18% in F1-Score, in addition to a modest AUC gain of 0.0073. The ROC curves presented in Fig. <a href="#Fig2" class="usa-link">2</a> illustrate that our method, CMMFNet, achieves the most optimal curve position, reflecting its excellent discriminative capacity across various classification thresholds.</p>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/d101b10660f3/41598_2025_13818_Fig3_HTML.jpg" loading="lazy" id="MO3" height="475" width="645" alt="Fig. 2"></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>ROC curves of different methods on LUNA-M.</p></figcaption></figure><p id="Par37">Similarly, our method ranked highest on the LPCD dataset, with an Accuracy of 81.16%, Precision of 87.89%, F1-Score of 64.23%, and AUC of 0.8567. These metrics also showed notable improvements over the next best results, with gains of 1.45% in Accuracy, 11.22% in Precision, 0.90% in F1-Score, and 0.0036 in AUC. The results on external datasets such as LPCD<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup> further validate the effectiveness of our proposed method and demonstrate its strong generalization capability, indicating its applicability to different datasets with similar modalities.</p>
<p id="Par38">Overall, our method achieved leading performance across both datasets. By leveraging state-of-the-art multimodal approaches<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a>,<a href="#CR37" class="usa-link" aria-describedby="CR37">37</a>–<a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>, rather than relying solely on CT images, we substantially enhanced accuracy. Furthermore, our proposed method outperformed existing techniques across all evaluated metrics, underscoring its effectiveness and robustness.</p></section><section id="Sec9"><h3 class="pmc_sec_title">Ablation study</h3>
<p id="Par40">To evaluate the effectiveness of each component, we conducted ablation studies on the LUNA-M dataset, with the results presented in Table <a href="#Tab2" class="usa-link">2</a>. The findings reveal that utilizing two scales of CT inputs increases accuracy by 1.24% compared to single-scale inputs, highlighting the benefit of multi-scale data in improving model accuracy. For 128-size CT scans, the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq126"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2a49eddd3af4/d33e2215.gif" loading="lazy" id="d33e2215" alt="Inline graphic"></span> enhanced with bounding boxes showed improvements of 15.17% in accuracy, 45.96% in precision, 30.96% in recall, 40.69% in F1-Score and 0.3057 in AUC compared to <em>CT</em>128, which is using original CT data. This underscores the advantage of incorporating lesion location information for CT enhancements. Our <em>CT</em>32, <em>CT</em>128 model is designed to focus on cropped lesion regions, or Regions of Interest (RoI), to reduce the inclusion of non-lesion areas. As shown in Table <a href="#Tab2" class="usa-link">2</a>, ablation experiments were performed using images with bounding boxes. In contrast, experiments involving <em>CT</em>32 and <em>CT</em>128 were conducted without the use of bounding boxes. Figure <a href="#Fig3" class="usa-link">3</a> illustrates how ROI-based feature extraction enhances semantic representation across different CT scan sizes, directing more attention to tumor areas.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Results of ablation experiment on LUNA-M. <strong>Bold</strong> text highlights the best indicator. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq96"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1825.gif" loading="lazy" id="d33e1825" alt="Inline graphic"></span> indicates an increase, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq97"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/3ad61e754407/d33e1831.gif" loading="lazy" id="d33e1831" alt="Inline graphic"></span> indicates a decrease.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Method</th>
<th align="left" colspan="1" rowspan="1">Accuracy (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">F1-Score (%)</th>
<th align="left" colspan="1" rowspan="1">AUC (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq98"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2543fae3df6f/d33e1859.gif" loading="lazy" id="d33e1859" alt="Inline graphic"></span>)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">
<em>CT</em>32</td>
<td align="left" colspan="1" rowspan="1">69.35</td>
<td align="left" colspan="1" rowspan="1">61.88</td>
<td align="left" colspan="1" rowspan="1">61.81</td>
<td align="left" colspan="1" rowspan="1">61.84</td>
<td align="left" colspan="1" rowspan="1">80.97</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq99"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2a7144b41b88/d33e1886.gif" loading="lazy" id="d33e1886" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">69.04</td>
<td align="left" colspan="1" rowspan="1">63.80</td>
<td align="left" colspan="1" rowspan="1">63.28</td>
<td align="left" colspan="1" rowspan="1">62.64</td>
<td align="left" colspan="1" rowspan="1">83.84</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<em>CT</em>128</td>
<td align="left" colspan="1" rowspan="1">53.56</td>
<td align="left" colspan="1" rowspan="1">17.85</td>
<td align="left" colspan="1" rowspan="1">33.33</td>
<td align="left" colspan="1" rowspan="1">23.25</td>
<td align="left" colspan="1" rowspan="1">52.41</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq100"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2a49eddd3af4/d33e1922.gif" loading="lazy" id="d33e1922" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">68.73</td>
<td align="left" colspan="1" rowspan="1">63.81</td>
<td align="left" colspan="1" rowspan="1">64.29</td>
<td align="left" colspan="1" rowspan="1">63.94</td>
<td align="left" colspan="1" rowspan="1">82.98</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq101"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/ed2c203f415a/d33e1942.gif" loading="lazy" id="d33e1942" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">70.59<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq102"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1951.gif" loading="lazy" id="d33e1951" alt="Inline graphic"></span><em>1.24</em>
</td>
<td align="left" colspan="1" rowspan="1">64.71<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq103"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1961.gif" loading="lazy" id="d33e1961" alt="Inline graphic"></span><em>0.90</em>
</td>
<td align="left" colspan="1" rowspan="1">65.13<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq104"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e1971.gif" loading="lazy" id="d33e1971" alt="Inline graphic"></span><em>0.84</em>
</td>
<td align="left" colspan="1" rowspan="1">63.20<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq105"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/3ad61e754407/d33e1981.gif" loading="lazy" id="d33e1981" alt="Inline graphic"></span>
<strong><em>0.74</em></strong>
</td>
<td align="left" colspan="1" rowspan="1">83.56<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq106"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/3ad61e754407/d33e1992.gif" loading="lazy" id="d33e1992" alt="Inline graphic"></span>
<strong><em>0.28</em></strong>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq107"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8d72f6d0a0ca/d33e2004.gif" loading="lazy" id="d33e2004" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">73.99</td>
<td align="left" colspan="1" rowspan="1">68.51</td>
<td align="left" colspan="1" rowspan="1">69.33</td>
<td align="left" colspan="1" rowspan="1">68.57</td>
<td align="left" colspan="1" rowspan="1">86.90</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq108"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/4ce537f4739a/d33e2024.gif" loading="lazy" id="d33e2024" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">78.02<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq109"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2033.gif" loading="lazy" id="d33e2033" alt="Inline graphic"></span><em>4.03</em>
</td>
<td align="left" colspan="1" rowspan="1">75.00<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq110"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2043.gif" loading="lazy" id="d33e2043" alt="Inline graphic"></span><em>6.49</em>
</td>
<td align="left" colspan="1" rowspan="1">76.29<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq111"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2053.gif" loading="lazy" id="d33e2053" alt="Inline graphic"></span><em>6.96</em>
</td>
<td align="left" colspan="1" rowspan="1">75.22<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq112"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2063.gif" loading="lazy" id="d33e2063" alt="Inline graphic"></span><em>6.65</em>
</td>
<td align="left" colspan="1" rowspan="1">89.26<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq113"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2073.gif" loading="lazy" id="d33e2073" alt="Inline graphic"></span><em>2.36</em>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq114"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0c0d1bddf1ce/d33e2084.gif" loading="lazy" id="d33e2084" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">78.64<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq115"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2093.gif" loading="lazy" id="d33e2093" alt="Inline graphic"></span><em>0.62</em>
</td>
<td align="left" colspan="1" rowspan="1">75.56<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq116"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2103.gif" loading="lazy" id="d33e2103" alt="Inline graphic"></span><em>0.56</em>
</td>
<td align="left" colspan="1" rowspan="1">76.95<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq117"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2113.gif" loading="lazy" id="d33e2113" alt="Inline graphic"></span><em>0.66</em>
</td>
<td align="left" colspan="1" rowspan="1">75.95<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq118"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2123.gif" loading="lazy" id="d33e2123" alt="Inline graphic"></span><em>0.73</em>
</td>
<td align="left" colspan="1" rowspan="1">89.87<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq119"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2133.gif" loading="lazy" id="d33e2133" alt="Inline graphic"></span><em>0.61</em>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq120"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0c0d1bddf1ce/d33e2143.gif" loading="lazy" id="d33e2143" alt="Inline graphic"></span> + DFF</td>
<td align="left" colspan="1" rowspan="1">
<strong>81.42</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq121"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2154.gif" loading="lazy" id="d33e2154" alt="Inline graphic"></span><em>2.78</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>78.12</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq122"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2166.gif" loading="lazy" id="d33e2166" alt="Inline graphic"></span><em>2.56</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>79.39</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq123"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2178.gif" loading="lazy" id="d33e2178" alt="Inline graphic"></span><em>2.44</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>78.63</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq124"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2190.gif" loading="lazy" id="d33e2190" alt="Inline graphic"></span><em>2.68</em>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>91.20</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq125"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/1cdeeae8e817/d33e2202.gif" loading="lazy" id="d33e2202" alt="Inline graphic"></span><em>1.33</em>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12365151_41598_2025_13818_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/cb291f9ebe45/41598_2025_13818_Fig2_HTML.jpg" loading="lazy" id="MO2" height="585" width="749" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Enhancing semantic representation through ROI-based feature extraction.</p></figcaption></figure><p id="Par41">The integration of clinical information further boosted the model’s performance. Compared to the best-performing single modality (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq127"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8d72f6d0a0ca/d33e2255.gif" loading="lazy" id="d33e2255" alt="Inline graphic"></span>), incorporating multimodal data resulted in improvements of 4.03% in accuracy, 6.49% in precision, 6.96% in recall, 6.65% in F1-Score, and 0.0236 in AUC. These results highlight the significant advantage of our approach in effectively integrating multimodal data for enhanced predictive performance. The results demonstrate that the combination of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq128"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0c0d1bddf1ce/d33e2261.gif" loading="lazy" id="d33e2261" alt="Inline graphic"></span> yields a consistent improvement of approximately 0.6% across all metrics compared to <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq129"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/4ce537f4739a/d33e2267.gif" loading="lazy" id="d33e2267" alt="Inline graphic"></span>. This suggests that including images with bounding boxes enhances performance.</p>
<p id="Par42">The integration of clinical information further boosted the model’s performance. Compared to the best-performing single modality (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq130"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/8d72f6d0a0ca/d33e2275.gif" loading="lazy" id="d33e2275" alt="Inline graphic"></span>), incorporating multimodal data resulted in improvements of 4.03% in accuracy, 6.49% in precision, 6.96% in recall, 6.65% in F1-Score, and 0.0236 in AUC. These results highlight the significant advantage of our approach in effectively integrating multimodal data for enhanced predictive performance. The results show that <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq131"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/0c0d1bddf1ce/d33e2281.gif" loading="lazy" id="d33e2281" alt="Inline graphic"></span> leads to a consistent improvement of approximately 0.6% across all metrics compared to <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq132"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/4ce537f4739a/d33e2287.gif" loading="lazy" id="d33e2287" alt="Inline graphic"></span>. This indicates that incorporating images with bounding boxes contributes to the performance boost.</p>
<p id="Par43">Experiments with the DFF module revealed that attention-based fusion significantly enhanced overall model performance. The model achieved peak accuracy and AUC scores of 81.42% and 0.9120, respectively, with the DFF module contributing improvements of 2.78% in accuracy, 2.56% in precision, 2.44% in recall, 2.68% in F1-Score, and 0.0133 in AUC across all metrics. These results indicate that each component of our approach, including multi-scale input, lesion-aware enhancement, and deep feature fusion, is crucial for enhancing model performance, working synergistically to optimize effectiveness. The AUC primarily assesses the model’s ability to rank positive and negative samples globally, offering a broad evaluation across various classification thresholds. It is not sensitive to class distribution. On the other hand, the DFF module is designed to enhance classification performance at a specific threshold, typically set at 0.5. Consequently, the DFF module markedly improves metrics like accuracy and F1-score, but its influence on the overall ranking performance, as indicated by AUC, is limited. This suggests that while the DFF module strengthens the model’s ability to distinguish samples around the decision threshold, its impact on global ranking capability is relatively minor.</p>
<p id="Par44"><strong>Student’s t-test: </strong> Statistical hypothesis testing is crucial for determining whether observed performance improvements are due to actual model enhancements or merely random variations in the data <sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup>. The Student’s t-test is a widely used method for comparing the means of two related groups to assess whether their differences are statistically significant. To determine if the improvements in experimental metrics were due to random fluctuations or genuine model enhancements, we conducted a t-test on various result sets. Specifically, we used a paired t-test, suitable for comparing performance metrics before and after model modifications. The calculated <em>p</em>-value for our approach in the comparative experiment is 0.0059, and in the ablation experiment, it is 0.0009. Since both values are statistically significant (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq133"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/44e623a9db3b/d33e2306.gif" loading="lazy" id="d33e2306" alt="Inline graphic"></span>), these results confirm that the observed improvements in performance are indeed due to the proposed model enhancements.</p>
<p id="Par45">The confusion matrix, as depicted in Fig. <a href="#Fig4" class="usa-link">4</a>, highlights the model’s performance in predicting lung adenocarcinoma subtypes (IA, MA, AIS). The model demonstrates high accuracy for IA, with 149 cases correctly classified and minimal misclassification into MA (18 cases) and AIS (6 cases). This suggests that the distinct features of IA are effectively captured by the model. However, there is significant confusion between MA and AIS, indicative of their overlapping radiological and pathological characteristics. Specifically, 7 cases of MA were misclassified as IA, and 12 as AIS, while 6 AIS cases were predicted as IA, and 11 as MA. These misclassifications underscore the challenge of distinguishing these subtypes, emphasizing the need for more refined feature extraction or model adjustments. Improving the model’s performance could involve incorporating richer imaging features or leveraging multimodal data, such as clinical and molecular biomarkers. These enhancements would better differentiate between the subtypes and reduce ambiguity, thereby improving the model’s clinical applicability and decision-making reliability.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/a71166db827f/41598_2025_13818_Fig4_HTML.jpg" loading="lazy" id="MO4" height="556" width="645" alt="Fig. 4"></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Confusion Matrix on LUNA-M.</p></figcaption></figure></section><section id="Sec10"><h3 class="pmc_sec_title">Performance analysis</h3>
<p id="Par47">In this study, we evaluate various models based on inference computation, memory usage, and parameter size, as shown in Table <a href="#Tab3" class="usa-link">3</a>. We use multiply-accumulate operations (MACs) as the key measure for computational complexity. Memory usage indicates the GPU memory required during inference, and parameter size reflects the model’s complexity.</p>
<section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Comparison of Model Inference Computation Across Different Datasets (Measured by MACs).</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Method</th>
<th align="left" colspan="1" rowspan="1">MACs (G)</th>
<th align="left" colspan="1" rowspan="1">Memory (MiB)</th>
<th align="left" colspan="1" rowspan="1">Params (M)</th>
<th align="left" colspan="1" rowspan="1">AUC (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq134"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/9a62/12365151/2543fae3df6f/d33e2352.gif" loading="lazy" id="d33e2352" alt="Inline graphic"></span>)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">ResNet18 (CT32)<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>0.48</strong>
</td>
<td align="left" colspan="1" rowspan="1">28.01</td>
<td align="left" colspan="1" rowspan="1">
<strong>33.16</strong>
</td>
<td align="left" colspan="1" rowspan="1">85.84</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ResNet18 (CT128)<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">7.88</td>
<td align="left" colspan="1" rowspan="1">34.08</td>
<td align="left" colspan="1" rowspan="1">
<strong>33.16</strong>
</td>
<td align="left" colspan="1" rowspan="1">83.86</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT (CT32)<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">5.55</td>
<td align="left" colspan="1" rowspan="1">
<strong>4.43</strong>
</td>
<td align="left" colspan="1" rowspan="1">85.42</td>
<td align="left" colspan="1" rowspan="1">80.97</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ViT (CT128)<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">87.54</td>
<td align="left" colspan="1" rowspan="1">141.23</td>
<td align="left" colspan="1" rowspan="1">85.42</td>
<td align="left" colspan="1" rowspan="1">52.41</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CLIP-Lung<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">58.71</td>
<td align="left" colspan="1" rowspan="1">70.19</td>
<td align="left" colspan="1" rowspan="1">36.42</td>
<td align="left" colspan="1" rowspan="1">85.60</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">TMSS<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">87.54</td>
<td align="left" colspan="1" rowspan="1">141.23</td>
<td align="left" colspan="1" rowspan="1">85.64</td>
<td align="left" colspan="1" rowspan="1">88.96</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MultiSurv<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">10.23</td>
<td align="left" colspan="1" rowspan="1">34.08</td>
<td align="left" colspan="1" rowspan="1">51.29</td>
<td align="left" colspan="1" rowspan="1">88.45</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">LLM-guided<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">53.22</td>
<td align="left" colspan="1" rowspan="1">33.42</td>
<td align="left" colspan="1" rowspan="1">37.02</td>
<td align="left" colspan="1" rowspan="1">90.47</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<strong>CMMFNet (Ours)</strong>
</td>
<td align="left" colspan="1" rowspan="1">94.11</td>
<td align="left" colspan="1" rowspan="1">141.73</td>
<td align="left" colspan="1" rowspan="1">284.42</td>
<td align="left" colspan="1" rowspan="1">
<strong>91.20</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par48">In the performance comparison, CMMFNet stands out, especially in achieving an AUC score of 91.20, outperforming all other models. Despite having relatively higher computational complexity, memory usage, and parameter count, these attributes contribute to its outstanding performance, particularly in managing complex tasks and enhancing classification accuracy. Models like TMSS and ViT (CT128) are similar to CMMFNet in terms of computational requirements, with MACs and memory usage figures of 87.54G and 141.23MiB, respectively. Nevertheless, they lag behind in AUC scores, achieving only 88.96 and 52.41 compared to CMMFNet’s superior results. Conversely, methods such as ResNet18, which demand less computational power, exhibit poor AUC performance when measured against CMMFNet. Consequently, CMMFNet makes efficient use of computational resources to deliver significant improvements across various performance metrics, underscoring its potential and advantages for real-world applications.</p></section></section><section id="Sec11"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par49">This paper introduces an innovative deep multimodal network, CMMFNet, designed to classify lung adenocarcinoma subtypes effectively. The network employs the CLIP module for feature extraction and contrastive learning, while the DFF module integrates features from different modalities. Comprehensive experiments on both in-house and public datasets show that CMMFNet outperforms several leading multimodal models across various metrics. By applying ROI-based feature extraction, the network can focus more effectively on the tumor areas, which is especially useful when dealing with adenocarcinoma subtypes that exhibit subtle histological differences. Contrastive learning further enhances the consistency between visual and text-based features, improving the model’s ability to align multimodal information. An attention-based feature fusion mechanism strengthens the coupling of features across different modalities, ensuring that the model can leverage both visual and textual data more effectively.</p>
<p id="Par50">Nevertheless, this study has several limitations. First, the method relies on initial annotations of lesion bounding boxes for CT cropping and ROI-based feature extraction. Accurate localization of the tumor is crucial for the network to capture both local and global information of the tumor areas. Second, in our approach to EHR data, we focused only on the values, without fully utilizing the semantic meaning of the EHR fields. Different EHR fields contain diverse indicators, and the network’s weights can only be applied to datasets that share the same EHR field structure. For datasets with different field configurations, additional training is required. However, training with additional fields necessitates a large amount of data, which can be a limiting factor. Future work will aim to improve lesion localization, facilitating automated image cropping and enabling the model to capture more detailed lesion features. We also plan to evolve the model into a more comprehensive end-to-end solution. Additionally, we will gather more EHR data to train on specific fields, improving the model’s ability to generate text feature representations in medical terms, ultimately enabling zero-shot transfer similar to CLIP.</p>
<p id="Par51">In conclusion, the proposed CMMFNet demonstrates reliability in accurately predicting lung adenocarcinoma subtypes. Additionally, its robust performance and integration of multimodal data highlight its potential as a valuable tool to assist doctors in clinical decision-making and treatment planning. In conclusion, the proposed CMMFNet demonstrates reliability in accurately predicting lung adenocarcinoma subtypes. Additionally, its robust performance and integration of multimodal data highlight its potential as a valuable tool to assist doctors in clinical decision-making and treatment planning.</p></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>This work was supported by Guangdong Basic and Applied Basic Research Foundation (No. 2025A1515011617, 2022A1515110570), the China Ministry of Education Humanities and Social Sciences Research-Planning Fund Project (No. 23YJA910007), Zhejiang Provincial Philosophy and Social Sciences Planning Project-Major Project (No. 40023SYS12ZD), First Class Discipline of Zhejiang - A (Zhejiang University of Finance and Economics-Statistics), Innovation Teams of Youth Innovation in Science and Technology of High Education Institutions of Shandong Province (No. 2021KJ088), Shenzhen Science and Technology Program (No.KCXFZ20201221173008022), Shenzhen High-level Hospital Construction Fund, Shenzhen Clinical Research Center for Cancer (No. 287).</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>Changmiao Wang: Conceptualization, Methodology, Software, Writing – review and editing, Funding acquisition. Lijian Liu: Formal analysis, Writing – review and editing. Chenchen Fan: Software, Validation, Writing – review and editing. Yifei Wang: Investigation. Yongquan Zhang: Project administration, Writing – review and editing, Funding acquisition, Supervisor. Zhijun Mai: Writing – review and editing, Data curation. Li Li: Conceptualization, Data curation, Resources. Zhou Liu: Supervisor, Conceptualization, Resources, Writing – review and editing, Funding acquisition. Yuan Tian: Data curation, Conceptualization. Jiahang Hu:Conceptualization, Data curation. Ahmed Elazab: Writing – review and editing, Methodology.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The datasets used and/or analysed during the current study available from the corresponding author on reasonable request.</p></section><section id="notes3"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par55">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="_ci93_" lang="en" class="contrib-info"><h2 class="pmc_sec_title">Contributor Information</h2>
<p>Yongquan Zhang, Email: zyq@zufe.edu.cn.</p>
<p>Zhou Liu, Email: zhou_liu8891@yeah.net.</p></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Lee, C. I., Haims, A. H., Monico, E. P., Brink, J. A. &amp; Forman, H. P. Diagnostic ct scans: assessment of patient, physician, and radiologist awareness of radiation dose and possible risks. <em>Radiology</em><strong>231</strong>, 393–398 (2004).
</cite> [<a href="https://doi.org/10.1148/radiol.2312030767" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/15031431/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Lee,%20C.%20I.,%20Haims,%20A.%20H.,%20Monico,%20E.%20P.,%20Brink,%20J.%20A.%20&amp;%20Forman,%20H.%20P.%20Diagnostic%20ct%20scans:%20assessment%20of%20patient,%20physician,%20and%20radiologist%20awareness%20of%20radiation%20dose%20and%20possible%20risks.%20Radiology231,%20393%E2%80%93398%20(2004)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Torre, L. A., Siegel, R. L. &amp; Jemal, A. Lung cancer statistics. <em>Lung Cancer and Personalized Medicine: Current Knowledge and Therapies</em> 1–19 (2016).</cite> [<a href="https://doi.org/10.1007/978-3-319-24223-1_1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26667336/" class="usa-link">PubMed</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Duma, N., Santana-Davila, R. &amp; Molina, J. R. Non-small cell lung cancer: epidemiology, screening, diagnosis, and treatment. <em>Mayo Clin. Proc.</em><strong>94</strong>, 1623–1640 (2019) (<strong>Elsevier</strong>).
</cite> [<a href="https://doi.org/10.1016/j.mayocp.2019.01.013" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31378236/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Duma,%20N.,%20Santana-Davila,%20R.%20&amp;%20Molina,%20J.%20R.%20Non-small%20cell%20lung%20cancer:%20epidemiology,%20screening,%20diagnosis,%20and%20treatment.%20Mayo%20Clin.%20Proc.94,%201623%E2%80%931640%20(2019)%20(Elsevier)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR4">
<span class="label">4.</span><cite>Relli, V., Trerotola, M., Guerra, E. &amp; Alberti, S. Abandoning the notion of non-small cell lung cancer. <em>Trends Mol. Med.</em><strong>25</strong>, 585–594 (2019).
</cite> [<a href="https://doi.org/10.1016/j.molmed.2019.04.012" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31155338/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Relli,%20V.,%20Trerotola,%20M.,%20Guerra,%20E.%20&amp;%20Alberti,%20S.%20Abandoning%20the%20notion%20of%20non-small%20cell%20lung%20cancer.%20Trends%20Mol.%20Med.25,%20585%E2%80%93594%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Travis, W. D., Brambilla, E., Burke, A. P., Marx, A. &amp; Nicholson, A. G. <em>WHO Classification of Tumours of the Lung, Pleura, Thymus and Heart</em>, vol. 7 of <em>World Health Organization Classification of Tumours</em> (International Agency for Research on Cancer (IARC), Lyon, France, 2015), 4th edn.</cite> [<a href="https://doi.org/10.1097/JTO.0000000000000663" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26291007/" class="usa-link">PubMed</a>]</li>
<li id="CR6">
<span class="label">6.</span><cite>Travis, W. D. et al. International association for the study of lung cancer/american thoracic society/european respiratory society international multidisciplinary classification of lung adenocarcinoma. <em>J. Thorac. Oncol.</em><strong>6</strong>, 244–285 (2011).
</cite> [<a href="https://doi.org/10.1097/JTO.0b013e318206a221" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4513953/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/21252716/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Travis,%20W.%20D.%20et%20al.%20International%20association%20for%20the%20study%20of%20lung%20cancer/american%20thoracic%20society/european%20respiratory%20society%20international%20multidisciplinary%20classification%20of%20lung%20adenocarcinoma.%20J.%20Thorac.%20Oncol.6,%20244%E2%80%93285%20(2011)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<span class="label">7.</span><cite>Bach, P. B. et al. Computed tomography screening and lung cancer outcomes. <em>JAMA</em><strong>297</strong>, 953–961 (2007).
</cite> [<a href="https://doi.org/10.1001/jama.297.9.953" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/17341709/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Bach,%20P.%20B.%20et%20al.%20Computed%20tomography%20screening%20and%20lung%20cancer%20outcomes.%20JAMA297,%20953%E2%80%93961%20(2007)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>Parekh, V. &amp; Jacobs, M. A. Radiomics: a new application from established techniques. <em>Expert Rev. Precis. Med. Drug Dev.</em><strong>1</strong>, 207–226 (2016).
</cite> [<a href="https://doi.org/10.1080/23808993.2016.1164013" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5193485/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28042608/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Parekh,%20V.%20&amp;%20Jacobs,%20M.%20A.%20Radiomics:%20a%20new%20application%20from%20established%20techniques.%20Expert%20Rev.%20Precis.%20Med.%20Drug%20Dev.1,%20207%E2%80%93226%20(2016)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>Dy, J. G. &amp; Brodley, C. E. Feature selection for unsupervised learning. <em>J. Mach. Learn. Res.</em><strong>5</strong>, 845–889 (2004).</cite> [<a href="https://scholar.google.com/scholar_lookup?Dy,%20J.%20G.%20&amp;%20Brodley,%20C.%20E.%20Feature%20selection%20for%20unsupervised%20learning.%20J.%20Mach.%20Learn.%20Res.5,%20845%E2%80%93889%20(2004)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR10">
<span class="label">10.</span><cite>Breiman, L. Random forests. <em>Mach. Learn.</em><strong>45</strong>, 5–32 (2001).</cite> [<a href="https://scholar.google.com/scholar_lookup?Breiman,%20L.%20Random%20forests.%20Mach.%20Learn.45,%205%E2%80%9332%20(2001)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>Hearst, M. A., Dumais, S. T., Osuna, E., Platt, J. &amp; Scholkopf, B. Support vector machines. <em>IEEE Intelligent Systems and Their Applications</em><strong>13</strong>, 18–28 (1998).</cite> [<a href="https://scholar.google.com/scholar_lookup?Hearst,%20M.%20A.,%20Dumais,%20S.%20T.,%20Osuna,%20E.,%20Platt,%20J.%20&amp;%20Scholkopf,%20B.%20Support%20vector%20machines.%20IEEE%20Intelligent%20Systems%20and%20Their%20Applications13,%2018%E2%80%9328%20(1998)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Nelder, J. A. &amp; Wedderburn, R. W. Generalized linear models. <em>J. R. Stat. Soc. Ser. A Stat. Soc.</em><strong>135</strong>, 370–384 (1972).</cite> [<a href="https://scholar.google.com/scholar_lookup?Nelder,%20J.%20A.%20&amp;%20Wedderburn,%20R.%20W.%20Generalized%20linear%20models.%20J.%20R.%20Stat.%20Soc.%20Ser.%20A%20Stat.%20Soc.135,%20370%E2%80%93384%20(1972)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>Guo, G., Wang, H., Bell, D., Bi, Y. &amp; Greer, K. Knn model-based approach in classification. In <em>On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE: OTM Confederated International Conferences, CoopIS, DOA, and ODBASE 2003, Catania, Sicily, Italy, November 3-7, 2003. Proceedings</em>, 986–996 (Springer, 2003).</cite>
</li>
<li id="CR14">
<span class="label">14.</span><cite>Zhang, Y., Oikonomou, A., Wong, A., Haider, M. A. &amp; Khalvati, F. Radiomics-based prognosis analysis for non-small cell lung cancer. <em>Sci. Rep.</em><strong>7</strong>, 46349 (2017).
</cite> [<a href="https://doi.org/10.1038/srep46349" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5394465/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28418006/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20Y.,%20Oikonomou,%20A.,%20Wong,%20A.,%20Haider,%20M.%20A.%20&amp;%20Khalvati,%20F.%20Radiomics-based%20prognosis%20analysis%20for%20non-small%20cell%20lung%20cancer.%20Sci.%20Rep.7,%2046349%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR15">
<span class="label">15.</span><cite>Hua, K.-L., Hsu, C.-H., Hidayati, S. C., Cheng, W.-H. &amp; Chen, Y.-J. Computer-aided classification of lung nodules on computed tomography images via deep learning technique. <em>OncoTargets and Therapy</em> 2015–2022 (2015).</cite> [<a href="https://doi.org/10.2147/OTT.S80733" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC4531007/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/26346558/" class="usa-link">PubMed</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–778 (2016).</cite>
</li>
<li id="CR17">
<span class="label">17.</span><cite>Vaswani, A., Shazeer, N., Parmar, N. et al. Attention is all you need. In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, vol. 30 (2017).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>Dosovitskiy, A. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint <a href="http://arxiv.org/abs/2010.11929" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv: 2010.11929</a> (2020).</cite>
</li>
<li id="CR19">
<span class="label">19.</span><cite>Radford, A., Kim, J. W., Hallacy, C. et al. Learning transferable visual models from natural language supervision. In <em>International Conference on Machine Learning (ICML)</em>, 8748–8763 (PMLR, 2021).</cite>
</li>
<li id="CR20">
<span class="label">20.</span><cite>Kumaran S, Y., Jeya, J. J., Khan, S. B., Alzahrani, S. &amp; Alojail, M. Explainable lung cancer classification with ensemble transfer learning of vgg16, resnet50 and inceptionv3 using grad-cam. <em>BMC Medical Imaging</em><strong>24</strong>, 176 (2024).</cite> [<a href="https://doi.org/10.1186/s12880-024-01345-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11264852/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39030496/" class="usa-link">PubMed</a>]</li>
<li id="CR21">
<span class="label">21.</span><cite>Lakshmanaprabu, S., Mohanty, S. N., Shankar, K., Arunkumar, N. &amp; Ramirez, G. Optimal deep learning model for classification of lung cancer on ct images. <em>Future Gener. Comput. Syst.</em><strong>92</strong>, 374–382 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Lakshmanaprabu,%20S.,%20Mohanty,%20S.%20N.,%20Shankar,%20K.,%20Arunkumar,%20N.%20&amp;%20Ramirez,%20G.%20Optimal%20deep%20learning%20model%20for%20classification%20of%20lung%20cancer%20on%20ct%20images.%20Future%20Gener.%20Comput.%20Syst.92,%20374%E2%80%93382%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Wang, C., Elazab, A., Jia, F., Wu, J. &amp; Hu, Q. Automated chest screening based on a hybrid model of transfer learning and convolutional sparse denoising autoencoder. <em>BioMed. Eng. Online</em><strong>17</strong>, 1–19 (2018).
</cite> [<a href="https://doi.org/10.1186/s12938-018-0496-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5966927/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/29792208/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wang,%20C.,%20Elazab,%20A.,%20Jia,%20F.,%20Wu,%20J.%20&amp;%20Hu,%20Q.%20Automated%20chest%20screening%20based%20on%20a%20hybrid%20model%20of%20transfer%20learning%20and%20convolutional%20sparse%20denoising%20autoencoder.%20BioMed.%20Eng.%20Online17,%201%E2%80%9319%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR23">
<span class="label">23.</span><cite>Zhou, J. et al. An ensemble deep learning model for risk stratification of invasive lung adenocarcinoma using thin-slice ct. <em>npj Digit. Med.</em><strong>6</strong>, 119 (2023).
</cite> [<a href="https://doi.org/10.1038/s41746-023-00866-z" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10322969/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37407729/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Zhou,%20J.%20et%20al.%20An%20ensemble%20deep%20learning%20model%20for%20risk%20stratification%20of%20invasive%20lung%20adenocarcinoma%20using%20thin-slice%20ct.%20npj%20Digit.%20Med.6,%20119%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Luo, Y. et al. Carl: cross-aligned representation learning for multi-view lung cancer histology classification. In <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</em>, 358–367 (Springer, 2023).</cite>
</li>
<li id="CR25">
<span class="label">25.</span><cite>Wang, F. et al. Comparison and fusion prediction model for lung adenocarcinoma with micropapillary and solid pattern using clinicoradiographic, radiomics and deep learning features. <em>Sci. Rep.</em><strong>13</strong>, 9302 (2023).
</cite> [<a href="https://doi.org/10.1038/s41598-023-36409-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10250309/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37291251/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wang,%20F.%20et%20al.%20Comparison%20and%20fusion%20prediction%20model%20for%20lung%20adenocarcinoma%20with%20micropapillary%20and%20solid%20pattern%20using%20clinicoradiographic,%20radiomics%20and%20deep%20learning%20features.%20Sci.%20Rep.13,%209302%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<span class="label">26.</span><cite>Agarwal, R., Ghosal, P., Murmu, N. &amp; Nandi, D. Spiking neural network in computer vision: Techniques, tools and trends. In <em>International Conference on Advanced Computational and Communication Paradigms</em>, 201–209 (Springer, 2023).</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Agarwal, R. et al. Deep quasi-recurrent self-attention with dual encoder-decoder in biomedical ct image segmentation. <em>IEEE J. Biomed. Health Inform.</em> (2024).</cite> [<a href="https://doi.org/10.1109/JBHI.2024.3447689" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39172619/" class="usa-link">PubMed</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Agarwal, R., Ghosal, P., Sadhu, A. K., Murmu, N. &amp; Nandi, D. Multi-scale dual-channel feature embedding decoder for biomedical image segmentation. <em>Comput. Methods Programs Biomed.</em><strong>257</strong>, 108464 (2024).
</cite> [<a href="https://doi.org/10.1016/j.cmpb.2024.108464" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39447437/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Agarwal,%20R.,%20Ghosal,%20P.,%20Sadhu,%20A.%20K.,%20Murmu,%20N.%20&amp;%20Nandi,%20D.%20Multi-scale%20dual-channel%20feature%20embedding%20decoder%20for%20biomedical%20image%20segmentation.%20Comput.%20Methods%20Programs%20Biomed.257,%20108464%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29">
<span class="label">29.</span><cite>Mandal, B. Optimization of quadratic curve fitting from data points using real coded genetic algorithm. In <em>Emerging Technologies in Data Mining and Information Security: Proceedings of IEMIS 2020, Volume 1</em>, 419–428 (Springer, 2021).</cite>
</li>
<li id="CR30">
<span class="label">30.</span><cite>Yang, Y., Gao, Y., Lu, F., Wang, E. &amp; Liu, H. Correlation of ct features of lung adenocarcinoma with sex and age. <em>Sci. Rep.</em><strong>14</strong>, 13414 (2024).
</cite> [<a href="https://doi.org/10.1038/s41598-024-64335-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11167049/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38862598/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Yang,%20Y.,%20Gao,%20Y.,%20Lu,%20F.,%20Wang,%20E.%20&amp;%20Liu,%20H.%20Correlation%20of%20ct%20features%20of%20lung%20adenocarcinoma%20with%20sex%20and%20age.%20Sci.%20Rep.14,%2013414%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Yu, X. et al. Ichpro: Intracerebral hemorrhage prognosis classification via joint-attention fusion-based 3d cross-modal network. In <em>2024 IEEE International Symposium on Biomedical Imaging (ISBI)</em>, 1–5 (2024).</cite>
</li>
<li id="CR32">
<span class="label">32.</span><cite>Guo, Z. et al. Pe-mvcnet: Multi-view and cross-modal fusion network for pulmonary embolism prediction. In <em>2024 IEEE International Symposium on Biomedical Imaging (ISBI)</em>, 1–5 (2024).</cite>
</li>
<li id="CR33">
<span class="label">33.</span><cite>Ding, W., Wang, J., Huang, J., Cheng, C. &amp; Jiang, S. Mfca: Collaborative prediction algorithm of brain age based on multimodal fuzzy feature fusion. <em>Inf. Sci.</em><strong>687</strong>, 121376 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ding,%20W.,%20Wang,%20J.,%20Huang,%20J.,%20Cheng,%20C.%20&amp;%20Jiang,%20S.%20Mfca:%20Collaborative%20prediction%20algorithm%20of%20brain%20age%20based%20on%20multimodal%20fuzzy%20feature%20fusion.%20Inf.%20Sci.687,%20121376%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR34">
<span class="label">34.</span><cite>Wang, C., Elazab, A., Wu, J. &amp; Hu, Q. Lung nodule classification using deep feature fusion in chest radiography. <em>Comput. Med. Imaging Graph.</em><strong>57</strong>, 10–18 (2017).
</cite> [<a href="https://doi.org/10.1016/j.compmedimag.2016.11.004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27986379/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wang,%20C.,%20Elazab,%20A.,%20Wu,%20J.%20&amp;%20Hu,%20Q.%20Lung%20nodule%20classification%20using%20deep%20feature%20fusion%20in%20chest%20radiography.%20Comput.%20Med.%20Imaging%20Graph.57,%2010%E2%80%9318%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Vale-Silva, L. A. &amp; Rohr, K. Long-term cancer survival prediction using multimodal deep learning. <em>Sci. Rep.</em><strong>11</strong>, 13505 (2021).
</cite> [<a href="https://doi.org/10.1038/s41598-021-92799-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8242026/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34188098/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Vale-Silva,%20L.%20A.%20&amp;%20Rohr,%20K.%20Long-term%20cancer%20survival%20prediction%20using%20multimodal%20deep%20learning.%20Sci.%20Rep.11,%2013505%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<span class="label">36.</span><cite>Xie, S., Girshick, R., Dollár, P., Tu, Z. &amp; He, K. Aggregated residual transformations for deep neural networks. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 1492–1500 (2017).</cite>
</li>
<li id="CR37">
<span class="label">37.</span><cite>Saeed, N., Sobirov, I., Al Majzoub, R. &amp; Yaqub, M. Tmss: an end-to-end transformer-based multimodal network for segmentation and survival prediction. In <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</em>, 319–329 (Springer, 2022).</cite>
</li>
<li id="CR38">
<span class="label">38.</span><cite>Lei, Y., Li, Z., Shen, Y., Zhang, J. &amp; Shan, H. Clip-lung: Textual knowledge-guided lung nodule malignancy prediction. In <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</em>, 403–412 (Springer, 2023).</cite>
</li>
<li id="CR39">
<span class="label">39.</span><cite>Kim, K. et al. LLM-guided Multi-modal Multiple Instance Learning for 5-year Overall Survival Prediction of Lung Cancer . In <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</em>, vol. LNCS 15003 (Springer Nature Switzerland, 2024).</cite>
</li>
<li id="CR40">
<span class="label">40.</span><cite>Cardoso, M. J. et al. Monai: An open-source framework for deep learning in healthcare. arXiv preprint <a href="http://arxiv.org/abs/2211.02701" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2211.02701</a> (2022).</cite>
</li>
<li id="CR41">
<span class="label">41.</span><cite>Lin, T.-Y., Goyal, P., Girshick, R., He, K. &amp; Dollár, P. Focal loss for dense object detection. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2980–2988 (2017).</cite>
</li>
<li id="CR42">
<span class="label">42.</span><cite>Li, P. et al. A large-scale ct and pet/ct dataset for lung cancer diagnosis [dataset]. <em>The Cancer Imaging Archive</em><strong>10</strong> (2020).</cite>
</li>
<li id="CR43">
<span class="label">43.</span><cite>Yushkevich, P. A. et al. User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability. <em>Neuroimage</em><strong>31</strong>, 1116–1128 (2006).
</cite> [<a href="https://doi.org/10.1016/j.neuroimage.2006.01.015" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/16545965/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Yushkevich,%20P.%20A.%20et%20al.%20User-guided%203D%20active%20contour%20segmentation%20of%20anatomical%20structures:%20Significantly%20improved%20efficiency%20and%20reliability.%20Neuroimage31,%201116%E2%80%931128%20(2006)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR44">
<span class="label">44.</span><cite>Student. The probable error of a mean. <em>Biometrika</em> 1–25 (1908).</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets used and/or analysed during the current study available from the corresponding author on reasonable request.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-13818-2"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_13818.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (3.3 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12365151/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12365151/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12365151%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12365151/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12365151/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12365151/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40830161/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12365151/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40830161/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12365151/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12365151/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="VfhUQFrIpOdialoVCoAw2PKBTAwxYfTxVgD8Uss9MEmSlXoRaWz8PNiy5NnZrGwM">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
