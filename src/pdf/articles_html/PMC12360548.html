
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4BDC78AF455E305BDC70001B05067.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12360548/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery">
<meta name="citation_author" content="Alessandro Pistola">
<meta name="citation_author_institution" content="Department of Computer Science and Engineering, University of Bologna, Bologna, Italy">
<meta name="citation_author" content="Valentina Orrù">
<meta name="citation_author_institution" content="Department of History and Cultures, University of Bologna, Italy">
<meta name="citation_author" content="Nicolò Marchetti">
<meta name="citation_author_institution" content="Department of History and Cultures, University of Bologna, Italy">
<meta name="citation_author" content="Marco Roccetti">
<meta name="citation_author_institution" content="Department of Computer Science and Engineering, University of Bologna, Bologna, Italy">
<meta name="citation_publication_date" content="2025 Aug 18">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0330419">
<meta name="citation_doi" content="10.1371/journal.pone.0330419">
<meta name="citation_pmid" content="40824971">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12360548/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12360548/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12360548/pdf/pone.0330419.pdf">
<meta name="description" content="By upgrading an existing deep learning model with the knowledge provided by one of the oldest sets of grayscale satellite imagery, known as CORONA, we improved the AI model’s attitude towards the automatic identification of archaeological sites in ...">
<meta name="og:title" content="AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="By upgrading an existing deep learning model with the knowledge provided by one of the oldest sets of grayscale satellite imagery, known as CORONA, we improved the AI model’s attitude towards the automatic identification of archaeological sites in ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12360548/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12360548">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0330419"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0330419.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12360548%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12360548/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12360548/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12360548/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0330419" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 18;20(8):e0330419. doi: <a href="https://doi.org/10.1371/journal.pone.0330419" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330419</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Pistola%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Alessandro Pistola</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Alessandro Pistola</span></h3>
<div class="p">
<sup>1</sup>Department of Computer Science and Engineering, University of Bologna, Bologna, Italy</div>
<div>Data curation, Formal analysis, Software, Visualization, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Pistola%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Alessandro Pistola</span></a>
</div>
</div>
<sup>1,</sup><sup>#</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Orr%C3%B9%20V%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Valentina Orrù</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Valentina Orrù</span></h3>
<div class="p">
<sup>2</sup>Department of History and Cultures, University of Bologna, Italy</div>
<div>Data curation, Investigation, Visualization, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Orr%C3%B9%20V%22%5BAuthor%5D" class="usa-link"><span class="name western">Valentina Orrù</span></a>
</div>
</div>
<sup>2,</sup><sup>#</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Marchetti%20N%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Nicolò Marchetti</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Nicolò Marchetti</span></h3>
<div class="p">
<sup>2</sup>Department of History and Cultures, University of Bologna, Italy</div>
<div>Conceptualization, Funding acquisition, Investigation, Methodology, Supervision, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Marchetti%20N%22%5BAuthor%5D" class="usa-link"><span class="name western">Nicolò Marchetti</span></a>
</div>
</div>
<sup>2,</sup><sup>‡</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Roccetti%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Marco Roccetti</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Marco Roccetti</span></h3>
<div class="p">
<sup>1</sup>Department of Computer Science and Engineering, University of Bologna, Bologna, Italy</div>
<div>Conceptualization, Methodology, Supervision, Writing – original draft, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Roccetti%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Marco Roccetti</span></a>
</div>
</div>
<sup>1,</sup><sup>‡,</sup><sup>*</sup>
</div>
<div class="cg p">Editor: <span class="name western">Shai Gordin</span><sup>3</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>Department of Computer Science and Engineering, University of Bologna, Bologna, Italy</div>
<div id="aff002">
<sup>2</sup>Department of History and Cultures, University of Bologna, Italy</div>
<div id="edit1">
<sup>3</sup>Ariel University, ISRAEL</div>
<div class="author-notes p">
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>The authors have declared that no competing interests exist.</p></div>
<div class="fn" id="econtrib001"><p>‡ These authors also contributed equally to this work.</p></div>
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>marco.roccetti@unibo.it</span></p>
</div>
<div class="fn" id="_eqcntrb93pmc__">
<sup>#</sup><p class="display-inline">Contributed equally.</p>
</div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Alessandro Pistola</span></strong>: <span class="role">Data curation, Formal analysis, Software, Visualization, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Valentina Orrù</span></strong>: <span class="role">Data curation, Investigation, Visualization, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Nicolò Marchetti</span></strong>: <span class="role">Conceptualization, Funding acquisition, Investigation, Methodology, Supervision, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Marco Roccetti</span></strong>: <span class="role">Conceptualization, Methodology, Supervision, Writing – original draft, Writing – review &amp; editing</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Shai Gordin</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Oct 18; Accepted 2025 Jul 30; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Pistola et al</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12360548  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40824971/" class="usa-link">40824971</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>By upgrading an existing deep learning model with the knowledge provided by one of the oldest sets of grayscale satellite imagery, known as CORONA, we improved the AI model’s attitude towards the automatic identification of archaeological sites in an environment which has been completely transformed in the last five decades, including the complete destruction of many of those same sites. The initial Bing-based convolutional network model was re-trained using CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad, central Mesopotamian floodplain. The results were twofold and surprising. First, the detection precision obtained on the area of interest increased sensibly: in particular, the <em>Intersection-over-Union</em> (IoU) values, at the image segmentation level, surpassed 85%, while the general accuracy in detecting archeological sites reached 90%. Second, our re-trained model allowed the identification of four new sites of archaeological interest (confirmed through field verification), previously not identified by archaeologists with traditional techniques. This has confirmed the efficacy of using AI techniques and the CORONA imagery from the 1960s to discover archaeological sites currently no longer visible, a concrete breakthrough with significant consequences for the study of landscapes with vanishing archaeological evidence induced by anthropization.</p></section><section id="sec001"><h2 class="pmc_sec_title">Introduction</h2>
<p>For the study of archaeological landscapes in the Near East and the reconstruction of settlement patterns therein, the primary objective is the identification, localization and chronological characterization of ancient settlements: those considered in this analysis are predominantly tells, both because they represent the most widespread type of ancient settlement evidence in the region under study and because of their high visibility starting already from preliminary remote sensing approaches. A tell is an artificial mound created by the accumulated debris from centuries of human habitation. These mounds typically form in regions such as the Mesopotamian floodplain, where communities repeatedly built and rebuilt their settlements at the same site with perishable material. Prior to any field verification, the very first step for site identification is remote sensing, which has been a key resource for archaeologists for many years, being a non-invasive method that contributes to the detection and preservation of cultural heritage, it requires, however, a large amount of expert human work and consequently requires a significant amount of time [<a href="#pone.0330419.ref001" class="usa-link" aria-describedby="pone.0330419.ref001">1</a>–<a href="#pone.0330419.ref006" class="usa-link" aria-describedby="pone.0330419.ref006">6</a>]. It goes without saying that using deep learning techniques as a support to human efforts could open new perspectives. For example, deep learning abilities can be put to good use for automatically analyzing satellite imagery, especially using a technique called <em>semantic segmentation</em> which, in simple words, consists of assigning a class label to each pixel in an image, up to a point where the entire image is recognized as easily interpretable by archaeologists. Several works in this field have confirmed the efficacy of this approach [<a href="#pone.0330419.ref007" class="usa-link" aria-describedby="pone.0330419.ref007">7</a>–<a href="#pone.0330419.ref010" class="usa-link" aria-describedby="pone.0330419.ref010">10</a>]. Finally, it is worth mentioning that since the time when we began our research activity in the area of Deep Learning applied to archaeology, many other similar initiatives have taken shape and have been brought to the forefront of scientific discussion. We are well aware that a wealth of studies have made relevant advances in this field [<a href="#pone.0330419.ref011" class="usa-link" aria-describedby="pone.0330419.ref011">11</a>–<a href="#pone.0330419.ref014" class="usa-link" aria-describedby="pone.0330419.ref014">14</a>]. In order to avoid any misunderstanding, we should state at the onset that here are (at least) two axes along which specialists can look at similarities or differences in these kinds of research. The first one concerns the remote sensing methodologies employed to gather the data on which various Artificial Intelligence techniques can work. Using passive or active approaches, satellites or aircrafts, radar or LiDAR or any other typology of sensors emitting their own signals, or a combination of some of them, constitutes a relevant difference that extends till the point of a distinction (or divergence) in the meaning of the results which can be obtained, irrespective of the type of data, since working with new rather than old (e.g., CORONA, in our specific case) high resolution imagery represents another important source of differentiation. The second axis is that of the distinction between Machine Learning (ML) and Deep Learning (DL). ML are algorithms that learn from <em>structured</em> data to predict outputs and discover patterns in that data. DL, instead, is always based on highly complex neural networks that mimic the way a human brain works to detect patterns in large <em>unstructured</em> data (like images). A traditional ML algorithm can be something as simple as linear regression or a search in a decisional tree, the driving force behind being often that of ordinary statistics. DL algorithms, instead, should be regarded as a sophisticated and mathematically complex evolution of ML. To achieve this result, DL mechanisms use a layered structure of algorithms, called artificial neural networks, with a specific design based on a cascade of several different computational blocks, inspired by the biological neural network of the human brain, and leading to a process of learning that is far more capable than that of standard ML models. It should be also considered that with DL one can often fall into an excess of inference to which it is difficult (even not possible) to give a formal explanation. An extension of this discussion, tailored to the archaeological field, has been reported elsewhere [<a href="#pone.0330419.ref015" class="usa-link" aria-describedby="pone.0330419.ref015">15</a>]. Consequently, applying either ML or DL makes an important difference, being often unfair (or nonsensical), in the light of the explanation above, a comparison between studies that adopt different approaches. Given these premises, we look at the wealth of researches that have investigated how well AI techniques can work in the archaeological field, not with the aim of conducting one-to-one comparisons with specific papers that could have followed different approaches, but rather with the responsibility of witnessing the level of productivity of the entire community in this specific field.</p>
<section id="sec002"><h3 class="pmc_sec_title">Our previous work</h3>
<p>Building on a long-term scientific collaboration between AI-ers and archaeologists at the University of Bologna, Italy [<a href="#pone.0330419.ref016" class="usa-link" aria-describedby="pone.0330419.ref016">16</a>–<a href="#pone.0330419.ref019" class="usa-link" aria-describedby="pone.0330419.ref019">19</a>], a deep learning model was recently proposed, enhanced with segmentation and self-attention mechanisms, which was able to detect mounded archaeological sites in the Mesopotamian floodplain in southern Iraq. A set of modern (Bing-based) georeferenced vector shapes were used as data source, corresponding to the outlines of the previously mentioned <em>tells</em> and surrounding areas, totaling 4934 shapefiles in the southern Mesopotamian floodplain, which constitute the entirety of the surveyed and published sites in the area [Floodplains Project; 15]. Each image in that dataset was subjected to a variety of image manipulation techniques (including, for example, random rotation, mirroring, brightness and contrast correction) and it was then given as an input to train a convolutional neural network, augmented with segmentation and self-attention mechanisms. In the end, the result of this activity was a deep learning model able to detect archaeological sites in the area of interest, which achieved during a test on a set of already known sites an <em>Intersection Over Union</em> (IoU) score of 0.81, with a general accuracy in the neighborhood of 80%. This first work had, nonetheless, two important limitations. First, an initial attempt to exploit CORONA satellite imagery was unsuccessful, probably due to our inability to integrate this panchromatic imagery with full color pictures. Second, the entire testing activity was conducted on hundreds of already known archaeological sites, without the possibility of challenging the machine predictions on sites not already groundtruthed. Computer scientists’ wish, instead, would have been to make those automatic predictions on sites not already certified as tells and, upon confirmation by the archaeologists, subjecting them to a process of ground-truthing, to understand in reality whether those predictions were accurate or not.</p></section><section id="sec003"><h3 class="pmc_sec_title">New developments</h3>
<p>The study area selected for this research is the district of Abu Ghraib, located in central Iraq within the Baghdad Governorate. This region, which had never been the subject of systematic archaeological investigations, except for its easternmost edge explored by Robert McCormick Adams [<a href="#pone.0330419.ref020" class="usa-link" aria-describedby="pone.0330419.ref020">20</a>], was also chosen due to the observed high degree of landscape transformation over recent decades. This contribution is part of a broader landscape archaeology project grounded in well-established traditional methodologies, including conventional remote sensing [<a href="#pone.0330419.ref021" class="usa-link" aria-describedby="pone.0330419.ref021">21</a>,<a href="#pone.0330419.ref022" class="usa-link" aria-describedby="pone.0330419.ref022">22</a>], field validation with collection and study of associated archaeological evidence, spatial analysis, integrated with AI-based approaches. Between 2023 and 2024, a systematic field survey was carried out with the aim of verifying on the ground all potential sites first identified through remote sensing, both by conventional analysis and by AI-based predictive models. The field investigations confirmed the presence or absence of archaeological remains and allowed, when necessary, for the refinement of site boundaries.</p>
<p>Since the inception of the project, a central role was played by the assessment of threats to the archaeological heritage. This analysis, a key component of the traditional methodological framework, was conducted by comparing recent satellite imagery with historical CORONA images acquired between 1960 and 1972 through the U.S. reconnaissance program [<a href="#pone.0330419.ref023" class="usa-link" aria-describedby="pone.0330419.ref023">23</a>–<a href="#pone.0330419.ref028" class="usa-link" aria-describedby="pone.0330419.ref028">28</a>]. The remote sensing analysis systematically documented landscape transformations, identifying major causes of site damage such as agricultural expansion, canal digging, and urban encroachment. These preliminary findings were subsequently validated and further investigated through fieldwork, which allowed for a more precise assessment of the current condition of each site. The results revealed that 38% of sites had been completely destroyed, 23% had lost more than half of their original extent, and only 38% retained more than half of their surface area, as it will be detailed in the final Section of this manuscript<em>.</em> As a direct consequence of these findings, a methodological decision was made to focus the training of AI-based predictive models on CORONA imagery, as the only source capable of capturing the archaeological landscape before extensive modern transformations. In essence, the study presented here thus focuses on the use of CORONA imagery to improve the performance of AI-based predictive models. Our previous convolutional neural network was retrained using transfer learning techniques and subjected to a two-stage fine-tuning process, resulting in three distinct configurations: (1) one based exclusively on Bing imagery, (2) one using only CORONA imagery, and (3) a combined configuration. The validation of results was carried out in two phases: the first on known sites and areas without archaeological evidence, and the second on new predictions generated by the models, which were also verified through fieldwork. The findings demonstrate that the integration of historical imagery significantly enhances the ability of AI models to detect archaeological sites, confirming the effectiveness of an approach that combines historical sources, technological innovation, and traditional archaeological methods in the reconstruction of historical Mesopotamian landscapes.</p></section></section><section id="sec004"><h2 class="pmc_sec_title">Materials and methods</h2>
<p>We first describe the data used in our study, and then we illustrate the methods employed to build our AI models (for accessing all the developed software and the data used in this study, see the Section: Data Availability Statement).</p>
<section id="sec005"><h3 class="pmc_sec_title">Data</h3>
<p>We begin by noticing that all the remote sensing operations we carried out to identify archaeological evidence and to extract the usable data were conducted on the basis of various publicly available basemaps, specifically: current (Google, Bing, and Esri) and historical satellite imagery (CORONA), and topographic maps (US Army 1:100,000 from 1942). During this phase, 88 potential tells were identified, recorded as vector shapefiles and classified with the abbreviation <em>GHR</em> (i.e., the initials of the geographical district of interest: Abu Ghraib) and a sequential number. Starting from that information, the image creation process was based on the following five steps: <strong>i)</strong> all the shapefiles of the area of interest were imported from Bing and CORONA basemaps into an open-source GIS software [QGIS; [<a href="#pone.0330419.ref029" class="usa-link" aria-describedby="pone.0330419.ref029">29</a>]], <strong>ii)</strong> sample squares each 2000 meters long, centered on the centroid of any given shapefile, were extracted from those images using a Python script developed by us (this was done in the same way both for Bing, through the QuickMapService plugin, and for the CORONA imagery, via the free services provided by the University of Arkansas’ Center for Advanced Spatial Technologies). At that point, <strong>iii)</strong> we generated the truth masks, that is the masks that put in evidence, at a pixel level, the points either included in a tell or not. After that phase, we were in the obvious situation of having an unbalanced dataset, with a prevalence of non-empty truth masks. To fill this gap, additional images, with an empty truth mask, were generated, to balance the dataset. This was done, <strong>iv)</strong> by choosing 120 random points in the area of interest (with relative surrounding images, not containing any tell). The final dataset consisted of 88 images (around 41%) each including a <em>tell</em>, and 120 images (almost 59%) not portraying any tell or its parts, totaling a final amount of 208 pictures, on which a training activity could be conducted. Nonetheless, given the relatively small size of this dataset, <strong>v)</strong> an <em>aggressive</em> data augmentation procedure was exploited that has prevented the overfitting phenomenon. In particular, using the Albumentations library [<a href="#pone.0330419.ref030" class="usa-link" aria-describedby="pone.0330419.ref030">30</a>], three subsequent transformations (geometric, color space and kernel filters) were applied to all the images (Bing, CORONA and truth masks), where each transformation was chosen, in turn, from one of the three separate groups shown in <a href="#pone.0330419.t001" class="usa-link">Table 1</a>, with a given probability. <a href="#pone.0330419.g001" class="usa-link">Fig 1</a> provides three examples of such a pipelined image augmentation process, where the transformations (RandomCrop, Flip, RandomRotate90, GaussNoise, Sharpen, Resize), (RandomCrop, Flip, RandomRotate90, CLAHE, GaussNoise, Sharpen, Resize) and (RandomCrop, Flip, RandomRotate90, RandomBrightnessContrast, MotionBlur, Sharpen, Resize) were applied in the reported cases following that exact sequence.</p>
<section class="tw xbox font-sm" id="pone.0330419.t001"><h4 class="obj_head">Table 1. Data augmentation pipeline.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Group</th>
<th align="left" rowspan="1" colspan="1">Technique</th>
<th align="left" rowspan="1" colspan="1">Probability of use</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">RandomCrop</td>
<td align="left" rowspan="1" colspan="1">1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">Flip</td>
<td align="left" rowspan="1" colspan="1">0.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">RandomRotate90</td>
<td align="left" rowspan="1" colspan="1">0.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Geometric</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">0.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">GridDistorsion</td>
<td align="left" rowspan="1" colspan="1">0.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">RandomGridShuffle</td>
<td align="left" rowspan="1" colspan="1">0.6</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Color space</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">0.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">CLAHE</td>
<td align="left" rowspan="1" colspan="1">0.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">RandomBrightnessContrast</td>
<td align="left" rowspan="1" colspan="1">0.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">ChannelShuffle</td>
<td align="left" rowspan="1" colspan="1">0.1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">ColorJitter</td>
<td align="left" rowspan="1" colspan="1">0.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">HueSaturationValue</td>
<td align="left" rowspan="1" colspan="1">0.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Kernel filters</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">0.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">Blur</td>
<td align="left" rowspan="1" colspan="1">0.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">GaussNoise</td>
<td align="left" rowspan="1" colspan="1">0.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">MotionBlur</td>
<td align="left" rowspan="1" colspan="1">0.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">Sharpen</td>
<td align="left" rowspan="1" colspan="1">0.1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">Resize</td>
<td align="left" rowspan="1" colspan="1">1</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330419.t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="pone.0330419.g001"><h4 class="obj_head">Fig 1. Image augmentation pipeline: an example.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12360548_pone.0330419.g001.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ef05/12360548/f64c20215371/pone.0330419.g001.jpg" loading="lazy" height="160" width="704" alt="Fig 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330419.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Disclaimer</strong>: All the displayed data fall under the condition of fair use utilization of geographical data for academic purposes. The list of all relevant data/software provider(s) is as follows: (i) satellite imagery is based on Copernicus Sentinel-2 data, freely available under the European Union’s open data policy (<a href="https://www.copernicus.eu/en/access-data/copernicus-open-access-hub" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.copernicus.eu/en/access-data/copernicus-open-access-hub</a>); (ii) maps display achieved with open source software, under the GNU licenses of QGIS (<a href="https://qgis.org/en/site/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://qgis.org/en/site/</a>) and QuickMapServices (<a href="https://github.com/nextgis/quickmapservices" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nextgis/quickmapservices</a>); (iii) final maps elaboration achieved with a software developed by the authors and available at (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></figcaption></figure></section><section id="sec006"><h3 class="pmc_sec_title">Methods</h3>
<p>The type of convolutional neural network and the method used to train it were similar to those adopted in our previous works [<a href="#pone.0330419.ref018" class="usa-link" aria-describedby="pone.0330419.ref018">18</a>,<a href="#pone.0330419.ref019" class="usa-link" aria-describedby="pone.0330419.ref019">19</a>]. Nevertheless, while a detailed description may be found there, for the benefit of readers one may remind as follows: our work started by using the PyTorch Segmentation Models library and by defining a Deep Convolutional Neural Network (DCNN) model, called MANet. Its complex architecture is summarized in <a href="#pone.0330419.g002" class="usa-link">Fig 2</a>. A MANet (Multi-scale Attention Network) is a deep-learning neural network tailored to learn robust and discriminative features from high resolution (remote sensing) images. It stacks various multi-scale attention blocks, aiming at reducing spatial and channel redundancy to accelerate convolution. As shown in <a href="#pone.0330419.g002" class="usa-link">Fig 2</a>, it is composed of three main blocks that act in sequence, one after another: an encoder, a decoder and a segmentation head. The encoder, represented by the topmost block in the central box of <a href="#pone.0330419.g002" class="usa-link">Fig 2</a>, constitutes a proper convolutional architecture, based on the well-known Efficientnet-b3 model [<a href="#pone.0330419.ref031" class="usa-link" aria-describedby="pone.0330419.ref031">31</a>,<a href="#pone.0330419.ref032" class="usa-link" aria-describedby="pone.0330419.ref032">32</a>]. Input to this encoder are high resolution images (leftmost box of <a href="#pone.0330419.g002" class="usa-link">Fig 2</a>), with a given number of channels (as satellite sensors can collect images at various regions of the electromagnetic spectrum) and a corresponding spatial resolution expressed as a matrix of pixels (n x n). This imagery passes, within the encoder, through a cascade of multiple sub-blocks, which implement a convolution procedure, followed by subsequent operations of batch normalization and swish activation. Essentially, the actual image convolution procedure takes place inside the encoder, which consists in converting many pixels within their receptive field (the area of the input that influences a particular feature) into single values, aiming at reducing the number of free parameters, while allowing the network to be deeper. In the end, in our case, the final layers of the encoder return feature maps at a reduced resolution (16 x16) over 384 channels. With a capillary flow of information from the encoder to the decoder, the latter is activated. The decoder represents the true operational core of a MANet (the bottommost block in the central box of <a href="#pone.0330419.g002" class="usa-link">Fig 2</a>). Its role is to perform a weighted recombination of the features extracted by the encoder, with the ultimate goal of returning segmentation maps (i.e., segmentation shapes). Those segmentation maps are the most-informative output of a MANet, as they can be used to highlight the sub-regions of interest (archaeological in our case) within the original input images. Our convolutional network, as previously mentioned, implements attention mechanisms. To this end, two important sub-blocks of the decoder are dedicated, specifically a Position-wise Attention Block (or PAB) and a sequence of Multi-scale Fusion Attention Blocks (or MFABs). Regarding these attention mechanisms, they allow our models to weigh various latent features within the images, effectively directing the model’s attention in this latent space for improved learning [<a href="#pone.0330419.ref032" class="usa-link" aria-describedby="pone.0330419.ref032">32</a>]. More precisely, the PAB incorporates a positional encoding mechanism. This mechanism produces an attention map, which effectively pinpoints pixels of greater significance, guiding our architecture to identify regions that require segmentation. Furthermore, through the cascading arrangement of several MFABs, we have implemented a multi-scale strategy. This method aggregates features to capture inter-channel relationships, thereby enhancing the robustness of the segmentation. Finally, we have the block at the rightmost position in the central box of <a href="#pone.0330419.g002" class="usa-link">Fig 2</a>, which represents the segmentation head. This is the final layer, responsible for computing the ultimate segmentation maps. It is the last step where the initial remote sensing images are partitioned into distinct regions, with their pixels homogeneously classified to identify sharper boundaries. At this point, however, as previously mentioned, the segmentation shapes are still at a low resolution. This is where the up-sampling blocks come in, simply used to return output maps (rightmost box of <a href="#pone.0330419.g002" class="usa-link">Fig 2</a>) with the same resolution as the input images. In closing, it is worth noticing that, at the completion of its function, the entire procedure is able to process around ten high resolution images in less than a second.</p>
<figure class="fig xbox font-sm" id="pone.0330419.g002"><h4 class="obj_head">Fig 2. Stylized diagram of the reference architecture for the DCNN with Attention used in this study (with input and output).</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12360548_pone.0330419.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ef05/12360548/6359be308841/pone.0330419.g002.jpg" loading="lazy" height="448" width="790" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330419.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Disclaimer</strong>: This figure was completely drawn by the authors in every part and is an entirely original product of their ingenuity. It does not utilize any graphical elements (in whole or in part) belonging to others, and consequently, it is in no way dependent on terms of use dictated by others. The authors, as creators of the Figure, authorize its publication for academic purposes and grant its use under a CC BY license, with attribution to them<em>.</em></p></figcaption></figure><p>Starting from the DCNN architecture discussed above, we have re-trained our models, initially pre-trained on both Imagenet and on the images exploited in our previous work, with the new 208 images introduced in the previous Section, resorting to traditional transfer learning techniques [<a href="#pone.0330419.ref019" class="usa-link" aria-describedby="pone.0330419.ref019">19</a>,<a href="#pone.0330419.ref033" class="usa-link" aria-describedby="pone.0330419.ref033">33</a>,<a href="#pone.0330419.ref034" class="usa-link" aria-describedby="pone.0330419.ref034">34</a>]. In essence, we aimed at obtaining three new deep learning models, that added to the three ones built during our previous study [<a href="#pone.0330419.ref019" class="usa-link" aria-describedby="pone.0330419.ref019">19</a>], where the re-training activity was based on the use of the new imagery, respectively, provided by Bing, CORONA and a combination of both. As already anticipated, after these training activities, we concluded with a further incremental step of fine-tuning, applied to all the AI models of interest, using a particular technique called two-stage fine-tuning [<a href="#pone.0330419.ref035" class="usa-link" aria-describedby="pone.0330419.ref035">35</a>]. Technically speaking, this additional two-stage fine tuning activity, included, in turn, a first phase where, keeping the learning rate unchanged, the weights of the deep layers were frozen, subjecting to training only the <em>segmentation head</em>. In the second phase of this procedure, instead, the weights were unfrozen, reducing the learning rate by a factor of ten, and carrying out a re-training of the entire model, thus reducing the risk of both overfitting and catastrophic forgetting. We conducted this final procedure with the number of training epochs not fixed and proceeded until we detected a stagnation of the loss on the validation set, indicating a possible overfitting to avoid. For the sake of clarity, we summarize this (only apparently) complex situation providing, in the list below, all the six different models, differentiated based on the combination of the training activities to which they were subjected and the type of image dataset used:</p>
<ul class="list" style="list-style-type:disc">
<li><p><strong>Bing</strong>: the deep learning model trained on Bing basemaps during our previous study [<a href="#pone.0330419.ref019" class="usa-link" aria-describedby="pone.0330419.ref019">19</a>].</p></li>
<li><p><strong>Bing_Bing</strong>: the deep learning model previously trained on Bing basemaps and now re-trained on new Bing basemaps and finally fine-tuned as explained below.</p></li>
<li><p><strong>CORONA</strong>: the deep learning model trained on CORONA basemaps during our previous study [<a href="#pone.0330419.ref019" class="usa-link" aria-describedby="pone.0330419.ref019">19</a>].</p></li>
<li><p><strong>CORONA_CORONA</strong>: the deep learning model previously trained on CORONA basemaps and now re-trained on new CORONA basemaps and finally fine-tuned as explained below.</p></li>
<li><p><strong>BingCORONA</strong>: the deep learning model trained on a combination of Bing and CORONA basemaps during our previous study [<a href="#pone.0330419.ref019" class="usa-link" aria-describedby="pone.0330419.ref019">19</a>].</p></li>
<li><p><strong>BingCORONA_BingCORONA</strong>: the deep learning model previously trained on a combination of Bing and CORONA basemaps and now re-trained on new Bing and CORONA basemaps and finally fine-tuned as explained below.</p></li>
</ul>
<p>Moving to the issue of the type of metrics used to evaluate the efficacy of our system, it is worth mentioning that the accuracy returned by our models was evaluated on the basis of the consideration of two different assessment perspectives: that is, a) through the lens of the semantic segmentation to which each image was subjected (i.e., trying to evaluate the achieved accuracy only at a pixel level), and b) at a more general level, analyzing the confusion matrix, to obtain an assessment of the accuracy and recall values. In particular, to evaluate the results produced by segmentation, we have used three different metrics: i) the Intersection over Union (IoU), ii) the binary Intersection Over Union, (bIoU), and finally iii) the Matthews Correlation Coefficient (MCC). The mathematical definitions of these metrics are beyond the scope of this paper and can be easily retrieved from the specialized literature, more interesting are the motivations for this choice which are as follows. The IoU metric is largely used in similar situations, but it may present various defects as it is recognized that it can return high values that could be not directly related to a better general object recognition, but just to a more precise identification of its contours. Indeed, we used it in this study, because it allowed a comparison with previous results. Its variant, the bIoU, is instead a better candidate to measure the accuracy of detection in terms of the pixels being recognized as a part of a tell. Finally, the measurement of MCC values was added as it should represent the most appropriate metric for the problem under consideration based on findings described in recent literature [<a href="#pone.0330419.ref036" class="usa-link" aria-describedby="pone.0330419.ref036">36</a>,<a href="#pone.0330419.ref037" class="usa-link" aria-describedby="pone.0330419.ref037">37</a>]. Coming to the final phase of our study, upon assessment of the performances of our AI models we chose the one with better accuracy: its results were plotted under the form of a corresponding heatmap, and then passed to the archaeological team on the field. Based on these heatmaps, the latter made the final decision on which were the more promising sites deserving a visit during the field survey campaigns.</p></section><section id="sec007"><h3 class="pmc_sec_title">Results</h3>
<p>We must preliminarily mention that, of the 208 initial images, only 10% (i.e., 20 images) were used as subjects of the testing phase. In fact, 156 images (75%) were used for re-training our models, with 32 of them (15%) used during the validation phase. Before discussing the results achieved on the 20 images which were never shown to our models before this testing phase, we deem of interest to report also on the results obtained with the 32 images of the validation phase. Nonetheless, it should be clear that this phase (i.e., the validation phase) constitutes an integral part of the re-training activities discussed in the previous Section. As such, the corresponding results do not represent the ultimate measurement of how well our models recognize tells when new imagery is proposed. Rather, they have given a preliminary assurance that our models have learnt effectively during re-training, with a generic propensity to generalize well to new images. Obviously, monitoring this tendency during training helped us to fine-tune our models for better results. With this in mind, <a href="#pone.0330419.t002" class="usa-link">Table 2</a> provides the results obtained with the 32 images of the validation phase. The following factors should be taken into consideration. First, being validation a part of the re-training activity, these results are provided only for the three re-trained models, namely: Bing_Bing, Bing_CORONA_BingCORONA, and CORONA_CORONA. Second, the numerical values of <a href="#pone.0330419.t002" class="usa-link">Table 2</a> are not given under the form of average and standard deviation, as they represent, instead, the better values the models achieved (during a specific epoch) before overfitting occurred. Third, these results only focus on the pixel-wise accuracy.</p>
<section class="tw xbox font-sm" id="pone.0330419.t002"><h4 class="obj_head">Table 2. Validation: pixel-wise accuracy for the three re-trained models.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Model</th>
<th align="left" rowspan="1" colspan="1">IoU</th>
<th align="left" rowspan="1" colspan="1">MCC</th>
<th align="left" rowspan="1" colspan="1">bIoU</th>
<th align="left" rowspan="1" colspan="1">Epoch</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>Bing_Bing</strong>
</td>
<td align="left" rowspan="1" colspan="1">83.07</td>
<td align="left" rowspan="1" colspan="1">55.25</td>
<td align="left" rowspan="1" colspan="1">37.00</td>
<td align="left" rowspan="1" colspan="1">15</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>Bing_CORONA_BingCORONA</strong>
</td>
<td align="left" rowspan="1" colspan="1">84.42</td>
<td align="left" rowspan="1" colspan="1">69.99</td>
<td align="left" rowspan="1" colspan="1">56.86</td>
<td align="left" rowspan="1" colspan="1">9</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>CORONA_CORONA</strong>
</td>
<td align="left" rowspan="1" colspan="1">82.25</td>
<td align="left" rowspan="1" colspan="1">59.30</td>
<td align="left" rowspan="1" colspan="1">43.70</td>
<td align="left" rowspan="1" colspan="1">27</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330419.t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p><a href="#pone.0330419.t003" class="usa-link">Table 3</a> reports, instead, the average results (plus standard deviation) we achieved with our testing activity conducted with the 20 images our models have never seen before. These results are based on the metrics that we have already indicated being the most appropriate to recognize a tell at a pixel level (that is: IoU, bIoU and MCC). Each test was repeated ten times.</p>
<section class="tw xbox font-sm" id="pone.0330419.t003"><h4 class="obj_head">Table 3. Testing: pixel-wise accuracy for all models (average values with standard deviation).</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Model</th>
<th align="left" rowspan="1" colspan="1">IoU</th>
<th align="left" rowspan="1" colspan="1">St.d.</th>
<th align="left" rowspan="1" colspan="1">MCC</th>
<th align="left" rowspan="1" colspan="1">St.d.</th>
<th align="left" rowspan="1" colspan="1">bIoU</th>
<th align="left" rowspan="1" colspan="1">St.d.</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Bing (previous)</td>
<td align="left" rowspan="1" colspan="1">82.24</td>
<td align="left" rowspan="1" colspan="1">2.88</td>
<td align="left" rowspan="1" colspan="1">35.24</td>
<td align="left" rowspan="1" colspan="1">6.36</td>
<td align="left" rowspan="1" colspan="1">22.70</td>
<td align="left" rowspan="1" colspan="1">5.55</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>Bing_Bing</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>86.12</strong>
</td>
<td align="left" rowspan="1" colspan="1">2.77</td>
<td align="left" rowspan="1" colspan="1">
<strong>34.03</strong>
</td>
<td align="left" rowspan="1" colspan="1">9.95</td>
<td align="left" rowspan="1" colspan="1">
<strong>21.53</strong>
</td>
<td align="left" rowspan="1" colspan="1">8.67</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Bing_CORONA (previous)</td>
<td align="left" rowspan="1" colspan="1">84.30</td>
<td align="left" rowspan="1" colspan="1">1.56</td>
<td align="left" rowspan="1" colspan="1">45.76</td>
<td align="left" rowspan="1" colspan="1">7.93</td>
<td align="left" rowspan="1" colspan="1">28.80</td>
<td align="left" rowspan="1" colspan="1">6.45</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>Bing_CORONA_BingCORONA</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>85.77</strong>
</td>
<td align="left" rowspan="1" colspan="1">2.03</td>
<td align="left" rowspan="1" colspan="1">
<strong>55.63</strong>
</td>
<td align="left" rowspan="1" colspan="1">5.88</td>
<td align="left" rowspan="1" colspan="1">
<strong>39.23</strong>
</td>
<td align="left" rowspan="1" colspan="1">6.37</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">CORONA (previous)</td>
<td align="left" rowspan="1" colspan="1">83.54</td>
<td align="left" rowspan="1" colspan="1">2.02</td>
<td align="left" rowspan="1" colspan="1">31.98</td>
<td align="left" rowspan="1" colspan="1">8.57</td>
<td align="left" rowspan="1" colspan="1">18.80</td>
<td align="left" rowspan="1" colspan="1">5.91</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>CORONA_CORONA</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>85.09</strong>
</td>
<td align="left" rowspan="1" colspan="1">3.32</td>
<td align="left" rowspan="1" colspan="1">
<strong>47.27</strong>
</td>
<td align="left" rowspan="1" colspan="1">9.84</td>
<td align="left" rowspan="1" colspan="1">
<strong>33.19</strong>
</td>
<td align="left" rowspan="1" colspan="1">8.84</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330419.t003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>The results of <a href="#pone.0330419.t003" class="usa-link">Table 3</a> show that the re-training activity we conducted in the present study, combined with the effects of the two-stage fine tuning procedure, has had very positive effects on both the so called <strong>BingCORONA_BingCORONA</strong> and <strong>CORONA_CORONA</strong> models, also when compared with the results of our previous study, yielding a notable increase in terms of all the considered metrics. As to the <strong>BING_BING</strong> model, instead, it only improves on the IoU parameter. The following fact is of great interest: as the most notable increase in accuracy has been achieved in both models based on CORONA satellite imagery, while the simple Bing model presented no significant variation, this adds experimental evidence to the intuition that the combination of the two stage fine tuning procedure with the activities of transfer learning becomes really effective (with more accurate results) only when the corresponding model was built on top of the <strong>CORONA</strong> imagery. While it is true that in other researches, including the one we conducted previously, the integration of CORONA satellite imagery into generic AI models had produced inconclusive results (with motivations ranging from low resolution up to environmental factors, like cloud cover for example), the present study supports the hypothesis that the inclusion of CORONA imagery has the potential to enhance an AI model’s performance that has the task of recognizing <em>tells</em> from satellite imagery. In other words, the improvement we have measured, at a pixel level, substantiates the thesis that transfer learning and complex fine-tuning activities may benefit from the additional contextual information provided by these kinds of imagery, thus corroborating long-established archaeological insights of the same sign. Nonetheless, the results of <a href="#pone.0330419.t003" class="usa-link">Table 3</a> have (simply) informed us about the ability of our models to recognize if a given pixel is either comprised within a tell or not. We are, obviously, also interested in elevating our comprehension about the ability of our AI models to recognize a tell as a whole. <a href="#pone.0330419.t004" class="usa-link">Table 4</a> gives an answer to this question by providing the results we achieved in terms of the general accuracy in detecting tells, as emerging from the testing activities conducted with the same 20 images mentioned before. The used metrics, here, were those of <em>accuracy</em> and <em>recall</em> (the mathematical definitions of which can be easily retrieved from the specialized literature), while TP stands for true positives, TN: true negatives, FP: false positives and FN: false negatives. <a href="#pone.0330419.t003" class="usa-link">Table 3</a>, again, highlights the increased ability of the BingCORONA_BingCORONA model in detecting tells, reaching a detection accuracy in the neighborhood of 90% (while our previous results hardly surpassed 80% [<a href="#pone.0330419.ref019" class="usa-link" aria-describedby="pone.0330419.ref019">19</a>]), with a very low percentage of both false positives and negatives.</p>
<section class="tw xbox font-sm" id="pone.0330419.t004"><h4 class="obj_head">Table 4. Testing: tell detection (accuracy and recall).</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Model</th>
<th align="left" rowspan="1" colspan="1">Accuracy</th>
<th align="left" rowspan="1" colspan="1">Recall</th>
<th align="left" rowspan="1" colspan="1">TP</th>
<th align="left" rowspan="1" colspan="1">TN</th>
<th align="left" rowspan="1" colspan="1">FP</th>
<th align="left" rowspan="1" colspan="1">FN</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>Bing_Bing</strong>
</td>
<td align="left" rowspan="1" colspan="1">0.75</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>BingCORONA_BingCORONA</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>0.90</strong>
</td>
<td align="left" rowspan="1" colspan="1">
<strong>0.88</strong>
</td>
<td align="left" rowspan="1" colspan="1">7</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<strong>CORONA_CORONA</strong>
</td>
<td align="left" rowspan="1" colspan="1">0.70</td>
<td align="left" rowspan="1" colspan="1">0.67</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">2</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330419.t004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec008"><h3 class="pmc_sec_title">New discoveries</h3>
<p>Beyond the positive results reported in <a href="#pone.0330419.t002" class="usa-link">Tables 2</a>, <a href="#pone.0330419.t003" class="usa-link">3</a>, and <a href="#pone.0330419.t004" class="usa-link">4</a>, the novelty of our work lies in the idea to use machine predictions (upon approval of the domain experts) to decide to extend the set of archaeological sites to be inspected during field survey campaigns, which we did. As already anticipated, our best AI model (i.e., BingCORONA_BingCORONA) produced prediction heatmaps, like that shown in <a href="#pone.0330419.g003" class="usa-link">Fig 3</a>.</p>
<figure class="fig xbox font-sm" id="pone.0330419.g003"><h4 class="obj_head">Fig 3. Example of an AI-generated heatmap used to predict the presence of archaeological sites.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12360548_pone.0330419.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ef05/12360548/ed928aee9ce7/pone.0330419.g003.jpg" loading="lazy" height="494" width="707" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330419.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Disclaimer</strong>: All the displayed data fall under the condition of fair use utilization of geographical data for academic purposes. The list of all relevant data/software provider(s) is as follows: (i) satellite imagery is based on Copernicus Sentinel-2 data, freely available under the European Union’s open data policy (<a href="https://www.copernicus.eu/en/access-data/copernicus-open-access-hub" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.copernicus.eu/en/access-data/copernicus-open-access-hub</a>); (ii) maps display achieved with open source software, under the GNU licenses of QGIS (<a href="https://qgis.org/en/site/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://qgis.org/en/site/</a>) and QuickMapServices (<a href="https://github.com/nextgis/quickmapservices" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nextgis/quickmapservices</a>); (iii) final maps elaboration achieved with a software developed by the authors and available at (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></figcaption></figure><p>These heatmaps were analyzed by the archaeologists who compared them with the list of sites of potential interest identified through standard remote sensing operations. In our case, of which the heatmap in <a href="#pone.0330419.g003" class="usa-link">Fig 3</a> is an example, our attention was mainly attracted by machine predictions for a number of specific sites that were not previously recognized as potential tells before through traditional methods. Of these sites, eight were accompanied with very high values of probability of being positive cases as returned by the AI model, which led to one of the key results presented in this paper. Subsequently, two field reconnaissance campaigns were conducted in January 2023 and January 2024, covering the Iraqi district of Abu Ghraib at the northwestern apex of the Mesopotamian floodplain. Field activities were directed at verifying sites identified using the CORONA imagery (both those identified with standard remote sensing procedures and those suggested by the AI model described above). During these two campaigns, a total of 96 potential sites were investigated (including the eight suggested by the AI). Of these 96, only 15 showed no signs of ancient anthropogenic activity and were thus false positives. The field survey results revealed, in fact, that 81 turned out to be positively confirmed sites. Of the eight sites suggested by the AI, four were among the 81 confirmed sites of archaeological relevance. To be noticed, again, is the fact that all the 81 sites were discovered by virtue of the analyses conducted on the CORONA satellite imagery, being based either on remote sensing or through AI. The validation of these sites was achieved through the collection and subsequent study of superficial ancient ceramics, which also enabled their dating. <a href="#pone.0330419.g004" class="usa-link">Fig 4</a> summarizes these field-survey results, showing the entire survey area inside which both remote sensing- and AI- based predicted sites are shown using dots of different colors (also based on the fact they were confirmed as either positive or negative cases).</p>
<figure class="fig xbox font-sm" id="pone.0330419.g004"><h4 class="obj_head">Fig 4. Sites discovered during the Abu Ghraib archaeological survey campaigns.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12360548_pone.0330419.g004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ef05/12360548/2e750af678f1/pone.0330419.g004.jpg" loading="lazy" height="527" width="750" alt="Fig 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330419.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Red: positive cases discovered by AI. Blue: positive cases discovered with remote sensing. Disclaimer</strong>: All the displayed data fall under the condition of fair use utilization of geographical data for academic purposes. The list of all relevant data/software provider(s) is as follows: (i) satellite imagery is based on Copernicus Sentinel-2 data, freely available under the European Union’s open data policy (<a href="https://www.copernicus.eu/en/access-data/copernicus-open-access-hub" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.copernicus.eu/en/access-data/copernicus-open-access-hub</a>); (ii) maps display achieved with open source software, under the GNU licenses of QGIS (<a href="https://qgis.org/en/site/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://qgis.org/en/site/</a>) and QuickMapServices (<a href="https://github.com/nextgis/quickmapservices" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nextgis/quickmapservices</a>); (iii) final maps elaboration achieved with a software developed by the authors and available at (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></figcaption></figure><p>As to the four archaeological sites discovered based on the suggestion of our BingCORONA_BingCORONA deep learning model, – show, to the left, the heatmap produced with the CORONA imagery and, to the right, an actual ground photo of the site (all the geographical coordinates of these four confirmed sites are listed in the <a href="#pone.0330419.s001" class="usa-link">S1 Appendix</a> below).</p>
<figure class="fig xbox font-sm" id="pone.0330419.g005"><h4 class="obj_head">Fig 5. GHR.036: heatmap produced by our model on CORONA imagery (left) and an on-site photo overview (right).</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12360548_pone.0330419.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ef05/12360548/25654cf4af38/pone.0330419.g005.jpg" loading="lazy" height="279" width="799" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330419.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Disclaimer</strong>: All the displayed data fall under the condition of fair use utilization of geographical data for academic purposes. The list of all relevant data/software provider(s) is as follows: (i) CORONA satellite imagery utilized here is freely available through the United States Geological Survey (USGS) under its Open Data Policy (<a href="https://www.usgs.gov/faqs/are-usgs-reportspublications-copyrighted" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.usgs.gov/faqs/are-usgs-reportspublications-copyrighted</a>); (ii) maps display achieved with open source software, under the GNU licenses of QGIS (<a href="https://qgis.org/en/site/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://qgis.org/en/site/</a>) and QuickMapServices (<a href="https://github.com/nextgis/quickmapservices" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nextgis/quickmapservices</a>); (iii) final maps elaboration achieved with a software developed by the authors and available at (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></figcaption></figure><figure class="fig xbox font-sm" id="pone.0330419.g006"><h4 class="obj_head">Fig 6. GHR.077: heatmap produced by our model on CORONA imagery (left) and an on-site photo overview (right).</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12360548_pone.0330419.g006.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ef05/12360548/7a042fa60817/pone.0330419.g006.jpg" loading="lazy" height="258" width="739" alt="Fig 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330419.g006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Disclaimer</strong>: All the displayed data fall under the condition of fair use utilization of geographical data for academic purposes. The list of all relevant data/software provider(s) is as follows: (i) CORONA satellite imagery utilized here is freely available through the United States Geological Survey (USGS) under its Open Data Policy (<a href="https://www.usgs.gov/faqs/are-usgs-reportspublications-copyrighted" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.usgs.gov/faqs/are-usgs-reportspublications-copyrighted</a>); (ii) maps display achieved with open source software, under the GNU licenses of QGIS (<a href="https://qgis.org/en/site/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://qgis.org/en/site/</a>) and QuickMapServices (<a href="https://github.com/nextgis/quickmapservices" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nextgis/quickmapservices</a>); (iii) final maps elaboration achieved with a software developed by the authors and available at (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></figcaption></figure><figure class="fig xbox font-sm" id="pone.0330419.g007"><h4 class="obj_head">Fig 7. GHR.078: heatmap produced by our model on CORONA imagery (left) and an on-site photo overview (right).</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12360548_pone.0330419.g007.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ef05/12360548/1033489aa27e/pone.0330419.g007.jpg" loading="lazy" height="258" width="739" alt="Fig 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330419.g007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Disclaimer</strong>: All the displayed data fall under the condition of fair use utilization of geographical data for academic purposes. The list of all relevant data/software provider(s) is as follows: (i) CORONA satellite imagery utilized here is freely available through the United States Geological Survey (USGS) under its Open Data Policy (<a href="https://www.usgs.gov/faqs/are-usgs-reportspublications-copyrighted" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.usgs.gov/faqs/are-usgs-reportspublications-copyrighted</a>); (ii) maps display achieved with open source software, under the GNU licenses of QGIS (<a href="https://qgis.org/en/site/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://qgis.org/en/site/</a>) and QuickMapServices (<a href="https://github.com/nextgis/quickmapservices" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nextgis/quickmapservices</a>); (iii) final maps elaboration achieved with a software developed by the authors and available at (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></figcaption></figure><figure class="fig xbox font-sm" id="pone.0330419.g008"><h4 class="obj_head">Fig 8. GHR.079: heatmap produced by our model on CORONA imagery (left) and an on-site photo overview (right).</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12360548_pone.0330419.g008.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ef05/12360548/0739840669fe/pone.0330419.g008.jpg" loading="lazy" height="259" width="739" alt="Fig 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330419.g008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Disclaimer</strong>: All the displayed data fall under the condition of fair use utilization of geographical data for academic purposes. The list of all relevant data/software provider(s) is as follows: (i) CORONA satellite imagery utilized here is freely available through the United States Geological Survey (USGS) under its Open Data Policy (<a href="https://www.usgs.gov/faqs/are-usgs-reportspublications-copyrighted" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.usgs.gov/faqs/are-usgs-reportspublications-copyrighted</a>); (ii) maps display achieved with open source software, under the GNU licenses of QGIS (<a href="https://qgis.org/en/site/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://qgis.org/en/site/</a>) and QuickMapServices (<a href="https://github.com/nextgis/quickmapservices" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nextgis/quickmapservices</a>); (iii) final maps elaboration achieved with a software developed by the authors and available at (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></figcaption></figure></section></section><section id="sec009"><h2 class="pmc_sec_title">Discussion</h2>
<p>In this research, the use of AI techniques has been of great help in support to a process which remains, nonetheless, guided by the archaeologist’s knowledge and expertise. Deep learning models have helped towards the aim to identify areas potentially containing archaeological sites, albeit neglected during normal remote sensing operations. It has remained an archaeologists’ task to take the final decision about the precise locations of the sites to visit, based on their professional experience. In this sense, our proposed AI-based approach to archaeology has been conceived just to provide additional support to the archaeologists, rather than to replace them. Beyond the impact of the AI, in some sense already documented in a previous study of ours [<a href="#pone.0330419.ref019" class="usa-link" aria-describedby="pone.0330419.ref019">19</a>], it has emerged here also the fundamental role played by the CORONA imagery dataset, and its versatility, especially when used in combination with a deep learning model. This consideration derives not only from our direct experience on the four archaeological sites which were not detectable using the Bing imagery alone, thereby highlighting the substantive impact of incorporating the CORONA satellite imagery into the process, but also considering the state of preservation of the sites which were the subject of the on-field campaigns of 2023 and 2024. To better illustrate this point, <a href="#pone.0330419.g009" class="usa-link">Fig 9</a> shows that, of the 81 archaeological sites discovered during those campaigns, almost all had been destroyed over the past decades: either completely (31) or largely (19) or partially (31); where <em>completely</em> means a destruction of almost 100%, <em>largely</em> means over the threshold of 50% and finally <em>partially</em> means below 50%. In this context, the inclusion of CORONA satellite imagery has been fundamental because many of the destroyed sites were no longer visible from modern basemaps (like Bing maps). The CORONA satellite imagery, from the 1960s and early 1970s, has the ability to document a world that has almost disappeared: in the specific case of Abu Ghraib, the loss of the possibility to identify sites with modern basemaps, in fact, would range from 40% to 55%, if totally destroyed sites alone, or totally plus largely destroyed ones, were considered. Thus, the development of an automatic process that is able to identify disappearing sites, by including historical imagery, allows everyone to start a fundamental reflection for the protection of the existing/remaining archaeological evidence.</p>
<figure class="fig xbox font-sm" id="pone.0330419.g009"><h3 class="obj_head">Fig 9. State of preservation of the 81 archaeological sites discovered during the 2023-2024 on-field campaigns.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12360548_pone.0330419.g009.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/ef05/12360548/3fd1c2860963/pone.0330419.g009.jpg" loading="lazy" height="526" width="750" alt="Fig 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330419.g009/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>Disclaimer</strong>: All the displayed data fall under the condition of fair use utilization of geographical data for academic purposes. The list of all relevant data/software provider(s) is as follows: (i) satellite imagery is based on Copernicus Sentinel-2 data, freely available under the European Union’s open data policy (<a href="https://www.copernicus.eu/en/access-data/copernicus-open-access-hub" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.copernicus.eu/en/access-data/copernicus-open-access-hub</a>); (ii) maps display achieved with open source software, under the GNU licenses of QGIS (<a href="https://qgis.org/en/site/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://qgis.org/en/site/</a>) and QuickMapServices (<a href="https://github.com/nextgis/quickmapservices" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/nextgis/quickmapservices</a>); (iii) final maps elaboration achieved with a software developed by the authors and available at (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></figcaption></figure><p>To conclude this Section, we would like to add that, while we have documented that an AI-based identification process has the potential to make unexpected discoveries, nonetheless, what should not be forgotten is the awareness that we still do not know how this happens. Precisely, this should be the reason that pushes towards the integration of AI with human experts, through collaborative processes, aimed at mitigating classification errors and incorrect interpretations [<a href="#pone.0330419.ref038" class="usa-link" aria-describedby="pone.0330419.ref038">38</a>–<a href="#pone.0330419.ref042" class="usa-link" aria-describedby="pone.0330419.ref042">42</a>].</p></section><section id="sec010"><h2 class="pmc_sec_title">Conclusion</h2>
<p>We have described a deep learning model designed to identify sites of potential archaeological interest in the Abu Ghraib district, West of Baghdad in the Mesopotamian floodplain. This AI model has been built incrementally over the past years, using transfer learning techniques and a final two stage fine tuning procedure that has elevated the level of detection accuracy up to 90% (while previous results did not surpass the threshold of 80%). The role played by the CORONA imagery dataset has been fundamental in this context of vanishing archaeological evidence, as it has allowed the AI to <em>see</em> sites no longer visible due to the process of anthropization. Surprisingly, this process has also led to the identification of unexpected archaeological sites, which thus far had not been identified in standard remote sensing operations. In particular, our archaeological team visited the eight new sites suggested by the AI model, also because they had the appropriate morphological characteristics. During our field survey campaign, four of these eight sites have been confirmed as positive cases. In fact, even if they were totally destroyed and no longer visible on the ground, some ceramic sherds could still be collected, making it possible to confirm their existence and date them. It must be acknowledged that without the AI’s suggestions, the areas where the sites were confirmed would not have been investigated during routinary field surveys. In the end, the development of AI models able to automatically identify potential sites, no more visible from current basemaps, represents a real breakthrough which could be further extended exploring the possibility of adding other technologies and methods like, for example, LIDAR and super-resolution ones [<a href="#pone.0330419.ref043" class="usa-link" aria-describedby="pone.0330419.ref043">43</a>–<a href="#pone.0330419.ref049" class="usa-link" aria-describedby="pone.0330419.ref049">49</a>]. While our work has focused on tell-based sites—characterized by distinctive morphologies well-suited to automatic segmentation—it is worth noting that extending this approach to non-mounded contexts would represent a theoretically significant development. However, the lack of recurrent morphological features, the semantic heterogeneity of archaeological traces, and the current scarcity of annotated datasets make such a direction presently difficult to pursue. Advancing in this area would require fundamentally different classification strategies, substantial refinement of source data, and a methodological rethinking that goes beyond the aims and operational scope of the present study.</p></section><section id="sec011"><h2 class="pmc_sec_title">Supporting information</h2>
<section class="sm xbox font-sm" id="pone.0330419.s001"><div class="caption p">
<span>S1 Appendix. Geographical coordinates of GHR.036, GHR.077, GHR.078 and GHR.079 archaeological sites.</span><p>(DOCX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12360548/bin/pone.0330419.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330419.s001.docx</a><sup> (13.9KB, docx) </sup>
</div></div></section></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>All results were obtained using open-source software and models, as well as publicly available data (images, annotations) and computational resources (Google Colab), making this type of work highly accessible and replicable even in resource-limited research environments. In addition to the specific information provided within the document, all the code, data, archeological annotations and various resources are available on GitHub (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>The 2022-2024 Abu Ghraib survey project, directed by N. Marchetti, has been authorized by the Iraqi State Board of Antiquities and Heritage (SBAH) and funded by the University of Bologna and the Italian Ministry of Foreign Affairs and International Cooperation. The research presented in this paper was supported through the “KALAM. Analysis, protection and development of archaeological landscapes in Iraq and Uzbekistan through ICTs and community-based approaches” project, coordinated by N. Marchetti and funded by the Volkswagen Foundation (<a href="https://www.kalam.unibo.it" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">www.kalam.unibo.it</a>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0330419.ref001">
<span class="label">1.</span><cite>Wilkinson TJ. Archaeological landscapes of the Near East. University of Arizona Press. 2003.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Archaeological%20landscapes%20of%20the%20Near%20East&amp;author=TJ%20Wilkinson&amp;publication_year=2003&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref002">
<span class="label">2.</span><cite>Bickler SH. Machine Learning Arrives in Archaeology. Adv archaeol pract. 2021;9(2):186–91. doi: 10.1017/aap.2021.6</cite> [<a href="https://doi.org/10.1017/aap.2021.6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20archaeol%20pract&amp;title=Machine%20Learning%20Arrives%20in%20Archaeology&amp;author=SH%20Bickler&amp;volume=9&amp;issue=2&amp;publication_year=2021&amp;pages=186-91&amp;doi=10.1017/aap.2021.6&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref003">
<span class="label">3.</span><cite>Mantovan L, Nanni L. The Computerization of Archaeology: Survey on Artificial Intelligence Techniques. SN COMPUT SCI. 2020;1(5). doi: 10.1007/s42979-020-00286-w</cite> [<a href="https://doi.org/10.1007/s42979-020-00286-w" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=SN%20COMPUT%20SCI&amp;title=The%20Computerization%20of%20Archaeology:%20Survey%20on%20Artificial%20Intelligence%20Techniques&amp;author=L%20Mantovan&amp;author=L%20Nanni&amp;volume=1&amp;issue=5&amp;publication_year=2020&amp;doi=10.1007/s42979-020-00286-w&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref004">
<span class="label">4.</span><cite>Tenzer M, Pistilli G, Bransden A, Shenfield A. Debating AI in archaeology: applications, implications, and ethical considerations. Internet Archaeology. 2024;(67).</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Internet%20Archaeology&amp;title=Debating%20AI%20in%20archaeology:%20applications,%20implications,%20and%20ethical%20considerations&amp;author=M%20Tenzer&amp;author=G%20Pistilli&amp;author=A%20Bransden&amp;author=A%20Shenfield&amp;issue=67&amp;publication_year=2024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref005">
<span class="label">5.</span><cite>Lyons TR, Hitchcock RK. Aerial remote sensing techniques in archeology. Chaco Center. 1977.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Chaco%20Center&amp;author=TR%20Lyons&amp;author=RK%20Hitchcock&amp;publication_year=1977&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref006">
<span class="label">6.</span><cite>Kucukkaya AG. Photogrammetry and remote sensing in archeology. Journal of Quantitative Spectroscopy and Radiative Transfer. 2004;88(1–3):83–8. doi: 10.1016/j.jqsrt.2003.12.030</cite> [<a href="https://doi.org/10.1016/j.jqsrt.2003.12.030" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Quantitative%20Spectroscopy%20and%20Radiative%20Transfer&amp;title=Photogrammetry%20and%20remote%20sensing%20in%20archeology&amp;author=AG%20Kucukkaya&amp;volume=88&amp;publication_year=2004&amp;pages=83-8&amp;doi=10.1016/j.jqsrt.2003.12.030&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref007">
<span class="label">7.</span><cite>Orengo HA, Conesa FC, Garcia-Molsosa A, Lobo A, Green AS, Madella M, et al. Automated detection of archaeological mounds using machine-learning classification of multisensor and multitemporal satellite data. Proc Natl Acad Sci U S A. 2020;117(31):18240–50. doi: 10.1073/pnas.2005583117

</cite> [<a href="https://doi.org/10.1073/pnas.2005583117" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7414161/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32690717/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proc%20Natl%20Acad%20Sci%20U%20S%20A&amp;title=Automated%20detection%20of%20archaeological%20mounds%20using%20machine-learning%20classification%20of%20multisensor%20and%20multitemporal%20satellite%20data&amp;author=HA%20Orengo&amp;author=FC%20Conesa&amp;author=A%20Garcia-Molsosa&amp;author=A%20Lobo&amp;author=AS%20Green&amp;volume=117&amp;issue=31&amp;publication_year=2020&amp;pages=18240-50&amp;pmid=32690717&amp;doi=10.1073/pnas.2005583117&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref008">
<span class="label">8.</span><cite>Guyot A, Lennon M, Lorho T, Hubert-Moy L. Combined detection and segmentation of archeological structures from LiDAR data using a deep learning approach. Journal of Computer Applications in Archaeology. 2021;4(1):1.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Computer%20Applications%20in%20Archaeology&amp;title=Combined%20detection%20and%20segmentation%20of%20archeological%20structures%20from%20LiDAR%20data%20using%20a%20deep%20learning%20approach&amp;author=A%20Guyot&amp;author=M%20Lennon&amp;author=T%20Lorho&amp;author=L%20Hubert-Moy&amp;volume=4&amp;issue=1&amp;publication_year=2021&amp;pages=1&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref009">
<span class="label">9.</span><cite>Argyrou A, Agapiou A. A Review of Artificial Intelligence and Remote Sensing for Archaeological Research. Remote Sensing. 2022;14(23):6000. doi: 10.3390/rs14236000</cite> [<a href="https://doi.org/10.3390/rs14236000" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Remote%20Sensing&amp;title=A%20Review%20of%20Artificial%20Intelligence%20and%20Remote%20Sensing%20for%20Archaeological%20Research&amp;author=A%20Argyrou&amp;author=A%20Agapiou&amp;volume=14&amp;issue=23&amp;publication_year=2022&amp;pages=6000&amp;doi=10.3390/rs14236000&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref010">
<span class="label">10.</span><cite>Caspari G, Crespo P. Convolutional neural networks for archaeological site detection – Finding “princely” tombs. Journal of Archaeological Science. 2019;110:104998. doi: 10.1016/j.jas.2019.104998</cite> [<a href="https://doi.org/10.1016/j.jas.2019.104998" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Archaeological%20Science&amp;title=Convolutional%20neural%20networks%20for%20archaeological%20site%20detection%20%E2%80%93%20Finding%20%E2%80%9Cprincely%E2%80%9D%20tombs&amp;author=G%20Caspari&amp;author=P%20Crespo&amp;volume=110&amp;publication_year=2019&amp;pages=104998&amp;doi=10.1016/j.jas.2019.104998&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref011">
<span class="label">11.</span><cite>Guyot A, Hubert-Moy L, Lorho T. Detecting Neolithic Burial Mounds from LiDAR-Derived Elevation Data Using a Multi-Scale Approach and Machine Learning Techniques. Remote Sensing. 2018;10(2):225. doi: 10.3390/rs10020225</cite> [<a href="https://doi.org/10.3390/rs10020225" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Remote%20Sensing&amp;title=Detecting%20Neolithic%20Burial%20Mounds%20from%20LiDAR-Derived%20Elevation%20Data%20Using%20a%20Multi-Scale%20Approach%20and%20Machine%20Learning%20Techniques&amp;author=A%20Guyot&amp;author=L%20Hubert-Moy&amp;author=T%20Lorho&amp;volume=10&amp;issue=2&amp;publication_year=2018&amp;pages=225&amp;doi=10.3390/rs10020225&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref012">
<span class="label">12.</span><cite>Soroush M, Mehrtash A, Khazraee E, Ur J. Deep learning in archaeological remote sensing: Automated qanat detection in the Kurdistan region of Iraq. Remote Sensing. 2020;12(3):500.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Remote%20Sensing&amp;title=Deep%20learning%20in%20archaeological%20remote%20sensing:%20Automated%20qanat%20detection%20in%20the%20Kurdistan%20region%20of%20Iraq&amp;author=M%20Soroush&amp;author=A%20Mehrtash&amp;author=E%20Khazraee&amp;author=J%20Ur&amp;volume=12&amp;issue=3&amp;publication_year=2020&amp;pages=500&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref013">
<span class="label">13.</span><cite>Ur J, Babakr N, Palermo R, Creamer P, Soroush M, Ramand S. The Erbil Plain Archaeological Survey: Preliminary Results, 2012–2020. Iraq. 2021;83:205–43.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Iraq&amp;title=The%20Erbil%20Plain%20Archaeological%20Survey:%20Preliminary%20Results,%202012%E2%80%932020&amp;author=J%20Ur&amp;author=N%20Babakr&amp;author=R%20Palermo&amp;author=P%20Creamer&amp;author=M%20Soroush&amp;volume=83&amp;publication_year=2021&amp;pages=205-43&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref014">
<span class="label">14.</span><cite>Landauer J, Klassen S, Wijker AP, van der Kroon J, Jaszkowski A, Verschoof-van der Vaart WB. Beyond the Greater Angkor Region: Automatic large-scale mapping of Angkorian-period reservoirs in satellite imagery using deep learning. PLoS One. 2025;20(3):e0320452. doi: 10.1371/journal.pone.0320452

</cite> [<a href="https://doi.org/10.1371/journal.pone.0320452" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11940504/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40138322/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Beyond%20the%20Greater%20Angkor%20Region:%20Automatic%20large-scale%20mapping%20of%20Angkorian-period%20reservoirs%20in%20satellite%20imagery%20using%20deep%20learning&amp;author=J%20Landauer&amp;author=S%20Klassen&amp;author=AP%20Wijker&amp;author=J%20van%20der%20Kroon&amp;author=A%20Jaszkowski&amp;volume=20&amp;issue=3&amp;publication_year=2025&amp;pmid=40138322&amp;doi=10.1371/journal.pone.0320452&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref015">
<span class="label">15.</span><cite>Gattiglia G. Managing artificial intelligence in archeology. An overview. Journal of Cultural Heritage. 2025;71:225–33.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Cultural%20Heritage&amp;title=Managing%20artificial%20intelligence%20in%20archeology.%20An%20overview&amp;author=G%20Gattiglia&amp;volume=71&amp;publication_year=2025&amp;pages=225-33&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref016">
<span class="label">16.</span><cite>Roccetti M, Casini L, Delnevo G, Orrù V, Marchetti N. Potential and limitations of designing a deep learning model for discovering new archaeological sites: A case with the Mesopotamian floodplain. In: Proceedings of the 6th EAI International Conference on Smart Objects and Technologies for Social Good, 2020. 216–21.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%206th%20EAI%20International%20Conference%20on%20Smart%20Objects%20and%20Technologies%20for%20Social%20Good&amp;title=Potential%20and%20limitations%20of%20designing%20a%20deep%20learning%20model%20for%20discovering%20new%20archaeological%20sites:%20A%20case%20with%20the%20Mesopotamian%20floodplain&amp;author=M%20Roccetti&amp;author=L%20Casini&amp;author=G%20Delnevo&amp;author=V%20Orr%C3%B9&amp;author=N%20Marchetti&amp;publication_year=2020&amp;pages=216-21&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref017">
<span class="label">17.</span><cite>Casini L, Roccetti M, Delnevo G, Marchetti N, Orrù V. The barrier of meaning in archaeological data science. arXiv preprint arXiv. 2021. <a href="https://arxiv.org/abs/2102.06022" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2102.06022</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint%20arXiv&amp;title=The%20barrier%20of%20meaning%20in%20archaeological%20data%20science&amp;author=L%20Casini&amp;author=M%20Roccetti&amp;author=G%20Delnevo&amp;author=N%20Marchetti&amp;author=V%20Orr%C3%B9&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref018">
<span class="label">18.</span><cite>Casini L, Orrù V, Roccetti M, Marchetti N. When machines find sites for the archaeologists: A preliminary study with semantic segmentation applied on satellite imagery of the Mesopotamian floodplain. In: Proceedings of the 2022 ACM Conference on Information Technology for Social Good, 2022. 378–83.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%202022%20ACM%20Conference%20on%20Information%20Technology%20for%20Social%20Good&amp;title=When%20machines%20find%20sites%20for%20the%20archaeologists:%20A%20preliminary%20study%20with%20semantic%20segmentation%20applied%20on%20satellite%20imagery%20of%20the%20Mesopotamian%20floodplain&amp;author=L%20Casini&amp;author=V%20Orr%C3%B9&amp;author=M%20Roccetti&amp;author=N%20Marchetti&amp;publication_year=2022&amp;pages=378-83&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref019">
<span class="label">19.</span><cite>Casini L, Marchetti N, Montanucci A, Orrù V, Roccetti M. A human-AI collaboration workflow for archaeological sites detection. Sci Rep. 2023;13(1):8699. doi: 10.1038/s41598-023-36015-5

</cite> [<a href="https://doi.org/10.1038/s41598-023-36015-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10227033/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37248310/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Rep&amp;title=A%20human-AI%20collaboration%20workflow%20for%20archaeological%20sites%20detection&amp;author=L%20Casini&amp;author=N%20Marchetti&amp;author=A%20Montanucci&amp;author=V%20Orr%C3%B9&amp;author=M%20Roccetti&amp;volume=13&amp;issue=1&amp;publication_year=2023&amp;pages=8699&amp;pmid=37248310&amp;doi=10.1038/s41598-023-36015-5&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref020">
<span class="label">20.</span><cite>Adams RM.
Settlement and Irrigation Patterns in Ancient Akkad. In: Gibson MG, editor. The City and Area of Kish. Field Research Projects. 1972. p. 182–208.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20City%20and%20Area%20of%20Kish&amp;author=RM.%20Adams&amp;author=MG%20Gibson&amp;publication_year=1972&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref021">
<span class="label">21.</span><cite>Marchetti N, Bortolini E, Menghi SJC, Orrù V, Zaina F. Long-Term Urban and Population Trends in the Southern Mesopotamian Floodplains. Journal of Archaeological Research. 2024;:1–42.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Journal%20of%20Archaeological%20Research&amp;title=Long-Term%20Urban%20and%20Population%20Trends%20in%20the%20Southern%20Mesopotamian%20Floodplains&amp;author=N%20Marchetti&amp;author=E%20Bortolini&amp;author=SJC%20Menghi&amp;author=V%20Orr%C3%B9&amp;author=F%20Zaina&amp;publication_year=2024&amp;pages=1-42&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref022">
<span class="label">22.</span><cite>Traviglia A, Cowley D, Lambers K. Finding common ground: Human and computer vision in archaeological prospection. AARGnews. 2016;53:11–24.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=AARGnews&amp;title=Finding%20common%20ground:%20Human%20and%20computer%20vision%20in%20archaeological%20prospection&amp;author=A%20Traviglia&amp;author=D%20Cowley&amp;author=K%20Lambers&amp;volume=53&amp;publication_year=2016&amp;pages=11-24&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref023">
<span class="label">23.</span><cite>Comer DC, Harrower MJ, Casana J, Cothren J. The CORONA atlas project: Orthorectification of CORONA satellite imagery and regional-scale archaeological exploration in the Near East. Mapping archaeological landscapes from space. 2013. p. 33–43.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Mapping%20archaeological%20landscapes%20from%20space&amp;author=DC%20Comer&amp;author=MJ%20Harrower&amp;author=J%20Casana&amp;author=J%20Cothren&amp;publication_year=2013&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref024">
<span class="label">24.</span><cite>Kennedy D. Declassified satellite photographs and archaeology in the Middle East: case studies from Turkey. Antiquity. 1998;72(277):553–61. doi: 10.1017/s0003598x0008697x</cite> [<a href="https://doi.org/10.1017/s0003598x0008697x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Antiquity&amp;title=Declassified%20satellite%20photographs%20and%20archaeology%20in%20the%20Middle%20East:%20case%20studies%20from%20Turkey&amp;author=D%20Kennedy&amp;volume=72&amp;issue=277&amp;publication_year=1998&amp;pages=553-61&amp;doi=10.1017/s0003598x0008697x&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref025">
<span class="label">25.</span><cite>Kouchoukos N. Satellite Images and Near Eastern Landscapes. Near Eastern Archaeology. 2001;64(1–2):80–91. doi: 10.2307/3210823</cite> [<a href="https://doi.org/10.2307/3210823" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Near%20Eastern%20Archaeology&amp;title=Satellite%20Images%20and%20Near%20Eastern%20Landscapes&amp;author=N%20Kouchoukos&amp;volume=64&amp;publication_year=2001&amp;pages=80-91&amp;doi=10.2307/3210823&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref026">
<span class="label">26.</span><cite>Philip G, Donoghue D, Beck A, Galiatsatos N. CORONA satellite photography: an archaeological application from the Middle East. Antiquity. 2002;76(291):109–18.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Antiquity&amp;title=CORONA%20satellite%20photography:%20an%20archaeological%20application%20from%20the%20Middle%20East&amp;author=G%20Philip&amp;author=D%20Donoghue&amp;author=A%20Beck&amp;author=N%20Galiatsatos&amp;volume=76&amp;issue=291&amp;publication_year=2002&amp;pages=109-18&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref027">
<span class="label">27.</span><cite>Ur JA. Settlement and landscape in northern Mesopotamia: the Tell Hamoukar survey 2000-2001. Akkadica. 2002;123(1):57–88.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Akkadica&amp;title=Settlement%20and%20landscape%20in%20northern%20Mesopotamia:%20the%20Tell%20Hamoukar%20survey%202000-2001&amp;author=JA%20Ur&amp;volume=123&amp;issue=1&amp;publication_year=2002&amp;pages=57-88&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref028">
<span class="label">28.</span><cite>Casana J, Wilkinson TJ. Settlement and landscapes in the Amuq region. The Amuq Valley Regional Projects. 2005. p. 1995–2002.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=The%20Amuq%20Valley%20Regional%20Projects&amp;author=J%20Casana&amp;author=TJ%20Wilkinson&amp;publication_year=2005&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref029">
<span class="label">29.</span><cite>QUANTUM G. Development Team. Quantum GIS geographic information system. 2011. Available from: <a href="http://qgis.osgeo.org" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://qgis.osgeo.org</a></cite>
</li>
<li id="pone.0330419.ref030">
<span class="label">30.</span><cite>Buslaev A, Iglovikov VI, Khvedchenya E, Parinov A, Druzhinin M, Kalinin AA. Albumentations: Fast and Flexible Image Augmentations. Information. 2020;11(2):125. doi: 10.3390/info11020125</cite> [<a href="https://doi.org/10.3390/info11020125" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Information&amp;title=Albumentations:%20Fast%20and%20Flexible%20Image%20Augmentations&amp;author=A%20Buslaev&amp;author=VI%20Iglovikov&amp;author=E%20Khvedchenya&amp;author=A%20Parinov&amp;author=M%20Druzhinin&amp;volume=11&amp;issue=2&amp;publication_year=2020&amp;pages=125&amp;doi=10.3390/info11020125&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref031">
<span class="label">31.</span><cite>Iakubovskii P. Segmentation Models Pytorch. 2020. Available fom: <a href="https://smpreadthedocsio/en/latest/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://smpreadthedocsio/en/latest/</a></cite>
</li>
<li id="pone.0330419.ref032">
<span class="label">32.</span><cite>Li R, Zheng S, Zhang C, Duan C, Su J, Wang L. Multiattention network for semantic segmentation of fine-resolution remote sensing images. IEEE Transactions on Geoscience and Remote Sensing. 2021;60:1–13.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Transactions%20on%20Geoscience%20and%20Remote%20Sensing&amp;title=Multiattention%20network%20for%20semantic%20segmentation%20of%20fine-resolution%20remote%20sensing%20images&amp;author=R%20Li&amp;author=S%20Zheng&amp;author=C%20Zhang&amp;author=C%20Duan&amp;author=J%20Su&amp;volume=60&amp;publication_year=2021&amp;pages=1-13&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref033">
<span class="label">33.</span><cite>Torrey L, Shavlik J. Transfer learning. Handbook of research on machine learning applications and trends: algorithms, methods, and techniques. IGI Global. 2010. p. 242–64.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Handbook%20of%20research%20on%20machine%20learning%20applications%20and%20trends:%20algorithms,%20methods,%20and%20techniques&amp;author=L%20Torrey&amp;author=J%20Shavlik&amp;publication_year=2010&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref034">
<span class="label">34.</span><cite>Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L. Imagenet: A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009. 248–55.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=2009%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition&amp;title=Imagenet:%20A%20large-scale%20hierarchical%20image%20database&amp;author=J%20Deng&amp;author=W%20Dong&amp;author=R%20Socher&amp;author=LJ%20Li&amp;author=K%20Li&amp;publication_year=2009&amp;pages=248-55&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref035">
<span class="label">35.</span><cite>Valizadeh Aslani T, Shi Y, Wang J, Ren P, Zhang Y, Hu M. Two-stage fine-tuning: A novel strategy for learning class-imbalanced data. arXiv preprint arXiv. 2022. <a href="https://arxiv.org/abs/2207.10858" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2207.10858</a></cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint%20arXiv&amp;title=Two-stage%20fine-tuning:%20A%20novel%20strategy%20for%20learning%20class-imbalanced%20data&amp;author=T%20Valizadeh%20Aslani&amp;author=Y%20Shi&amp;author=J%20Wang&amp;author=P%20Ren&amp;author=Y%20Zhang&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref036">
<span class="label">36.</span><cite>Chicco D, Jurman G. The Matthews correlation coefficient (MCC) should replace the ROC AUC as the standard metric for assessing binary classification. BioData Mining. 2023;16(1):1–23.
</cite> [<a href="https://doi.org/10.1186/s13040-023-00322-4" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9938573/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36800973/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=BioData%20Mining&amp;title=The%20Matthews%20correlation%20coefficient%20(MCC)%20should%20replace%20the%20ROC%20AUC%20as%20the%20standard%20metric%20for%20assessing%20binary%20classification&amp;author=D%20Chicco&amp;author=G%20Jurman&amp;volume=16&amp;issue=1&amp;publication_year=2023&amp;pages=1-23&amp;pmid=36800973&amp;doi=10.1186/s13040-023-00322-4&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref037">
<span class="label">37.</span><cite>Sech G, Soleni P, Verschoof-van der Vaart WB, Kokalj Z, Traviglia A, Fiorucci M. Transfer learning of semantic segmentation methods for identifying buried archaeological structures on LiDAR data. In: IGARSS 2023-2023 IEEE International Geoscience and Remote Sensing Symposium, 2023. 6987–90.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=IGARSS%202023-2023%20IEEE%20International%20Geoscience%20and%20Remote%20Sensing%20Symposium&amp;author=G%20Sech&amp;author=P%20Soleni&amp;author=WB%20Verschoof-van%20der%20Vaart&amp;author=Z%20Kokalj&amp;author=A%20Traviglia&amp;publication_year=2023&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref038">
<span class="label">38.</span><cite>Baeza YR, Estevez AM.
The relevance of non-human errors in machine learning. In: Hernandez-Orallo J, Cheke L, Tenebaum J, Ullman T, Martınez-Plumed F, Rutar D, et al. editors. Proceedings of the Workshop on AI Evaluation Beyond Metrics (EBeM 2022); 2022.  Jul 25; Vienna, Austria. [Aachen]: CEUR-WS; 2022. CEUR Workshop Proceedings; 2022. p. 6.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20Workshop%20on%20AI%20Evaluation%20Beyond%20Metrics%20(EBeM%202022)&amp;author=YR%20Baeza&amp;author=AM.%20Estevez&amp;author=J%20Hernandez-Orallo&amp;author=L%20Cheke&amp;author=J%20Tenebaum&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref039">
<span class="label">39.</span><cite>Yampolskiy RV. On monitorability of AI. AI and Ethics. 2024;:1–19.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=AI%20and%20Ethics&amp;title=On%20monitorability%20of%20AI&amp;author=RV%20Yampolskiy&amp;publication_year=2024&amp;pages=1-19&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref040">
<span class="label">40.</span><cite>Roccetti M, Tenace M, Cappiello G. Prescient perspectives on football tactics: A case with Liverpool FC, corners and AI. 2024. 10.13140/RG.2.2.27842.59847/16</cite> [<a href="https://doi.org/10.13140/RG.2.2.27842.59847/16" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="pone.0330419.ref041">
<span class="label">41.</span><cite>Gao L, Guan L. Interpretability of Machine Learning: Recent Advances and Future Prospects. IEEE MultiMedia. 2023;30(4):105–18. doi: 10.1109/mmul.2023.3272513</cite> [<a href="https://doi.org/10.1109/mmul.2023.3272513" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20MultiMedia&amp;title=Interpretability%20of%20Machine%20Learning:%20Recent%20Advances%20and%20Future%20Prospects&amp;author=L%20Gao&amp;author=L%20Guan&amp;volume=30&amp;issue=4&amp;publication_year=2023&amp;pages=105-18&amp;doi=10.1109/mmul.2023.3272513&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref042">
<span class="label">42.</span><cite>Messeri L, Crockett MJ. Artificial intelligence and illusions of understanding in scientific research. Nature. 2024;627(8002):49–58. doi: 10.1038/s41586-024-07146-0

</cite> [<a href="https://doi.org/10.1038/s41586-024-07146-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38448693/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Nature&amp;title=Artificial%20intelligence%20and%20illusions%20of%20understanding%20in%20scientific%20research&amp;author=L%20Messeri&amp;author=MJ%20Crockett&amp;volume=627&amp;issue=8002&amp;publication_year=2024&amp;pages=49-58&amp;pmid=38448693&amp;doi=10.1038/s41586-024-07146-0&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref043">
<span class="label">43.</span><cite>Ji J, Qiu T, Chen B, Zhang B, Lou H, Wang K. Ai alignment: A comprehensive survey. arXiv preprint. 2023. doi: 10.48550/arXiv.2310.19852</cite> [<a href="https://doi.org/10.48550/arXiv.2310.19852" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=Ai%20alignment:%20A%20comprehensive%20survey&amp;author=J%20Ji&amp;author=T%20Qiu&amp;author=B%20Chen&amp;author=B%20Zhang&amp;author=H%20Lou&amp;publication_year=2023&amp;doi=10.48550/arXiv.2310.19852&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref044">
<span class="label">44.</span><cite>Thabeng OL, Adam E, Merlo S. Evaluating the Performance of Geographic Object-Based Image Analysis in Mapping Archaeological Landscapes Previously Occupied by Farming Communities: A Case of Shashi–Limpopo Confluence Area. Remote Sensing. 2023;15(23):5491. doi: 10.3390/rs15235491</cite> [<a href="https://doi.org/10.3390/rs15235491" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Remote%20Sensing&amp;title=Evaluating%20the%20Performance%20of%20Geographic%20Object-Based%20Image%20Analysis%20in%20Mapping%20Archaeological%20Landscapes%20Previously%20Occupied%20by%20Farming%20Communities:%20A%20Case%20of%20Shashi%E2%80%93Limpopo%20Confluence%20Area&amp;author=OL%20Thabeng&amp;author=E%20Adam&amp;author=S%20Merlo&amp;volume=15&amp;issue=23&amp;publication_year=2023&amp;pages=5491&amp;doi=10.3390/rs15235491&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref045">
<span class="label">45.</span><cite>Canedo D, Hipolito J, Fonte J, Dias R, do Pereiro T, Georgieva P, et al. The synergy between artificial intelligence, remote sensing, and archaeological fieldwork validation. Remote Sensing. 2024;16(11):1933.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Remote%20Sensing&amp;title=The%20synergy%20between%20artificial%20intelligence,%20remote%20sensing,%20and%20archaeological%20fieldwork%20validation&amp;author=D%20Canedo&amp;author=J%20Hipolito&amp;author=J%20Fonte&amp;author=R%20Dias&amp;author=T%20do%20Pereiro&amp;volume=16&amp;issue=11&amp;publication_year=2024&amp;pages=1933&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref046">
<span class="label">46.</span><cite>Lepcha DC, Goyal B, Dogra A, Goyal V. Image super-resolution: A comprehensive review, recent trends, challenges and applications. Information Fusion. 2023;91:230–60. doi: 10.1016/j.inffus.2022.10.007</cite> [<a href="https://doi.org/10.1016/j.inffus.2022.10.007" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Information%20Fusion&amp;title=Image%20super-resolution:%20A%20comprehensive%20review,%20recent%20trends,%20challenges%20and%20applications&amp;author=DC%20Lepcha&amp;author=B%20Goyal&amp;author=A%20Dogra&amp;author=V%20Goyal&amp;volume=91&amp;publication_year=2023&amp;pages=230-60&amp;doi=10.1016/j.inffus.2022.10.007&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref047">
<span class="label">47.</span><cite>Salvetti F, Mazzia V, Khaliq A, Chiaberge M. Multi-Image Super Resolution of Remotely Sensed Images Using Residual Attention Deep Neural Networks. Remote Sensing. 2020;12(14):2207. doi: 10.3390/rs12142207</cite> [<a href="https://doi.org/10.3390/rs12142207" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Remote%20Sensing&amp;title=Multi-Image%20Super%20Resolution%20of%20Remotely%20Sensed%20Images%20Using%20Residual%20Attention%20Deep%20Neural%20Networks&amp;author=F%20Salvetti&amp;author=V%20Mazzia&amp;author=A%20Khaliq&amp;author=M%20Chiaberge&amp;volume=12&amp;issue=14&amp;publication_year=2020&amp;pages=2207&amp;doi=10.3390/rs12142207&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref048">
<span class="label">48.</span><cite>Wei Z, Zhang S. Small object detection in satellite remote sensing images based on super-resolution enhanced DETR. In: International Conference on Remote Sensing, Mapping, and Image Processing (RSMIP 2024). vol. 13167. SPIE; 2024. p. 33–42.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=International%20Conference%20on%20Remote%20Sensing,%20Mapping,%20and%20Image%20Processing%20(RSMIP%202024).%20vol.%2013167.%20SPIE&amp;title=Small%20object%20detection%20in%20satellite%20remote%20sensing%20images%20based%20on%20super-resolution%20enhanced%20DETR&amp;author=Z%20Wei&amp;author=S%20Zhang&amp;publication_year=2024&amp;pages=33-42&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330419.ref049">
<span class="label">49.</span><cite>Afzal Z, Yan J, Barriot J-P, Sun S, Haider Z, Aslam RW. Evaluating the contribution of Tianwen-4 mission to Jupiter’s gravity field estimation using inter-satellite tracking. A&amp;A. 2025;699:A318. doi: 10.1051/0004-6361/202554439</cite> [<a href="https://doi.org/10.1051/0004-6361/202554439" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=A&amp;A&amp;title=Evaluating%20the%20contribution%20of%20Tianwen-4%20mission%20to%20Jupiter%E2%80%99s%20gravity%20field%20estimation%20using%20inter-satellite%20tracking&amp;author=Z%20Afzal&amp;author=J%20Yan&amp;author=J-P%20Barriot&amp;author=S%20Sun&amp;author=Z%20Haider&amp;volume=699&amp;publication_year=2025&amp;doi=10.1051/0004-6361/202554439&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section></section><article class="sub-article" id="pone.0330419.r001"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330419.r001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330419.r001</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 0</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gordin%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Shai Gordin</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Shai Gordin</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gordin%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Shai Gordin</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.c" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.c" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.c" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Shai Gordin</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.c" class="d-panel p" style="display: none">
<div>© 2025 Shai Gordin</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>16 Mar 2025</em>
</p>
<p>Dear Dr. ROCCETTI,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
<p>Please submit your revised manuscript by Apr 30 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span> . When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a> .</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Shai Gordin, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Journal Requirements:</p>
<p>When submitting your revision, we need you to address these additional requirements.</p>
<p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p>
<p><a href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</a> and</p>
<p>
<a href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</a>
</p>
<p>2. In your manuscript, please provide additional information regarding the specimens used in your study. Ensure that you have reported human remain specimen numbers and complete repository information, including museum name and geographic location.</p>
<p>If permits were required, please ensure that you have provided details for all permits that were obtained, including the full name of the issuing authority, and add the following statement:</p>
<p>'All necessary permits were obtained for the described study, which complied with all relevant regulations.'</p>
<p>If no permits were required, please include the following statement:</p>
<p>'No permits were required for the described study, which complied with all relevant regulations.'</p>
<p>For more information on PLOS ONE's requirements for paleontology and archeology research, see <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-paleontology-and-archaeology-research" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-paleontology-and-archaeology-research</a> .</p>
<p>3. Thank you for stating the following financial disclosure:</p>
<p>“The 2022-2024 Abu Ghraib survey project, directed by N. Marchetti, has been authorized by the Iraqi State Board of Antiquities and Heritage (SBAH) and funded by the University of Bologna and the Italian Ministry of Foreign Affairs and International Cooperation. The research presented in this paper was supported through the “KALAM”. Analysis, protection and development of archaeological landscapes in Iraq and Uzbekistan through ICTs and community-based approaches” project, coordinated by N. Marchetti and funded by the Volkswagen Foundation (<a href="https://www.kalam.unibo.it" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">www.kalam.unibo.it).”</a></p>
<p>Please state what role the funders took in the study.  If the funders had no role, please state: "The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."</p>
<p>If this statement is not correct you must amend it as needed.</p>
<p>Please include this amended Role of Funder statement in your cover letter; we will change the online submission form on your behalf.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>
<strong>Comments to the Author</strong>
</p>
<p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>Reviewer #1: Partly</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>2. Has the statistical analysis been performed appropriately and rigorously? --&gt;?&gt;</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>3. Have the authors made all data underlying the findings in their manuscript fully available??&gt;</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a></p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>4. Is the manuscript presented in an intelligible fashion and written in standard English??&gt;</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>Reviewer #1: This is a paper focused on using deep learning techniques to auto-discover archaeological sites in Mesopotamia. The paper reflects a growing trend in the use of AI-based techniques to automate discovery of archaeological sites. Overall, the paper is interesting but requires more work prior to publication. I list some of my comments below.</p>
<p>1). The review of previous literature misses quite a lot of developments in the area of remote sensing and archaeology using AI-based techniques. There have been various studies on also using mounds on maps or imagery to find sites. Although many of these do not focus on CORONA, perhaps incorporating these could be useful as it shows where the methodology is best understood in this growing literature. I think incorporating what has been done with UAVs, non-optical data, and other areas could be useful for better understanding many of these related techniques and ultimately how this work is different.</p>
<p>2) The methodology, which is described and presented, needs a lot more description in the main text in my opinion. I think a better discussion on the steps and algorithm is critical. I understand that the details are best presented elsewhere, such as the code, but the paper should also have enough detail to give the reader they understand the mechanics of the technique. To me this needs more work and detail. There is not much discussion on the loss function, parameterisation, and other steps.</p>
<p>3). Its good to see the tests done regarding accuracy. However, do we get a sense of sites being missed by survey and this technique? I know it is hard to tell without knowing what sites exist but random survey of the region studied could help identify what is being missed by these more focused approaches that are looking at areas seen on the results.</p>
<p>4). Although tell sites are obviously common, many sites are flat in Mesopotamia or show little mounding. I understand that these might be difficult on optical data but this may mean other data should be used such as LiDAR (e.g., from UAVs), multispectral, and even higher resolution optical. It seems to me many sites could be missed with this technique. In fact, if one were to use Digital Elevation Data and use that to find sites, as tells become obvious with that data, could we not simply use that to find mounded sites? I would have tested how accurate this technique is relative to someone using DEM data to find sites and see which is more accurate. In any case, I think more discussion on the limitations of the research and why other data sources could not be used to find non-tell (mounded) sites is warranted as there are now a number of papers on finding mounded sites using deep learning in archaeology (see my comment on the literature review).</p>
<p>5. More discussion on usability of this would be good. For instance, on scaling the approach, time it takes to do this work, and benefits of this technique versus other techniques in the literature, such as Mask R-CNN, Random Forest, and other DL techniques.</p>
<p>Reviewer #2: Overall, this is an interesting article employing a methodology which I hope to become more developed and common in broadscale settlement analysis. However, a concern is that the authors present this method as a novel approach, when uses of machine learning algorithms have been employed to this end before in similar areas (Menze et al 2006, Soroush et al 2020, Guyot et al 2018, Orengo et al 2020, etc.). It is concerning that other studies such as these are not cited. I am also concerned at the apparent conflation of “tells” with archaeological sites more broadly. One of the main benefits of this technology is that it should not just be able to identify tells, which are easily identifiable without ML. However, throughout the article, it is difficult to understand if the authors are referring only to the identification of tells (which seems to be the case) or all archaeological sites. Certainly, there are plenty of “flat” sites.</p>
<p>My second main concern with this article is that the AI/ML approach used to identify sites does not actually assist the preexisting survey methods in a meaningful way. If I am reading this correctly, then it appears that the ultimate result of this study is 4 sites and 4 false positives identified that were not initially discovered during survey. However, when compared to the 80+ sites that were identified and surveyed, this number seems almost negligible and not at all like a “real breakthrough” that the authors purport it to be. In fact, in the conclusions section they suggest that these "discovered" sites by the model had "the appropriate morphological characteristics" - does that mean that normal exploratory foot survey would have discovered these sites? Ultimately, it doesn't seem that the model made much of a difference at all to the results of a survey that was using CORONA imagery to identify potential sites in the first place. The authors have given information for the accuracy of the different models they tested, but how does it compare to the accuracy of "normal" site identification with satellite imagery? While the accuracy here is 50/50 in terms of "newly identified" sites, in my experience a normal positive-false positive ratio is more akin to 60-75%-40-25%. (It is also unclear why CORONA makes such a difference in the case of this model - I'm assuming it's the result of modern urbanization and industrial agriculture which have now obscured these sites, but this is unclear in the text itself.)</p>
<p>I think that an article discussing this approach has merit, but should be tested on a larger dataset with more distinct results to merit publication in this journal. My conclusion is to reject this manuscript, but with hope that the authors will continue to develop and apply this model to further material to produce meaningful results in the future.</p>
<p>**********</p>
<p><a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a> ). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a></p>
<p>Reviewer #1: No</p>
<p>Reviewer #2: No</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span> . Please note that Supporting Information files do not need this step.</p></section></article><article class="sub-article" id="pone.0330419.r002"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 18;20(8):e0330419. doi: <a href="https://doi.org/10.1371/journal.pone.0330419.r002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330419.r002</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 1</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.d" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.d" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.d" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub2" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.d" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>23 Apr 2025</em>
</p>
<p>We attached a letter for Reviewers as requested.</p>
<section class="sm xbox font-sm" id="pone.0330419.s003"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>PONE-reviewer-letter-2.0.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12360548/bin/pone.0330419.s003.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330419.s003.pdf</a><sup> (256.2KB, pdf) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0330419.r003"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330419.r003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330419.r003</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 1</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gordin%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Shai Gordin</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Shai Gordin</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gordin%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Shai Gordin</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.e" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.e" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.e" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Shai Gordin</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.e" class="d-panel p" style="display: none">
<div>© 2025 Shai Gordin</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>27 Jun 2025</em>
</p>
<p>Dear Dr. ROCCETTI,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
<p>Please submit your revised manuscript by Aug 10 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span> . When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a> .</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Shai Gordin, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Journal Requirements:</p>
<p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>
<strong>Comments to the Author</strong>
</p>
<p>Reviewer #1: (No Response)</p>
<p>**********</p>
<p>2. Is the manuscript technically sound, and do the data support the conclusions??&gt;</p>
<p>Reviewer #1: Partly</p>
<p>**********</p>
<p>3. Has the statistical analysis been performed appropriately and rigorously? --&gt;?&gt;</p>
<p>Reviewer #1: No</p>
<p>**********</p>
<p>4. Have the authors made all data underlying the findings in their manuscript fully available??&gt;</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a></p>
<p>Reviewer #1: Yes</p>
<p>**********</p>
<p>5. Is the manuscript presented in an intelligible fashion and written in standard English??&gt;</p>
<p>Reviewer #1: Yes</p>
<p>**********</p>
<p>Reviewer #1: I still find a few things missing in this work based on this current version. I think they need to be addressed before the manuscript can be accepted. I list these below.</p>
<p>1) The review of existing literature in this area is still somewhat minimal. I know this is not a review paper, so referencing all works is not that critical but I think more around the specific architecture chosen and the background into that would help because there are other architectures one could have chose and it is not clear to me why this particular method say is better or more appropriate over other CNN-based or DL models/architectures.</p>
<p>2) Validation test are somewhat limited. A broader range of statistical tests could be applied to test how accurate and reliable the quality of the image identification is. Usually I would expect a wider range, including RMSE and other tests to be there.</p>
<p>3) I still think more can be done with the non-tell based sites but I can accept to leave that for now. Some discussion at least as to how this could be an improvement in the future might be warranted (i.e., how to improve non-tell based site identification as this would really be transformative if we can go beyound mound identification).</p>
<p>**********</p>
<p><a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a> ). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a></p>
<p>Reviewer #1: No</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span></p></section></article><article class="sub-article" id="pone.0330419.r004"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 18;20(8):e0330419. doi: <a href="https://doi.org/10.1371/journal.pone.0330419.r004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330419.r004</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 2</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.f" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.f" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.f" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub4" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.f" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>23 Jul 2025</em>
</p>
<p>Dear Editor,</p>
<p>as suggested to us, we have revised our paper: “AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery”, submitted for a possible publication to PLOS ONE.</p>
<p>We have taken into serious consideration ALL the comments and suggestions that the Reviewer has made, and we have modified our paper accordingly (further adding one reference and one new discussion).</p>
<p>A letter answering to the comments of the Reviewer is reported below.</p>
<p>It describes how we have modified (or not) our paper in response to all his/her comments. The modifications we have made, have been highlighted with the reddish color in the text of the revised manuscript.</p>
<p>Sincerely,</p>
<p>The Authors</p>
<p>------</p>
<p>Answers to the Reviewer’s Comments:</p>
<p>1) The review of existing literature in this area is still somewhat minimal. I know this is not a review paper, so referencing all works is not that critical but I think more around the specific architecture chosen and the background into that would help because there are other architectures one could have chose and it is not clear to me why this particular method say is better or more appropriate over other CNN-based or DL models/architectures</p>
<p>Answer to 1)</p>
<p>We feel confident in asserting that a list of 48 references, with more than half addressing issues related to AI, allied technologies, and their application in the archaeological domain, is already sufficiently broad and encompassing as to preclude the need for further expansion.</p>
<p>Nonetheless, solely to provide a further confirmation regarding the type of impact our work on the use of AI techniques on satellite data has had in the scientific world, even outside the archaeological context, we are adding a very recent reference, number 49 below.</p>
<p>In this reference, experts from the Chinese government, from one of the most prestigious Chinese laboratories (located in Wuhan), engage with our method, aiming to use satellite data for calculating plausible routes for orbiting towards the planet Jupiter.</p>
<p>[49] Afzal Z, Yan J, Barriot J-P, Sun S, Haider Z, et al. Evaluating the contribution of Tianwen-4 mission to Jupiter's gravity field estimation using inter-satellite tracking. Astronomy &amp; AstroPhysics, 2025, doi: 10.1051/0004-6361/202554439</p>
<p>In closing this issue, concerning a potential additional comparison with other neural networks, it seems clear that the Reviewer fails to adequately acknowledge that there is not a single definitive number of neural network types, as the field is constantly evolving with new architectures and variations being developed. For the record, here are the main categories:</p>
<p>- Feedforward Neural Networks (FFNNs):</p>
<p>- Perceptron (Single-Layer Perceptron): The simplest form, with only one layer of neurons.</p>
<p>- Multi-Layer Perceptron (MLP): The most common and fundamental type, with at least one hidden layer. It forms the basis for many other architectures.</p>
<p>- Recurrent Neural Networks (RNNs): Designed to handle sequential data, where the output at one step depends on the input and the hidden state of the previous step.</p>
<p>- Long Short-Term Memory (LSTM): An RNN variant that solves the vanishing gradient problem, excellent for long sequences.</p>
<p>- Gated Recurrent Unit (GRU): Similar to LSTMs but simpler, with fewer parameters.</p>
<p>- Bidirectional RNN (Bi-RNN): Processes sequences forward and backward to capture context from both directions.</p>
<p>- Convolutional Neural Networks (CNNs): Excellent for analyzing data with a grid-like topology, such as images (2D) or time series (1D). For example: LeNet, AlexNet, VGG, ResNet, Inception, MobileNet are all xamples of specific CNN architectures that have become standard for image recognition (just our case). Also: 1D, 2D, 3D CNNs depending on the dimensionality of the input data.</p>
<p>- Generative Adversarial Networks (GANs): Composed of two competing networks (a generator and a discriminator) that work together to produce new, realistic data.</p>
<p>- DCGAN (Deep Convolutional GAN), with StyleGAN, BigGAN as more advanced examples.</p>
<p>- Autoencoders: Networks that learn a compressed representation (encoding) of the input data and then attempt to reconstruct the original input (decoding). Used for dimensionality reduction and denoising. Example: Vanilla Autoencoder</p>
<p>- Variational Autoencoder (VAE): Used for data generation.</p>
<p>- Denoising Autoencoder, Sparse Autoencoder: Variants for specific purposes.</p>
<p>- Transformer Networks: Revolutionary for Natural Language Processing (NLP), they rely on a self-attention mechanism to weigh the importance of different parts of the input. For example, BERT, GPT (Generative Pre-trained Transformer) are well-known examples of Transformer-based models.</p>
<p>- Radial Basis Function Networks (RBFNs): Used for function approximation and classification, with neurons in the hidden layer that have radial basis activation functions.</p>
<p>- Self-Organizing Maps (SOMs): A type of unsupervised neural network used for dimensionality reduction and visualizing high-dimensional datasets.</p>
<p>- Liquid State Machines (LSMs) / Echo State Networks (ESNs): A type of recurrent neural network that uses a "reservoir" of neurons with fixed, random connections, where only the output layers are trained.</p>
<p>- Neural Turing Machines (NTMs) / Differentiable Neural Computers (DNCs): Combine neural networks with external, addressable memory, allowing them to learn complex algorithms.</p>
<p>And this list only provides an overview of the main categories, with some of the most prominent architectures. Each category, in turn, includes countless variations and specializations.</p>
<p>What is important is that the choice of a neural network type largely depends on the specific data to be processed and obviously the problem to be solved.</p>
<p>With satellite images, like in our case, the use of a DCNN of MaNet/Imagenet -type is a standard practice and, among experts, should not necessitate additional clarification beyond that already given.</p>
<p>2) Validation test are somewhat limited. A broader range of statistical tests could be applied to test how accurate and reliable the quality of the image identification is. Usually I would expect a wider range, including RMSE and other tests to be there</p>
<p>Answer to 2)</p>
<p>We honestly find it very challenging to believe that requesting RMSE test values is appropriate in this context.</p>
<p>We are sure that the Reviewer knows that Root Mean Squared Error is a common metric typically used in regression problems (which is not exactly our case) to quantify the average magnitude of the errors between predicted values and actual observed values.</p>
<p>In our specific situation (considering that the subject are images), any expert in the scientific world would agree that the measures we have provided - IoU, bIoU, MCC, and then Accuracy and Recall - are far more significant and impactful for validating the results we have obtained than RMSE.</p>
<p>Hence, the decision not to measure RMSE, which would border on the nonsensical, statistically speaking.</p>
<p>3) I still think more can be done with the non-tell based sites but I can accept to leave that for now. Some discussion at least as to how this could be an improvement in the future might be warranted (i.e., how to improve non-tell based site identification as this would really be transformative if we can go beyound mound identification).</p>
<p>Answer to 3)</p>
<p>To respond to this issue, we have inserted the following text in the revised manuscript”:</p>
<p>“While our work has focused on tell-based sites—characterized by distinctive morphologies well-suited to automatic segmentation—it is worth noting that extending this approach to non-mounded contexts would represent a theoretically significant development. However, the lack of recurrent morphological features, the semantic heterogeneity of archaeological traces, and the current scarcity of annotated datasets make such a direction presently difficult to pursue. Advancing in this area would require fundamentally different classification strategies, substantial refinement of source data, and a methodological rethinking that goes beyond the aims and operational scope of the present study.”</p>
<section class="sm xbox font-sm" id="pone.0330419.s004"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Ultimate-Letter-to-Reviewer.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12360548/bin/pone.0330419.s004.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330419.s004.pdf</a><sup> (77.9KB, pdf) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0330419.r005"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330419.r005" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330419.r005</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 2</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gordin%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Shai Gordin</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Shai Gordin</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gordin%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Shai Gordin</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.g" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.g" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.g" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Shai Gordin</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.g" class="d-panel p" style="display: none">
<div>© 2025 Shai Gordin</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>1 Aug 2025</em>
</p>
<p>AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery</p>
<p>PONE-D-24-47137R2</p>
<p>Dear Dr. ROCCETTI,</p>
<p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
<p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
<p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Editorial Manager®</a>  and clicking the ‘Update My Information' link at the top of the page. For questions related to billing, please contact <a href="https://plos.my.site.com/s/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">billing support</a> .</p>
<p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>Kind regards,</p>
<p>Shai Gordin, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Additional Editor Comments (optional):</p>
<p>Reviewers' comments:</p></section></article><article class="sub-article" id="pone.0330419.r006"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330419.r006" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330419.r006</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Acceptance letter</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gordin%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Shai Gordin</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Shai Gordin</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gordin%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Shai Gordin</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.h" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.h" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.h" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Shai Gordin</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.h" class="d-panel p" style="display: none">
<div>© 2025 Shai Gordin</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>PONE-D-24-47137R2</p>
<p>PLOS ONE</p>
<p>Dear Dr. Roccetti,</p>
<p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p>
<p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p>
<p>* All references, tables, and figures are properly cited</p>
<p>* All relevant supporting information is included in the manuscript submission,</p>
<p>* There are no issues that prevent the paper from being properly typeset</p>
<p>You will receive further instructions from the production team, including instructions on how to review your proof when it is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p>
<p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>You will receive an invoice from PLOS for your publication fee after your manuscript has reached the completed accept phase. If you receive an email requesting payment before acceptance or for any other service, this may be a phishing scheme. Learn how to identify phishing emails and protect your accounts at <a href="https://explore.plos.org/phishing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://explore.plos.org/phishing</a>.</p>
<p>If we can help with anything else, please email us at customercare@plos.org.</p>
<p>Thank you for submitting your work to PLOS ONE and supporting open access.</p>
<p>Kind regards,</p>
<p>PLOS ONE Editorial Office Staff</p>
<p>on behalf of</p>
<p>Dr. Shai Gordin</p>
<p>Academic Editor</p>
<p>PLOS ONE</p></section></article><article class="sub-article" id="_ad93_"><section class="pmc-layout__citation font-secondary font-xs"><div></div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Associated Data</h1></hgroup><ul class="d-buttons inline-list"></ul>
<div class="d-panels font-secondary-light"></div>
<div></div>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
</div></section><section class="body sub-article-body"><section id="_adsm93_" lang="en" class="supplementary-materials"><h2 class="pmc_sec_title">Supplementary Materials</h2>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="caption p">
<span>S1 Appendix. Geographical coordinates of GHR.036, GHR.077, GHR.078 and GHR.079 archaeological sites.</span><p>(DOCX)</p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12360548/bin/pone.0330419.s001.docx" data-ga-action="click_feat_suppl" class="usa-link">pone.0330419.s001.docx</a><sup> (13.9KB, docx) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>PONE-reviewer-letter-2.0.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12360548/bin/pone.0330419.s003.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330419.s003.pdf</a><sup> (256.2KB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material3_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Ultimate-Letter-to-Reviewer.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12360548/bin/pone.0330419.s004.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330419.s004.pdf</a><sup> (77.9KB, pdf) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h2 class="pmc_sec_title">Data Availability Statement</h2>
<p>All results were obtained using open-source software and models, as well as publicly available data (images, annotations) and computational resources (Google Colab), making this type of work highly accessible and replicable even in resource-limited research environments. In addition to the specific information provided within the document, all the code, data, archeological annotations and various resources are available on GitHub (<a href="https://github.com/alepistola/AI_floodplains" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/alepistola/AI_floodplains</a>).</p></section></section></article><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0330419"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0330419.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (2.8 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12360548/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12360548/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12360548%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12360548/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12360548/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12360548/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40824971/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12360548/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40824971/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12360548/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12360548/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="afMXvdUJZg3JhdDNOZqXkc6qYwxPaXXoGrfANT7dzAsGMmeV9tvDkGzRnfX8AZHW">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
