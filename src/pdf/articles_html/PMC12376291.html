
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            RoadDiffBox: Automatic Road Distress Diagnosis through Controlled Image Generation and Semi-Supervised Learning - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="AE532E758AF21EA3052E75002169F2DC.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="research">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12376291/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Research">
<meta name="citation_title" content="RoadDiffBox: Automatic Road Distress Diagnosis through Controlled Image Generation and Semi-Supervised Learning">
<meta name="citation_author" content="Yuanyuan Hu">
<meta name="citation_author_institution" content="Institute of Highway Engineering, RWTH Aachen University, Aachen, Germany.">
<meta name="citation_author" content="Ning Chen">
<meta name="citation_author_institution" content="Beijing Key Laboratory of Traffic Engineering, Beijing University of Technology, Beijing, China.">
<meta name="citation_author" content="Hancheng Zhang">
<meta name="citation_author_institution" content="Institute of Highway Engineering, RWTH Aachen University, Aachen, Germany.">
<meta name="citation_author" content="Yue Hou">
<meta name="citation_author_institution" content="Department of Civil Engineering, Faculty of Science and Engineering, Swansea University, Swansea, UK.">
<meta name="citation_author" content="Pengfei Liu">
<meta name="citation_author_institution" content="Institute of Highway Engineering, RWTH Aachen University, Aachen, Germany.">
<meta name="citation_publication_date" content="2025 Aug 25">
<meta name="citation_volume" content="8">
<meta name="citation_firstpage" content="0833">
<meta name="citation_doi" content="10.34133/research.0833">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12376291/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12376291/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12376291/pdf/research.0833.pdf">
<meta name="description" content="During the designed service life, road infrastructures will bear repeated loading conditions from vehicle weights and environmental conditions, resulting in the inevitable occurrence of road distresses including cracks, potholes, etc. The ...">
<meta name="og:title" content="RoadDiffBox: Automatic Road Distress Diagnosis through Controlled Image Generation and Semi-Supervised Learning">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="During the designed service life, road infrastructures will bear repeated loading conditions from vehicle weights and environmental conditions, resulting in the inevitable occurrence of road distresses including cracks, potholes, etc. The ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12376291/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12376291">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.34133/research.0833"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/research.0833.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12376291%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12376291/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12376291/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12376291/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-research.png" alt="Research logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Research" title="Link to Research" shape="default" href="https://spj.sciencemag.org/research/" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Research (Wash D C)</button></div>. 2025 Aug 25;8:0833. doi: <a href="https://doi.org/10.34133/research.0833" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.34133/research.0833</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Research%20(Wash%20D%20C)%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Research%20(Wash%20D%20C)%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Research%20(Wash%20D%20C)%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Research%20(Wash%20D%20C)%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>RoadDiffBox: Automatic Road Distress Diagnosis through Controlled Image Generation and Semi-Supervised Learning</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hu%20Y%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Yuanyuan Hu</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Yuanyuan Hu</span></h3>
<div class="p">
<sup><sup>1</sup></sup>Institute of Highway Engineering, 
RWTH Aachen University, Aachen, Germany.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hu%20Y%22%5BAuthor%5D" class="usa-link"><span class="name western">Yuanyuan Hu</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20N%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Ning Chen</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Ning Chen</span></h3>
<div class="p">
<sup><sup>2</sup></sup>Beijing Key Laboratory of Traffic Engineering, 
Beijing University of Technology, Beijing, China.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20N%22%5BAuthor%5D" class="usa-link"><span class="name western">Ning Chen</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20H%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Hancheng Zhang</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Hancheng Zhang</span></h3>
<div class="p">
<sup><sup>1</sup></sup>Institute of Highway Engineering, 
RWTH Aachen University, Aachen, Germany.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20H%22%5BAuthor%5D" class="usa-link"><span class="name western">Hancheng Zhang</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hou%20Y%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Yue Hou</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Yue Hou</span></h3>
<div class="p">
<sup><sup>3</sup></sup>Department of Civil Engineering, Faculty of Science and Engineering, 
Swansea University, Swansea, UK.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Hou%20Y%22%5BAuthor%5D" class="usa-link"><span class="name western">Yue Hou</span></a>
</div>
</div>
<sup>3,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20P%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Pengfei Liu</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Pengfei Liu</span></h3>
<div class="p">
<sup><sup>1</sup></sup>Institute of Highway Engineering, 
RWTH Aachen University, Aachen, Germany.</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20P%22%5BAuthor%5D" class="usa-link"><span class="name western">Pengfei Liu</span></a>
</div>
</div>
<sup>1,</sup><sup>*</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff1">
<sup><sup>1</sup></sup>Institute of Highway Engineering, 
RWTH Aachen University, Aachen, Germany.</div>
<div id="aff2">
<sup><sup>2</sup></sup>Beijing Key Laboratory of Traffic Engineering, 
Beijing University of Technology, Beijing, China.</div>
<div id="aff3">
<sup><sup>3</sup></sup>Department of Civil Engineering, Faculty of Science and Engineering, 
Swansea University, Swansea, UK.</div>
<div class="author-notes p"><div class="fn" id="corr1">
<sup>*</sup><p class="display-inline">Address correspondence to: <span>liu@isac.rwth-aachen.de</span> (P.L.); <span>yue.hou@swansea.ac.uk</span> (Y.H.)</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2024 Oct 29; Revised 2025 Jul 20; Accepted 2025 Jul 21; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>Copyright © 2025 Yuanyuan Hu et al.</div>
<p>Exclusive licensee Science and Technology Review Publishing House. No claim to original U.S. Government Works. Distributed under a <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License (CC BY 4.0)</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12376291</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>During the designed service life, road infrastructures will bear repeated loading conditions from vehicle weights and environmental conditions, resulting in the inevitable occurrence of road distresses including cracks, potholes, etc. The traditional inspection methods by transportation engineers are normally costly and labor-intensive. In recent years, artificial intelligence (AI)-based road distress detection methods have been widely used as convenient and automated approaches, while the AI-based methods heavily depend on a large amount of high-quality images, limiting the real engineering applications. To address the issues, this study introduces RoadDiffBox, a novel framework employing controlled image generation and semi-supervised learning. The framework addresses dataset imbalances through class control and accelerates image generation by utilizing the denoising diffusion implicit model’s reverse process sampling method, while employing knowledge distillation techniques optimized for resource-constrained mobile devices. It generates diverse and high-quality road distress images with automatic bounding box annotations, substantially reducing manual labeling requirements. Test results show that RoadDiffBox demonstrates strong generalizability across geographic regions (Germany, China, and India) and shows cross-domain potential in medical imaging applications. Performance evaluations demonstrate RoadDiffBox’s effectiveness, with classification models achieving an F1-score of 0.95 and detection models reaching a mean average precision (mAP@50) of 0.95 and an F1-score of 0.91 in controlled settings, while maintaining robust performance (an F1-score of 0.86 and a mAP@50 of 0.91) during on-site testing in real-world conditions. On server-class hardware, the model achieves generation times as low as 0.18 s per image. It is discovered that RoadDiffBox can serve as a scalable and efficient solution for real-time road maintenance with limited datasets.</p></section><section id="sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p>The structural integrity, service quality, and functional performance of roadway infrastructure are critical to ensuring public transportation safety [<a href="#B1" class="usa-link" aria-describedby="B1">1</a>]. Under long-term servicing in severe environmental conditions, there will occur inevitable distresses in the road infrastructures, including various kinds of pavement damages such as cracks and potholes, posing serious risks to vehicle operation and public safety [<a href="#B2" class="usa-link" aria-describedby="B2">2</a>]. If these problems are not addressed appropriately, the deteriorations can accelerate the overall degradation of infrastructure, leading to more extensive repair costs and higher structural failure risks [<a href="#B3" class="usa-link" aria-describedby="B3">3</a>,<a href="#B4" class="usa-link" aria-describedby="B4">4</a>]. Accurate and efficient road distress detection is crucial for modern pavement management systems, enabling precise condition assessments and informed maintenance planning decisions. Timely maintenance to detect road distresses before they develop into structural failures by the transportation engineers are, therefore, essential to mitigating these impacts and extending the service life of road networks. As road infrastructure systems worldwide reach or surpass their designed service life, countries across the globe are increasingly prioritizing large-scale road maintenance initiatives [<a href="#B5" class="usa-link" aria-describedby="B5">5</a>]. However, traditional inspection methods, typically reliant on labor-intensive and manual assessments by transportation engineers, are costly and time-consuming [<a href="#B6" class="usa-link" aria-describedby="B6">6</a>], indicating an urgent need for innovative solutions to reduce inspection and maintenance costs while improving efficiency.</p>
<p>Recently, there have been many engineering demonstrations and applications for real road distresses detection using artificial intelligence (AI) worldwide. However, there exist a major problem for AI-based road distress detection tasks; i.e., training a reliable detection model normally requires hundreds of thousands of high-quality images [<a href="#B7" class="usa-link" aria-describedby="B7">7</a>,<a href="#B8" class="usa-link" aria-describedby="B8">8</a>], while the traditional labor-intensive and costly nature of manual annotation for crack detection by experienced transportation engineers result in a substantial challenge in obtaining large and high-quality labeled datasets [<a href="#B9" class="usa-link" aria-describedby="B9">9</a>,<a href="#B10" class="usa-link" aria-describedby="B10">10</a>] of road distresses. Various methodological approaches have been proposed to address this fundamental challenge, including data augmentation techniques across different sensor modalities. These encompass traditional image-based augmentation methods [<a href="#B11" class="usa-link" aria-describedby="B11">11</a>–<a href="#B14" class="usa-link" aria-describedby="B14">14</a>], acoustic signal processing techniques for structural health monitoring [<a href="#B15" class="usa-link" aria-describedby="B15">15</a>,<a href="#B16" class="usa-link" aria-describedby="B16">16</a>], and vibration-based data enhancement strategies [<a href="#B17" class="usa-link" aria-describedby="B17">17</a>,<a href="#B18" class="usa-link" aria-describedby="B18">18</a>]. However, in the context of road distress detection, image-based augmentation remains the predominant approach due to the visual nature of pavement damage assessment. Neverheless, these traditional image augmentation techniques, while enhancing model robustness through geometric transformations and noise injection, cannot generate examples beyond the existing data distribution, limiting their ability to address substantial data gaps or imbalances in road distress categories. Another approach is domain adaptation in transfer learning [<a href="#B19" class="usa-link" aria-describedby="B19">19</a>–<a href="#B21" class="usa-link" aria-describedby="B21">21</a>], which utilizes knowledge from other domains to enhance the accuracy of road crack detection, reduce training time and computational resources, and improve model generalization by incorporating knowledge from diverse fields. Despite its success in various areas, this method faces challenges in detecting road distress, as cracks and potholes differ markedly from typical objects in standard datasets. This fundamental domain gap limits effective knowledge transfer and necessitates alternative approaches that can better use available data resources. Semi-supervised learning methodologies [<a href="#B22" class="usa-link" aria-describedby="B22">22</a>–<a href="#B25" class="usa-link" aria-describedby="B25">25</a>] present a cost-effective approach to road distress detection by simultaneously utilizing labeled and unlabeled data. This approach is particularly valuable for road infrastructure monitoring applications, where expert-labeled data are both scarce and expensive to obtain. Through techniques such as pseudo-labeling, consistency regularization, and self-training, semi-supervised learning facilitates model training with minimal manual annotation requirements. Despite these advantages, the method faces substantial challenges when applied to road distress detection. The inherent visual characteristics of pavement damage—including irregular morphology, elongated patterns, and low contrast against the background—substantially complicate the pseudo-labeling process. Consequently, generated pseudo-labels often contain errors that introduce noise into the training pipeline. These inaccuracies can propagate through the model training process, ultimately degrading detection performance and limiting the practical utility of semi-supervised approaches for road distress applications. Generative models like variational autoencoders (VAEs) [<a href="#B26" class="usa-link" aria-describedby="B26">26</a>,<a href="#B27" class="usa-link" aria-describedby="B27">27</a>] and generative adversarial networks (GANs) [<a href="#B28" class="usa-link" aria-describedby="B28">28</a>] create synthetic crack images. VAEs often produce blurry images, limiting their utility for fine-grained crack detection, while GANs, despite producing realistic images, suffer from issues like mode collapse and artifacts [<a href="#B29" class="usa-link" aria-describedby="B29">29</a>]. Additionally, neither model can readily generate accompanying ground-truth annotations, such as bounding boxes, which are essential for training robust object detection models.</p>
<p>To address the challenge of generating high-quality datasets, the concept of AI-generated content (AIGC) [<a href="#B30" class="usa-link" aria-describedby="B30">30</a>] has emerged, bringing potential to fields like image generation and enhancement [<a href="#B31" class="usa-link" aria-describedby="B31">31</a>]. AIGC employs advanced AI techniques to automatically create diverse and high-quality content. Among these techniques, diffusion models have gained attention as a state-of-the-art—referring to the highest level of achievement in the field at the current time—approach in image generation, demonstrating outstanding performance in producing high-resolution and complex images for applications such as medical imaging, autonomous driving, and creative arts [<a href="#B32" class="usa-link" aria-describedby="B32">32</a>,<a href="#B33" class="usa-link" aria-describedby="B33">33</a>]. Akrout et al. [<a href="#B34" class="usa-link" aria-describedby="B34">34</a>] demonstrated that synthetic images from diffusion models enhance skin classifier accuracy, with hybrid synthetic–real data training outperforming single-source approaches. Universal transformer diffusion modeling for vulnerable pedestrian trajectory prediction (UTD-PTP) [<a href="#B35" class="usa-link" aria-describedby="B35">35</a>] successfully predicts vulnerable pedestrian trajectories in complex environments, while Diffusion-ES [<a href="#B36" class="usa-link" aria-describedby="B36">36</a>] optimizes black-box objectives while preserving trajectory naturalism in autonomous driving applications. In road engineering, diffusion models show promise for generating synthetic images of road conditions to enhance distress detection datasets. Cano-Ortiz et al. [<a href="#B37" class="usa-link" aria-describedby="B37">37</a>] proposed a generative diffusion model for data augmentation that produces synthetic images of rare defects, investigating methods to enhance image quality and reduce production time. Leveraging a diffusion model—crack diffusion model (CDM)—generates synthetic cracks with controlled morphology and positioning, enhancing detection and segmentation in complex environments [<a href="#B38" class="usa-link" aria-describedby="B38">38</a>]. Despite the promising potential of AIGC for road distress detection, several substantial limitations hinder its widespread application in real-world scenarios. Most current diffusion models lack effective class control, making it difficult to guide the generation process toward specific types of road distresses, leading to dataset imbalances with certain crack types overrepresented while others are underrepresented. This creates a long-tail problem where rare distress categories appear infrequently in the dataset, challenging model training and generalization. Additionally, while diffusion models can produce synthetic images, these images typically require manual annotation for object detection tasks that need bounding boxes or segmentation masks, which reintroduces high annotation costs and negates a primary advantage of AIGC approaches. Furthermore, conventional diffusion models suffer from slow generation speed during computations, with single image production taking minutes, making these models impractical for large-scale applications. This computational burden, coupled with high resource demands, makes diffusion models difficult to deploy in real-time scenarios or on servers with limited computational capabilities, particularly for urgent road distress detection tasks by transportation engineers.</p>
<p>Evaluating the progression of road distress detection approaches from traditional techniques to advanced generative methods reveals substantial advancements alongside persistent challenges in each methodology. <a href="#T1" class="usa-link">Table</a> provides a comprehensive summary of related works on image-based data augmentation for road distress detection, highlighting their relative strengths and limitations.</p>
<section class="tw xbox font-sm" id="T1"><h3 class="obj_head">Table.</h3>
<div class="caption p"><p>Summary of related works in image-based data augmentation for road distress detection. Icons by Larea, fatimahazzahra, and BEARicons from The Noun Project (CC BY 3.0).</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" rowspan="1" colspan="1">Category</th>
<th align="center" rowspan="1" colspan="1">Publication</th>
<th align="center" rowspan="1" colspan="1">Method</th>
<th align="center" rowspan="1" colspan="1">Limitation</th>
</tr></thead>
<tbody>
<tr>
<td rowspan="2" align="left" colspan="1">Traditional data augmentation techniques<img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/7fd184c01bc1/research.0833.inline-fig.001.jpg" loading="lazy" alt="Inline graphic">
</td>
<td align="left" rowspan="1" colspan="1">Chen et al. [<a href="#B11" class="usa-link" aria-describedby="B11">11</a>], Xu et al. [<a href="#B12" class="usa-link" aria-describedby="B12">12</a>]</td>
<td align="left" rowspan="1" colspan="1">Traditional augmentation was done through rotation and flipping</td>
<td rowspan="2" align="left" colspan="1">Remaining confined to the existing data distribution limits the model generalization. This makes it challenging to capture region-specific variations in distress patterns</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Wang et al. [<a href="#B13" class="usa-link" aria-describedby="B13">13</a>], Ouma and Hahn [<a href="#B14" class="usa-link" aria-describedby="B14">14</a>]</td>
<td align="left" rowspan="1" colspan="1">Color adjustment and geometric transformations were applied for data augmentation to enhance dataset quality and diversity</td>
</tr>
<tr>
<td rowspan="2" align="left" colspan="1">Domain adaption in transfer learning<img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/ade66f16e7bc/research.0833.inline-fig.002.jpg" loading="lazy" alt="Inline graphic">
</td>
<td align="left" rowspan="1" colspan="1">Li et al. [<a href="#B19" class="usa-link" aria-describedby="B19">19</a>], Wu et al. [<a href="#B20" class="usa-link" aria-describedby="B20">20</a>]</td>
<td align="left" rowspan="1" colspan="1">Transferring existing image data to new scene styles, followed by domain adaptation to apply extracted features to detection in the new scenario</td>
<td rowspan="2" align="left" colspan="1">Facing challenges in road distress detection, this method struggles with domain gaps. Substantial differences between cracks, potholes, and typical objects in standard datasets limit effective knowledge transfer.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Zhang et al. [<a href="#B21" class="usa-link" aria-describedby="B21">21</a>]</td>
<td align="left" rowspan="1" colspan="1">Leveraging stagewise domain adaption RoadDA aligns features and refines intradomain consistency</td>
</tr>
<tr>
<td rowspan="2" align="left" colspan="1">Semi-supervised learning<img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/1e8ed12d55a8/research.0833.inline-fig.003.jpg" loading="lazy" alt="Inline graphic">
</td>
<td align="left" rowspan="1" colspan="1">Shamsabadi et al. [<a href="#B22" class="usa-link" aria-describedby="B22">22</a>], He et al. [<a href="#B23" class="usa-link" aria-describedby="B23">23</a>]</td>
<td align="left" rowspan="1" colspan="1">The framework combines consistency regularization and certainty-based self-training to enhance accuracy with limited labeled data.</td>
<td rowspan="2" align="left" colspan="1">Ensuring pseudo-label reliability in road distress detection is challenging, as irregular, low-contrast cracks and potholes introduce noise, leading to error propagation and performance degradation.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Arce-Sáenz et al. [<a href="#B24" class="usa-link" aria-describedby="B24">24</a>], Tao [<a href="#B25" class="usa-link" aria-describedby="B25">25</a>]</td>
<td align="left" rowspan="1" colspan="1">Employing semi-supervised training, the method uses pseudo-labels for model retraining to enhance performance.</td>
</tr>
<tr>
<td rowspan="2" align="left" colspan="1">Variational autoencoders (VAEs)<img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/d14c6842da10/research.0833.inline-fig.004.jpg" loading="lazy" alt="Inline graphic">
</td>
<td align="left" rowspan="1" colspan="1">Hu et al. [<a href="#B26" class="usa-link" aria-describedby="B26">26</a>]</td>
<td align="left" rowspan="1" colspan="1">Utilizing 4 VAE models, DG-MMF augments bearing fault data by generating multiscale features for improved diagnosis.</td>
<td rowspan="2" align="left" colspan="1">Generating inherently blurry images, VAEs struggle with fine-grained crack detected. This reduces their effectiveness in capturing detailed distress patterns.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Faber et al. [<a href="#B27" class="usa-link" aria-describedby="B27">27</a>]</td>
<td align="left" rowspan="1" colspan="1">Introducing VLAD, a VAE-based lifelong anomaly detection method with hierarchical memory and adaptive model updates.</td>
</tr>
<tr>
<td rowspan="2" align="left" colspan="1">Generative adversarial networks (GANs)<img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/fc7c650ad51f/research.0833.inline-fig.005.jpg" loading="lazy" alt="Inline graphic">
</td>
<td align="left" rowspan="1" colspan="1">Zhang et al. [<a href="#B28" class="usa-link" aria-describedby="B28">28</a>]</td>
<td align="left" rowspan="1" colspan="1">Proposing a commonsense-driven GAN to generate photo-realistic images based on entity-related knowledge.</td>
<td rowspan="2" align="left" colspan="1">While generating realistic images, GANs struggle with mode collapse and artifacts. They cannot produce essential ground-truth annotations like bounding boxes for object detection.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Yang et al. [<a href="#B59" class="usa-link" aria-describedby="B59">59</a>]</td>
<td align="left" rowspan="1" colspan="1">AG-GAN generates virtual pavement images with segregation failure, enabling enhanced analysis and evaluation.</td>
</tr>
<tr>
<td rowspan="2" align="left" colspan="1">Diffusion models<img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/619fa0f02fa4/research.0833.inline-fig.006.jpg" loading="lazy" alt="Inline graphic">
</td>
<td align="left" rowspan="1" colspan="1">Cano-Ortiz et al. [<a href="#B37" class="usa-link" aria-describedby="B37">37</a>]</td>
<td align="left" rowspan="1" colspan="1">A generative diffusion model enhances data augmentation by synthesizing rare defect images with improved quality and efficiency.</td>
<td rowspan="2" align="left" colspan="1">Slow generation speed, limited class control, and the need for manual annotation. These restrict the practical application of diffusion models in real-world engineering.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Zhang et al. [<a href="#B38" class="usa-link" aria-describedby="B38">38</a>]</td>
<td align="left" rowspan="1" colspan="1">CDM enhances crack synthesis with precise control over position and morphology improving detection and segmentation.</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/T1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>In this paper, RoadDiffBox is presented as a lightweight diffusion model specifically designed to address the challenges of road distress generation and detection. The name reflects its application to road infrastructure (“Road”), diffusion model architecture (“Diff”), and automated bounding box annotation capability (“Box”). The model provides several key contributions:</p>
<ul class="list" style="list-style-type:none">
<li>
<span class="label">1.</span><p class="display-inline">RoadDiffBox offers controlled generation of various crack types, effectively creating balanced datasets and mitigating the long-tail problem where certain distress categories are underrepresented, thereby resolving classification challenges in road distress detection by ensuring adequate representation of all damage categories in the training data.</p>
</li>
<li>
<span class="label">2.</span><p class="display-inline">The model automatically produces corresponding bounding boxes during image generation, eliminating manual annotation requirements and enabling semi-supervised learning techniques with minimal labeling costs, effectively addressing the object detection challenges in road distress analysis by providing precise localization data for training robust detection models without human intervention.</p>
</li>
<li>
<span class="label">3.</span><p class="display-inline">The framework employs denoising diffusion implicit model (DDIM) sampling to accelerate image generation and applies knowledge distillation techniques for model compression, allowing efficient deployment on devices with varying computational capacities.</p>
</li>
</ul></section><section id="sec2"><h2 class="pmc_sec_title">Results</h2>
<p>This study’s workflow is divided into 6 main stages, as shown in Fig. <a href="#F1" class="usa-link">1</a>, which illustrates the complete pipeline from limited data collection to real-world deployment:</p>
<ul class="list" style="list-style-type:none">
<li>
<span class="label">1.</span><p class="display-inline">Data Collection: Road distress images were gathered from Germany (Aachen) and China (Beijing) to capture common crack development patterns caused by different environmental and structural factors. This initial dataset requires costly manual annotation by transportation engineers for accurate labeling. The dataset is categorized into 4 types of cracks: longitudinal cracks, alligator cracks, repaired pavement, and transverse cracks. This manually labeled dataset provides essential ground truth and serves as the foundation for subsequent data augmentation through the RoadDiffBox framework.</p>
</li>
<li>
<span class="label">2.</span><p class="display-inline">Design of the RoadDiffBox Framework: RoadDiffBox serves as the core generative model that addresses the data scarcity challenge. The model is trained on the limited manually labeled dataset to learn road distress patterns and characteristics. Once trained, RoadDiffBox can generate diverse road distress images by simply specifying the desired crack type through class control parameters. The framework employs knowledge distillation to optimize the diffusion process, enabling efficient production of synthetic images with automatically generated bounding box annotations, thereby substantially expanding the available training data for detection model development.</p>
</li>
</ul>
<figure class="fig xbox font-sm" id="F1"><h3 class="obj_head">Fig. 1.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/9d66b454ce6c/research.0833.fig.001.jpg" loading="lazy" height="357" width="660" alt="Fig. 1."></p>
<div class="p text-right font-secondary"><a href="figure/F1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Overview of the RoadDiffBox workflow for road distress image generation and detection. Icons by Rahe, Naba A’la Lail, Naba A’la Lail, nangicon, Putri Creative, and okta from The Noun Project (CC BY 3.0). Map tiles adapted from Stamen Design under CC BY 3.0. Data © OpenStreetMap contributors (ODbL).</p></figcaption></figure><ul class="list" style="list-style-type:none">
<li>
<span class="label">3.</span><p class="display-inline">Detection Model Training and Evaluation: The automatically generated annotated dataset from RoadDiffBox is utilized to train various detection and classification models, demonstrating how synthetic data can effectively substitute for large-scale manual annotation. Comparative evaluations are conducted between models trained on the augmented dataset versus those trained on the original limited dataset, revealing substantial performance improvements achieved through data augmentation. The trained models are subsequently validated through real-world field testing, confirming its accuracy and efficiency in practical environments.</p>
</li>
<li>
<span class="label">4.</span><p class="display-inline">Generative Model Generalization: The generative model’s ability to generalize was evaluated using datasets from various geographic regions, particularly examining its performance on Indian road datasets, and further tested on medical imaging data to assess its adaptability beyond road distress detection, demonstrating its robustness across diverse applications.</p>
</li>
<li>
<span class="label">5.</span><p class="display-inline">Deployment on Server-Class Hardware: The optimized model was successfully deployed on servers with varying performance capabilities after applying additional optimizations. This deployment demonstrated the model’s lightweight design, with substantial improvements in generation speed across different server configurations and suitability for road maintenance and monitoring systems.</p>
</li>
<li>
<span class="label">6.</span><p class="display-inline">Development of a Cloud-Based Database and Web-Based Demo: Generated images and detection results are stored and periodically organized for centralized management through cloud-based infrastructure. This database enables engineers and researchers to access annotated road distress data centrally and supports continuous improvement of the generative model by integrating feedback from real-world detection results, fostering an adaptive and self-improving system.</p>
</li>
</ul>
<section id="sec3"><h3 class="pmc_sec_title">RoadDiffBox framework design and image generation</h3>
<p>As shown in Fig. <a href="#F2" class="usa-link">2</a>, the RoadDiffBox framework addresses the specific challenges of road distress detection with emphasis on real-time performance and rapid deployment. This lightweight model enables precise class control across 4 distress categories (longitudinal cracks, alligator cracks, repaired pavement, and transverse cracks) while simultaneously generating corresponding bounding boxes. The architecture incorporates advanced techniques to enhance both generation efficiency and output quality. The automatically generated bounding boxes precisely localize distress areas and serve as ground truth for training detection models without manual annotation, effectively eliminating a major bottleneck in developing accurate detection systems. This approach meets the practical needs of transportation engineers by delivering engineering-level precision while maintaining a substantially reduced parameter count.</p>
<figure class="fig xbox font-sm" id="F2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/fec8eaa898ed/research.0833.fig.002.jpg" loading="lazy" height="660" width="546" alt="Fig. 2."></p>
<div class="p text-right font-secondary"><a href="figure/F2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>RoadDiffBox architecture: Diffusion processes and knowledge distillation. (A) Forward and reverse noise processes in diffusion model. (B) Architecture of teacher and lightweight student models.</p></figcaption></figure><p>Figure <a href="#F2" class="usa-link">2</a>A illustrates the core diffusion process in RoadDiffBox. During the forward diffusion phase, noise is gradually added to the input data, transforming clear images into random noise. In the reverse diffusion phase, the model progressively denoises the data, reconstructing realistic images from the noisy input [<a href="#B39" class="usa-link" aria-describedby="B39">39</a>]. To improve the efficiency of this reverse process, RoadDiffBox employs the DDIM method [<a href="#B40" class="usa-link" aria-describedby="B40">40</a>], which substantially accelerates sampling speed compared to traditional diffusion models, reducing the time required to generate images. Additionally, predefined labels are integrated into the reverse process, enabling the model not only to produce specific types of road distress images but also to generate corresponding bounding boxes. This ensures that the output images are realistic and annotated for direct use in detection tasks.</p>
<p>Figure <a href="#F2" class="usa-link">2</a>B depicts the knowledge distillation process used for model compression within RoadDiffBox. Knowledge distillation plays a crucial role in reducing computational demands while maintaining model effectiveness, enabling efficient deployment in diverse environments [<a href="#B41" class="usa-link" aria-describedby="B41">41</a>]. By utilizing a teacher–student architecture, the large teacher model (60,489,476 parameters) transfers essential knowledge through soft labels, which represent the probability distribution over classes rather than discrete hard labels. Soft labels convey class relationships, allowing the student model to capture subtle distinctions and inter-class similarities, which improves generalization, mitigates overfitting, and enhances robustness to noisy data. As a result, the student model learns critical representations while substantially reducing complexity. The compressed student model, with only 5,040,799 parameters—a 91.67% reduction—preserves key functions such as class control and bounding box generation, ensuring high performance with lower computational resources. This approach enhances inference efficiency and reduces memory consumption while enabling real-time road distress detection on resource-constrained devices, supporting deployment across edge platforms, mobile applications, and cloud-based monitoring systems for scalable and intelligent road maintenance.</p>
<p>To further enhance the model’s generalization capability, self-attention mechanisms were incorporated into the architecture, as shown in Fig. <a href="#F2" class="usa-link">2</a>B. These mechanisms enable the model to capture long-range dependencies within the input data, improving its ability to handle complex road distress patterns. This feature ensures robust performance across various geographic regions and road conditions, making RoadDiffBox adaptable to a range of real-world applications.</p>
<p>Following the establishment of the RoadDiffBox framework, its ability to generate class-controllable road distress images with corresponding labels was tested. Figure <a href="#F3" class="usa-link">3</a> shows the results, displaying 4 different categories of road distress images generated by RoadDiffBox, each with automatically generated bounding boxes. The black and white bounding boxes in the second and fourth rows indicate object localization, where white boxes specifically highlight the detected distress regions within the bounding area. This demonstrates the model’s precise control over crack-type generation, offering a versatile solution for creating diverse, annotated datasets suitable for various conditions.</p>
<figure class="fig xbox font-sm" id="F3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/47c85dcc5c48/research.0833.fig.003.jpg" loading="lazy" height="542" width="660" alt="Fig. 3."></p>
<div class="p text-right font-secondary"><a href="figure/F3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Category-controlled road distress images with generated bounding boxes. (A) Longitudinal crack, (B) alligator crack, (C) repaired pavement, and (D) transverse crack.</p></figcaption></figure></section><section id="sec4"><h3 class="pmc_sec_title">RoadDiffBox model performance evaluation</h3>
<p>The performance of RoadDiffBox was evaluated through classification and object detection tasks to assess the quality and effectiveness of the generated synthetic data for training detection models. These evaluation methods have substantial physical significance, as they directly assess the model’s ability to identify distress types and precisely locate damaged areas on actual road surfaces—capabilities essential for real-world infrastructure maintenance decision-making. A key advantage of the proposed model is its ability to generate images paired with specific class labels and corresponding bounding boxes, enabling training without manual labeling. This approach facilitates semi-supervised learning, as the generated image–label pairs can automatically train detection models, by substantially lowering the need for manually labeled datasets.</p>
<p>To demonstrate RoadDiffBox’s capabilities, the model generated 100,000 image–label pairs across 4 different road distress categories, with detailed dataset distribution shown in Fig. <a href="#F4" class="usa-link">4</a>A. The effectiveness of these pairs was first assessed in classification tasks using 4 different models: ResNet [<a href="#B42" class="usa-link" aria-describedby="B42">42</a>], LeViT [<a href="#B43" class="usa-link" aria-describedby="B43">43</a>], BEiT [<a href="#B44" class="usa-link" aria-describedby="B44">44</a>], and ViT [<a href="#B45" class="usa-link" aria-describedby="B45">45</a>]. The confusion matrices presented in Fig. <a href="#F4" class="usa-link">4</a>B highlight each model’s performance, with F1-scores as follows: ResNet (0.92), LeViT (0.91), BEiT (0.94), and ViT (0.95). These results confirm that RoadDiffBox can generate well-labeled data that are accurately classified across all 4 categories.</p>
<figure class="fig xbox font-sm" id="F4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/5b63337b05cd/research.0833.fig.004.jpg" loading="lazy" height="355" width="660" alt="Fig. 4."></p>
<div class="p text-right font-secondary"><a href="figure/F4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Performance evaluation of RoadDiffBox: Classification and object detection results. (A) Dataset used in experiments. (B) Confusion matrices for classification using ResNet, LeViT, BEiT, and ViT. LO, longitudinal cracks; AG, alligator cracks;p RP, repaired pavement; TR, transverse cracks. (C) Object detection performance of YOLOv8, DiffusionDet, ViTDet, and EfficientDet on generated bounding boxes. (D) On-site detection results using YOLOv8 in Aachen: Real-time testing. Map tiles adapted from Stamen Design under CC BY 3.0. Data © OpenStreetMap contributors (ODbL).</p></figcaption></figure><p>The effectiveness of the generated bounding boxes was further evaluated through object detection tasks. The generated dataset was used to train 4 advanced detection models: YOLOv8 [<a href="#B46" class="usa-link" aria-describedby="B46">46</a>], DiffusionDet [<a href="#B47" class="usa-link" aria-describedby="B47">47</a>], ViTDet [<a href="#B48" class="usa-link" aria-describedby="B48">48</a>], and EfficientDet [<a href="#B49" class="usa-link" aria-describedby="B49">49</a>], as shown in Fig. <a href="#F4" class="usa-link">4</a>C. Each model’s performance was assessed using metrics such as mean average precision (mAP@50), precision, and F1-score. The detection task followed a 4-class classification scheme, distinguishing between longitudinal cracks, alligator cracks, repaired pavement, and transverse cracks, ensuring the evaluation captured class-specific performance. YOLOv8 performed best overall, achieving an mAP@50 of 0.95 and an F1-score of 0.91, which validates the accuracy and efficiency of the bounding boxes generated by RoadDiffBox for training object detection models.</p>
<p>To validate the detection model’s effectiveness under real-world conditions, an on-site detection test was conducted using the top-performing YOLOv8 model. This test took place in Aachen, Germany, with vehicles capturing images in real time as they traversed urban areas, covering a total of 568 images and 738 cracks. As shown in Fig. <a href="#F4" class="usa-link">4</a>D, the model’s detection results were compared with manual inspection, achieving an F1-score of 0.86 and an mAP@50 of 0.91. These results demonstrate the robustness and applicability of RoadDiffBox in practical road distress detection tasks.</p></section><section id="sec5"><h3 class="pmc_sec_title">Evaluation on datasets from different regions and cross-domain applications</h3>
<p>The generalization capability of RoadDiffBox was evaluated on road distress datasets from different regions and tested on a medical skin lesion dataset, providing an assessment of its adaptability across specific domains.</p>
<p>To assess the model’s performance within the same domain but across different geographic regions, RoadDiffBox was applied to an Indian road distress dataset consisting primarily of pothole images [<a href="#B50" class="usa-link" aria-describedby="B50">50</a>]. As shown in Fig. <a href="#F5" class="usa-link">5</a>A, the model successfully generated realistic road distress images with accurate bounding box annotations. These results confirm that the model can adapt to geographic variations in road conditions, maintaining the ability to produce accurately annotated images despite regional differences in distress features.</p>
<figure class="fig xbox font-sm" id="F5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/e7295befbd5b/research.0833.fig.005.jpg" loading="lazy" height="528" width="660" alt="Fig. 5."></p>
<div class="p text-right font-secondary"><a href="figure/F5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Generalization evaluation of RoadDiffBox: Same-domain and cross-domain results. (A) Generated road distress images and bounding boxes on the Indian pothole dataset. (B) Generated medical images and bounding boxes on the skin lesion dataset.</p></figcaption></figure><p>For cross-domain testing, RoadDiffBox was applied to a medical dataset focused on skin lesion images [<a href="#B51" class="usa-link" aria-describedby="B51">51</a>]. As shown in Fig. <a href="#F5" class="usa-link">5</a>B, the model was able to generate high-quality medical images with precise bounding box annotations. This demonstrates the model’s potential to extend beyond its original application in road distress detection and adapt to entirely different fields, such as medical imaging. While these cross-domain results are visually promising, it should be noted that further quantitative evaluation metrics would be helpful to precisely measure the generalization performance in these alternative domains.</p>
<p>In summary, RoadDiffBox demonstrated generalization capability when tested on road distress datasets from different regions and when applied to the medical skin lesion dataset. The incorporation of self-attention mechanisms in the model architecture contributed to its performance across these specific test conditions.</p></section><section id="sec6"><h3 class="pmc_sec_title">Deployment of RoadDiffBox on server-class hardware</h3>
<p>To further evaluate the practicality and efficiency of RoadDiffBox across different levels of computational power, the model was deployed on various server-grade GPUs after optimization through FP16 quantization and TensorRT conversion. These optimization techniques were implemented to reduce the model’s memory footprint and computational requirements while maintaining detection accuracy. FP16 quantization reduces the precision of floating-point calculations from 32 to 16 bits, effectively halving memory usage and increasing processing speed [<a href="#B52" class="usa-link" aria-describedby="B52">52</a>], while TensorRT conversion optimizes the model for NVIDIA GPUs by fusing operations and selecting efficient kernel implementations [<a href="#B46" class="usa-link" aria-describedby="B46">46</a>]. These optimizations were essential to ensure efficient operation on a range of hardware configurations with varying resources, making RoadDiffBox deployable in both resource-constrained field devices and high-performance server environments.</p>
<p>The optimized RoadDiffBox model was tested on 4 different GPUs: RTX 4090, RTX 3080 Ti, RTX 2080 Ti, and RTX 3060, which were selected to represent a range of high-end, mid-range, and previous-generation GPUs commonly used in real-world engineering applications. The RTX 4090 serves as a high-performance reference, the RTX 3080 Ti and RTX 2080 Ti reflect widely used research and industrial setups, while the RTX 3060 represents resource-constrained deployment scenarios. The deployment aimed to assess generation speeds across devices with different performance capabilities. For benchmarking, the time required to generate an image and its corresponding bounding box annotation was measured. Additionally, 2 different batch sizes (1 and 2) were tested to maximize utilization of the computing resources available on each device, and the time required per image was recorded.</p>
<p>The results, as shown in Fig. <a href="#F6" class="usa-link">6</a>A, highlight the model’s efficiency across various GPUs. The RTX 4090, the most powerful device tested, achieved the fastest generation time per image–label pair, averaging just 0.18 s. The RTX 3060, although less powerful and more cost-effective, generated an image in 0.77 s—a performance still suitable for real-world engineering applications that require efficient image generation without extensive hardware investment. These results demonstrate that the lightweight optimizations applied to RoadDiffBox make it highly versatile, capable of rapid image generation even on less powerful hardware.</p>
<figure class="fig xbox font-sm" id="F6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/1b15/12376291/fe94c69601dd/research.0833.fig.006.jpg" loading="lazy" height="268" width="660" alt="Fig. 6."></p>
<div class="p text-right font-secondary"><a href="figure/F6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>RoadDiffBox performance on server-grade GPUs and cloud-based integration. (A) Comparison of image generation speed on server-grade GPUs with different model configurations. (B) Cloud-based database for centralized data storage and model improvement. Icons by shashank singh and Matt Saling from The Noun Project (CC BY 3.0). Map tiles adapted from stamen design under CC BY 3.0. Data © OpenStreetMap contributors (ODbL).</p></figcaption></figure></section><section id="sec7"><h3 class="pmc_sec_title">Development of a cloud-based database and web-based prototype system for RoadDiffBox</h3>
<p>In addition to deploying the RoadDiffBox model for road distress detection, a cloud-based database was constructed to systematically store and manage the model’s generated and detected data. This database is designed to enable centralized data management and provide standardized access to annotated road distress datasets for research and practical applications.</p>
<p>As illustrated in Fig. <a href="#F6" class="usa-link">6</a>B, the outputs generated by RoadDiffBox, including synthesized road distress images and bounding box annotations, are organized within this database for centralized storage and systematic management. This approach will enable centralized access to high-quality, annotated road distress data, benefiting a broad audience of practitioners, researchers, and policymakers engaged in infrastructure management. Additionally, the database incorporates detection results from field-deployed models as part of a self-improving feedback system. These detection outputs are periodically fed back into the training pipeline of the diffusion model, allowing it to adapt and improve its generative capabilities over time. This iterative cycle not only enhances the quality of the synthetic data generated but also progressively refines the accuracy and robustness of the detection models, fostering a sustainable feedback loop for continuous model improvement.</p>
<p>Additionally, a web-based prototype system of RoadDiffBox was developed, providing road maintenance professionals worldwide with a hands-on experience of the model’s capabilities. This platform allows users to explore real-time road distress image generation through an intuitive interface, where they can specify the type of road distress and view the resulting images, complete with bounding box annotations. Access the prototype system here: <a href="http://skingserver.top:44455" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://skingserver.top:44455</a>. This interactive tool offers users a practical glimpse into the potential of RoadDiffBox to enhance road maintenance workflows.</p></section></section><section id="sec8"><h2 class="pmc_sec_title">Discussion</h2>
<p>This study introduces a lightweight diffusion model, RoadDiffBox, specifically designed for generating road distress images. By utilizing DDIM sampling, the model achieves faster generation speeds while incorporating architecture modifications to enable class control, producing images with corresponding bounding boxes. Additionally, the integration of self-attention mechanisms enhances RoadDiffBox’s generalization ability, allowing it to adapt to diverse road conditions across regions. The flexibility of this framework—especially its ability to generate annotated datasets without manual intervention—makes it an ideal tool for implementing real-time road monitoring and maintenance in data-limited and computational resources-limited scenarios.</p>
<p>Performance evaluations have confirmed the effectiveness of RoadDiffBox in generating high-quality image–label pairs. By employing semi-supervised learning, the model substantially reduces reliance on labeled data. The trained model maintains high accuracy in classification and object detection tasks, with on-site experiments further validating the detection model’s effectiveness. Results demonstrate that RoadDiffBox can generate well-labeled data across multiple road distress categories, providing precise bounding boxes for detection models.</p>
<p>Generalization testing showed RoadDiffBox’s ability to process road distress data from different geographic contexts, including successful application to the Indian pothole dataset. The examination of cross-domain application to skin lesion images suggests potential utility beyond road infrastructure, though further domain-specific validation would be required for conclusive assessment of other specialized applications.</p>
<p>For deployment, RoadDiffBox was optimized and successfully implemented on various server-class hardware using TensorRT conversion and FP16 quantization. Results showed a substantial reduction in generation time, with the fastest server (RTX 4090) producing each image–label pair in just 0.18 s, while the more cost-effective RTX 3060 achieved a generation time of 0.77 s per image. These improvements in speed and efficiency demonstrate RoadDiffBox’s adaptability for real-time, cost-effective applications across a range of server configurations, underscoring its practicality for road monitoring systems that require rapid, accurate image generation.</p>
<p>Finally, a cloud-based database was developed to support continuous improvement of the RoadDiffBox framework. The database systematically stores the generated images and detection results, providing centralized access to annotated road distress data through cloud-based infrastructure. Additionally, a web-based demo was created to offer road maintenance professionals an interactive experience, enabling them to specify distress types and view generated images with bounding box annotations in real time. This comprehensive infrastructure delivers 2 key advantages. A feedback loop uses field detection results to continuously enhance the generative model. Additionally, maintenance teams gain broader access to advanced distress detection technology. Together, these benefits enable more responsive and cost-effective infrastructure management strategies.</p>
<p>RoadDiffBox addresses the fundamental challenge of data scarcity and high acquisition costs that have long hindered the development of intelligent road infrastructure monitoring systems. The framework substantially enhances detection model accuracy through data augmentation, enabling more precise automated road assessment and evaluation processes. By generating comprehensive synthetic datasets with accurate annotations, RoadDiffBox facilitates the integration of advanced detection capabilities into pavement management systems, potentially improving the accuracy of pavement condition index assessments. The enhanced data availability and detection precision contribute to more accurate decision-making processes in road maintenance scheduling and resource allocation. The AIGC method demonstrated here holds broader importance, with potential applications in fields such as construction, satellite imaging, medical imaging, and aerospace, where large-scale annotated datasets are essential for monitoring and detection tasks.</p></section><section id="sec9"><h2 class="pmc_sec_title">Methods</h2>
<section id="sec10"><h3 class="pmc_sec_title">Data collection</h3>
<p>This study utilized 3 distinct datasets to comprehensively evaluate RoadDiffBox’s performance, cross-regional generalization, and cross-domain applicability.</p>
<p>The first dataset is used for training the RoadDiffBox model and aims to capture various road distress conditions from different geographic regions. To enhance dataset robustness, images were collected from Germany (Aachen) and China (Beijing), where road distress characteristics differ due to varied environmental and traffic conditions. Aachen experiences moderate seasonal temperature variations and high traffic intensity, particularly on arterial roads and highways, leading to distress patterns influenced by both thermal fluctuations and heavy dynamic loads. Beijing, characterized by extreme seasonal temperature shifts, undergoes pronounced thermal expansion and contraction cycles, impacting pavement durability, especially in resurfaced layers. This dataset includes 778 images covering 4 distinct categories, each annotated with bounding boxes indicating crack locations. While comprehensive, the dataset size remains relatively limited, and additional geographic diversity, as well as variations in imaging conditions such as weather, lighting, and road materials, could further enhance model generalization and adaptability to diverse real-world scenarios.</p>
<p>The second dataset evaluates the model’s generalization capability for cross-regional road distress detection. For this purpose, a pothole image dataset containing 245 images from India was used [<a href="#B50" class="usa-link" aria-describedby="B50">50</a>]. To align with the model’s output format, segmentation masks in this dataset were converted to bounding box annotations. This dataset is crucial for testing the model’s adaptability across different geographic regions.</p>
<p>The third dataset assesses the model’s cross-domain generalization ability. For this, the International Skin Imaging Collaboration Challenge dataset [<a href="#B51" class="usa-link" aria-describedby="B51">51</a>,<a href="#B53" class="usa-link" aria-describedby="B53">53</a>], consisting of 2,594 skin lesion images from the medical field, was used. Similar to the second dataset, segmentation information was converted to bounding box annotations to meet the model’s training requirements. This dataset enables an evaluation of RoadDiffBox’s versatility, demonstrating its ability to generate accurately labeled images in an entirely different domain, highlighting its potential beyond road distress detection.</p></section><section id="sec11"><h3 class="pmc_sec_title">Experiment environment</h3>
<p>The RoadDiffBox model was trained on a system running Ubuntu 22.04, equipped with 4 RTX 4090 GPUs, each with 48 GB of VRAM, using Python 3.8 and PyTorch 2.1. The server GPUs used for deployment included the RTX 4090 (16,384 CUDA cores, 24 GB GDDR6X, 82.6 TFLOPS), RTX 3080 Ti (10,240 CUDA cores, 12 GB GDDR6X, 34.1 TFLOPS), RTX 2080 Ti (4,352 CUDA cores, 11 GB GDDR6, 13.4 TFLOPS), and RTX 3060 (3,584 CUDA cores, 12 GB GDDR6, 12.7 TFLOPS).</p></section><section id="sec12"><h3 class="pmc_sec_title">Diffusion model</h3>
<p>The RoadDiffBox model is based on the diffusion process [<a href="#B39" class="usa-link" aria-describedby="B39">39</a>], which includes 2 key phases: forward diffusion and reverse diffusion. Through this diffusion process, the model can generate road distress images with controllable categories and corresponding bounding box annotations.</p>
<p>In the forward diffusion phase, noise is gradually added over several time steps to both the road distress image and its bounding box, transforming the input into a noisy version. At each time step <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M1" display="inline" overflow="linebreak"><mi>t</mi></math></span>, the model generates noisy versions of the image <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M2" display="inline" overflow="linebreak"><msub><mi>x</mi><mi>t</mi></msub></math></span> and bounding box <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M3" display="inline" overflow="linebreak"><msub><mi>b</mi><mi>t</mi></msub></math></span>. The forward diffusion process is defined as follows [<a href="#B39" class="usa-link" aria-describedby="B39">39</a>]:</p>
<table class="disp-formula p" id="EQ1"><tr>
<td class="formula"><math id="M4" display="block" overflow="linebreak"><mtable><mtr><mtd columnalign="left"><mrow><mi>q</mi><mfenced open="(" close=")" separators=",|,"><msub><mi>x</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><msub><mi>b</mi><mi>t</mi></msub></mrow><mrow><mspace width=".15em"></mspace><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><mspace width=".15em"></mspace><msub><mi>b</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfenced><mo>=</mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mi mathvariant="script">N</mi><mfenced open="(" close=")" separators=";,"><msub><mi>x</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mi>β</mi><mi>t</mi></msub></mrow></msqrt><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><mspace width=".15em"></mspace><msub><mi>β</mi><mi>t</mi></msub><mi>I</mi></mrow></mfenced><mi mathvariant="script">N</mi><mfenced open="(" close=")" separators=";,"><msub><mi>b</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mi>β</mi><mi>t</mi></msub></mrow></msqrt><msub><mi>b</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><mspace width=".15em"></mspace><msub><mi>β</mi><mi>t</mi></msub><mi>I</mi></mrow></mfenced></mrow></mtd></mtr></mtable></math></td>
<td class="label">(1)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M5" display="inline" overflow="linebreak"><msub><mi>β</mi><mi>t</mi></msub></math></span> is a variance schedule that controls the amount of noise added at each step, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M6" display="inline" overflow="linebreak"><mi>I</mi></math></span> is the identity matrix, ensuring that noise is applied isotropically (uniformly in all directions). This process gradually adds Gaussian noise to both the image and bounding box annotations, until they converge to pure noise at the final time step <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M7" display="inline" overflow="linebreak"><mi>T</mi></math></span>.</p>
<p>The reverse diffusion phase aims to recover the original image and bounding box by progressively removing the noise introduced in the forward phase. The model learns the reverse conditional distribution [<a href="#B39" class="usa-link" aria-describedby="B39">39</a>]:</p>
<table class="disp-formula p" id="EQ2"><tr>
<td class="formula"><math id="M8" display="block" overflow="linebreak"><mtable><mtr><mtd columnalign="left"><mrow><mtable><mtr><mtd columnalign="left"><mrow><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>p</mi><mi>θ</mi></msub><mfenced open="(" close=")" separators=",|,,"><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mrow><mspace width=".15em"></mspace><msub><mi>b</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><mspace width=".15em"></mspace><msub><mi>x</mi><mi>t</mi></msub></mrow><mrow><mspace width=".15em"></mspace><msub><mi>b</mi><mi>t</mi></msub></mrow><mrow><mspace width=".15em"></mspace><mi>c</mi></mrow></mfenced><mo>=</mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mi mathvariant="script">N</mi><mfenced open="(" close=")" separators=";,"><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mrow><mspace width=".15em"></mspace><msub><mi>μ</mi><mi>θ</mi></msub><mfenced open="(" close=")" separators=",,"><msub><mi>x</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><mi>t</mi></mrow><mrow><mspace width=".15em"></mspace><mi>c</mi></mrow></mfenced></mrow><mrow><mspace width=".15em"></mspace><msub><mi mathvariant="normal">Σ</mi><mi>θ</mi></msub><mfenced open="(" close=")" separators=",,"><msub><mi>x</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><mi>t</mi></mrow><mrow><mspace width=".15em"></mspace><mi>c</mi></mrow></mfenced></mrow></mfenced><mi mathvariant="script">N</mi><mfenced open="(" close=")" separators=";,"><msub><mi>b</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mrow><mspace width=".15em"></mspace><msub><mi>μ</mi><mi>θ</mi></msub><mfenced open="(" close=")" separators=",,"><msub><mi>b</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><mi>t</mi></mrow><mrow><mspace width=".15em"></mspace><mi>c</mi></mrow></mfenced></mrow><mrow><mspace width=".15em"></mspace><msub><mi mathvariant="normal">Σ</mi><mi>θ</mi></msub><mfenced open="(" close=")" separators=",,"><msub><mi>b</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><mi>t</mi></mrow><mrow><mspace width=".15em"></mspace><mi>c</mi></mrow></mfenced></mrow></mfenced></mrow></mtd></mtr></mtable></mrow></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math></td>
<td class="label">(2)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M9" display="inline" overflow="linebreak"><mi>c</mi></math></span> is the class control input, which allows the model to generate specific types of road distress images, such as longitudinal or reflective cracks. In this equation, <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M10" display="inline" overflow="linebreak"><msub><mi>μ</mi><mi>θ</mi></msub></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M11" display="inline" overflow="linebreak"><msub><mi mathvariant="normal">Σ</mi><mi>θ</mi></msub></math></span> represent the model’s predicted mean and variance for the image and bounding box at each step, conditioned on class <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M12" display="inline" overflow="linebreak"><mi>c</mi></math></span>.</p>
<p>To further accelerate the reverse diffusion process, the DDIM method was applied. This method reduces the number of reverse diffusion steps by providing a deterministic mapping between noisy images and their denoised counterparts, as well as their bounding boxes. For image <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M13" display="inline" overflow="linebreak"><msub><mi>x</mi><mi>t</mi></msub></math></span> and bounding box <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M14" display="inline" overflow="linebreak"><msub><mi>b</mi><mi>t</mi></msub></math></span>, the DDIM reverse process is defined as [<a href="#B40" class="usa-link" aria-describedby="B40">40</a>]:</p>
<table class="disp-formula p" id="EQ3"><tr>
<td class="formula"><math id="M15" display="block" overflow="linebreak"><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msqrt><msub><mi>α</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></msqrt><mfenced open="(" close=")"><mfrac><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>−</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mi>α</mi><mi>t</mi></msub></mrow></msqrt><msub><mi>ϵ</mi><mi>θ</mi></msub><mfenced open="(" close=")" separators=","><msub><mi>x</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><mi>t</mi></mrow></mfenced></mrow><msqrt><msub><mi>α</mi><mi>t</mi></msub></msqrt></mfrac></mfenced><mo>+</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mi>α</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msqrt><msub><mi>ϵ</mi><mi>θ</mi></msub><mfenced open="(" close=")" separators=","><msub><mi>x</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><mi>t</mi></mrow></mfenced></mrow></mtd></mtr></mtable></math></td>
<td class="label">(3)</td>
</tr></table>
<table class="disp-formula p" id="EQ4"><tr>
<td class="formula"><math id="M16" display="block" overflow="linebreak"><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>b</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msqrt><msub><mi>α</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></msqrt><mfenced open="(" close=")"><mfrac><mrow><msub><mi>b</mi><mi>t</mi></msub><mo>−</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mi>α</mi><mi>t</mi></msub></mrow></msqrt><msub><mi>ϵ</mi><mi>θ</mi></msub><mfenced open="(" close=")" separators=","><msub><mi>b</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><mi>t</mi></mrow></mfenced></mrow><msqrt><msub><mi>α</mi><mi>t</mi></msub></msqrt></mfrac></mfenced><mo>+</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mi>α</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msqrt><msub><mi>ϵ</mi><mi>θ</mi></msub><mfenced open="(" close=")" separators=","><msub><mi>b</mi><mi>t</mi></msub><mrow><mspace width=".15em"></mspace><mi>t</mi></mrow></mfenced></mrow></mtd></mtr></mtable></math></td>
<td class="label">(4)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M17" display="inline" overflow="linebreak"><msub><mi>α</mi><mi>t</mi></msub></math></span> is the noise schedule, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M18" display="inline" overflow="linebreak"><msub><mi>ϵ</mi><mi>θ</mi></msub></math></span> represents the model’s predicted noise at each step for both the image and bounding box. This approach substantially speeds up the generation process while preserving the quality of both the image and bounding box.</p>
<p>The model was trained over 20 million iterations with a learning rate of 1×10<sup>−4</sup> and a batch size of 16, ensuring efficient convergence and robust performance.</p></section><section id="sec13"><h3 class="pmc_sec_title">Knowledge distillation</h3>
<p>To construct the RoadDiffBox model, knowledge distillation [<a href="#B41" class="usa-link" aria-describedby="B41">41</a>] was applied. Knowledge distillation is a technique that transfers knowledge from a larger, more complex model (the teacher model) to a smaller, more efficient model (the student model). The main objective of this process is to enable the student model to achieve performance comparable to the teacher model while substantially reducing the number of parameters and computational requirements.</p>
<p>In the distillation process, the teacher model is first trained on the dataset, and the student model is then trained to mimic its output. Traditionally, models are trained using hard labels, where each sample is assigned a single discrete class, treating all incorrect classes as equally distant from the correct one [<a href="#B41" class="usa-link" aria-describedby="B41">41</a>]. While effective, this approach can lead to overfitting and poor generalization, especially in cases with ambiguous or noisy data. Instead, the student model in knowledge distillation learns from soft labels generated by the teacher model. Soft labels represent a probability distribution over all possible classes rather than a single categorical value, conveying relative confidence levels across different classes [<a href="#B41" class="usa-link" aria-describedby="B41">41</a>]. This provides richer information about class relationships and uncertainty, allowing the student model to capture finer distinctions, improve generalization, and enhance robustness to noise. By utilizing these more informative targets, the student model effectively retains essential knowledge while achieving substantial reductions in computational complexity.</p>
<p>In this work, mean squared error (MSE) loss [<a href="#B54" class="usa-link" aria-describedby="B54">54</a>] was used as the distillation loss to measure the difference between the predictions of the student and teacher models. The objective is to minimize the MSE between the teacher’s output <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M19" display="inline" overflow="linebreak"><msub><mi>z</mi><mtext>teacher</mtext></msub></math></span> and the student’s output <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M20" display="inline" overflow="linebreak"><msub><mi>z</mi><mtext>student</mtext></msub></math></span>, expressed as:</p>
<table class="disp-formula p" id="EQ5"><tr>
<td class="formula"><math id="M21" display="block" overflow="linebreak"><mtable><mtr><mtd columnalign="left"><mrow><mtext>MSE</mtext><mspace width=".15em"></mspace><mtext>Loss</mtext><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mfenced open="(" close=")"><mrow><msubsup><mi>z</mi><mtext>teacher</mtext><mfenced open="(" close=")"><mi>i</mi></mfenced></msubsup><mo>−</mo><msubsup><mi>z</mi><mtext>student</mtext><mfenced open="(" close=")"><mi>i</mi></mfenced></msubsup></mrow></mfenced><mn>2</mn></msup></mrow></mtd></mtr></mtable></math></td>
<td class="label">(5)</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M22" display="inline" overflow="linebreak"><mi>n</mi></math></span> is the number of predictions, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M23" display="inline" overflow="linebreak"><msubsup><mi>z</mi><mtext>teacher</mtext><mfenced open="(" close=")"><mi>i</mi></mfenced></msubsup></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML"><math id="M24" display="inline" overflow="linebreak"><msubsup><mi>z</mi><mtext>student</mtext><mfenced open="(" close=")"><mi>i</mi></mfenced></msubsup></math></span> represent the outputs of the teacher and student models, respectively.</p>
<p>By applying this distillation loss, the student model effectively learns from the teacher model and approximates its performance while substantially reducing the number of parameters. In this implementation, the student model achieved a 91.67% reduction in parameters compared to the teacher model, while maintaining similar accuracy in image generation and bounding box prediction.</p></section><section id="sec14"><h3 class="pmc_sec_title">Evaluation metrics</h3>
<p>To comprehensively evaluate the performance of the RoadDiffBox model, several key metrics tailored for classification and detection tasks were used. These metrics help assess the model’s ability to generate accurately labeled road distress images and bounding box annotations.</p>
<p>For classification tasks, a confusion matrix [<a href="#B55" class="usa-link" aria-describedby="B55">55</a>] and F1-score [<a href="#B56" class="usa-link" aria-describedby="B56">56</a>] were used as evaluation metrics. For detection tasks, metrics included mAP@50 [<a href="#B57" class="usa-link" aria-describedby="B57">57</a>], precision, and F1-score.</p>
<p>The F1-score, a harmonic mean of precision and recall, provides a balanced measure of model performance, especially useful in cases of class imbalance. It is defined as:</p>
<table class="disp-formula p" id="EQ6"><tr>
<td class="formula"><math id="M25" display="block" overflow="linebreak"><mi mathvariant="normal">F</mi><mn>1</mn><mo>-</mo><mtext>score</mtext><mo>=</mo><mn>2</mn><mo>×</mo><mfrac><mrow><mtext>Precision</mtext><mo>×</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></math></td>
<td class="label">(6)</td>
</tr></table>
<p>mAP@50 assesses the overlap between predicted and ground-truth bounding boxes, with an intersection over union (IoU) threshold of 0.50. The IoU formula is:</p>
<table class="disp-formula p" id="EQ7"><tr>
<td class="formula"><math id="M26" display="block" overflow="linebreak"><mi>IoU</mi><mo>=</mo><mfrac><mtext>Area of overlap</mtext><mtext>Area of union</mtext></mfrac></math></td>
<td class="label">(7)</td>
</tr></table>
<p>mAP@50 is calculated by averaging the precision across different recall levels when IoU is at least 0.50.</p>
<p>Precision measures the proportion of true positive predictions among all positive predictions, making it a valuable metric for both classification and detection tasks. It is defined as:</p>
<table class="disp-formula p" id="EQ8"><tr>
<td class="formula"><math id="M27" display="block" overflow="linebreak"><mtext>Precision</mtext><mo>=</mo><mfrac><mi>TP</mi><mrow><mi>TP</mi><mo>+</mo><mi>FP</mi></mrow></mfrac></math></td>
<td class="label">(8)</td>
</tr></table>
<p>Using these metrics, the RoadDiffBox model’s performance in both classification and detection was thoroughly evaluated, ensuring accurate predictions in image classification and bounding box generation tasks.</p></section><section id="sec15"><h3 class="pmc_sec_title">Semi-supervised learning for road distress detection</h3>
<p>Semi-supervised learning is a machine learning paradigm that leverages both labeled and unlabeled data to enhance model performance while reducing dependence on extensive manual annotations [<a href="#B58" class="usa-link" aria-describedby="B58">58</a>]. Traditional semi-supervised learning approaches, such as pseudo-labeling and consistency regularization, enable models to learn from a small set of labeled samples while extracting meaningful patterns from unlabeled data. This strategy is particularly useful in applications where large-scale annotation is costly and labor-intensive.</p>
<p>In this study, semi-supervised learning is defined as a training strategy that eliminates the need for additional manual annotation by directly utilizing the automatically generated dataset from RoadDiffBox. Unlike conventional semi-supervised learning methods that require human-annotated seed datasets, the proposed framework benefits from RoadDiffBox’s ability to generate synthetic road distress images along with corresponding bounding box annotations. The generated dataset can be seamlessly integrated into object detection model training, creating a semi-supervised learning pipeline that does not rely on additional labeling efforts.</p>
<p>The semi-supervised learning process begins with RRoadDiffBox generating a diverse dataset of road distress images, each automatically annotated with bounding boxes. These synthetic images complement real-world labeled data, expanding the distribution of distress patterns encountered during training. A base object detection model, pre-trained on real-world samples, is further refined using the generated dataset, enabling it to learn from both real and synthetic data. By leveraging RoadDiffBox’s annotations, the model improves its ability to generalize to unseen road conditions, while self-training further enhances feature learning and reduces dependence on manual labeling.</p></section><section id="sec16"><h3 class="pmc_sec_title">RoadDiffBox model deployment on server-class hardware</h3>
<p>To ensure that the RoadDiffBox model could be effectively deployed on server-class hardware for real-time road condition detection, a series of optimization steps were implemented. First, FP16 quantization was applied, reducing the model’s computational precision from FP32 to FP16 (16-bit floating point). This reduction substantially lowered memory requirements and computational load, which is essential for efficient operation on a variety of server configurations.</p>
<p>Next, the quantized model was converted to TensorRT format. This conversion introduced key performance improvements through techniques such as layer fusion, precision calibration, and kernel auto-tuning, which greatly reduced inference time. These optimizations allowed the model to operate efficiently across different NVIDIA GPUs.</p>
<p>The optimized model was then deployed on 4 server GPUs: RTX 4090, RTX 3080 Ti, RTX 2080 Ti, and RTX 3060, each with distinct performance capabilities. This deployment enabled a comprehensive evaluation of the model’s efficiency across various hardware configurations, from high-performance to more cost-effective, energy-efficient options. The results validated the model’s adaptability and scalability, demonstrating its suitability for road monitoring applications on servers with varying computational resources.</p></section></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgments</h2>
<p>The language and grammar of this manuscript were partially refined using a large language model (Claude). The authors would like to thank the Autodl platform for providing access to server-class hardware, including RTX 4090, RTX 3080 Ti, RTX 2080 Ti, and RTX 3060 GPUs.</p>
<p><strong>Funding:</strong> This paper is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - SFB/TRR 339, Project-ID 453596084. The authors are solely responsible for the content.</p>
<p><strong>Author contributions:</strong> Y. Hu conceived the idea, designed the experiments, and led the development of the RoadDiffBox model. N.C. and H.Z. conducted the data collection, model training, and performance evaluation. Y. Hu and H.Z. were responsible for developing the deployment strategy and implementing the model on server-class hardware. Y. Hou and P.L. provided guidance as corresponding authors, contributed to the methodology, and supervised the overall research process. All authors contributed to the writing and revision of the manuscript.</p>
<p><strong>Competing interests:</strong> The authors declare that they have no competing interests.</p></section><section id="sec17"><h2 class="pmc_sec_title">Data Availability</h2>
<p>The datasets and code supporting this study are openly available at <a href="http://skingserver.top:44455" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://skingserver.top:44455</a>. Further details and inquiries can be directed to the corresponding author.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="B1">
<span class="label">1.</span><cite>El Hakea AH, Fakhr MW. 
Recent computer vision applications for pavement distress and condition assessment. Autom Constr. 2023;146:
Article 104664.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Autom%20Constr&amp;title=Recent%20computer%20vision%20applications%20for%20pavement%20distress%20and%20condition%20assessment&amp;volume=146&amp;publication_year=2023&amp;pages=Article%20104664&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B2">
<span class="label">2.</span><cite>Valipour PS, Golroo A, Kheirati A, Fahmani M, Amani MJ. 
Automatic pavement distress severity detection using deep learning. Road Mater Pavement Des. 2024;25(8):1830–1846.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Road%20Mater%20Pavement%20Des&amp;title=Automatic%20pavement%20distress%20severity%20detection%20using%20deep%20learning&amp;volume=25&amp;issue=8&amp;publication_year=2024&amp;pages=1830-1846&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B3">
<span class="label">3.</span><cite>Kothai R, Prabakaran N, Srinivasa Murthy YV, Reddy Cenkeramaddi L, Kakani V. 
Pavement distress detection, classification, and analysis using machine learning algorithms: A survey. IEEE Access. 2024;12:126943–126960.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=Pavement%20distress%20detection,%20classification,%20and%20analysis%20using%20machine%20learning%20algorithms:%20A%20survey&amp;volume=12&amp;publication_year=2024&amp;pages=126943-126960&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B4">
<span class="label">4.</span><cite>Zhang J, Sun S, Song W, Li Y, Teng Q. 
Automated pavement distress detection based on convolutional neural network. IEEE Access. 2024;12:105055–105068.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Access&amp;title=Automated%20pavement%20distress%20detection%20based%20on%20convolutional%20neural%20network&amp;volume=12&amp;publication_year=2024&amp;pages=105055-105068&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B5">
<span class="label">5.</span><cite>Wang X, Huang J, Tian Y, Sun C, Yang L, Lou S, Lv C, Sun C, Wang FY. 
Parallel driving with big models and foundation intelligence in cyber–physical–social spaces. Research. 2024;7:
Article 0349.
</cite> [<a href="https://doi.org/10.34133/research.0349" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11103184/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38770105/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Research&amp;title=Parallel%20driving%20with%20big%20models%20and%20foundation%20intelligence%20in%20cyber%E2%80%93physical%E2%80%93social%20spaces&amp;volume=7&amp;publication_year=2024&amp;pages=Article%200349&amp;pmid=38770105&amp;doi=10.34133/research.0349&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B6">
<span class="label">6.</span><cite>Zihan ZU, Smadi O, Tilberg M, Yamany MS. 
Synthesizing the performance of deep learning in vision-based pavement distress detection. Innov Infrastruct Solut. 2023;8:
Article 299.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Innov%20Infrastruct%20Solut&amp;title=Synthesizing%20the%20performance%20of%20deep%20learning%20in%20vision-based%20pavement%20distress%20detection&amp;volume=8&amp;publication_year=2023&amp;pages=Article%20299&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B7">
<span class="label">7.</span><cite>Mei S, Lian J, Wang X, Su Y, Ma M, Chau LP. 
A comprehensive study on the robustness of deep learning-based image classification and object detection in remote sensing: Surveying and benchmarking. J Remote Sens. 2024;4:
Article 0219.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=J%20Remote%20Sens&amp;title=A%20comprehensive%20study%20on%20the%20robustness%20of%20deep%20learning-based%20image%20classification%20and%20object%20detection%20in%20remote%20sensing:%20Surveying%20and%20benchmarking&amp;volume=4&amp;publication_year=2024&amp;pages=Article%200219&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B8">
<span class="label">8.</span><cite>Zheng L, Xiao J, Wang Y, Wu W, Chen Z, Yuan D, Jiang W. 
Deep learning-based intelligent detection of pavement distress. Autom Constr. 2024;168:
Article 105772.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Autom%20Constr&amp;title=Deep%20learning-based%20intelligent%20detection%20of%20pavement%20distress&amp;volume=168&amp;publication_year=2024&amp;pages=Article%20105772&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B9">
<span class="label">9.</span><cite>Reiss MV. Testing the reliability of chatgpt for text annotation and classification: A cautionary remark. arXiv. 2023. 10.48550/arXiv.2304.11085</cite> [<a href="https://doi.org/10.48550/arXiv.2304.11085" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B10">
<span class="label">10.</span><cite>Jiao L, Wang Y, Liu X, Li L, Liu F, Ma W, Guo Y, Chen P, Yang S, Hou B. 
Causal inference meets deep learning: A comprehensive survey. Research. 2024;7:
Article 0467.
</cite> [<a href="https://doi.org/10.34133/research.0467" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11384545/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39257419/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Research&amp;title=Causal%20inference%20meets%20deep%20learning:%20A%20comprehensive%20survey&amp;volume=7&amp;publication_year=2024&amp;pages=Article%200467&amp;pmid=39257419&amp;doi=10.34133/research.0467&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B11">
<span class="label">11.</span><cite>Chen N, Xu Z, Liu Z, Chen Y, Miao Y, Li Q, Hou Y, Wang L. 
Data augmentation and intelligent recognition in pavement texture using a deep learning. IEEE Trans Intell Transp Syst. 2022;23(12):25427–25436.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Intell%20Transp%20Syst&amp;title=Data%20augmentation%20and%20intelligent%20recognition%20in%20pavement%20texture%20using%20a%20deep%20learning&amp;volume=23&amp;issue=12&amp;publication_year=2022&amp;pages=25427-25436&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B12">
<span class="label">12.</span><cite>Shen S, Xu M, Zhang F, Shao P, Liu H, Xu L, Zhang C, Liu P, Yao P, Xu RX. 
A low-cost high-performance data augmentation for deep learning-based skin lesion classification. BME Front. 2022.</cite> [<a href="https://doi.org/10.34133/2022/9765307" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10521644/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37850173/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=BME%20Front&amp;title=A%20low-cost%20high-performance%20data%20augmentation%20for%20deep%20learning-based%20skin%20lesion%20classification&amp;publication_year=2022&amp;pmid=37850173&amp;doi=10.34133/2022/9765307&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B13">
<span class="label">13.</span><cite>Wang D, Liu Z, Gu X, Wu W, Chen Y, Wang L. 
Automatic detection of pothole distress in asphalt pavement using improved convolutional neural networks. Remote Sens. 2022;14(16):
Article 3892.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Remote%20Sens&amp;title=Automatic%20detection%20of%20pothole%20distress%20in%20asphalt%20pavement%20using%20improved%20convolutional%20neural%20networks&amp;volume=14&amp;issue=16&amp;publication_year=2022&amp;pages=Article%203892&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B14">
<span class="label">14.</span><cite>Ouma YO, Hahn M. 
Wavelet-morphology based detection of incipient linear cracks in asphalt pavements from RGB camera imagery and classification using circular radon transform. Adv Eng Inform. 2016;30(3):481–499.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Eng%20Inform&amp;title=Wavelet-morphology%20based%20detection%20of%20incipient%20linear%20cracks%20in%20asphalt%20pavements%20from%20RGB%20camera%20imagery%20and%20classification%20using%20circular%20radon%20transform&amp;volume=30&amp;issue=3&amp;publication_year=2016&amp;pages=481-499&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B15">
<span class="label">15.</span><cite>Praticò FG, Fedele R, Naumov V, Sauer T. 
Detection and monitoring of bottom-up cracks in road pavement using a machine-learning approach. Algorithms. 2020;13(4):
Article 81.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Algorithms&amp;title=Detection%20and%20monitoring%20of%20bottom-up%20cracks%20in%20road%20pavement%20using%20a%20machine-learning%20approach&amp;volume=13&amp;issue=4&amp;publication_year=2020&amp;pages=Article%2081&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B16">
<span class="label">16.</span><cite>Liu Q, Liu Z. Method for the detection of road bridge pavement crack depth based on acoustic signal analysis. In: <em>International Conference on Remote Sensing, Surveying, and Mapping (RSSM 2024)</em>. SPIE; 2024. Vol. 13170, p. 144–51.</cite>
</li>
<li id="B17">
<span class="label">17.</span><cite>Raslan E, Alrahmawy MF, Mohammed Y, Tolba AS. 
Evaluation of data representation techniques for vibration based road surface condition classification. Sci Rep. 2024;14:
Article 11620.
</cite> [<a href="https://doi.org/10.1038/s41598-024-61757-1" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11109277/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38773123/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Rep&amp;title=Evaluation%20of%20data%20representation%20techniques%20for%20vibration%20based%20road%20surface%20condition%20classification&amp;volume=14&amp;publication_year=2024&amp;pages=Article%2011620&amp;pmid=38773123&amp;doi=10.1038/s41598-024-61757-1&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B18">
<span class="label">18.</span><cite>Zar A, Hussain Z, Akbar M, Rabczuk T, Lin Z, Li S, Ahmed B. 
Towards vibration-based damage detection of civil engineering structures: Overview, challenges, and future prospects. Int J Mech Mater Des. 2024;20(3):591–662.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Mech%20Mater%20Des&amp;title=Towards%20vibration-based%20damage%20detection%20of%20civil%20engineering%20structures:%20Overview,%20challenges,%20and%20future%20prospects&amp;volume=20&amp;issue=3&amp;publication_year=2024&amp;pages=591-662&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B19">
<span class="label">19.</span><cite>Li Y, Che P, Liu C, Wu D, Du Y. 
Cross-scene pavement distress detection by a novel transfer learning framework. Comput Aided Civ Inf Eng. 2021;36(11):1398–1415.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Aided%20Civ%20Inf%20Eng&amp;title=Cross-scene%20pavement%20distress%20detection%20by%20a%20novel%20transfer%20learning%20framework&amp;volume=36&amp;issue=11&amp;publication_year=2021&amp;pages=1398-1415&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B20">
<span class="label">20.</span><cite>Wu Y, Hong M, Li A, Huang S, Liu H, Ge Y. 
Self-supervised adversarial learning for domain adaptation of pavement distress classification. IEEE Trans Intell Transp Syst. 2023;25(2):1966–1977.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Intell%20Transp%20Syst&amp;title=Self-supervised%20adversarial%20learning%20for%20domain%20adaptation%20of%20pavement%20distress%20classification&amp;volume=25&amp;issue=2&amp;publication_year=2023&amp;pages=1966-1977&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B21">
<span class="label">21.</span><cite>Zhang L, Lan M, Zhang J, Tao D. 
Stagewise unsupervised domain adaptation with adversarial self-training for road segmentation of remote-sensing images. IEEE Trans Geosci Remote Sens. 2021;60:1–13.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Geosci%20Remote%20Sens&amp;title=Stagewise%20unsupervised%20domain%20adaptation%20with%20adversarial%20self-training%20for%20road%20segmentation%20of%20remote-sensing%20images&amp;volume=60&amp;publication_year=2021&amp;pages=1-13&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B22">
<span class="label">22.</span><cite>Shamsabadi EA, Erfani SMH, Xu C, Dias-da-Costa D. 
Efficient semi-supervised surface crack segmentation with small datasets based on consistency regularisation and pseudo-labelling. Autom Constr. 2024;158:
Article 105181.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Autom%20Constr&amp;title=Efficient%20semi-supervised%20surface%20crack%20segmentation%20with%20small%20datasets%20based%20on%20consistency%20regularisation%20and%20pseudo-labelling&amp;volume=158&amp;publication_year=2024&amp;pages=Article%20105181&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B23">
<span class="label">23.</span><cite>He Q, Yan K, Luo Q, Yi D, Wang P, Han H, Liu D. 
Exploring unlabeled data in multiple aspects for semi-supervised MRI segmentation. Health Data Sci. 2024;4:
Article 0166.
</cite> [<a href="https://doi.org/10.34133/hds.0166" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11298716/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39104600/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Health%20Data%20Sci&amp;title=Exploring%20unlabeled%20data%20in%20multiple%20aspects%20for%20semi-supervised%20MRI%20segmentation&amp;volume=4&amp;publication_year=2024&amp;pages=Article%200166&amp;pmid=39104600&amp;doi=10.34133/hds.0166&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B24">
<span class="label">24.</span><cite>Arce-Sáenz LA, Izquierdo-Reyes J, and Bustamante-Bello R. Road surface monitoring system using semi-supervised machine learning ensemble models. In: <em>2023 International Symposium on Electromobility (ISEM)</em>. IEEE; 2023. p. 1–6.</cite>
</li>
<li id="B25">
<span class="label">25.</span><cite>Tao H. 
Weakly-supervised pavement surface crack segmentation based on dual separation and domain generalization. IEEE Trans Intell Transp Syst. 2024;25(12):19729–19743.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Intell%20Transp%20Syst&amp;title=Weakly-supervised%20pavement%20surface%20crack%20segmentation%20based%20on%20dual%20separation%20and%20domain%20generalization&amp;volume=25&amp;issue=12&amp;publication_year=2024&amp;pages=19729-19743&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B26">
<span class="label">26.</span><cite>Hu H, Cai Y, Hu Q, Zhang Y. 
A deep generative model with multiscale features enabled industrial internet of things for intelligent fault diagnosis of bearings. Research. 2023;6:
Article 0176.
</cite> [<a href="https://doi.org/10.34133/research.0176" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10328390/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37426474/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Research&amp;title=A%20deep%20generative%20model%20with%20multiscale%20features%20enabled%20industrial%20internet%20of%20things%20for%20intelligent%20fault%20diagnosis%20of%20bearings&amp;volume=6&amp;publication_year=2023&amp;pages=Article%200176&amp;pmid=37426474&amp;doi=10.34133/research.0176&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B27">
<span class="label">27.</span><cite>Faber K, Corizzo R, Sniezynski B, Japkowicz N. 
VLAD: Task-agnostic VAE-based lifelong anomaly detection. Neural Netw. 2023;165:248–273.
</cite> [<a href="https://doi.org/10.1016/j.neunet.2023.05.032" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37307668/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neural%20Netw&amp;title=VLAD:%20Task-agnostic%20VAE-based%20lifelong%20anomaly%20detection&amp;volume=165&amp;publication_year=2023&amp;pages=248-273&amp;pmid=37307668&amp;doi=10.1016/j.neunet.2023.05.032&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B28">
<span class="label">28.</span><cite>Zhang G, Xu N, Yan C, Zheng B, Duan Y, Lv B, Liu AA. 
CD-GAN: Commonsense-driven generative adversarial network with hierarchical refinement for text-to-image synthesis. Intell Comput. 2023;2:
Article 0017.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Intell%20Comput&amp;title=CD-GAN:%20Commonsense-driven%20generative%20adversarial%20network%20with%20hierarchical%20refinement%20for%20text-to-image%20synthesis&amp;volume=2&amp;publication_year=2023&amp;pages=Article%200017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B29">
<span class="label">29.</span><cite>Nayak AA, Venugopala P, Ashwini B. 
A systematic review on generative adversarial network (GAN): Challenges and future directions. Arch Comput Method Eng. 2024;31(8):4739–4772.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Arch%20Comput%20Method%20Eng&amp;title=A%20systematic%20review%20on%20generative%20adversarial%20network%20(GAN):%20Challenges%20and%20future%20directions&amp;volume=31&amp;issue=8&amp;publication_year=2024&amp;pages=4739-4772&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B30">
<span class="label">30.</span><cite>Cao Y, Li S, Liu Y, Yan Z, Dai Y, Yu PS, Sun L. A comprehensive survey of AI-generated content (AIGC): A history of generative AI from GAN to ChatGPT. arXiv. 2023. 10.48550/arXiv.2303.04226</cite> [<a href="https://doi.org/10.48550/arXiv.2303.04226" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B31">
<span class="label">31.</span><cite>Xu M, Du H, Niyato D, Kang J, Xiong Z, Mao S, Han Z, Jamalipour A, Kim DI, Shen X, et al. 
Unleashing the power of edge-cloud generative ai in mobile networks: A survey of AIGC services. IEEE Commun Surv Tutor. 2024;26(2):1127–1170.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Commun%20Surv%20Tutor&amp;title=Unleashing%20the%20power%20of%20edge-cloud%20generative%20ai%20in%20mobile%20networks:%20A%20survey%20of%20AIGC%20services&amp;volume=26&amp;issue=2&amp;publication_year=2024&amp;pages=1127-1170&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B32">
<span class="label">32.</span><cite>Croitoru FA, Hondru V, Ionescu RT, Shah M. 
Diffusion models in vision: A survey. IEEE Trans Pattern Anal Mach Intell. 2023;45:10850–10869.
</cite> [<a href="https://doi.org/10.1109/TPAMI.2023.3261988" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37030794/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;title=Diffusion%20models%20in%20vision:%20A%20survey&amp;volume=45&amp;publication_year=2023&amp;pages=10850-10869&amp;pmid=37030794&amp;doi=10.1109/TPAMI.2023.3261988&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B33">
<span class="label">33.</span><cite>Yang L, Zhang Z, Song Y, Hong S, Xu R, Zhao Y, Zhang W, Cui B, Yang MH. 
Diffusion models: A comprehensive survey of methods and applications. ACM Comput Surv. 2023;56(4):
Article 105.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Comput%20Surv&amp;title=Diffusion%20models:%20A%20comprehensive%20survey%20of%20methods%20and%20applications&amp;volume=56&amp;issue=4&amp;publication_year=2023&amp;pages=Article%20105&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B34">
<span class="label">34.</span><cite>Akrout M, Gyepesi B, Holló P, Poór A, Kincső B, Solis S, Cirone K, Kawahara J, Slade D, Abid L, et al. Diffusion-based data augmentation for skin disease classification: Impact across original medical datasets to fully synthetic images. arXiv. 2023. 10.48550/arXiv.2301.04802</cite> [<a href="https://doi.org/10.48550/arXiv.2301.04802" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B35">
<span class="label">35.</span><cite>Tang Y, He H, Wang Y, Wu Y. 
Using a diffusion model for pedestrian trajectory prediction in semi-open autonomous driving environments. IEEE Sensors J. 2024;24(10):17208–17218.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Sensors%20J&amp;title=Using%20a%20diffusion%20model%20for%20pedestrian%20trajectory%20prediction%20in%20semi-open%20autonomous%20driving%20environments&amp;volume=24&amp;issue=10&amp;publication_year=2024&amp;pages=17208-17218&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B36">
<span class="label">36.</span><cite>Yang B, Su H, Gkanatsios N, Ke TW, Jain A, Schneider J, Fragkiadaki K.. Diffusion-ES: Gradient-free planning with diffusion for autonomous driving and zero-shot instruction following. arXiv. 2024. 10.48550/arXiv.2402.06559</cite> [<a href="https://doi.org/10.48550/arXiv.2402.06559" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B37">
<span class="label">37.</span><cite>Cano-Ortiz S, Iglesias LL, Árbol PM, Castro-Fresno D. 
Improving detection of asphalt distresses with deep learning-based diffusion model for intelligent road maintenance. Dev Built Environ. 2024;17:
Article 100315.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Dev%20Built%20Environ&amp;title=Improving%20detection%20of%20asphalt%20distresses%20with%20deep%20learning-based%20diffusion%20model%20for%20intelligent%20road%20maintenance&amp;volume=17&amp;publication_year=2024&amp;pages=Article%20100315&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B38">
<span class="label">38.</span><cite>Zhang H, Qian Z, Zhou W, Min Y, Liu P. 
A controllable generative model for generating pavement crack images in complex scenes. Comput Aided Civ Infrastruct Eng. 2024;39(12):1795–1810.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Comput%20Aided%20Civ%20Infrastruct%20Eng&amp;title=A%20controllable%20generative%20model%20for%20generating%20pavement%20crack%20images%20in%20complex%20scenes&amp;volume=39&amp;issue=12&amp;publication_year=2024&amp;pages=1795-1810&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B39">
<span class="label">39.</span><cite>Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models. arXiv. 2020. 10.48550/arXiv.2006.11239</cite> [<a href="https://doi.org/10.48550/arXiv.2006.11239" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B40">
<span class="label">40.</span><cite>Song J,Meng C, Ermon S. Denoising diffusion implicit models. arXiv. 2020. 10.48550/arXiv.2010.02502</cite> [<a href="https://doi.org/10.48550/arXiv.2010.02502" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B41">
<span class="label">41.</span><cite>Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network. arXiv. 2015. <a href="https://arxiv.org/abs/1503.02531" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1503.02531</a></cite>
</li>
<li id="B42">
<span class="label">42.</span><cite>He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. IEEE; 2016. p. 770–778.</cite>
</li>
<li id="B43">
<span class="label">43.</span><cite>Graham B, El-Nouby A, Touvron H, Stock P, Joulin A, Jégou H, Douze M. LeViT: A vision transformer in ConvNet’s clothing for faster inference. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>. IEEE; 2021. p. 12259–12269.</cite>
</li>
<li id="B44">
<span class="label">44.</span><cite>Bao H, Dong L, Piao S, Wei F. BEiT: BERT pre-training of image transformers. arXiv. 2021. https://doi.org/10.48550/arXiv.2106.08254</cite>
</li>
<li id="B45">
<span class="label">45.</span><cite>Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In: <em>International Conference on Learning Representations</em>. 2021. <a href="https://openreview.net/forum?id=YicbFdNTTy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=YicbFdNTTy</a></cite>
</li>
<li id="B46">
<span class="label">46.</span><cite>Jocher G, Qiu J, Chaurasia A. Ultralytics YOLO. Version 8.0.0. 2023. <a href="https://github.com/ultralytics/ultralytics" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/ultralytics/ultralytics</a></cite>
</li>
<li id="B47">
<span class="label">47.</span><cite>Chen S, Sun P, Song Y, Luo P. Diffusiondet: Diffusion model for object detection. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. IEEE; 2023. p. 19830–19843.</cite>
</li>
<li id="B48">
<span class="label">48.</span><cite>Li Y, Mao H, Girshick R, He K. Exploring plain vision transformer backbones for object detection. In: European Conference on Computer Vision. Springer; 2022. p. 280–296. </cite> [<a href="https://scholar.google.com/scholar_lookup?title=European%20Conference%20on%20Computer%20Vision&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B49">
<span class="label">49.</span><cite>Tan M, Pang R, Le QV. Efficientdet: Scalable and efficient object detection. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. IEEE; 2020. p. 10781–10790.</cite>
</li>
<li id="B50">
<span class="label">50.</span><cite>Rahman A, Patel S. Annotated potholes image dataset. 2020. <a href="https://www.kaggle.com/dsv/973710" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/dsv/973710</a></cite>
</li>
<li id="B51">
<span class="label">51.</span><cite>Tschandl P, Rosendahl C, Kittler H. 
The HAM10000 dataset, a large collection of multisource dermatoscopic images of common pigmented skin lesions. Sci Data. 2018;5:
Article 180161.
</cite> [<a href="https://doi.org/10.1038/sdata.2018.161" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC6091241/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/30106392/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Sci%20Data&amp;title=The%20HAM10000%20dataset,%20a%20large%20collection%20of%20multisource%20dermatoscopic%20images%20of%20common%20pigmented%20skin%20lesions&amp;volume=5&amp;publication_year=2018&amp;pages=Article%20180161&amp;pmid=30106392&amp;doi=10.1038/sdata.2018.161&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B52">
<span class="label">52.</span><cite>Rokh B, Azarpeyvand A, Khanteymoori A. 
A comprehensive survey on model quantization for deep neural networks in image classification. ACM Trans Intell Syst Technol. 2023;14:
Article 97.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ACM%20Trans%20Intell%20Syst%20Technol&amp;title=A%20comprehensive%20survey%20on%20model%20quantization%20for%20deep%20neural%20networks%20in%20image%20classification&amp;volume=14&amp;publication_year=2023&amp;pages=Article%2097&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B53">
<span class="label">53.</span><cite>Codella N, Rotemberg V, Tschandl P, Celebi ME, Dusza S, Gutman D, Helba B, Kalloo A, Liopyris K, Marchetti M, et al. Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the International Skin Imaging Collaboration (ISIC). arXiv. 2019. 10.48550/arXiv.1902.03368</cite> [<a href="https://doi.org/10.48550/arXiv.1902.03368" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B54">
<span class="label">54.</span><cite>Mathieu M, Couprie C, LeCun Y. Deep multi-scale video prediction beyond mean square error. arXiv. 2015. 10.48550/arXiv.1511.05440</cite> [<a href="https://doi.org/10.48550/arXiv.1511.05440" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>]</li>
<li id="B55">
<span class="label">55.</span><cite>Caelen O. 
A Bayesian interpretation of the confusion matrix. Ann Math Artif Intell. 2017;81(3):429–450.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Ann%20Math%20Artif%20Intell&amp;title=A%20Bayesian%20interpretation%20of%20the%20confusion%20matrix&amp;volume=81&amp;issue=3&amp;publication_year=2017&amp;pages=429-450&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="B56">
<span class="label">56.</span><cite>Yacouby R, Axman D. Probabilistic extension of precision, recall, and F1 score for more thorough evaluation of classification models. In: <em>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</em>. Online: Association for Computational Linguistics; 2020. p. 79–91.</cite>
</li>
<li id="B57">
<span class="label">57.</span><cite>Henderson P, Ferrari V. End-to-end training of object class detectors for mean average precision. In: <em>Computer Vision–ACCV 2016: 13th Asian Conference on Computer Vision</em>. Springer; 2017. p. 198–213.</cite>
</li>
<li id="B58">
<span class="label">58.</span><cite>Zhu XJ. Semi-supervised learning literature survey. 2005.</cite>
</li>
<li id="B59">
<span class="label">59.</span><cite>Yang H, Ma T, Huyan J, Han C, Wang H. 
Aggregation segregation generative adversarial network (AG-GAN) facilitated multi-scale segregation detection in asphalt pavement paving stage. Eng Appl Artif Intell. 2024;129:
Article 107663.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Eng%20Appl%20Artif%20Intell&amp;title=Aggregation%20segregation%20generative%20adversarial%20network%20(AG-GAN)%20facilitated%20multi-scale%20segregation%20detection%20in%20asphalt%20pavement%20paving%20stage&amp;volume=129&amp;publication_year=2024&amp;pages=Article%20107663&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets and code supporting this study are openly available at <a href="http://skingserver.top:44455" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://skingserver.top:44455</a>. Further details and inquiries can be directed to the corresponding author.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Research are provided here courtesy of <strong>American Association for the Advancement of Science (AAAS) and Science and Technology Review Publishing House</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.34133/research.0833"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/research.0833.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (12.2 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12376291/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12376291/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12376291%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12376291/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12376291/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12376291/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12376291/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12376291/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12376291/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="G6mcBJn1sP2FJQFkoLIT5JLvFYQj5TrmDAOyLu6olzoqhCGfb5RM7yPWbnbODlj6">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-574fdcc6.js"></script>
    
    

    </body>
</html>
