
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Dual-stream hybrid architecture with adaptive multi-scale boundary-aware mechanisms for robust urban change detection in smart cities - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="AE5331AC8AF268630531AC000E743621.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371053/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Dual-stream hybrid architecture with adaptive multi-scale boundary-aware mechanisms for robust urban change detection in smart cities">
<meta name="citation_author" content="Israr Ahmad">
<meta name="citation_author_institution" content="Department of Computer Science &amp; Technology, Chongqing University of Posts and Telecommunications, Chongqing, China">
<meta name="citation_author" content="Fengjun Shang">
<meta name="citation_author_institution" content="Department of Computer Science &amp; Technology, Chongqing University of Posts and Telecommunications, Chongqing, China">
<meta name="citation_author" content="Muhammad Salman Pathan">
<meta name="citation_author_institution" content="School of Computing, Dublin City University, Dublin, Ireland">
<meta name="citation_author" content="Ahsan Wajahat">
<meta name="citation_author_institution" content="School of Software, Northwestern Polytechnical University, Xi’an, China">
<meta name="citation_author" content="Yun-Su Kim">
<meta name="citation_author_institution" content="Department of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea">
<meta name="citation_publication_date" content="2025 Aug 21">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30729">
<meta name="citation_doi" content="10.1038/s41598-025-16148-5">
<meta name="citation_pmid" content="40841450">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371053/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371053/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371053/pdf/41598_2025_Article_16148.pdf">
<meta name="description" content="Urban environments undergo continuous changes due to natural processes and human activities, which necessitates robust methods for monitoring changes in land cover and infrastructure for sustainable developments. Change detection in remote sensing ...">
<meta name="og:title" content="Dual-stream hybrid architecture with adaptive multi-scale boundary-aware mechanisms for robust urban change detection in smart cities">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Urban environments undergo continuous changes due to natural processes and human activities, which necessitates robust methods for monitoring changes in land cover and infrastructure for sustainable developments. Change detection in remote sensing ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371053/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12371053">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-16148-5"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_16148.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12371053%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12371053/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12371053/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371053/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 21;15:30729. doi: <a href="https://doi.org/10.1038/s41598-025-16148-5" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-16148-5</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Dual-stream hybrid architecture with adaptive multi-scale boundary-aware mechanisms for robust urban change detection in smart cities</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ahmad%20I%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Israr Ahmad</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Israr Ahmad</span></h3>
<div class="p">
<sup>1</sup>Department of Computer Science &amp; Technology, Chongqing University of Posts and Telecommunications, Chongqing, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ahmad%20I%22%5BAuthor%5D" class="usa-link"><span class="name western">Israr Ahmad</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Shang%20F%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Fengjun Shang</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Fengjun Shang</span></h3>
<div class="p">
<sup>1</sup>Department of Computer Science &amp; Technology, Chongqing University of Posts and Telecommunications, Chongqing, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Shang%20F%22%5BAuthor%5D" class="usa-link"><span class="name western">Fengjun Shang</span></a>
</div>
</div>
<sup>1,</sup><sup>#</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Pathan%20MS%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Muhammad Salman Pathan</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Muhammad Salman Pathan</span></h3>
<div class="p">
<sup>2</sup>School of Computing, Dublin City University, Dublin, Ireland </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Pathan%20MS%22%5BAuthor%5D" class="usa-link"><span class="name western">Muhammad Salman Pathan</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wajahat%20A%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Ahsan Wajahat</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Ahsan Wajahat</span></h3>
<div class="p">
<sup>3</sup>School of Software, Northwestern Polytechnical University, Xi’an, China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wajahat%20A%22%5BAuthor%5D" class="usa-link"><span class="name western">Ahsan Wajahat</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kim%20YS%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Yun-Su Kim</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Yun-Su Kim</span></h3>
<div class="p">
<sup>4</sup>Department of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kim%20YS%22%5BAuthor%5D" class="usa-link"><span class="name western">Yun-Su Kim</span></a>
</div>
</div>
<sup>4,</sup><sup>✉</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Department of Computer Science &amp; Technology, Chongqing University of Posts and Telecommunications, Chongqing, China </div>
<div id="Aff2">
<sup>2</sup>School of Computing, Dublin City University, Dublin, Ireland </div>
<div id="Aff3">
<sup>3</sup>School of Software, Northwestern Polytechnical University, Xi’an, China </div>
<div id="Aff4">
<sup>4</sup>Department of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea </div>
<div class="author-notes p">
<div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div>
<div class="fn" id="_eqcntrb93pmc__">
<sup>#</sup><p class="display-inline">Contributed equally.</p>
</div>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Mar 2; Accepted 2025 Aug 13; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12371053  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40841450/" class="usa-link">40841450</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Urban environments undergo continuous changes due to natural processes and human activities, which necessitates robust methods for monitoring changes in land cover and infrastructure for sustainable developments. Change detection in remote sensing plays a pivotal role in analyzing these temporal variations and supports various applications, including environmental monitoring. Many deep learning-based methods have been widely investigated for change detection in the literature. Most of them are typically regarded as per-pixel labeling and show their dominance, but they still struggle in complex scenarios with multi-scale features, imprecise &amp; blurring boundaries, and domain shifts between temporal shifts. To address these challenges, we propose a novel Dual-Stream Hybrid Architecture (DSHA) that combines the strengths of ResNet34 and Modified Pyramid Vision Transformer (PVT-v2) for robust change detection for smart cities. The decoder integrates a boundary-aware module, along with multiscale attention for accurate object boundary detection. For the experiments, we incorporated the LEVIR-MCI dataset, and the results demonstrate the superior performance of our approach by achieving an mIoU of 92.28% and an F1 score of 92.50%. Ablation studies highlight the contribution of each component by showing significant improvements in the evaluation metrics. In comparison with existing methods, DSHA outperformed the existing state-of-the-art methods on the benchmark dataset. These advancements demonstrate our proposed approach’s potential for accurate and reliable urban change detection, making it highly suitable for smart city monitoring applications focused on sustainable urban development.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Change Detection, Remote Sensing, Dual-Stream Encoder, Smart Cities Monitoring</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Computational science, Computer science, Information technology, Software</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">The dynamic properties of urban environments are continuously influenced by both natural processes and human activities, resulting in the constant transformation of the Earth’s surface. Understanding and monitoring these changes has become increasingly critical for urban planning, environmental management, and sustainable development. Change detection (CD), as a fundamental technology in earth observation, provides an essential tool for analyzing and quantifying these temporal variations in land cover and urban structures. The basic objective of CD is to detect and identify significant changes between bi-temporal remote sensing images of the same geographical region, enabling comprehensive interpretation of surface modifications over time. This capability has proven invaluable across numerous applications, including urban expansion monitoring, natural disaster assessment, land-use change analysis, and environmental protection.</p>
<p id="Par3">In the context of smart cities, CD systems are essential for supporting automated urban planning<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>, real-time infrastructure monitoring<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>, and evidence-based policy decisions<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>. The integration of robust change detection systems into smart city frameworks facilitates continuous assessment of urban development patterns and serves as a critical tool for urban planners and municipal decision-makers. By providing comprehensive monitoring capabilities, CD systems enhance informed governance, infrastructure management, and regulatory compliance. These systems enable data-driven governance through real-time monitoring of urban transformations and facilitate urban planning and resource allocation that directly support the achievement of Sustainable Development Goal 11: making cities and human settlements inclusive, safe, resilient, and sustainable<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a>,<a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup>.</p>
<p id="Par4">Traditional change detection approaches, primarily based on manual feature extraction and pixel-level comparison, have shown significant limitations in handling complex scenarios and large-scale datasets<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>. These methods often struggle with the inherent variability in remote sensing imagery, including illumination changes, seasonal variations, and atmospheric effects. Moreover, their reliance on hand-crafted features limits their ability to capture subtle changes and adapt to diverse urban landscapes<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>. Over the past decade, deep learning technology has revolutionized the field of change detection through Convolutional Neural Networks (CNNs), which demonstrate remarkable success due to their superior learning ability and automatic feature extraction capabilities<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a>,<a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. Early CNN-based approaches focused on adapting semantic segmentation architectures and treating the change detection as a binary classification problem at the pixel level<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>. These methods typically process bi-temporal images either through a single-stream architecture with early fusion<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup> or siamese networks with separate feature extraction paths<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>.</p>
<p id="Par5">The advancement of these deep learning methods employs model fusion that combine different satellite imagery, like multispectral<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>, multi-layer attention mechanisms for precise feature extraction<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>, generative models like GANs for improved change mapping in noisy environments<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a>,<a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>, 3D-CNNs for multi-temporal analysis<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>, and temporal feature refinement for continuous urban monitoring<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. Furthermore, the introduction of Vision Transformers (ViTs), marks a significant milestone in deep learning advancement for object detection. A Vision Transformer in principle divides an image into patches, processes them through layers to capture features, and outputs bounding box coordinates and class predictions for detected objects<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a>,<a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>. Several studies have explored ViTs in various configurations, and have showed significant improvements in detecting changes in urban features such as buildings and roads<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>.</p>
<p id="Par6">While these deep learning-based approaches have shown promising results, but using them as a single solution often faces problems. Specially, the CNN-based models often struggle to effectively capture the complex relationships between temporal features<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup> and maintain spatial consistency in the change detection results. Similarly, the transformer-based models alone face challenges in effectively integrating local and global features while maintaining temporal coherence throughout the network, particularly when dealing with small objects and complex urban structures<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>. The primary challenge lies in the network’s inability to simultaneously preserve fine-grained spatial details and capture broader contextual information<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>. This limitation often leads to either over-segmentation or missed changes, particularly in complex urban environments where transformations occur at various spatial scales<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>. The situation becomes more complex due to suboptimal handling of multi-scale temporal relationships and domain shifts between different temporal states, which impacts the system’s overall robustness and generalization capability<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. When we look at the mathematical formulation of these challenges, it can be expressed as minimizing the empirical loss <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/0bb5e6eca9dc/d33e328.gif" loading="lazy" id="d33e328" alt="Inline graphic"></span>, where X1 and X2 represent bi-temporal images and Y shows the ground truth change map. What makes this different from regular semantic segmentation is the need for state-of-the-art mechanisms that can analyze corresponding features at different time points while dealing with real-world complications like climate changes, preprocessing variations, and different imaging conditions<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>. Although the hybrid models can be a solution to these challenges<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>, but simple hybrid approaches still face challenges in achieving robust change detection performance. Issues such as false positives, missed detections, and boundary blurring persist due to factors like lighting changes and scale differences<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a>,<a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>.</p>
<p id="Par7">To address these challenges, we propose a sophisticated hybrid architecture, a novel dual-stream hybrid architecture that combines the strengths of both CNN and transformer architectures. We introduced a modified U-Net with a state-of-the-art dual-stream integration mechanism as the backbone, incorporating a customized PVT-v2 and ResNet34 that work in parallel, where PVT-v2 handles global context through its transformer-based structure while ResNet34 preserves fine-grained spatial details. This dual-stream approach is enhanced by cross-attention fusion mechanisms and adaptive boundary-aware modules that enable effective feature interaction while maintaining the distinctive characteristics of each temporal state, allowing for more precise detection of changes at various scales. The core innovation lies in our hierarchical feature processing pipeline, which integrates our carefully designed cross-stream connections and boundary-aware modules within an improved decoder framework. Our architecture maintains distinct processing streams while enabling controlled interaction through cross-attention fusion mechanisms, allowing for effective modeling of temporal dependencies. It is further strengthened by a multi-scale attention mechanism that dynamically processes features across different spatial scales, enhancing change detection accuracy in urban environments. The main contributions of this work can be summarized as follows:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par8">To the best of our knowledge, this is the first work that synergistically combines a customized Pyramid Vision Transformer (PVT-v2) and ResNet34 in a dual-stream architecture as the backbone for a U-Net framework. The customization includes modified 6-channel inputs for bi-temporal images, enhanced spatial-channel attention mechanisms in PVT-v2 for global context, and attention gates in ResNet34 for adaptive feature fusion. This design enables simultaneous capture of both global context and fine-grained details.</p></li>
<li><p id="Par9">We propose a specialized boundary-aware module (BAM) that integrates Sobel operators with a multi-scale attention mechanism. Unlike existing fixed edge-detection methods, our approach dynamically adjusts to different scales of urban changes, ensuring improved boundary detection. We also incorporated a hierarchical cross-temporal attention (CTA) mechanism that intelligently fuses features across temporal states.</p></li>
<li><p id="Par10">A combined loss function incorporating four losses addresses class imbalance and helps the proposed model to adjust its weights to improve the segmentation overlap.</p></li>
</ul>
<p>Through experiments, we demonstrate that these innovations work together to achieve superior performance in challenging urban scenarios, particularly in cases where subtle changes and complex structures are involved.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Related works</h2>
<section id="Sec3"><h3 class="pmc_sec_title">CNN-based remote sensing change detection methods</h3>
<p id="Par11">CNN-Based Remote Sensing Change Detection Methods In the recent literature, CNN-based remote sensing change detection methods are widely employed. In these studies, most approaches adopted an encoder and decoder architecture along with the Siamese encoder, which extracts deep representation from the bi-temporal images, followed by a decoder that generates a pixel-wise change map. For example, Zhang et al.<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup> proposed a two-channel CNN with shared weights to generate multi-scale feature difference maps in remote sensing change detection. Similarly, Mou et al.<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup> introduced the CNN combined with a recurrent neural network to learn the temporal dependencies in remote sensing images. On top of this, to capture high contextual information, the researchers focused on incorporating multiscale feature learning strategies such as deep supervision<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup> and dense connection<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>. For the rich contextual information, the method MFINet<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> incorporating cross-scale interactions to improve the change-aware representations, was also used, and other studies<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a>,<a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> also focused on the cross-scale feature interaction and feature fusion methods to obtain the change-aware representations.</p>
<p id="Par12">However, the traditional CNN-based change detection methods often struggled with disintegrated object boundaries and noise due to pixel-wise classification. To address this problem, object-based methods gained importance. For instance, Wang et al.<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup> used the ensembled learning on multiple features to preserve object boundaries in urban environments. Zhang et al.<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup> proposed the squeeze-and-excitation W-Net to fuse the multi-source data to reduce the noise in prediction changes. Recent advances in CNN architectures have introduced sophisticated boundary-aware mechanisms to address these limitations. Yang et al.<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup> proposed an enhanced hybrid CNN and transformer network (EHCTNet) that integrates dual-branch feature extraction with boundary refinement modules for robust change detection. Additionally, Li et al.<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup> developed a change detection network based on transformer and transfer learning, focusing on boundary completeness and internal compactness in change regions.</p>
<p id="Par13">Furthermore, attention mechanisms such as multiscale features were developed to capture the object information at multiple scales from remote sensing images<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a>,<a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>. The evolution of CNN-based approaches has led to increasingly sophisticated architectures that address the challenges of arbitrary-oriented object detection in aerial imagery. Notably, Huang et al.<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup> introduced task-wise sampling convolutions (TS-Conv) for arbitrary-oriented object detection in aerial images, demonstrating adaptive feature sampling from task-specific sensitive regions. This approach addresses the inconsistent features between localization and classification tasks that often constrain detection performance in complex aerial scenes. The existing studies predict the change detection in remote sensing images using the pixel-wise assignment of changed or unchanged. Although this approach tries to capture various changes in the bi-temporal images, it often generates the detected objects in fragmented boundaries and isolated noises due to the convolutional operations.</p></section><section id="Sec4"><h3 class="pmc_sec_title">Transformer-based remote sensing change detection methods</h3>
<p id="Par14">Recent transformers with their self-attention mechanisms have been widely adopted to address the shortcomings of the CNN-based methods in remote sensing change detection. These models can model the long-range dependencies in the remote sensing images. Chen et al.<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup> proposed a transformer-based framework to enhance the performance in capturing global semantic information for remote sensing change detection. Bandra et al.<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup> presented a hierarchical transformer encoder for better feature learning. Similarly, Liu et al.<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup> proposed AMTNet, a multi-scale transformer network, to combine its strength with CNNs. Likewise, Zheng et al.<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup> introduced a hybrid architecture, L-Former, to get good results on the remote sensing benchmark datasets. In another study<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup> transformer was used to tokenize the global contextual features obtained into patch-wise features.</p>
<p id="Par15">The advancement of transformer architectures has led to more sophisticated change detection frameworks that specifically address the challenges boundaries and diverse shapes in change areas. Advanced transformer-based methods have also incorporated domain-specific attention mechanisms to improve change detection accuracy. Yin et al.<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup> proposed CTCANet, a CNN-transformer network combining convolutional block attention module (CBAM) for change detection in high-resolution remote sensing images, demonstrating superior performance in capturing both local details and global context. Although transformers have gained significant success in context modeling in remote sensing change detection, they often struggle to extract the local and fine-grained information of objects in long-range modeling dependencies.</p></section><section id="Sec5"><h3 class="pmc_sec_title">Attention mechanism</h3>
<p id="Par16">The attention mechanism is widely adapted in the many deep learning-based models, including encoder-decoder-based architectures, to focus on the spatial and channel features. The attention mechanism also plays a pivotal role. In remote sensing change detection tasks, as these images contain objects at different scales and color ranges. Peng et al.<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup> proposed attention in image difference to overcome false positives to improve the accuracy of change detection in remote sensing images. Similarly, Eftekhari et al.<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup> presented a parallel spatial attention block for the change detection task to reduce the false alarms caused by occlusions. Feng et al.<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup> proposed ICIF-Net using multiscale attention to capture the local and global contextual information. Furthermore, Jiang et al.<sup><a href="#CR49" class="usa-link" aria-describedby="CR49">49</a></sup> introduced a multi-scale difference and attention network (MDANet) for high-resolution city change detection. Building upon this work, Zhan et al.<sup><a href="#CR50" class="usa-link" aria-describedby="CR50">50</a></sup> proposed AMFNet, an attention-guided multi-scale fusion network for bi-temporal change detection that integrates innovative feature fusion techniques for enhanced performance.</p>
<p id="Par17">Li et al.<sup><a href="#CR51" class="usa-link" aria-describedby="CR51">51</a></sup> introduced a multi-scale fusion Siamese network based on a three-branch attention mechanism for high-resolution remote sensing image change detection, addressing challenges in edge detection and small target detection. Likewise, Farooque et al.<sup><a href="#CR52" class="usa-link" aria-describedby="CR52">52</a></sup> proposed a dual attention-driven multi-scale multi-level feature fusion approach for hyperspectral image classification. Recent studies highlighted the importance of attention mechanisms in improving the detection performance in remote sensing change detection<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>. Furthermore, Guo et al.<sup><a href="#CR54" class="usa-link" aria-describedby="CR54">54</a></sup> developed MSFNet, a multi-scale spatial-frequency feature fusion network that replaces traditional CNN operations with shift window self-attention (SWSA) for direct processing of remote sensing images. However, attention-based models often need careful design to balance between computational efficiency and accuracy because the unbalanced and excessive attention mechanism in any model can lead to reduced computational efficiency.</p></section><section id="Sec6"><h3 class="pmc_sec_title">Feature fusion</h3>
<p id="Par18">In many deep learning tasks, feature fusion is considered an important process, including segmentation<sup><a href="#CR55" class="usa-link" aria-describedby="CR55">55</a></sup>, multimodal tasks<sup><a href="#CR56" class="usa-link" aria-describedby="CR56">56</a></sup>, and classification<sup><a href="#CR57" class="usa-link" aria-describedby="CR57">57</a></sup>. Liu et al.<sup><a href="#CR58" class="usa-link" aria-describedby="CR58">58</a></sup> introduced SoftFormer, a SAR-optical fusion transformer for urban land use and land cover classification, demonstrating the effectiveness of transformer-based fusion strategies. Their approach employs an interior self-attention mechanism that enables shallow transformer layers to extract local features similar to CNNs while maintaining the global modeling capabilities of transformers. The fusion in deep learning models happens at multiple scales, with heterogeneous features and multiple levels. The feature fusion in some cases is just a simple operation based on the problem level; for some cases, simple concatenation can be enough<sup><a href="#CR59" class="usa-link" aria-describedby="CR59">59</a></sup>. The feature fusion can be more reliable and flexible when using multiple attention-based techniques<sup><a href="#CR60" class="usa-link" aria-describedby="CR60">60</a>–<a href="#CR62" class="usa-link" aria-describedby="CR62">62</a></sup>. In remote sensing, feature fusion mechanisms are employed using attention mechanisms, temporal, and spatial-temporal attentions. Furthermore, using the flow field and deformable convolution<sup><a href="#CR63" class="usa-link" aria-describedby="CR63">63</a></sup> to focus on the alignment-based fusion to align features of different levels in the spatial dimensions<sup><a href="#CR64" class="usa-link" aria-describedby="CR64">64</a>,<a href="#CR65" class="usa-link" aria-describedby="CR65">65</a></sup>.</p></section></section><section id="Sec7"><h2 class="pmc_sec_title">Materials and methods</h2>
<p id="Par19">The proposed methodology introduces a novel hybrid semantic segmentation architecture for change detection in remote sensing imagery for smart cities, monitoring, and development. At its core, the system employs a modified U-Net framework in which the traditional single encoder is replaced by an enhanced dual-stream encoder as the backbone, along with multiple specialized modules. The Channel Attention module utilizes average and maximum pooling with shared MLP networks for channel-wise attention, while the Spatial Attention module employs convolution for spatial relationships. A key innovation in the architecture is the Boundary Aware Module using Sobel operators for object boundary detection, the Multi-Scale Attention module processing at different scales for multi-scale context, and the Improved Decoder module combining boundary awareness with multi-scale attention. The architecture is enhanced by the Cross Attention Fusion module for transformer-like feature exchange and dynamic feature combination through channel attention.</p>
<section id="Sec8"><h3 class="pmc_sec_title">Dual stream encoder</h3>
<p id="Par20">The modified UNet encompasses a dual-encoder stream that integrates the strengths of both transformer-based and convolutional neural network-based architectures for advanced semantic segmentation for change detection in smart cities. As the image pairs in CD are captured across time, changes occur at different scales and vary at different scales, like small changes (e.g., new small buildings), medium changes (e.g., road construction in the t2 image), and large-scale changes like urban development. To extract the hierarchical changes, the Pyramid Vision Transformer (PVT-v2) is employed to process the input through multiple stages. This architecture gives a bigger picture, like a bird’s-eye view. In parallel, we incorporated the ResNet34 encoder to capture complementary features, which provides strong local feature extraction capabilities and focuses on fine details, much like having a magnifying glass. To adapt the ResNet34 encoder for 6-channel input (3 channels <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e550.gif" loading="lazy" id="d33e550" alt="Inline graphic"></span> 2 temporal images), we initialize the first convolutional layer by averaging the pretrained weights across the RGB channels and replicating them for the temporal pairs. Let <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/8d0b92ccc4a5/d33e556.gif" loading="lazy" id="d33e556" alt="Inline graphic"></span> represent the pretrained weights from ImageNet. We compute channel-averaged weights <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/182434a21dfa/d33e562.gif" loading="lazy" id="d33e562" alt="Inline graphic"></span> as follows:</p>
<table class="disp-formula p" id="Equ1"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/3afd900bd8d6/d33e568.gif" loading="lazy" id="d33e568" alt="graphic file with name d33e568.gif"></td></tr></table>
<p>These averaged weights are then replicated across the temporal pairs to initialize the 6-channel input:</p>
<table class="disp-formula p" id="Equ2"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/c91b474bd784/d33e574.gif" loading="lazy" id="d33e574" alt="graphic file with name d33e574.gif"></td></tr></table>
<p>This initialization preserves the spectral characteristics of the pretrained model while enabling effective processing of bi-temporal imagery. The dual-stream encoder architecture processes image pairs <em>t</em>1, and <em>t</em>2, each image is represented as a three-dimensional tensor <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq5"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/70574f7758d4/d33e587.gif" loading="lazy" id="d33e587" alt="Inline graphic"></span>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq6"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/30fc2c345088/d33e593.gif" loading="lazy" id="d33e593" alt="Inline graphic"></span>, where 3 represents the RGB color channels. So, each image is represented as a tensor of size 3<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq7"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e599.gif" loading="lazy" id="d33e599" alt="Inline graphic"></span>256<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq8"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e605.gif" loading="lazy" id="d33e605" alt="Inline graphic"></span>256. These two images are concatenated to form a single input . The || symbol represents the concatenation of 3 channels from the first image with the 3 channels from the second image, resulting in a 6-channel input and creating a tensor of size 6<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq9"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e612.gif" loading="lazy" id="d33e612" alt="Inline graphic"></span>256<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq10"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e618.gif" loading="lazy" id="d33e618" alt="Inline graphic"></span>256.</p>
<section id="Sec9"><h4 class="pmc_sec_title">Pyramid vision transformer stream processing</h4>
<p id="Par21">The modified UNet encompasses a dual-encoder stream that integrates the strengths of both transformer-based and convolutional neural network-based architectures for advanced semantic segmentation for change detection in smart cities. As the image pairs in CD are captured across time, changes occur at different scales and vary at different scales, like small changes (e.g., new small buildings), medium changes (e.g., road construction in the t2 image), and large-scale changes like urban development. To extract the hierarchical changes, the Pyramid Vision Transformer (PVT-v2) is employed to process the input through multiple stages. This architecture gives a bigger picture, like a bird’s-eye view. In parallel, we incorporated the ResNet34 encoder to capture complementary features, which provides strong local feature extraction capabilities and focuses on fine details, much like having a magnifying glass. To adapt the ResNet34 encoder for 6-channel input (3 channels <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq11"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e628.gif" loading="lazy" id="d33e628" alt="Inline graphic"></span> 2 temporal images), we initialize the first convolutional layer by averaging the pretrained weights across the RGB channels and replicating them for the temporal pairs. Let <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq12"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/8d0b92ccc4a5/d33e634.gif" loading="lazy" id="d33e634" alt="Inline graphic"></span> represent the pretrained weights from ImageNet. We compute channel-averaged weights <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq13"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/182434a21dfa/d33e640.gif" loading="lazy" id="d33e640" alt="Inline graphic"></span> as follows:</p>
<table class="disp-formula p" id="Equ3"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/3afd900bd8d6/d33e646.gif" loading="lazy" id="d33e646" alt="graphic file with name d33e646.gif"></td></tr></table>
<p>These averaged weights are then replicated across the temporal pairs to initialize the 6-channel input:</p>
<table class="disp-formula p" id="Equ4"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/c91b474bd784/d33e652.gif" loading="lazy" id="d33e652" alt="graphic file with name d33e652.gif"></td></tr></table>
<p>This initialization preserves the spectral characteristics of the pretrained model while enabling effective processing of bi-temporal imagery. The dual-stream encoder architecture processes image pairs <em>t</em>1, and <em>t</em>2, each image is represented as a three-dimensional tensor <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq14"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/70574f7758d4/d33e665.gif" loading="lazy" id="d33e665" alt="Inline graphic"></span>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq15"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/30fc2c345088/d33e671.gif" loading="lazy" id="d33e671" alt="Inline graphic"></span>, where 3 represents the RGB color channels. So, each image is represented as a tensor of size 3<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq16"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e677.gif" loading="lazy" id="d33e677" alt="Inline graphic"></span>256<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq17"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e683.gif" loading="lazy" id="d33e683" alt="Inline graphic"></span>256. These two images are concatenated to form a single input . The || symbol represents the concatenation of 3 channels from the first image with the 3 channels from the second image, resulting in a 6-channel input and creating a tensor of size 6<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq18"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e690.gif" loading="lazy" id="d33e690" alt="Inline graphic"></span>256<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq19"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e696.gif" loading="lazy" id="d33e696" alt="Inline graphic"></span>256.</p></section><section id="Sec10"><h4 class="pmc_sec_title">Pyramid vision transformer stream processing</h4>
<p id="Par22">The PVT-v2 encoder serves as the primary feature extraction stream, with a hierarchical transformer structure to process visual information at multiple scales. The PVT-v2 encoder stream starts with an overlapping patch embedding layer, which projects the input image into a sequence of tokens:</p>
<table class="disp-formula p" id="Equ5"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/2b4c9cd254aa/d33e706.gif" loading="lazy" id="d33e706" alt="graphic file with name d33e706.gif"></td></tr></table>
<p>Where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq20"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/9c0d06def4cd/d33e712.gif" loading="lazy" id="d33e712" alt="Inline graphic"></span> represents the patch embedding output, <em>PE</em> are learnable position embeddings to encode spatial information, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq21"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/445a6e6f5eda/d33e721.gif" loading="lazy" id="d33e721" alt="Inline graphic"></span> is the patch size, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq22"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/b52e0eb46c5a/d33e727.gif" loading="lazy" id="d33e727" alt="Inline graphic"></span> is the stride. This encoder then processes these tokens through four progressive stages, each maintaining different feature dimensions:</p>
<table class="disp-formula p" id="Equ6"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/37be834649fa/d33e734.gif" loading="lazy" id="d33e734" alt="graphic file with name d33e734.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq23"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/d764ffa517eb/d33e740.gif" loading="lazy" id="d33e740" alt="Inline graphic"></span> represent multi-scale feature maps extracted from the 1st, 2nd, 3rd, and 4th encoder blocks of PVT-v2 backbone respectively, with channel dimensions 64, 128, 320, and 512.</p>
<p id="Par23">Each stage captures increasingly complex temporal changes by implementing a modified transformer block that includes multi-head self-attention (MHSA) with spatial reduction. In which, <em>Q</em>, <em>K</em>, <em>V</em> represent query, key, and value, feature maps respectively, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq24"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/cb73fc79fcb5/d33e757.gif" loading="lazy" id="d33e757" alt="Inline graphic"></span> is the output projection matrix, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq25"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/063a57995ce2/d33e763.gif" loading="lazy" id="d33e763" alt="Inline graphic"></span> are per-head projection matrices for <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq26"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/f5f8422b8fac/d33e770.gif" loading="lazy" id="d33e770" alt="Inline graphic"></span> where <em>h</em> is total the number of attention heads.</p>
<table class="disp-formula p" id="Equ7"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/d2799ee7baba/d33e779.gif" loading="lazy" id="d33e779" alt="graphic file with name d33e779.gif"></td></tr></table>
<p>where each head computes:</p>
<table class="disp-formula p" id="Equ8"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/a1fe6b503ee1/d33e785.gif" loading="lazy" id="d33e785" alt="graphic file with name d33e785.gif"></td></tr></table>
<p>And a feed-forward network (FFN) with depth-wise convolution processes through three sequential operations:</p>
<table class="disp-formula p" id="Equ9"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/05055b9594e7/d33e791.gif" loading="lazy" id="d33e791" alt="graphic file with name d33e791.gif"></td></tr></table>
<p>The structural diagram of the modified PVT-V2 is presented in Fig. <a href="#Fig1" class="usa-link">1</a>.</p>
<figure class="fig xbox font-sm" id="Fig1"><h5 class="obj_head">Fig. 1.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371053_41598_2025_16148_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/37f4aacd44e0/41598_2025_16148_Fig1_HTML.jpg" loading="lazy" id="MO1" height="224" width="669" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Structural diagram of the modified Pyramid Vision Transformer (PVT-v2) encoder. The architecture processes the input through overlapping patch embedding, hierarchical transformer blocks, and MHSA mechanisms. The figure highlights the four progressive stages (f p 1 to f p 4) with varying feature dimensions, which enables the model to extract hierarchical representations for precise change detection. The satellite image on the left side is from the Levir-MCI dataset<sup><a href="#CR66" class="usa-link" aria-describedby="CR66">66</a></sup>.</p></figcaption></figure></section><section id="Sec11"><h4 class="pmc_sec_title">Residual network stream processing</h4>
<p id="Par24">The ResNet34 stream processes in parallel, providing complementary analysis through its initial modified convolution operation:</p>
<table class="disp-formula p" id="Equ10"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/dda37a7f9b28/d33e817.gif" loading="lazy" id="d33e817" alt="graphic file with name d33e817.gif"></td></tr></table>
<p>Where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq27"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/66d2b7f2dc80/d33e823.gif" loading="lazy" id="d33e823" alt="Inline graphic"></span> represents the bi-temporal image pair input. The initialization process includes ReLU activation and batch normalization. The ResNet stream processes the input through modified convolution layers:</p>
<table class="disp-formula p" id="Equ11"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/733c9f094a81/d33e829.gif" loading="lazy" id="d33e829" alt="graphic file with name d33e829.gif"></td></tr></table>
<p>Where, weights specifically adapted for 6-channel input through channel-wise averaging:</p>
<table class="disp-formula p" id="Equ12"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/fe6110fca762/d33e835.gif" loading="lazy" id="d33e835" alt="graphic file with name d33e835.gif"></td></tr></table>
<p>This stream maintains its hierarchical feature extraction through residual blocks <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq28"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/bdf4af1da07b/d33e841.gif" loading="lazy" id="d33e841" alt="Inline graphic"></span> producing a feature hierarchy:</p>
<table class="disp-formula p" id="Equ13"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/2a03c11fbc5b/d33e848.gif" loading="lazy" id="d33e848" alt="graphic file with name d33e848.gif"></td></tr></table>
<p>With dimensions <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq29"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/f901000958ab/d33e854.gif" loading="lazy" id="d33e854" alt="Inline graphic"></span>, where each level serves specific purposes: the first level <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq30"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/f9f9f5e2ccfe/d33e860.gif" loading="lazy" id="d33e860" alt="Inline graphic"></span> captures fine-grained temporal differences in textures and edges, the second level <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq31"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/263ea6cdf6de/d33e866.gif" loading="lazy" id="d33e866" alt="Inline graphic"></span> identifies changes in local patterns and shapes, the third level <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq32"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/79666888ee67/d33e872.gif" loading="lazy" id="d33e872" alt="Inline graphic"></span> adapts its features to match the PVT-v2’s dimensional space for better feature fusion, and the fourth level <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq33"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/dfc33397d686/d33e878.gif" loading="lazy" id="d33e878" alt="Inline graphic"></span> comprehends complex local transformation patterns. The structural diagram for the ResNet34 encoder is presented in the Fig. <a href="#Fig2" class="usa-link">2</a>.</p>
<figure class="fig xbox font-sm" id="Fig2"><h5 class="obj_head">Fig. 2.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371053_41598_2025_16148_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/3ad430d99253/41598_2025_16148_Fig2_HTML.jpg" loading="lazy" id="MO2" height="554" width="749" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Structural diagram of the modified ResNet34 encoder. The input is modified to accept 6 channels (3 channels x 2 temporal images), Conv1 weights are initialized by averaging pretrainted RGB weights. The match channel layer converts the 256 to 320 channels to align with PVT-v2, and attention gates fuses features from ResNet34 and PVT-v2 streams.</p></figcaption></figure></section></section><section id="Sec12"><h3 class="pmc_sec_title">Channel and spatial attention mechanisms</h3>
<p id="Par25">The proposed methodology introduces a dual-attention mechanism specifically designed for detecting changes in remote sensing imagery from the Levir-MCI dataset, which contains bi-temporal high-resolution satellite images focusing on building and road changes. The architecture implements two complementary attention modules: Channel Attention (CA) and Spatial Attention (SA), working in concert to enhance feature representation for precise change detection.</p>
<section id="Sec13"><h4 class="pmc_sec_title">Channel attention mechanism</h4>
<p id="Par26">The channel attention (CA) module advances the feature representation by dynamically computing the channel-wise importance weights for change detection. First, the module computes global average and max pooling to capture channel-wise statistics:</p>
<table class="disp-formula p" id="Equ14"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/86435ecb0260/d33e903.gif" loading="lazy" id="d33e903" alt="graphic file with name d33e903.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq34"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/fad902bcd487/d33e909.gif" loading="lazy" id="d33e909" alt="Inline graphic"></span> are average-pooled and max-pooled feature maps, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq35"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/fc7c7cc6b295/d33e915.gif" loading="lazy" id="d33e915" alt="Inline graphic"></span> are the height and width (spatial dimensions) of input feature map, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq36"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/813fc9852b38/d33e921.gif" loading="lazy" id="d33e921" alt="Inline graphic"></span> input feature at spatial position. Through average and max pooling, the mean activation and most prominent activation per channel are captured.</p>
<table class="disp-formula p" id="Equ15"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/82263681d950/d33e927.gif" loading="lazy" id="d33e927" alt="graphic file with name d33e927.gif"></td></tr></table>
<p>In this operation, a shared bottleneck is achieved through dimension reduction. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq37"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/24167fa7fc11/d33e934.gif" loading="lazy" id="d33e934" alt="Inline graphic"></span> represents the reduced feature map, <em>Conv</em>1<em>d</em> represents the convolution operation, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq38"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/61a8d07513d5/d33e946.gif" loading="lazy" id="d33e946" alt="Inline graphic"></span> represents the learnable weights for dimension reduction ratio.</p>
<table class="disp-formula p" id="Equ16"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/326cdf0ab0a6/d33e952.gif" loading="lazy" id="d33e952" alt="graphic file with name d33e952.gif"></td></tr></table>
<p>Through this Equation, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq39"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/a9799e595083/d33e958.gif" loading="lazy" id="d33e958" alt="Inline graphic"></span> adds the non-linearity, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq40"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/02f0c9818556/d33e965.gif" loading="lazy" id="d33e965" alt="Inline graphic"></span> represents to learnable weights for channel expansion. This operation generates channel-wise importance weights.</p>
<table class="disp-formula p" id="Equ17"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/b9bc84b3e86d/d33e971.gif" loading="lazy" id="d33e971" alt="graphic file with name d33e971.gif"></td></tr></table>
<p>Finally, the refined features are derived; <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq41"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/e86af15f9fcb/d33e977.gif" loading="lazy" id="d33e977" alt="Inline graphic"></span> representing the normalization of the attention weights for channel importance and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq42"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/19a719214a21/d33e983.gif" loading="lazy" id="d33e983" alt="Inline graphic"></span> representing the channel-wise multiplication. The final operation emphasizes on critical channels for detecting changes from the remote sensing images. The CA mechanism is like an adaptive lens system that functions like an intelligent filter that provides both a bird’s eye view and a magnifying glass. Just as an aerial photographer uses specialized color filters to highlight specific features in the urban environment.</p></section><section id="Sec14"><h4 class="pmc_sec_title">Spatial attention</h4>
<p id="Par27">The spatial attention (SA) module highlights the spatial locations, which plays a critical role in detecting the localized changes; in our case, these changes could be the development of the road or building or the demolishing of these. SA first calculates the channel-wise average and max feature maps and concatenates both maps and processes through the convolution layer to generate a spatial attention map.</p>
<table class="disp-formula p" id="Equ18"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/8cbc4bd7717b/d33e993.gif" loading="lazy" id="d33e993" alt="graphic file with name d33e993.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq43"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/a19de41351e3/d33e999.gif" loading="lazy" id="d33e999" alt="Inline graphic"></span> represents the learnable weights for spatial attention. The refined feature map <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq44"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/9dd4340d270e/d33e1005.gif" loading="lazy" id="d33e1005" alt="Inline graphic"></span> is obtained by multiplying the channel-wise attention feature map with the spatial attention feature map <em>S</em>. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq45"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/c1279d3a1a8a/d33e1014.gif" loading="lazy" id="d33e1014" alt="Inline graphic"></span> The combination of CA and SA modules ensures the model focuses on spatial regions where changes are most likely to occur, improving detection accuracy for small-scale urban changes. The SA mechanism acts like a dynamic spotlight system that enhances both bird’s eye view and magnifying glass capabilities. The SA module creates intelligent attention maps that guide both global and local processing streams to focus computational resources on spatial regions where changes are most likely to occur.</p></section></section><section id="Sec15"><h3 class="pmc_sec_title">Boundary-aware feature enhancement</h3>
<p id="Par28">The proposed methodology introduces an advanced boundary-aware module (BAM) specifically designed to emphasize on the boundaries of the objects for detecting changes in remote sensing imagery from the Levir-MCI dataset. The BAM incorporates an advanced dual-directional gradient computation approach based on the Sobel operators (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq46"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/08951a9ed47d/d33e1024.gif" loading="lazy" id="d33e1024" alt="Inline graphic"></span> matrices) to extract boundary information from feature maps <em>X</em>, representing the number of channels, height, and width. The horizontal and vertical gradients are computed using two-dimensional convolution operations with Sobel kernels:</p>
<table class="disp-formula p" id="Equ19"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/6cd51900f24e/d33e1033.gif" loading="lazy" id="d33e1033" alt="graphic file with name d33e1033.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq47"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/6a36f133c840/d33e1039.gif" loading="lazy" id="d33e1039" alt="Inline graphic"></span> represents the horizontal Sobel Kernel, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq48"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/4af6ac347955/d33e1045.gif" loading="lazy" id="d33e1045" alt="Inline graphic"></span> represents the vertical Sobel Kernel. The horizontal and vertical gradients (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq49"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/75369a2aa876/d33e1052.gif" loading="lazy" id="d33e1052" alt="Inline graphic"></span>) are computed using the two-dimensional convolution operations with Sobel Kernels:</p>
<table class="disp-formula p" id="Equ20"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/86ad1a72b8aa/d33e1058.gif" loading="lazy" id="d33e1058" alt="graphic file with name d33e1058.gif"></td></tr></table>
<p>After computing the horizontal and vertical gradients, these gradients are combined through the Pythagorean theorem to compute the edge magnitude of each pixel. For each pixel position, it takes the square root of the sum of squared gradients, giving us the total edge strength.</p>
<table class="disp-formula p" id="Equ21"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/ea1391ae7d98/d33e1064.gif" loading="lazy" id="d33e1064" alt="graphic file with name d33e1064.gif"></td></tr></table>
<p>The final-boundary enhanced feature map <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq50"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/62aeeade397c/d33e1070.gif" loading="lazy" id="d33e1070" alt="Inline graphic"></span> is obtained by element-wise multiplication with sigmoid-activated <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq51"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/2aabd379ce4a/d33e1076.gif" loading="lazy" id="d33e1076" alt="Inline graphic"></span> edge map.</p>
<table class="disp-formula p" id="Equ22"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/fb24474d545b/d33e1082.gif" loading="lazy" id="d33e1082" alt="graphic file with name d33e1082.gif"></td></tr></table>
<p>The integration of BAM significantly improves feature representation by emphasizing boundary information, leading to more accurate change detection in urban environments from remote sensing images.</p></section><section id="Sec16"><h3 class="pmc_sec_title">Multi-scale attention mechanism</h3>
<p id="Par29">The multi-scale attention (MSA) mechanism is incorporated for change detection in remote sensing from smart cities. The changes in urban environments (like single buildings, stacks of buildings, and roads)happen at various scales, and these changes look different from different perspectives. The MSA framework decomposes the input into a feature map. We implemented the three scales decomposition strategy using bilinear interpolation, which can be represented as:</p>
<table class="disp-formula p" id="Equ23"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/2bb7d48d26ba/d33e1092.gif" loading="lazy" id="d33e1092" alt="graphic file with name d33e1092.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq52"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/860c3868b2b6/d33e1098.gif" loading="lazy" id="d33e1098" alt="Inline graphic"></span> is the target height and width for the scaled feature map, <em>F</em> which represents the input feature map, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq53"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/d807e01f4c7e/d33e1107.gif" loading="lazy" id="d33e1107" alt="Inline graphic"></span> represents the bilinear interpolation function, which resizes the feature map to desired dimensions. Each scaled feature map <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq54"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/4b93e7769186/d33e1113.gif" loading="lazy" id="d33e1113" alt="Inline graphic"></span> is then processed using scale-specific convolutional layers to capture multi-scale contextual information.</p>
<table class="disp-formula p" id="Equ24"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/08523825c3b2/d33e1120.gif" loading="lazy" id="d33e1120" alt="graphic file with name d33e1120.gif"></td></tr></table>
<p>The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq55"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/a19de41351e3/d33e1126.gif" loading="lazy" id="d33e1126" alt="Inline graphic"></span> in this context represents learnable weights for each scale. The bilinear interpolation is again used to upsample the enhanced feature for all scales to the original resolution.</p>
<table class="disp-formula p" id="Equ25"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/d0ea2eea023d/d33e1132.gif" loading="lazy" id="d33e1132" alt="graphic file with name d33e1132.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq56"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/0a9ffd9fb119/d33e1138.gif" loading="lazy" id="d33e1138" alt="Inline graphic"></span> represents the upsamples feature map at scale <em>s</em>. All three upsampled feature maps are then concatenated to form the final multi-scale attention feature map, the mathematical representation is as follows:</p>
<table class="disp-formula p" id="Equ26"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/ead0dfcdab4c/d33e1147.gif" loading="lazy" id="d33e1147" alt="graphic file with name d33e1147.gif"></td></tr></table>
<p>This concatenated feature map is then processed through a convolutional layer to refine the multi-scale features.</p>
<table class="disp-formula p" id="Equ27"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/ad7779da35f2/d33e1154.gif" loading="lazy" id="d33e1154" alt="graphic file with name d33e1154.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq57"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/f0246485b49c/d33e1160.gif" loading="lazy" id="d33e1160" alt="Inline graphic"></span> represents the learnable weights for the convolutional layer. The refined <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq58"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/21d8b1fe2e7d/d33e1166.gif" loading="lazy" id="d33e1166" alt="Inline graphic"></span> enhances the model’s capability to capture the changes at multiple scales and improves detection accuracy for both small and large-scale buildings and roads from remote sensing images. The MSA mechanism extends the dual perception system by implementing a telescopic zoom array that operates simultaneously at multiple magnification levels. The primary bird’s-eye view provides the standard aerial perspective and the magnifying glass focuses on fine details. The MSA creates additional viewing scales, like deploying multiple surveillance drones at different altitudes (100%, 50%, and 25% zoom levels) over the same urban area.</p></section><section id="Sec17"><h3 class="pmc_sec_title">Cross-temporal attention mechanism</h3>
<p id="Par30">The cross-temporal attention (CTA) module is incorporated to enable the model to correlate the features from bi-temporal remote sensing images. The CTA enhances the ability to detect structural changes (such as buildings and roads) over time in evolving smart city environments. The CTA mechanism first projects feature maps from the two temporal states into query (<em>Q</em>), key (<em>K</em>), and value (<em>V</em>) spaces using the learnable metrics, mathematically represented as follows:</p>
<table class="disp-formula p" id="Equ28"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/178862a3508a/d33e1185.gif" loading="lazy" id="d33e1185" alt="graphic file with name d33e1185.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq59"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/cdfebeff4fd0/d33e1191.gif" loading="lazy" id="d33e1191" alt="Inline graphic"></span> represents feature maps from the first and second temporal states. This formulation transforms the features into spaces like <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq60"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/70390ac3396b/d33e1198.gif" loading="lazy" id="d33e1198" alt="Inline graphic"></span>, and <em>V</em> which are helpful in the attention computation. To compute the attention scores scaled dot-product attention is used.</p>
<table class="disp-formula p" id="Equ29"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/ea1b0d79b35c/d33e1207.gif" loading="lazy" id="d33e1207" alt="graphic file with name d33e1207.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq61"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/6cf492552b3f/d33e1213.gif" loading="lazy" id="d33e1213" alt="Inline graphic"></span> represents the dimension of the key vectors. This equation calculates the similarity between <em>Q</em> and <em>K</em> vectors, and enables the model to identify corresponding regions in both temporal states. From this a context-aware map is generated</p>
<table class="disp-formula p" id="Equ30"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/582c721d866b/d33e1226.gif" loading="lazy" id="d33e1226" alt="graphic file with name d33e1226.gif"></td></tr></table>
<p>This operation captures the relevant information from the second temporal state based on the computed attention scores. Next a residual connection is applied.</p>
<table class="disp-formula p" id="Equ31"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/32891aeada59/d33e1232.gif" loading="lazy" id="d33e1232" alt="graphic file with name d33e1232.gif"></td></tr></table>
<p>Finally, the module combines the original feature map from the first temporal state <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq62"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/be3792b2e763/d33e1238.gif" loading="lazy" id="d33e1238" alt="Inline graphic"></span> with a context-aware map <em>C</em>. The attained fused feature map integrates temporal dependencies and enhances the model’s capability to detect changes from bi-temporal remote sensing images while skipping the irrelevant variations. The CTA operates like a time-lapse analyst who not only captures changes over time but also intelligently correlates specific regions between temporal states. It is a sophisticated photo alignment system that works through both our bird’s-eye and magnifying glass perspectives.</p></section><section id="Sec18"><h3 class="pmc_sec_title">Decoder module</h3>
<p id="Par31">To process the feature maps from the encoder stages, we implemented a three-stream feature processing decoder as depicted in the Fig. 4 (block diagram for the proposed model). It refines BAM features and multi-scale features to generate the final change detection output. The decoder integrates features from the encoder and enhances them through up sampling and feature fusion. For each stage, the module first applies BAM to emphasize the boundaries of objects and fine details.</p>
<table class="disp-formula p" id="Equ32"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/0156e3e1c4d3/d33e1258.gif" loading="lazy" id="d33e1258" alt="graphic file with name d33e1258.gif"></td></tr></table>
<p>Here, the represents to the boundary-aware module, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq63"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/f0853abb5977/d33e1264.gif" loading="lazy" id="d33e1264" alt="Inline graphic"></span> are the input features maps from the encoder stages. The BAM uses the Sobel operators (as explained in section 3.3) and MSA to refine the boundaries. Where the MSA is applied to capture the contextual information at different resolution or scales:</p>
<table class="disp-formula p" id="Equ33"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/e305619d6a89/d33e1270.gif" loading="lazy" id="d33e1270" alt="graphic file with name d33e1270.gif"></td></tr></table>
<p>This operation combines the features from the multiple scales to pay attention to the fine-grained and contextual information. Then these feature maps are up sampled to the next resolution using the bilinear interpolation.</p>
<table class="disp-formula p" id="Equ34"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/d8f0bbf92a98/d33e1276.gif" loading="lazy" id="d33e1276" alt="graphic file with name d33e1276.gif"></td></tr></table>
<p>The scale factor, such as s=2, doubles the resolution for fine-grained analysis. The upsampled features are combined with residual connections from previous decoder steps.</p>
<table class="disp-formula p" id="Equ35"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/57898d2e78af/d33e1282.gif" loading="lazy" id="d33e1282" alt="graphic file with name d33e1282.gif"></td></tr></table>
<p>This final output is passed through as a convolutional layer to generate the predicted change map.</p>
<table class="disp-formula p" id="Equ36"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/cf11025b9819/d33e1289.gif" loading="lazy" id="d33e1289" alt="graphic file with name d33e1289.gif"></td></tr></table>
<p>The final feature is obtained through convolution. This step refines the features and predicts the changes from bi-temporal remote sensing images for smart cities monitoring and other applications. The detailed block diagram of our proposed DSHA architecture is presented in Fig. <a href="#Fig3" class="usa-link">3</a>, which illustrates the integration of the different modules, and the visualisation of the DSHA architecture’s processing pipeline, from input to images to the final change map, is shown in Fig. <a href="#Fig4" class="usa-link">4</a>.</p>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/770bc186ff9a/41598_2025_16148_Fig3_HTML.jpg" loading="lazy" id="MO3" height="1573" width="628" alt="Fig. 3"></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Block diagram of the proposed dual-stream hybrid architecture. The architecture integrates a dual-stream encoder (PVT-v2 on the left and ResNet34 on the right), cross-attention fusion modules, boundary-aware modules, and a multi-scale attention mechanism. The diagram illustrates the flow of features through the network, highlighting the hierarchical processing, feature fusion, and refinement stages to generate the final change map.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371053_41598_2025_16148_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/9790b39aa680/41598_2025_16148_Fig4_HTML.jpg" loading="lazy" id="MO4" height="213" width="708" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Visualization of the proposed DSHA architecture’s processing pipeline. (Left) Input bi-temporal satellite images form LEVIR-MCI<sup><a href="#CR66" class="usa-link" aria-describedby="CR66">66</a></sup>. (Center) Feature extraction via the dual-stream encoder, followed by cross-temporal attention and feature fusion. (Right) Decoder with boundary-aware modules, producing the final change detection output with refined boundaries.</p></figcaption></figure></section></section><section id="Sec19"><h2 class="pmc_sec_title">Experimental results</h2>
<section id="Sec20"><h3 class="pmc_sec_title">Datasets</h3>
<p id="Par32">We conducted experiments using the two change detection datasets.</p>
<p id="Par33"><strong>LEVIR-MCI dataset</strong>: The LEVIR-MCI dataset<sup><a href="#CR66" class="usa-link" aria-describedby="CR66">66</a></sup> contains the 10077 bi-temporal aerial image pairs. The dataset contains a pre-image (A), an image taken before, and a post-image (B), which was taken after a time gap to indicate whether the area is changed or not, along with the grayscale &amp; RGB segmentation masks. We have used the RGB masks in this study. The colormap of masks consists of three labels: (0, 0, 0) for the background pixel represented with 0; (255, 255, 0), which represents the roads with hot encoded with 1; and (255, 0, 0) for buildings, represented with 2. The distribution of the dataset is as follows: 6815 training image pairs, 1333 validation image pairs, and 1929 testing image pairs.</p>
<p id="Par34">Before feeding the bi-temporal images pairs into the dual-stream encoder, our preprocessing pipeline applies the minimal transformations by resizing the images to 256x256 pixels and converting them to tensors with pixel values in the [0,1] range to perserve the spectral integrity of remote sensing imagery. We do not apply the ImageNet normalization paramters (also known as ImageNet stats) because our preliminary experiments revealed severe degradation loss of important spectral information by making images too dark when these stats were applied to LEVIR-MCI images; resultantly, it was compromising the change detection performance.</p>
<p id="Par35"><strong>Change Detection Dataset</strong>: The Change Detection Dataset (CDD) consists of season-varying remote sensing images of the same region, obtained from Google Earth (DigitalGlobe)<sup><a href="#CR67" class="usa-link" aria-describedby="CR67">67</a></sup>. The CDD dataset contains cropped images with a size of 256 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq64"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/db3c1d08ff04/d33e1337.gif" loading="lazy" id="d33e1337" alt="Inline graphic"></span> 256 pixels and includes multiple augmentations<sup><a href="#CR67" class="usa-link" aria-describedby="CR67">67</a></sup>. The spatial resolution of CDD ranges from 3 to 100 cm/px. The dataset comprises 10,000 training image pairs, 3,000 validation image pairs, and 3,000 test image pairs. For preprocessing, we used the same settings as LEVIR-MCI. The images from the LEVIR-MCI dataset and CDD datasets are shown in Fig. <a href="#Fig5" class="usa-link">5</a>.</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371053_41598_2025_16148_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/aac10a4eb00d/41598_2025_16148_Fig5_HTML.jpg" loading="lazy" id="MO5" height="736" width="749" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Sample bi-temporal satellite image pairs from LEVIR-MCI and CDD datasets showing various urban changes including building construction and road development. The upper row shows LEVIR-MCI dataset images<sup><a href="#CR66" class="usa-link" aria-describedby="CR66">66</a></sup> and bottom row shows CDD dataset images<sup><a href="#CR67" class="usa-link" aria-describedby="CR67">67</a></sup>.</p></figcaption></figure></section><section id="Sec21"><h3 class="pmc_sec_title">Loss function and evaluation metrics</h3>
<p id="Par36">To handle the class imbalance and improve the segmentation overlap, we incorporated the combined loss for the deep supervision. The combined loss comprises the four multiple losses: CrossEntropy (CE) loss, Dice loss, Focal loss, and Boundary Aware (BA) loss. The mathematical representation of CE is as follows:</p>
<table class="disp-formula p" id="Equ37"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/ce3e4233b4ed/d33e1370.gif" loading="lazy" id="d33e1370" alt="graphic file with name d33e1370.gif"></td></tr></table>
<p>Here, <em>B</em> is the batch size of the images, <em>C</em> is the number of classes (3 in our case), and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq65"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/424b72abd7b8/d33e1382.gif" loading="lazy" id="d33e1382" alt="Inline graphic"></span> is the ground truth label for class <em>c</em> in the batch sample. <em>b</em>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq66"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/0ba33e11c3c7/d33e1395.gif" loading="lazy" id="d33e1395" alt="Inline graphic"></span> is the predicted probability for class <em>c</em> in output <em>i</em>. It guides the model to adjust weights so that the prediction matches with the ground truth. The Dice loss can be represented as below:</p>
<table class="disp-formula p" id="Equ38"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/9ef9a13d1b1b/d33e1407.gif" loading="lazy" id="d33e1407" alt="graphic file with name d33e1407.gif"></td></tr></table>
<p>In Dice loss, the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq67"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/b2bc129adef3/d33e1413.gif" loading="lazy" id="d33e1413" alt="Inline graphic"></span> is <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq68"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/9b35f04b3a30/d33e1420.gif" loading="lazy" id="d33e1420" alt="Inline graphic"></span> as a soothing factor, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq69"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/9255093cf072/d33e1426.gif" loading="lazy" id="d33e1426" alt="Inline graphic"></span> is true positives, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq70"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/0ab13ea1be9a/d33e1432.gif" loading="lazy" id="d33e1432" alt="Inline graphic"></span> is false positives, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq71"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/413dfa5c3026/d33e1438.gif" loading="lazy" id="d33e1438" alt="Inline graphic"></span> is false negatives. The mathematical formulation of focal loss is as follows:</p>
<table class="disp-formula p" id="Equ39"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/7ccb4c2bcd99/d33e1444.gif" loading="lazy" id="d33e1444" alt="graphic file with name d33e1444.gif"></td></tr></table>
<p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq72"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/020274b3a072/d33e1449.gif" loading="lazy" id="d33e1449" alt="Inline graphic"></span> is <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq73"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/d9eab7bfcc7c/d33e1456.gif" loading="lazy" id="d33e1456" alt="Inline graphic"></span>, which is the focusing parameter, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq74"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/4374c107c3b8/d33e1462.gif" loading="lazy" id="d33e1462" alt="Inline graphic"></span> are class weights (computed for dataset imbalance in our case). The formulation for BA loss is as follows:</p>
<table class="disp-formula p" id="Equ40"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/83ad4c263514/d33e1468.gif" loading="lazy" id="d33e1468" alt="graphic file with name d33e1468.gif"></td></tr></table>
<p>Here <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq75"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/48db0cac0c37/d33e1474.gif" loading="lazy" id="d33e1474" alt="Inline graphic"></span> represents the gradient computation using the Sobel filters. The final output is the weighted sum of all these losses. It means each loss function has a different weight (importance) in the final calculation. The weighting scheme for deep supervision is as follows: for CE, 0.7; for Dice loss, 0.7; for Focal loss, 0.3; and for BA loss, 0.5. The total loss calculation formula is as follows:</p>
<table class="disp-formula p" id="Equ41"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/770fa93ec691/d33e1480.gif" loading="lazy" id="d33e1480" alt="graphic file with name d33e1480.gif"></td></tr></table>
<p>In the combined loss function, we set the CE weight to 0.3, which provided fundamental classification guidance for changed and unchanged pixels. Since our study focused on the CD task where spatial coherence matters more than individual pixel accuracy, we set the Dice loss weight to 0.7 to detect coherent changed regions (buildings and roads) rather than scattered pixels. Further we included the Focal loss was to address the imbalance between changed and unchanged pixels and to focus on learning hard-to-detect changes, like smaller buildings and narrow roads. To prevent over-emphasis on hard examples that could lead to noise detection, we set the Focal loss weight to a moderate value of 0.3. Finally, we incorporated BA loss to address blurry boundary issues, with a balance weight of 0.5 to precisely balance boundary detection and overall performance. This combination of the loss functions facilitates our model in dealing with class imbalance, smooth boundary prediction, and improving segmentation overlap, resulting in better performance.</p>
<p id="Par37">For the evaluation, five commonly used metrics are employed: Accuracy (A), F1-Score (F1), precision (P), recall (R), and mean Intersection over Union (mIoU). Each evaluation metric serves an important purpose. The overaccuracy characterises the percentage of correctly detected pixels among all the samples. The F1 represents the harmonic mean of the precision and recall. The P represents the ratio of correctly detected changed pixels to all the pixels that are identified as changed in the map. The recall represents the percentage of correctly detected changed pixels to the number of all pixels that should be detected as changed pixels. While the mIoU, an important evaluation metric in the change detection task, represents the overlap between prediction and ground truth, it reveals the precise coverage of changed and unchanged pixels in the detected change map compared with ground truth. The mathematical representations of these metrics are as follows:</p>
<table class="disp-formula p" id="Equ42"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/b591d1d9136d/d33e1488.gif" loading="lazy" id="d33e1488" alt="graphic file with name d33e1488.gif"></td></tr></table>
<p>Here, <em>TP</em> is the correctly detected changed pixels, <em>TN</em> is the number of accurately detected unchanged pixels, <em>FP</em> is the number of incorrectly detected pixels, and <em>FN</em> is the number of missed detected changed pixels.</p>
<table class="disp-formula p" id="Equ43"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/209e568ab824/d33e1507.gif" loading="lazy" id="d33e1507" alt="graphic file with name d33e1507.gif"></td></tr></table>
<table class="disp-formula p" id="Equ44"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/8df3ae1e0774/d33e1512.gif" loading="lazy" id="d33e1512" alt="graphic file with name d33e1512.gif"></td></tr></table>
<table class="disp-formula p" id="Equ45"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/30d1df6d67db/d33e1517.gif" loading="lazy" id="d33e1517" alt="graphic file with name d33e1517.gif"></td></tr></table>
<table class="disp-formula p" id="Equ46"><tr><td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/8f1b9f0afc01/d33e1522.gif" loading="lazy" id="d33e1522" alt="graphic file with name d33e1522.gif"></td></tr></table>
<p>Here, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq76"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/2f7322ef01b0/d33e1528.gif" loading="lazy" id="d33e1528" alt="Inline graphic"></span> is the number of ground truth pixels in class <em>c</em>.</p></section><section id="Sec22"><h3 class="pmc_sec_title">Implementation details</h3>
<p id="Par38">The proposed DSHA model is implemented based on the open-source deep learning framework PyTorch and initialized with two pretrained models, PVTv2 and ResNet34, for the backbone of the U-Net architecture. For model training, the AdamW optimizer was adopted with an initial learning rate of 5e-4, a weight decay of 0.0005, and parameters <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq77"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/69a183f49fa5/d33e1541.gif" loading="lazy" id="d33e1541" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq78"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/c56ad0088acc/d33e1547.gif" loading="lazy" id="d33e1547" alt="Inline graphic"></span> as 0.9 and 0.999, respectively. The batch size was set to 8 for each GPU. In addition, we employ a OneCycleLR scheduler with a maximum learning rate of 1e-3, a cosine annealing strategy, and gradual reduction to ensure better model convergence. For the loss function, a combined loss is used, which incorporates CE loss, Dice loss, Focal loss, and BA loss with weights [0.3, 0.7, 0.3, 0.5], along with class weighting and deep supervision weights [1.0, 0.8, 0.6, 0.4]. All the experiments were conducted on the Nvidia RTX 4060 Ti with 16 GB RAM with the Windows 11 Pro operating system.</p></section><section id="Sec23"><h3 class="pmc_sec_title">Model complexity</h3>
<p id="Par39">The DSHA architecture comprises 25.69M parameters, which are strategically distributed across components. The base architecture contains 25.63M parameters, and our final dual-stream architecture with all other configurations introduces a modest 60K additional parameters. This represents a parameter efficiency of 0.23% overhead for the achieved performance improvements.</p></section><section id="Sec24"><h3 class="pmc_sec_title">Ablation studies</h3>
<p id="Par40">To validate the effectiveness of our proposed method, we conducted a series of ablation experiments. We removed the following components from the U-Net architecture: the attention mechanism, multiscale processing, the boundary-aware module, the encoder &amp; dual encoder, and integrated them one by one. Their results are presented in numeric form in Table <a href="#Tab1" class="usa-link">1</a>.</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>Comprehensive Analysis of Training and Validation Metrics Across Different Model Configurations in the Ablation Study. The metrics include Loss, A (%), P (%), R (%), F1 (%), and mIoU (%) for both training and validation phases, demonstrating progressive improvement. Abbreviations: AT - Attention Mechanism; MP - Multiscale Processing; PvT-V2 Enc - Pyramid Vision Transformer V2 Encoder; DSEnc - Dual Stream Encoder; BAM - Boundary Aware Module</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="2" rowspan="1">Metrics</th>
<th align="left" colspan="1" rowspan="1">U-Net+AM</th>
<th align="left" colspan="1" rowspan="1">U-Net+ AM+ MP</th>
<th align="left" colspan="1" rowspan="1">U-Net+ AM+ MP+PE</th>
<th align="left" colspan="1" rowspan="1">U-Net+ AM+ MP+DSEnc</th>
<th align="left" colspan="1" rowspan="1">U-Net+AM+ MP+DS</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="2" colspan="1">A (%)</td>
<td align="left" colspan="1" rowspan="1">Train</td>
<td align="left" colspan="1" rowspan="1">65.09</td>
<td align="left" colspan="1" rowspan="1">73.13</td>
<td align="left" colspan="1" rowspan="1">82.17</td>
<td align="left" colspan="1" rowspan="1">92.33</td>
<td align="left" colspan="1" rowspan="1">97.19</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Val</td>
<td align="left" colspan="1" rowspan="1">63.80</td>
<td align="left" colspan="1" rowspan="1">71.69</td>
<td align="left" colspan="1" rowspan="1">80.55</td>
<td align="left" colspan="1" rowspan="1">90.51</td>
<td align="left" colspan="1" rowspan="1">95.27</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">Loss</td>
<td align="left" colspan="1" rowspan="1">Train</td>
<td align="left" colspan="1" rowspan="1">0.3709</td>
<td align="left" colspan="1" rowspan="1">0.3341</td>
<td align="left" colspan="1" rowspan="1">0.3010</td>
<td align="left" colspan="1" rowspan="1">0.2712</td>
<td align="left" colspan="1" rowspan="1">0.2583</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Val</td>
<td align="left" colspan="1" rowspan="1">0.4303</td>
<td align="left" colspan="1" rowspan="1">0.3886</td>
<td align="left" colspan="1" rowspan="1">0.3493</td>
<td align="left" colspan="1" rowspan="1">0.3147</td>
<td align="left" colspan="1" rowspan="1">0.2997</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">P (%)</td>
<td align="left" colspan="1" rowspan="1">Train</td>
<td align="left" colspan="1" rowspan="1">63.80</td>
<td align="left" colspan="1" rowspan="1">72.88</td>
<td align="left" colspan="1" rowspan="1">81.21</td>
<td align="left" colspan="1" rowspan="1">92.38</td>
<td align="left" colspan="1" rowspan="1">96.54</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Val</td>
<td align="left" colspan="1" rowspan="1">62.34</td>
<td align="left" colspan="1" rowspan="1">69.91</td>
<td align="left" colspan="1" rowspan="1">77.20</td>
<td align="left" colspan="1" rowspan="1">88.97</td>
<td align="left" colspan="1" rowspan="1">93.86</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">R (%)</td>
<td align="left" colspan="1" rowspan="1">Train</td>
<td align="left" colspan="1" rowspan="1">61.40</td>
<td align="left" colspan="1" rowspan="1">69.14</td>
<td align="left" colspan="1" rowspan="1">83.39</td>
<td align="left" colspan="1" rowspan="1">87.99</td>
<td align="left" colspan="1" rowspan="1">92.37</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Val</td>
<td align="left" colspan="1" rowspan="1">59.83</td>
<td align="left" colspan="1" rowspan="1">67.92</td>
<td align="left" colspan="1" rowspan="1">81.85</td>
<td align="left" colspan="1" rowspan="1">86.26</td>
<td align="left" colspan="1" rowspan="1">91.28</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">F1 (%)</td>
<td align="left" colspan="1" rowspan="1">Train</td>
<td align="left" colspan="1" rowspan="1">62.24</td>
<td align="left" colspan="1" rowspan="1">70.12</td>
<td align="left" colspan="1" rowspan="1">78.97</td>
<td align="left" colspan="1" rowspan="1">89.24</td>
<td align="left" colspan="1" rowspan="1">93.05</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Val</td>
<td align="left" colspan="1" rowspan="1">61.01</td>
<td align="left" colspan="1" rowspan="1">68.89</td>
<td align="left" colspan="1" rowspan="1">78.15</td>
<td align="left" colspan="1" rowspan="1">87.50</td>
<td align="left" colspan="1" rowspan="1">92.51</td>
</tr>
<tr>
<td align="left" rowspan="2" colspan="1">mIoU (%)</td>
<td align="left" colspan="1" rowspan="1">Train</td>
<td align="left" colspan="1" rowspan="1">63.10</td>
<td align="left" colspan="1" rowspan="1">70.90</td>
<td align="left" colspan="1" rowspan="1">79.66</td>
<td align="left" colspan="1" rowspan="1">89.50</td>
<td align="left" colspan="1" rowspan="1">94.21</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Val</td>
<td align="left" colspan="1" rowspan="1">61.81</td>
<td align="left" colspan="1" rowspan="1">69.45</td>
<td align="left" colspan="1" rowspan="1">78.03</td>
<td align="left" colspan="1" rowspan="1">87.67</td>
<td align="left" colspan="1" rowspan="1">92.28</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par41">As can be observed from the results, the baseline UNet + Attention Mechanism, although achieving a modest performance, struggled with high validation loss (0.4303), low validation accuracy (63.80%), lowest P (62.34%), lowest R (61.40%), lowest F1 or F1-Score (61.01%), and low validation mIoU (61.81%), which limits its ability to process all features within context. The addition of multiscale processing in the U-Net + attention mechanism significantly improved the performance. Specifically, the validation loss was reduced to 0.3886, representing a 9.69% decrease compared to the baseline configuration. Besides the decrease in validation loss, the validation accuracy increased to 71.69%, precision improved to 69.91%, recall increased to 67.92%, the F1-score reached 68.89%, and the mIoU was elevated to 69.45%. The achieved scores of these metrics collectively indicate that the integration of multiscale processing not only reduced prediction errors but also enhanced the model’s overall robustness capabilities by allowing the model to look at the feature at different scales. The integration of the PvT-V2 encoder in the U-Net + attention mechanism + multiprocessing further strengthened the model’s performance.</p>
<p id="Par42">The validation loss decreased by an additional 10.11%, reaching 0.3493, while the validation accuracy increased to 80.55%, precision improved to 77.20%, recall amplified to 81.85%, the F1 score improved to 78.15%, and the mIoU reached 78.03%. This enhancement underscores the importance of leveraging advanced transformer-based architectures in our change detection task, which made it easier for the model to strongly capture the global context and long-range dependencies. Introducing the dual-stream encoder in place of the single-stream PvT-V2 encoder in the U-Net + attention mechanism + multiprocessing marked another significant milestone in the ablation study. The dual-stream architecture allowed the model to process bitemporal images more effectively by separately extracting local and global features from each temporal input before fusing them. As a result, the validation loss was further reduced by 9.91%, dropping to 0.3147, while the validation accuracy rose to 90.51%, precision improved to 88.97%, recall increased to 86.26%, the F1 score reached 87.50%, and the mIoU climbed to 87.67%.</p>
<p id="Par43">The final configuration, which incorporated the boundary-aware module alongside all previously mentioned components, achieved the best overall performance. The boundary-aware module played a crucial role in refining edges and fine details, which are critical for accurately detecting changes in remote sensing images. With this addition, the validation loss was decreased to its lowest value of 0.2997, while the validation accuracy peaked at 95.27%, precision reached 93.86%, recall improved to 91.28%, the F1 score increased to 92.51%, and the mIoU achieved an impressive 92.28%. These results demonstrate the effectiveness of the boundary-aware module in addressing challenges related to edge detection and segmentation accuracy, which are common limitations in baseline models.</p>
<p id="Par44">On comparing the final configuration (our proposed) with the baseline model configuration, it showed remarkable improvement. Our proposed method achieved a 49.29% improvement in mIoU, a 50.41% increase in accuracy, a 51.29% increase in precision, a 52.62% improvement in recall, and a 51.63% improvement in F1-score.</p>
<p id="Par45">These enhancements in the results suggest the effectiveness of our proposed model over the simple baseline model in better handling the global and local context in segmentations. Therefore, our suggested model has significant potential in smart cities for accurate and reliable change detection from bitemporal remote sensing images, which can be extremely helpful for urban planning policymakers to monitor land use changes and track infrastructure development and environmental impacts over time. Figure 7 presents the ablation study results across validation metrics results in graphical form. The graphical representation of the results of our proposed model also shows the significance and effectiveness in the change detection task by achieving the peak mIoU and overall accuracy, along with other metrics, and the loss significantly decreased to its minimalist level. The graphical results representation is divided into two sub-images Fig. <a href="#Fig6" class="usa-link">6</a> and Fig. <a href="#Fig7" class="usa-link">7</a> for the clarity of analysis.</p>
<figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371053_41598_2025_16148_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/3134cf48f680/41598_2025_16148_Fig6_HTML.jpg" loading="lazy" id="MO6" height="384" width="708" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Visual comparisons of the validation performance metrics of different model configurations in the ablation study. The left y-axisrepresents A and mIoU in percentage, while the right y-axis shows the loss values.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371053_41598_2025_16148_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/a7e8ce1cc427/41598_2025_16148_Fig7_HTML.jpg" loading="lazy" id="MO7" height="368" width="661" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Visual comparisons of the validation performance metrics of different model configurations in the ablation study. The validationresults for the three-evaluation metrics (P, R, and F1) are shown.</p></figcaption></figure><p id="Par46">After training and validation of the proposed DSHA model, we exported the model and tested it on the test dataset. We performed the testing with two configurations: one with a single stream PVT-V2 encoder, and the final configuration of U-Net with a dual stream module as a backbone coupled with a boundary aware module. Their qualitative results are displayed in Fig. <a href="#Fig8" class="usa-link">8</a>.</p>
<figure class="fig xbox font-sm" id="Fig8"><h4 class="obj_head">Fig. 8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371053_41598_2025_16148_Fig8_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/746b084096b7/41598_2025_16148_Fig8_HTML.jpg" loading="lazy" id="MO8" height="536" width="669" alt="Fig. 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Visual comparison of change detection results on the LEVIR-MCI<sup><a href="#CR66" class="usa-link" aria-describedby="CR66">66</a></sup>: Left side (column a) Pre-change images, (column b) Post-change images, (column c) Ground truth masks, (column d) Predictions using single-stream modified PVT-V2 encoder backbone, and right side (column e) Predictions using proposed DSHA model with dual-stream encoder. The first and second rows (columns d and e) represent building change predictions in red color masks, while rows 3 and 4 (columns d and e) show road changes in yellow and building change predictions.</p></figcaption></figure><p id="Par47">By looking closely at the results in Fig. <a href="#Fig9" class="usa-link">9</a>, the customized single stream encoder detected the changes from the given bitemporal images but lacks the fine details and failed to identify the exact boundaries of the buildings and roads. Therefore, it produced blurry, smooth-out segmentation masks, and in some areas, it overly predicted. In contrast, our proposed model with the boundary-aware module detected the objects’ edges with fine details, resulting in detecting almost the same boundaries as in the ground truth. Our proposed model performed equally well in detecting small building objects as well as a series of building objects; also, road structures are perfectly detected. These results further validate that our proposed hybrid model is the optimal solution for detecting changes at multiscale, including the small and complex building structures and roads from the remote sensing bi-temporal images for smart cities.</p>
<figure class="fig xbox font-sm" id="Fig9"><h4 class="obj_head">Fig. 9.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12371053_41598_2025_16148_Fig9_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/e6c3/12371053/45e89e459bea/41598_2025_16148_Fig9_HTML.jpg" loading="lazy" id="MO9" height="226" width="708" alt="Fig. 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Closeup results for the comparison are presented. (left side mask) ground truth, (center mask) PvT-v2’s detection, (right side mask) our proposed model’s detection.</p></figcaption></figure></section><section id="Sec25"><h3 class="pmc_sec_title">Comperative experiment</h3>
<p id="Par48">To further validate the robustness of the DSHA, we conducted the comparative experiments on CDD. For the experiments we used the baseline UNet + AM and final configuration of our proposed DSHA for testing on CDD with the same training parameters settings as before. The results of these experiments are presented in Table <a href="#Tab2" class="usa-link">2</a>, which further contributes to the analysis of the performance metrics of the different models.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Performance comparison of baseline UNet+AM and the Proposed DSHA Model on CDD Dataset. The evlaution metrics include A(%), P(%), R(%), F1-Score(%) and mIoU(%).</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Models</th>
<th align="left" colspan="1" rowspan="1">A(%)</th>
<th align="left" colspan="1" rowspan="1">P(%)</th>
<th align="left" colspan="1" rowspan="1">R(%)</th>
<th align="left" colspan="1" rowspan="1">F1-Score(%)</th>
<th align="left" colspan="1" rowspan="1">mIoU(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">UNet+AM</td>
<td align="left" colspan="1" rowspan="1">69.03</td>
<td align="left" colspan="1" rowspan="1">68.72</td>
<td align="left" colspan="1" rowspan="1">62.11</td>
<td align="left" colspan="1" rowspan="1">65.31</td>
<td align="left" colspan="1" rowspan="1">65.24</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DSHA</td>
<td align="left" colspan="1" rowspan="1">92.17</td>
<td align="left" colspan="1" rowspan="1">91.78</td>
<td align="left" colspan="1" rowspan="1">87.83</td>
<td align="left" colspan="1" rowspan="1">89.81</td>
<td align="left" colspan="1" rowspan="1">89.97</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par49">The experimental results demonstrate significant performance improvements across all evaluation metrics when comparing the baseline UNet+AM with our proposed DSHA architecture. Specifically, the DSHA model achieved an accuracy of 92.17%, representing a substantial improvement of 23.14 percentage points over the baseline UNet+AM (69.03%). Similarly, the precision metric also showed remarkable improvement, with DSHA achieving 91.78% compared to 68.72% for the baseline, indicating a 23.06 percentage point improvement. The recall performance displayed the higher substantial improvement, with DSHA achieving 87.83% compared 62.11% for UNet+AM, representing a significant improvement of 25.72 percentage points. Furthermore, the F1-score, which provides a balanced measure of precision and recall, also demonstrated substantial improvement from 65.31% (UNet+AM) to 89.81% (DSHA), representing a performance enhancement of 24.50 percentage points. Likewise, the mIoU, which is considered the primary evaluation metric, demonstrated exceptional improvement from 65.24% to 89.97%, achieving a significant improvement of 24.73 percentage points. These comprehensive comparative results on the CDD dataset further validate the effectiveness and robustness of our proposed DSHA architecture, demonstrating its superior capability in handling diverse urban change detection scenarios across different datasets. The consistent performance improvements across all evaluation metrics highlight the architecture’s ability to generalize effectively to various remote sensing imagery characteristics and change detection challenges, so, confirming its potential for practical deployment in smart city monitoring applications.</p></section><section id="Sec26"><h3 class="pmc_sec_title">Comparison with SOTA</h3>
<p id="Par50">The proposed DSHA is compared with several recent state-of the-art deep learning-based remote sensing change detection methods (2023-2025); a Siamese network integrated MaskChanger<sup><a href="#CR68" class="usa-link" aria-describedby="CR68">68</a></sup>, a cross-level change representation perceiver based MaskCD<sup><a href="#CR69" class="usa-link" aria-describedby="CR69">69</a></sup>, Residual networks based ChangerEx<sup><a href="#CR70" class="usa-link" aria-describedby="CR70">70</a></sup>, a bitemporal image transformer (BIT)<sup><a href="#CR69" class="usa-link" aria-describedby="CR69">69</a></sup>, a patch-mode joint feature fusion model SRC-Net<sup><a href="#CR71" class="usa-link" aria-describedby="CR71">71</a></sup>, a dual branch multi-level inter-temporal network (DMINet)<sup><a href="#CR69" class="usa-link" aria-describedby="CR69">69</a></sup>, a two-transformer-based SiamixFormer<sup><a href="#CR72" class="usa-link" aria-describedby="CR72">72</a></sup>, an integrated Siamese network and nested U-Net (SNUNet)<sup><a href="#CR69" class="usa-link" aria-describedby="CR69">69</a></sup>, an early fusion and deep supervision based LightCDNet<sup><a href="#CR73" class="usa-link" aria-describedby="CR73">73</a></sup>, a global and lightweight decoder based CrossCDNet<sup><a href="#CR74" class="usa-link" aria-describedby="CR74">74</a></sup>, an omnidirectional selective scan based RS-Mamba<sup><a href="#CR75" class="usa-link" aria-describedby="CR75">75</a></sup>, a U-Net based USSFC-Net<sup><a href="#CR76" class="usa-link" aria-describedby="CR76">76</a></sup>, and transformer-based ChangerFormer<sup><a href="#CR77" class="usa-link" aria-describedby="CR77">77</a></sup>.</p>
<p id="Par51">In Table <a href="#Tab3" class="usa-link">3</a>, we compared the performance of our proposed DSHA model with several state-of-the-art methods of deep learning-based remote sensing change detection methods on the LEVIR-MCI dataset. Maskchanger adopted the segmentation-specialized Mask2Former architecture by incorporating Siamese networks to extract features separately from bi-temporal images, while retaining the original mask transformer decode. Maskchanger used IoU evaluation to calculate the degree of overlap between the detected masks and the ground truth, and it achieved an IoU score of 85.12% and an F1 score of 91.96% on the LEVIR-CD dataset. Mask (MaskCD) reformulated change detection as a mask classification problem by using a Cross-Level Change Representation Perceiver (CLCRP) with deformable attention to generate change-aware representations, followed by a Masked Attention-based Detection Transformer (MA-DETR) decoder that predicts object masks and their corresponding change/no-change classifications instead of performing pixel-wise labeling. It achieved an mIoU score of 91.13% and an F1-score of 90.84%, showcasing its effectiveness in handling complex tasks. ChangerEx used parameter-free feature exchange operations (spatial exchange in early stages and channel exchange in later stages) between bi-temporal features during feature extraction, combined with Flow Dual-Alignment Fusion (FDAF) for interactive alignment and fusion to achieve effective change detection. ChangerEx demonstrated the high scores with 91.77% of F1-score, a recall of 90.61%, and precision of 92.97%, however, the IoU or mIoU score was not mentioned in the results. BIT uses a transformer-based approach that incorporates self-attention mechanisms to model long range dependencies with deep features but achieved relatively lower scores, with a mIoU of 73.54% and F1-score of 66.83%.</p>
<section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Comparison of different methods on LEVIR CD Dataset. The table shows IoU(%), mIoU(%), and F1-Score(%) metrics. Best results are highlighted in bold.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Method</th>
<th align="left" colspan="1" rowspan="1">Dataset</th>
<th align="left" colspan="1" rowspan="1">IoU(%)</th>
<th align="left" colspan="1" rowspan="1">mIoU(%)</th>
<th align="left" colspan="1" rowspan="1">F1-Score (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">MaskChanger</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1">85.12</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">91.96</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Mask Classification (MaskCD)</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">91.13</td>
<td align="left" colspan="1" rowspan="1">90.84</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ChangerEx</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1">x</td>
<td align="left" colspan="1" rowspan="1">x</td>
<td align="left" colspan="1" rowspan="1">91.77</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">BIT</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">73.54</td>
<td align="left" colspan="1" rowspan="1">0.6683</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SRC-Net</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1">85.60</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">92.24</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DMINet</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">83.57</td>
<td align="left" colspan="1" rowspan="1">80.57</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SiamixFormer5</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1">85.38</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">91.58</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SNUNet</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">87.07</td>
<td align="left" colspan="1" rowspan="1">86.08</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">LightCDNet</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1">84.21</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">91.43</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CrossCDNet</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1">84.65</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">91.69</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RS-Mamba</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1">83.66</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">91.1</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">USSFC-Net</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1">83.55</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">91.04</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Chage Former</td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1">82.48</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1">90.40</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><strong>Ours</strong></td>
<td align="left" colspan="1" rowspan="1">LEVIR CD</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="left" colspan="1" rowspan="1"><strong>92.28</strong></td>
<td align="left" colspan="1" rowspan="1"><strong>92.50</strong></td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par52">SRC-Net employed a Perception and Interaction Module with cross-branch perception mechanisms and a Patch-Mode joint Feature Fusion Module to leverage bi-temporal spatial relationships between features at the same location at different times for enhanced change detection and used the IoU for the evaluation. It achieved a relatively high IoU score of 85.60 and an F1-score of 92.24% compared to BIT on the LEVIR-CD dataset. A dual-branch multi-level inter-temporal network DMINet achieved an mIoU of 83.57% and F1-score of 80.57% on the LEVIR-CD dataset, which demonstrates a reasonably good performance in change detection. A two SegFormers used two parallel SegFormer encoders to extract hierarchical features from bi-temporal images, applies temporal transformers at each stage for cross-attention fusion (query from T1, key-value from T2), and employs a lightweight MLP decoder to generate building detection or change detection maps, and achieved an IoU score of 85.38 and F1-score of 91.57% on LEVIR-CD dataset. SiamixFormer employed two SegFormers to independently extract features from bi-temporal images and achieved an IoU score of 85.38% on the LEVIR-CD dataset, particularly excelling in detecting building changes. SNUNet, an integrated Siamese network combined with a nested U-Net architecture, achieved an mIoU score of 87.07% and an F1-score of 86.08%. The score of SNUNet shows its ability to capture the features for change detection from bi-temporal remote sensing images. LightCDNet, used an early fusion backbone network with a Deep Supervised Fusion Module (DSFM) to guide the fusion of primary features from bi-temporal images, combined with a pyramid decoder for end-to-end lightweight change detection while preserving input information; achieved an IoU score of 84.21% and an F1-score of 91.43% on the LEVIR-CD dataset.</p>
<p id="Par53">CrossCDNet employed a Siamese neural network with an Instance Normalization and Batch Normalization Module (IBNM) as the encoder backbone to extract and fuse bi-temporal feature maps, followed by a simple MLP decoder for cross-domain change detection with enhanced generalization capability. It used the IoU for measuring the degree of overlap between detection and ground truth. CrossCDNet achieved an IoU score of 84.65% and an F1-score of 91.96% on the LEVIR-CD dataset. The RS-Mamba incorporated an omnidirectional selective scan module to capture global context in multiple spatial directions with linear complexity, enabling efficient dense prediction on large VHR remote sensing images without the quadratic computational overhead of transformers. RS-Mamba achieved an IoU score of 83.66% and F1-score of 91.1% on the LEVIR-CD dataset. USSFC-Net used multi-scale decoupled convolution (MSDConv) for efficient multi-scale feature extraction and a spatial-spectral feature cooperation strategy (SSFC) that generates 3D attention weights without additional parameters to model spatial-spectral feature interactions for ultra-lightweight change detection, and achieved an IoU score of 83.55% and F1-score of 91.04% for remote sensing change detection task. Finally, ChangeFormer, used a hierarchical transformer encoder in a Siamese network to extract multi-scale features from bi-temporal images, computes feature differences through difference modules at multiple scales, and employs a lightweight MLP decoder to fuse these multi-level feature differences for change detection. It achieved an IoU score of 82.48% and an F1-score of 90.40% on the LEVIR-CD dataset.</p>
<p id="Par54">Our proposed DSHA model incorporating a dual stream encoder with multi-scale attention and boundary aware module achieved a better mIoU score of 92.28% and a better F1-score of 92.50% and outperformed the currently existing state-of-the-art models on the LEVIR-CD dataset for change detection. This comparison shows the superior capability of DSHA in detecting changes at multiple scales, including the small and complex building structures as well as road structures, which makes it an optimal solution for remote sensing bi-temporal images’ image analysis in smart cities. The superior performance demonstrates DSHA’s practical viability for deployment in smart city monitoring networks, where accurate detection of small-scale urban changes is crucial for municipal asset management, building permit compliance verification, and infrastructure development oversight. This enhanced precision in multi-scale change detection fills a critical gap in operational urban monitoring systems, providing city administrators with reliable automated analysis tools necessary for maintaining comprehensive urban development records and supporting regulatory enforcement processes. This evidence-based support for policymakers in sustainable urban planning and controlling urban expansion and infrastructural changes directly contributes to the Sustainable Cities and Communities goal of the Sustainable Development Goals<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>.</p></section></section><section id="Sec27"><h2 class="pmc_sec_title">Conclusion and disscussion</h2>
<p id="Par55">In this article, we have proposed the DSHA with adaptive multi-scale boundary-aware mechanisms for robust urban change detection in smart cities. Our approach synergistically combines the strengths of both CNN and transformer by employing ResNet34 and customized PVT-v2 in a dual-stream encoder as the backbone for a U-Net framework. This integration enables the simultaneous capture of both the global context and fine-grained details. The feature fusion layers fuse the global and fine-grained features. The integration of a boundary-aware module, along with multi-scale attention in the decoder, significantly enhances the model’s ability to detect object boundaries and capture changes at various scales accurately. Furthermore, the combined loss function, which is a combination of four losses, also helps the model to adjust its weights for better detections. The experimental results of our proposed DSHA, ablation studies, and comparison with SOTA demonstrate a substantial improvement over the existing state-of-the-art methods on the change detection benchmark dataset. The proposed DSHA model achieved an mIoU score of 92.28, which is a primary evaluation metric in segmentation tasks that calculates the ratio of overlap between ground truth and detections. Moreover, the DSHA also showed its superior performance in other evaluation metrics, such as achieving the F1-score of 92.50, precision of 93.86, recall of 91.28, and accuracy of 95.27. The qualitative results also show the better detection capability of our proposed DSHA, including the small and complex building and road structures. These advancements in the results clearly demonstrate a significant advancement in urban change detection from bi-temporal remote sensing images for smart cities. Future research could explore extending to land use changes and other remote sensing applications and scenarios to enhance its robustness.</p>
<p id="Par56">Beyond the technical achievements demonstrated, this research contributes significantly to the smart cities paradigm by providing urban planners and policymakers with a robust technological tool for evidence-based decision-making in sustainable urban development. The DSHA model’s superior performance in detecting dense urban changes aligns with Sustainable Development Goal 11’s targets for making cities inclusive, safe, resilient, and sustainable, while also supporting SDG 15 through accurate monitoring of land use changes and environmental impacts. The real-time monitoring capabilities enabled by this approach represent a crucial component of smart city infrastructure, facilitating data-driven governance and supporting the achievement of multiple Sustainable Development Goals through comprehensive urban change analysis. This work demonstrates how advanced remote sensing technologies can bridge the gap between technical innovation and practical urban planning applications, providing the monitoring foundation necessary for sustainable smart city development and contributing to global efforts toward achieving the 2030 Sustainable Development Agenda.</p></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>This work was supported by the Gwangju Institute of Science and Technology (GIST) research fund (Future-leading Specialized Research Project, 2025).</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>I.A. conceived and designed the study, established the experiments of the proposed model, and wrote the initial manuscript draft. F.S. supervised the experiments, formally analyzed the results, and guided the preparation of the figures and tables. A.W. proofread the manuscript and assisted with the conversion to LaTeX format. M.S.P also supervised the manuscript, reviewed it, provided suggestions repeatedly until the final version. Y.S.K (CA) Directed and supervised the additional experiments specifically requested by the reviewers, providing essential resources and technical expertise crucial for addressing the review comments. Secured funding for the research. And Participated equally in the revision of the final manuscript. All authors participated in the revision of the final manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The datasets used in this study are publicly available at: Agent, L. C. LEVIRMCl Dataset. <a href="https://huggingface.codatasetslcybuaalevir-mcitreemain/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://huggingface.codatasetslcybuaalevir-mcitreemain/</a> (2025). Accessed:20250308, and <a href="https://drive.google.comfiled/1GX656JqqOyBi_Ef0w65kDGVtonHrNs9edit" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://drive.google.comfiled/1GX656JqqOyBi_Ef0w65kDGVtonHrNs9edit</a>. Accessed: 20250613.</p></section><section id="notes3"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par60">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm">
<div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div>
<div class="fn p" id="fn2"><p>Fengjun Shang contributed equally to this work.</p></div>
</div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Hadiyana, T. &amp; Ji-hoon, S. Ai-driven urban planning: Enhancing efficiency and sustainability in smart cities. <em>ITEJ (Information Technology Engineering Journals)</em><strong>9</strong>, 23–35 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Hadiyana,%20T.%20&amp;%20Ji-hoon,%20S.%20Ai-driven%20urban%20planning:%20Enhancing%20efficiency%20and%20sustainability%20in%20smart%20cities.%20ITEJ%20(Information%20Technology%20Engineering%20Journals)9,%2023%E2%80%9335%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Adreani, L., Bellini, P., Fanfani, M., Nesi, P. &amp; Pantaleo, G. Smart city digital twin framework for real-time multi-data integration and wide public distribution. <em>IEEE Access</em> (2024).</cite>
</li>
<li id="CR3">
<span class="label">3.</span><cite>Onoja, J. P. &amp; Ajala, O. A. Smart city governance and digital platforms: A framework for inclusive community engagement and real-time decision-making. <em>GSC Adv. Res. Rev.</em> (2023).</cite>
</li>
<li id="CR4">
<span class="label">4.</span><cite>Sharifi, A., Allam, Z., Bibri, S. E. &amp; Khavarian-Garmsir, A. R. Smart cities and sustainable development goals (sdgs): A systematic literature review of co-benefits and trade-offs. <em>Cities</em><strong>146</strong>, 104659 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Sharifi,%20A.,%20Allam,%20Z.,%20Bibri,%20S.%20E.%20&amp;%20Khavarian-Garmsir,%20A.%20R.%20Smart%20cities%20and%20sustainable%20development%20goals%20(sdgs):%20A%20systematic%20literature%20review%20of%20co-benefits%20and%20trade-offs.%20Cities146,%20104659%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Kellison, T. An overview of sustainable development goal 11. <em>The Routledge handbook of sport and sustainable development</em> 261–275 (2022).</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>Huang, Y., Wei, M., Ge, B., Zhang, Y. &amp; Ji, Z. Change detection in dual-temporal remote sensing data based on a lightweight siamese network with effective preprocessing. In <em>IGARSS 2024-2024 IEEE International Geoscience and Remote Sensing Symposium</em>, 8397–8400 (IEEE, 2024).</cite>
</li>
<li id="CR7">
<span class="label">7.</span><cite>Javed, A., Kim, T., Lee, C. &amp; Han, Y. Deep learning framework for semantic change detection in urban green spaces along with overall urban areas. In <em>IGARSS 2024-2024 IEEE International Geoscience and Remote Sensing Symposium</em>, 10039–10043 (IEEE, 2024).</cite>
</li>
<li id="CR8">
<span class="label">8.</span><cite>Adiga, S., Parthasarthy, S., Vivek, R., Sarvade, A. &amp; Natarajan, S. Mlafe-net-multi-layer attention and feature extraction for change detection. In <em>2024 Asia Pacific Conference on Innovation in Technology (APCIT)</em>, 1–6 (IEEE, 2024).</cite>
</li>
<li id="CR9">
<span class="label">9.</span><cite>Yu, W., Zhang, X., Das, S., Zhu, X. X. &amp; Ghamisi, P. Maskcd: A remote sensing change detection network based on mask classification. <em>IEEE Trans. Geosci. Remote Sens.</em> (2024).</cite>
</li>
<li id="CR10">
<span class="label">10.</span><cite>Jayarajan, K., Alzubaidi, L. H., Vasanthakumar, G., Lande, J. &amp; Deiwakumari, K. Domain adaptive based convolutional neural network for change detection using time series satellite imagery. In <em>2024 International Conference on Intelligent Algorithms for Computational Intelligence Systems (IACIS)</em>, 1–4 (IEEE, 2024).</cite>
</li>
<li id="CR11">
<span class="label">11.</span><cite>Gao, Y. et al. Relating cnn-transformer fusion network for remote sensing change detection. In <em>2024 IEEE International Conference on Multimedia and Expo (ICME)</em>, 1–6 (IEEE, 2024).</cite>
</li>
<li id="CR12">
<span class="label">12.</span><cite>Wang, X., Guo, Z. &amp; Feng, R. A cnn-and transformer-based dual-branch network for change detection with cross-layer feature fusion and edge constraints. <em>Remote Sens.</em><strong>16</strong> (2024).</cite>
</li>
<li id="CR13">
<span class="label">13.</span><cite>Saidi, S., Idbraim, S., Karmoude, Y., Masse, A. &amp; Arbelo, M. Deep-learning for change detection using multi-modal fusion of remote sensing images: A review. <em>Remote Sens.</em><strong>16</strong>, 3852 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Saidi,%20S.,%20Idbraim,%20S.,%20Karmoude,%20Y.,%20Masse,%20A.%20&amp;%20Arbelo,%20M.%20Deep-learning%20for%20change%20detection%20using%20multi-modal%20fusion%20of%20remote%20sensing%20images:%20A%20review.%20Remote%20Sens.16,%203852%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Kiruluta, A., Lundy, E. &amp; Lemos, A. Novel change detection framework in remote sensing imagery using diffusion models and structural similarity index (ssim). arXiv preprint <a href="http://arxiv.org/abs/2408.10619" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2408.10619</a> (2024).</cite>
</li>
<li id="CR15">
<span class="label">15.</span><cite>Sghaier, M. O., Hadzagic, M., Yu, J. Y., Shton, S. &amp; Shahbazian, E. Leveraging generative deep learning models for enhanced change detection in heterogeneous remote sensing data. In <em>2024 27th International Conference on Information Fusion (FUSION)</em>, 1–8 (IEEE, 2024).</cite>
</li>
<li id="CR16">
<span class="label">16.</span><cite>Yu, S., Tao, C., Zhang, G., Xuan, Y. &amp; Wang, X. Remote sensing image change detection based on deep learning: Multi-level feature cross-fusion with 3d-convolutional neural networks. <em>Appl. Sci.</em><strong>14</strong>, (2076-3417) (2024).</cite>
</li>
<li id="CR17">
<span class="label">17.</span><cite>Hafner, S., Fang, H., Azizpour, H. &amp; Ban, Y. Continuous urban change detection from satellite image time series with temporal feature refinement and multi-task integration. arXiv preprint <a href="http://arxiv.org/abs/2406.17458" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2406.17458</a> (2024).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>He, Q. et al. Ast: Adaptive self-supervised transformer for optical remote sensing representation. <em>ISPRS J. Photogramm. Remote Sens.</em><strong>200</strong>, 41–54 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?He,%20Q.%20et%20al.%20Ast:%20Adaptive%20self-supervised%20transformer%20for%20optical%20remote%20sensing%20representation.%20ISPRS%20J.%20Photogramm.%20Remote%20Sens.200,%2041%E2%80%9354%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19">
<span class="label">19.</span><cite>Deng, K. et al. Cross-modal change detection using historical land use maps and current remote sensing images. <em>ISPRS J. Photogramm. Remote Sens.</em><strong>218</strong>, 114–132 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Deng,%20K.%20et%20al.%20Cross-modal%20change%20detection%20using%20historical%20land%20use%20maps%20and%20current%20remote%20sensing%20images.%20ISPRS%20J.%20Photogramm.%20Remote%20Sens.218,%20114%E2%80%93132%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Jiao, W., Persello, C. &amp; Vosselman, G. Polyr-cnn: R-cnn for end-to-end polygonal building outline extraction. <em>ISPRS J. Photogramm. Remote Sens.</em><strong>218</strong>, 33–43 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Jiao,%20W.,%20Persello,%20C.%20&amp;%20Vosselman,%20G.%20Polyr-cnn:%20R-cnn%20for%20end-to-end%20polygonal%20building%20outline%20extraction.%20ISPRS%20J.%20Photogramm.%20Remote%20Sens.218,%2033%E2%80%9343%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR21">
<span class="label">21.</span><cite>Wang, Y. et al. Msgfnet: Multi-scale gated fusion network for remote sensing image change detection. <em>Remote Sens.</em><strong>16</strong>, 572 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20Y.%20et%20al.%20Msgfnet:%20Multi-scale%20gated%20fusion%20network%20for%20remote%20sensing%20image%20change%20detection.%20Remote%20Sens.16,%20572%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Yu, B. et al. Multi-scale differential network for landslide extraction from remote sensing images with different scenarios. <em>Int. J. Digit. Earth</em><strong>17</strong>, 2441920 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yu,%20B.%20et%20al.%20Multi-scale%20differential%20network%20for%20landslide%20extraction%20from%20remote%20sensing%20images%20with%20different%20scenarios.%20Int.%20J.%20Digit.%20Earth17,%202441920%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR23">
<span class="label">23.</span><cite>Wang, L., Zhang, M., Gao, X. &amp; Shi, W. Advances and challenges in deep learning-based change detection for remote sensing images: A review through various learning paradigms. <em>Remote Sens.</em><strong>16</strong>, 804 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20L.,%20Zhang,%20M.,%20Gao,%20X.%20&amp;%20Shi,%20W.%20Advances%20and%20challenges%20in%20deep%20learning-based%20change%20detection%20for%20remote%20sensing%20images:%20A%20review%20through%20various%20learning%20paradigms.%20Remote%20Sens.16,%20804%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Vincent, E., Ponce, J. &amp; Aubry, M. Satellite image time series semantic change detection: Novel architecture and analysis of domain shift. arXiv preprint <a href="http://arxiv.org/abs/2407.07616" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2407.07616</a> (2024).</cite>
</li>
<li id="CR25">
<span class="label">25.</span><cite>Zou, C. &amp; Wang, Z. A semi-parallel cnn-transformer fusion network for semantic change detection. <em>Image Vis. Comput.</em><strong>149</strong>, 105157 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zou,%20C.%20&amp;%20Wang,%20Z.%20A%20semi-parallel%20cnn-transformer%20fusion%20network%20for%20semantic%20change%20detection.%20Image%20Vis.%20Comput.149,%20105157%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<span class="label">26.</span><cite>Chen, M., Jiang, W. &amp; Zhou, Y. Dtt-cginet: A dual temporal transformer network with multi-scale contour-guided graph interaction for change detection. <em>Remote Sens.</em><strong>16</strong>, 844 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20M.,%20Jiang,%20W.%20&amp;%20Zhou,%20Y.%20Dtt-cginet:%20A%20dual%20temporal%20transformer%20network%20with%20multi-scale%20contour-guided%20graph%20interaction%20for%20change%20detection.%20Remote%20Sens.16,%20844%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR27">
<span class="label">27.</span><cite>Zhang, M. &amp; Shi, W. A feature difference convolutional neural network-based change detection method. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>58</strong>, 7232–7246 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20M.%20&amp;%20Shi,%20W.%20A%20feature%20difference%20convolutional%20neural%20network-based%20change%20detection%20method.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.58,%207232%E2%80%937246%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Mou, L., Bruzzone, L. &amp; Zhu, X. X. Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>57</strong>, 924–935 (2018).</cite> [<a href="https://scholar.google.com/scholar_lookup?Mou,%20L.,%20Bruzzone,%20L.%20&amp;%20Zhu,%20X.%20X.%20Learning%20spectral-spatial-temporal%20features%20via%20a%20recurrent%20convolutional%20neural%20network%20for%20change%20detection%20in%20multispectral%20imagery.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.57,%20924%E2%80%93935%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29">
<span class="label">29.</span><cite>Fang, S., Li, K., Shao, J. &amp; Li, Z. Snunet-cd: A densely connected siamese network for change detection of vhr images. <em>IEEE Geoscience and Remote Sensing Letters</em><strong>19</strong>, 1–5 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Fang,%20S.,%20Li,%20K.,%20Shao,%20J.%20&amp;%20Li,%20Z.%20Snunet-cd:%20A%20densely%20connected%20siamese%20network%20for%20change%20detection%20of%20vhr%20images.%20IEEE%20Geoscience%20and%20Remote%20Sensing%20Letters19,%201%E2%80%935%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR30">
<span class="label">30.</span><cite>Ren, W., Wang, Z., Xia, M. &amp; Lin, H. Mfinet: Multi-scale feature interaction network for change detection of high-resolution remote sensing images. <em>Remote Sens.</em><strong>16</strong>, 1269 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ren,%20W.,%20Wang,%20Z.,%20Xia,%20M.%20&amp;%20Lin,%20H.%20Mfinet:%20Multi-scale%20feature%20interaction%20network%20for%20change%20detection%20of%20high-resolution%20remote%20sensing%20images.%20Remote%20Sens.16,%201269%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Feng, Y., Jiang, J., Xu, H. &amp; Zheng, J. Change detection on remote sensing images using dual-branch multilevel intertemporal network. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>61</strong>, 1–15 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Feng,%20Y.,%20Jiang,%20J.,%20Xu,%20H.%20&amp;%20Zheng,%20J.%20Change%20detection%20on%20remote%20sensing%20images%20using%20dual-branch%20multilevel%20intertemporal%20network.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.61,%201%E2%80%9315%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Fang, S., Li, K. &amp; Li, Z. Changer: Feature interaction is what you need for change detection. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>61</strong>, 1–11 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Fang,%20S.,%20Li,%20K.%20&amp;%20Li,%20Z.%20Changer:%20Feature%20interaction%20is%20what%20you%20need%20for%20change%20detection.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.61,%201%E2%80%9311%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33">
<span class="label">33.</span><cite>Wang, X. et al. Object-based change detection in urban areas from high spatial resolution images based on multiple features and ensemble learning. <em>Remote Sens.</em><strong>10</strong>, 276 (2018).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20X.%20et%20al.%20Object-based%20change%20detection%20in%20urban%20areas%20from%20high%20spatial%20resolution%20images%20based%20on%20multiple%20features%20and%20ensemble%20learning.%20Remote%20Sens.10,%20276%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR34">
<span class="label">34.</span><cite>Zhang, H. et al. A novel squeeze-and-excitation w-net for 2d and 3d building change detection with multi-source and multi-feature remote sensing data. <em>Remote Sens.</em><strong>13</strong>, 440 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20H.%20et%20al.%20A%20novel%20squeeze-and-excitation%20w-net%20for%202d%20and%203d%20building%20change%20detection%20with%20multi-source%20and%20multi-feature%20remote%20sensing%20data.%20Remote%20Sens.13,%20440%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Yang, J., Wan, H. &amp; Shang, Z. Enhanced hybrid cnn and transformer network for remote sensing image change detection. <em>Sci. Rep.</em><strong>15</strong>, 10161 (2025).
</cite> [<a href="https://doi.org/10.1038/s41598-025-94544-7" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11933460/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/40128281/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Yang,%20J.,%20Wan,%20H.%20&amp;%20Shang,%20Z.%20Enhanced%20hybrid%20cnn%20and%20transformer%20network%20for%20remote%20sensing%20image%20change%20detection.%20Sci.%20Rep.15,%2010161%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<span class="label">36.</span><cite>Li, H. et al. Change detection network based on transformer and transfer learning. <em>IEEE Access</em> (2025).</cite>
</li>
<li id="CR37">
<span class="label">37.</span><cite>Han, C., Wu, C., Guo, H., Hu, M. &amp; Chen, H. Hanet: A hierarchical attention network for change detection with bitemporal very-high-resolution remote sensing images. <em>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</em><strong>16</strong>, 3867–3878 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Han,%20C.,%20Wu,%20C.,%20Guo,%20H.,%20Hu,%20M.%20&amp;%20Chen,%20H.%20Hanet:%20A%20hierarchical%20attention%20network%20for%20change%20detection%20with%20bitemporal%20very-high-resolution%20remote%20sensing%20images.%20IEEE%20J.%20Sel.%20Top.%20Appl.%20Earth%20Obs.%20Remote%20Sens.16,%203867%E2%80%933878%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR38">
<span class="label">38.</span><cite>Li, Z. et al. Lightweight remote sensing change detection with progressive feature aggregation and supervised attention. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>61</strong>, 1–12 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20Z.%20et%20al.%20Lightweight%20remote%20sensing%20change%20detection%20with%20progressive%20feature%20aggregation%20and%20supervised%20attention.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.61,%201%E2%80%9312%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR39">
<span class="label">39.</span><cite>Huang, Z., Li, W., Xia, X.-G., Wang, H. &amp; Tao, R. Task-wise sampling convolutions for arbitrary-oriented object detection in aerial images. <em>IEEE Trans. Neural Netw. Learn. Syst.</em> (2024).</cite> [<a href="https://doi.org/10.1109/TNNLS.2024.3367331" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38412084/" class="usa-link">PubMed</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Chen, H., Qi, Z. &amp; Shi, Z. Remote sensing image change detection with transformers. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>60</strong>, 1–14 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20H.,%20Qi,%20Z.%20&amp;%20Shi,%20Z.%20Remote%20sensing%20image%20change%20detection%20with%20transformers.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.60,%201%E2%80%9314%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR41">
<span class="label">41.</span><cite>Bandara, W. G. C. &amp; Patel, V. M. A transformer-based siamese network for change detection. In <em>IGARSS 2022-2022 IEEE International Geoscience and Remote Sensing Symposium</em>, 207–210 (IEEE, 2022).</cite>
</li>
<li id="CR42">
<span class="label">42.</span><cite>Liu, W., Lin, Y., Liu, W., Yu, Y. &amp; Li, J. An attention-based multiscale transformer network for remote sensing image change detection. <em>ISPRS J. Photogramm. Remote Sens.</em><strong>202</strong>, 599–609 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20W.,%20Lin,%20Y.,%20Liu,%20W.,%20Yu,%20Y.%20&amp;%20Li,%20J.%20An%20attention-based%20multiscale%20transformer%20network%20for%20remote%20sensing%20image%20change%20detection.%20ISPRS%20J.%20Photogramm.%20Remote%20Sens.202,%20599%E2%80%93609%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR43">
<span class="label">43.</span><cite>Ramsey, N. Some dynamics in real quadratic fields with applications to inhomogeneous minima. arXiv preprint <a href="http://arxiv.org/abs/2206.12345" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">arXiv:2206.12345</a> (2022).</cite>
</li>
<li id="CR44">
<span class="label">44.</span><cite>Kirillov, A., Girshick, R., He, K. &amp; Dollár, P. Panoptic feature pyramid networks. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 6399–6408 (2019).</cite>
</li>
<li id="CR45">
<span class="label">45.</span><cite>Yin, M., Chen, Z. &amp; Zhang, C. A cnn-transformer network combining cbam for change detection in high-resolution remote sensing images. <em>Remote Sens.</em><strong>15</strong>, 2406 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yin,%20M.,%20Chen,%20Z.%20&amp;%20Zhang,%20C.%20A%20cnn-transformer%20network%20combining%20cbam%20for%20change%20detection%20in%20high-resolution%20remote%20sensing%20images.%20Remote%20Sens.15,%202406%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR46">
<span class="label">46.</span><cite>Peng, X., Zhong, R., Li, Z. &amp; Li, Q. Optical remote sensing image change detection based on attention mechanism and image difference. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>59</strong>, 7296–7307 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Peng,%20X.,%20Zhong,%20R.,%20Li,%20Z.%20&amp;%20Li,%20Q.%20Optical%20remote%20sensing%20image%20change%20detection%20based%20on%20attention%20mechanism%20and%20image%20difference.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.59,%207296%E2%80%937307%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR47">
<span class="label">47.</span><cite>Eftekhari, A., Samadzadegan, F. &amp; Javan, F. D. Building change detection using the parallel spatial-channel attention block and edge-guided deep network. <em>Int. J. Appl. Earth Obs. Geoinf.</em><strong>117</strong>, 103180 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Eftekhari,%20A.,%20Samadzadegan,%20F.%20&amp;%20Javan,%20F.%20D.%20Building%20change%20detection%20using%20the%20parallel%20spatial-channel%20attention%20block%20and%20edge-guided%20deep%20network.%20Int.%20J.%20Appl.%20Earth%20Obs.%20Geoinf.117,%20103180%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR48">
<span class="label">48.</span><cite>Feng, Y., Xu, H., Jiang, J., Liu, H. &amp; Zheng, J. Icif-net: Intra-scale cross-interaction and inter-scale feature fusion network for bitemporal remote sensing images change detection. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>60</strong>, 1–13 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Feng,%20Y.,%20Xu,%20H.,%20Jiang,%20J.,%20Liu,%20H.%20&amp;%20Zheng,%20J.%20Icif-net:%20Intra-scale%20cross-interaction%20and%20inter-scale%20feature%20fusion%20network%20for%20bitemporal%20remote%20sensing%20images%20change%20detection.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.60,%201%E2%80%9313%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR49">
<span class="label">49.</span><cite>Jiang, S. et al. Mdanet: A high-resolution city change detection network based on difference and attention mechanisms under multi-scale feature fusion. <em>Remote Sens.</em><strong>16</strong>, 1387 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Jiang,%20S.%20et%20al.%20Mdanet:%20A%20high-resolution%20city%20change%20detection%20network%20based%20on%20difference%20and%20attention%20mechanisms%20under%20multi-scale%20feature%20fusion.%20Remote%20Sens.16,%201387%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR50">
<span class="label">50.</span><cite>Zhan, Z. et al. Amfnet: Attention-guided multi-scale fusion network for bi-temporal change detection in remote sensing images. <em>Remote Sens.</em><strong>16</strong>, 1765 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhan,%20Z.%20et%20al.%20Amfnet:%20Attention-guided%20multi-scale%20fusion%20network%20for%20bi-temporal%20change%20detection%20in%20remote%20sensing%20images.%20Remote%20Sens.16,%201765%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR51">
<span class="label">51.</span><cite>Li, Y., Weng, L., Xia, M., Hu, K. &amp; Lin, H. Multi-scale fusion siamese network based on three-branch attention mechanism for high-resolution remote sensing image change detection. <em>Remote Sens.</em><strong>16</strong>, 1665 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20Y.,%20Weng,%20L.,%20Xia,%20M.,%20Hu,%20K.%20&amp;%20Lin,%20H.%20Multi-scale%20fusion%20siamese%20network%20based%20on%20three-branch%20attention%20mechanism%20for%20high-resolution%20remote%20sensing%20image%20change%20detection.%20Remote%20Sens.16,%201665%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR52">
<span class="label">52.</span><cite>Farooque, G., Xiao, L., Sargano, A. B., Abid, F. &amp; Hadi, F. A dual attention driven multiscale-multilevel feature fusion approach for hyperspectral image classification. <em>Int. J. Remote Sens.</em><strong>44</strong>, 1151–1178 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Farooque,%20G.,%20Xiao,%20L.,%20Sargano,%20A.%20B.,%20Abid,%20F.%20&amp;%20Hadi,%20F.%20A%20dual%20attention%20driven%20multiscale-multilevel%20feature%20fusion%20approach%20for%20hyperspectral%20image%20classification.%20Int.%20J.%20Remote%20Sens.44,%201151%E2%80%931178%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR53">
<span class="label">53.</span><cite>Sun, L., Wang, X., Zheng, Y., Wu, Z. &amp; Fu, L. Multiscale 3-d-2-d mixed cnn and lightweight attention-free transformer for hyperspectral and lidar classification. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>62</strong>, 1–16 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Sun,%20L.,%20Wang,%20X.,%20Zheng,%20Y.,%20Wu,%20Z.%20&amp;%20Fu,%20L.%20Multiscale%203-d-2-d%20mixed%20cnn%20and%20lightweight%20attention-free%20transformer%20for%20hyperspectral%20and%20lidar%20classification.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.62,%201%E2%80%9316%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR54">
<span class="label">54.</span><cite>Guo, Z., Chen, H. &amp; He, F. Msfnet: Multi-scale spatial-frequency feature fusion network for remote sensing change detection. <em>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</em> (2024).</cite>
</li>
<li id="CR55">
<span class="label">55.</span><cite>Wang, A., Cai, J., Lu, J. &amp; Cham, T.-J. Modality and component aware feature fusion for rgb-d scene classification. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 5995–6004 (2016).</cite>
</li>
<li id="CR56">
<span class="label">56.</span><cite>Sun, K., Xiao, B., Liu, D. &amp; Wang, J. Deep high-resolution representation learning for human pose estimation. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 5693–5703 (2019).</cite>
</li>
<li id="CR57">
<span class="label">57.</span><cite>Lin, T.-Y., RoyChowdhury, A. &amp; Maji, S. Bilinear cnn models for fine-grained visual recognition. In <em>Proceedings of the IEEE international conference on computer vision</em>, 1449–1457 (2015).</cite>
</li>
<li id="CR58">
<span class="label">58.</span><cite>Liu, R., Ling, J. &amp; Zhang, H. Softformer: Sar-optical fusion transformer for urban land use and land cover classification. <em>ISPRS J. Photogramm. Remote Sens.</em><strong>218</strong>, 277–293 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20R.,%20Ling,%20J.%20&amp;%20Zhang,%20H.%20Softformer:%20Sar-optical%20fusion%20transformer%20for%20urban%20land%20use%20and%20land%20cover%20classification.%20ISPRS%20J.%20Photogramm.%20Remote%20Sens.218,%20277%E2%80%93293%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR59">
<span class="label">59.</span><cite>Bandara, W. G. C. &amp; Patel, V. M. Hypertransformer: A textural and spectral feature fusion transformer for pansharpening. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 1767–1777 (2022).</cite>
</li>
<li id="CR60">
<span class="label">60.</span><cite>Dai, Y., Gieseke, F., Oehmcke, S., Wu, Y. &amp; Barnard, K. Attentional feature fusion. In <em>Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, 3560–3569 (2021).</cite>
</li>
<li id="CR61">
<span class="label">61.</span><cite>Li, X. et al. Gated fully fusion for semantic segmentation. <em>Proc. AAAI Conf. Artif. Intell.</em><strong>34</strong>, 11418–11425 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20X.%20et%20al.%20Gated%20fully%20fusion%20for%20semantic%20segmentation.%20Proc.%20AAAI%20Conf.%20Artif.%20Intell.34,%2011418%E2%80%9311425%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR62">
<span class="label">62.</span><cite>Zhou, H. et al. Canet: Co-attention network for rgb-d semantic segmentation. <em>Pattern Recognit.</em><strong>124</strong>, 108468 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhou,%20H.%20et%20al.%20Canet:%20Co-attention%20network%20for%20rgb-d%20semantic%20segmentation.%20Pattern%20Recognit.124,%20108468%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR63">
<span class="label">63.</span><cite>Dai, J. et al. Deformable convolutional networks. In <em>Proceedings of the IEEE international conference on computer vision</em>, 764–773 (2017).</cite>
</li>
<li id="CR64">
<span class="label">64.</span><cite>Huang, Z. et al. Alignseg: Feature-aligned segmentation networks. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em><strong>44</strong>, 550–557 (2021).
</cite> [<a href="https://doi.org/10.1109/TPAMI.2021.3062772" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33646946/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Huang,%20Z.%20et%20al.%20Alignseg:%20Feature-aligned%20segmentation%20networks.%20IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.44,%20550%E2%80%93557%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR65">
<span class="label">65.</span><cite>Li, X. et al. Semantic flow for fast and accurate scene parsing. In <em>Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16</em>, 775–793 (Springer, 2020).</cite>
</li>
<li id="CR66">
<span class="label">66.</span><cite>Liu, C. et al. Change-agent: Toward interactive comprehensive remote sensing change interpretation and analysis. <em>IEEE Trans. Geosci. Remote Sens.</em> 1–1, 10.1109/TGRS.2024.3425815 (2024).</cite>
</li>
<li id="CR67">
<span class="label">67.</span><cite>Lebedev, M., Vizilter, Y. V., Vygolov, O., Knyaz, V. A. &amp; Rubis, A. Y. Change detection in remote sensing images using conditional adversarial networks. <em>Int. Arch. Photogramm. Remote Sens. Spat. Inf. S</em><strong>42</strong>, 565–571 (2018).</cite> [<a href="https://scholar.google.com/scholar_lookup?Lebedev,%20M.,%20Vizilter,%20Y.%20V.,%20Vygolov,%20O.,%20Knyaz,%20V.%20A.%20&amp;%20Rubis,%20A.%20Y.%20Change%20detection%20in%20remote%20sensing%20images%20using%20conditional%20adversarial%20networks.%20Int.%20Arch.%20Photogramm.%20Remote%20Sens.%20Spat.%20Inf.%20S42,%20565%E2%80%93571%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR68">
<span class="label">68.</span><cite>Ebrahimzadeh, M. &amp; Manzuri, M. T. Maskchanger: A transformer-based model tailoring change detection with mask classification. In <em>2024 13th Iranian/3rd International Machine Vision and Image Processing Conference (MVIP)</em>, 1–6 (IEEE, 2024).</cite>
</li>
<li id="CR69">
<span class="label">69.</span><cite>Yu, W., Zhang, X., Das, S., Zhu, X. X. &amp; Ghamisi, P. Maskcd: A remote sensing change detection network based on mask classification. <em>IEEE Trans. Geosci. Remote Sens.</em> (2024).</cite>
</li>
<li id="CR70">
<span class="label">70.</span><cite>Fang, S., Li, K. &amp; Li, Z. Changer: Feature interaction is what you need for change detection. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>61</strong>, 1–11 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Fang,%20S.,%20Li,%20K.%20&amp;%20Li,%20Z.%20Changer:%20Feature%20interaction%20is%20what%20you%20need%20for%20change%20detection.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.61,%201%E2%80%9311%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR71">
<span class="label">71.</span><cite>Chen, H., Xu, X. &amp; Pu, F. Src-net: Bi-temporal spatial relationship concerned network for change detection. <em>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</em> (2024).</cite>
</li>
<li id="CR72">
<span class="label">72.</span><cite>Mohammadian, A. &amp; Ghaderi, F. Siamixformer: A fully-transformer siamese network with temporal fusion for accurate building detection and change detection in bi-temporal remote sensing images. <em>Int. J. Remote Sens.</em><strong>44</strong>, 3660–3678 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Mohammadian,%20A.%20&amp;%20Ghaderi,%20F.%20Siamixformer:%20A%20fully-transformer%20siamese%20network%20with%20temporal%20fusion%20for%20accurate%20building%20detection%20and%20change%20detection%20in%20bi-temporal%20remote%20sensing%20images.%20Int.%20J.%20Remote%20Sens.44,%203660%E2%80%933678%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR73">
<span class="label">73.</span><cite>Xing, Y. et al. Lightcdnet: Lightweight change detection network based on vhr images. <em>IEEE Geosci. Remote Sens. Lett.</em><strong>20</strong>, 1–5 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Xing,%20Y.%20et%20al.%20Lightcdnet:%20Lightweight%20change%20detection%20network%20based%20on%20vhr%20images.%20IEEE%20Geosci.%20Remote%20Sens.%20Lett.20,%201%E2%80%935%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR74">
<span class="label">74.</span><cite>Song, Y. et al. A cross-domain change detection network based on instance normalization. <em>Remote Sens.</em><strong>15</strong>, 5785 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Song,%20Y.%20et%20al.%20A%20cross-domain%20change%20detection%20network%20based%20on%20instance%20normalization.%20Remote%20Sens.15,%205785%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR75">
<span class="label">75.</span><cite>Zhao, S. et al. Rs-mamba for large remote sensing image dense prediction. <em>IEEE Trans. Geosci. Remote Sens.</em> (2024).</cite>
</li>
<li id="CR76">
<span class="label">76.</span><cite>Lei, T. et al. Ultralightweight spatial-spectral feature cooperation network for change detection in remote sensing images. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>61</strong>, 1–14 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Lei,%20T.%20et%20al.%20Ultralightweight%20spatial-spectral%20feature%20cooperation%20network%20for%20change%20detection%20in%20remote%20sensing%20images.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.61,%201%E2%80%9314%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR77">
<span class="label">77.</span><cite>Bandara, W. G. C. &amp; Patel, V. M. A transformer-based siamese network for change detection. In <em>IGARSS 2022-2022 IEEE International Geoscience and Remote Sensing Symposium</em>, 207–210 (IEEE, 2022).</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets used in this study are publicly available at: Agent, L. C. LEVIRMCl Dataset. <a href="https://huggingface.codatasetslcybuaalevir-mcitreemain/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://huggingface.codatasetslcybuaalevir-mcitreemain/</a> (2025). Accessed:20250308, and <a href="https://drive.google.comfiled/1GX656JqqOyBi_Ef0w65kDGVtonHrNs9edit" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://drive.google.comfiled/1GX656JqqOyBi_Ef0w65kDGVtonHrNs9edit</a>. Accessed: 20250613.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-16148-5"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_16148.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (4.8 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12371053/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12371053/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12371053%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12371053/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12371053/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12371053/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40841450/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12371053/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40841450/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12371053/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12371053/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="BqXw8ZJBL91wCGbwm3A7Sw8tf2VjEu8vPVUHEdl6Yn44bCFXCLacOOIhoUfcyJXh">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
