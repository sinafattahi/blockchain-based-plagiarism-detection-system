
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            M-ReDet: A mamba-based method for remote sensing ship object detection and fine-grained recognition - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4A38C8AF2095305A38C0044DD3960.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="plosone">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370123/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="PLOS One">
<meta name="citation_title" content="M-ReDet: A mamba-based method for remote sensing ship object detection and fine-grained recognition">
<meta name="citation_author" content="Xuhui Liu">
<meta name="citation_author_institution" content="School of Economics, Management and Law, Jilin Normal University, Siping, China">
<meta name="citation_author" content="Chi Feng">
<meta name="citation_author_institution" content="School of Information Technology, Jilin Normal University, Siping, China">
<meta name="citation_author" content="Shuran Zi">
<meta name="citation_author_institution" content="School of Economics, Management and Law, Jilin Normal University, Siping, China">
<meta name="citation_author" content="Zhengkun Qin">
<meta name="citation_author_institution" content="School of Information Technology, Jilin Normal University, Siping, China">
<meta name="citation_author" content="Qinghe Guan">
<meta name="citation_author_institution" content="College of Electronic and Information Engineering, Changchun University of Science and Technology, Changchun, China">
<meta name="citation_publication_date" content="2025 Aug 21">
<meta name="citation_volume" content="20">
<meta name="citation_issue" content="8">
<meta name="citation_firstpage" content="e0330485">
<meta name="citation_doi" content="10.1371/journal.pone.0330485">
<meta name="citation_pmid" content="40839647">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370123/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370123/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370123/pdf/pone.0330485.pdf">
<meta name="description" content="Ship object detection and fine-grained recognition of remote sensing images are hot topics in remote sensing image processing, with applications in fishing vessel operation command, merchant ship navigation route planning, and other fields. In order ...">
<meta name="og:title" content="M-ReDet: A mamba-based method for remote sensing ship object detection and fine-grained recognition">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Ship object detection and fine-grained recognition of remote sensing images are hot topics in remote sensing image processing, with applications in fishing vessel operation command, merchant ship navigation route planning, and other fields. In order ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370123/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12370123">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1371/journal.pone.0330485"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/pone.0330485.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370123%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12370123/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12370123/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370123/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png" alt="PLOS One logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to PLOS One" title="Link to PLOS One" shape="default" href="https://doi.org/10.1371/journal.pone.0330485" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">PLoS One</button></div>. 2025 Aug 21;20(8):e0330485. doi: <a href="https://doi.org/10.1371/journal.pone.0330485" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330485</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22PLoS%20One%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22PLoS%20One%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22PLoS%20One%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>M-ReDet: A mamba-based method for remote sensing ship object detection and fine-grained recognition</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20X%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Xuhui Liu</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Xuhui Liu</span></h3>
<div class="p">
<sup>1</sup>School of Economics, Management and Law, Jilin Normal University, Siping, China</div>
<div>Conceptualization, Data curation, Funding acquisition, Methodology, Software, Writing – original draft</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20X%22%5BAuthor%5D" class="usa-link"><span class="name western">Xuhui Liu</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Feng%20C%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Chi Feng</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Chi Feng</span></h3>
<div class="p">
<sup>2</sup>School of Information Technology, Jilin Normal University, Siping, China</div>
<div>Conceptualization, Methodology, Software, Writing – original draft</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Feng%20C%22%5BAuthor%5D" class="usa-link"><span class="name western">Chi Feng</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zi%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Shuran Zi</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Shuran Zi</span></h3>
<div class="p">
<sup>1</sup>School of Economics, Management and Law, Jilin Normal University, Siping, China</div>
<div>Data curation, Writing – original draft, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zi%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Shuran Zi</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Qin%20Z%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Zhengkun Qin</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Zhengkun Qin</span></h3>
<div class="p">
<sup>2</sup>School of Information Technology, Jilin Normal University, Siping, China</div>
<div>Supervision, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Qin%20Z%22%5BAuthor%5D" class="usa-link"><span class="name western">Zhengkun Qin</span></a>
</div>
</div>
<sup>2,</sup><sup>*</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Guan%20Q%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Qinghe Guan</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Qinghe Guan</span></h3>
<div class="p">
<sup>3</sup>College of Electronic and Information Engineering, Changchun University of Science and Technology, Changchun, China</div>
<div>Software, Writing – original draft, Writing – review &amp; editing</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Guan%20Q%22%5BAuthor%5D" class="usa-link"><span class="name western">Qinghe Guan</span></a>
</div>
</div>
<sup>3</sup>
</div>
<div class="cg p">Editor: <span class="name western">Fatih Uysal</span><sup>4</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="aff001">
<sup>1</sup>School of Economics, Management and Law, Jilin Normal University, Siping, China</div>
<div id="aff002">
<sup>2</sup>School of Information Technology, Jilin Normal University, Siping, China</div>
<div id="aff003">
<sup>3</sup>College of Electronic and Information Engineering, Changchun University of Science and Technology, Changchun, China</div>
<div id="edit1">
<sup>4</sup>Kafkas University: Kafkas Universitesi, TÜRKIYE</div>
<div class="author-notes p">
<div class="fn" id="cor001">
<sup>✉</sup><p class="display-inline">* E-mail: <span>qin_zhengkun@126.com</span></p>
</div>
<div class="fn" id="coi001"><p><strong>Competing Interests: </strong>The authors have declared that no competing interests exist.</p></div>
</div>
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Xuhui Liu</span></strong>: <span class="role">Conceptualization, Data curation, Funding acquisition, Methodology, Software, Writing – original draft</span>
</div>
<div>
<strong class="contrib"><span class="name western">Chi Feng</span></strong>: <span class="role">Conceptualization, Methodology, Software, Writing – original draft</span>
</div>
<div>
<strong class="contrib"><span class="name western">Shuran Zi</span></strong>: <span class="role">Data curation, Writing – original draft, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Zhengkun Qin</span></strong>: <span class="role">Supervision, Writing – review &amp; editing</span>
</div>
<div>
<strong class="contrib"><span class="name western">Qinghe Guan</span></strong>: <span class="role">Software, Writing – original draft, Writing – review &amp; editing</span>
</div>
<div class="p">
<strong class="contrib"><span class="name western">Fatih Uysal</span></strong>: <span class="role">Editor</span>
</div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Apr 17; Accepted 2025 Aug 1; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© 2025 Liu et al</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12370123  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40839647/" class="usa-link">40839647</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="abstract1"><h2>Abstract</h2>
<p>Ship object detection and fine-grained recognition of remote sensing images are hot topics in remote sensing image processing, with applications in fishing vessel operation command, merchant ship navigation route planning, and other fields. In order to improve the detection accuracy for different types of remote sensing ship objects, this paper proposes a ship object perception and feature refinement method based on the improved ReDet, called Mamba-ReDet (M-ReDet). First, this paper designs a ship object fine-grained feature extraction backbone (Mamba-ReResNet, M-ReResNet), which selects and reconstructs the unique features of different types of ship objects through the Mamba’s selective memory to improve the algorithm’s ability to extract fine-grained features. Secondly, the M-ReDet consists of the Ship Object Perception Module (SOPM) and the Ship Feature Refinement Module (SFRM), which can extract the ship’s spatial position information from the feature map, fuse different scales of spatial position information and use this information to refine the fine-grained features to improve the detection accuracy of the algorithm for different categories of ships. Finally, we use the KFIoU and Focal Loss as the regression loss and classification loss of the algorithm to improve the accuracy of the training. The experimental results show that the mAP<sub>0.5</sub> of the M-ReDet algorithm on the FAIR1M(ship) and DOTAv1.0 visible light (RGB) remote sensing image datasets are 43.29% and 82.09%, respectively, which is 2.78% and 3.34% higher than that of the ReDet.</p></section><section id="sec001"><h2 class="pmc_sec_title">Introduction</h2>
<p>Remote sensing ship object detection and fine-grained recognition have important research value in fishery monitoring, navigation planning, and other fields. However, problems when using optical remote sensing images for these tasks include inconsistent ship size and rotation angle, as well as complex backgrounds. Therefore, many problems must be solved when designing a remote sensing object detection algorithm that can extract the fine-grained features of ship objects and enhance spatial positioning and feature refinement capabilities.</p>
<p>The commonly used remote sensing object detection algorithms can be categorized into three classes: the Convolutional Neural Network (CNN), the Transformer [<a href="#pone.0330485.ref001" class="usa-link" aria-describedby="pone.0330485.ref001">1</a>], and the Mamba [<a href="#pone.0330485.ref002" class="usa-link" aria-describedby="pone.0330485.ref002">2</a>] based remote sensing object detection algorithm. Firstly, the first class of algorithms is mainly improved based on classical networks such as YOLO [<a href="#pone.0330485.ref003" class="usa-link" aria-describedby="pone.0330485.ref003">3</a>], SSD [<a href="#pone.0330485.ref004" class="usa-link" aria-describedby="pone.0330485.ref004">4</a>], Faster R-CNN [<a href="#pone.0330485.ref005" class="usa-link" aria-describedby="pone.0330485.ref005">5</a>], and ResNet [<a href="#pone.0330485.ref006" class="usa-link" aria-describedby="pone.0330485.ref006">6</a>]. In 2021, XueYang et al. proposed the GWD algorithm [<a href="#pone.0330485.ref007" class="usa-link" aria-describedby="pone.0330485.ref007">7</a>], which uses the Gaussian Wasserstein Distance to characterize the distances between rotated boxes and solve the problem of a discontinuous range of rotation angles. The same year, XueYang et al. proposed the R3Det algorithm [<a href="#pone.0330485.ref008" class="usa-link" aria-describedby="pone.0330485.ref008">8</a>], which designed a feature refinement module to obtain the object’s position information and realize feature alignment. Jiaming Han et al. proposed the ReDet algorithm [<a href="#pone.0330485.ref009" class="usa-link" aria-describedby="pone.0330485.ref009">9</a>], which encodes rotational equivariant and rotational invariant features to improve the detection accuracy of remote-sensing objects. In 2022, Liping Hou et al. proposed the SASM algorithm [<a href="#pone.0330485.ref010" class="usa-link" aria-describedby="pone.0330485.ref010">10</a>], which uses two strategies, the Shape Adaptive Selection (SAS) and the Shape Adaptive Measurement (SAM), to realize the selection and evaluation of positive samples.</p>
<p>The size differences of remote sensing objects are significant, and the information in the background is rich. It is important to separate the unique information of the object from the background, and the Transformer can effectively solve this problem. In 2022, Li Qingyun et al. proposed the TRD algorithm [<a href="#pone.0330485.ref011" class="usa-link" aria-describedby="pone.0330485.ref011">11</a>], which reconstructed the Transformer and effectively extracted the spatial position information of the object and the correlation information between instances. In 2023, Wei Liu et al. proposed the AMTNet algorithm [<a href="#pone.0330485.ref012" class="usa-link" aria-describedby="pone.0330485.ref012">12</a>], which combines CNN and Transformer to reconstruct the backbone network. Through feature exchange, different scales of feature maps are used to improve the network’s feature extraction performance for changing regions. In 2024, Mingji Yang et al. proposed the Hybrid DETR algorithm [<a href="#pone.0330485.ref013" class="usa-link" aria-describedby="pone.0330485.ref013">13</a>], which can extract alienation features between remote sensing objects and then use the alienation features to distinguish small objects in complex backgrounds through the SODM module, improving the algorithm’s detection ability for small remote sensing objects.</p>
<p>Mamba-based remote sensing object detection algorithm is a new type of algorithm that has become more popular in recent years [<a href="#pone.0330485.ref014" class="usa-link" aria-describedby="pone.0330485.ref014">14</a>], and its selective structure reduces the computational complexity of the Transformer. It can highlight the remote sensing object’s adequate feature information to improve the algorithm’s robustness. In 2024, Yue Zhan et al. proposed the MambaSOD algorithm [<a href="#pone.0330485.ref015" class="usa-link" aria-describedby="pone.0330485.ref015">15</a>], which uses a dual Mamba-driven feature extractor for RGB and depth information to model remote dependencies in multimodal inputs with linear complexity. Moreover, designs a cross-modal fusion Mamba model to capture multimodal features. In the same year, Tushar Verma et al. proposed the SOAR algorithm [<a href="#pone.0330485.ref016" class="usa-link" aria-describedby="pone.0330485.ref016">16</a>], which combines the mamba and YOLOv9 [<a href="#pone.0330485.ref017" class="usa-link" aria-describedby="pone.0330485.ref017">17</a>], reduces the loss of practical information, expands the receptive field, and can effectively detect remote sensing objects.</p>
<p>The above three types of remote sensing rotated object detection algorithms have good detection results in their respective fields but have the following problems [<a href="#pone.0330485.ref018" class="usa-link" aria-describedby="pone.0330485.ref018">18</a>–<a href="#pone.0330485.ref019" class="usa-link" aria-describedby="pone.0330485.ref019">19</a>]:</p>
<ol class="list" style="list-style-type:decimal">
<li><p>Many algorithms are incompatible with different sizes of ship objects; some algorithms are only improved for small objects, and others are only improved for large and obscured objects.</p></li>
<li><p>With the deepening of the network layers, the fine-grained features of remote-sensing ship objects will be gradually lost, and some algorithms do not make long-term memory retention for the fine-grained features of remote-sensing ships.</p></li>
<li><p>The complementary information fusion between feature maps of different scales is critical, but this process also requires selective learning to reduce the interference of redundant contextual information on remote sensing ship detection and fine-grained recognition of different sizes.</p></li>
</ol>
<p>To address the three problems mentioned above, this paper proposes the M-ReDet algorithm, which has the following main innovations:</p>
<ol class="list" style="list-style-type:decimal">
<li><p>We propose the M-Bottleneck to construct a new backbone network to achieve selective retention of fine-grained features of small ships in shallow feature maps and long-term memory of semantic features of large ships in deep feature maps and, at the same time to expand the receptive field to reduce the algorithm’s false detection of objects such as berths.</p></li>
<li><p>This article designs the SFRM module to reconstruct feature maps of different resolutions and selectively supplement the information difference between feature maps of different levels to improve the algorithm’s detection and fine-grained recognition capabilities for ships of different sizes.</p></li>
<li><p>This paper compares the effects of different combinations of CrossEntropyLoss, SmoothL1Loss, Focal Loss, and KFIoU used in the algorithm’s training and designs several groups of comparative experiments to find the optimal loss function configuration, improve the algorithm’s regression and classification accuracy, and reduce the loss of accuracy due to the imbalance in the number of different ship categories.</p></li>
<li><p>This article proposes a new remote sensing ship object detection algorithm called M-ReDet and conducts multiple comparative and ablation experiments on the FAIR1M(ship) [<a href="#pone.0330485.ref020" class="usa-link" aria-describedby="pone.0330485.ref020">20</a>] and DOTA datasets [<a href="#pone.0330485.ref021" class="usa-link" aria-describedby="pone.0330485.ref021">21</a>] to verify the effectiveness of the SOPM, SFRM modules, and optimized loss functions.</p></li>
</ol></section><section id="sec002"><h2 class="pmc_sec_title">Related work</h2>
<section id="sec003"><h3 class="pmc_sec_title">CNN</h3>
<p>ReDet is a classic remote sensing object detection algorithm based on convolutional neural networks. The design of the ReDet [<a href="#pone.0330485.ref022" class="usa-link" aria-describedby="pone.0330485.ref022">22</a>] mainly aims to solve two problems. First, based on the rotation variation characteristics of convolutional neural networks, ReDet proposes a rotation-invariant backbone network to extract the rotation-invariant features of remote sensing objects. Second, the RRoIAlign only performed a spatial alignment, and there was no alignment in the channel dimension. The RiRoI Align module, which consists of RPN (Region Proposal Network) and RT (ROI Transformer), can extract the features of the rotation recommendation region for classification and regression.</p>
<p>The overall structure of the ReDet mainly consists of a backbone network (ReResNet50) and Neck (ReFPN) [<a href="#pone.0330485.ref023" class="usa-link" aria-describedby="pone.0330485.ref023">23</a>], which can effectively extract the rotation-invariant features of the remote-sensing object. After the remote sensing image passes through ReResNet50, the algorithm can first obtain the rotation-invariant features of the remote sensing object. In the computation of the rotational features of multiple orientations, these computations share the weights, dramatically reducing the number of computational parameters required for each rotation. The rotation-invariant features of multiple orientations can be obtained by inputting the remote-sensing image of a fixed orientation. After that, we can fuse the feature maps of different layers in the ReFPN, and after the RiRoI Align module, the rotation-invariant features of the same remote sensing object can be extracted from the rotation-isotropic features, which contain features such as the tail and wing of an airplane, the aspect ratio of a ship, can enhance the accuracy of the remote sensing object detection. <a href="#pone.0330485.g001" class="usa-link">Fig 1</a> shows the network structure of the ReDet.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g001"><h4 class="obj_head">Fig 1. Structure of the ReDet.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g001.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/416d3fd4c4d0/pone.0330485.g001.jpg" loading="lazy" height="392" width="750" alt="Fig 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec004"><h3 class="pmc_sec_title">Transformer</h3>
<p>Two of the most widespread transformer in 2025, GQA (Group Query Attention) [<a href="#pone.0330485.ref024" class="usa-link" aria-describedby="pone.0330485.ref024">24</a>] and MLA (Multi-Head Latent Attention) [<a href="#pone.0330485.ref025" class="usa-link" aria-describedby="pone.0330485.ref025">25</a>], have driven the development of large models such as the Qwen [<a href="#pone.0330485.ref026" class="usa-link" aria-describedby="pone.0330485.ref026">26</a>] and DeepSeek [<a href="#pone.0330485.ref027" class="usa-link" aria-describedby="pone.0330485.ref027">27</a>], and have also provided CNN networks variety of optimization schemes, such as MSTrans [<a href="#pone.0330485.ref028" class="usa-link" aria-describedby="pone.0330485.ref028">28</a>], which uses Q, K, and V in GQA to reorganize the input vectors, and uses the MST module to multiple hierarchical feature maps for enhancing the model’s ability to extract pixel-level features from buildings. Among them, the basis of attention in GQA is self-attention, which is to serialize data such as text or images and reconstruct the sequence by calculating the correlation between the sequence and the sequence to complete the extraction of specific object features. The computation processes of self-attention are shown below:</p>
<table class="disp-formula p" id="pone.0330485.e001"><tr>
<td class="formula"><math id="M1" display="block" overflow="linebreak"><mrow><mrow><msup><mi>Q</mi><mi>i</mi></msup></mrow><mo>=</mo><mrow><msup><mi>W</mi><mi>q</mi></msup></mrow><mrow><msup><mi>a</mi><mi>i</mi></msup></mrow></mrow></math></td>
<td class="label">(1)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e002"><tr>
<td class="formula"><math id="M2" display="block" overflow="linebreak"><mrow><mrow><msup><mi>K</mi><mi>i</mi></msup></mrow><mo>=</mo><mrow><msup><mi>W</mi><mi>k</mi></msup></mrow><mrow><msup><mi>a</mi><mi>i</mi></msup></mrow></mrow></math></td>
<td class="label">(2)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e003"><tr>
<td class="formula"><math id="M3" display="block" overflow="linebreak"><mrow><mrow><msup><mi>V</mi><mi>i</mi></msup></mrow><mo>=</mo><mrow><msup><mi>W</mi><mi>v</mi></msup></mrow><mrow><msup><mi>a</mi><mi>i</mi></msup></mrow></mrow></math></td>
<td class="label">(3)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e004"><tr>
<td class="formula"><math id="M4" display="block" overflow="linebreak"><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mfrac><mrow><mrow><mi>Q</mi><mrow><msup><mi>K</mi><mi>T</mi></msup></mrow></mrow></mrow><mrow><mrow><msqrt><mrow><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></mrow></msqrt></mrow></mrow></mfrac></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mi>V</mi></mrow></math></td>
<td class="label">(4)</td>
</tr></table>
<p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e005"><math id="M5" display="inline" overflow="linebreak"><mrow><mi>a</mi></mrow></math></span> is the input vector, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e006"><math id="M6" display="inline" overflow="linebreak"><mrow><mi>W</mi></mrow></math></span> is the learnable parameter, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e007"><math id="M7" display="inline" overflow="linebreak"><mrow><mi>Q</mi></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e008"><math id="M8" display="inline" overflow="linebreak"><mrow><mi>K</mi></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e009"><math id="M9" display="inline" overflow="linebreak"><mrow><mi>V</mi></mrow></math></span> are the sequence computation units, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e010"><math id="M10" display="inline" overflow="linebreak"><mrow><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></mrow></math></span> is the data dimension. With the increase of parallel computation and the demand to reduce the computational quantity, self-attention gradually evolves into Multi-Head Attention (MHA), Group Query Attention (GQA), and Multi-Head Latent Attention (MLA); the most important difference between them is the different methods to avoid redundant computation by KV-cache technique, the following are their computational formulas respectively:</p>
<table class="disp-formula p" id="pone.0330485.e011"><tr>
<td class="formula"><math id="M11" display="block" overflow="linebreak"><mrow><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>;</mi><mi>i</mi></mrow></msub></mrow><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mfrac><mrow><mrow><mrow><msub><mi>q</mi><mrow><mi>t</mi><mi>;</mi><mi>i</mi></mrow></msub></mrow><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mrow><msubsup><mi>k</mi><mrow><mn>1</mn><mi>;</mi><mi>i</mi></mrow><mi>T</mi></msubsup><mo>,</mo><mo>⋯</mo><mo>,</mo><msubsup><mi>k</mi><mrow><mi>t</mi><mi>;</mi><mi>i</mi></mrow><mi>T</mi></msubsup></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></mrow><mrow><mrow><msqrt><mi>d</mi></msqrt></mrow></mrow></mfrac></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mrow><mtable><mtr><mtd><mrow><mo>*</mo><mrow><mn>20</mn></mrow><mrow><mi>c</mi></mrow></mrow><mrow><mrow><msub><mi>v</mi><mrow><mn>1</mn><mi>;</mi><mi>i</mi></mrow></msub></mrow></mrow></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mrow><mrow><msub><mi>v</mi><mrow><mi>t</mi><mi>;</mi><mi>i</mi></mrow></msub></mrow></mrow></mtd></mtr></mtable></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></td>
<td class="label">(5)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e012"><tr>
<td class="formula"><math id="M12" display="block" overflow="linebreak"><mrow><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>;</mi><mi>i</mi></mrow></msub></mrow><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mfrac><mrow><mrow><mrow><msub><mi>q</mi><mrow><mi>t</mi><mi>;</mi><mi>i</mi></mrow></msub></mrow><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mrow><msubsup><mi>k</mi><mrow><mn>1</mn><mi>;</mi><mrow><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow><mi>T</mi></msubsup><mo>,</mo><mo>⋯</mo><mo>,</mo><msubsup><mi>k</mi><mrow><mi>t</mi><mi>;</mi><mrow><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow><mi>T</mi></msubsup></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></mrow><mrow><mrow><msqrt><mi>d</mi></msqrt></mrow></mrow></mfrac></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mrow><mo fence="true" form="prefix" stretchy="true">[</mo><mrow><mtable><mtr><mtd><mrow><mo>*</mo><mrow><mn>20</mn></mrow><mrow><mi>c</mi></mrow></mrow><mrow><mrow><msub><mi>v</mi><mrow><mn>1</mn><mi>;</mi><mrow><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow></msub></mrow></mrow></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mrow><mrow><msub><mi>v</mi><mrow><mi>t</mi><mi>;</mi><mrow><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow></msub></mrow></mrow></mtd></mtr></mtable></mrow><mo fence="true" form="postfix" stretchy="true">]</mo></mrow></mrow></math></td>
<td class="label">(6)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e013"><tr>
<td class="formula"><math id="M13" display="block" overflow="linebreak"><mrow><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>;</mi><mi>i</mi></mrow></msub></mrow><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mfrac><mrow><mrow><mrow><msub><mi>q</mi><mrow><mi>t</mi><mi>;</mi><mi>i</mi></mrow></msub></mrow><msubsup><mi>K</mi><mrow><mo>≤</mo><mi>t</mi><mi>;</mi><mi>i</mi></mrow><mi>T</mi></msubsup></mrow></mrow><mrow><mrow><msqrt><mi>d</mi></msqrt></mrow></mrow></mfrac></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mrow><msub><mi>V</mi><mrow><mo>≤</mo><mi>t</mi><mi>;</mi><mi>i</mi></mrow></msub></mrow></mrow></math></td>
<td class="label">(7)</td>
</tr></table>
<p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e014"><math id="M14" display="inline" overflow="linebreak"><mrow><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>;</mi><mi>i</mi></mrow></msub></mrow></mrow></math></span> is the output of the attention mechanism; <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e015"><math id="M15" display="inline" overflow="linebreak"><mrow><mi>q</mi></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e016"><math id="M16" display="inline" overflow="linebreak"><mrow><mi>k</mi></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e017"><math id="M17" display="inline" overflow="linebreak"><mrow><mi>v</mi></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e018"><math id="M18" display="inline" overflow="linebreak"><mrow><mi>d</mi></mrow></math></span> are the same as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e019"><math id="M19" display="inline" overflow="linebreak"><mrow><mi>Q</mi></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e020"><math id="M20" display="inline" overflow="linebreak"><mrow><mi>K</mi></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e021"><math id="M21" display="inline" overflow="linebreak"><mrow><mi>V</mi></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e022"><math id="M22" display="inline" overflow="linebreak"><mrow><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></mrow></math></span><sub>,</sub>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e023"><math id="M23" display="inline" overflow="linebreak"><mrow><mi>g</mi></mrow></math></span> is the number of groups.</p></section><section id="sec005"><h3 class="pmc_sec_title">Mamba</h3>
<p>Mamba is a selective attention mechanism that can reduce the computation of the Transformer, selectively extract object features, and break through the bottleneck of the traditional model in content-aware and long-range modeling. The traditional state space model (SSM) parameters are static; Mamba introduces the gating parameter <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e024"><math id="M24" display="inline" overflow="linebreak"><mrow><mi>Δ</mi></mrow></math></span>, which discretizes the sequence. The gating parameter of Mamba, the hidden state, and the output are calculated as follows:</p>
<table class="disp-formula p" id="pone.0330485.e025"><tr>
<td class="formula"><math id="M25" display="block" overflow="linebreak"><mrow><msub><mrow><mi>Δ</mi></mrow><mrow><mi>t</mi></mrow></msub><mo>=</mo><mi>S</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msub><mrow><mi>W</mi></mrow><mrow><mrow><mi>∆</mi></mrow></mrow></msub><msub><mrow><mi>z</mi></mrow><mrow><mi>t</mi></mrow></msub><mo>+</mo><msub><mrow><mi>b</mi></mrow><mrow><mrow><mi>∆</mi></mrow></mrow></msub><mo fence="true" form="postfix" stretchy="true">)</mo></mrow></mrow></math></td>
<td class="label">(8)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e026"><tr>
<td class="formula"><math id="M26" display="block" overflow="linebreak"><mrow><msub><mrow><mover><mrow><mi>A</mi></mrow><mo accent="true">―</mo></mover></mrow><mrow><mi>t</mi></mrow></msub><mo>=</mo><msup><mrow><mi>e</mi></mrow><mrow><msub><mrow><mi>∆</mi></mrow><mrow><mi>t</mi></mrow></msub><mi>A</mi></mrow></msup></mrow></math></td>
<td class="label">(9)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e027"><tr>
<td class="formula"><math id="M27" display="block" overflow="linebreak"><mrow><msub><mrow><mover><mrow><mi>B</mi></mrow><mo accent="true">―</mo></mover></mrow><mrow><mi>t</mi></mrow></msub><mo>=</mo><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><msub><mrow><mi>Δ</mi></mrow><mrow><mi>t</mi></mrow></msub><mrow><mo> </mo></mrow><mi>B</mi><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>⊙</mo><msub><mrow><mi>z</mi></mrow><mrow><mi>t</mi></mrow></msub></mrow></math></td>
<td class="label">(10)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e028"><tr>
<td class="formula"><math id="M28" display="block" overflow="linebreak"><mrow><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><mo>=</mo><mrow><msub><mover><mi>A</mi><mo stretchy="true">¯</mo></mover><mi>t</mi></msub></mrow><mo>⊙</mo><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msub><mover><mi>B</mi><mo stretchy="true">¯</mo></mover><mi>t</mi></msub></mrow></mrow></math></td>
<td class="label">(11)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e029"><tr>
<td class="formula"><math id="M29" display="block" overflow="linebreak"><mrow><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>C</mi><mi>·</mi><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><mo>+</mo><mi>D</mi><mo>⊙</mo><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow></mrow></math></td>
<td class="label">(12)</td>
</tr></table>
<p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e030"><math id="M30" display="inline" overflow="linebreak"><mrow><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow></mrow></math></span> is the input sequence, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e031"><math id="M31" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>W</mi></mrow><mrow><mrow><mi>∆</mi></mrow></mrow></msub></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e032"><math id="M32" display="inline" overflow="linebreak"><mrow><msub><mrow><mi>b</mi></mrow><mrow><mrow><mi>∆</mi></mrow></mrow></msub></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e033"><math id="M33" display="inline" overflow="linebreak"><mrow><mi>C</mi></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e034"><math id="M34" display="inline" overflow="linebreak"><mrow><mi>D</mi></mrow></math></span> are the learnable parameters, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e035"><math id="M35" display="inline" overflow="linebreak"><mrow><mrow><msub><mover><mi>A</mi><mo stretchy="true">¯</mo></mover><mi>t</mi></msub></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e036"><math id="M36" display="inline" overflow="linebreak"><mrow><mrow><msub><mover><mi>B</mi><mo stretchy="true">¯</mo></mover><mi>t</mi></msub></mrow></mrow></math></span> are the discretization parameters, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e037"><math id="M37" display="inline" overflow="linebreak"><mrow><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow></mrow></math></span> is the hidden state, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e038"><math id="M38" display="inline" overflow="linebreak"><mrow><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow></mrow></math></span> is the output sequence. Mamba’s memorability can capture the contextual information of smaller targets, and the selectivity can dynamically adjust the proportion of the tail-category features to reduce the accuracy impact brought about by long-tailed distribution.</p></section></section><section id="sec006"><h2 class="pmc_sec_title">Our work</h2>
<section id="sec007"><h3 class="pmc_sec_title">M-ReDet</h3>
<p>This paper proposes the M-ReDet algorithm based on the ReDet algorithm by improving its Backbone, Neck, and Head to improve its detection and fine-grained recognition ability for ships of all sizes. The overall framework of the M-ReDet algorithm is shown in <a href="#pone.0330485.g002" class="usa-link">Fig 2</a>, which consists of the M-ReResNet50, the M-ReFPN, and the Head, respectively. The specific structure of each module after improvement will be described in detail in the subsequent subsections.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g002"><h4 class="obj_head">Fig 2. The structure of the M-ReDet network.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g002.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/9ca60af8f49d/pone.0330485.g002.jpg" loading="lazy" height="565" width="737" alt="Fig 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec008"><h3 class="pmc_sec_title">SOPM</h3>
<p>In this paper, the ship object perception module can better extract the edge, texture, semantic, and other features of remote sensing ship objects and improve the network’s ability to detect ships and fine-grained recognition in complex sea areas such as ports. The SOPM, as the main component of the backbone, can selectively extract and highlight the features of the object’s area. Its memorability can also pass the features extracted by the upper level of the feature map to the next layer so that the next layer can selectively extract the contextual information of different sizes of remote-sensing ship objects. The specific structure of SOPM is shown in <a href="#pone.0330485.g003" class="usa-link">Fig 3</a>.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g003"><h4 class="obj_head">Fig 3. The structure of the SOPM module.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g003.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/beceef17cd08/pone.0330485.g003.jpg" loading="lazy" height="414" width="749" alt="Fig 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>Before the feature map enters the SOPM, the remote sensing image passes through a 7 × 7 convolutional layer to initially extract the features such as ship texture and edges. Then the feature map enters into the multilayered ResLayer, which are all composed of M-Bottleneck and Bottleneck. Inside the M-Bottleneck, the feature map is first computed by the convolution, MLP, and Sigmoid to calculate the feature map channel weights, which weight the semantic, edge, and other features of the remote sensing ships to get the fine-grained features needed for ship detection and recognition. Then, the SOPM module serializes the feature map. After the serialized features pass through the mamba’s SSM module, the module can selectively extract the features of the ship object corresponding to the size of the feature map at that level and pass other features to the next level through memory to supplement the contextual information and fine-grained features of the ship corresponding to the size of the next level.</p>
<p>Corresponding to the four-layer ResLayer of ReResNet50, M-ReResNet50 also has four layers of ResLayers. However, the number of M-Bottlenecks in each layer differs from that of ReResNet50, and the ResLayers with different numbers of M-Bottlenecks have different capabilities for feature extraction, object perception, and localization for remote sensing ship objects. In order to find the optimal quantity ratio and ensure the total number of M-Bottleneck and Bottleneck is 3-4-6-3, this paper conducts the following experiments, setting the quantity of M-Bottleneck in each layer as 0-0-0-0, 0-1-1-0, 1-1-1-1, 1-2-2-1 and 2-2-2-2 respectively, and observing the effect of the M-Bottleneck quantity configuration on the algorithm. M-Bottleneck number configuration and the experimental results are shown in <a href="#pone.0330485.t001" class="usa-link">Table 1</a>.</p>
<section class="tw xbox font-sm" id="pone.0330485.t001"><h4 class="obj_head">Table 1. Experimental results of the M-Bottleneck configuration.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Number</th>
<th align="left" rowspan="1" colspan="1">M-Bottleneck Set</th>
<th align="left" rowspan="1" colspan="1">mAP0.5(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0-0-0-0</td>
<td align="left" rowspan="1" colspan="1">40.51</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">0-1-1-0</td>
<td align="left" rowspan="1" colspan="1">40.12</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">1-1-1-1</td>
<td align="left" rowspan="1" colspan="1">41.52</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">1-2-2-1</td>
<td align="left" rowspan="1" colspan="1">41.43</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">2-2-2-2</td>
<td align="left" rowspan="1" colspan="1">41.50</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330485.t001/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>According to <a href="#pone.0330485.t001" class="usa-link">Table 1</a>, when the number of M-Bottleneck in each layer of ResLayer is set to 1-1-1-1, the mAP<sub>0.5</sub> of the algorithm is the highest, which is 41.52%, and the accuracy is improved by 1.01% compared to ResLayer in ReResNet50. With the increase of the number of M-Bottlenecks, M-ReDet’s accuracy does not continue to increase but stabilizes around 41.52%; adding too many M-Bottlenecks will introduce additional computation, so in this paper, we chose 1-1-1-1 as the configuration parameter of ResLayer.</p></section><section id="sec009"><h3 class="pmc_sec_title">SFRM</h3>
<p>Compared with ReResNet stacking a large number of small convolutional kernels, stacking a small number of M-Bottleneck can obtain a substantial receptive field enhancement, and theoretically, M-Bottleneck can obtain the global receptive field, which can effectively improve the algorithm’s extraction ability for the contextual information of ship objects. The feature maps in the backbone network pass through the SOPM module; the algorithm can obtain the rotation-isotropic, rotation-invariant, and fine-grained features of various ship objects. The SFRM module in this section can fully use these features to fuse the upper-layer low-resolution ship fine-grained semantic features with the lower-layer high-resolution ship localization information to improve the algorithm’s ship detection and fine-grained recognition accuracy. The structure diagram of the SFRM module is shown in <a href="#pone.0330485.g004" class="usa-link">Fig 4</a>.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g004"><h4 class="obj_head">Fig 4. Structure of the SFRM module.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g004.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/4ad7535277b5/pone.0330485.g004.jpg" loading="lazy" height="275" width="743" alt="Fig 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section><section id="sec010"><h3 class="pmc_sec_title">Changing the loss function</h3>
<p>The ReDet algorithm uses CrossEntropy Loss [<a href="#pone.0330485.ref029" class="usa-link" aria-describedby="pone.0330485.ref029">29</a>] and SmoothL1 Loss [<a href="#pone.0330485.ref030" class="usa-link" aria-describedby="pone.0330485.ref030">30</a>] as the classification and regression losses in the Head, respectively. The selection of the loss function has a particular impact on the algorithm’s classification and regression accuracy. In this subsection, the classification and regression losses of the M-ReDet are modified as the Focal Loss and KFIoU Loss.</p>
<p>Focal Loss can deal with the long-tailed distribution problem that exists in ship object detection and fine-grained recognition tasks by adjusting the balance factor and focus factor to change the value size of the classification loss of each ship in the FAIR1M(ship) dataset; there are a total of nine categories of ships, namely Dry Cargo ship, Engineering Ship, Fishing Boat, Liquid Cargo Ship, Motorboat, Passenger Ship, Tugboat, W-ship, and other-ship, the number of ships in each category varies is shown in <a href="#pone.0330485.g005" class="usa-link">Fig 5</a>.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g005"><h4 class="obj_head">Fig 5. The number of each type ship in the FAIR1M(ship) dataset.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g005.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/2c36fdc8e5d2/pone.0330485.g005.jpg" loading="lazy" height="337" width="756" alt="Fig 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>KFIoU Loss [<a href="#pone.0330485.ref031" class="usa-link" aria-describedby="pone.0330485.ref031">31</a>] is an approximation of SkewIoU, which represents the overlapping region of the rotated boxes by Kalman filtering; it essentially calculates the overlap rate to replace the IOU without introducing additional parameters and has a complete derivation and computation for non-overlapping scenarios, which is effective and improves the accuracy in the field of rotated object detection and fine-grained recognition. Focal Loss [<a href="#pone.0330485.ref032" class="usa-link" aria-describedby="pone.0330485.ref032">32</a>] and KFIoU Loss are calculated as shown below:</p>
<table class="disp-formula p" id="pone.0330485.e039"><tr>
<td class="formula"><math id="M39" display="block" overflow="linebreak"><mrow><mi>F</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mrow><msub><mi>p</mi><mi>t</mi></msub></mrow></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mo>=</mo><mo>−</mo><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><mrow><msup><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msub><mi>p</mi><mi>t</mi></msub></mrow></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow><mi>γ</mi></msup></mrow><mi>log</mi><mrow><mo fence="true" form="prefix" stretchy="true">(</mo><mrow><mrow><msub><mi>p</mi><mi>t</mi></msub></mrow></mrow><mo fence="true" form="postfix" stretchy="true">)</mo></mrow></mrow></math></td>
<td class="label">(13)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e040"><tr>
<td class="formula"><math id="M40" display="block" overflow="linebreak"><mrow><msub><mi>𝒱</mi><mrow><mi>ℬ</mi></mrow></msub><mo stretchy="false">(</mo><mi>Σ</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mrow><mn>2</mn></mrow><mrow><mi>n</mi></mrow></msup><msqrt><mrow><mo>∏</mo><mrow><mtext>eig</mtext></mrow><mo stretchy="false">(</mo><mi>Σ</mi><mo stretchy="false">)</mo></mrow></msqrt><mo>=</mo><msup><mrow><mn>2</mn></mrow><mrow><mi>n</mi></mrow></msup><mo> </mo><mrow><mo fence="true" form="prefix" stretchy="true">|</mo><msup><mrow><mi>Σ</mi></mrow><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></msup><mo fence="true" form="postfix" stretchy="true">|</mo></mrow><mo>=</mo><msup><mrow><mn>2</mn></mrow><mrow><mi>n</mi></mrow></msup><mo stretchy="false">|</mo><mi>Σ</mi><msup><mo stretchy="false">|</mo><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></msup><mo> </mo></mrow></math></td>
<td class="label">(14)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e041"><tr>
<td class="formula"><math id="M41" display="block" overflow="linebreak"><mrow><mi>K</mi><mi>F</mi><mi>I</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mrow><mrow><mrow><msub><mrow><mi mathvariant="script">V</mi></mrow><mrow><mrow><msub><mrow><mi mathvariant="script">B</mi></mrow><mn>3</mn></msub></mrow></mrow></msub></mrow><mo stretchy="false">(</mo><mi>Σ</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mrow><mrow><msub><mrow><mi mathvariant="script">V</mi></mrow><mrow><mrow><msub><mrow><mi mathvariant="script">B</mi></mrow><mn>1</mn></msub></mrow></mrow></msub></mrow><mo stretchy="false">(</mo><mrow><msub><mi>Σ</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo><mo>+</mo><mrow><msub><mrow><mi mathvariant="script">V</mi></mrow><mrow><mrow><msub><mrow><mi mathvariant="script">B</mi></mrow><mn>2</mn></msub></mrow></mrow></msub></mrow><mo stretchy="false">(</mo><mrow><msub><mi>Σ</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo><mo>+</mo><mrow><msub><mrow><mi mathvariant="script">V</mi></mrow><mrow><mrow><msub><mrow><mi mathvariant="script">B</mi></mrow><mn>3</mn></msub></mrow></mrow></msub></mrow><mo stretchy="false">(</mo><mrow><msub><mi>Σ</mi><mn>3</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></math></td>
<td class="label">(15)</td>
</tr></table>
<p>Where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e042"><math id="M42" display="inline" overflow="linebreak"><mrow><mrow><msub><mi>p</mi><mi>t</mi></msub></mrow></mrow></math></span> is the class label prediction probability, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e043"><math id="M43" display="inline" overflow="linebreak"><mrow><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow></mrow></math></span> is the balance factor, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e044"><math id="M44" display="inline" overflow="linebreak"><mrow><mi>γ</mi></mrow></math></span> is the focus factor, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e045"><math id="M45" display="inline" overflow="linebreak"><mrow><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e046"><math id="M46" display="inline" overflow="linebreak"><mrow><mi>γ</mi></mrow></math></span> can adjust the effect of different categories of samples on the classification loss. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e047"><math id="M47" display="inline" overflow="linebreak"><mrow><mrow><msub><mrow><mi mathvariant="script">V</mi></mrow><mrow><mrow><msub><mrow><mi mathvariant="script">B</mi></mrow><mn>1</mn></msub></mrow></mrow></msub></mrow></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e048"><math id="M48" display="inline" overflow="linebreak"><mrow><mrow><msub><mrow><mi mathvariant="script">V</mi></mrow><mrow><mrow><msub><mrow><mi mathvariant="script">B</mi></mrow><mn>2</mn></msub></mrow></mrow></msub></mrow></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e049"><math id="M49" display="inline" overflow="linebreak"><mrow><mrow><msub><mrow><mi mathvariant="script">V</mi></mrow><mrow><mrow><msub><mrow><mi mathvariant="script">B</mi></mrow><mn>3</mn></msub></mrow></mrow></msub></mrow></mrow></math></span> are the areas of the prediction box, the actual box, and the overlapping region, respectively. This subsection also investigates the effect of different combinations of loss functions on the accuracy of ship object detection and fine-grained recognition, and <a href="#pone.0330485.t002" class="usa-link">Table 2</a> shows the results.</p>
<section class="tw xbox font-sm" id="pone.0330485.t002"><h4 class="obj_head">Table 2. Experimental results for different loss functions.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Algorithm</th>
<th align="left" rowspan="1" colspan="1">Loss Set</th>
<th align="left" rowspan="1" colspan="1">mAP0.5(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="4" colspan="1">ReDet</td>
<td align="left" rowspan="1" colspan="1">CrossEntropyLoss+SmoothL1Loss (original)</td>
<td align="left" rowspan="1" colspan="1">40.51</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Focal Loss+SmoothL1Loss</td>
<td align="left" rowspan="1" colspan="1">40.32</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Focal Loss+KFIoU</td>
<td align="left" rowspan="1" colspan="1">41.39</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">CrossEntropyLoss+KFIoU</td>
<td align="left" rowspan="1" colspan="1">40.87</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330485.t002/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>As <a href="#pone.0330485.t002" class="usa-link">Table 2</a> shows, among the four loss function combinations, the mAP<sub>0.5</sub> of the Focal Loss + KFIoU is 41.39%, which is 0.88%, 1.07% and 0.52% higher than the CrossEntropy Loss + SmoothL1 Loss, Focal Loss + SmoothL1 Loss, and CrossEntropy Loss + KFIoU combinations, respectively. Thus, it is suitable for training the M-ReDet algorithm.</p></section></section><section id="sec011"><h2 class="pmc_sec_title">Experiment and result analysis</h2>
<section id="sec012"><h3 class="pmc_sec_title">Experimental environment and parameter configuration</h3>
<p>The experimental environment used for the M-Redet remote sensing ship target detection and fine-grained recognition algorithm designed in this paper is CUDA11.1, Intel i7-11700 CPU, NVIDIA GeForce RTX 3080Ti, and the deep learning framework is PyTorch. In this paper, the comparison and ablation experiments are conducted using DOTAv1.0 and FAIR1M(ship) datasets to evaluate the detection accuracy of M-Redet and the effectiveness of each module. The experiment selects SGD as an optimizer, set lr = 0.0025, momentum = 0.9, weight_decay = 0.0001, warmup_iters = 500, warmup_ratio = 1.0/3, and the learning rate strategy is linear. The overall training has 100 epochs, and if the algorithm converges ahead, then end the training round early. <a href="#pone.0330485.g006" class="usa-link">Fig 6</a> shows the loss convergence curve of M-ReDet on the DOTAv1.0 and FAIR1M(ship) datasets. <a href="#pone.0330485.t003" class="usa-link">Table 3</a> shows the experimental environment, parameter configuration, and model resource consumption.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g006"><h4 class="obj_head">Fig 6. Loss function curves for the training.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g006.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/eef0a4bba413/pone.0330485.g006.jpg" loading="lazy" height="645" width="799" alt="Fig 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><section class="tw xbox font-sm" id="pone.0330485.t003"><h4 class="obj_head">Table 3. Experimental details table.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Parameter</th>
<th align="left" rowspan="1" colspan="1">Set</th>
<th align="left" rowspan="1" colspan="1">Algorithm</th>
<th align="left" rowspan="1" colspan="1">Resource consumption</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">CPU</td>
<td align="left" rowspan="1" colspan="1">Intel i7-11700</td>
<td align="left" rowspan="5" colspan="1">ReDet</td>
<td align="left" rowspan="5" colspan="1">Batch:5<br>GPU memory usage: 10.557G<br>Single batch time consumption: 43.8 min(8.76 min/batch)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GPU</td>
<td align="left" rowspan="1" colspan="1">RTX 3080Ti</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">CUDA</td>
<td align="left" rowspan="1" colspan="1">11.1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">optimizer</td>
<td align="left" rowspan="1" colspan="1">SGD</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">lr</td>
<td align="left" rowspan="1" colspan="1">0.0025</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">momentum</td>
<td align="left" rowspan="1" colspan="1">0.9</td>
<td align="left" rowspan="5" colspan="1">M-ReDet (Ours)</td>
<td align="left" rowspan="5" colspan="1">Batch:4<br>GPU memory usage: 10.896G<br>Single batch time consumption: 31.8 min(7.95 min/batch)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">weight_decay</td>
<td align="left" rowspan="1" colspan="1">0.0001</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">warmup_iters</td>
<td align="left" rowspan="1" colspan="1">500</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">warmup_ratio</td>
<td align="left" rowspan="1" colspan="1">1.0/ 3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">DL Framework</td>
<td align="left" rowspan="1" colspan="1">pytorch</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330485.t003/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="sec013"><h3 class="pmc_sec_title">Datasets</h3>
<p>The DOTAv1.0 dataset has a total of 2806 images, which contains 15 types of remote sensing targets, namely the plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field, swimming pool, and the number of instances is 188282. The number of training sets in the DOTAv1.0 dataset is 1411, the number of validation sets is 458, and the number of test sets is 937. In order to facilitate the training of the algorithm, in this paper, the remote sensing images in the DOTAv1.0 dataset are cropped into 1024 × 1024 size, totaling 21046 remote sensing images. <a href="#pone.0330485.g007" class="usa-link">Fig 7</a> shows the example of the DOTAv1.0 dataset. <a href="#pone.0330485.g008" class="usa-link">Fig 8</a> illustrates the distribution of quantities for each category in the DOTAv1.0 dataset.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g007"><h4 class="obj_head">Fig 7. Examples of the DOTA dataset.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g007.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/fdb491764849/pone.0330485.g007.jpg" loading="lazy" height="617" width="617" alt="Fig 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><a href="#pone.0330485.g007" class="usa-link">Fig 7</a> is attributed to the DOTA open-source database and is available from the DOTA database (URL (s): <a href="https://captain-whu.github.io/DOTA/dataset.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://captain-whu.github.io/DOTA/dataset.html</a>).</p></figcaption></figure><figure class="fig xbox font-sm" id="pone.0330485.g008"><h4 class="obj_head">Fig 8. The number of various types in the DOTA.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g008.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/6912ae0ecd0a/pone.0330485.g008.jpg" loading="lazy" height="600" width="750" alt="Fig 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>The FAIR1M(ship) dataset contains all remote sensing images containing remote sensing ship objects in the FAIR1M2.0 dataset. The dataset contains a total of 13238 remote sensing ship images, the number of instances is 58982, and there are a total of nine types of ship instances, namely the Dry Cargo Ship, Engineering Ship, Fishing Boat, Liquid Cargo Ship, Motorboat, Passenger Ship, Tugboat, W-ship, and other-ship. The number of ships in each category is 17474, 3897, 9031, 3883, 15445, 1924, 1897, 1178 and 4253, respectively. The FAIR1M(ship) dataset is rich in all kinds of ships, which is suitable for the research of remote sensing ship object detection and fine-grained recognition. <a href="#pone.0330485.g009" class="usa-link">Fig 9</a> shows part of the FAIR1M(ship) dataset examples.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g009"><h4 class="obj_head">Fig 9. Examples of the FAIR1M(ship) dataset.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/ac089483d051/pone.0330485.g009.jpg" loading="lazy" height="789" width="792" alt="Fig 9"></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g009/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><a href="#pone.0330485.g009" class="usa-link">Fig 9</a> is attributed to the FAIR1M open-source database and is available from the FAIR1M database (URL (s): <a href="https://gaofen-challenge.com/benchmark" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://gaofen-challenge.com/benchmark</a>).</p></figcaption></figure></section><section id="sec014"><h3 class="pmc_sec_title">Experimental evaluation indicators</h3>
<p>We use <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e050"><math id="M50" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mrow></mrow></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e051"><math id="M51" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi></mrow></mrow></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e052"><math id="M52" display="inline" overflow="linebreak"><mrow><mi>A</mi><mi>P</mi></mrow></math></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e053"><math id="M53" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">P</mi></mrow></mrow></mrow></math></span> as experimental evaluation indicators to validate the M-ReDet algorithm and the performance enhancement of each module in the comparison and ablation experiments. The formulas for <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e054"><math id="M54" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mrow></mrow></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e055"><math id="M55" display="inline" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi></mrow></mrow></mrow></math></span> are as follows:</p>
<table class="disp-formula p" id="pone.0330485.e056"><tr>
<td class="formula"><math id="M56" display="block" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mo>=</mo></mrow></mrow><mfrac><mrow><mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">P</mi></mrow></mrow></mrow><mrow><mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">P</mi></mrow><mo>+</mo><mrow><mi mathvariant="normal">F</mi><mi mathvariant="normal">P</mi></mrow></mrow></mrow></mfrac></mrow></math></td>
<td class="label">(16)</td>
</tr></table>
<table class="disp-formula p" id="pone.0330485.e057"><tr>
<td class="formula"><math id="M57" display="block" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi><mo>=</mo></mrow></mrow><mfrac><mrow><mrow><mtext>TP</mtext></mrow></mrow><mrow><mrow><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mrow></mfrac></mrow></math></td>
<td class="label">(17)</td>
</tr></table>
<p><span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e058"><math id="M58" display="inline" overflow="linebreak"><mrow><mi>T</mi><mi>P</mi></mrow></math></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e059"><math id="M59" display="inline" overflow="linebreak"><mrow><mi>F</mi><mi>P</mi></mrow></math></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e060"><math id="M60" display="inline" overflow="linebreak"><mrow><mi>F</mi><mi>N</mi></mrow></math></span> represent the number of true positives, false positives, and false negatives, respectively. Average Precision is an averaging of the accuracies at different recall rates, and in general, the model The higher the average accuracy for a specific category of target detection, the larger the AP value. The formula is as follows:</p>
<table class="disp-formula p" id="pone.0330485.e061"><tr>
<td class="formula"><math id="M61" display="block" overflow="linebreak"><mrow><mtext>AP</mtext><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mrow><mtext>P</mtext><mo stretchy="false">(</mo><mtext>R</mtext><mo stretchy="false">)</mo><mtext>dR</mtext></mrow></mrow></math></td>
<td class="label">(18)</td>
</tr></table>
<p>Where R is Recall, for a complete response to the model’s overall accuracy, mAP is the average AP of all objects:</p>
<table class="disp-formula p" id="pone.0330485.e062"><tr>
<td class="formula"><math id="M62" display="block" overflow="linebreak"><mrow><mrow><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">P</mi></mrow></mrow><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>N</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>A</mi><mrow><msub><mi>P</mi><mi>i</mi></msub></mrow></mrow></mrow></math></td>
<td class="label">(19)</td>
</tr></table>
<p>N is the number of object categories, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="pone.0330485.e063"><math id="M63" display="inline" overflow="linebreak"><mrow><mi>A</mi><mrow><msub><mi>P</mi><mi>i</mi></msub></mrow></mrow></math></span> is the average accuracy of each category.</p></section><section id="sec015"><h3 class="pmc_sec_title">Analysis of experimental results</h3>
<section id="sec016"><h4 class="pmc_sec_title">Comparison experiments on the DOTA dataset.</h4>
<p>This subsection compares the M-ReDet algorithm with the commonly used rotated object detection algorithms such as the GWD, R<sup>3</sup>Det, RoI Transformer [<a href="#pone.0330485.ref033" class="usa-link" aria-describedby="pone.0330485.ref033">33</a>], Rotated Faster RCNN, Rotated RetinaNet, Rotated Reppoints [<a href="#pone.0330485.ref034" class="usa-link" aria-describedby="pone.0330485.ref034">34</a>], S<sup>2</sup>ANet [<a href="#pone.0330485.ref035" class="usa-link" aria-describedby="pone.0330485.ref035">35</a>], SASM, KFIoU, and ReDet to verify its effectiveness on the DOTA dataset. <a href="#pone.0330485.g010" class="usa-link">Fig 10</a> shows the detection results of the M-ReDet algorithm on the DOTA dataset.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g010"><h5 class="obj_head">Fig 10. Detection results of the M-ReDet on the DOTA dataset.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g010.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/120834613e1a/pone.0330485.g010.jpg" loading="lazy" height="466" width="697" alt="Fig 10"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g010/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><a href="#pone.0330485.g010" class="usa-link">Fig 10</a> is attributed to the DOTA open-source database and is available from the DOTA database (URL <strong>(s)</strong>: <a href="https://captain-whu.github.io/DOTA/dataset.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://captain-whu.github.io/DOTA/dataset.html</a>).</p></figcaption></figure><p>The detection result shows that the M-ReDet can detect small, medium and large remote-sensing objects well. Faced with simple remote-sensing objects such as tennis courts and object-intensive scenarios such as harbors, the M-ReDet’s detection accuracies are mostly above 90%, and some occluded and small objects have false and missed detection. <a href="#pone.0330485.t004" class="usa-link">Table 4</a> shows the experimental results of comparing the M-ReDet and the above 10 algorithms.</p>
<section class="tw xbox font-sm" id="pone.0330485.t004"><h5 class="obj_head">Table 4. Results of the DOTA dataset comparison experiment.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Algorithm</th>
<th align="left" rowspan="1" colspan="1">Size</th>
<th align="left" rowspan="1" colspan="1">mAP<sub>0.5</sub>(%)</th>
<th align="left" rowspan="1" colspan="1">AP-max(%)</th>
<th align="left" rowspan="1" colspan="1">AP-min(%)</th>
<th align="left" rowspan="1" colspan="1">Recall-max(%)</th>
<th align="left" rowspan="1" colspan="1">Recall-min(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Roi-Transformer</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">80.53<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90.9</td>
<td align="left" rowspan="1" colspan="1">62.3</td>
<td align="left" rowspan="1" colspan="1">99.6</td>
<td align="left" rowspan="1" colspan="1">72.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">SASM</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">70.81<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90.7</td>
<td align="left" rowspan="1" colspan="1">42.6</td>
<td align="left" rowspan="1" colspan="1">96.8</td>
<td align="left" rowspan="1" colspan="1">71.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ReDet</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">78.75<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90.9</td>
<td align="left" rowspan="1" colspan="1">61.3</td>
<td align="left" rowspan="1" colspan="1">97.7</td>
<td align="left" rowspan="1" colspan="1">75.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">R<sup>3</sup>Det</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">75.37<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90.9</td>
<td align="left" rowspan="1" colspan="1">56</td>
<td align="left" rowspan="1" colspan="1">96.4</td>
<td align="left" rowspan="1" colspan="1">74.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Faster-Rcnn</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">78.60<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90.8</td>
<td align="left" rowspan="1" colspan="1">59.1</td>
<td align="left" rowspan="1" colspan="1">97.7</td>
<td align="left" rowspan="1" colspan="1">69.6</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Rotated RetinaNet</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">77.57<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90.8</td>
<td align="left" rowspan="1" colspan="1">52.6</td>
<td align="left" rowspan="1" colspan="1">99.2</td>
<td align="left" rowspan="1" colspan="1">73.6</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Rotate-reppoints</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">66.18<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90</td>
<td align="left" rowspan="1" colspan="1">34.4</td>
<td align="left" rowspan="1" colspan="1">96.5</td>
<td align="left" rowspan="1" colspan="1">68.9</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">KFIoU</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">79.88<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90.9</td>
<td align="left" rowspan="1" colspan="1">62.6</td>
<td align="left" rowspan="1" colspan="1">99.2</td>
<td align="left" rowspan="1" colspan="1">78.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GWD</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">79.41<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90.9</td>
<td align="left" rowspan="1" colspan="1">62.0</td>
<td align="left" rowspan="1" colspan="1">98.9</td>
<td align="left" rowspan="1" colspan="1">83.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">S<sup>2</sup>ANet.</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">79.58<a href="#t004fn001" class="usa-link"><sup>*</sup></a>
</td>
<td align="left" rowspan="1" colspan="1">90.9</td>
<td align="left" rowspan="1" colspan="1">62.6</td>
<td align="left" rowspan="1" colspan="1">99.2</td>
<td align="left" rowspan="1" colspan="1">78.0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">M-ReDet(Ours)</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">82.09</td>
<td align="left" rowspan="1" colspan="1">90.8</td>
<td align="left" rowspan="1" colspan="1">65.3</td>
<td align="left" rowspan="1" colspan="1">97.2</td>
<td align="left" rowspan="1" colspan="1">72.9</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330485.t004/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p">
<div class="fn" id="t004fn002"><p>In this table,</p></div>
<div class="fn" id="t004fn001"><p>* means that the results from our previous work [<a href="#pone.0330485.ref036" class="usa-link" aria-describedby="pone.0330485.ref036">36</a>].</p></div>
</div></section><p>The AP-max of M-ReDet is similar to other algorithms but has the highest AP-min, which verifies its effectiveness in enhancing the classification and detection accuracy of remote-sensing objects with small samples. <a href="#pone.0330485.t005" class="usa-link">Table 5</a> shows the detection accuracies of various remote-sensing objects by the M-ReDet algorithm on the DOTA dataset.</p>
<section class="tw xbox font-sm" id="pone.0330485.t005"><h5 class="obj_head">Table 5. Detection results of various types of remote sensing objects.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Class</th>
<th align="left" rowspan="1" colspan="1">Gts</th>
<th align="left" rowspan="1" colspan="1">Dets</th>
<th align="left" rowspan="1" colspan="1">Recall(%)</th>
<th align="left" rowspan="1" colspan="1">AP(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">plane</td>
<td align="left" rowspan="1" colspan="1">4449</td>
<td align="left" rowspan="1" colspan="1">4926</td>
<td align="left" rowspan="1" colspan="1">96.1</td>
<td align="left" rowspan="1" colspan="1">90.7</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ship</td>
<td align="left" rowspan="1" colspan="1">18537</td>
<td align="left" rowspan="1" colspan="1">21015</td>
<td align="left" rowspan="1" colspan="1">95.7</td>
<td align="left" rowspan="1" colspan="1">90.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">storage tank</td>
<td align="left" rowspan="1" colspan="1">4740</td>
<td align="left" rowspan="1" colspan="1">4354</td>
<td align="left" rowspan="1" colspan="1">76.4</td>
<td align="left" rowspan="1" colspan="1">72.0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">baseball diamond</td>
<td align="left" rowspan="1" colspan="1">358</td>
<td align="left" rowspan="1" colspan="1">318</td>
<td align="left" rowspan="1" colspan="1">72.9</td>
<td align="left" rowspan="1" colspan="1">70.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">tennis course</td>
<td align="left" rowspan="1" colspan="1">1512</td>
<td align="left" rowspan="1" colspan="1">1654</td>
<td align="left" rowspan="1" colspan="1">96.6</td>
<td align="left" rowspan="1" colspan="1">90.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">basketball course</td>
<td align="left" rowspan="1" colspan="1">266</td>
<td align="left" rowspan="1" colspan="1">361</td>
<td align="left" rowspan="1" colspan="1">95.9</td>
<td align="left" rowspan="1" colspan="1">89.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ground track field</td>
<td align="left" rowspan="1" colspan="1">212</td>
<td align="left" rowspan="1" colspan="1">589</td>
<td align="left" rowspan="1" colspan="1">95.3</td>
<td align="left" rowspan="1" colspan="1">83.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">harbor</td>
<td align="left" rowspan="1" colspan="1">4167</td>
<td align="left" rowspan="1" colspan="1">5378</td>
<td align="left" rowspan="1" colspan="1">91.6</td>
<td align="left" rowspan="1" colspan="1">87.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">bridge</td>
<td align="left" rowspan="1" colspan="1">785</td>
<td align="left" rowspan="1" colspan="1">1179</td>
<td align="left" rowspan="1" colspan="1">79.7</td>
<td align="left" rowspan="1" colspan="1">65.3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">large vehicle</td>
<td align="left" rowspan="1" colspan="1">8819</td>
<td align="left" rowspan="1" colspan="1">13333</td>
<td align="left" rowspan="1" colspan="1">97.2</td>
<td align="left" rowspan="1" colspan="1">89.0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">small vehicle</td>
<td align="left" rowspan="1" colspan="1">10579</td>
<td align="left" rowspan="1" colspan="1">15373</td>
<td align="left" rowspan="1" colspan="1">88.7</td>
<td align="left" rowspan="1" colspan="1">77.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">helicopter</td>
<td align="left" rowspan="1" colspan="1">122</td>
<td align="left" rowspan="1" colspan="1">175</td>
<td align="left" rowspan="1" colspan="1">96.7</td>
<td align="left" rowspan="1" colspan="1">89.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">roundabout</td>
<td align="left" rowspan="1" colspan="1">275</td>
<td align="left" rowspan="1" colspan="1">325</td>
<td align="left" rowspan="1" colspan="1">84.0</td>
<td align="left" rowspan="1" colspan="1">79.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">soccer ball field</td>
<td align="left" rowspan="1" colspan="1">251</td>
<td align="left" rowspan="1" colspan="1">495</td>
<td align="left" rowspan="1" colspan="1">93.2</td>
<td align="left" rowspan="1" colspan="1">83.9</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">swimming pool</td>
<td align="left" rowspan="1" colspan="1">732</td>
<td align="left" rowspan="1" colspan="1">1133</td>
<td align="left" rowspan="1" colspan="1">85.9</td>
<td align="left" rowspan="1" colspan="1">71.7</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330485.t005/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>The M-ReDet has the highest detection accuracy for the tennis course with an AP value of 90.8%, and for the remote sensing objects, such as planes, ships, basketball courses, harbors, large vehicles, and helicopters, also have high detection accuracies of around 90%. Since the M-ReDet uses M-Bottleneck to expand the receptive field when facing remote sensing objects with large sizes and high aspect ratios, such as bridges, the algorithms in this paper can combine the contextual information, such as selectively utilizing the road information on both sides of the bridge to make the detection judgment. Finally, there is a 4% accuracy improvement in AP-min(bridge). <a href="#pone.0330485.g011" class="usa-link">Fig 11</a> shows the training results of the above types of algorithms.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g011"><h5 class="obj_head">Fig 11. Training results of each algorithm on the DOTA dataset.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g011.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/998091a5d1bc/pone.0330485.g011.jpg" loading="lazy" height="548" width="686" alt="Fig 11"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g011/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure><p>The 11 algorithms all use the pre-trained model to train; as seen from <a href="#pone.0330485.g010" class="usa-link">Fig 10</a>, the SASM and Rotated RepPoints algorithms’ convergence is relatively slower compared to the other algorithms. Most of the algorithms converge to the optimal mAP<sub>0.5</sub> in about 10 epochs. The M-ReDet algorithm achieves convergence after 10 epochs, and the final mAP0.5 is stabilized at about 82.09%. Its training curve is also in the top left corner, indicating optimal convergence speed and accuracy.</p></section><section id="sec017"><h4 class="pmc_sec_title">Comparison experiments on the FAIR1M(ship) dataset.</h4>
<p>The M-ReDet algorithm performs well in dense ship distribution, sparse ship distribution, simple remote sensing background, and complex remote sensing background. <a href="#pone.0330485.g012" class="usa-link">Fig 12</a> shows its ship detection and fine-grained recognition results on the FAIR1M(ship) dataset. Since the appearance and aspect ratio of these nine types of ship objects are similar, it is necessary to extract the fine-grained features of each type of ship from the texture and semantic information in order to recognize the type of ship accurately. The M-ReDet algorithm first extracts the fine-grained features of the spatial location of the ship by using the SOPM and then extracts the different features between the different types of ships by using the SFRM.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g012"><h5 class="obj_head">Fig 12. Ship object detection and fine-grained recognition results.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g012.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/4713e892c716/pone.0330485.g012.jpg" loading="lazy" height="466" width="698" alt="Fig 12"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g012/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><a href="#pone.0330485.g012" class="usa-link">Fig 12</a> is attributed to the FAIR1M open-source database and is available from the FAIR1M database (URL <strong>(s)</strong>: <a href="https://gaofen-challenge.com/benchmark" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://gaofen-challenge.com/benchmark</a>).</p></figcaption></figure><p>In the ship detection and fine-grained recognition comparison experiments, the M-ReDet and RoI Transformer, SASM, ReDet, R<sup>3</sup>Det, Faster RCNN, Rotated RetinaNet, GWD, S<sup>2</sup>ANet, KFIoU, LSKNet [<a href="#pone.0330485.ref037" class="usa-link" aria-describedby="pone.0330485.ref037">37</a>] algorithms use the same experimental hyper-parameters and the remote sensing ship images used for training are cropped to 1024 × 1024 size. The mAP<sub>0.5</sub> of M-ReDet is 43.29%, which is higher than the above algorithms by 7.85%, 13.71%, 2.78%, 13.46%, 12.11%, 18.61%, 13.57%, 9.59%, 7.7% and 3.17%, respectively. M-ReDet has the highest AP-max and AP-min, which reflects from the side that the algorithm in this paper uses the SOPM and SFRM modules to enhance the size of the receptive field so that the M-ReDet can extract more contextual information, which can enhance the detection accuracy of the ship objects of all size of ships. <a href="#pone.0330485.t006" class="usa-link">Table 6</a> shows the detection results of various algorithms on the FAIR1M(ship) dataset.</p>
<section class="tw xbox font-sm" id="pone.0330485.t006"><h5 class="obj_head">Table 6. Comparison experiment results on the FAIR1M(ship) dataset.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Algorithm</th>
<th align="left" rowspan="1" colspan="1">Size</th>
<th align="left" rowspan="1" colspan="1">mAP<sub>0.5</sub>(%)</th>
<th align="left" rowspan="1" colspan="1">AP<sub>max</sub>(%)</th>
<th align="left" rowspan="1" colspan="1">AP<sub>min</sub>(%)</th>
<th align="left" rowspan="1" colspan="1">Recall<sub>max</sub>(%)</th>
<th align="left" rowspan="1" colspan="1">Recall<sub>min</sub>(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">RoI Transformer</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">35.44</td>
<td align="left" rowspan="1" colspan="1">60.00</td>
<td align="left" rowspan="1" colspan="1">10.10</td>
<td align="left" rowspan="1" colspan="1">76.10</td>
<td align="left" rowspan="1" colspan="1">53.10</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">SASM</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">29.58</td>
<td align="left" rowspan="1" colspan="1">54.10</td>
<td align="left" rowspan="1" colspan="1">6.80</td>
<td align="left" rowspan="1" colspan="1">94.10</td>
<td align="left" rowspan="1" colspan="1">61.70</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ReDet</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">40.51</td>
<td align="left" rowspan="1" colspan="1">66.30</td>
<td align="left" rowspan="1" colspan="1">10.80</td>
<td align="left" rowspan="1" colspan="1">80.50</td>
<td align="left" rowspan="1" colspan="1">54.30</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">R<sup>3</sup>Det</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">29.83</td>
<td align="left" rowspan="1" colspan="1">55.40</td>
<td align="left" rowspan="1" colspan="1">7.20</td>
<td align="left" rowspan="1" colspan="1">88.00</td>
<td align="left" rowspan="1" colspan="1">50.50</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Faster RCNN</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">31.18</td>
<td align="left" rowspan="1" colspan="1">54.20</td>
<td align="left" rowspan="1" colspan="1">13.40</td>
<td align="left" rowspan="1" colspan="1">75.70</td>
<td align="left" rowspan="1" colspan="1">47.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Rotated RetinaNet</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">24.68</td>
<td align="left" rowspan="1" colspan="1">44.30</td>
<td align="left" rowspan="1" colspan="1">11.00</td>
<td align="left" rowspan="1" colspan="1">89.10</td>
<td align="left" rowspan="1" colspan="1">56.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GWD</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">29.72</td>
<td align="left" rowspan="1" colspan="1">53.50</td>
<td align="left" rowspan="1" colspan="1">6.80</td>
<td align="left" rowspan="1" colspan="1">85.90</td>
<td align="left" rowspan="1" colspan="1">48.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">S<sub>2</sub>ANet.</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">33.70</td>
<td align="left" rowspan="1" colspan="1">60.10</td>
<td align="left" rowspan="1" colspan="1">7.10</td>
<td align="left" rowspan="1" colspan="1">91.60</td>
<td align="left" rowspan="1" colspan="1">56.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">KFIoU</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">35.59</td>
<td align="left" rowspan="1" colspan="1">60.90</td>
<td align="left" rowspan="1" colspan="1">10.60</td>
<td align="left" rowspan="1" colspan="1">76.80</td>
<td align="left" rowspan="1" colspan="1">50.90</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LSKNet</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">40.12</td>
<td align="left" rowspan="1" colspan="1">66.90</td>
<td align="left" rowspan="1" colspan="1">8.50</td>
<td align="left" rowspan="1" colspan="1">87.40</td>
<td align="left" rowspan="1" colspan="1">64.80</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">M-ReDet</td>
<td align="left" rowspan="1" colspan="1">1024 × 1024</td>
<td align="left" rowspan="1" colspan="1">43.29</td>
<td align="left" rowspan="1" colspan="1">71.20</td>
<td align="left" rowspan="1" colspan="1">15.90</td>
<td align="left" rowspan="1" colspan="1">90.10</td>
<td align="left" rowspan="1" colspan="1">69.60</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330485.t006/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>There are nine categories of ship objects in the FAIR1M(ship) dataset. The detection and fine-grained recognition accuracy of ships is proportional to the number of instances of this category, except that there are many ships in the category of “other-ships” that do not have subdivided categories or the category determination information is ambiguous, which results in their detection and fine-grained recognition accuracy of only 15.9%. The M-ReDet algorithm has the highest detection accuracy for “Dry-Cargo-Ship,” with an AP of 71.20%. The “Motorboat” has 7921 instances, but because of its small size, the overall AP is lower than “Dry-Cargo-Ship” at 64.7%. <a href="#pone.0330485.t007" class="usa-link">Table 7</a> shows the training results for each of the nine categories of ships.</p>
<section class="tw xbox font-sm" id="pone.0330485.t007"><h5 class="obj_head">Table 7. Training results for various types of ships.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Class</th>
<th align="left" rowspan="1" colspan="1">Gts</th>
<th align="left" rowspan="1" colspan="1">Dets</th>
<th align="left" rowspan="1" colspan="1">Recall(%)</th>
<th align="left" rowspan="1" colspan="1">AP(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Dry-Cargo-Ship</td>
<td align="left" rowspan="1" colspan="1">8302</td>
<td align="left" rowspan="1" colspan="1">23204</td>
<td align="left" rowspan="1" colspan="1">90.10</td>
<td align="left" rowspan="1" colspan="1">71.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Engineering-Ship</td>
<td align="left" rowspan="1" colspan="1">2604</td>
<td align="left" rowspan="1" colspan="1">7775</td>
<td align="left" rowspan="1" colspan="1">76.70</td>
<td align="left" rowspan="1" colspan="1">49.60</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Fishing-Boat</td>
<td align="left" rowspan="1" colspan="1">4084</td>
<td align="left" rowspan="1" colspan="1">20058</td>
<td align="left" rowspan="1" colspan="1">77.40</td>
<td align="left" rowspan="1" colspan="1">43.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Motorboat</td>
<td align="left" rowspan="1" colspan="1">7921</td>
<td align="left" rowspan="1" colspan="1">20769</td>
<td align="left" rowspan="1" colspan="1">85.00</td>
<td align="left" rowspan="1" colspan="1">64.70</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Liquid-Cargo-Ship</td>
<td align="left" rowspan="1" colspan="1">1021</td>
<td align="left" rowspan="1" colspan="1">8704</td>
<td align="left" rowspan="1" colspan="1">69.60</td>
<td align="left" rowspan="1" colspan="1">42.40</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Passenger-Ship</td>
<td align="left" rowspan="1" colspan="1">1391</td>
<td align="left" rowspan="1" colspan="1">6337</td>
<td align="left" rowspan="1" colspan="1">76.30</td>
<td align="left" rowspan="1" colspan="1">33.00</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Tugboat</td>
<td align="left" rowspan="1" colspan="1">460</td>
<td align="left" rowspan="1" colspan="1">5953</td>
<td align="left" rowspan="1" colspan="1">88.00</td>
<td align="left" rowspan="1" colspan="1">30.30</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">W-ship</td>
<td align="left" rowspan="1" colspan="1">597</td>
<td align="left" rowspan="1" colspan="1">4702</td>
<td align="left" rowspan="1" colspan="1">87.60</td>
<td align="left" rowspan="1" colspan="1">39.40</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">other-ship</td>
<td align="left" rowspan="1" colspan="1">2143</td>
<td align="left" rowspan="1" colspan="1">20766</td>
<td align="left" rowspan="1" colspan="1">73.60</td>
<td align="left" rowspan="1" colspan="1">15.90</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330485.t007/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p><a href="#pone.0330485.t008" class="usa-link">Table 8</a> demonstrates the AP values of 9 types of ships detected by the 11 algorithms, respectively, to show more clearly the impact of the M-ReDet algorithm on the detection and fine-grained recognition of various types of ships. As seen from the table, the M-ReDet algorithm achieves the optimal AP in the detection results of 7 classes of ships. It is lower than the LSKNet and ReDet only in Passenger-Ship and W-ship detection results.</p>
<section class="tw xbox font-sm" id="pone.0330485.t008"><h5 class="obj_head">Table 8. Detection and fine-grained recognition results for 9 types of ships.</h5>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Algorithm</th>
<th align="left" rowspan="1" colspan="1">SASM</th>
<th align="left" rowspan="1" colspan="1">RoI Transformer</th>
<th align="left" rowspan="1" colspan="1">R<sup>3</sup>Det</th>
<th align="left" rowspan="1" colspan="1">Faster RCNN</th>
<th align="left" rowspan="1" colspan="1">Rotated RetinaNet</th>
<th align="left" rowspan="1" colspan="1">GWD</th>
<th align="left" rowspan="1" colspan="1">S<sup>2</sup>ANet.</th>
<th align="left" rowspan="1" colspan="1">KFIoU</th>
<th align="left" rowspan="1" colspan="1">LSKNet</th>
<th align="left" rowspan="1" colspan="1">ReDet</th>
<th align="left" rowspan="1" colspan="1">M-ReDet</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Dry-Cargo-Ship</td>
<td align="left" rowspan="1" colspan="1">54.10</td>
<td align="left" rowspan="1" colspan="1">60.00</td>
<td align="left" rowspan="1" colspan="1">55.40</td>
<td align="left" rowspan="1" colspan="1">54.20</td>
<td align="left" rowspan="1" colspan="1">44.30</td>
<td align="left" rowspan="1" colspan="1">53.50</td>
<td align="left" rowspan="1" colspan="1">60.10</td>
<td align="left" rowspan="1" colspan="1">60.90</td>
<td align="left" rowspan="1" colspan="1">66.90</td>
<td align="left" rowspan="1" colspan="1">66.30</td>
<td align="left" rowspan="1" colspan="1">71.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Engineering-Ship</td>
<td align="left" rowspan="1" colspan="1">31.70</td>
<td align="left" rowspan="1" colspan="1">40.60</td>
<td align="left" rowspan="1" colspan="1">35.00</td>
<td align="left" rowspan="1" colspan="1">30.10</td>
<td align="left" rowspan="1" colspan="1">36.50</td>
<td align="left" rowspan="1" colspan="1">34.40</td>
<td align="left" rowspan="1" colspan="1">34.10</td>
<td align="left" rowspan="1" colspan="1">36.90</td>
<td align="left" rowspan="1" colspan="1">42.00</td>
<td align="left" rowspan="1" colspan="1">46.40</td>
<td align="left" rowspan="1" colspan="1">49.60</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Fishing-Boat</td>
<td align="left" rowspan="1" colspan="1">21.40</td>
<td align="left" rowspan="1" colspan="1">28.70</td>
<td align="left" rowspan="1" colspan="1">22.30</td>
<td align="left" rowspan="1" colspan="1">19.00</td>
<td align="left" rowspan="1" colspan="1">21.60</td>
<td align="left" rowspan="1" colspan="1">20.60</td>
<td align="left" rowspan="1" colspan="1">27.70</td>
<td align="left" rowspan="1" colspan="1">29.30</td>
<td align="left" rowspan="1" colspan="1">39.40</td>
<td align="left" rowspan="1" colspan="1">38.20</td>
<td align="left" rowspan="1" colspan="1">43.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Motorboat</td>
<td align="left" rowspan="1" colspan="1">42.80</td>
<td align="left" rowspan="1" colspan="1">54.10</td>
<td align="left" rowspan="1" colspan="1">38.30</td>
<td align="left" rowspan="1" colspan="1">46.40</td>
<td align="left" rowspan="1" colspan="1">32.00</td>
<td align="left" rowspan="1" colspan="1">36.40</td>
<td align="left" rowspan="1" colspan="1">44.00</td>
<td align="left" rowspan="1" colspan="1">55.20</td>
<td align="left" rowspan="1" colspan="1">60.20</td>
<td align="left" rowspan="1" colspan="1">56.80</td>
<td align="left" rowspan="1" colspan="1">64.70</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Liquid-Cargo-Ship</td>
<td align="left" rowspan="1" colspan="1">34.00</td>
<td align="left" rowspan="1" colspan="1">34.00</td>
<td align="left" rowspan="1" colspan="1">31.30</td>
<td align="left" rowspan="1" colspan="1">30.50</td>
<td align="left" rowspan="1" colspan="1">17.60</td>
<td align="left" rowspan="1" colspan="1">33.10</td>
<td align="left" rowspan="1" colspan="1">35.60</td>
<td align="left" rowspan="1" colspan="1">34.60</td>
<td align="left" rowspan="1" colspan="1">39.60</td>
<td align="left" rowspan="1" colspan="1">37.80</td>
<td align="left" rowspan="1" colspan="1">42.40</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Passenger-Ship</td>
<td align="left" rowspan="1" colspan="1">23.10</td>
<td align="left" rowspan="1" colspan="1">25.10</td>
<td align="left" rowspan="1" colspan="1">26.80</td>
<td align="left" rowspan="1" colspan="1">28.60</td>
<td align="left" rowspan="1" colspan="1">20.30</td>
<td align="left" rowspan="1" colspan="1">24.30</td>
<td align="left" rowspan="1" colspan="1">27.10</td>
<td align="left" rowspan="1" colspan="1">29.70</td>
<td align="left" rowspan="1" colspan="1">35.90</td>
<td align="left" rowspan="1" colspan="1">34.90</td>
<td align="left" rowspan="1" colspan="1">33.00</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Tugboat</td>
<td align="left" rowspan="1" colspan="1">21.60</td>
<td align="left" rowspan="1" colspan="1">27.50</td>
<td align="left" rowspan="1" colspan="1">25.80</td>
<td align="left" rowspan="1" colspan="1">27.70</td>
<td align="left" rowspan="1" colspan="1">25.30</td>
<td align="left" rowspan="1" colspan="1">25.90</td>
<td align="left" rowspan="1" colspan="1">27.30</td>
<td align="left" rowspan="1" colspan="1">28.70</td>
<td align="left" rowspan="1" colspan="1">26.40</td>
<td align="left" rowspan="1" colspan="1">28.30</td>
<td align="left" rowspan="1" colspan="1">30.30</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">W-ship</td>
<td align="left" rowspan="1" colspan="1">30.80</td>
<td align="left" rowspan="1" colspan="1">38.90</td>
<td align="left" rowspan="1" colspan="1">26.40</td>
<td align="left" rowspan="1" colspan="1">30.60</td>
<td align="left" rowspan="1" colspan="1">11.00</td>
<td align="left" rowspan="1" colspan="1">32.50</td>
<td align="left" rowspan="1" colspan="1">40.30</td>
<td align="left" rowspan="1" colspan="1">34.40</td>
<td align="left" rowspan="1" colspan="1">42.10</td>
<td align="left" rowspan="1" colspan="1">45.20</td>
<td align="left" rowspan="1" colspan="1">39.40</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">other-ship</td>
<td align="left" rowspan="1" colspan="1">6.80</td>
<td align="left" rowspan="1" colspan="1">10.10</td>
<td align="left" rowspan="1" colspan="1">7.20</td>
<td align="left" rowspan="1" colspan="1">13.40</td>
<td align="left" rowspan="1" colspan="1">13.50</td>
<td align="left" rowspan="1" colspan="1">6.80</td>
<td align="left" rowspan="1" colspan="1">7.10</td>
<td align="left" rowspan="1" colspan="1">10.60</td>
<td align="left" rowspan="1" colspan="1">8.50</td>
<td align="left" rowspan="1" colspan="1">10.80</td>
<td align="left" rowspan="1" colspan="1">15.90</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">mAP(%)</td>
<td align="left" rowspan="1" colspan="1">29.58</td>
<td align="left" rowspan="1" colspan="1">35.44</td>
<td align="left" rowspan="1" colspan="1">29.83</td>
<td align="left" rowspan="1" colspan="1">31.18</td>
<td align="left" rowspan="1" colspan="1">24.68</td>
<td align="left" rowspan="1" colspan="1">29.72</td>
<td align="left" rowspan="1" colspan="1">33.70</td>
<td align="left" rowspan="1" colspan="1">35.59</td>
<td align="left" rowspan="1" colspan="1">40.12</td>
<td align="left" rowspan="1" colspan="1">40.51</td>
<td align="left" rowspan="1" colspan="1">43.29</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330485.t008/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>After 15 epochs of training, the mAP<sub>0.5</sub> of the M-ReDet algorithm is finally stabilized near 43.29%; because of the addition of M-Bottleneck, stabilizing the internal parameters of the module requires more training epochs of updating, and its convergence speed is slightly slower compared to the other algorithms. However, it has the highest accuracy of ship detection and fine-grained recognition, which is shown in <a href="#pone.0330485.g013" class="usa-link">Fig 13</a> for the comparison of the M-ReDet algorithm with the other 10 algorithms. <a href="#pone.0330485.g013" class="usa-link">Fig 13</a> shows the training results of all algorithms.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g013"><h5 class="obj_head">Fig 13. Comparison of training results on the FAIR1M(ship) dataset.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g013.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/763c99784778/pone.0330485.g013.jpg" loading="lazy" height="548" width="686" alt="Fig 13"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g013/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></figure></section></section><section id="sec018"><h3 class="pmc_sec_title">Ablation experiments</h3>
<p>The M-ReDet mainly consists of the M-ReResNet50, M-ReFPN and detection head, where the SFRM adds the M-Bottleneck and initializes it as configured in subsection 3.2. The SFRM and the FPN constitute the M-ReFPN module together. <a href="#pone.0330485.t009" class="usa-link">Table 9</a> shows the results of the ablation experiments of M-ReDet, which mainly investigate the effects of the SOPM, SFRM, KFIoU and Focal Loss alone or both or together on the mAP<sub>0.5</sub> of the M-ReDet algorithm, and the results of the ablation experiments are as follows.</p>
<section class="tw xbox font-sm" id="pone.0330485.t009"><h4 class="obj_head">Table 9. Ablation experiments of the M-ReDet.</h4>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<colgroup span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
<col align="left" valign="middle" span="1">
</colgroup>
<thead><tr>
<th align="left" rowspan="1" colspan="1">Algorithm</th>
<th align="left" rowspan="1" colspan="1">SOPM</th>
<th align="left" rowspan="1" colspan="1">SFRM</th>
<th align="left" rowspan="1" colspan="1">KFIoU+Focal</th>
<th align="left" rowspan="1" colspan="1">mAP<sub>0.5</sub>(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">ReDet</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">40.51</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ReDet+SOPM</td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">41.52</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ReDet+SFRM</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">41.45</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ReDet + KFIoU+Focal</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1">41.39</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ReDet+SOPM+KFIoU+Focal</td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1">42.25</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ReDet+SOPM+SFRM</td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">42.60</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ReDet+SFRM+KFIoU+Focal</td>
<td align="left" rowspan="1" colspan="1"></td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1">42.00</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">M-ReDet(ours)</td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1">√</td>
<td align="left" rowspan="1" colspan="1">43.29</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/pone.0330485.t009/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p>It can be seen from <a href="#pone.0330485.t009" class="usa-link">Table 9</a> that using each module alone can improve the mAP<sub>0.5</sub> of the M-ReDet algorithm; the mAP<sub>0.5</sub> are 41.52%, 41.45%, and 41.39%, respectively, which are improved by 1.01%, 0.94%, and 0.88% compared to the baseline model. Improvement modules used two by two can also improve the mAP<sub>0.5</sub> of the algorithm. Finally, M-ReDet using SOPM, SFRM, KFIoU, and Focal Loss at the same time achieves the optimal ship detection and fine-grained recognition accuracy, with a mAP0.5 of 43.29%, which is 2.78% higher than that of the baseline model ReDet, and <a href="#pone.0330485.g014" class="usa-link">Fig 14</a> shows the difference between the ReDet and the M-ReDet on the FAIRM (ship) dataset for ship detection and fine-grained results.</p>
<figure class="fig xbox font-sm" id="pone.0330485.g014"><h4 class="obj_head">Fig 14. Ship detection and fine-grained results.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370123_pone.0330485.g014.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/c407/12370123/18785f232b5f/pone.0330485.g014.jpg" loading="lazy" height="988" width="698" alt="Fig 14"></a></p>
<div class="p text-right font-secondary"><a href="figure/pone.0330485.g014/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>(a)</strong>Detection and fine-grained results of the ReDet; <strong>(b)</strong>Detection and fine-grained results of the M-ReDet. <a href="#pone.0330485.g014" class="usa-link">Fig 14</a> is attributed to the FAIR1M open-source database and is available from the FAIR1M database (URL (s): <a href="https://gaofen-challenge.com/benchmark" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://gaofen-challenge.com/benchmark</a>).</p></figcaption></figure><p>From <a href="#pone.0330485.g014" class="usa-link">Fig 14</a>, it can be found that the SOPM module in the M-ReDet algorithm expands the receptive field, which enables the algorithm to extract more contextual information, such as the sea surface and ports. With that information, the M-ReDet algorithm can detect the remote sensing ship objects that are in the edge position or obscured; for example, in the first line of the remote sensing image, M-ReDet successfully detects the incomplete ship objects that are on the top, right and bottom left, respectively. Moreover, the SOPM and SFRM modules can selectively memorize the fine-grained features of remote sensing ship objects of different sizes to minimize the loss of features of small ship objects in the downsampling process and to improve the algorithm’s ability to detect small ships, for example, the first image in the second row, M-ReDet detects a tiny “other-ship” object located in the center of the image, but its size is too small, and the classification confidence is low. In addition, the enlarged receptive field of M-ReDet can also avoid the false detection of large buildings such as bridges and dock berths; for example, in the second row, ReDet recognizes the dock berth in the second image and the bridge in the third image as a ship.</p></section></section><section id="sec019"><h2 class="pmc_sec_title">Discussion</h2>
<p>The main innovation of this paper is the design of the M-ReDet algorithm framework. The proposal of two improved modules, SOPM and SFRM, which form a new backbone network (M-ReResNet50) and a new Neck (M-ReFPN), which enable the algorithm to selectively learn and retain fine-grained information about objects of different sizes of ships, and to fuse the contextual information required for memorization, which improves the algorithm’s detection and fine-grained recognition ability for ships of different size. After several sets of comparison and ablation experiments, the detection robustness of the M-ReDet algorithm designed in this paper is optimal.</p>
<p>Small ship feature information is easily lost in the downsampling process, resulting in the loss of detection and fine-grained recognition accuracy. The M-Bottleneck in the SOPM module can extract and memorize the fine-grained features of small ships and send them to the subsequent layers for further feature extraction. In the SFRM module, the feature maps of different layers can also realize the complementary selective information so that the information on ships of different sizes can be better retained. The SOPM module can also expand the receptive field and cooperate with the SFRM module to supplement the contextual information required by different scale feature maps to reduce the probability of misdetection of similar ship objects. The optimization of classification loss and regression loss is also essential for the algorithm training process. However, this paper selects only four commonly used loss functions for permutation and combination to try the optimal loss function scheme. There are no further attempts to compare and analyze the recent excellent loss functions. Due to the limitation of hardware memory, this paper also did not further improve the number of configurations of M-Bottleneck in the backbone network to optimize the module structure and training hyperparameters of M-Bottleneck based on the experimental training results. In the future, we will make improvements to the structure and configuration of M-Bottleneck in subsection 3.2, continue to optimize the training loss function of the algorithm, and try to incorporate the fine-grained feature information into the loss function for the ship detection and fine-grained recognition tasks further to improve the regression and classification accuracy of the algorithm. In addition, this study can be integrated with other remote sensing data in the future, allowing for the design of multimodal remote sensing image fusion modules that combine visible light, SAR, and infrared data to enhance the algorithm’s remote sensing ship detection capability in harsh weather conditions, such as cloudy and foggy scenes; other fields can also utilize this work, such as agricultural object detection [<a href="#pone.0330485.ref038" class="usa-link" aria-describedby="pone.0330485.ref038">38</a>–<a href="#pone.0330485.ref039" class="usa-link" aria-describedby="pone.0330485.ref039">39</a>].</p></section><section id="sec020"><h2 class="pmc_sec_title">Conclusion</h2>
<p>Remote sensing ship detection and fine-grained recognition face significant challenges due to high inter-class similarity in aspect ratios, ambiguous appearance features among vessel categories, arbitrary orientation variations, and multi-scale object characteristics. This paper proposes the M-ReDet, a memory-augmented ship perception network with feature refinement mechanisms, to address these issues. The optimization of the loss function of M-ReDet further improves the classification and regression accuracy of the algorithm. The comparative and ablation experiments on the FAIRM(ship) and DOTA datasets prove the effectiveness of this paper’s improved modules. Finally, in the remote sensing ship and fine-grained recognition task, the M-ReDet’s mAP<sub>0.5</sub> is 43.29%, validating the effectiveness of our algorithm in complex maritime scenarios.</p></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgments</h2>
<p>The author would like to express thanks to anonymous reviewers for all careful review of the paper and kind suggestions made to improve overall quality of the manuscript.</p></section><section id="notes1"><h2 class="pmc_sec_title">Data Availability</h2>
<p>The curated data and code can be accessed at the following link: <a href="https://github.com/LG973641114/M-ReDet/releases" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/LG973641114/M-ReDet/releases</a>.</p></section><section id="funding-statement1" lang="en"><h2 class="pmc_sec_title">Funding Statement</h2>
<p>This work is supported by the Department of Science and Technology of Jilin Province, China [YDZJ202501ZYTS600]. There was no additional external funding received for this study. Funded studies: Initials of the author who received the award: Liu Xuhui Grant numbers awarded to the author: YDZJ202501ZYTS600 Full name of the funder: Jilin Provincial Department of Science and Technology URL of the funder website: <a href="http://kjt.jl.gov.cn/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://kjt.jl.gov.cn/</a>. The sponsors (Jilin Provincial Department of Science and Technology) provided financial support throughthegrant YDZJ202501ZYTS600, managed by Prof. Wang Jia. However, they did not participate in the study design, data collection and analysis, decision to publish, or preparation of the manuscript. Although Prof. Wang Jia oversawthe funding allocation and supported the research infrastructure, her contributions were limited to administrative andfinancial management, and she is not listed as an author in this paper.</p></section><section id="ref-list1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="ref-list1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="pone.0330485.ref001">
<span class="label">1.</span><cite>Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, G o m e z A. Attention is all you need. Adv Neural Inform Process Syst. 2017;30.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inform%20Process%20Syst&amp;title=Attention%20is%20all%20you%20need&amp;author=A%20Vaswani&amp;author=N%20Shazeer&amp;author=N%20Parmar&amp;author=J%20Uszkoreit&amp;author=L%20Jones&amp;volume=30&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref002">
<span class="label">2.</span><cite>Hu J, Cao A, Feng Z, Zhang S, Wang Y, Jia L, et al. Vision mamba mender. Adv Neural Inform Process Syst. 2024;37:51905–29.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inform%20Process%20Syst&amp;title=Vision%20mamba%20mender&amp;author=J%20Hu&amp;author=A%20Cao&amp;author=Z%20Feng&amp;author=S%20Zhang&amp;author=Y%20Wang&amp;volume=37&amp;publication_year=2024&amp;pages=51905-29&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref003">
<span class="label">3.</span><cite>Redmon J, Divvala S, Girshick R, Farhadi A. You only look once: unified, real-time object detection. In: 2016 IEEE Conference on computer vision and pattern recognition (CVPR). 2016. 779–88. doi: 10.1109/cvpr.2016.91</cite> [<a href="https://doi.org/10.1109/cvpr.2016.91" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=2016%20IEEE%20Conference%20on%20computer%20vision%20and%20pattern%20recognition%20(CVPR)&amp;title=You%20only%20look%20once:%20unified,%20real-time%20object%20detection.&amp;author=J%20Redmon&amp;author=S%20Divvala&amp;author=R%20Girshick&amp;author=A%20Farhadi&amp;publication_year=2016&amp;pages=779-88&amp;doi=10.1109/cvpr.2016.91&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref004">
<span class="label">4.</span><cite>Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu C, et al. Ssd: Single shot multibox detector. In: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I. Amsterdam, The Netherlands; 2016. 21–37.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Computer%20Vision%E2%80%93ECCV%202016:%2014th%20European%20Conference,%20Amsterdam,%20The%20Netherlands,%20October%2011%E2%80%9314,%202016,%20Proceedings,%20Part%20I&amp;author=W%20Liu&amp;author=D%20Anguelov&amp;author=D%20Erhan&amp;author=C%20Szegedy&amp;author=S%20Reed&amp;publication_year=2016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref005">
<span class="label">5.</span><cite>Ren S, He K, Girshick R, Sun J. Faster R-CNN: towards real-time object detection with region proposal networks. IEEE Trans Pattern Anal Mach Intell. 2017;39(6):1137–49. doi: 10.1109/TPAMI.2016.2577031

</cite> [<a href="https://doi.org/10.1109/TPAMI.2016.2577031" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/27295650/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;title=Faster%20R-CNN:%20towards%20real-time%20object%20detection%20with%20region%20proposal%20networks&amp;author=S%20Ren&amp;author=K%20He&amp;author=R%20Girshick&amp;author=J%20Sun&amp;volume=39&amp;issue=6&amp;publication_year=2017&amp;pages=1137-49&amp;pmid=27295650&amp;doi=10.1109/TPAMI.2016.2577031&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref006">
<span class="label">6.</span><cite>He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. 770–8.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20IEEE%20conference%20on%20computer%20vision%20and%20pattern%20recognition&amp;author=K%20He&amp;author=X%20Zhang&amp;author=S%20Ren&amp;author=J%20Sun&amp;publication_year=2016&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref007">
<span class="label">7.</span><cite>Yang X, Yan J, Ming Q, Wang W, Zhang X, Tian Q. Rethinking rotated object detection with gaussian wasserstein distance loss. In: International conference on machine learning. 2021. 11830–41.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=International%20conference%20on%20machine%20learning&amp;author=X%20Yang&amp;author=J%20Yan&amp;author=Q%20Ming&amp;author=W%20Wang&amp;author=X%20Zhang&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref008">
<span class="label">8.</span><cite>Yang X, Yan J, Feng Z, He T. R3Det: refined single-stage detector with feature refinement for rotating object. In: Proceedings of the AAAI conference on artificial intelligence. 2021;35(4):3163–71. doi: 10.1609/aaai.v35i4.16426</cite> [<a href="https://doi.org/10.1609/aaai.v35i4.16426" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Proceedings%20of%20the%20AAAI%20conference%20on%20artificial%20intelligence&amp;title=R3Det:%20refined%20single-stage%20detector%20with%20feature%20refinement%20for%20rotating%20object.&amp;author=X%20Yang&amp;author=J%20Yan&amp;author=Z%20Feng&amp;author=T%20He&amp;volume=35&amp;issue=4&amp;publication_year=2021&amp;pages=3163-71&amp;doi=10.1609/aaai.v35i4.16426&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref009">
<span class="label">9.</span><cite>Han J, Ding J, Xue N, Xia G. Redet: a rotation-equivariant detector for aerial object detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021. 2786–95.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20IEEE/CVF%20conference%20on%20computer%20vision%20and%20pattern%20recognition&amp;author=J%20Han&amp;author=J%20Ding&amp;author=N%20Xue&amp;author=G%20Xia&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref010">
<span class="label">10.</span><cite>Hou L, Lu K, Xue J, Li Y. Shape-adaptive selection and measurement for oriented object detection. In: Proceedings of the AAAI conference on artificial intelligence, 2022. 923–32.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20AAAI%20conference%20on%20artificial%20intelligence&amp;author=L%20Hou&amp;author=K%20Lu&amp;author=J%20Xue&amp;author=Y%20Li&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref011">
<span class="label">11.</span><cite>Li Q, Chen Y, Zeng Y. Transformer with transfer CNN for remote-sensing-image object detection. Remote Sensing. 2022;14(4):984. doi: 10.3390/rs14040984</cite> [<a href="https://doi.org/10.3390/rs14040984" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Remote%20Sensing&amp;title=Transformer%20with%20transfer%20CNN%20for%20remote-sensing-image%20object%20detection&amp;author=Q%20Li&amp;author=Y%20Chen&amp;author=Y%20Zeng&amp;volume=14&amp;issue=4&amp;publication_year=2022&amp;pages=984&amp;doi=10.3390/rs14040984&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref012">
<span class="label">12.</span><cite>Liu W, Lin Y, Liu W, Yu Y, Li J. An attention-based multiscale transformer network for remote sensing image change detection. ISPRS J Photogram Remote Sens. 2023;202:599–609.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=ISPRS%20J%20Photogram%20Remote%20Sens&amp;title=An%20attention-based%20multiscale%20transformer%20network%20for%20remote%20sensing%20image%20change%20detection&amp;author=W%20Liu&amp;author=Y%20Lin&amp;author=W%20Liu&amp;author=Y%20Yu&amp;author=J%20Li&amp;volume=202&amp;publication_year=2023&amp;pages=599-609&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref013">
<span class="label">13.</span><cite>Yang M, Xu R, Yang C, Wu H, Wang A. Hybrid-DETR: a differentiated module-based model for object detection in remote sensing images. Electronics. 2024;13(24):5014. doi: 10.3390/electronics13245014</cite> [<a href="https://doi.org/10.3390/electronics13245014" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Electronics&amp;title=Hybrid-DETR:%20a%20differentiated%20module-based%20model%20for%20object%20detection%20in%20remote%20sensing%20images&amp;author=M%20Yang&amp;author=R%20Xu&amp;author=C%20Yang&amp;author=H%20Wu&amp;author=A%20Wang&amp;volume=13&amp;issue=24&amp;publication_year=2024&amp;pages=5014&amp;doi=10.3390/electronics13245014&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref014">
<span class="label">14.</span><cite>Liu Y, Tian Y, Zhao Y, Yu H, Xie L, Wang Y. Vmamba: visual state space model. Adv Neural Inform Process Syst. 2024;37:103031–63.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=Adv%20Neural%20Inform%20Process%20Syst&amp;title=Vmamba:%20visual%20state%20space%20model&amp;author=Y%20Liu&amp;author=Y%20Tian&amp;author=Y%20Zhao&amp;author=H%20Yu&amp;author=L%20Xie&amp;volume=37&amp;publication_year=2024&amp;pages=103031-63&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref015">
<span class="label">15.</span><cite>Zhan Y, Zeng Z, Liu H, Tan X, Tian Y. MambaSOD: dual mamba-driven cross-modal fusion network for RGB-D salient object detection. Neurocomputing. 2025;631:129718. doi: 10.1016/j.neucom.2025.129718</cite> [<a href="https://doi.org/10.1016/j.neucom.2025.129718" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Neurocomputing&amp;title=MambaSOD:%20dual%20mamba-driven%20cross-modal%20fusion%20network%20for%20RGB-D%20salient%20object%20detection&amp;author=Y%20Zhan&amp;author=Z%20Zeng&amp;author=H%20Liu&amp;author=X%20Tan&amp;author=Y%20Tian&amp;volume=631&amp;publication_year=2025&amp;pages=129718&amp;doi=10.1016/j.neucom.2025.129718&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref016">
<span class="label">16.</span><cite>Verma T, Singh J, Bhartari Y, Jarwal R, Singh S, Singh S. SOAR: advancements in small body object detection for aerial imagery using state space models and programmable gradients. arXiv preprint. 2024. doi: 10.48550/arXiv.2405.01699</cite> [<a href="https://doi.org/10.48550/arXiv.2405.01699" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=SOAR:%20advancements%20in%20small%20body%20object%20detection%20for%20aerial%20imagery%20using%20state%20space%20models%20and%20programmable%20gradients&amp;author=T%20Verma&amp;author=J%20Singh&amp;author=Y%20Bhartari&amp;author=R%20Jarwal&amp;author=S%20Singh&amp;publication_year=2024&amp;doi=10.48550/arXiv.2405.01699&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref017">
<span class="label">17.</span><cite>Wang CY, Yeh IH, Liao HY. Yolov9: Learning what you want to learn using programmable gradient information. In: European conference on computer vision. 2024. 1–21.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=European%20conference%20on%20computer%20vision&amp;author=CY%20Wang&amp;author=IH%20Yeh&amp;author=HY%20Liao&amp;publication_year=2024&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref018">
<span class="label">18.</span><cite>Shi H, Yang W, Chen D, Wang M. ASG-YOLOv5: Improved YOLOv5 unmanned aerial vehicle remote sensing aerial images scenario for small object detection based on attention and spatial gating. PLoS One. 2024;19(6):e0298698. doi: 10.1371/journal.pone.0298698

</cite> [<a href="https://doi.org/10.1371/journal.pone.0298698" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11146694/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38829850/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=ASG-YOLOv5:%20Improved%20YOLOv5%20unmanned%20aerial%20vehicle%20remote%20sensing%20aerial%20images%20scenario%20for%20small%20object%20detection%20based%20on%20attention%20and%20spatial%20gating&amp;author=H%20Shi&amp;author=W%20Yang&amp;author=D%20Chen&amp;author=M%20Wang&amp;volume=19&amp;issue=6&amp;publication_year=2024&amp;pmid=38829850&amp;doi=10.1371/journal.pone.0298698&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref019">
<span class="label">19.</span><cite>Wu W, Liu H, Li L, Long Y, Wang X, Wang Z, et al. Application of local fully convolutional neural network combined with YOLO v5 algorithm in small target detection of remote sensing image. PLoS One. 2021;16(10):e0259283. doi: 10.1371/journal.pone.0259283

</cite> [<a href="https://doi.org/10.1371/journal.pone.0259283" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC8555847/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/34714878/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Application%20of%20local%20fully%20convolutional%20neural%20network%20combined%20with%20YOLO%20v5%20algorithm%20in%20small%20target%20detection%20of%20remote%20sensing%20image&amp;author=W%20Wu&amp;author=H%20Liu&amp;author=L%20Li&amp;author=Y%20Long&amp;author=X%20Wang&amp;volume=16&amp;issue=10&amp;publication_year=2021&amp;pmid=34714878&amp;doi=10.1371/journal.pone.0259283&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>] [<a href="/articles/PMC10484450/" class="text-red">Retracted</a>]</li>
<li id="pone.0330485.ref020">
<span class="label">20.</span><cite>Sun X, Wang P, Yan Z, Xu F, Wang R, Diao W, et al. FAIR1M: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery. ISPRS J Photogram Remote Sens. 2022;184:116–30. doi: 10.1016/j.isprsjprs.2021.12.004</cite> [<a href="https://doi.org/10.1016/j.isprsjprs.2021.12.004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=ISPRS%20J%20Photogram%20Remote%20Sens&amp;title=FAIR1M:%20A%20benchmark%20dataset%20for%20fine-grained%20object%20recognition%20in%20high-resolution%20remote%20sensing%20imagery&amp;author=X%20Sun&amp;author=P%20Wang&amp;author=Z%20Yan&amp;author=F%20Xu&amp;author=R%20Wang&amp;volume=184&amp;publication_year=2022&amp;pages=116-30&amp;doi=10.1016/j.isprsjprs.2021.12.004&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref021">
<span class="label">21.</span><cite>Xia GS, Bai X, Ding J, Zhu Z, Belongie S, Luo J. DOTA: a large-scale dataset for object detection in aerial images. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. 3974–83.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20IEEE%20conference%20on%20computer%20vision%20and%20pattern%20recognition&amp;author=GS%20Xia&amp;author=X%20Bai&amp;author=J%20Ding&amp;author=Z%20Zhu&amp;author=S%20Belongie&amp;publication_year=2018&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref022">
<span class="label">22.</span><cite>Han J, Ding J, Xue N, Xia G. Redet: a rotation-equivariant detector for aerial object detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021. 2786–95.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20IEEE/CVF%20conference%20on%20computer%20vision%20and%20pattern%20recognition&amp;author=J%20Han&amp;author=J%20Ding&amp;author=N%20Xue&amp;author=G%20Xia&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref023">
<span class="label">23.</span><cite>Lin TY, Dollár P, Girshick R, He K, Hariharan B, Belongie S. Feature pyramid networks for object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. 2117–25.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20IEEE%20conference%20on%20computer%20vision%20and%20pattern%20recognition&amp;author=TY%20Lin&amp;author=P%20Doll%C3%A1r&amp;author=R%20Girshick&amp;author=K%20He&amp;author=B%20Hariharan&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref024">
<span class="label">24.</span><cite>Ainslie J, Lee-Thorp J, De Jong M, Zemlyanskiy Y, Lebrón F, Sanghai S. Gqa: training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint. 2023. doi: 10.48550/arXiv.2305.13245</cite> [<a href="https://doi.org/10.48550/arXiv.2305.13245" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=Gqa:%20training%20generalized%20multi-query%20transformer%20models%20from%20multi-head%20checkpoints&amp;author=J%20Ainslie&amp;author=J%20Lee-Thorp&amp;author=M%20De%20Jong&amp;author=Y%20Zemlyanskiy&amp;author=F%20Lebr%C3%B3n&amp;publication_year=2023&amp;doi=10.48550/arXiv.2305.13245&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref025">
<span class="label">25.</span><cite>Meng F, Tang P, Tang X, Yao Z, Sun X, Zhang M. Transmla: multi-head latent attention is all you need. arXiv preprint. 2025. doi: 10.48550/arXiv.2502.07864</cite> [<a href="https://doi.org/10.48550/arXiv.2502.07864" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=Transmla:%20multi-head%20latent%20attention%20is%20all%20you%20need&amp;author=F%20Meng&amp;author=P%20Tang&amp;author=X%20Tang&amp;author=Z%20Yao&amp;author=X%20Sun&amp;publication_year=2025&amp;doi=10.48550/arXiv.2502.07864&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref026">
<span class="label">26.</span><cite>Yang A, Yu B, Li C, Liu D, Huang F, Huang H. Qwen2. 5-1M technical report. arXiv. 2025. doi: arXiv:2501.15383</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv&amp;title=Qwen2.%205-1M%20technical%20report&amp;author=A%20Yang&amp;author=B%20Yu&amp;author=C%20Li&amp;author=D%20Liu&amp;author=F%20Huang&amp;publication_year=2025&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref027">
<span class="label">27.</span><cite>Liu A, Feng B, Wang B, Wang B, Liu B, Zhao C. Deepseek-v2: a strong, economical, and efficient mixture-of-experts language model. arXiv preprint. 2024. doi: 10.48550/arXiv.2405.04434</cite> [<a href="https://doi.org/10.48550/arXiv.2405.04434" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=Deepseek-v2:%20a%20strong,%20economical,%20and%20efficient%20mixture-of-experts%20language%20model&amp;author=A%20Liu&amp;author=B%20Feng&amp;author=B%20Wang&amp;author=B%20Wang&amp;author=B%20Liu&amp;publication_year=2024&amp;doi=10.48550/arXiv.2405.04434&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref028">
<span class="label">28.</span><cite>Yang F, Jiang F, Li J, Lu L. MSTrans: multi-scale transformer for building extraction from HR remote sensing images. Electronics. 2024;13(23):4610. doi: 10.3390/electronics13234610</cite> [<a href="https://doi.org/10.3390/electronics13234610" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Electronics&amp;title=MSTrans:%20multi-scale%20transformer%20for%20building%20extraction%20from%20HR%20remote%20sensing%20images&amp;author=F%20Yang&amp;author=F%20Jiang&amp;author=J%20Li&amp;author=L%20Lu&amp;volume=13&amp;issue=23&amp;publication_year=2024&amp;pages=4610&amp;doi=10.3390/electronics13234610&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref029">
<span class="label">29.</span><cite>Mao A, Mohri M, Zhong Y. Cross-entropy loss functions: Theoretical analysis and applications. In: International conference on machine learning. 2023. 23803–28.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=International%20conference%20on%20machine%20learning&amp;author=A%20Mao&amp;author=M%20Mohri&amp;author=Y%20Zhong&amp;publication_year=2023&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref030">
<span class="label">30.</span><cite>Liu C, Yu S, Yu M, Wei B, Li B, Li G. Adaptive smooth L1 loss: A better way to regress scene texts with extreme aspect ratios. In: 2021 IEEE symposium on computers and communications (ISCC). 2021. 1–7.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=2021%20IEEE%20symposium%20on%20computers%20and%20communications%20(ISCC)&amp;author=C%20Liu&amp;author=S%20Yu&amp;author=M%20Yu&amp;author=B%20Wei&amp;author=B%20Li&amp;publication_year=2021&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref031">
<span class="label">31.</span><cite>Yang X, Zhou Y, Zhang G, Yang J, Wang W, Yan J. The KFIoU loss for rotated object detection. arXiv preprint. 2022;2201.12558.</cite> [<a href="https://scholar.google.com/scholar_lookup?journal=arXiv%20preprint&amp;title=The%20KFIoU%20loss%20for%20rotated%20object%20detection&amp;author=X%20Yang&amp;author=Y%20Zhou&amp;author=G%20Zhang&amp;author=J%20Yang&amp;author=W%20Wang&amp;publication_year=2022&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref032">
<span class="label">32.</span><cite>Lin TY, Goyal P, Girshick R, He K, Dollar PF. Focal loss for dense object detection. In: Proceedings of the IEEE International conference on computer vision. 2017. 2980–8.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20IEEE%20International%20conference%20on%20computer%20vision&amp;author=TY%20Lin&amp;author=P%20Goyal&amp;author=R%20Girshick&amp;author=K%20He&amp;author=PF%20Dollar&amp;publication_year=2017&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref033">
<span class="label">33.</span><cite>Ding J, Xue N, Long Y, Xia G, Lu Q. Learning RoI transformer for oriented object detection in aerial images. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019. 2849–58.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20IEEE/CVF%20conference%20on%20computer%20vision%20and%20pattern%20recognition&amp;author=J%20Ding&amp;author=N%20Xue&amp;author=Y%20Long&amp;author=G%20Xia&amp;author=Q%20Lu&amp;publication_year=2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref034">
<span class="label">34.</span><cite>Yang Z, Liu S, Hu H, Wang L, Lin S. Reppoints: Point set representation for object detection. In: Proceedings of the IEEE/CVF International conference on computer vision. 2019. 9657–66.</cite> [<a href="https://scholar.google.com/scholar_lookup?title=Proceedings%20of%20the%20IEEE/CVF%20International%20conference%20on%20computer%20vision&amp;author=Z%20Yang&amp;author=S%20Liu&amp;author=H%20Hu&amp;author=L%20Wang&amp;author=S%20Lin&amp;publication_year=2019&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref035">
<span class="label">35.</span><cite>Han J, Ding J, Li J, Xia G-S. Align deep features for oriented object detection. IEEE Trans Geosci Remote Sensing. 2022;60:1–11. doi: 10.1109/tgrs.2021.3062048</cite> [<a href="https://doi.org/10.1109/tgrs.2021.3062048" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=IEEE%20Trans%20Geosci%20Remote%20Sensing&amp;title=Align%20deep%20features%20for%20oriented%20object%20detection&amp;author=J%20Han&amp;author=J%20Ding&amp;author=J%20Li&amp;author=G-S%20Xia&amp;volume=60&amp;publication_year=2022&amp;pages=1-11&amp;doi=10.1109/tgrs.2021.3062048&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref036">
<span class="label">36.</span><cite>Guan Q, Liu Y, Chen L, Li G, Li Y. A deformable split fusion method for object detection in high-resolution optical remote sensing image. Remote Sensing. 2024;16(23):4487. doi: 10.3390/rs16234487</cite> [<a href="https://doi.org/10.3390/rs16234487" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Remote%20Sensing&amp;title=A%20deformable%20split%20fusion%20method%20for%20object%20detection%20in%20high-resolution%20optical%20remote%20sensing%20image&amp;author=Q%20Guan&amp;author=Y%20Liu&amp;author=L%20Chen&amp;author=G%20Li&amp;author=Y%20Li&amp;volume=16&amp;issue=23&amp;publication_year=2024&amp;pages=4487&amp;doi=10.3390/rs16234487&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref037">
<span class="label">37.</span><cite>Li Y, Li X, Dai Y, Hou Q, Liu L, Liu Y, et al. LSKNet: a foundation lightweight backbone for remote sensing. Int J Comput Vis. 2024;133(3):1410–31. doi: 10.1007/s11263-024-02247-9</cite> [<a href="https://doi.org/10.1007/s11263-024-02247-9" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=Int%20J%20Comput%20Vis&amp;title=LSKNet:%20a%20foundation%20lightweight%20backbone%20for%20remote%20sensing&amp;author=Y%20Li&amp;author=X%20Li&amp;author=Y%20Dai&amp;author=Q%20Hou&amp;author=L%20Liu&amp;volume=133&amp;issue=3&amp;publication_year=2024&amp;pages=1410-31&amp;doi=10.1007/s11263-024-02247-9&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref038">
<span class="label">38.</span><cite>Lu D, Wang Y. MAR-YOLOv9: a multi-dataset object detection method for agricultural fields based on YOLOv9. PLoS One. 2024;19(10):e0307643. doi: 10.1371/journal.pone.0307643

</cite> [<a href="https://doi.org/10.1371/journal.pone.0307643" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11521258/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39471150/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=MAR-YOLOv9:%20a%20multi-dataset%20object%20detection%20method%20for%20agricultural%20fields%20based%20on%20YOLOv9&amp;author=D%20Lu&amp;author=Y%20Wang&amp;volume=19&amp;issue=10&amp;publication_year=2024&amp;pmid=39471150&amp;doi=10.1371/journal.pone.0307643&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="pone.0330485.ref039">
<span class="label">39.</span><cite>Shi M, Zheng D, Wu T, Zhang W, Fu R, Huang K. Small object detection algorithm incorporating swin transformer for tea buds. PLoS One. 2024;19(3):e0299902. doi: 10.1371/journal.pone.0299902

</cite> [<a href="https://doi.org/10.1371/journal.pone.0299902" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10956868/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38512917/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?journal=PLoS%20One&amp;title=Small%20object%20detection%20algorithm%20incorporating%20swin%20transformer%20for%20tea%20buds&amp;author=M%20Shi&amp;author=D%20Zheng&amp;author=T%20Wu&amp;author=W%20Zhang&amp;author=R%20Fu&amp;volume=19&amp;issue=3&amp;publication_year=2024&amp;pmid=38512917&amp;doi=10.1371/journal.pone.0299902&amp;" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section></section><article class="sub-article" id="pone.0330485.r001"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330485.r001" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330485.r001</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 0</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Fatih Uysal</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Fatih Uysal</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link"><span class="name western">Fatih Uysal</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.c" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.c" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.c" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Fatih Uysal</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.c" class="d-panel p" style="display: none">
<div>© 2025 Fatih Uysal</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>4 Jul 2025</em>
</p>
<p>PONE-D-25-19846M-ReDet: A Mamba-based Method for Remote Sensing Ship Object Detection and Fine-grained RecognitionPLOS ONE</p>
<p>Dear Dr. Qin,</p>
<p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
<p><strong>Revise the paper based on referee comments.</strong> Please submit your revised manuscript by Aug 18 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <span>plosone@plos.org</span> . When you're ready to submit your revision, log on to <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.editorialmanager.com/pone/</a> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
<p>Please include the following items when submitting your revised manuscript:</p>
<ul class="list" style="list-style-type:disc">
<li><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></li>
<li><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></li>
<li><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></li>
</ul>
<p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
<p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <a href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</a> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <a href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</a> .</p>
<p>We look forward to receiving your revised manuscript.</p>
<p>Kind regards,</p>
<p>Fatih Uysal, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>
<strong>Journal Requirements:</strong>
</p>
<p>1. When submitting your revision, we need you to address these additional requirements. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at <a href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</a> and <a href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</a> 2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, we expect all author-generated code to be made available without restrictions upon publication of the work. Please review our guidelines at <a href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</a> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse. 3. Thank you for stating in your Funding Statement: This work is supported by the Department of Science and Technology of Jilin Province, China [YDZJ202501ZYTS600].Funded studies:Initials of the author who received the award: Liu XuhuiGrant numbers awarded to the author: YDZJ202501ZYTS600Full name of the funder: Jilin Provincial Department of Science and TechnologyURL of the funder website: <a href="http://kjt.jl.gov.cn/The" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://kjt.jl.gov.cn/The</a> sponsors (Jilin Provincial Department of Science and Technology) provided financial support through the grant YDZJ202501ZYTS600, managed by Prof. Wang Jia. However, they did not participate in the study design, data collection and analysis, decision to publish, or preparation of the manuscript. Although Prof. Wang Jia oversaw the funding allocation and supported the research infrastructure, her contributions were limited to administrative and financial management, and she is not listed as an author in this paper.  Please provide an amended statement that declares *all* the funding or sources of support (whether external or internal to your organization) received during this study, as detailed online in our guide for authors at <a href="http://journals.plos.org/plosone/s/submit-now" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://journals.plos.org/plosone/s/submit-now. </a> Please also include the statement “There was no additional external funding received for this study.” in your updated Funding Statement. Please include your amended Funding Statement within your cover letter. We will change the online submission form on your behalf. 4. Thank you for uploading your study's underlying data set. Unfortunately, the repository you have noted in your Data Availability statement does not qualify as an acceptable data repository according to PLOS's standards. At this time, please upload the minimal data set necessary to replicate your study's findings to a stable, public repository (such as figshare or Dryad) and provide us with the relevant URLs, DOIs, or accession numbers that may be used to access these data. For a list of recommended repositories and additional information on PLOS standards for data deposition, please see <a href="https://journals.plos.org/plosone/s/recommended-repositories" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://journals.plos.org/plosone/s/recommended-repositories. 5</a>. We note that Figures 7, 8, 9, 11, 13a and 13b in your submission contain satellite images which may be copyrighted. All PLOS content is published under the Creative Commons Attribution License (CC BY 4.0), which means that the manuscript, images, and Supporting Information files will be freely available online, and any third party is permitted to access, download, copy, distribute, and use these materials in any way, even commercially, with proper attribution. For these reasons, we cannot publish previously copyrighted maps or satellite images created using proprietary data, such as Google software (Google Maps, Street View, and Earth). For more information, see our copyright guidelines: <a href="http://journals.plos.org/plosone/s/licenses-and-copyright" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://journals.plos.org/plosone/s/licenses-and-copyright. We</a> require you to either present written permission from the copyright holder to publish these figures specifically under the CC BY 4.0 license, or remove the figures from your submission: a. You may seek permission from the original copyright holder of Figures 7, 8, 9, 11, 13a and 13b to publish the content specifically under the CC BY 4.0 license.   We recommend that you contact the original copyright holder with the Content Permission Form (<a href="http://journals.plos.org/plosone/s/file?id=7c09/content-permission-form.pdf" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://journals.plos.org/plosone/s/file?id=7c09/content-permission-form.pdf</a>) and the following text:“I request permission for the open-access journal PLOS ONE to publish XXX under the Creative Commons Attribution License (CCAL) CC BY 4.0 (<a href="http://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>). Please be aware that this license allows unrestricted use and distribution, even commercially, by third parties. Please reply and provide explicit written permission to publish XXX under a CC BY license and complete the attached form.” Please upload the completed Content Permission Form or other proof of granted permissions as an "Other" file with your submission. In the figure caption of the copyrighted figure, please include the following text: “Reprinted from [ref] under a CC BY license, with permission from [name of publisher], original copyright [original copyright year].” b. If you are unable to obtain permission from the original copyright holder to publish these figures under the CC BY 4.0 license or if the copyright holder’s requirements are incompatible with the CC BY 4.0 license, please either i) remove the figure or ii) supply a replacement figure that complies with the CC BY 4.0 license. Please check copyright information on all replacement figures and update the figure caption with source information. If applicable, please specify in the figure caption text when a figure is similar but not identical to the original image and is therefore for illustrative purposes only.The following resources for replacing copyrighted map figures may be helpful: USGS National Map Viewer (public domain): <a href="http://viewer.nationalmap.gov/viewer/The" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://viewer.nationalmap.gov/viewer/The</a> Gateway to Astronaut Photography of Earth (public domain): <a href="http://eol.jsc.nasa.gov/sseop/clickmap/Maps" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://eol.jsc.nasa.gov/sseop/clickmap/Maps</a> at the CIA (public domain): <a href="https://www.cia.gov/library/publications/the-world-factbook/index.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.cia.gov/library/publications/the-world-factbook/index.html</a> and <a href="https://www.cia.gov/library/publications/cia-maps-publications/index.htmlNASA" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.cia.gov/library/publications/cia-maps-publications/index.htmlNASA</a> Earth Observatory (public domain): <a href="http://earthobservatory.nasa.gov/Landsat" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://earthobservatory.nasa.gov/Landsat:</a>
<a href="http://landsat.visibleearth.nasa.gov/USGS" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://landsat.visibleearth.nasa.gov/USGS</a> EROS (Earth Resources Observatory and Science (EROS) Center) (public domain): <a href="http://eros.usgs.gov/#Natural" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://eros.usgs.gov/#Natural</a> Earth (public domain): <a href="http://www.naturalearthdata.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://www.naturalearthdata.com/</a></p>
<p>6. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
<p>
<strong>Additional Editor Comments:</strong>
</p>
<p>Revise the paper based on referee comments.</p>
<p>[Note: HTML markup is below. Please do not edit.]</p>
<p>Reviewers' comments:</p>
<p>Reviewer's Responses to Questions</p>
<p>
<strong>Comments to the Author</strong>
</p>
<p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
<p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>2. Has the statistical analysis been performed appropriately and rigorously? </p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
<p>The <a href="http://www.plosone.org/static/policies.action#sharing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">PLOS Data policy</a> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
<p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
<p>Reviewer #1: Yes</p>
<p>Reviewer #2: Yes</p>
<p>**********</p>
<p>5. Review Comments to the Author</p>
<p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
<p><strong>Reviewer #1: </strong> The work is well organised and successfully presented. The following paper sets out a proposal for a new detection algorithm with regard to the identification of ships. It is hypothesised that the study will achieve a higher level of success if the following recommendations are taken into consideration during the process of its update.</p>
<p>1. It is important to note that all tables and graphs in the study were prepared with the mAP metric exclusively. The application of additional overlap metrics is also recommended. It is recommended that metrics such as DSC, IoU,RAVD and HD95 be deployed.</p>
<p>2. It is imperative that references are updated and expanded.</p>
<p>3. It is recommended that the resolution quality of the images is increased, with a view to enhancing their clearness and legibility.</p>
<p><strong>Reviewer #2:</strong>  I appreciate the effort put into this manuscript. Addressing the suggested revisions will significantly enhance the clarity and impact of your work. By refining these sections, the manuscript will provide greater value to the research community and result in a more robust and informative publication.</p>
<p>**********</p>
<p>6. PLOS authors have the option to publish the peer review history of their article (<a href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">what does this mean?</a> ). If published, this will include your full peer review and any attached files.</p>
<p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
<p><strong>Do you want your identity to be public for this peer review?</strong> For information about this choice, including consent withdrawal, please see our <a href="https://www.plos.org/privacy-policy" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Privacy Policy</a> .</p>
<p>Reviewer #1: <strong>Yes: </strong> Mehmet Süleyman YILDIRIM</p>
<p>Reviewer #2: <strong>Yes: </strong> Ozan PEKER</p>
<p>**********</p>
<p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
<p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <a href="https://pacev2.apexcovantage.com/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://pacev2.apexcovantage.com/</a> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <span>figures@plos.org</span> . Please note that Supporting Information files do not need this step.</p>
<section class="sm xbox font-sm" id="pone.0330485.s001"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Comments.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12370123/bin/pone.0330485.s001.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330485.s001.pdf</a><sup> (111.3KB, pdf) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0330485.r002"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. 2025 Aug 21;20(8):e0330485. doi: <a href="https://doi.org/10.1371/journal.pone.0330485.r002" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330485.r002</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Author response to Decision Letter 1</h1></hgroup><ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="anp_a.d" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a.d" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="anp_a.d" class="d-panel p" style="display: none"><div class="notes p"><section id="historyfront-stub2" class="history"><p>Collection date 2025.</p></section></div></div>
<div id="clp_a.d" class="d-panel p" style="display: none"><div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div></div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>11 Jul 2025</em>
</p>
<p>Response to the Reviewers’ Comments</p>
<p>Thank you very much for giving us the opportunity to improve the manuscript. The authors are grateful for the reviewers’ valuable comments, which have helped us to understand the underlying approach in depth and improved the original manuscript. We think carefully about comments raised by the reviewers and provide an item-by-item list to explain how these comments were addressed.</p>
<p>In the 'Revised Manuscript with Track Changes' file, we use the highlighting to mark the modifications. Point-to-point responses to reviewers’ comments are below. Reviewers’ comments are in italics and blue, immediately followed by our response.</p>
<p>Reviewer: 1</p>
<p>Comments to the Author:</p>
<p>The work is well organised and successfully presented. The following paper sets out a proposal for a new detection algorithm with regard to the identification of ships. It is hypothesised that the study will achieve a higher level of success if the following recommendations are taken into consideration during the process of its update.</p>
<p>Response�Thank you very much for your careful review and recognition of our manuscript. We will make cautious revisions based on your feedback. Thank you very much.</p>
<p>Specific comments:</p>
<p>1. It is important to note that all tables and graphs in the study were prepared with the mAP metric exclusively. The application of additional overlap metrics is also recommended. It is recommended that metrics such as DSC, IoU,RAVD and HD95 be deployed.</p>
<p>Response�Thank you very much for your valuable feedback. The mAP is critical in remote sensing object detection tasks. Additionally, the evaluation metrics in the manuscript also utilize AP and Recall. In the loss function, we also use IoU as the training basis for the algorithm. The DSC and other metrics you mentioned are used mainly in the field of image segmentation. Thank you again for your valuable feedback.</p>
<p>2. It is imperative that references are updated and expanded.</p>
<p>Response�Thank you very much for raising the questions in the references. We have revised the format of the references and added some references.</p>
<p>3.It is recommended that the resolution quality of the images is increased, with a view to enhancing their clearness and legibility.</p>
<p>Response�Thank you very much for your valuable feedback. Our minimum image resolution is 1024 × 1024, and the image displayed in the PDF file should be a resized version of the original image. You can download and view the original image in the system. Thank you very much for your efforts and recognition.</p>
<p>Reviewer: 2</p>
<p>Comments to the Author:</p>
<p>I appreciate the effort put into this manuscript. Addressing the suggested revisions will significantly enhance the clarity and impact of your work. By refining these sections, the manuscript will provide greater value to the research community and result in a more robust and informative publication.</p>
<p>Response�Thank you very much for giving us the opportunity to improve the manuscript and pointing out the shortcomings in the manuscript. In this iteration, we have revised the manuscript on the following issues.</p>
<p>Specific comments:</p>
<p>1. Abstract: In addition to emphasizing that ship detection in remote sensing imagery is currently a hot topic, it would be beneficial to specify the application domains in which such detection tasks are conducted. Doing so would help the reader better understand the practical relevance of the research question and potentially reveal its target users or beneficiaries. Please specify the image format (RGB,Multispectral,etc.) of the dataset used in the study within the abstract. This information will enhance the clarity and reproducibility of the work.</p>
<p>Response�Thank you very much for your valuable feedback. Adding application areas and image formats to the abstract can indeed improve the clarity of the research. We have added relevant content.</p>
<p>2. Introduction: Please clarify the novelty of the study more explicitly at the end of the Introduction section. Clearly stating the unique contributions will help distinguish this work from existing literature.</p>
<p>Response�Thank you for your efforts during the review process. We have made revisions to the work description and innovative design at the end of the manuscript introduction section.</p>
<p>3. Related work: The content of the Related Work section is expected to align with the statement in your sentence: "The commonly used remote sensing object detection algorithms can be categorized into three classes: the Convolutional Neural Network, the Transformer [1], and the Mamba...". Ensuring coherence between this categorization and the subsequent discussion will improve the clarity and structure of the manuscript.</p>
<p>Response�Thank you very much for your careful review. We have revised the titles of the relevant work sections to align them with the descriptions in the introduction section of the manuscript.</p>
<p>4. Our Work: Under the heading "Optimizing the Loss Function," merely changing the loss function appears inconsistent. Readers may expect an actual optimization method or strategy within this section. It is advisable to either revise the heading to better reflect the content or expand the section to include optimization techniques. In addition to KFIoU Loss, it is recommended to include a information about Focal Loss as well.</p>
<p>Response�We are very grateful for your valuable suggestions. We have revised the title "Optimizing the Loss Function" to "Changing the Loss Function" to avoid confusion for readers. The manuscript also includes a description of Focal loss.</p>
<p>5. Experiment and Result Analysis: Presenting the details under the "Experimental Environment and Parameter Configuration" section in a tabular format would enhance readability and clarity for the reader. Additionally, it is recommended to include resource consumption metrics of your developed model compared to the baseline model, such as RAM usage and training time per epoch. These details will provide a more comprehensive evaluation of the model’s efficiency. Please provide detailed visualizations of the distribution within the datasets using appropriate graphs. This will help readers better understand the data composition and support the validity of the experimental results.</p>
<p>Response�Thank you for your question regarding the experimental configuration and dataset situation. We have added Table 3, which outlines the parameters configuration and resource consumption, and Fig 8 to illustrate the dataset's distribution.</p>
<p>6. Discussion: In the Discussion section, it would be beneficial to mention potential future work, such as leveraging SAR imagery for improved performance and addressing enhancements to cope with adverse weather conditions. Including these aspects could strengthen the manuscript by highlighting promising directions for further research.</p>
<p>Response�Thank you for your suggestion about future work in the discussion section. Indeed, adding possible future application directions and plans can provide readers with more inspiration. We have added some potential future work and application directions in the discussion section.</p>
<section class="sm xbox font-sm" id="pone.0330485.s002"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12370123/bin/pone.0330485.s002.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330485.s002.pdf</a><sup> (138.8KB, pdf) </sup>
</div></div></section></section></article><article class="sub-article" id="pone.0330485.r003"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330485.r003" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330485.r003</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Decision Letter 1</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link" aria-describedby="id7"><span class="name western">Fatih Uysal</span></a><div hidden="hidden" id="id7">
<h3><span class="name western">Fatih Uysal</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link"><span class="name western">Fatih Uysal</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.e" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.e" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.e" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Fatih Uysal</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.e" class="d-panel p" style="display: none">
<div>© 2025 Fatih Uysal</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>
<em>4 Aug 2025</em>
</p>
<p>M-ReDet: A Mamba-based Method for Remote Sensing Ship Object Detection and Fine-grained Recognition</p>
<p>PONE-D-25-19846R1</p>
<p>Dear Dr. Qin,</p>
<p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
<p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
<p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <a href="https://www.editorialmanager.com/pone/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Editorial Manager®</a>  and clicking the ‘Update My Information' link at the top of the page. For questions related to billing, please contact <a href="https://plos.my.site.com/s/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">billing support</a> .</p>
<p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>Kind regards,</p>
<p>Fatih Uysal, Ph.D.</p>
<p>Academic Editor</p>
<p>PLOS ONE</p>
<p>Additional Editor Comments (optional):</p>
<p>Following a thorough evaluation of the final version of the paper, the responses to the reviewer comments, and the revisions implemented, the manuscript was accepted based on its potential contribution to the literature.</p>
<p>Reviewers' comments:</p></section></article><article class="sub-article" id="pone.0330485.r004"><section class="pmc-layout__citation font-secondary font-xs"><div>PLoS One. doi: <a href="https://doi.org/10.1371/journal.pone.0330485.r004" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1371/journal.pone.0330485.r004</a>
</div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Acceptance letter</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link" aria-describedby="id8"><span class="name western">Fatih Uysal</span></a><div hidden="hidden" id="id8">
<h3><span class="name western">Fatih Uysal</span></h3>
<div>Academic Editor</div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Uysal%20F%22%5BAuthor%5D" class="usa-link"><span class="name western">Fatih Uysal</span></a>
</div>
</div>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a.f" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="clp_a.f" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a.f" class="d-panel p" style="display: none">
<h4 class="font-secondary">Roles</h4>
<div class="p">
<strong class="contrib"><span class="name western">Fatih Uysal</span></strong>: <span class="role">Academic Editor</span>
</div>
</div>
<div id="clp_a.f" class="d-panel p" style="display: none">
<div>© 2025 Fatih Uysal</div>
<p>This is an open access article distributed under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Creative Commons Attribution License</a> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div></div>
</div></section><section class="body sub-article-body"><hr class="headless">
<p>PONE-D-25-19846R1</p>
<p>PLOS ONE</p>
<p>Dear Dr. Qin,</p>
<p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p>
<p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p>
<p>* All references, tables, and figures are properly cited</p>
<p>* All relevant supporting information is included in the manuscript submission,</p>
<p>* There are no issues that prevent the paper from being properly typeset</p>
<p>You will receive further instructions from the production team, including instructions on how to review your proof when it is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p>
<p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p>
<p>You will receive an invoice from PLOS for your publication fee after your manuscript has reached the completed accept phase. If you receive an email requesting payment before acceptance or for any other service, this may be a phishing scheme. Learn how to identify phishing emails and protect your accounts at <a href="https://explore.plos.org/phishing" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://explore.plos.org/phishing</a>.</p>
<p>If we can help with anything else, please email us at customercare@plos.org.</p>
<p>Thank you for submitting your work to PLOS ONE and supporting open access.</p>
<p>Kind regards,</p>
<p>PLOS ONE Editorial Office Staff</p>
<p>on behalf of</p>
<p>Assoc. Prof. Dr. Fatih Uysal</p>
<p>Academic Editor</p>
<p>PLOS ONE</p></section></article><article class="sub-article" id="_ad93_"><section class="pmc-layout__citation font-secondary font-xs"><div></div></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Associated Data</h1></hgroup><ul class="d-buttons inline-list"></ul>
<div class="d-panels font-secondary-light"></div>
<div></div>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
</div></section><section class="body sub-article-body"><section id="_adsm93_" lang="en" class="supplementary-materials"><h2 class="pmc_sec_title">Supplementary Materials</h2>
<section class="sm xbox font-sm" id="db_ds_supplementary-material1_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Comments.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12370123/bin/pone.0330485.s001.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330485.s001.pdf</a><sup> (111.3KB, pdf) </sup>
</div></div></section><section class="sm xbox font-sm" id="db_ds_supplementary-material2_reqid_"><div class="caption p">
<span>Attachment</span><p>Submitted filename: <em>Response to Reviewers.pdf</em></p>
</div>
<div class="media p"><div class="caption">
<a href="/articles/instance/12370123/bin/pone.0330485.s002.pdf" data-ga-action="click_feat_suppl" class="usa-link">pone.0330485.s002.pdf</a><sup> (138.8KB, pdf) </sup>
</div></div></section></section><section id="_adda93_" lang="en" class="data-availability-statement"><h2 class="pmc_sec_title">Data Availability Statement</h2>
<p>The curated data and code can be accessed at the following link: <a href="https://github.com/LG973641114/M-ReDet/releases" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/LG973641114/M-ReDet/releases</a>.</p></section></section></article><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from PLOS One are provided here courtesy of <strong>PLOS</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1371/journal.pone.0330485"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/pone.0330485.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (3.2 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12370123/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12370123/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370123%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370123/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12370123/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12370123/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40839647/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12370123/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40839647/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12370123/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12370123/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="ShbiQ2UAl4qTF7Kmfjvc2QwoL79inWjbFJuKX0JBVkiiXYi5JBTK92h7XuZlF4CZ">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
        <script type="module" crossorigin="" src="/static/assets/math-574fdcc6.js"></script>
    
    

    </body>
</html>
