
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Visual language transformer framework for multimodal dance performance evaluation and progression monitoring - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="1525426D8AF2D02305426D004A4744EE.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368089/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Visual language transformer framework for multimodal dance performance evaluation and progression monitoring">
<meta name="citation_author" content="Lei Chen">
<meta name="citation_author_institution" content="Art College, Chengdu Sport University, Chengdu, 610041 China">
<meta name="citation_publication_date" content="2025 Aug 20">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30649">
<meta name="citation_doi" content="10.1038/s41598-025-16345-2">
<meta name="citation_pmid" content="40835876">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368089/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368089/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368089/pdf/41598_2025_Article_16345.pdf">
<meta name="description" content="Dance is often perceived as complex due to the need for coordinating multiple body movements and precisely aligning them with musical rhythm and content. Research in automatic dance performance assessment has the potential to enhance individuals’ ...">
<meta name="og:title" content="Visual language transformer framework for multimodal dance performance evaluation and progression monitoring">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Dance is often perceived as complex due to the need for coordinating multiple body movements and precisely aligning them with musical rhythm and content. Research in automatic dance performance assessment has the potential to enhance individuals’ ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368089/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12368089">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-16345-2"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_16345.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12368089%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12368089/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12368089/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368089/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 20;15:30649. doi: <a href="https://doi.org/10.1038/s41598-025-16345-2" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-16345-2</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Visual language transformer framework for multimodal dance performance evaluation and progression monitoring</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20L%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Lei Chen</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Lei Chen</span></h3>
<div class="p">
<sup>1</sup>Art College, Chengdu Sport University, Chengdu, 610041 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Chen%20L%22%5BAuthor%5D" class="usa-link"><span class="name western">Lei Chen</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Art College, Chengdu Sport University, Chengdu, 610041 China </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Apr 15; Accepted 2025 Aug 14; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12368089  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40835876/" class="usa-link">40835876</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Dance is often perceived as complex due to the need for coordinating multiple body movements and precisely aligning them with musical rhythm and content. Research in automatic dance performance assessment has the potential to enhance individuals’ sensorimotor skills and motion analysis. Recent studies on dance performance assessment primarily focus on evaluating simple dance movements using a single task, typically estimating final performance scores. We propose a novel transformer-based visual-language framework for multi-modal dance performance evaluation and progression monitoring. Our approach addresses two core challenges: the learning of feature representations for complex dance movements synchronized with music across diverse styles, genres, and expertise levels, and capturing the multi-task nature of dance performance evaluation. To achieve this, we integrate contrastive self-supervised learning, spatiotemporal graph convolutional networks (STGCN), long short-term memory networks (LSTM), and transformer-based text prompting. Our model evaluates three key tasks: (i) multilabel dance classification, (ii) dance quality estimation, and (iii) dance-music synchronization, leveraging primitive-based segmentation and multi-modal inputs. During the pre-training phase, we utilize contrastive loss to capture primitive-based features from complex dance motion and music data. For downstream tasks, we propose a transformer-based text prompting approach to conduct multi-task evaluations for the three assessment objectives. Our model outperforms in diverse downstream tasks. For multilabel dance classification, our model achieves a score of 75.20, representing a 10.25% improvement over CotrastiveDance, on the dance quality estimation task, the proposed model achieved a 92.09% lower loss on CotrastiveDance. For dance-music synchronization, our model excels with a score of 2.52, outperforming CotrastiveDance by 48.67%.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Deep learning, Transformer, Graph convolutional network, Dance performance monitoring, Multimodal analysis</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Computational science, Computer science, Information technology, Software</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Dance is a universal art form that serves as a medium of artistic expression and holds cultural, social, and therapeutic significance. Its complexity lies in the precise coordination of physical movements, timing, and expression, often tailored to diverse styles, genres, and rhythms. Despite its widespread importance, systematic methods for evaluating dance performance remain underdeveloped, creating a gap in assessing and improving performance quality objectively<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>. The evaluation of dance proficiency involves intricate processes, including the analysis of movement dynamics, rhythm synchronization, and aesthetic expression, all of which require substantial expertise and experience. This creates a pressing need for automated frameworks that can mimic expert evaluation while offering scalability and accessibility.</p>
<p id="Par3">Skilled dancers execute intricate movements and synchronize them with musical beats, a proficiency acquired through extensive practice and training. However, access to professional training is often costly and constrained by time and location. To address this limitation, there is potential for developing a multi-task dance performance assessment model that continuously evaluates a dancer’s performance quality<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>. Most previous research on Action Quality Assessment (AQA) and Skill Assessment (SA) has predominantly focused on areas such as surgical tasks<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>, Olympic sports<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>, and everyday human activities<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup>, with limited emphasis on dance performance evaluation. Recent studies on dance and music analysis have largely concentrated on tasks like dance motion generation<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup> and motion-music synthesis<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>, rather than the integrated assessment of dance performance. Other methods of dance motion analysis<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup> have focused on simplified, controlled conditions, often neglecting music features or offering only final performance scores. Therefore, two key challenges emerge in the automatic assessment of dance performance: (i) learning feature representations for complex dance movements and music, where representations vary significantly across different styles, genres, choreographies, and expertise levels; and (ii) capturing the multi-task nature of dance performance evaluation, which involves interpreting diverse multimedia content, such as identifying elements in the dance sequence, scoring each element, and analyzing the relationship between motion and music.</p>
<p id="Par4">Dance performance, however, presents unique challenges due to its inherently multimodal nature, requiring a deep understanding of both motion and music features. Furthermore, the variability across dance styles, choreographies, and expertise levels introduces additional complexity<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>. Addressing these challenges not only fills a critical gap in the field of motion analysis but also has the potential to revolutionize professional training, democratize access to dance education, and contribute to broader applications such as rehabilitation, entertainment, and cultural preservation.</p>
<p id="Par5">Primitive-based segmentation plays a pivotal role in our proposed framework by breaking down dance sequences into smaller, rhythmically-aligned units known as primitives. This segmentation enables the model to capture the intricate relationship between dance movements and the rhythmic structure of the music at a granular level. By focusing on primitives, the framework can evaluate subtle details of motion synchronization, transitions, and rhythmic accuracy that are often overlooked in holistic performance analysis. The Eight-Beats Segmentation (EBS) method is employed to systematically divide dance sequences into rhythmic intervals aligned with musical beats. This alignment is critical for extracting temporal and spatial features that encapsulate the synergy between motion and music, providing a robust foundation for downstream tasks such as multilabel classification, quality estimation, and synchronization analysis.</p>
<p id="Par6">Our proposed method tackles the aforementioned challenges by incorporating advanced machine learning techniques tailored for complex, real-world professional dance training environments. The framework leverages the diversity of dance genres, choreographies, and expertise levels, ensuring its robustness in handling the intricate nuances of dance performances. Figures <a href="#Fig1" class="usa-link">1</a> and  <a href="#Fig2" class="usa-link">2</a> illustrate the self-supervised learning pipeline of our approach, which focuses on extracting meaningful representations from both motion and music data. At the core of our method is a contrastive learning framework that effectively learns feature representations of primitive-based dance sequences. Primitive-based segmentation breaks down the dance sequences into smaller, rhythmically-aligned units, allowing the model to capture fine-grained details of movement and their synchronization with musical elements. This segmentation is crucial for assessing a dancer’s performance not just holistically but also at the micro-level, ensuring that subtle transitions and rhythmic accuracy are evaluated. The contrastive learning mechanism optimizes the model to distinguish between positive (synchronized) and negative (unsynchronized) pairs of motion and music features, thus enabling the framework to generalize across dancers with varying levels of expertise. By training on such pairs, the model learns to recognize the complex temporal and spatial relationships that define dance proficiency, while accounting for variations in skill levels. This allows for a more granular and nuanced evaluation of dance performance, addressing both the aesthetic quality of movements and their rhythmic synchronization with music. Our approach emphasizes domain adaptability through contrastive learning, ensuring that the model is capable of performing well in dynamic environments and across different dance styles and genres without the need for extensive labeled data. These dance sequences encompass a wide range of music genres and choreographies. Rather than evaluating the entire dance sequence, as in previous work<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a>,<a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>, our model is trained using motion and music primitives. Each motion and music sequence is segmented into primitives using the Eight-Beats segmentation (EBS) method, which captures the rhythmic structure in the data<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. We utilize SpatioTemporal Graph Convolutional Networks (STGCN)<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup> and Long Short-Term Memory network (LSTM)<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup> as encoders to learn the features of these dance primitives.</p>
<figure class="fig xbox font-sm" id="Fig1"><h3 class="obj_head">Fig. 1.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/9a08c5e19388/41598_2025_16345_Fig1_HTML.jpg" loading="lazy" id="MO1" height="1217" width="708" alt="Fig. 1"></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The diagram provides a high-level view of the contrastive self-supervised framework for dance motion-music feature representation. It employs two encoders: a motion encoder and a music encoder, processing respective features from the input data. These features are embedded into a shared space, with MLP modules projecting the embeddings, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/6b5f5ae7b14a/d33e190.gif" loading="lazy" id="d33e190" alt="Inline graphic"></span> (motion) and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/46ee293b0c67/d33e196.gif" loading="lazy" id="d33e196" alt="Inline graphic"></span> (music), into the same latent space for comparison. This framework learns robust motion-music alignment for downstream tasks like performance evaluation and rhythm synchronization.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig2"><h3 class="obj_head">Fig. 2.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/d122c9c26056/41598_2025_16345_Fig2_HTML.jpg" loading="lazy" id="MO2" height="723" width="786" alt="Fig. 2"></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The diagram illustrates the architecture for multi-task dance performance evaluation, consisting of three tasks: (i) Multilabel Dance Classification, (ii) Dance Quality Estimation, and (iii) Dance-Music Synchronization. Frozen pre-trained encoders extract features from dance motion-music input, which are then fed into three task heads. Task 1 classifies dance choreography, genres, and expertise levels. Task 2 estimates dance quality through performance scoring. Task 3 aligns motion sequences with musical beats for synchronization evaluation. The architecture effectively unifies dance and music feature analysis, leveraging pre-trained encoders for streamlined performance assessment.</p></figcaption></figure><p id="Par7">Once the motion and music primitive features are extracted, we apply a transformer-based text prompting technique<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup> to assess the dance performance across three downstream tasks. The challenge of evaluating diverse dance genres and choreographies emerging globally requires adaptability. Continuously training new models for incoming data is impractical, and providing accurate predictions often demands large datasets of dance sequences under similar conditions (e.g., the same choreography, music, or time steps), which can be difficult to gather and label. To address the inherent complexities associated with evaluating diverse and unseen dance motions and music, we incorporate prompt tuning into our pre-trained model, a technique that has proven to be more efficient and effective than conventional fine-tuning methods in handling downstream tasks. Prompt tuning leverages pre-trained knowledge while requiring minimal adjustments to the model parameters, allowing it to adapt to new tasks by simply modifying the input prompt. This approach significantly reduces the need for extensive retraining, particularly in scenarios where acquiring large amounts of labeled data is impractical, such as professional dance evaluation across diverse genres and styles. Our experimental results provide compelling evidence that prompt tuning offers superior performance compared to traditional fine-tuning methods when applied to three critical downstream tasks in dance performance assessment: (i) multilabel dance classification, (ii) dance quality estimation, and (iii) dance-music synchronization. By leveraging prompt tuning, our model can generalize more effectively to diverse and previously unseen dance sequences and musical inputs, demonstrating enhanced adaptability and performance stability.</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par8">We present a contrastive self-supervised learning framework designed to capture and represent the features of primitive-based motion and music sequences.</p></li>
<li><p id="Par9">We introduce a transformer-based text prompting mechanism, which is used to artistically model the downstream multi-task dance performance assessment process.</p></li>
<li><p id="Par10">Proposed multi-modal framework is evaluated on three downstream tasks including (i) multilabel dance classification, involving the identification of dance motion primitives; (ii) dance quality estimation, focusing on estimating quality score distributions; and (iii) dance-music synchronization, which involves regressing the alignment rate between music and motion.</p></li>
</ul>
<p>The literature review is presented in “Literature review”. The proposed multi-modal framework for dance performance assessment and problem formulation is explained in “Method”. The dataset, experimental setup, and performance comparisons are presented in “Experiment and result”, while “Conclusion” summarises the study and concludes it with a future direction.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Literature review</h2>
<section id="Sec3"><h3 class="pmc_sec_title">Human motion analysis</h3>
<p id="Par11">The study of human motion has seen significant advancements in areas such as motion generation<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>, rehabilitation<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>, and sports performance analysis<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>. A particular area of focus dance motion analysis, which aims to automatically and quantitatively assess the quality of dance performances has gained increasing attention, primarily due to the advancements in motion analysis<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup> and multimedia signal processing technologies<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. However, most existing methodologies are limited to evaluating simple dance motions based on specific choreographies and expertise levels, which limits their generalizability for real-world applications in assistive technologies. Recent work<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup> introduced a self-supervised approach to evaluate dancers’ expertise levels across different dance genres and choreographies. Nonetheless, other crucial tasks such as dance-music synchronization and dance quality estimation require further exploration.</p></section><section id="Sec4"><h3 class="pmc_sec_title">Quality assessment</h3>
<p id="Par12">In addition to motion analysis, assessing the skill level associated with human motion is integral to evaluating dance performance. While a large portion of existing AQA research has focused on surgical proficiency<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup> or evaluating Olympic-level sports performance<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>, little attention has been paid to dance performance assessment. These approaches have demonstrated effectiveness by concentrating on feature extraction for surgical or sports tasks. However, applying these methods to dance assessment remains challenging, as they often focus solely on motion feature representation and overlook the correlation between motion and other key elements, such as musical rhythm.</p></section><section id="Sec5"><h3 class="pmc_sec_title">Self-supervised learning</h3>
<p id="Par13">Self-supervised learning (SSL) has shown remarkable success across a wide range of applications, including computer vision, natural language processing, and audio analysis<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a>,<a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>. Its key advantage is the ability to leverage large amounts of unlabeled data to learn meaningful feature representations, reducing reliance on costly labeled datasets. This is especially valuable in domains like human motion analysis, where obtaining accurate labeled data is challenging. In human motion analysis, SSL enables robust and generalized feature learning by discovering patterns within the data without explicit labels. Traditional supervised methods often rely on annotated data, introducing biases and limiting generalizability. SSL, on the other hand, uses pretext tasks like contrastive learning to train models that distinguish between similar and dissimilar motion sequences, capturing complex spatial and temporal dependencies in motion data. This approach significantly enhances the model’s ability to generalize across different motion styles, expertise levels, and contexts, making it highly effective for tasks like action recognition, skill assessment, and performance evaluation. By focusing on intrinsic motion features, SSL-based models offer greater scalability and adaptability, pushing the boundaries of motion analysis without the constraints of labeled data. Recent works in this area have utilized contrastive loss to learn similarities between sample pairs. For instance,<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup> demonstrated that contrastive self-supervised learning enables models to achieve state-of-the-art performance in image classification, proving that well-designed data augmentation strategies can result in robust feature representations.</p>
<p id="Par14">While SSL techniques have been extensively applied in visual tasks, the application of such methods to human 3D-skeleton data remains comparatively underexplored. Prior work, such as that by<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>, has demonstrated the potential of leveraging human 3D-skeleton data, introducing skeleton augmentation techniques to enhance the performance of human action recognition. However, the full potential of contrastive learning for 3D-skeleton-based motion data has yet to be realized in more complex domains like dance performance. In this study, we adopt a contrastive learning framework to learn robust representations of 3D-skeleton dance motion data synchronized with music features. Our approach focuses on mapping the dance motion and music features into a shared latent space, where the temporal and spatial dynamics of the dance motion, along with rhythmic and acoustic elements of the music, are preserved. By doing so, we facilitate the performance of three key downstream tasks: (i) multilabel dance classification, which involves the identification of dance genres, choreographies, and expertise levels; (ii) dance quality estimation, focusing on the prediction of performance scores; and (iii) dance-music synchronization, which measures the alignment between motion and musical rhythm<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. The integration of contrastive learning enables the model to effectively distinguish between different dance sequences, capturing both intra- and inter-class variations in dance movements and their corresponding musical features. This approach significantly enhances the model’s capacity to generalize to unseen dance performances, demonstrating the efficacy of contrastive learning in multimodal contexts involving 3D-skeleton data and music.</p></section><section id="Sec6"><h3 class="pmc_sec_title">Prompt tuning</h3>
<p id="Par15">The success of SS has significant advancements in improving the generalization capabilities of pre-trained models, especially for downstream tasks. This research emphasis is particularly critical in scenarios requiring few-shot or zero-shot learning, where models are expected to perform well with minimal labeled data or in the absence of task-specific training data. By leveraging SSL, models can capture robust feature representations that enhance their adaptability to new, unseen tasks. This research direction has been exemplified by models such as Generative Pre-trained Transformer-3 (GPT-3)<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>. Among the various techniques aimed at improving generalization, prompt tuning has emerged as one of the most prominent approaches. Inspired by the recent advancements in prompting techniques in both vision and language models<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a>,<a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup>, we adapt prompt tuning to our dance-music multimodal model. This adaptation allows the model to dynamically adjust to diverse, previously unseen dance motions and musical inputs with minimal modifications. By incorporating prompt tuning, we leverage the pre-trained model’s extensive prior knowledge while introducing task-specific prompts that guide the model towards new tasks without the need for full re-training. This approach addresses key challenges in generalizing dance performance assessment across multiple styles, genres, and choreography variations. It also significantly reduces the computational complexity typically associated with fine-tuning. Through prompt tuning, the model efficiently adapts to new dance and music combinations.</p></section></section><section id="Sec7"><h2 class="pmc_sec_title">Method</h2>
<p id="Par16">The proposed framework integrates multiple state-of-the-art techniques in a cohesive manner to enhance multi-modal dance performance evaluation. At its core, the system leverages a <em>contrastive self-supervised learning (SSL)</em> approach to learn robust feature representations from unlabelled motion and music data. The <em>Spatio-Temporal Graph Convolutional Network (STGCN)</em> and <em>Long Short-Term Memory (LSTM)</em> encoders are used in tandem to model the spatial-temporal dynamics of skeletal motion and the temporal progression of music features. STGCN captures the spatial relationships and temporal dependencies within skeletal motion sequences, while LSTM complements this by learning sequential patterns in both motion and music. These embeddings are further refined and aligned using the SSL framework with the InfoNCE loss, ensuring meaningful feature extraction across modalities. To bridge these embeddings with downstream tasks, the model incorporates a <em>transformer-based text prompting mechanism</em>, which integrates textual descriptions (e.g., genres, choreography, or expertise level) as context. This prompting technique enables seamless adaptation of pre-trained representations to specific evaluation tasks by aligning textual, motion, and music embeddings within a unified framework.</p>
<p id="Par17">The synergy between these components ensures that the model captures intricate interactions between motion, music, and textual information, enhancing its ability to generalize across diverse tasks. Figure <a href="#Fig3" class="usa-link">3</a> presents a detailed overview of the proposed multi-modal framework for dance performance evaluation, which combines motion, music, and textual data to achieve high-level dance analysis. The framework consists of three key inputs: (i) Dance Motion Input, representing skeletal motion data derived from dance sequences, (ii) Dance Music Input, which provides the corresponding audio features, and (iii) Textual Description Input, containing task-related metadata for specific evaluation tasks such as multilabel dance classification, dance quality estimation, and dance-music synchronization.</p>
<figure class="fig xbox font-sm" id="Fig3"><h3 class="obj_head">Fig. 3.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368089_41598_2025_16345_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/174aa5ff2083/41598_2025_16345_Fig3_HTML.jpg" loading="lazy" id="MO3" height="424" width="668" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The diagram presents a multi-modal framework for dance performance evaluation using motion, music, and text inputs. It includes three components: (i) a frozen STGCN for motion encoding, (ii) a frozen LSTM network for music encoding, and (iii) a trainable Transformer-based text encoder. These encoders generate features for three tasks: Multilabel Dance Classification, Dance Quality Estimation, and Dance-Music Synchronization. Inputs include dance motion sequences, music waveforms, and text descriptions. MLPs process the features and prompt queries to refine textual inputs for dynamic task evaluation.</p></figcaption></figure><section id="Sec8"><h3 class="pmc_sec_title">Problem formulation</h3>
<p id="Par18">In Fig. <a href="#Fig3" class="usa-link">3</a>, we introduce an SSL framework for multi-task, primitive-based dance performance evaluation. The framework is designed to address the complexity of evaluating dance performances by decomposing them into manageable components, or primitives, and conducting assessments across multiple tasks. The primary goal of this framework is to systematically evaluate the quality of input dance sequences through the execution of three critical tasks: (i) Multilabel Dance Classification, (ii) Dance Quality Estimation, and (iii) Dance-Music Synchronization.</p>
<p id="Par19">Initially, we employ a combination of STGCN and LSTM encoders to capture the spatial-temporal dependencies in both dance motion and music features. This is achieved through a contrastive learning framework, leveraging unlabelled data and optimizing the feature representations with the Information Noise-Contrastive Estimation (InfoNCE) loss function. The model’s input consists of a set of sequences, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/775db7c0382d/d33e415.gif" loading="lazy" id="d33e415" alt="Inline graphic"></span>, representing both the skeletal motion data of the dancer and the corresponding music features. The STGCN is responsible for extracting the spatial-temporal features from the skeletal data, while the LSTM encoder captures the temporal progression of both the dance motions and the music. Once the dance motion and music embeddings are extracted, we proceed with multi-task downstream knowledge transfer on unseen data.</p>
<p id="Par20">This is achieved using a transformer-based text prompt tuning methodology, which fine-tunes the pre-trained model to adapt to new downstream tasks. Unlike traditional fine-tuning approaches, this method introduces text prompts that describe the specific attributes of the input data, such as dance genre, choreography, or performance quality, serving as cues for the model to align the learned representations with the task at hand. By leveraging textual prompts as additional context, the model can efficiently transfer knowledge and adapt to diverse evaluation tasks, enhancing its flexibility and performance across different domains. The transformer architecture allows for the seamless integration of these prompts with the learned representations of motion and music, enabling the model to generalize effectively across diverse tasks. This technique improves the model’s capacity to handle complex and unfamiliar dance sequences.</p></section><section id="Sec9"><h3 class="pmc_sec_title">Pre-training</h3>
<section id="Sec10"><h4 class="pmc_sec_title">Eight-beats segmentation</h4>
<p id="Par21">Primitive-based segmentation is implemented using the Eight-Beats Segmentation (EBS) method to break down dance sequences into rhythmically aligned units, or primitives. Each primitive corresponds to an interval of eight musical beats, ensuring that the segmented dance motions mirror the rhythmic patterns in the music. This segmentation is essential for capturing fine-grained details of motion-music synchronization, as it allows the model to isolate and analyze dynamic transitions, subtle gestures, and rhythmic accuracy. By structuring the data in this way, EBS facilitates more effective feature extraction for learning complex temporal and spatial dependencies across both modalities. The segmented primitives serve as the foundation for downstream tasks, enabling the model to evaluate performance at a granular level while maintaining a focus on the interplay between motion and music.</p>
<p id="Par22">Prior to training, we employ an EBS<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup> technique to identify the motion and music primitives within the input dance sequences. The Eight-Beats approach involves breaking down and counting dance movements<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a>,<a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. In this approach, the dance sequences are systematically segmented into discrete motion primitives, aligned with the rhythmic structure of the accompanying music. The segmentation process follows the Eight-Beats method, where each musical sequence is divided into intervals corresponding to every eight beats of the music. This partitioning captures the temporal alignment between the dancer’s movements and the underlying musical rhythm, providing a granular representation of the performance. Specifically, the musical sequence is divided into three distinct groups, each containing eight beats, as denoted by the pink dashed lines. This segmentation method ensures that the rhythmic patterns of the music are closely mirrored by the corresponding motion primitives, facilitating a more detailed analysis of the synchronization between the dancer’s movements and the music. By employing this structured approach, the segmentation preserves both the temporal and rhythmic nuances of the dance, allowing the model to capture complex motion-music correlations for downstream tasks such as performance evaluation and rhythm synchronization.</p>
<p id="Par23">The beats are detected and segmented using the Librosa audio signal processing library<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>, which facilitates the identification of rhythmic structures within the music. Following this, the corresponding dance motions are similarly divided into three groups of motion primitives, ensuring synchronization with the identified musical beats. From the analysis, we observe that the dance movements in the first group display contrasting patterns compared to those in the third group, highlighting the dynamic nature of the choreography across the sequence. This segmentation approach, termed the Eight-Beats method, proves effective in isolating meaningful and rhythmically aligned dance movements. It captures the intricate relationship between choreography and musical content, making it a robust tool for analyzing motion in conjunction with music. This technique allows the model to learn from the segmentation and analyze dance performances in a granular manner, which is essential for multi-modal tasks such as dance quality evaluation and dance-music synchronization.</p></section><section id="Sec11"><h4 class="pmc_sec_title">Motion and music encoders</h4>
<p id="Par24">In this study, we employ a Graph-Based Neural Network<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>, specifically the STGCN<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>, to extract both the spatial and temporal features from human motion. The input to the graph consists of sequences of dance motion <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/ecf443b37162/d33e460.gif" loading="lazy" id="d33e460" alt="Inline graphic"></span>, where <em>i</em> represents the index of the body joints and <em>k</em> corresponds to the frame number. This encoder is expressed as:</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/bc83e385b9e4/d33e473.gif" loading="lazy" id="d33e473" alt="graphic file with name d33e473.gif"></td>
<td class="label">1</td>
</tr></table>
<p>For the extraction of music features, the raw music is first pre-processed to derive acoustic features, as these are expected to provide more relevant information than the raw data. The acoustic features are extracted using the librosa library<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>. Specifically, we focus on five categories of features <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq5"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/bbacefe250ab/d33e484.gif" loading="lazy" id="d33e484" alt="Inline graphic"></span>, which include the Mel-frequency cepstral coefficients (MFCC)<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>, MFCC-delta, constant-Q chromagram, tempogram, and onset strength. Following this extraction, we utilize an LSTM-based encoder <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq6"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/e7b4e081507a/d33e494.gif" loading="lazy" id="d33e494" alt="Inline graphic"></span> to model the acoustic features<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>, formulated as:</p>
<table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/275926ccec9f/d33e505.gif" loading="lazy" id="d33e505" alt="graphic file with name d33e505.gif"></td>
<td class="label">2</td>
</tr></table>
<p>The motion and music embeddings, denoted as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq7"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/433be34aa427/d33e512.gif" loading="lazy" id="d33e512" alt="Inline graphic"></span>, are subsequently obtained by concatenating the output of the two encoders and processed through a multi-layer perceptron (MLP).</p></section><section id="Sec12"><h4 class="pmc_sec_title">ContrastiveInfoNCE loss</h4>
<p id="Par25">Once the motion and music embeddings <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq8"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/433be34aa427/d33e522.gif" loading="lazy" id="d33e522" alt="Inline graphic"></span> are obtained, the InfoNCE loss<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup> is employed as it effectively facilitates the learning of feature representations and enhances generalizability across various domains<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>. The loss function operates by contrasting the distance between positive and negative sample pairs in the network’s output space, encouraging the model to bring similar (positive) pairs closer together and push dissimilar (negative) pairs further apart in the latent space. During training, augmented pairs are generated from each sample, producing a total of 2<em>N</em> data points, where <em>N</em> represents the number of samples in the mini-batch. This augmentation helps the model to generalize better by learning robust feature representations across varied instances of the data, facilitating improved performance across downstream tasks. As described by<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>, apart from the positive sample, the remaining <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq9"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/fe44133d6cae/d33e547.gif" loading="lazy" id="d33e547" alt="Inline graphic"></span> samples within the mini-batch are considered negative. When (<em>i</em>, <em>j</em>) forms a positive pair, the loss is defined as:</p>
<table class="disp-formula p" id="Equ3"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/666b2bf611cb/d33e559.gif" loading="lazy" id="d33e559" alt="graphic file with name d33e559.gif"></td>
<td class="label">3</td>
</tr></table>
<p>where the indicator function <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq10"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/5dda8c219643/d33e566.gif" loading="lazy" id="d33e566" alt="Inline graphic"></span> equals 1 if <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq11"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/f9504ec1e20f/d33e573.gif" loading="lazy" id="d33e573" alt="Inline graphic"></span>. From the results, it can be inferred that a lower contrastive loss value signifies that positive sample pairs (i.e., corresponding dance and music segments) have been effectively mapped to similar or closer representations within the latent space. Conversely, negative sample pairs (i.e., non-corresponding dance and music segments) are pushed apart, being mapped to more dissimilar or distant representations. This behavior is crucial for ensuring that the model learns to differentiate between synchronized and non-synchronized dance-music pairs. The generation of these positive and negative pairs is achieved through the use of data augmentation techniques, which introduce variations in the training data to enhance the model’s robustness and generalizability. These techniques augment the dance and music sequences, creating multiple representations of the same motion-music relationship while preserving the essential temporal and rhythmic alignment. By optimizing the contrastive loss, the model becomes more proficient at capturing fine-grained dependencies between dance motions and musical rhythms, a critical factor in tasks such as dance-music synchronization and dance quality estimation.</p></section></section><section id="Sec13"><h3 class="pmc_sec_title">Transformer-based text prompting</h3>
<p id="Par26">Figure <a href="#Fig3" class="usa-link">3</a> demonstrates the proposed downstream text-prompting workflow. In this approach, the content of the test dance sequences (e.g., dance genres, expertise levels, or performance scores) is initially converted into text descriptions that serve as input to the model. A transformer encoder is employed to derive feature embeddings from this raw textual input, capturing the semantic meaning of the descriptions. Both the text input and the encoded embeddings act as prompts, guiding the model’s predictions. The text features generated by the transformer are then concatenated with the corresponding motion and music features extracted from the pre-trained encoders. In this framework, the integrated motion-music features, along with textual descriptions, are processed to generate task-specific predictions, which are subsequently compared against the ground truth for each downstream task using a task-agnostic loss function. A critical aspect of this approach is that the pre-trained parameters of the motion and music encoders remain fixed throughout the downstream evaluation phase. This ensures that only the text encoder and the final layer of the MLP are fine-tuned, allowing the model to adapt to new tasks while preserving the robustness of the learned motion-music representations. The model effectively fuses textual descriptions with the corresponding motion and music data, enabling enhanced performance across multiple downstream tasks. Figure <a href="#Fig3" class="usa-link">3</a> illustrates examples of input text descriptions utilized for the evaluation tasks, such as genre, choreography, expertise level, and specific dance movements, which are mapped to the encoded features to produce task-specific predictions. This approach enhances the flexibility of the model in handling complex, multimodal inputs while optimizing performance across various assessment tasks. For instance, in the context of the first task, the text encoder might receive a structured input such as <em>Ballet, choreography 1, beginner, and move 2</em>. This text serves as a verbal description of the relevant attributes within the sequence. Such descriptions are processed by the model to align with corresponding motion and music features.</p></section><section id="Sec14"><h3 class="pmc_sec_title">Downstream tasks</h3>
<p id="Par27">Evaluating dance performance requires a professional analysis of multimodal data. In this work, we address this by formulating it into three tasks: (i) multilabel dance classification, (ii) dance quality estimation, and (iii) dance-music synchronization. During the downstream phase, unseen test dance sequences are input into the pre-trained model, and each task is evaluated using a text prompting approach as shown in Fig. <a href="#Fig4" class="usa-link">4</a>.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368089_41598_2025_16345_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/e53ff9d76943/41598_2025_16345_Fig4_HTML.jpg" loading="lazy" id="MO4" height="446" width="669" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Workflow diagram illustrating the three downstream tasks of the proposed multi-modal dance performance evaluation framework. The diagram shows how extracted motion, music, and textual features are fed into distinct task-specific branches: (i) Multilabel Dance Classification, which identifies genre, choreography, expertise level, and primitive motions using cross-entropy loss; (ii) Dance Quality Estimation, which predicts a probabilistic performance score distribution using Gaussian regression with aleatoric uncertainty modeling; and (iii) Dance-Music Synchronization, which quantifies rhythmic alignment between motion and music through intensity correlation and MSR, optimized via mean squared error.</p></figcaption></figure><section id="Sec15"><h4 class="pmc_sec_title">Multilabel dance classification</h4>
<p id="Par28">The task of multilabel dance classification entails the identification and categorization of multiple attributes associated with dance sequences, including <em>dance genres</em>, <em>choreographies</em>, <em>expertise levels</em>, and <em>motion primitives</em>. This task is inherently a <em>multi-label classification problem</em>, where each dance sequence may be associated with multiple labels across these dimensions. Specifically, the dataset used for training consists of 20 distinct choreographies, 3 levels of dancer expertise, and 3 primitive motions. This combination results in 180 unique classes, computed as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq12"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/937277466753/d33e619.gif" loading="lazy" id="d33e619" alt="Inline graphic"></span>, which capture the diverse range of possible configurations within the dataset. To effectively classify the dance vocabulary, we employ a <em>cross-entropy loss function</em> during network training. Cross-entropy loss is particularly suited for multi-label classification tasks, as it quantifies the divergence between the predicted label distribution and the true label distribution. By optimizing this loss, the model learns to assign appropriate labels to dance sequences, accurately distinguishing between variations in genres, choreographies, expertise levels, and motion primitives. This approach ensures that the network can generalize well across the broad spectrum of dance elements, facilitating robust performance in identifying complex dance patterns across multiple labels.</p></section><section id="Sec16"><h4 class="pmc_sec_title">Dance quality estimation</h4>
<p id="Par29">Task 2 addresses the problem of dance performance evaluation by formulating it as a <em>score distribution prediction</em> task. In this approach, each dance sequence within the dataset is annotated with a Quality score, which serves as the ground truth for training and evaluation. These scores are provided by professional dancers, ensuring that the annotations are both expert-driven and reflect real-world dance assessment criteria. However, given the subjective nature of dance performance scoring, individual assessments can introduce variability and bias into the dataset. To account for this inherent uncertainty, our method does not focus on predicting a single, exact score. Instead, we model the problem as a <em>distributional prediction</em>, where the model learns to map the extracted dance features to a <em>probabilistic score distribution</em>. This allows the system to capture the range of possible scores that may be assigned to a given performance, thus incorporating <em>aleatoric uncertainty</em> type of uncertainty that arises from the data itself due to ambiguities or noise. By predicting a distribution rather than a single score, the model can better account for the subjectivity and variability introduced by different evaluators during the labeling process. This approach not only reduces the impact of individual biases but also provides a more nuanced and flexible evaluation framework. It reflects the variability inherent in human judgments and allows the system to express confidence in its predictions by modeling the variance within the score distribution. Such a probabilistic treatment of dance performance evaluation is critical for tasks that involve complex, subjective assessments, as it improves robustness and provides more informative output. This method enables a more accurate representation of the true range of scores a performance might receive, leading to more reliable and interpretable results in downstream tasks. A Gaussian distribution is used to model the predicted score and its corresponding variance. Thus, the loss function is formulated as follows:</p>
<table class="disp-formula p" id="Equ4"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/bd5ce4e3b3a6/d33e651.gif" loading="lazy" id="d33e651" alt="graphic file with name d33e651.gif"></td>
<td class="label">4</td>
</tr></table>
<p>where <em>x</em> and <em>y</em> represent the input data and the target score distribution, respectively, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq13"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/522dcb2e5063/d33e665.gif" loading="lazy" id="d33e665" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq14"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/2aabd379ce4a/d33e671.gif" loading="lazy" id="d33e671" alt="Inline graphic"></span> denote the mean and variance of the predicted distribution. The parameters <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq15"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/73975112fc4a/d33e677.gif" loading="lazy" id="d33e677" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq16"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/f5e0e8aa5fab/d33e683.gif" loading="lazy" id="d33e683" alt="Inline graphic"></span> control the weight of uncertainty and the constant component of the loss.</p></section><section id="Sec17"><h4 class="pmc_sec_title">Dance-music synchronization</h4>
<p id="Par30">Evaluating the dance performance requires a precise understanding of the relationship between a dancer’s movements and the accompanying musical rhythm, as the synchronization between motion and music is a key indicator of technical proficiency and artistic expression. Achieving proper alignment between these two components is essential for the overall quality of the performance. In this study, we introduce the <em>Motion-music synchrony rate</em> (MSR)<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> as a quantitative metric to assess the degree of synchronization between the dancer’s motion and the corresponding musical rhythm. The MSR is designed to measure how well the intensity of a dancer’s movements correlates with the intensity of the music, where “intensity” is used as a proxy for rhythmic dynamics. This allows us to capture temporal coherence, ensuring that movements are properly aligned with the beat structure of the music. Specifically, professional dancers were recruited to provide expert labels regarding the intensity of the music, guided by the choreography. These expert annotations serve as the ground truth for the rhythmic intensity of the music.</p>
<p id="Par31">On the motion side, intensity is calculated using kinematic data from the dancer’s movements, particularly focusing on the velocities of key body joints, such as the arms, legs, and torso. This approach ensures that our model can capture both large, dynamic movements and subtle gestures, which may align with different musical elements, such as tempo or accentuation. By computing the velocity of these joints over time, we derive a continuous measure of motion intensity that can be compared against the musical intensity. The MSR is then calculated by comparing these two intensity measures (music and motion) throughout the dance sequence, focusing on their temporal alignment. This comparison allows us to quantify the correlation between the dancer’s physical movements and the musical rhythm, providing a robust indicator of how well the dancer synchronizes their performance to the accompanying soundtrack. Higher MSR values reflect closer alignment between music and motion, indicating better synchronization and, by extension, a higher quality performance. This metric is critical for evaluating rhythmic accuracy in dance performance, as it highlights not only the technical precision of the dancer but also their ability to maintain a coherent and expressive connection with the musical accompaniment. We observe a strong correlation between music and motion intensities, with expert dancers demonstrating a close match between these two metrics.</p>
<p id="Par32">In our approach, the alignment between motion and music intensities is quantitatively assessed based on the temporal proximity of their respective intensity peaks. Specifically, the motion and music intensities are considered to be synchronized when the absolute difference between the time stamps of corresponding intensity peaks is less than or equal to 0.4 seconds. This threshold accounts for both the inherent variability in human motion and minor temporal discrepancies in music synchronization, ensuring that slight deviations do not disproportionately affect the evaluation. To derive this alignment metric, we first calculate the intensity peaks for both the motion and music sequences. Motion intensity is computed by analyzing the velocities of key body joints (such as the arms, legs, and torso), while music intensity is derived from the amplitude and dynamic changes in the audio signal. For each dance sequence, we identify the time points where the peaks in motion intensity and music intensity occur. When these peaks fall within the 0.4-second window, they are considered matched, reflecting a successful synchronization between the dancer’s movements and the music’s rhythm. The percentage of matched intensity peaks over the entire sequence is then computed and used as the ground truth for training and evaluation. This percentage serves as a robust indicator of rhythmic alignment, providing a continuous measure of synchronization quality across the duration of the performance. To train our model, we apply the Mean Squared Error (MSE) function to encourage the model to improve its precision in identifying synchronization patterns. By applying this loss function, the model is optimized to produce increasingly accurate predictions of motion-music synchronization, ultimately enhancing its ability to evaluate dance performance with a high degree of fidelity.</p>
<p id="Par33">To provide a clearer understanding of the operational flow of the proposed framework, Algorithm 1 presents the detailed pseudo-code outlining each phase of the multi-modal dance performance evaluation system.</p>
<figure class="fig xbox font-sm" id="Figa"><h5 class="obj_head">Algorithm 1.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368089_41598_2025_16345_Figa_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/fdbf8b0fc71b/41598_2025_16345_Figa_HTML.jpg" loading="lazy" id="MO5" height="609" width="669" alt="Algorithm 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Figa/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Pseudo-code of the multi-modal dance evaluation framework.</p></figcaption></figure></section></section></section><section id="Sec18"><h2 class="pmc_sec_title">Experiment and result</h2>
<p id="Par35">This section presents a comprehensive evaluation of the proposed model’s performance across multiple tasks using two key datasets: the AIST++<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup> and ImperialDance<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup> datasets. The datasets provide diverse dance motion and music data, essential for testing multimodal learning models. The experiments focus on assessing the model’s capacity for dance vocabulary classification, dance quality estimation, and dance-music synchronization. The datasets allow for the analysis of both spatial-temporal motion features and audio-visual synchronization, enabling a holistic evaluation of dance performance. By leveraging a combination of self-supervised learning and contrastive InfoNCE loss, the model demonstrates its effectiveness in capturing complex patterns within dance motions and music alignment.</p>
<section id="Sec19"><h3 class="pmc_sec_title">Dataset</h3>
<section id="Sec20"><h4 class="pmc_sec_title">AIST++ dataset</h4>
<p id="Par36">The AIST++ dataset<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup> is a large-scale dataset aimed at advancing research in dance motion generation<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup>, choreography analysis<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>, and motion-music alignment. It builds upon the original AIST Dance Video Dataset, extending it with high-quality 3D motion capture data. AIST++ includes over 1,400 3D motion sequences synchronized with 10 different genres of music, such as house, hip-hop, jazz, and waacking, among others. Each dance sequence is paired with high-resolution audio, ensuring a rich multimodal dataset that covers a wide range of dance styles. One of the distinguishing features of AIST++ is its provision of high-fidelity 3D skeletal motion data, captured using motion capture systems, which allows for detailed spatial and temporal analysis of dance movements. The dataset’s combination of motion sequences, music, and metadata facilitates tasks such as motion prediction, dance choreography synthesis, and motion-to-music alignment. The dataset is split into training, validation, and testing sets, providing a standardized benchmark for evaluating dance motion models. AIST++ is particularly valuable for cross-genre motion generation research, as its diverse dance styles enable the training of models that generalize well across different dance forms. In addition, the inclusion of real-world 3D motion data makes AIST++ an essential resource for applications requiring accurate motion representation, such as virtual dance training, choreography generation, and human-robot interaction in dance.</p></section><section id="Sec21"><h4 class="pmc_sec_title">ImperialDance dataset</h4>
<p id="Par37">The ImperialDance dataset<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup> is specifically designed to support research in multi-task dance performance assessment, motion-music analysis, and skill progression monitoring. It contains 69,300 seconds of recorded dance motions, spanning five distinct genres, 20 choreographies, and 20 music pieces, with performances captured from dancers of three different expertise levels (beginner, intermediate, and expert). One of the key contributions of the ImperialDance Dataset is the comprehensive recording of expertise levels, which enables detailed analysis of the progression of skill across dancers. Each choreography is repeated 100 times per class, ensuring a significant number of samples are available for each combination of genre, choreography, and expertise level. This high level of repetition allows the dataset to be particularly useful for fine-grained feature learning and for tracking performance improvements over time. A unique characteristic of the dataset is its segmentation of dance sequences into primitive motions, using the EBS method. This approach captures rhythmic structure in the data, enhancing the ability to extract meaningful temporal and spatial features from both dance motion and music. Additionally, the dataset’s multimodal design ensures that music features, such as rhythm and melody, are consistently aligned with the dance motion features, providing a robust foundation for motion-music synthesis and dance performance evaluation tasks. These features make the ImperialDance dataset especially suited for real-world dance training and performance assessment applications.</p></section></section><section id="Sec22"><h3 class="pmc_sec_title">Experimental setup</h3>
<p id="Par38">For pre-training, we use data from 10 different dance choreographies spanning five distinct genres. Each choreography is represented by 100 repeated samples, capturing variations across different expertise levels and motion primitives. To ensure uniformity in data processing, all dance sequences are standardized to a fixed duration of 10 s. These sequences are further segmented into three distinct segments, each lasting 3 s, following the Eight-Beats segmentation method. This segmentation captures the rhythmic structure of the music, aligning each motion primitive with the corresponding musical beats, thereby facilitating more granular feature extraction and enhancing the model’s ability to learn complex dance-music interactions. This process results in 90,000 training dance pieces (calculated as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq17"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/606765dc88bf/d33e758.gif" loading="lazy" id="d33e758" alt="Inline graphic"></span>). For downstream tasks, we use data from five additional choreographies, each belonging to a different genre, resulting in 4500 testing pieces (calculated as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq18"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/c2aeef554f53/d33e764.gif" loading="lazy" id="d33e764" alt="Inline graphic"></span>). We use three metrics to evaluate performance across the three assessment tasks: (i) classification accuracy, (ii) log-likelihood value, and (iii) MSE loss.</p>
<p id="Par39">The experiments were conducted on a system equipped with an NVIDIA Tesla V100 GPU with 32 GB of VRAM, an Intel Xeon Gold 6226R processor, and 128 GB of RAM. The model was implemented using PyTorch v1.12.1 and trained using the Adam optimizer with a learning rate of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq19"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/9282db406ceb/d33e772.gif" loading="lazy" id="d33e772" alt="Inline graphic"></span>, a batch size of 32, and a cosine annealing scheduler to adaptively reduce the learning rate during training. Dropout layers with a rate of 0.3 were used to prevent overfitting, and early stopping was employed based on validation loss. The InfoNCE loss function was applied with a temperature parameter <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq20"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/769edb236b87/d33e778.gif" loading="lazy" id="d33e778" alt="Inline graphic"></span>, optimizing the alignment between motion and music features during training. Dance motion data, represented as 3D skeleton sequences, were preprocessed using the OpenPose library to extract joint coordinates and remove outliers. The accompanying audio signals were preprocessed with the Librosa library to extract features such as Mel-frequency cepstral coefficients (MFCC), MFCC-deltas, constant-Q chromagrams, and tempograms. These features were normalized to ensure consistent scaling across samples. Data augmentation techniques were applied, including the addition of random noise to motion sequences and the application of time stretching and pitch shifting to audio features, simulating real-world variations in dance performances.</p>
<p id="Par40">To further evaluate the model’s generalization capacity, we conducted additional testing using five choreographies held out from the training set, each representing different dance genres. These sequences were not exposed to the model during either the pre-training or prompt tuning stages. Despite the lack of exposure, the model exhibited high performance across all three downstream tasks when evaluated on these unseen sequences, with only marginal declines in accuracy, NLL, and MSE. This confirms the framework’s capability to adapt to novel dance patterns and musical structures, emphasizing its effectiveness in real-world deployment scenarios where encountering new styles is common.</p></section><section id="Sec23"><h3 class="pmc_sec_title">Evaluation metrics</h3>
<p id="Par41">The Musical Motion-Music Synchrony Rate (MSR)<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> is defined as a quantitative measure of how well a dancer’s movements are synchronized with the beats of the accompanying music. The MSR ranges from 0 to 1, where a value of 1 indicates perfect alignment between the dance motions and the musical beats, and a value closer to 0 indicates poor alignment. By adjusting the threshold <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq21"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/549c4fa8ad3b/d33e808.gif" loading="lazy" id="d33e808" alt="Inline graphic"></span>, the strictness of the alignment criterion can be modulated. A smaller <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq22"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/549c4fa8ad3b/d33e814.gif" loading="lazy" id="d33e814" alt="Inline graphic"></span> requires closer synchronization for the motion to be considered aligned with the music, whereas a larger <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq23"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/549c4fa8ad3b/d33e820.gif" loading="lazy" id="d33e820" alt="Inline graphic"></span> allows for more flexible synchronization. In all our experiments, we consistently set the threshold <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq24"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/63cb46cfc542/d33e826.gif" loading="lazy" id="d33e826" alt="Inline graphic"></span> seconds, following prior literature<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>. This value balances the sensitivity and robustness of synchronization detection, accounting for minor variations in motion execution and beat perception</p>
<p id="Par42">For a given dance sequence, the MSR can be mathematically formulated as:</p>
<table class="disp-formula p" id="Equ5"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/8a636a459fe8/d33e839.gif" loading="lazy" id="d33e839" alt="graphic file with name d33e839.gif"></td>
<td class="label">5</td>
</tr></table>
<p>Where, <em>N</em> is the total number of beats in the music, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq25"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/9572fc702e3a/d33e849.gif" loading="lazy" id="d33e849" alt="Inline graphic"></span> represents the timestamp at which the <em>i</em>-th motion event (or key frame of motion) occurs, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq26"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/c315cb176f62/d33e858.gif" loading="lazy" id="d33e858" alt="Inline graphic"></span> is the timestamp of the <em>i</em>-th musical beat, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq27"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/549c4fa8ad3b/d33e868.gif" loading="lazy" id="d33e868" alt="Inline graphic"></span> is a threshold that defines the maximum allowable deviation between the motion and beat timestamps for them to be considered aligned and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq28"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/b8461c166254/d33e874.gif" loading="lazy" id="d33e874" alt="Inline graphic"></span> is the indicator function that outputs 1 if the condition inside is true (i.e., the difference between <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq29"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/9572fc702e3a/d33e880.gif" loading="lazy" id="d33e880" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq30"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/c315cb176f62/d33e886.gif" loading="lazy" id="d33e886" alt="Inline graphic"></span> is within the allowed range), and 0 otherwise.</p></section><section id="Sec24"><h3 class="pmc_sec_title">Ablation study</h3>
<p id="Par43">This section presents an ablation study evaluating the performance of the proposed model across three key tasks: motion-music synchrony progression, multilabel dance classification, and dance quality estimation. Multiple figures are referenced throughout to support and visualize the analysis. Figure <a href="#Fig5" class="usa-link">5</a> illustrates the motion-music synchrony rates across different expertise levels and training iterations, providing insights into synchronization improvements among beginner, intermediate, and expert dancers. Figure <a href="#Fig6" class="usa-link">6</a> compares model architectures for multilabel dance classification accuracy across genres, choreographies, motion primitives, and expertise levels. Figure <a href="#Fig7" class="usa-link">7</a> presents model-wise performance for dance quality estimation based on negative log-likelihood loss, highlighting the capability of different architectures to handle aleatoric uncertainty. Lastly, Fig. <a href="#Fig8" class="usa-link">8</a> provides a comparative analysis of dance-music synchronization performance using MSE, assessing rhythmic alignment between motion and music. These visual results are discussed in detail in the following subsections.</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368089_41598_2025_16345_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/d61770785e0c/41598_2025_16345_Fig5_HTML.jpg" loading="lazy" id="MO6" height="160" width="668" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Motion-music synchrony rates across expertise levels on ImperialDance dataset. This figure presents the distribution of motion-music synchrony rates for dancers of varying expertise (beginner, intermediate, expert) across different attempts. Subfigures (<strong>a–c</strong>) show the synchrony rates for the first 10 attempts compared to the last 10 attempts for beginner, intermediate, and expert dancers, respectively. The progression in synchrony is evident for beginners and intermediates, with experts maintaining consistently high rates. Subfigure (<strong>d</strong>) compares the synchrony rates across all expertise levels, illustrating that as the expertise level increases, dancers demonstrate higher alignment rates and lower variability in synchronization.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368089_41598_2025_16345_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/69646697bf8e/41598_2025_16345_Fig6_HTML.jpg" loading="lazy" id="MO7" height="457" width="776" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Performance comparison for Task 1 (multilabel dance classification) on ImperialDance dataset. This MSR chart illustrates the performance of various model architectures on the task of multilabel dance classification, measured by the classification accuracy across 180 unique classes (genres, choreographies, expertise levels, and motion primitives).</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368089_41598_2025_16345_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/35f450ea0fe0/41598_2025_16345_Fig7_HTML.jpg" loading="lazy" id="MO8" height="394" width="668" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Performance Comparison of motion-music architectures for Task 2 (dance quality estimation) on ImperialDance dataset. It compares the performance of various motion-music architectures on Task 2: dance quality estimation, measured by the negative log-likelihood loss. Lower loss values indicate better accuracy in predicting the performance score distribution with aleatoric uncertainty.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig8"><h4 class="obj_head">Fig. 8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368089_41598_2025_16345_Fig8_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/daf4efebd7dc/41598_2025_16345_Fig8_HTML.jpg" loading="lazy" id="MO9" height="456" width="776" alt="Fig. 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Comparison of MSE Loss values for dance-music synchronization using various Motion-Music-Loss model combinations on ImperialDance dataset. This chart highlights the performance of different models in aligning dance motions with musical rhythm, where a lower Mean MSE indicates superior synchronization.</p></figcaption></figure><p id="Par44">We also conduct a modality ablation to understand the isolated and joint effects of motion and music features on the model’s performance. This helps quantify the benefit of multi-modal fusion compared to unimodal inputs.</p>
<section id="Sec25"><h4 class="pmc_sec_title">Motion-music synchrony evaluation</h4>
<p id="Par45">Our analysis tracks the progression of motion-music synchrony across different expertise levels (Beginner, Intermediate, Expert) by examining the alignment rates during two key periods: the first 10 attempts and the last 10 attempts of practice. The evaluation criterion used is the Motion-Music Synchrony Rate (MSR), which quantifies how well the dancers synchronize their movements with the music beats. We compute the average MSR for both the initial and final attempts for each expertise level, as shown in Fig. <a href="#Fig5" class="usa-link">5</a>. Additionally, we aggregate the MSR for all 100 attempts across the three expertise levels (Fig. <a href="#Fig5" class="usa-link">5</a>d).</p>
<p id="Par46">For Progression of synchrony for beginners and intermediates: In Fig. <a href="#Fig5" class="usa-link">5</a>a,b, the mean and median synchrony rates for the last 10 attempts are higher than those for the first 10 attempts, both for beginner and intermediate dancers. This indicates that these dancers improve their ability to synchronize with the music after repeated practice sessions. The greater consistency in the later attempts, especially for intermediates, suggests an enhanced ability to follow the rhythm after practice. By comparing Fig. <a href="#Fig5" class="usa-link">5</a>a,b, it is clear that intermediate dancers make more substantial progress than beginners. While beginners show improvement, intermediates exhibit a larger increase in synchrony, as reflected in their tighter distributions and higher median values. This outcome is expected, as intermediate dancers are typically more adept at quickly learning to align their movements with the music, leading to greater progression in their MSR over time.</p>
<p id="Par47">Figure <a href="#Fig5" class="usa-link">5</a>c reveals that expert dancers maintain a high and consistent MSR across both the first and last 10 attempts, with little to no progression identified. This is to be expected, as expert dancers already demonstrate near-optimal synchronization with the music from the beginning, indicating that additional practice sessions do not significantly impact their performance. As illustrated in Fig. <a href="#Fig5" class="usa-link">5</a>d, there is a clear increase in the mean and median synchrony rates as the expertise level rises from beginner to expert. Expert dancers exhibit the highest MSR, while beginners show the lowest rates and the greatest variability. The results highlight that, as dancers advance in skill level, their ability to synchronize with music improves, with experts maintaining high alignment consistently across all attempts. These findings confirm the ability of our model to capture the progression of dance performance, particularly for beginner and intermediate dancers, and underscore the stable performance of experts in maintaining high synchronization rates throughout the evaluation.</p></section><section id="Sec26"><h4 class="pmc_sec_title">Task 1: multilabel dance classification</h4>
<p id="Par48">In Task 1, the objective of multilabel dance classification focuses on identifying a variety of dance elements such as genres, choreographies, expertise levels, and motion primitives within a sequence. This task is modeled as a multi-label classification problem where the dataset is composed of 180 unique classes, derived from 20 choreographies, 3 expertise levels, and 3 primitive motions. The classification process is optimized during network training using the cross-entropy loss function, which aims to accurately categorize each label across the diverse set of dance sequences. Figure <a href="#Fig6" class="usa-link">6</a> presents the performance results for different combinations of motion-music architectures and loss functions, where higher scores (indicated by the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq31"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/1cdeeae8e817/d33e950.gif" loading="lazy" id="d33e950" alt="Inline graphic"></span> symbol) signify better performance in Task 1. The best-performing model is Ours (STGCN-LSTM-InfoNCE), which achieves a score of 75.20, the highest among all models. This score represents the classification accuracy achieved by the model across 180 unique classes, including genres, choreographies, expertise levels, and motion primitives. A higher accuracy score indicates better performance in correctly identifying the diverse attributes of the dance sequences.</p>
<p id="Par49">This suggests that the model effectively captures the intricate patterns within dance vocabulary through its unique combination of STGCN for spatiotemporal motion analysis, LSTM networks for sequence modeling, and the InfoNCE loss for robust contrastive learning. The Res18-LSTM-InfoNCE model, achieving a score of 61.62, represents the baseline performance. While this model uses ResNet-18 for feature extraction and LSTM for temporal analysis, its lower accuracy indicates that it struggles to fully capture the complex dance motions. Compared to this, Ours (STGCN-LSTM-InfoNCE) demonstrates a significant 22.02% improvement in performance, underscoring the value of the STGCN architecture in learning spatial-temporal dynamics of human motion, which are essential for dance recognition. Res50-LSTM-InfoNCE performs slightly better, with a score of 63.41, indicating a 2.91% improvement over Res18-LSTM-InfoNCE. This enhancement is likely due to the deeper feature extraction capabilities of ResNet-50. However, Ours still outperforms Res50-LSTM-InfoNCE by 18.57%, highlighting that the STGCN architecture paired with LSTM provides a better representation of dance movements, leading to higher classification accuracy.</p>
<p id="Par50">The models incorporating the STGCN (STGCN-Res18-InfoNCE and STGCN-Res50-InfoNCE) show marked improvements in performance. STGCN-Res18-InfoNCE, with a score of 67.35, achieves a 9.86% improvement over Res18-LSTM-InfoNCE, while STGCN-Res50-InfoNCE further increases the score to 70.24. This demonstrates the effectiveness of STGCN in capturing the intricate spatial-temporal relationships inherent in dance movements. Nevertheless, Ours (STGCN-LSTM-InfoNCE) still outperforms these models, showing a 7.07% improvement over STGCN-Res50-InfoNCE, which suggests that the integration of LSTM for sequence modeling, alongside InfoNCE loss, further refines the model’s ability to classify complex dance patterns. Lastly, STGCN-LSTM-SupCon<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup> achieves a score of 72.59, reflecting the advantages of using supervised contrastive learning (SupCon) to better distinguish between various dance labels. Despite this improvement, Ours (STGCN-LSTM-InfoNCE) still outperforms it by 3.59%, indicating that the combination of STGCN, LSTM, and InfoNCE is better suited for the nuanced task of multilabel dance classification. Overall, Ours (STGCN-LSTM-InfoNCE) demonstrates the best performance across all models, with an improvement range of 3.59% to 22.02% over other architectures. The results clearly show that this model, through its effective use of spatiotemporal graph networks, sequence modeling, and contrastive learning, is highly capable of recognizing various dance vocabulary elements. The <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq32"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/1cdeeae8e817/d33e971.gif" loading="lazy" id="d33e971" alt="Inline graphic"></span> symbol indicates that higher values reflect high accuracy as Fig. <a href="#Fig6" class="usa-link">6</a>.</p></section><section id="Sec27"><h4 class="pmc_sec_title">Task 2: dance quality estimation</h4>
<p id="Par51">In Task 2, the goal is to evaluate Dance Quality Estimation, where each dance sequence is annotated with a performance score by professional dancers. This task is designed to predict a score distribution rather than exact scores. The model learns to map dance features to a score distribution that incorporates aleatoric uncertainty, a key component that accounts for the inherent subjectivity in the labeling process. By modeling this uncertainty, the approach seeks to minimize bias during the labeling process. The scoring mechanism is framed using a Gaussian distribution, and the model is optimized to minimize the negative log-likelihood of predicting the target score distribution. In this context, a lower value (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq33"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e984.gif" loading="lazy" id="d33e984" alt="Inline graphic"></span> symbol) indicates better performance, reflecting the model’s ability to reduce errors in score prediction. Figure <a href="#Fig7" class="usa-link">7</a> presents the performance of different models in Task 2 based on their ability to minimize the negative log-likelihood loss. Ours (STGCN-LSTM-InfoNCE) achieves the lowest loss of 0.000775, outperforming all other models in this task. This loss value represents the negative log-likelihood loss, which quantifies the error in predicting the distribution of dance quality scores. A lower loss indicates that the model is better at estimating the score distribution while accounting for aleatoric uncertainty, which is essential for handling the subjectivity and variability in professional evaluations.</p>
<p id="Par52">This demonstrates its high accuracy in mapping the complex dance features to a performance score distribution. The comparison with other models highlights substantial performance improvements with the STGCN-LSTM-InfoNCE. Res18-LSTM-InfoNCE achieves a loss of 0.003471, which is significantly higher than Ours. This indicates that while this model benefits from combining ResNet-18 with LSTM, it struggles to accurately model the uncertainty in the score distribution. The STGCN-LSTM-InfoNCE achieves a 77.68% improvement over Res18-LSTM-InfoNCE, showing that the incorporation of STGCN, which captures spatial-temporal motion features, provides a significant advantage. Similarly, Res50-LSTM-InfoNCE shows a loss of 0.004425, performing worse than Res18-LSTM-InfoNCE. Despite the deeper architecture of ResNet-50, the model’s ability to handle aleatoric uncertainty in performance scoring remains limited. Ours demonstrates an 82.48% improvement over Res50-LSTM-InfoNCE, emphasizing that the integration of STGCN with LSTM offers a more refined modeling of temporal sequences in the context of performance scoring.</p>
<p id="Par53">Moving to the STGCN based models, STGCN-Res18-InfoNCE and STGCN-Res50-InfoNCE show losses of 0.005041 and 0.062043 respectively. While the inclusion of STGCN allows these models to capture spatial-temporal dynamics, they still fall short in handling the score distribution effectively. In comparison, Ours (STGCN-LSTM-InfoNCE) outperforms these models by 84.62% and 98.75%, respectively. The results demonstrate the importance of incorporating LSTM for sequence modeling, which helps in effectively reducing errors in the predicted score distribution. Lastly, STGCN-LSTM-SupCon achieves a loss of 0.008160, which, while an improvement over some other models, does not match the performance of Ours. The STGCN-LSTM-SupCon model uses supervised contrastive learning (SupCon) to distinguish between different features, but it is less effective than the InfoNCE loss when it comes to capturing the nuanced differences required for performance scoring. The 90.50% improvement by Ours over this model demonstrates the efficacy of InfoNCE in enhancing feature representation and handling the complexities of aleatoric uncertainty. Ours (STGCN-LSTM-InfoNCE) achieves the best performance with a loss of 0.000775, outperforming all other models by significant margins, with improvements ranging from 77.68 to 98.75%.</p></section><section id="Sec28"><h4 class="pmc_sec_title">Task 3: dance-music synchronization</h4>
<p id="Par54">In Task 3, the objective is to assess Dance-Music Synchronization, focusing on how well the dancer’s movements synchronize with the musical rhythm. This synchronization is quantified using the MSR, which measures how closely the intensity of a dancer’s motion matches the intensity of the accompanying music. The correlation between motion and music is considered successful when the peak difference between them does not exceed 0.4 s, consistent with the threshold adopted for MSR computation The MSE loss is employed as the evaluation metric, with lower values indicating better alignment between motion and music. Results for various models in Task 3. Ours (ST-GCN-LSTM-InfoNCE) are shown in Fig. <a href="#Fig8" class="usa-link">8</a> where our model has the lowest MSE of 2.52, indicating the highest level of synchronization between motion and music. This score reflects the MSE between the motion intensity peaks and musical beats. A lower MSE indicates better temporal alignment, meaning the dancer’s movements are more closely synchronized with the rhythm of the accompanying music.</p>
<p id="Par55">This demonstrates the superior ability of the ST-GCN-LSTM architecture combined with the InfoNCE loss function to model the temporal and rhythmic alignment required for dance performance. In comparison, the Res18-LSTM-InfoNCE model has an MSE of 3.11, which is 18.96% higher than Ours, suggesting that the addition of ST-GCN significantly improves performance in rhythmic tasks.</p>
<p id="Par56">Similarly, Res50-LSTM-InfoNCE performs with an MSE of 3.50, and Ours shows a 28.00% improvement over this model. Despite the deeper ResNet-50 architecture, the absence of ST-GCN results in less effective rhythm modeling. On the other hand, ST-GCN-Res18-InfoNCE and ST-GCN-Res50-InfoNCE perform similarly with MSE values of 3.77 and 3.72, respectively. Ours outperforms both models, improving their performance by 33.16% and 32.26%, further highlighting the importance of the LSTM component in capturing long-term dependencies in the dance sequences. The ST-GCN-LSTM-SupCon model demonstrates the highest error with an MSE of 5.28, showing that the supervised contrastive loss (SupCon) is less effective in this task. In comparison, Ours achieves a 52.27% improvement over ST-GCN-LSTM-SupCon, which underscores the advantage of using InfoNCE loss for feature representation in tasks requiring precise synchronization between motion and music. Ours (ST-GCN-LSTM-InfoNCE) significantly outperforms all other models in Task 3, with performance improvements ranging from 18.96 to 52.27%.</p>
<p id="Par57">The proposed model achieves a steady processing rate of approximately 29 FPS when evaluating complex dance sequences with high temporal and spatial resolutions. This performance is achieved through optimized GPU utilization, mixed-precision computations, and efficient data processing pipelines. At this rate, the framework provides near-real-time feedback, making it suitable for professional training and live performance monitoring scenarios. While not instantaneous, the 29 FPS rate ensures that the system can process inputs with minimal latency, enabling practical applications such as rehearsal evaluation and interactive coaching systems. Additionally, this performance reflects a balance between computational demands and output quality, addressing the challenges posed by large-scale motion-music data analysis.</p></section><section id="Sec29"><h4 class="pmc_sec_title">Zero-shot generalization across dance genres</h4>
<p id="Par58">To evaluate the generalization ability of the proposed model in zero-shot settings, we conducted a genre-based ablation experiment where the model was tested on entirely unseen dance genres. Specifically, we divided the ImperialDance dataset into two non-overlapping genre sets: five genres were used exclusively for training and prompt tuning, while the remaining five were held out for zero-shot evaluation. This ensures that the model was never exposed to any choreography, motion pattern, or music sequence from the test genres during the training phase.</p>
<p id="Par59">During evaluation, the model received prompt-based textual descriptions related to genre, choreography, and expertise level, but no parameter updates were performed. We assessed the performance of the model across all three downstream tasks: (i) Multilabel Dance Classification, (ii) Dance Quality Estimation, and (iii) Dance-Music Synchronization. Evaluation metrics included classification accuracy for Task 1, negative log-likelihood (NLL) for Task 2, and mean squared error (MSE) for Task 3. The results, summarized in Table <a href="#Tab1" class="usa-link">1</a>, demonstrate that the model maintains strong performance even on unseen genres. The classification accuracy on unseen genres decreased only marginally compared to the in-domain setting, and the degradation in NLL and MSE was minimal. These findings confirm that the integration of contrastive self-supervised learning and transformer-based prompt tuning facilitates effective generalization to novel dance styles without requiring additional fine-tuning.</p>
<section class="tw xbox font-sm" id="Tab1"><h5 class="obj_head">Table 1.</h5>
<div class="caption p"><p>Zero-shot generalization performance on unseen dance genres. The model was trained on five genres and evaluated on five held-out genres using prompt-based inference.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Setting</th>
<th align="left" colspan="1" rowspan="1">Task 1 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq34"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/1cdeeae8e817/d33e1038.gif" loading="lazy" id="d33e1038" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 2 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq35"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1046.gif" loading="lazy" id="d33e1046" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq36"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1054.gif" loading="lazy" id="d33e1054" alt="Inline graphic"></span>
</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">In-domain (all genres)</td>
<td align="left" colspan="1" rowspan="1">75.20</td>
<td align="left" colspan="1" rowspan="1">0.000775</td>
<td align="left" colspan="1" rowspan="1">2.52</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Zero-shot (unseen genres)</td>
<td align="left" colspan="1" rowspan="1">71.64</td>
<td align="left" colspan="1" rowspan="1">0.001023</td>
<td align="left" colspan="1" rowspan="1">2.91</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec30"><h4 class="pmc_sec_title">Ablation study on input modalities</h4>
<p id="Par60">To assess the contribution of each input modality, we conducted an ablation study comparing three model variants: motion-only, music-only, and the proposed multi-modal (motion+music) configuration. In the motion-only model, only the STGCN encoder was active, processing the skeletal motion features, while the LSTM music encoder was disabled. Conversely, the music-only model retained the LSTM music encoder and excluded the motion stream. The multi-modal model used both encoders with contrastive learning to jointly embed motion and music primitives. Table <a href="#Tab2" class="usa-link">2</a> presents the results across the three downstream tasks. As expected, the multi-modal model achieved the best performance across all tasks. Notably, the motion-only model performed relatively well on Task 1 (classification) and Task 2 (quality estimation), indicating that motion features alone contain rich information about choreography and expertise. However, its performance in Task 3 (synchronization) significantly degraded, highlighting its inability to capture alignment with musical beats.</p>
<section class="tw xbox font-sm" id="Tab2"><h5 class="obj_head">Table 2.</h5>
<div class="caption p"><p>Ablation study comparing the performance of motion-only, music-only, and multi-modal input configurations across three downstream tasks.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Model Variant</th>
<th align="left" colspan="1" rowspan="1">Task 1 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq37"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/1cdeeae8e817/d33e1109.gif" loading="lazy" id="d33e1109" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 2 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq38"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1117.gif" loading="lazy" id="d33e1117" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq39"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1125.gif" loading="lazy" id="d33e1125" alt="Inline graphic"></span>
</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Motion-only</td>
<td align="left" colspan="1" rowspan="1">68.84</td>
<td align="left" colspan="1" rowspan="1">0.002103</td>
<td align="left" colspan="1" rowspan="1">4.02</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Music-only</td>
<td align="left" colspan="1" rowspan="1">45.32</td>
<td align="left" colspan="1" rowspan="1">0.003961</td>
<td align="left" colspan="1" rowspan="1">3.88</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Motion+music (ours)</td>
<td align="left" colspan="1" rowspan="1"><strong>75.20</strong></td>
<td align="left" colspan="1" rowspan="1"><strong>0.000775</strong></td>
<td align="left" colspan="1" rowspan="1"><strong>2.52</strong></td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par61">In contrast, the music-only model yielded the lowest performance in Task 1, reflecting insufficient information for genre or choreography recognition when motion is excluded. Its performance on Task 3 (synchronization) was also weaker than the multi-modal setup, although marginally better than the motion-only model due to access to rhythmic cues. These results confirm that while each modality contributes uniquely to certain tasks, the integration of both is essential for achieving robust, generalizable performance across all evaluation dimensions. Hence, the multi-modal framework is not only superior in aggregate performance but also necessary for rhythm-sensitive evaluations such as synchronization.</p></section><section id="Sec31"><h4 class="pmc_sec_title">Ablation study on prompt tuning effectiveness</h4>
<p id="Par62">To evaluate the specific contribution of prompt tuning in our framework, we conducted a controlled ablation study comparing two model variants: (i) the full model with prompt tuning applied during downstream evaluation, and (ii) a baseline variant where no prompting was used and all downstream predictions relied solely on fixed motion and music encoders. Both variants were trained and evaluated under identical conditions on the ImperialDance dataset.</p>
<p id="Par63">Table <a href="#Tab3" class="usa-link">3</a> reports the results across the three downstream tasks. The model with prompt tuning demonstrates a consistent and notable performance advantage. For Task 1 (Multilabel Dance Classification), prompt tuning improves accuracy from 70.33 to 75.20, yielding a relative gain of 6.93%. In Task 2 (Dance Quality Estimation), the negative log-likelihood (NLL) loss is reduced from 0.001311 to 0.000775, reflecting a 40.89% improvement in modeling aleatoric uncertainty in performance scoring. Task 3 (Dance-Music Synchronization) also benefits, with the mean squared error (MSE) decreasing from 3.61 to 2.52, a gain of 30.19%. These results highlight the role of prompt tuning in enhancing the adaptability and generalization of the model across diverse downstream evaluation tasks. By providing task-specific textual cues, the model effectively aligns multimodal representations to produce more accurate and semantically meaningful predictions. This is especially impactful in scenarios involving diverse choreography styles and unseen input conditions, where traditional fine-tuning methods may struggle.</p>
<section class="tw xbox font-sm" id="Tab3"><h5 class="obj_head">Table 3.</h5>
<div class="caption p"><p>Ablation study quantifying the effect of prompt tuning on all three downstream tasks.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Model variant</th>
<th align="left" colspan="1" rowspan="1">Task 1 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq40"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/1cdeeae8e817/d33e1190.gif" loading="lazy" id="d33e1190" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 2 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq41"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1198.gif" loading="lazy" id="d33e1198" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq42"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1206.gif" loading="lazy" id="d33e1206" alt="Inline graphic"></span>
</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Without prompt tuning</td>
<td align="left" colspan="1" rowspan="1">70.33</td>
<td align="left" colspan="1" rowspan="1">0.001311</td>
<td align="left" colspan="1" rowspan="1">3.61</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">With prompt tuning (ours)</td>
<td align="left" colspan="1" rowspan="1"><strong>75.20</strong></td>
<td align="left" colspan="1" rowspan="1"><strong>0.000775</strong></td>
<td align="left" colspan="1" rowspan="1"><strong>2.52</strong></td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec32"><h4 class="pmc_sec_title">Ablation study on genre-specific robustness</h4>
<p id="Par64">To evaluate the model’s robustness across diverse music genres, we conducted an ablation study analyzing performance separately on five distinct musical genres from the AIST++ dataset: hip-hop, jazz, house, waacking, and break. For each genre, we assessed the performance of the full model (STGCN-LSTM-InfoNCE with prompt tuning) across the three downstream tasks: (i) multilabel dance classification, (ii) dance quality estimation, and (iii) dance-music synchronization. Table <a href="#Tab4" class="usa-link">4</a> summarizes the results. Notably, although slight performance fluctuations are observed due to rhythmic complexity (e.g., in jazz and break), the model consistently delivers strong results without retraining or genre-specific tuning. It validates the model’s genre-agnostic capability, attributable to (1) the contrastive learning strategy that enforces rhythm-aware but genre-invariant embeddings, and (2) the EBS method that normalizes rhythm structure across different musical contexts. The small variation in performance confirms the model’s generalization capacity across a wide range of musical styles, making it suitable for real-world applications involving heterogeneous dance-music inputs.</p>
<section class="tw xbox font-sm" id="Tab4"><h5 class="obj_head">Table 4.</h5>
<div class="caption p"><p>Genre-wise ablation study on AIST++ showing classification accuracy, NLL loss for quality estimation, and MSE for dance-music synchronization across five music genres.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Music genre</th>
<th align="left" colspan="1" rowspan="1">Task 1 Acc. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq43"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/1cdeeae8e817/d33e1260.gif" loading="lazy" id="d33e1260" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 2 NLL <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq44"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1268.gif" loading="lazy" id="d33e1268" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 3 MSE <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq45"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1276.gif" loading="lazy" id="d33e1276" alt="Inline graphic"></span>
</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Hip-Hop</td>
<td align="left" colspan="1" rowspan="1">74.63</td>
<td align="left" colspan="1" rowspan="1">0.000802</td>
<td align="left" colspan="1" rowspan="1">2.49</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Jazz</td>
<td align="left" colspan="1" rowspan="1">72.91</td>
<td align="left" colspan="1" rowspan="1">0.000847</td>
<td align="left" colspan="1" rowspan="1">2.60</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">House</td>
<td align="left" colspan="1" rowspan="1">75.88</td>
<td align="left" colspan="1" rowspan="1">0.000765</td>
<td align="left" colspan="1" rowspan="1">2.46</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Waacking</td>
<td align="left" colspan="1" rowspan="1">74.17</td>
<td align="left" colspan="1" rowspan="1">0.000793</td>
<td align="left" colspan="1" rowspan="1">2.54</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Break</td>
<td align="left" colspan="1" rowspan="1">73.54</td>
<td align="left" colspan="1" rowspan="1">0.000832</td>
<td align="left" colspan="1" rowspan="1">2.58</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Average</td>
<td align="left" colspan="1" rowspan="1">74.23</td>
<td align="left" colspan="1" rowspan="1">0.000808</td>
<td align="left" colspan="1" rowspan="1">2.53</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec33"><h4 class="pmc_sec_title">Real-time inference and deployment feasibility</h4>
<p id="Par65">Although the proposed framework integrates a combination of computationally intensive modules and includes STGCN for spatial-temporal skeletal modeling, LSTM for sequential music encoding, and transformer-based prompt system tuning is designed for efficiency during inference. In practical deployment, all encoders are frozen and used as feature extractors, while only lightweight MLP heads and prompt-encoded representations are actively involved in downstream evaluation. This design choice significantly reduces the computational overhead compared to traditional end-to-end training paradigms. To empirically validate the framework’s suitability for real-time or near-real-time applications, we measured the average inference speed across the three downstream tasks using the ImperialDance dataset on an NVIDIA Tesla V100 GPU (32 GB VRAM), with a batch size of 32 and mixed-precision (fp16) acceleration enabled. As shown in Table <a href="#Tab5" class="usa-link">5</a>, the model achieves an overall average throughput of 28.94 FPS, with task-specific frame rates ranging from 28.07 to 30.12 FPS.</p>
<section class="tw xbox font-sm" id="Tab5"><h5 class="obj_head">Table 5.</h5>
<div class="caption p"><p>Inference speed (in frames per second) across the three downstream tasks on an NVIDIA Tesla V100 GPU using mixed-precision and a batch size of 32. The model achieves near-real-time performance suitable for live coaching scenarios.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Task</th>
<th align="left" colspan="1" rowspan="1">Average inference speed (FPS)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Multilabel dance classification</td>
<td align="left" colspan="1" rowspan="1">30.12</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Dance quality estimation</td>
<td align="left" colspan="1" rowspan="1">28.07</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Dance-music synchronization</td>
<td align="left" colspan="1" rowspan="1">28.63</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Overall average</td>
<td align="left" colspan="1" rowspan="1">28.94</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par66">Further latency profiling indicates that over 91% of the inference time is consumed within GPU-forward passes through the frozen STGCN and LSTM encoders, as well as the transformer-based prompt module. This confirms that the majority of the model’s operations are GPU-optimized and do not involve expensive backpropagation or fine-tuning during evaluation. Moreover, the modular design facilitates pipeline parallelism and delayed batch processing, which can be used to further reduce perceived latency in interactive systems.</p></section><section id="Sec34"><h4 class="pmc_sec_title">Qualitative visualization of expertise-level distinctions</h4>
<p id="Par68">To complement the quantitative evaluation, we provide qualitative visualizations that highlight how the proposed model distinguishes between dancers of varying expertise levels. Figure <a href="#Fig9" class="usa-link">9</a> presents representative examples of dance motion trajectories for three expertise categories such as beginner, intermediate, and expert, based on 3D skeletal joint sequences. Each example is color-coded to indicate the velocity magnitude of motion across time, where warmer colors represent higher dynamic intensity.</p>
<figure class="fig xbox font-sm" id="Fig9"><h5 class="obj_head">Fig. 9.</h5>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12368089_41598_2025_16345_Fig9_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/6ee421b8f0f7/41598_2025_16345_Fig9_HTML.jpg" loading="lazy" id="MO10" height="255" width="739" alt="Fig. 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Qualitative visualization of 3D skeleton-based motion trajectories for (<strong>a</strong>) beginner, (<strong>b</strong>) intermediate, and (<strong>c</strong>) expert dancers. Color encodes joint velocity magnitude over time. Expert dancers demonstrate smoother, rhythmically aligned, and spatially consistent trajectories, while beginners exhibit erratic and less synchronized movements.</p></figcaption></figure><p id="Par69">As shown in Fig. <a href="#Fig9" class="usa-link">9</a>, beginner dancers exhibit more erratic and spatially dispersed trajectories with inconsistent intensity, reflecting lower control and synchronization. Intermediate dancers show moderate regularity with improved beat alignment and smoother transitions. Expert dancers display highly consistent motion paths, fluid transitions, and peak alignment with musical beats. These visual trends confirm that the model’s feature extraction pipeline (STGCN + LSTM + InfoNCE) captures the fine-grained spatial-temporal patterns that distinguish skill levels. Furthermore, attention maps from the transformer-based prompting module indicate higher activation around temporally coherent motion primitives in expert sequences, demonstrating the model’s ability to semantically align expertise with motion quality and rhythm fidelity.</p></section></section><section id="Sec35"><h3 class="pmc_sec_title">Performance comparison with SOTA models</h3>
<p id="Par70">We compare the performance of various SOTA methods with our proposed model (STGCN-LSTM-InfoNCE) across three different tasks and the results are shown in Table <a href="#Tab6" class="usa-link">6</a>. Task 1 evaluates multilabel dance classification, Task 2 measures dance quality estimation, and Task 3 assesses dance-music synchronization. The upward arrow (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq46"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/1cdeeae8e817/d33e1420.gif" loading="lazy" id="d33e1420" alt="Inline graphic"></span>) for Task 1 indicates that higher values represent better performance, while the downward arrow (<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq47"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1426.gif" loading="lazy" id="d33e1426" alt="Inline graphic"></span>) for Tasks 2 and 3 indicates that lower values signify better performance. Our model, STGCN-LSTM-InfoNCE, achieves a performance of 75.20 in Task 1, outperforming all other methods listed. When compared to CotrastiveDance, which is the second-best performing method in this task with a score of 68.21, our model shows an improvement of approximately 10.25%. Compared to SupCon<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>, which yields a score of 32.05, our model demonstrates a significantly higher performance with a relative improvement of about 134.7%. Methods like SimCLR <sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup> (36.29) and MoCo<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup> (14.37) show considerably lower performance, indicating the superiority of our approach in handling multi-label classification for dance genres, choreographies, and expertise levels.</p>
<section class="tw xbox font-sm" id="Tab6"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>Performance comparison of state-of-the-art models with our proposed STGCN-LSTM-InfoNCE model on the AIST++ dataset.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Method</th>
<th align="left" colspan="1" rowspan="1">Task 1 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq49"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/1cdeeae8e817/d33e1478.gif" loading="lazy" id="d33e1478" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 2 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq50"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1486.gif" loading="lazy" id="d33e1486" alt="Inline graphic"></span>
</th>
<th align="left" colspan="1" rowspan="1">Task 3 <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq51"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/3ad61e754407/d33e1494.gif" loading="lazy" id="d33e1494" alt="Inline graphic"></span>
</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">SimCLR<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">36.29</td>
<td align="left" colspan="1" rowspan="1">0.071</td>
<td align="left" colspan="1" rowspan="1">5.43</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MoCo<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">14.37</td>
<td align="left" colspan="1" rowspan="1">1.61<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq52"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/d59336ad438c/d33e1525.gif" loading="lazy" id="d33e1525" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">13.75</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SupCon<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">32.05</td>
<td align="left" colspan="1" rowspan="1">0.199</td>
<td align="left" colspan="1" rowspan="1">5.28</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CotrastiveDance<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">68.21</td>
<td align="left" colspan="1" rowspan="1">0.0098</td>
<td align="left" colspan="1" rowspan="1">4.91</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">STGCN<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>
</td>
<td align="left" colspan="1" rowspan="1">17.49</td>
<td align="left" colspan="1" rowspan="1">2.72<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq53"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/d59336ad438c/d33e1570.gif" loading="lazy" id="d33e1570" alt="Inline graphic"></span>
</td>
<td align="left" colspan="1" rowspan="1">11.04</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">STGCN-LSTM-InfoNCE (ours)</td>
<td align="left" colspan="1" rowspan="1"><strong>75.20</strong></td>
<td align="left" colspan="1" rowspan="1"><strong>0.000775</strong></td>
<td align="left" colspan="1" rowspan="1"><strong>2.52</strong></td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par71">For Task 2, where lower values are preferred due to the negative log-likelihood optimization, our model achieves an exceptionally low score of 0.000775, demonstrating its ability to effectively capture aleatoric uncertainty in dance quality estimation. Compared to CotrastiveDance, which scores 0.0098, our model improves the performance by 92.09%. The STGCN model<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>, scoring <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq48"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/57308ee3e7b9/d33e1451.gif" loading="lazy" id="d33e1451" alt="Inline graphic"></span>, also demonstrates a weaker performance in comparison to our model, further supporting the robustness of STGCN-LSTM-InfoNCE in addressing this task. Other methods like SupCon (0.199) and SimCLR (0.071) exhibit even poorer performance, indicating that these methods struggle with capturing the variability in performance scoring. In Task 3, which measures the alignment between motion and musical rhythm, our model achieves a score of 2.52, significantly lower than all other models, including CotrastiveDance<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>(4.91) and SimCLR (5.43). This indicates that our model excels in aligning dance motions with musical beats. The improvement over ContrastiveDance is approximately 48.67%, demonstrating the superior ability of our model to evaluate rhythm synchrony. The other methods like MoCo (13.75) and STGCN (11.04) display even higher losses, indicating their inability to effectively model this task.</p></section><section id="Sec36"><h3 class="pmc_sec_title">Discussion</h3>
<section id="Sec37"><h4 class="pmc_sec_title">Computational cost and resource requirements</h4>
<p id="Par72">To evaluate the feasibility and deployment potential of the proposed framework, we report the computational cost in terms of training time, inference speed, and hardware requirements. <em>Training time:</em> the complete training process spanned 100 epochs over approximately 90,000 training sequences. With a batch size of 32 and an initial learning rate of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq54"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/edd3/12368089/9282db406ceb/d33e1601.gif" loading="lazy" id="d33e1601" alt="Inline graphic"></span>, using cosine annealing for dynamic learning rate scheduling, the model required approximately 16 min per epoch. This resulted in a total pre-training time of roughly 26–27 h An additional 4–5 h were required for prompt tuning across the downstream tasks. Inference speed: the model achieves an average inference speed of 28.94 frames per second (FPS) across the three downstream tasks: multilabel dance classification (30.12 FPS), dance quality estimation (28.07 FPS), and dance-music synchronization (28.63 FPS). This performance enables near-real-time feedback, making the framework suitable for live dance coaching and rehearsal evaluation applications.</p>
<p id="Par73">Hardware requirements: all training and evaluation were conducted on a high-performance machine equipped with an NVIDIA Tesla V100 GPU (32 GB VRAM), Intel Xeon Gold 6226R CPU, and 128 GB of RAM. The implementation was based on PyTorch v1.12.1 with mixed-precision (fp16) enabled to optimize GPU memory usage and computational throughput. Resource optimization: during inference, the STGCN and LSTM encoders, along with the transformer-based text prompt module, operate in evaluation mode with frozen parameters. Only the task-specific MLP heads are actively updated or evaluated, resulting in significantly reduced computational overhead. Latency profiling indicates that over 91% of inference time is attributed to GPU-forward passes through the frozen encoders and transformer blocks, confirming the pipeline’s efficiency under GPU acceleration. Scalability: the modular nature of the framework allows for deployment in resource-constrained environments by replacing heavy encoders with lightweight alternatives or using encoder pruning. This extensibility supports practical applications in mobile AR/VR systems, interactive choreography tools, and real-time dance performance monitoring.</p></section><section id="Sec38"><h4 class="pmc_sec_title">Limitations</h4>
<p id="Par74">While the proposed framework demonstrates significant advancements in multi-modal dance performance evaluation, several aspects warrant further exploration to enhance its applicability and effectiveness. A primary limitation lies in the generalizability of the model to more diverse datasets that encompass unconventional or experimental dance styles. Current validation has been performed on datasets with predefined genres and choreographies, which may not fully represent the complexities and variabilities found in real-world scenarios. Future work could focus on training and testing the model on datasets with greater diversity to ensure robustness across a wider range of applications.</p>
<p id="Par75">Another limitation pertains to the model’s ability to handle complex synchronization scenarios, such as performances involving irregular or polyrhythmic music. While the model excels in rhythm-based synchronization tasks, assessing and adapting to these more intricate temporal structures remains a challenge. Future research could explore advanced techniques for handling such musical complexities to further refine the model’s synchronization capabilities.</p>
<p id="Par76">Although the model demonstrates promising generalization to unseen choreographies from held-out genres, its performance may degrade on highly unconventional or experimental dance styles that diverge significantly from the training data distribution. Future work may include domain adaptation or meta-learning approaches to further enhance generalization.</p>
<p id="Par77">In terms of real-world deployment, particularly within immersive augmented and virtual reality (AR/VR) environments, several practical constraints must be addressed. For instance, AR/VR systems require real-time processing of high-fidelity 3D motion data, which presents substantial computational and latency challenges. The current framework, while capable of near-real-time inference on 2D skeleton and audio inputs, would require significant architectural modifications to handle continuous 3D skeletal streams, depth-aware context, and interaction-based evaluation. Integrating real-time motion capture from AR/VR sensors introduces potential noise, occlusions, and incomplete data frames that the current model is not explicitly designed to handle. Moreover, maintaining synchronization between the rendered virtual environment and live motion-music analysis adds further temporal constraints that exceed the current system’s latency tolerance. Future extensions could investigate lightweight transformer variants, real-time 3D mesh encoders, and multi-threaded streaming pipelines to support efficient deployment in AR/VR coaching or choreography tools.</p>
<p id="Par78">Additionally, real-world dance performances are often affected by occlusion or missing motion data due to overlapping dancers or suboptimal camera angles. The current framework does not explicitly address these challenges, which could impact performance evaluation in practical scenarios. Incorporating robust motion completion strategies and sensor fusion techniques may help improve the system’s resilience in such deployment settings.</p></section></section></section><section id="Sec39"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par79">In this article, we introduced a novel Transformer-based Visual-Language framework designed for multi-modal dance performance evaluation and progression monitoring. Our approach addresses key challenges in the automatic assessment of complex dance movements and music synchronization, which are crucial for professional dance training environments. By integrating SpatioTemporal GCN and LSTM network with a contrastive self-supervised learning strategy, our model effectively captures and represents the intricate spatial-temporal dynamics of dance motion and music features. Additionally, we introduced a transformer-based text prompting mechanism to handle downstream multi-task evaluations, including multilabel dance classification, dance quality estimation, and dance-music synchronization. We also presented the ImperialDance dataset, a unique and comprehensive dataset designed to support multi-task dance performance assessment, motion-music analysis, and skill progression monitoring. Through extensive experimentation and evaluation on both the ImperialDance and AIST++ datasets, our proposed model outperformed existing methods across all three tasks, with significant improvements in classification accuracy, reduction in error for performance scoring, and superior alignment in dance-music synchronization. The results obtained indicate that our model’s ability to effectively learn and represent dance and music primitives makes it highly suitable for real-world applications, such as virtual dance training and automated performance evaluation. By addressing the limitations of existing approaches and demonstrating substantial performance gains, our framework sets a new standard for multi-modal dance performance evaluation, offering a scalable and adaptable solution for professional and amateur dancers alike.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>L.C. conceived the framework, designed and conducted the experiments, and analyzed the results. L.C. also developed the manuscript and reviewed its final version. All aspects of the research, including problem formulation, data preprocessing, model development, evaluation, and writing, were solely performed by L.C.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The datasets analyzed during the current study are publicly available. The AIST++ Dataset is available at <a href="https://google.github.io/aistplusplus_dataset/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://google.github.io/aistplusplus_dataset/</a>, and the ImperialDance dataset is available at <a href="https://github.com/YunZhongNikki/ImperialDance-Dataset" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/YunZhongNikki/ImperialDance-Dataset</a>. These resources were used to evaluate the proposed model and are accessible for research purposes.</p></section><section id="notes3"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par83">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Chen, K. et al. Choreomaster: Choreography-oriented music-driven dance synthesis. <em>ACM Trans. Graph. (TOG)</em><strong>40</strong>, 1–3 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20K.%20et%20al.%20Choreomaster:%20Choreography-oriented%20music-driven%20dance%20synthesis.%20ACM%20Trans.%20Graph.%20(TOG)40,%201%E2%80%933%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Jaque, S. V. et al. Creative flow and physiologic states in dancers during performance. <em>Front. Psychol.</em><strong>11</strong>, 1000 (2020).
</cite> [<a href="https://doi.org/10.3389/fpsyg.2020.01000" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC7266962/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32528376/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Jaque,%20S.%20V.%20et%20al.%20Creative%20flow%20and%20physiologic%20states%20in%20dancers%20during%20performance.%20Front.%20Psychol.11,%201000%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Liu, D. et al. Towards unified surgical skill assessment. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 9522–9531 (2021).</cite>
</li>
<li id="CR4">
<span class="label">4.</span><cite>Xu, J. et al. Finediving: A fine-grained dataset for procedure-aware action quality assessment. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2949–2958 (2022).</cite>
</li>
<li id="CR5">
<span class="label">5.</span><cite>Zeng, L. A. et al. Hybrid dynamic-static context-aware attention network for action assessment in long videos. In <em>Proceedings of the 28th ACM International Conference on Multimedia</em>. 2526–2534 (2020).</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>Siyao, L. et al. Bailando: 3D dance generation by actor-critic GPT with choreographic memory. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 11050–11059 (2022).</cite>
</li>
<li id="CR7">
<span class="label">7.</span><cite>Zhong, Y., Zhang, F. &amp; Demiris, Y. Contrastive self-supervised learning for automated multi-modal dance performance assessment. In <em>ICASSP 2023 - IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. 1–5 (2023).</cite>
</li>
<li id="CR8">
<span class="label">8.</span><cite>Zhong, Y. &amp; Demiris, Y. Dancemvp: Self-supervised learning for multi-task primitive-based dance performance assessment via transformer text prompting. <em>Proc. AAAI Conf. Artif. Intell.</em><strong>38</strong>, 10270–10278 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhong,%20Y.%20&amp;%20Demiris,%20Y.%20Dancemvp:%20Self-supervised%20learning%20for%20multi-task%20primitive-based%20dance%20performance%20assessment%20via%20transformer%20text%20prompting.%20Proc.%20AAAI%20Conf.%20Artif.%20Intell.38,%2010270%E2%80%9310278%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>of Dance, R. A. QCF Examinations Information, Rules and Regulations - Royal Academy of Dance (2020). Available online.</cite>
</li>
<li id="CR10">
<span class="label">10.</span><cite>Yan, S., Xiong, Y. &amp; Lin, D. Spatial temporal graph convolutional networks for skeleton-based action recognition. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>. Vol. 32 (2018).</cite>
</li>
<li id="CR11">
<span class="label">11.</span><cite>Sun, L. et al. Lattice long short-term memory for human action recognition. In <em>Proceedings of the IEEE International Conference on Computer Vision</em>. 2147–2156 (2017).</cite>
</li>
<li id="CR12">
<span class="label">12.</span><cite>Jia, M. et al. Visual prompt tuning. In <em>European Conference on Computer Vision</em>. 709–727 (Springer, 2022).</cite>
</li>
<li id="CR13">
<span class="label">13.</span><cite>Li, B., Zhao, Y., Zhelun, S. &amp; Sheng, L. Danceformer: Music conditioned 3D dance generation with parametric motion transformer. <em>Proc. AAAI Conf. Artif. Intell.</em><strong>36</strong>, 1272–1279 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20B.,%20Zhao,%20Y.,%20Zhelun,%20S.%20&amp;%20Sheng,%20L.%20Danceformer:%20Music%20conditioned%203D%20dance%20generation%20with%20parametric%20motion%20transformer.%20Proc.%20AAAI%20Conf.%20Artif.%20Intell.36,%201272%E2%80%931279%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Persine, S. et al. Walking abilities improvements are associated with pelvis and trunk kinematic adaptations in transfemoral amputees after rehabilitation. <em>Clin. Biomech.</em><strong>94</strong>, 105619 (2022).</cite> [<a href="https://doi.org/10.1016/j.clinbiomech.2022.105619" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35306365/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Persine,%20S.%20et%20al.%20Walking%20abilities%20improvements%20are%20associated%20with%20pelvis%20and%20trunk%20kinematic%20adaptations%20in%20transfemoral%20amputees%20after%20rehabilitation.%20Clin.%20Biomech.94,%20105619%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR15">
<span class="label">15.</span><cite>Rekik, G. et al. Acquiring basketball plays through varied speeds of video demonstration: Effect of time of day. <em>Chronobiol. Int.</em><strong>41</strong>, 1093–1103 (2024).
</cite> [<a href="https://doi.org/10.1080/07420528.2024.2379579" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39037116/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Rekik,%20G.%20et%20al.%20Acquiring%20basketball%20plays%20through%20varied%20speeds%20of%20video%20demonstration:%20Effect%20of%20time%20of%20day.%20Chronobiol.%20Int.41,%201093%E2%80%931103%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>Mukhtar, H. &amp; Khan, M. U. Stmmot: Advancing multi-object tracking through spatiotemporal memory networks and multi-scale attention pyramids. <em>Neural Netw.</em><strong>168</strong>, 363–379 (2023).
</cite> [<a href="https://doi.org/10.1016/j.neunet.2023.09.047" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37801917/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Mukhtar,%20H.%20&amp;%20Khan,%20M.%20U.%20Stmmot:%20Advancing%20multi-object%20tracking%20through%20spatiotemporal%20memory%20networks%20and%20multi-scale%20attention%20pyramids.%20Neural%20Netw.168,%20363%E2%80%93379%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR17">
<span class="label">17.</span><cite>Harris, F. J. Multirate Signal Processing for Communication Systems (River Publishers, 2022).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>Jian, Z. et al. Multitask learning for video-based surgical skill assessment. In <em>2020 Digital Image Computing: Techniques and Applications (DICTA)</em>. 1–8 (IEEE, 2020).</cite>
</li>
<li id="CR19">
<span class="label">19.</span><cite>Tang, Y. et al. Uncertainty-aware score distribution learning for action quality assessment. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 9839–9848 (2020).</cite>
</li>
<li id="CR20">
<span class="label">20.</span><cite>Ren, X., Li, H., Huang, Z. &amp; Chen, Q. Self-supervised dance video synthesis conditioned on music. In <em>Proceedings of the 28th ACM International Conference on Multimedia</em>. 46–54 (2020).</cite>
</li>
<li id="CR21">
<span class="label">21.</span><cite>Zhang, F. &amp; Demiris, Y. Learning garment manipulation policies toward robot-assisted dressing. <em>Sci. Robot.</em><strong>7</strong>, eabm6010 (2022).</cite> [<a href="https://doi.org/10.1126/scirobotics.abm6010" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35385294/" class="usa-link">PubMed</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Chen, T., Kornblith, S., Norouzi, M. &amp; Hinton, G. A simple framework for contrastive learning of visual representations. In <em>International Conference on Machine Learning</em>. 1597–1607 (PMLR, 2020).</cite>
</li>
<li id="CR23">
<span class="label">23.</span><cite>Thoker, F. M., Doughty, H. &amp; Snoek, C. G. Skeleton-contrastive 3D action representation learning. In <em>Proceedings of the 29th ACM International Conference on Multimedia</em>. 1655–1663 (2021).</cite>
</li>
<li id="CR24">
<span class="label">24.</span><cite>Radford, A. et al. Learning transferable visual models from natural language supervision. In <em>International Conference on Machine Learning</em>. 8748–8763 (PMLR, 2021).</cite>
</li>
<li id="CR25">
<span class="label">25.</span><cite>Dong, H. W., Chen, K., Dubnov, S., McAuley, J. &amp; Berg-Kirkpatrick, T. Multitrack music transformer. In <em>ICASSP 2023 - IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. 1–5 (2023).</cite>
</li>
<li id="CR26">
<span class="label">26.</span><cite>McFee, B. et al. librosa: Audio and music signal analysis in python. In <em>SciPy</em>. 18–24 (2015).</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Wu, Z. et al. A comprehensive survey on graph neural networks. <em>IEEE Trans. Neural Netw. Learn. Syst.</em><strong>32</strong>, 4–24 (2020).</cite> [<a href="https://doi.org/10.1109/TNNLS.2020.2978386" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/32217482/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wu,%20Z.%20et%20al.%20A%20comprehensive%20survey%20on%20graph%20neural%20networks.%20IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.32,%204%E2%80%9324%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Abdul, Z. K. &amp; Al-Talabani, A. K. Mel frequency cepstral coefficient and its applications: A review. <em>IEEE Access</em><strong>10</strong>, 122136–122158 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Abdul,%20Z.%20K.%20&amp;%20Al-Talabani,%20A.%20K.%20Mel%20frequency%20cepstral%20coefficient%20and%20its%20applications:%20A%20review.%20IEEE%20Access10,%20122136%E2%80%93122158%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29">
<span class="label">29.</span><cite>Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In <em>Advances in Neural Information Processing Systems</em>. Vol. 29 (2016).</cite>
</li>
<li id="CR30">
<span class="label">30.</span><cite>Davis, A. &amp; Agrawala, M. Visual rhythm and beat. <em>ACM Trans. Graph. (TOG)</em><strong>37</strong>, 1–1 (2018).</cite> [<a href="https://scholar.google.com/scholar_lookup?Davis,%20A.%20&amp;%20Agrawala,%20M.%20Visual%20rhythm%20and%20beat.%20ACM%20Trans.%20Graph.%20(TOG)37,%201%E2%80%931%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Li, R., Yang, S., Ross, D. A. &amp; Kanazawa, A. AI choreographer: Music conditioned 3D dance generation with AIST++. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 13401–13412 (2021).</cite>
</li>
<li id="CR32">
<span class="label">32.</span><cite>Zhuang, W. et al. Music2dance: Dancenet for music-driven dance generation. In <em>ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</em>. Vol. 18. 1–21 (2022).</cite>
</li>
<li id="CR33">
<span class="label">33.</span><cite>Dai, F. et al. A choreography analysis approach for microservice composition in cyber-physical-social systems. <em>IEEE Access</em><strong>8</strong>, 53215–53222 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Dai,%20F.%20et%20al.%20A%20choreography%20analysis%20approach%20for%20microservice%20composition%20in%20cyber-physical-social%20systems.%20IEEE%20Access8,%2053215%E2%80%9353222%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR34">
<span class="label">34.</span><cite>Khosla, P. et al. Supervised contrastive learning. <em>Adv. Neural Inf. Process. Syst.</em><strong>33</strong>, 18661–18673 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Khosla,%20P.%20et%20al.%20Supervised%20contrastive%20learning.%20Adv.%20Neural%20Inf.%20Process.%20Syst.33,%2018661%E2%80%9318673%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>He, K., Fan, H., Wu, Y., Xie, S. &amp; Girshick, R. Momentum contrast for unsupervised visual representation learning. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 9729–9738 (2020).</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets analyzed during the current study are publicly available. The AIST++ Dataset is available at <a href="https://google.github.io/aistplusplus_dataset/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://google.github.io/aistplusplus_dataset/</a>, and the ImperialDance dataset is available at <a href="https://github.com/YunZhongNikki/ImperialDance-Dataset" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://github.com/YunZhongNikki/ImperialDance-Dataset</a>. These resources were used to evaluate the proposed model and are accessible for research purposes.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-16345-2"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_16345.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (2.8 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12368089/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12368089/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12368089%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12368089/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12368089/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12368089/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40835876/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12368089/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40835876/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12368089/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12368089/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="pud4e4K57LptSa30Hovd0jQbpeuLwFStaIvqMA7flQk1nmEaGIX2vFuuP5e0ooei">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
