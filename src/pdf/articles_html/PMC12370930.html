
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            High order Interaction and Wavelet Convolution Network for visible infrared person reidentification - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="AE532D3A8AF202F3052D3A0050F0D7FC.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370930/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="High order Interaction and Wavelet Convolution Network for visible infrared person reidentification">
<meta name="citation_author" content="Li Ma">
<meta name="citation_author_institution" content="College of Communication and Information Engineering, Xi’an University of Science and Technology, Xi’an, 710054 China">
<meta name="citation_author" content="Rui Kong">
<meta name="citation_author_institution" content="College of Communication and Information Engineering, Xi’an University of Science and Technology, Xi’an, 710054 China">
<meta name="citation_author" content="XinGuan Dai">
<meta name="citation_author_institution" content="College of Communication and Information Engineering, Xi’an University of Science and Technology, Xi’an, 710054 China">
<meta name="citation_publication_date" content="2025 Aug 21">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30718">
<meta name="citation_doi" content="10.1038/s41598-025-14978-x">
<meta name="citation_pmid" content="40841746">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370930/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370930/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370930/pdf/41598_2025_Article_14978.pdf">
<meta name="description" content="Visible-infrared person re-identification (VI-ReID) remains a challenging task due to significant cross-modal discrepancies and poor image quality. While existing methods predominantly employ deep and complex neural networks to extract shared ...">
<meta name="og:title" content="High order Interaction and Wavelet Convolution Network for visible infrared person reidentification">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Visible-infrared person re-identification (VI-ReID) remains a challenging task due to significant cross-modal discrepancies and poor image quality. While existing methods predominantly employ deep and complex neural networks to extract shared ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370930/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12370930">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-14978-x"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_14978.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370930%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12370930/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12370930/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370930/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 21;15:30718. doi: <a href="https://doi.org/10.1038/s41598-025-14978-x" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-14978-x</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>High order Interaction and Wavelet Convolution Network for visible infrared person reidentification</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ma%20L%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Li Ma</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Li Ma</span></h3>
<div class="p">
<sup>1</sup>College of Communication and Information Engineering, Xi’an University of Science and Technology, Xi’an, 710054 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Ma%20L%22%5BAuthor%5D" class="usa-link"><span class="name western">Li Ma</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kong%20R%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Rui Kong</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Rui Kong</span></h3>
<div class="p">
<sup>1</sup>College of Communication and Information Engineering, Xi’an University of Science and Technology, Xi’an, 710054 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kong%20R%22%5BAuthor%5D" class="usa-link"><span class="name western">Rui Kong</span></a>
</div>
</div>
<sup>1,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Dai%20X%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">XinGuan Dai</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">XinGuan Dai</span></h3>
<div class="p">
<sup>1</sup>College of Communication and Information Engineering, Xi’an University of Science and Technology, Xi’an, 710054 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Dai%20X%22%5BAuthor%5D" class="usa-link"><span class="name western">XinGuan Dai</span></a>
</div>
</div>
<sup>1</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>College of Communication and Information Engineering, Xi’an University of Science and Technology, Xi’an, 710054 China </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 May 23; Accepted 2025 Aug 5; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12370930  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40841746/" class="usa-link">40841746</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Visible-infrared person re-identification (VI-ReID) remains a challenging task due to significant cross-modal discrepancies and poor image quality. While existing methods predominantly employ deep and complex neural networks to extract shared cross-modal features, these approaches inevitably discard critical primitive features during high-level feature abstraction. To address this limitation, we propose the High-order Interaction and Wavelet Convolution Network (HIW-Net) that systematically integrates primitive features at multiple feature interaction stages, thereby compensating for information loss in High-order representations. Furthermore, our framework uses wavelet convolution to mine more diverse features and solve the problem of insufficient feature extraction. We create the RegDB_shape datasets with the help of the Segment Anything Model(SAM) tool to supplement the training set. Extensive experiments on the SYSU-MM01 and RegDB datasets show the superiority of the proposed HIW-Net over several other state-of-the-art methods, proves the effectiveness of this method.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Visible-infrared person re-identification, High-order interaction, Wavelet convolution, Deep learning</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Computer science, Information technology, Software</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Person Re-identification (Re-ID) is a computer vision task aimed at identifying the same individual by analyzing and comparing images captured at different times, from varying perspectives, and across different cameras. This technology is widely used in security monitoring, intelligent transportation, and smart retail<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a>-<a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>. Currently, methods such as QAConv-GS, AKA, SPT, FastReID, and CLIP-ReID have demonstrated strong performance in person re-identification within the visible light spectrum<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a>-<a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>. However, in practical scenarios, challenges such as poor lighting conditions, complex environments, weather variations, and day-night transitions hinder effective person re-identification based solely on visible light images. Infrared cameras, thermal images, and other devices can ignore these effects and image the human body. Therefore, many scholars are actively exploring cross-modal person re-identification between visible light and infrared modalities, proposing methods such as YYDS + CMKR, DEN, NFS, and FMCNet<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a>-<a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>, which aim to develop invariant feature representations across modalities and camera perspectives.</p>
<p id="Par4">The difference between visible light and infrared images represents a significant challenge in this domain. Visible light images primarily rely on visible light reflected by objects, while infrared images depend on thermal radiation emitted by objects, resulting in substantial differences between the two modalities in color, texture, and contrast<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a>-<a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>. As shown in Fig. <a href="#Fig1" class="usa-link">1</a>, while visible and infrared images contain identity features of the same person, the identity features of the same person in the two modalities are different, but also have same parts. (represented by the white triangle region in Fig. <a href="#Fig1" class="usa-link">1</a>, referred to as shared features in this paper). The main challenge is to learn these features effectively. We divide existing methods into two categories, nongenerative-based and generative-based.</p>
<figure class="fig xbox font-sm" id="Fig1"><h3 class="obj_head">Fig. 1.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370930_41598_2025_14978_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/93c5be02bfad/41598_2025_14978_Fig1_HTML.jpg" loading="lazy" id="MO1" height="365" width="709" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Visualization of modal differences. The diamonds represent the identity feature representations in the visible light modality, and the triangles represent the identity feature representations in the infrared modality. In the overlap of the schematics for both modalities, the white triangle region indicates shared invariant features of the same person across different modalities, while the non-overlapping regions show the differences in identity feature representations between the visible light and infrared modalities.</p></figcaption></figure><p id="Par5">Generative methods such as the GECNet network, GECNet uses grayscale transformation to make visible and infrared images similar<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>. Some methods employ GANs for modal transformation, converting images from one modality into another to minimize modal differences<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a>-<a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>. In this field, VIS-IR image pairs are often essential for guiding modality generation, but some mainstream datasets, such as the SYSU-MM01 dataset, do not fulfill this requirement. The absence of VIS-IR image pairs can introduce noise into the generated images. Additionally, generative methods have notable limitations in practical applications.</p>
<p id="Par6">Non-generative approaches embed features from different modalities into a unified feature space<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a>,<a href="#CR20" class="usa-link" aria-describedby="CR20">20</a>,<a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>. However, this process may discard certain features from visible and infrared modalities, including critical details essential for person re-identification. Most existing methods focus on reducing modal differences, such as bidirectional modality information interaction network using of Dynamic Aggregation (DA) module<sup><a href="#CR54" class="usa-link" aria-describedby="CR54">54</a></sup> and modality shared-specific features cooperative separation network<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>, which are represented in Fig. <a href="#Fig1" class="usa-link">1</a> as increasing overlapping white triangular areas. Although these approaches achieve promising results, they ignore feature loss at deeper network stages and fail to fully utilize shared features.</p>
<p id="Par8">The HIW network proposed in this paper not only increases shared features but also focuses on their effective utilization. It consists of a third-order primitive feature interaction module (TPFI) and a diversified feature mining module based on wavelet convolution (wtDFM). Firstly, to reduce the differences between different modalities, the TPFI module is used to interact the features of different modalities through channel and spatial dimensions. At the same time, to reduce the potential loss of features during the network extraction process, the primitive features, low order features, and High-order features are aggregated separately in the channel and spatial dimensions. Low order and original features are used to compensate for High-order features, ensuring sufficient extraction of shared features, and reducing the redundant parameter amount of aggregation in the entire stage. Then, the wtDFE module applies wavelet convolution branches with different receptive fields to the features after TPFI. This enhances the utilization of shared features and explores diversified features, including shape, color, texture, and more abstract attributes(as shown in Fig. <a href="#Fig2" class="usa-link">2</a>).</p>
<figure class="fig xbox font-sm" id="Fig2"><h3 class="obj_head">Fig. 2.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370930_41598_2025_14978_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/3950dc5bb6de/41598_2025_14978_Fig2_HTML.jpg" loading="lazy" id="MO2" height="365" width="794" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>HIW-Net module function diagram.</p></figcaption></figure><p id="Par9">As shown in Fig. <a href="#Fig2" class="usa-link">2</a>, to achieve visible-infrared person re-identification and address the issues of insufficient feature extraction and underutilization of features in existing methods, the proposed HIW network introduces two novel modules: TPFI and wtDFM. In addition, inspired by the SGIEL<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>, where shape, as an invariant shared feature across modalities, is the key information to achieve person re-recognition. SGIEL uses orthogonal projections to eliminate shape information in one projection, which is used to force the network to learn features other than shape. In this paper, the opposite idea is taken, where only shape information is retained and used to force the network to enhance the learning of shape features. In the first stage, the network inputs images that contain all the information, and in the second stage the network inputs images that contain only shape information, by weighing the shape loss from the second stage with the loss from the first stage, and fed back into the training process, the network is trained to better utilize shape features.</p>
<p id="Par11">In addition, the low quality of the RegDB datasets hinders the effective creation of shape diagrams using existing human body analysis networks<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a>,<a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>. To address this, this paper uses SAM to annotate the datasets and construct the RegDB_shape datasets<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>. As shown in Fig. <a href="#Fig3" class="usa-link">3</a>, the newly created datasets addresses issues of incomplete person profiles and offers valuable support for research in person re-identification.</p>
<figure class="fig xbox font-sm" id="Fig3"><h3 class="obj_head">Fig. 3.</h3>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/6bb96c0132ef/41598_2025_14978_Fig3_HTML.jpg" loading="lazy" id="MO3" height="557" width="787" alt="Fig. 3"></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Comparison of shape diagrams from the RegDB_shape dataset generated by SCHP and manually annotated diagrams from this paper.</p></figcaption></figure><p id="Par12">The main contributions are as follows:</p>
<ol class="list" style="list-style-type:decimal">
<li><p id="Par14">This paper proposes a novel third-order primitive feature interaction module (TPFI) that minimizes inter-modal differences through channel and spatial dimension interactions while mitigating shared feature loss through third-order feature aggregation.</p></li>
<li><p id="Par15">The Wavelet convolution was introduced to achieve diversified feature mining. The proposed wtDFM module utilizes different wavelet convolution branches to fully exploit shared features.</p></li>
<li><p id="Par16">The HIW network integrates the strengths of TPFI and wtDFM to enhance cross-modal shared feature representation and maximize the utilization of shared features. In addition, shape loss and modality loss were weighed to optimize the total loss and enhance the utilization of shape features.</p></li>
<li><p id="Par17">This paper creates RegDB_shape datasets to advance research in the field of person re-identification.</p></li>
<li><p id="Par18">Extensive experiments demonstrate that the HIW network outperforms other networks on the mainstream SYSU-MM01 and RegDB datasets.</p></li>
</ol></section><section id="Sec2"><h2 class="pmc_sec_title">Related work</h2>
<p id="Par19">Current visible-infrared person re-identification (VI-ReID) methods can be categorized into generative and non-generative approaches<sup><a href="#CR55" class="usa-link" aria-describedby="CR55">55</a></sup>. Generative models primarily employ generative adversarial networks (GAN) or encoder-decoder modules to facilitate mutual conversion between the two modalities or to create intermediate ones. Sometimes, they also require the fusion of visible and infrared images, such as the SIHP network<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>, which uses corresponding visible and infrared images to explore complex interactions in image fusion. However, generative methods need VIS-IR image pairs, which have certain limitations in practical applications. Therefore, this paper mainly focuses on non-generative methods.</p>
<p id="Par20">This paper focuses on enhancing the baseline AGW method for cross-modal person re-identification. The AGW method incorporates a Non-local Attention module into ResNet50, primarily utilizing a <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ef08190c93d2/d33e341.gif" loading="lazy" id="d33e341" alt="Inline graphic"></span> convolution kernel. This module enables the model to capture long-range dependencies in images, compute the weighted sum of all positional features, and enhance the feature representation capability. Generalized-mean Pooling (GeMPooling), a learnable pooling layer, is used to provide a continuous transition between maximum pooling and average pooling, GeMPooling adjusts its pooling behavior via a learnable hyperparameter <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/02faa2a54e6d/d33e347.gif" loading="lazy" id="d33e347" alt="Inline graphic"></span>, it behaves similarly to maximum pooling as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/02faa2a54e6d/d33e353.gif" loading="lazy" id="d33e353" alt="Inline graphic"></span> approaches infinity and is equivalent to average pooling when <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/c90b77846954/d33e359.gif" loading="lazy" id="d33e359" alt="Inline graphic"></span>. Specifically, GeMPooling calculates the generalized average of each feature channel, adjusting <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq5"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/02faa2a54e6d/d33e365.gif" loading="lazy" id="d33e365" alt="Inline graphic"></span> enhances the contrast of the pooled feature map, emphasizing more significant image features. AGW puts Weighted Regression Triplet loss, an improvement over traditional triplet loss, this loss function inherits the advantages of relative distance optimization between positive and negative pairs while avoiding the introduction of additional margin parameters, it optimizes triplet positive set <em>P</em> and negative set <em>N</em> in batches using a weighted regularization approach, thereby improving the model’s discrimination ability[28]. But, AGW also has many drawbacks, it merely concatenates visible and infrared features, resulting in large modal differences. There is also no diversity mining of the features, which are directly input into the triplet loss function, leading to underutilization of the features.</p>
<p id="Par21">To obtain more effective features, this paper aggregates features learned at different stages, this strategy validated by the ANN network proposed by Zhu Z<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>. The ANN network puts Asymmetric Fusion Non-local Block (APNB) module, which integrates features across different levels while accounting for long-range dependencies. This approach significantly enhances performance, demonstrating the effectiveness of feature aggregation at various levels in tasks such as semantic segmentation and classification. However, full-stage feature aggregation may lead to feature redundancy and high computational costs.</p>
<p id="Par22">To reduce modality differences, the features extracted from visible light and infrared images can be interacted with in channel and spatial dimensions, such as the SCSN network uses an Residual Dual Attention Module (RDAM) module composed of Channel-wise Attention Module (CAM) and Residual Spatial Attention Module (RSAM). In addition, The SCSN network mines diverse salient features through the use of cascading Salient Feature Extraction (SFE) units. SFE suppresses salient features learned in the previous cascading stage, adaptively extracts additional potential salient features, and integrates these features into the final representation through cascading. However, a person’s unique features are finite, and excessive suppression may lead the network to learn non-robust features, resulting in feature vector redundancy and a dilution of salient features<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>.</p>
<p id="Par23">After extracting sufficient features, it is also necessary to make full use of these features and strengthen the diversity mining of features. The SGIEL forcibly removes body shape-related features, compelling the VI-ReID model to extract additional features shared across modalities for recognition. By orthogonal decomposition of the feature space, shape-related features are captured in a subspace, while shape-erased features are mapped to an orthogonal complement space, enhancing feature diversity<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>. However, this method relies heavily on prior knowledge of body shape, making it susceptible to inaccuracies or incompleteness in body shape information, which can adversely impact feature extraction.</p>
<p id="Par24">The DEEN Network<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> uses different expansion rates to simulate different receptive fields and extract multi-scale features, but this may result in tiny details being overlooked. Dilated convolution can introduce more gaps, leading to information loss and ambiguity. Although dilated convolution expands receptive fields, it significantly increases computational cost. As the expansion rate increases, the interval between elements in the convolution kernel becomes larger, which means that each element can cover a larger input area, which increases the computational complexity and thus affects the utilization of GPU.</p>
<p id="Par25">Inspired by the above methods, between the full-stage aggregation of the ANN method and the second-order aggregation of the SCSN method, this paper proposes the third-order primitive feature interaction, which takes ResNet50 as the backbone, take the features of the image after a simple convolution as the primitive features. From the third stage onwards, these primitive features are inputted into channel-wise and spatial feature interactions. This method, termed third-order primitive feature interaction (TPFI), compensates for potential losses of shared features during deep network extraction. At the cost of increasing a small amount of computational cost, it significantly reduces the loss of shared features in the deep network extraction process.</p>
<p id="Par26">To enhance the diversity of shared features, this paper uses Wavelet Transform (WT) to create a wavelet convolution module, it achieves a large receptive field without excessive parameterization<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>. Wavelet convolution can better capture the low-frequency information in the image, thus enhancing the response to shape, which is well combined with the idea of using shape loss and modality loss to get the total loss to enhance the network’s extraction of shape features, and further enhance the network’s response to shape features.</p></section><section id="Sec3"><h2 class="pmc_sec_title">Method</h2>
<section id="Sec4"><h3 class="pmc_sec_title">Model architecture</h3>
<p id="Par28">Figure <a href="#Fig4" class="usa-link">4</a> outlines the network framework for High-order Interaction and Wavelet Convolution Network for visible-infrared person re-identification (HIW-Net). It uses Resnet50 as the backbone and consists of two passes.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370930_41598_2025_14978_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/e24053d3f592/41598_2025_14978_Fig4_HTML.jpg" loading="lazy" id="MO4" height="358" width="708" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The structure of the HIW-Net. The FI module performs interactions across channel and spatial dimensions, aggregating higher-order and low-order features to reduce modality differences. and feature loss. TPFI, based on FI, aggregating primitive features (F<sub>v</sub> and the features after F<sub>I</sub> concatenation), decreasing feature loss in deep networks. The wtConv module extracts image features of different frequencies from a frequency perspective. Total loss is the weighted sum of Shape Loss and Modality Loss. Both losses are the sum of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq6"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/6d1c3912e98d/d33e424.gif" loading="lazy" id="d33e424" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq7"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/2edc8160ddb6/d33e430.gif" loading="lazy" id="d33e430" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq8"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/92ce1cfe5bd9/d33e436.gif" loading="lazy" id="d33e436" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq9"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/cb7e630d445d/d33e443.gif" loading="lazy" id="d33e443" alt="Inline graphic"></span>. Shape Loss uses processed shape images of visible and infrared images, while Modality Loss uses visible and infrared images directly.</p></figcaption></figure><p id="Par29">In the first pass, visible and infrared images are input. The preprocessing stage of Resnet50 extracts primitive features. Then, for each layerX, it aggregates the input (low-level features), output (high-level features), and primitive features. In the first stage, as the layer’s input is the primitive feature, it uses basic feature interaction (FI). For other stages, it uses third-order primitive feature interaction (TPFI), which involves interaction in both channel and spatial dimensions. For the RegDB datasets, only the first two stages of Resnet50 are used, and for the SYSU datasets, the first three stages are used. Then, The wavelet convolution module splits the aggregated features into diverse frequency bands. It then processes each band with convolutions, using branches with different wavelet convolution kernel sizes to capture more varied feature information. Finally, the model loss, which is the sum of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq10"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/6d1c3912e98d/d33e461.gif" loading="lazy" id="d33e461" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq11"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/2edc8160ddb6/d33e467.gif" loading="lazy" id="d33e467" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq12"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/92ce1cfe5bd9/d33e473.gif" loading="lazy" id="d33e473" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq13"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/cb7e630d445d/d33e479.gif" loading="lazy" id="d33e479" alt="Inline graphic"></span>, is calculated using these diverse features.</p>
<p id="Par30">The second pass takes the shape images of visible and infrared images (containing only human shape features) as input. The calculation method is the same as the first pass, yielding the shape loss.</p>
<p id="Par31">The total loss is the weighted sum of model loss and shape loss.</p></section><section id="Sec5"><h3 class="pmc_sec_title">Third-order primitive feature interaction</h3>
<p id="Par32">Figure <a href="#Fig4" class="usa-link">4</a> illustrates the structure of the third-order primitive feature interaction module. In this paper, features preceding the layer are called low-order feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq14"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dd3562c8f797/d33e496.gif" loading="lazy" id="d33e496" alt="Inline graphic"></span>, features following the layer are called High-order feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq15"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ebb80828417e/d33e502.gif" loading="lazy" id="d33e502" alt="Inline graphic"></span>, and features resulting from the initial image convolution, batch normalization(BN), ReLU, and max-pooling operations are defined as primitive feature</p>
<p id="Par33"><strong> Channel interaction</strong>: Channel interaction between the primitive feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq16"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/bc7f336328d4/d33e512.gif" loading="lazy" id="d33e512" alt="Inline graphic"></span> and the higher-order feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq17"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ebb80828417e/d33e518.gif" loading="lazy" id="d33e518" alt="Inline graphic"></span>, use three convolution <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq18"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/5a0f748d9869/d33e524.gif" loading="lazy" id="d33e524" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq19"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/5e8a4e723b29/d33e530.gif" loading="lazy" id="d33e530" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq20"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/48fc90c21f29/d33e536.gif" loading="lazy" id="d33e536" alt="Inline graphic"></span> to preprocess the primitive and higher-order features, generating three compact outputs <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq21"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/85556f8f7b1b/d33e543.gif" loading="lazy" id="d33e543" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq22"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/2b97222f6cf3/d33e549.gif" loading="lazy" id="d33e549" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq23"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/b6c9e581e87a/d33e555.gif" loading="lazy" id="d33e555" alt="Inline graphic"></span>with the same feature dimensions as the primitive feature, the feature map is flattened in the last dimension, and then use matrix multiplication and softmax to calculate the channel similarity of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq24"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/bc7f336328d4/d33e561.gif" loading="lazy" id="d33e561" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq25"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ebb80828417e/d33e567.gif" loading="lazy" id="d33e567" alt="Inline graphic"></span>, then obtain the channel similarity matrix.</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/a0876f68b2ae/d33e575.gif" loading="lazy" id="d33e575" alt="graphic file with name d33e575.gif"></td>
<td class="label">1</td>
</tr></table>
<p id="Par35">Then, the preprocessed High-order feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq26"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/b6c9e581e87a/d33e584.gif" loading="lazy" id="d33e584" alt="Inline graphic"></span>and channel similarity matrix <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq27"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/3ac7aa802cc7/d33e590.gif" loading="lazy" id="d33e590" alt="Inline graphic"></span> are multiplied to achieve feature interaction across different stages. Then use a <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq28"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ef08190c93d2/d33e596.gif" loading="lazy" id="d33e596" alt="Inline graphic"></span> convolution <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq29"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/900d8e9195e1/d33e602.gif" loading="lazy" id="d33e602" alt="Inline graphic"></span> to restore the interaction features to same shape as the higher-order feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq30"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ebb80828417e/d33e608.gif" loading="lazy" id="d33e608" alt="Inline graphic"></span>:</p>
<table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/d1285f54cc21/d33e615.gif" loading="lazy" id="d33e615" alt="graphic file with name d33e615.gif"></td>
<td class="label">2</td>
</tr></table>
<p id="Par36">Use the same method to perform channel interaction on low-order features <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq31"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dd3562c8f797/d33e623.gif" loading="lazy" id="d33e623" alt="Inline graphic"></span> and High-order features <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq32"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ebb80828417e/d33e629.gif" loading="lazy" id="d33e629" alt="Inline graphic"></span> to obtain:</p>
<table class="disp-formula p" id="Equ3"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/f34f7be01d7a/d33e637.gif" loading="lazy" id="d33e637" alt="graphic file with name d33e637.gif"></td>
<td class="label">3</td>
</tr></table>
<p id="Par38">Finally, we utilize matrix addition to perform feature aggregation along the channel dimension in the third-order primitive feature interaction module:</p>
<table class="disp-formula p" id="Equ4"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/c49eb27b9452/d33e646.gif" loading="lazy" id="d33e646" alt="graphic file with name d33e646.gif"></td>
<td class="label">4</td>
</tr></table>
<p id="Par39"><strong>Spatial interaction:</strong> Then, we performed the spatial interaction of High-order features of channel aggregation have been completed <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq33"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/fbdeb4569a11/d33e656.gif" loading="lazy" id="d33e656" alt="Inline graphic"></span>, low-order features <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq34"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dd3562c8f797/d33e662.gif" loading="lazy" id="d33e662" alt="Inline graphic"></span> and primitive features <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq35"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/bc7f336328d4/d33e668.gif" loading="lazy" id="d33e668" alt="Inline graphic"></span>,this operation is similar to channel interaction, the difference is that in the feature pre-processing stage, file the channel dimension with the features of the spatial dimension, so that the spatial size of the pre-processed low-order features <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq36"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dd3562c8f797/d33e674.gif" loading="lazy" id="d33e674" alt="Inline graphic"></span> and primitive features <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq37"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/bc7f336328d4/d33e680.gif" loading="lazy" id="d33e680" alt="Inline graphic"></span> is consistent with the High-order. Finally, we obtain the output of the third-order primitive feature interaction.</p>
<table class="disp-formula p" id="Equ5"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/f32a0ab029f9/d33e688.gif" loading="lazy" id="d33e688" alt="graphic file with name d33e688.gif"></td>
<td class="label">5</td>
</tr></table>
<p id="Par41">where, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq38"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/d807e01f4c7e/d33e697.gif" loading="lazy" id="d33e697" alt="Inline graphic"></span>denotes <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq39"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ef08190c93d2/d33e703.gif" loading="lazy" id="d33e703" alt="Inline graphic"></span> convolution in spatial interaction, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq40"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/8ecf1f468848/d33e709.gif" loading="lazy" id="d33e709" alt="Inline graphic"></span> represents the spatial similarity matrix.</p></section><section id="Sec6"><h3 class="pmc_sec_title">Diverse features mining for wavelet Convolution</h3>
<p id="Par42"><strong>Parameter efficiency and Multi-Frequency emphasis:</strong> For <em>l</em>-level decomposition and a <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq41"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/326903476749/d33e724.gif" loading="lazy" id="d33e724" alt="Inline graphic"></span> kernel, wtConv’s[34] parameters scale as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq42"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/284be677ebc3/d33e730.gif" loading="lazy" id="d33e730" alt="Inline graphic"></span>, whereas its effective receptive field(ERF) grows as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq43"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/680a42aa28c9/d33e736.gif" loading="lazy" id="d33e736" alt="Inline graphic"></span>. For example, a 3-level wtConv with <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq44"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/08951a9ed47d/d33e742.gif" loading="lazy" id="d33e742" alt="Inline graphic"></span> kernels achieves a <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq45"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/8b4f1ac52d0f/d33e749.gif" loading="lazy" id="d33e749" alt="Inline graphic"></span> ERF using only 117 parameters(108 parameters plus bias terms) per channel, compared to 576 parameters for a <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq46"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/8b4f1ac52d0f/d33e755.gif" loading="lazy" id="d33e755" alt="Inline graphic"></span>standard convolution.</p>
<p id="Par43">By focusing convolutions on low-frequency subbands, wtConv enhances shape bias and robustness to high-frequency noise. This aligns with the observation that low frequencies encode structural information, while high frequencies correspond to textures.</p>
<p id="Par44"><strong>Wavelet transform integration with CNNs:</strong> The wtConv serves as a drop-in replacement for depth-wise convolutions in existing architectures. Its implementation requires no architectural modifications, ensuring compatibility with standard training pipelines and downstream tasks.</p>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/994c99b40814/41598_2025_14978_Figa_HTML.jpg" loading="lazy" id="MO6" height="388" width="630" alt="graphic file with name 41598_2025_14978_Figa_HTML.jpg"></p>
<p id="Par46">This paper uses <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq47"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/08951a9ed47d/d33e775.gif" loading="lazy" id="d33e775" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq48"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dee23de8cc2f/d33e781.gif" loading="lazy" id="d33e781" alt="Inline graphic"></span> wtConv branches to extract features. The network concatenates features from different branches to obtain the final feature:</p>
<table class="disp-formula p" id="Equ6"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/6f4c91fa576f/d33e789.gif" loading="lazy" id="d33e789" alt="graphic file with name d33e789.gif"></td>
<td class="label">6</td>
</tr></table>
<p id="Par48">where, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq49"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/045e0502d00f/d33e798.gif" loading="lazy" id="d33e798" alt="Inline graphic"></span>represents a convolution of kernel size 1, changing dimension as same as <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq50"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/2873583b4995/d33e804.gif" loading="lazy" id="d33e804" alt="Inline graphic"></span>.</p></section><section id="Sec7"><h3 class="pmc_sec_title">Loss functions</h3>
<p id="Par49">This section describes several loss functions for network training. It introduces shape images, performs two similar computations, and calculates the total loss via a simple weighted sum.</p>
<p id="Par50"><strong>Identity classification loss</strong>
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq51"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/f24d16b4fd69/d33e818.gif" loading="lazy" id="d33e818" alt="Inline graphic"></span> uses the cross - entropy loss function<sup><a href="#CR51" class="usa-link" aria-describedby="CR51">51</a></sup>. This helps the model learn to distinguish pedestrians’ identity features and increases inter - class differences. It classifies the output feature <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq52"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/07aa9823ee2f/d33e828.gif" loading="lazy" id="d33e828" alt="Inline graphic"></span> to mitigate the negative impact of modality differences, ensuring the network correctly identifies different identities.</p>
<table class="disp-formula p" id="Equ7"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/8660b5576dc5/d33e836.gif" loading="lazy" id="d33e836" alt="graphic file with name d33e836.gif"></td>
<td class="label">7</td>
</tr></table>
<p id="Par52">where, <em>N</em> represents the number of samples, <em>C</em> the number of classes, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq53"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/3d519a2bafba/d33e851.gif" loading="lazy" id="d33e851" alt="Inline graphic"></span> is the one - hot encoded labels, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq54"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ed3644043fe1/d33e857.gif" loading="lazy" id="d33e857" alt="Inline graphic"></span> is the predicted probabilities.</p>
<p id="Par53"><strong>Triplet loss</strong> To learn a cross - modal shared metric space, this paper adopts triplet loss<sup><a href="#CR49" class="usa-link" aria-describedby="CR49">49</a></sup>. It narrows anchor - positive pairs (same - identity cross - modal samples) and widens negative pairs (different - identity samples), enhancing the feature space’s discriminative power.</p>
<table class="disp-formula p" id="Equ8"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/a7bb84871fc0/d33e871.gif" loading="lazy" id="d33e871" alt="graphic file with name d33e871.gif"></td>
<td class="label">8</td>
</tr></table>
<p id="Par54">where, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq55"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/be23e8d5f072/d33e879.gif" loading="lazy" id="d33e879" alt="Inline graphic"></span> denotes the Euclidean distance metric, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq56"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/3cf226898201/d33e885.gif" loading="lazy" id="d33e885" alt="Inline graphic"></span> is the margin hyperparameter, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq57"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/89508f7b984a/d33e891.gif" loading="lazy" id="d33e891" alt="Inline graphic"></span> represents the anchor sample, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq58"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/3f5773fe51ff/d33e897.gif" loading="lazy" id="d33e897" alt="Inline graphic"></span> the positive sample, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq59"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/e249057d329a/d33e903.gif" loading="lazy" id="d33e903" alt="Inline graphic"></span> the negative sample.</p>
<p id="Par55"><strong>Orthogonality loss</strong> In order to ensure the diversity of features, this paper uses orthogonal loss<sup><a href="#CR50" class="usa-link" aria-describedby="CR50">50</a></sup> to constrain the orthogonality between feature vectors of different branches and reduce feature redundancy. Orthogonal loss is represented by:</p>
<table class="disp-formula p" id="Equ9"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/cd9335bfb62a/d33e919.gif" loading="lazy" id="d33e919" alt="graphic file with name d33e919.gif"></td>
<td class="label">9</td>
</tr></table>
<p id="Par57">where, <em>m</em> and <em>n</em> are the <em>m-th</em> and <em>n-th</em> features generated from <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq60"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/2873583b4995/d33e940.gif" loading="lazy" id="d33e940" alt="Inline graphic"></span>, respectively.</p>
<p id="Par58"><strong>Center-Guided Pair Mining Loss.</strong> The center-guided pair mining loss, proposed by Zhang Y et al.<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup>, is designed for generating diversified features for multi-branch network structures. It narrows intra-class distances across modalities, reducing VIS-IR modality gaps, while widening distances between generated and original features to mine diverse cross-modal clues. Crucially, it ensures inter-class distances exceed intra-class ones.</p>
<table class="disp-formula p" id="Equ10"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/59808a928998/d33e954.gif" loading="lazy" id="d33e954" alt="graphic file with name d33e954.gif"></td>
<td class="label">10</td>
</tr></table>
<p id="Par59">where, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq61"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/f432933c623a/d33e963.gif" loading="lazy" id="d33e963" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq62"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/d68aa789d968/d33e969.gif" loading="lazy" id="d33e969" alt="Inline graphic"></span> are the VIS and IR features from TPFI block, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq63"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/6f3e2424b22a/d33e975.gif" loading="lazy" id="d33e975" alt="Inline graphic"></span> is the features generated from the <em>i-th</em> branch of the <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq64"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/f432933c623a/d33e984.gif" loading="lazy" id="d33e984" alt="Inline graphic"></span>. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq65"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/0e28972b8078/d33e991.gif" loading="lazy" id="d33e991" alt="Inline graphic"></span> is the Euclidean distance between two features. <em>j</em>, <em>k</em> are different identities in a mini-batch, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq66"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/2fcc36430760/d33e1003.gif" loading="lazy" id="d33e1003" alt="Inline graphic"></span>.</p>
<p id="Par60">In Eq. (<a href="#Equ10" class="usa-link">10</a>), <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq67"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/caf07d7a933b/d33e1014.gif" loading="lazy" id="d33e1014" alt="Inline graphic"></span> represents the infrared feature of identity <em>j</em>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq68"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/3c8b15fe75b9/d33e1023.gif" loading="lazy" id="d33e1023" alt="Inline graphic"></span> represents the visible feature of identity <em>j</em>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq69"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/4b0078da6105/d33e1033.gif" loading="lazy" id="d33e1033" alt="Inline graphic"></span> represents the feature of generated by <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq70"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/3c8b15fe75b9/d33e1039.gif" loading="lazy" id="d33e1039" alt="Inline graphic"></span> in the <em>i</em> branch. The first term reduces the distance between newly - generated visible features and original infrared ones, thus decreasing the modality difference. The second <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq71"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/e16d116689dd/d33e1048.gif" loading="lazy" id="d33e1048" alt="Inline graphic"></span>term increases the distance between newly - generated visible features and original visible ones, prompting the network to learn diverse features. The third term ensures the intra - class distance is smaller than the inter - class distance.</p>
<p id="Par61">Similarly, for the feature generated by <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq72"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/d68aa789d968/d33e1056.gif" loading="lazy" id="d33e1056" alt="Inline graphic"></span> in branch <em>i</em>, the loss function that needs to be satisfied is:</p>
<table class="disp-formula p" id="Equ11"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/77bcb2fa111b/d33e1067.gif" loading="lazy" id="d33e1067" alt="graphic file with name d33e1067.gif"></td>
<td class="label">11</td>
</tr></table>
<p id="Par63">Therefore, the final CPM loss can be expressed as:</p>
<table class="disp-formula p" id="Equ12"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/6fde6116c952/d33e1076.gif" loading="lazy" id="d33e1076" alt="graphic file with name d33e1076.gif"></td>
<td class="label">12</td>
</tr></table>
<p id="Par64"><strong>Total Loss.</strong> This paper employs four loss functions. The network undergoes two computational passes. In the first pass, visible and infrared images are input, and the network computes the sum of the four loss functions to obtain the modality loss.</p>
<table class="disp-formula p" id="Equ13"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/d2d8274155d4/d33e1086.gif" loading="lazy" id="d33e1086" alt="graphic file with name d33e1086.gif"></td>
<td class="label">13</td>
</tr></table>
<p id="Par65">where, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq73"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/fdeb63f7d006/d33e1094.gif" loading="lazy" id="d33e1094" alt="Inline graphic"></span> represents the loss calculated using modal images.</p>
<p id="Par66">In the second pass, the network takes shape images, derived from visible and infrared images, as input. It calculates the sum of the four loss functions to obtain the shape loss.</p>
<table class="disp-formula p" id="Equ14"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/6c50e4f8a5cb/d33e1102.gif" loading="lazy" id="d33e1102" alt="graphic file with name d33e1102.gif"></td>
<td class="label">14</td>
</tr></table>
<p id="Par67">where, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq74"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/9ee72c9ad1c7/d33e1110.gif" loading="lazy" id="d33e1110" alt="Inline graphic"></span> represents the loss calculated using shape images.</p>
<p id="Par68">Finally, the network is trained by minimizing the sum of modal and shape losses.</p>
<table class="disp-formula p" id="Equ15"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/d43bbf24b378/d33e1118.gif" loading="lazy" id="d33e1118" alt="graphic file with name d33e1118.gif"></td>
<td class="label">15</td>
</tr></table>
<p id="Par69">where, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq75"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/95c7c0c413ff/d33e1126.gif" loading="lazy" id="d33e1126" alt="Inline graphic"></span> denotes the weight that adjusts the loss ratio to control the proportion of shape enhanced features in the network. Experimental exploration shows the network performs best when <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq76"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/95c7c0c413ff/d33e1132.gif" loading="lazy" id="d33e1132" alt="Inline graphic"></span> = 0.9.</p></section></section><section id="Sec8"><h2 class="pmc_sec_title">Experiments</h2>
<p id="Par70">This paper conducted experiments on two public datasets, SYSU-MM01 and RegDB, to evaluate the effectiveness of the proposed method and compare it with recent approaches, demonstrating its superiority. Additionally, ablation experiments were conducted to assess the contributions of TPFI, wtDFM, and ShapeLoss.</p>
<section id="Sec9"><h3 class="pmc_sec_title">Dataset and evaluation protocol</h3>
<p id="Par71">The RegDB dataset contains a total of 412 people, each person has 10 visible light images and corresponding 10 thermal images, these images exhibit variations in body posture, capture distance, and lighting conditions, among the 412 people, there are 254 females and 158 males. In addition, 156 people were photographed from the front and the remaining 256 people were photographed from other angles. The images in this dataset are small and have poor clarity. The RGB and thermal images for each identity are in a one-to-one correspondence<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>. The SYSU-MM01 dataset contains images of 491 people captured by 4 RGB cameras and 2 infrared cameras, totaling 30,071 RGB images and 15,792 infrared images. For testing, it supports two evaluation settings: all-search mode and indoor-search mode. The query set contains 3,803 images captured from IR cameras 3 and 6 in both settings, while the gallery set in all-search mode includes all visible images from the four RGB cameras. In indoor-search mode, the gallery set includes images only from the two indoor RGB cameras.</p>
<p id="Par72">Shape dataset. The dataset is converted into shape maps. The SCHP network is used to create shape maps for the SYSU-MM01 dataset<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>. However, due to the low resolution and poor image quality of the RegDB dataset, existing human body analysis networks produce unsatisfactory results<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a>,<a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup>. This paper manually annotates the RegDB dataset to create the first high-quality RegDB shape dataset.</p>
<p id="Par73">To evaluate the performance of the network, this paper uses two evaluation metrics: Rank and Mean Average Precision (mAP). The Rank indicator, particularly Rank-1, is a key metric for evaluating the performance of person re-identification algorithms. It indicates the proportion of correctly identified samples ranked first by the algorithm. A higher Rank-1 value signifies better performance in person re-identification tasks<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>. The mAP is another widely used evaluation metric. It measures the model’s average performance across all categories and is calculated by averaging the Average Precision(AP) of each query. mAP accounts for both precision and recall during the query process, providing a more comprehensive assessment of person re-identification performance<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>.</p></section><section id="Sec10"><h3 class="pmc_sec_title">Implementation details</h3>
<p id="Par74">This paper uses a single RTX 4090 GPU for experiments. After preprocessing the input image using horizontal flipping, random cropping, and random erasing techniques, the preprocessed image is fed into the network, which uses ResNet50 as its backbone. For the RegDB datasets, the fourth stage of ResNet50 is removed, the TPFI module proposed in this paper is added to the first and second stages, and the wtDFM module is added to the second stage. For the SYSU-MM01 datasets, the TPFI module is added after each of the first three stages, and the wtDFM module is added after the third stage. In the first 10 rounds of warming up the model, the learning rate increases from 0.01 to 0.1, then remains unchanged at 0.1 from 10 to 20 rounds. It then decreases to 0.01 from 20 to 80 rounds and to 0.001 from 80 to 120 rounds. Beyond 120 rounds, the learning rate is further reduced to 0.0001, for a total of 150 training rounds. The network uses the ImageNet pre-trained weight file.</p></section><section id="Sec11"><h3 class="pmc_sec_title">Comparison with state-of-the-art methods</h3>
<p id="Par75">This paper compared the proposed method with existing state-of-the-art VI-ReID methods, including FMCNet<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>, SGIEL<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>, DEEN<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup>, PMT<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>, AGMNet<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>, LCNL<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>, MCJA<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup>, DCPLNet<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>, MPMN<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>, PMCM<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup>, CSDN<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup>, MIP<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup>, AGCC<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>, RCC<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup>, DNS<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup>, MSCMNet<sup><a href="#CR52" class="usa-link" aria-describedby="CR52">52</a></sup>, LAReViT<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>. Extensive experiments demonstrate that the HIW-Net proposed in this paper achieves superior or comparable performance on both the SYSU-MM01 and RegDB datasets.</p>
<p id="Par76"><strong>RegDB.</strong> As shown in Table <a href="#Tab1" class="usa-link">1</a>, for the VIS-to-IR mode of RegDB, the HIW-Net achieves a Rank-1 accuracy of 94.88%, representing the best performance, while the mAP metric demonstrates comparable performance. For the IR to VIS mode, a Rank-1 accuracy of 94.32% is achieved, representing the best performance, while mAP achieves the second-best result at 87.18%.</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>Comparison of the proposed method with state-of-the-art approaches on the RegDB dataset. Bold values indicate the best performance, while underlined values represent the second-best results.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">methods</th>
<th align="left" rowspan="2" colspan="1">venue</th>
<th align="left" colspan="2" rowspan="1">VIS to IR</th>
<th align="left" colspan="2" rowspan="1">IR to VIS</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">rank-1</th>
<th align="left" colspan="1" rowspan="1">mAP</th>
<th align="left" colspan="1" rowspan="1">rank-1</th>
<th align="left" colspan="1" rowspan="1">mAP</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">FMCNet<sup>[[<a href="#CR38" class="usa-link" aria-describedby="CR38">38</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">CVPR22</td>
<td align="center" colspan="1" rowspan="1">89.12</td>
<td align="center" colspan="1" rowspan="1">84.43</td>
<td align="center" colspan="1" rowspan="1">88.38</td>
<td align="center" colspan="1" rowspan="1">83.86</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SGIEL<sup>[[<a href="#CR23" class="usa-link" aria-describedby="CR23">23</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">CVPR23</td>
<td align="center" colspan="1" rowspan="1">91.07</td>
<td align="center" colspan="1" rowspan="1">85.23</td>
<td align="center" colspan="1" rowspan="1">92.18</td>
<td align="center" colspan="1" rowspan="1">86.59</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DEEN<sup>[[<a href="#CR33" class="usa-link" aria-describedby="CR33">33</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">CVPR23</td>
<td align="center" colspan="1" rowspan="1">91.1</td>
<td align="center" colspan="1" rowspan="1">85.1</td>
<td align="center" colspan="1" rowspan="1">89.5</td>
<td align="center" colspan="1" rowspan="1">83.4</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PMT<sup>[[<a href="#CR39" class="usa-link" aria-describedby="CR39">39</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">AAAI23</td>
<td align="center" colspan="1" rowspan="1">84.83</td>
<td align="center" colspan="1" rowspan="1">76.55</td>
<td align="center" colspan="1" rowspan="1">84.16</td>
<td align="center" colspan="1" rowspan="1">75.13</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">AGMNet<sup>[[<a href="#CR40" class="usa-link" aria-describedby="CR40">40</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">IEEE23</td>
<td align="center" colspan="1" rowspan="1">88.40</td>
<td align="center" colspan="1" rowspan="1">81.45</td>
<td align="center" colspan="1" rowspan="1">85.34</td>
<td align="center" colspan="1" rowspan="1">81.19</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">LCNL<sup>[[<a href="#CR41" class="usa-link" aria-describedby="CR41">41</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">IJCV24</td>
<td align="center" colspan="1" rowspan="1">85.6</td>
<td align="center" colspan="1" rowspan="1">78.7</td>
<td align="center" colspan="1" rowspan="1">84.0</td>
<td align="center" colspan="1" rowspan="1">76.9</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MCJA<sup>[[<a href="#CR42" class="usa-link" aria-describedby="CR42">42</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">IEEE24</td>
<td align="center" colspan="1" rowspan="1">91.80</td>
<td align="center" colspan="1" rowspan="1">86.08</td>
<td align="center" colspan="1" rowspan="1">88.06</td>
<td align="center" colspan="1" rowspan="1">83.06</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DCPLNet<sup>[[<a href="#CR43" class="usa-link" aria-describedby="CR43">43</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">TIT24</td>
<td align="center" colspan="1" rowspan="1">
<em>94.2</em>
</td>
<td align="center" colspan="1" rowspan="1">87.3</td>
<td align="center" colspan="1" rowspan="1">91.7</td>
<td align="center" colspan="1" rowspan="1">84.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MPMN<sup>[[<a href="#CR44" class="usa-link" aria-describedby="CR44">44</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">TMM24</td>
<td align="center" colspan="1" rowspan="1">85.3</td>
<td align="center" colspan="1" rowspan="1">76.4</td>
<td align="center" colspan="1" rowspan="1">83.8</td>
<td align="center" colspan="1" rowspan="1">75.2</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PMCM<sup>[[<a href="#CR45" class="usa-link" aria-describedby="CR45">45</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">93.09</td>
<td align="center" colspan="1" rowspan="1">
<strong>89.57</strong>
</td>
<td align="center" colspan="1" rowspan="1">91.44</td>
<td align="center" colspan="1" rowspan="1">87.15</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CSDN<sup>[[<a href="#CR46" class="usa-link" aria-describedby="CR46">46</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">89.0</td>
<td align="center" colspan="1" rowspan="1">84.7</td>
<td align="center" colspan="1" rowspan="1">88.2</td>
<td align="center" colspan="1" rowspan="1">82.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MIP<sup>[[<a href="#CR47" class="usa-link" aria-describedby="CR47">47</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">91.26</td>
<td align="center" colspan="1" rowspan="1">85.90</td>
<td align="center" colspan="1" rowspan="1">92.38</td>
<td align="center" colspan="1" rowspan="1">85.99</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">AGCC<sup>[[<a href="#CR48" class="usa-link" aria-describedby="CR48">48</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">92.59</td>
<td align="center" colspan="1" rowspan="1">86.18</td>
<td align="center" colspan="1" rowspan="1">91.35</td>
<td align="center" colspan="1" rowspan="1">84.92</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RCC<sup>[[<a href="#CR49" class="usa-link" aria-describedby="CR49">49</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">92.00</td>
<td align="center" colspan="1" rowspan="1">88.01</td>
<td align="center" colspan="1" rowspan="1">90.01</td>
<td align="center" colspan="1" rowspan="1">86.15</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DNS<sup>[[<a href="#CR50" class="usa-link" aria-describedby="CR50">50</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">ECCV24</td>
<td align="center" colspan="1" rowspan="1">93.48</td>
<td align="center" colspan="1" rowspan="1">
<em>88.10</em>
</td>
<td align="center" colspan="1" rowspan="1">
<em>93.01</em>
</td>
<td align="center" colspan="1" rowspan="1">
<strong>88.56</strong>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MSCMNet<sup>[[<a href="#CR54" class="usa-link" aria-describedby="CR54">54</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2025</td>
<td align="center" colspan="1" rowspan="1">90.4</td>
<td align="center" colspan="1" rowspan="1">81.2</td>
<td align="center" colspan="1" rowspan="1">87.7</td>
<td align="center" colspan="1" rowspan="1">78.2</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">LAReViT<sup>[[<a href="#CR55" class="usa-link" aria-describedby="CR55">55</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2025</td>
<td align="center" colspan="1" rowspan="1">90.4</td>
<td align="center" colspan="1" rowspan="1">84.7</td>
<td align="center" colspan="1" rowspan="1">90.5</td>
<td align="center" colspan="1" rowspan="1">85.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">HIW(ours)</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="center" colspan="1" rowspan="1">
<strong>94.88</strong>
</td>
<td align="center" colspan="1" rowspan="1">86.59</td>
<td align="center" colspan="1" rowspan="1">
<strong>94.32</strong>
</td>
<td align="center" colspan="1" rowspan="1">
<em>87.18</em>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par77"><strong>SYSU-MM01.</strong> As shown in Table <a href="#Tab2" class="usa-link">2</a>, in the all-search mode of SYSU-MM01, the HIW-Net achieves the best performance in Rank-1, Rank-10, and Rank-20, achieving 85.05%, 97.67%, and 99.67%, respectively, which are 8.23%, 0.07%, and 0.36% higher than the second-best results. For the indoor-search mode, achieves the best performance in Rank-1, Rank-10, and mAP, achieving 87.50%, 99.32% and 87.75%, surpassing the second-best result by 3.29%, 0.32% and 0.92%. In other metrics, it demonstrated performance comparable to the best results.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Comparison of the proposed method with state-of-the-art approaches on the SYSU-MM01 dataset. Bold values indicate the best performance, while underlined values represent the second-best results.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">methods</th>
<th align="left" rowspan="2" colspan="1">Venue</th>
<th align="left" colspan="4" rowspan="1">All-Search</th>
<th align="left" colspan="4" rowspan="1">Indoor-Search</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">rank-1</th>
<th align="left" colspan="1" rowspan="1">rank-10</th>
<th align="left" colspan="1" rowspan="1">rank-20</th>
<th align="left" colspan="1" rowspan="1">mAP</th>
<th align="left" colspan="1" rowspan="1">rank-1</th>
<th align="left" colspan="1" rowspan="1">rank-10</th>
<th align="left" colspan="1" rowspan="1">rank-20</th>
<th align="left" colspan="1" rowspan="1">mAP</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">FMCNet<sup>[[<a href="#CR38" class="usa-link" aria-describedby="CR38">38</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">CVPR22</td>
<td align="center" colspan="1" rowspan="1">66.34</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="center" colspan="1" rowspan="1">62.51</td>
<td align="center" colspan="1" rowspan="1">68.15</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="center" colspan="1" rowspan="1">74.09</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SGIEL<sup>[[<a href="#CR23" class="usa-link" aria-describedby="CR23">23</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">CVPR23</td>
<td align="center" colspan="1" rowspan="1">75.18</td>
<td align="left" colspan="1" rowspan="1">96.87</td>
<td align="left" colspan="1" rowspan="1">99.13</td>
<td align="center" colspan="1" rowspan="1">70.12</td>
<td align="center" colspan="1" rowspan="1">78.40</td>
<td align="left" colspan="1" rowspan="1">97.46</td>
<td align="left" colspan="1" rowspan="1">98.91</td>
<td align="center" colspan="1" rowspan="1">81.20</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DEEN<sup>[[<a href="#CR33" class="usa-link" aria-describedby="CR33">33</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">CVPR23</td>
<td align="center" colspan="1" rowspan="1">74.7</td>
<td align="left" colspan="1" rowspan="1">
<em>97.6</em>
</td>
<td align="left" colspan="1" rowspan="1">99.2</td>
<td align="center" colspan="1" rowspan="1">71.8</td>
<td align="center" colspan="1" rowspan="1">80.3</td>
<td align="left" colspan="1" rowspan="1">99.0</td>
<td align="left" colspan="1" rowspan="1">99.8</td>
<td align="center" colspan="1" rowspan="1">83.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PMT<sup>[[<a href="#CR39" class="usa-link" aria-describedby="CR39">39</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">AAAI23</td>
<td align="center" colspan="1" rowspan="1">67.53</td>
<td align="left" colspan="1" rowspan="1">95.36</td>
<td align="left" colspan="1" rowspan="1">98.64</td>
<td align="center" colspan="1" rowspan="1">64.98</td>
<td align="center" colspan="1" rowspan="1">71.66</td>
<td align="left" colspan="1" rowspan="1">96.73</td>
<td align="left" colspan="1" rowspan="1">99.25</td>
<td align="center" colspan="1" rowspan="1">76.52</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">AGMNet<sup>[[<a href="#CR40" class="usa-link" aria-describedby="CR40">40</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">IEEE23</td>
<td align="center" colspan="1" rowspan="1">69.63</td>
<td align="left" colspan="1" rowspan="1">96.27</td>
<td align="left" colspan="1" rowspan="1">98.82</td>
<td align="center" colspan="1" rowspan="1">66.11</td>
<td align="center" colspan="1" rowspan="1">74.68</td>
<td align="left" colspan="1" rowspan="1">97.51</td>
<td align="left" colspan="1" rowspan="1">99.14</td>
<td align="center" colspan="1" rowspan="1">78.30</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">LCNL<sup>[[<a href="#CR41" class="usa-link" aria-describedby="CR41">41</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">IJCV24</td>
<td align="center" colspan="1" rowspan="1">70.2</td>
<td align="left" colspan="1" rowspan="1">96.4</td>
<td align="left" colspan="1" rowspan="1">99.0</td>
<td align="center" colspan="1" rowspan="1">68.0</td>
<td align="center" colspan="1" rowspan="1">76.2</td>
<td align="left" colspan="1" rowspan="1">98.2</td>
<td align="left" colspan="1" rowspan="1">99.8</td>
<td align="center" colspan="1" rowspan="1">80.3</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MCJA<sup>[[<a href="#CR42" class="usa-link" aria-describedby="CR42">42</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">IEEE24</td>
<td align="center" colspan="1" rowspan="1">74.48</td>
<td align="left" colspan="1" rowspan="1">96.99</td>
<td align="left" colspan="1" rowspan="1">
<em>99.31</em>
</td>
<td align="center" colspan="1" rowspan="1">71.34</td>
<td align="center" colspan="1" rowspan="1">82.79</td>
<td align="left" colspan="1" rowspan="1">98.88</td>
<td align="left" colspan="1" rowspan="1">
<strong>99.92</strong>
</td>
<td align="center" colspan="1" rowspan="1">85.26</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DCPLNet<sup>[[<a href="#CR43" class="usa-link" aria-describedby="CR43">43</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">TIT24</td>
<td align="center" colspan="1" rowspan="1">74.0</td>
<td align="left" colspan="1" rowspan="1">96.5</td>
<td align="left" colspan="1" rowspan="1">98.9</td>
<td align="center" colspan="1" rowspan="1">70.3</td>
<td align="center" colspan="1" rowspan="1">78.3</td>
<td align="left" colspan="1" rowspan="1">98.7</td>
<td align="left" colspan="1" rowspan="1">
<em>99.8</em>
</td>
<td align="center" colspan="1" rowspan="1">81.9</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MPMN<sup>[[<a href="#CR44" class="usa-link" aria-describedby="CR44">44</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">TMM24</td>
<td align="center" colspan="1" rowspan="1">70.6</td>
<td align="left" colspan="1" rowspan="1">96.2</td>
<td align="left" colspan="1" rowspan="1">98.7</td>
<td align="center" colspan="1" rowspan="1">67.5</td>
<td align="center" colspan="1" rowspan="1">75.9</td>
<td align="left" colspan="1" rowspan="1">98.1</td>
<td align="left" colspan="1" rowspan="1">99.6</td>
<td align="center" colspan="1" rowspan="1">80.2</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PMCM<sup>[[<a href="#CR45" class="usa-link" aria-describedby="CR45">45</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">75.54</td>
<td align="left" colspan="1" rowspan="1">97.49</td>
<td align="left" colspan="1" rowspan="1">99.30</td>
<td align="center" colspan="1" rowspan="1">71.16</td>
<td align="center" colspan="1" rowspan="1">81.52</td>
<td align="left" colspan="1" rowspan="1">98.99</td>
<td align="left" colspan="1" rowspan="1">99.71</td>
<td align="center" colspan="1" rowspan="1">84.33</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">CSDN<sup>[[<a href="#CR46" class="usa-link" aria-describedby="CR46">46</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">75.2</td>
<td align="left" colspan="1" rowspan="1">96.6</td>
<td align="left" colspan="1" rowspan="1">98.8</td>
<td align="center" colspan="1" rowspan="1">71.8</td>
<td align="center" colspan="1" rowspan="1">82.0</td>
<td align="left" colspan="1" rowspan="1">98.7</td>
<td align="left" colspan="1" rowspan="1">99.5</td>
<td align="center" colspan="1" rowspan="1">85.0</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MIP<sup>[[<a href="#CR47" class="usa-link" aria-describedby="CR47">47</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">70.84</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="center" colspan="1" rowspan="1">66.41</td>
<td align="center" colspan="1" rowspan="1">78.80</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="center" colspan="1" rowspan="1">79.92</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">AGCC<sup>[[<a href="#CR48" class="usa-link" aria-describedby="CR48">48</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">75.91</td>
<td align="left" colspan="1" rowspan="1">97.01</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="center" colspan="1" rowspan="1">72.96</td>
<td align="center" colspan="1" rowspan="1">79.34</td>
<td align="left" colspan="1" rowspan="1">98.97</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="center" colspan="1" rowspan="1">84.62</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">RCC<sup>[[<a href="#CR49" class="usa-link" aria-describedby="CR49">49</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2024</td>
<td align="center" colspan="1" rowspan="1">72.57</td>
<td align="left" colspan="1" rowspan="1">96.60</td>
<td align="left" colspan="1" rowspan="1">98.89</td>
<td align="center" colspan="1" rowspan="1">68.61</td>
<td align="center" colspan="1" rowspan="1">78.01</td>
<td align="left" colspan="1" rowspan="1">98.13</td>
<td align="left" colspan="1" rowspan="1">99.64</td>
<td align="center" colspan="1" rowspan="1">81.39</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DNS<sup>[[<a href="#CR50" class="usa-link" aria-describedby="CR50">50</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">ECCV24</td>
<td align="center" colspan="1" rowspan="1">77.27</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="center" colspan="1" rowspan="1">
<strong>74.35</strong>
</td>
<td align="center" colspan="1" rowspan="1">84.21</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="center" colspan="1" rowspan="1">
<em>86.83</em>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MSCMNet<sup>[[<a href="#CR54" class="usa-link" aria-describedby="CR54">54</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2025</td>
<td align="center" colspan="1" rowspan="1">
<em>78.53</em>
</td>
<td align="left" colspan="1" rowspan="1">97.51</td>
<td align="left" colspan="1" rowspan="1">99.23</td>
<td align="center" colspan="1" rowspan="1">
<em>74.20</em>
</td>
<td align="center" colspan="1" rowspan="1">83.0</td>
<td align="left" colspan="1" rowspan="1">98.99</td>
<td align="left" colspan="1" rowspan="1">99.8</td>
<td align="center" colspan="1" rowspan="1">85.54</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">LAReViT<sup>[[<a href="#CR55" class="usa-link" aria-describedby="CR55">55</a>]]</sup>
</td>
<td align="left" colspan="1" rowspan="1">2025</td>
<td align="center" colspan="1" rowspan="1">76.71</td>
<td align="left" colspan="1" rowspan="1">97.33</td>
<td align="left" colspan="1" rowspan="1">99.05</td>
<td align="center" colspan="1" rowspan="1">72.95</td>
<td align="center" colspan="1" rowspan="1">
<em>84.22</em>
</td>
<td align="left" colspan="1" rowspan="1">
<em>99.02</em>
</td>
<td align="left" colspan="1" rowspan="1">
<em>99.85</em>
</td>
<td align="center" colspan="1" rowspan="1">86.26</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">HIW(ours)</td>
<td align="left" colspan="1" rowspan="1"></td>
<td align="center" colspan="1" rowspan="1">
<strong>85.05</strong>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>97.67</strong>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>99.67</strong>
</td>
<td align="center" colspan="1" rowspan="1">68.15</td>
<td align="center" colspan="1" rowspan="1">
<strong>87.50</strong>
</td>
<td align="left" colspan="1" rowspan="1">
<strong>99.32</strong>
</td>
<td align="left" colspan="1" rowspan="1">99.11</td>
<td align="center" colspan="1" rowspan="1">
<strong>87.75</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par78">The triplet loss employed by HIW-Net emphasizes the distance relationships between difficult sample pairs, aiming to place the ranking of target people images at front and enhance the Rank index. While the network effectively captures the shape and other key features of target people, it does not sufficiently distinguish between non-target people. This lack of fine-grained discrimination causes non-target people to appear relatively higher in the overall ranking, thereby affecting the mAP.</p></section><section id="Sec12"><h3 class="pmc_sec_title">Ablation studies and analyses</h3>
<p id="Par81">In this section, we conduct ablation studies and analyses to evaluate the effectiveness of each component in this paper proposed High-order Interaction and Wavelet Convolution Network (HIW). All experiments were conducted on the RegDB dataset under the same baseline in the IR-to-VIS mode. The wavelet convolution kernel size and shape loss weight are consistent. Results are shown in Table <a href="#Tab3" class="usa-link">3</a>.</p>
<section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>The impact of each component on HIW-Net. All experiments are conducted in the IR to VIS mode, with wavelet Convolution kernel sizes of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq77"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/d4521d9b214b/d33e2283.gif" loading="lazy" id="d33e2283" alt="Inline graphic"></span>and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq78"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dee23de8cc2f/d33e2289.gif" loading="lazy" id="d33e2289" alt="Inline graphic"></span>, and shape loss and modality loss coefficients set to 0.1 and 0.9, respectively.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1"></th>
<th align="left" rowspan="2" colspan="1">Methods</th>
<th align="left" colspan="1" rowspan="1">RegDB</th>
</tr>
<tr><th align="left" colspan="1" rowspan="1">rank-1/mAP</th></tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">Baseline</td>
<td align="center" colspan="1" rowspan="1">87.47%/63.84%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">2</td>
<td align="left" colspan="1" rowspan="1">Baseline + TPFI</td>
<td align="center" colspan="1" rowspan="1">90.8%/83.92%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">3</td>
<td align="left" colspan="1" rowspan="1">+wtDFM</td>
<td align="center" colspan="1" rowspan="1">90.15%/82.00%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">4</td>
<td align="left" colspan="1" rowspan="1">+shape</td>
<td align="center" colspan="1" rowspan="1">90.07%/83.03%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">5</td>
<td align="left" colspan="1" rowspan="1">+TPFI + wtDFM</td>
<td align="center" colspan="1" rowspan="1">91.39%/84.01%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">6</td>
<td align="left" colspan="1" rowspan="1">+TPFI + shape</td>
<td align="center" colspan="1" rowspan="1">91.17%/84.05%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">7</td>
<td align="left" colspan="1" rowspan="1">+wtDFM + shape</td>
<td align="center" colspan="1" rowspan="1">92.97%/84.20%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">8</td>
<td align="left" colspan="1" rowspan="1">+TPFI + wtDFM + shape</td>
<td align="center" colspan="1" rowspan="1">94.32%/87.18%</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par82">Adding each module alone increases the rank-1 metric by over 2.6% and the mAP metric by over 18%, proving each module’s effectiveness. When the wtDFM module and Shape dataset are added together, the rank-1 metric increases by 5.43%, and the mAP metric by 20.36%, this shows the complementarity of the wtDFM module and Shape dataset, both enhancing focus on shape features. Using all three modules at once boosts the rank-1 metric by 6.85% and the mAP metric by 23.34%.</p>
<p id="Par84"><strong>The effectiveness of third-order primitive feature interaction(TPFI): </strong>As shown in Table <a href="#Tab3" class="usa-link">3</a>. Adding the TPFI module to the baseline improves the rank-1 by 3.33% and the mAP by 20.38%. Visualizations in Fig. <a href="#Fig5" class="usa-link">5</a> show that features from general second-order interaction modules (Fig. <a href="#Fig5" class="usa-link">5</a>.(b)) lose some chest, abdomen, and foot features compared to the primitive features (Fig. <a href="#Fig5" class="usa-link">5</a>.(a)). However, Fig. <a href="#Fig5" class="usa-link">5</a>.c, which uses the TPFI module to incorporate primitive features into deep network inputs, reduces such feature loss.(Fig .<a href="#Fig6" class="usa-link">6</a>)</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/ec8fe31077bc/41598_2025_14978_Fig5_HTML.jpg" loading="lazy" id="MO5" height="480" width="787" alt="Fig. 5"></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Third-order Primitive Feature Interaction.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/215b6cfee86a/41598_2025_14978_Fig6_HTML.jpg" loading="lazy" id="MO7" height="356" width="633" alt="Fig. 6"></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> represents the primitive feature map, <strong>b</strong> depicts the second-order feature interaction map, and <strong>c</strong> illustrates the third-order primitive feature interaction(TPFI) map.</p></figcaption></figure><p id="Par86"><strong>The effectiveness of diverse features mining for wavelet convolution(wtDFM)</strong>As shown in Table <a href="#Tab3" class="usa-link">3</a>, adding the wtDFM module to the baseline boosts the rank-1 by 2.68% and the mAP by 18.16%. Visualizations in Fig. <a href="#Fig7" class="usa-link">7</a>.(b), which uses the wtDFM module, capture more features like the chest, abdomen, and legs, and focus more on the human silhouette than Fig. <a href="#Fig7" class="usa-link">7</a>.(a), which doesn’t use the module.</p>
<figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/1ee6f940cb15/41598_2025_14978_Fig7_HTML.jpg" loading="lazy" id="MO8" height="537" width="768" alt="Fig. 7"></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> shows the feature map of the module without wtDFM, while <strong>b</strong> depicts the feature map of the module with wtDFM.</p></figcaption></figure><p id="Par87">To determine the optimal wavelet convolution kernel size for the wtDFM module, this section experiments with kernel sizes of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq79"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/08951a9ed47d/d33e2431.gif" loading="lazy" id="d33e2431" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq80"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dee23de8cc2f/d33e2437.gif" loading="lazy" id="d33e2437" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq81"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/08951a9ed47d/d33e2443.gif" loading="lazy" id="d33e2443" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq82"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/fe32181dc30e/d33e2449.gif" loading="lazy" id="d33e2449" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq83"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dee23de8cc2f/d33e2455.gif" loading="lazy" id="d33e2455" alt="Inline graphic"></span>, as shown in Table <a href="#Tab4" class="usa-link">4</a>. When the wavelet convolution kernel size is <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq84"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/08951a9ed47d/d33e2465.gif" loading="lazy" id="d33e2465" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq85"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dee23de8cc2f/d33e2471.gif" loading="lazy" id="d33e2471" alt="Inline graphic"></span>, the network achieves the best performance.</p>
<section class="tw xbox font-sm" id="Tab4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>The impact of wavelet Convolution kernel size on HIW-Net. All experiments set shape loss to 0.1 and modality loss to 0.9.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">wtConv kernel size</th>
<th align="left" colspan="2" rowspan="1">RegDB(Rank-1)</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">VIS to IR</th>
<th align="left" colspan="1" rowspan="1">IR to VIS</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq86"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/fe32181dc30e/d33e2519.gif" loading="lazy" id="d33e2519" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq87"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dee23de8cc2f/d33e2525.gif" loading="lazy" id="d33e2525" alt="Inline graphic"></span>
</td>
<td align="center" colspan="1" rowspan="1">91.70</td>
<td align="center" colspan="1" rowspan="1">90.05</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq88"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/08951a9ed47d/d33e2537.gif" loading="lazy" id="d33e2537" alt="Inline graphic"></span> and 7<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq89"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/e74d9179d84a/d33e2543.gif" loading="lazy" id="d33e2543" alt="Inline graphic"></span>
</td>
<td align="center" colspan="1" rowspan="1">92.33</td>
<td align="center" colspan="1" rowspan="1">92.33</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq90"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/08951a9ed47d/d33e2555.gif" loading="lazy" id="d33e2555" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq91"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dee23de8cc2f/d33e2561.gif" loading="lazy" id="d33e2561" alt="Inline graphic"></span>
</td>
<td align="center" colspan="1" rowspan="1">93.74</td>
<td align="center" colspan="1" rowspan="1">92.48</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par90"><strong>The effectiveness of shape dateset</strong>: As shown in Table <a href="#Tab3" class="usa-link">3</a>, training on the baseline with the addition of the shape dataset increases the rank-1 accuracy by 2.60% and the mAP value by 19.19%. The weight coefficients of the model loss and shape loss are adjusted to explore the optimal combination, As shown in Table <a href="#Tab5" class="usa-link">5</a>, when <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq92"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/630702c3c913/d33e2581.gif" loading="lazy" id="d33e2581" alt="Inline graphic"></span> and<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq93"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/987b14814bff/d33e2587.gif" loading="lazy" id="d33e2587" alt="Inline graphic"></span>, the network achieve the best performance. Figure <a href="#Fig8" class="usa-link">8</a> visualizes feature graphs of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq94"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/630702c3c913/d33e2597.gif" loading="lazy" id="d33e2597" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq95"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/9845166b8690/d33e2603.gif" loading="lazy" id="d33e2603" alt="Inline graphic"></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq96"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/e97db3d707af/d33e2609.gif" loading="lazy" id="d33e2609" alt="Inline graphic"></span>, showing that as the weight coefficient of shape loss increases, the network places greater focus on shape features.</p>
<section class="tw xbox font-sm" id="Tab5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>The impact of weight coefficients <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq97"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/d15a6b778733/d33e2624.gif" loading="lazy" id="d33e2624" alt="Inline graphic"></span>and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq98"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/b0055a7d219a/d33e2630.gif" loading="lazy" id="d33e2630" alt="Inline graphic"></span> on HIW-Net. All experiments used wavelet Convolution kernel size of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq99"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/08951a9ed47d/d33e2636.gif" loading="lazy" id="d33e2636" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq100"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/dee23de8cc2f/d33e2642.gif" loading="lazy" id="d33e2642" alt="Inline graphic"></span>.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq101"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/baeff91cfd5d/d33e2656.gif" loading="lazy" id="d33e2656" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq102"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/b0055a7d219a/d33e2662.gif" loading="lazy" id="d33e2662" alt="Inline graphic"></span>
</th>
<th align="left" colspan="2" rowspan="1">RegDB(Rank-1)</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">VIS to IR</th>
<th align="left" colspan="1" rowspan="1">IR to VIS</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq103"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/e97db3d707af/d33e2678.gif" loading="lazy" id="d33e2678" alt="Inline graphic"></span>,<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq104"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/38fc90a4f5ad/d33e2684.gif" loading="lazy" id="d33e2684" alt="Inline graphic"></span>
</td>
<td align="center" colspan="1" rowspan="1">91.03</td>
<td align="center" colspan="1" rowspan="1">90.97</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq105"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/9845166b8690/d33e2696.gif" loading="lazy" id="d33e2696" alt="Inline graphic"></span>,<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq106"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/4eceb2422619/d33e2702.gif" loading="lazy" id="d33e2702" alt="Inline graphic"></span>
</td>
<td align="center" colspan="1" rowspan="1">89.80</td>
<td align="center" colspan="1" rowspan="1">91.80</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq107"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/630702c3c913/d33e2714.gif" loading="lazy" id="d33e2714" alt="Inline graphic"></span>,<span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq108"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/987b14814bff/d33e2720.gif" loading="lazy" id="d33e2720" alt="Inline graphic"></span>
</td>
<td align="center" colspan="1" rowspan="1">93.74</td>
<td align="center" colspan="1" rowspan="1">92.48</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><figure class="fig xbox font-sm" id="Fig8"><h4 class="obj_head">Fig. 8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/849956d3e5e3/41598_2025_14978_Fig8_HTML.jpg" loading="lazy" id="MO9" height="354" width="626" alt="Fig. 8"></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p><strong>a</strong> represents the feature map of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq109"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/54baf3f0d045/d33e2740.gif" loading="lazy" id="d33e2740" alt="Inline graphic"></span>, <strong>b</strong> represents the feature map of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq110"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/aa03e9bf0eb7/d33e2749.gif" loading="lazy" id="d33e2749" alt="Inline graphic"></span>, and <strong>c</strong> represents the feature map of <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq111"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/2ad73db3f57e/d33e2758.gif" loading="lazy" id="d33e2758" alt="Inline graphic"></span>.</p></figcaption></figure><p id="Par93"><strong>Visualization Analyses</strong>:</p>
<p id="Par96">To investigate the effectiveness of HIW-Net, we visualize the inter class and intra class distances on the SYSU-MM01 dataset, as shown in Fig. <a href="#Fig9" class="usa-link">9</a>. This indicates that HIW-Net achieves a larger gap between intra-class and inter-class distances, where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq112"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/e843e00f0b4f/d33e2796.gif" loading="lazy" id="d33e2796" alt="Inline graphic"></span>.Thus, HIW-Net can effectively reduce the modality discrepancy between the VIS and the IR images. As shown in Fig. <a href="#Fig10" class="usa-link">10</a>, t-SNE visualization of the identity features learned by the model reveals that the baseline model’s projections for the same identity are scattered and hard to distinguish. In contrast, HIW-Net, leveraging TPFI and wtDFM, extracts more comprehensive and diverse features. This enables it to effectively distinguish and aggregate people features.</p>
<figure class="fig xbox font-sm" id="Fig9"><h4 class="obj_head">Fig. 9.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370930_41598_2025_14978_Fig9_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/6317898c2821/41598_2025_14978_Fig9_HTML.jpg" loading="lazy" id="MO10" height="279" width="708" alt="Fig. 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The feature distances of intra-and-inter classes visualization.</p></figcaption></figure><figure class="fig xbox font-sm" id="Fig10"><h4 class="obj_head">Fig. 10.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370930_41598_2025_14978_Fig10_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/b22a7e78d08f/41598_2025_14978_Fig10_HTML.jpg" loading="lazy" id="MO11" height="319" width="794" alt="Fig. 10"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig10/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>T-SNE visualization result of baseline and HIW-Net.</p></figcaption></figure><p id="Par98">To further show the effectiveness of HIW-Net, we also show some rank-10 retrieval results of HIW-Net on SYSU-MM01 dataset in Fig. <a href="#Fig11" class="usa-link">11</a>, the red ones mean the incorrect matches. The results show that HIW-NET can achieve better person re-identification performance compared to the baseline.</p>
<figure class="fig xbox font-sm" id="Fig11"><h4 class="obj_head">Fig. 11.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370930_41598_2025_14978_Fig11_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/396a/12370930/9f217a42dfdf/41598_2025_14978_Fig11_HTML.jpg" loading="lazy" id="MO12" height="370" width="794" alt="Fig. 11"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig11/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Some Rank-10 retrieval results obtained by the baseline and the proposed HIW-NET on SYSU-MM01 dataset.</p></figcaption></figure></section></section><section id="Sec13"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par99">This paper addresses visible-infrared person re-identification by studying how to reduce feature loss during network feature extraction and more effectively mine features while reducing modality differences. The proposed HIW-Net, which consists of third-order primitive feature interaction (TPFI) and wavelet convolution diversity feature expansion (wtDFM), addresses these challenges. In addition, a shape loss weighting strategy is introduced to enhance the network’s attention to shape features. This paper also creates the RegDB_Shape dataset, manually annotating the low-quality RegDB dataset to generate shape maps of person. Extensive experiments on the SYSU and RegDB datasets demonstrate that the proposed HIW-Net outperforms existing methods.</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>All authors contributed equally to this work. Li Ma: Funding acquisition, Gave technical support and conceptual advice, Review. Rui Kong: wrote code, ran the model, and analysed output data, administered the experiment and wrote the manuscript. XinGuan Dai: Gave technical support and conceptual advice, Review. All authors discussed the results and implications and commented on the manuscript at all stages.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The SYSU-MM01 dataset and RegDB dataset used in the paper are publicly available and can be accessed through https://github.com/wuancong/SYSU-MM01 and https://github.com/WSR-001/HIW.The RegDB_Shape dataset is created based on RegDB and can be accessed through https://github.com/WSR-001/HIW.</p></section><section id="notes3"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar8"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par100">The authors declare no competing interests.</p></section><section id="notes5"><p>.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Huang, N. et al. Deep learning for visible-infrared cross-modality person re-identification: A comprehensive review[J]. <em>Inform. Fusion</em>. <strong>91</strong>, 396–411 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Huang,%20N.%20et%20al.%20Deep%20learning%20for%20visible-infrared%20cross-modality%20person%20re-identification:%20A%20comprehensive%20review%5BJ%5D.%20Inform.%20Fusion.%2091,%20396%E2%80%93411%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Ning, E. et al. Occluded person re-identification with deep learning: a survey and perspectives[J]. <em>Expert Syst. Appl.</em><strong>239</strong>, 122419 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ning,%20E.%20et%20al.%20Occluded%20person%20re-identification%20with%20deep%20learning:%20a%20survey%20and%20perspectives%5BJ%5D.%20Expert%20Syst.%20Appl.239,%20122419%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Ming, Z. et al. Deep learning-based person re-identification methods: A survey and outlook of recent works[J]. <em>Image Vis. Comput.</em><strong>119</strong>, 104394 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ming,%20Z.%20et%20al.%20Deep%20learning-based%20person%20re-identification%20methods:%20A%20survey%20and%20outlook%20of%20recent%20works%5BJ%5D.%20Image%20Vis.%20Comput.119,%20104394%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR4">
<span class="label">4.</span><cite>Liao, S. &amp; Shao, L. Graph sampling based deep metric learning for generalizable person re-identification[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 7359–7368. (2022).</cite>
</li>
<li id="CR5">
<span class="label">5.</span><cite>Pu, N. et al. Lifelong person re-identification via adaptive knowledge accumulation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 7901–7910. (2021).</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>Tan, L. et al. Occluded Person Re-identification via Saliency-Guided Patch Transfer[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 38(5): 5070–5078. (2024).</cite>
</li>
<li id="CR7">
<span class="label">7.</span><cite>He, L. et al. Fastreid: A pytorch toolbox for general instance re-identification[C]//Proceedings of the 31st ACM International Conference on Multimedia. : 9664–9667. (2023).</cite>
</li>
<li id="CR8">
<span class="label">8.</span><cite>Li, S., Sun, L. &amp; Li, Q. CLIP-ReID: exploiting vision-language model for image re-identification without concrete text labels[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 37(1): 1405–1413. (2023).</cite>
</li>
<li id="CR9">
<span class="label">9.</span><cite>Du, Y., Zhao, Z. &amp; Su, F. Y. Y. D. S. Visible-Infrared Person Re-Identification with Coarse Descriptions[J]. arXiv preprint arXiv:2403.04183, (2024).</cite>
</li>
<li id="CR10">
<span class="label">10.</span><cite>Kim, S., Gwon, S. &amp; Seo, K. Enhancing diverse intra-identity representation for visible-infrared person re-identification[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. : 2513–2522. (2024).</cite>
</li>
<li id="CR11">
<span class="label">11.</span><cite>Chen, Y. et al. Neural feature search for rgb-infrared person re-identification[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 587–597. (2021).</cite>
</li>
<li id="CR12">
<span class="label">12.</span><cite>Zhang, Q. et al. Fmcnet: Feature-level modality compensation for visible-infrared person re-identification[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 7349–7358. (2022).</cite>
</li>
<li id="CR13">
<span class="label">13.</span><cite>Wang, J. et al. Visible–Infrared person Re-Identification via global feature constraints led by local features[J]. <em>Electronics</em><strong>11</strong> (17), 2645 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20J.%20et%20al.%20Visible%E2%80%93Infrared%20person%20Re-Identification%20via%20global%20feature%20constraints%20led%20by%20local%20features%5BJ%5D.%20Electronics11%20(17),%202645%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Hou, F. et al. Review on infrared imaging technology[J]. <em>Sustainability</em><strong>14</strong> (18), 11161 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Hou,%20F.%20et%20al.%20Review%20on%20infrared%20imaging%20technology%5BJ%5D.%20Sustainability14%20(18),%2011161%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR15">
<span class="label">15.</span><cite>Bustos, N. et al. A systematic literature review on object detection using near infrared and thermal images[J]. Neurocomputing, : 126804. (2023).</cite>
</li>
<li id="CR16">
<span class="label">16.</span><cite>Zhong, X. et al. Grayscale enhancement colorization network for Visible-infrared person Re-identification[J].IEEE transactions on circuits and systems for video technology, 2021, <strong>PP</strong>(99):1–1 .10.1109/TCSVT.2021.3072171</cite>
</li>
<li id="CR17">
<span class="label">17.</span><cite>Yu, H. et al. Modality unifying network for visible-infrared person re-identification[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. : 11185–11195. (2023).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>Lin, Y. &amp; Wang, B. Cross-modality person re-identification via modality-synergy alignment learning[J]. <em>Mach. Vis. Appl.</em><strong>35</strong> (6), 130 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Lin,%20Y.%20&amp;%20Wang,%20B.%20Cross-modality%20person%20re-identification%20via%20modality-synergy%20alignment%20learning%5BJ%5D.%20Mach.%20Vis.%20Appl.35%20(6),%20130%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19">
<span class="label">19.</span><cite>Zheng, A. et al. Visible-infrared person re-identification via specific and shared representations learning[J]. <em>Visual Intell.</em><strong>1</strong> (1), 29 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zheng,%20A.%20et%20al.%20Visible-infrared%20person%20re-identification%20via%20specific%20and%20shared%20representations%20learning%5BJ%5D.%20Visual%20Intell.1%20(1),%2029%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Fu, C. et al. CM-NAS: Cross-modality neural architecture search for visible-infrared person re-identification[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. : 11823–11832. (2021).</cite>
</li>
<li id="CR21">
<span class="label">21.</span><cite>Liu, H., Tan, X. &amp; Zhou, X. Parameter sharing exploration and hetero-center triplet loss for visible-thermal person re-identification[J]. <em>IEEE Trans. Multimedia</em>. <strong>23</strong>, 4414–4425 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20H.,%20Tan,%20X.%20&amp;%20Zhou,%20X.%20Parameter%20sharing%20exploration%20and%20hetero-center%20triplet%20loss%20for%20visible-thermal%20person%20re-identification%5BJ%5D.%20IEEE%20Trans.%20Multimedia.%2023,%204414%E2%80%934425%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Feng, J., Wu, A. &amp; Zheng, W. S. Shape-erased feature learning for visible-infrared person re-identification[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 22752–22761. (2023).</cite>
</li>
<li id="CR23">
<span class="label">23.</span><cite>Li, P. et al. Self-correction for human parsing[J]. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em><strong>44</strong> (6), 3260–3271 (2020).</cite> [<a href="https://doi.org/10.1109/TPAMI.2020.3048039" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33373297/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Li,%20P.%20et%20al.%20Self-correction%20for%20human%20parsing%5BJ%5D.%20IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.44%20(6),%203260%E2%80%933271%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR24">
<span class="label">24.</span><cite>Yang, L. et al. Parsing r-cnn for instance-level human analysis[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 364–373. (2019).</cite>
</li>
<li id="CR25">
<span class="label">25.</span><cite>Chu, J. et al. Single-stage multi-human parsing via point sets and center-based offsets[C]//Proceedings of the 31st ACM International Conference on Multimedia. : 1863–1873. (2023).</cite>
</li>
<li id="CR26">
<span class="label">26.</span><cite>Kirillov, A. et al. Segment anything[C]//Proceedings of the IEEE/CVF international conference on computer vision. : 4015–4026. (2023).</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Ye, M. et al. Deep learning for person re-identification: A survey and outlook[J]. <em>IEEE Trans. Pattern Anal. Mach. Intell.</em><strong>44</strong> (6), 2872–2893 (2021).</cite> [<a href="https://doi.org/10.1109/TPAMI.2021.3054775" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/33497329/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Ye,%20M.%20et%20al.%20Deep%20learning%20for%20person%20re-identification:%20A%20survey%20and%20outlook%5BJ%5D.%20IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.44%20(6),%202872%E2%80%932893%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Chen, X. et al. Salience-guided cascaded suppression network for person re-identification[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 3300–3310. (2020).</cite>
</li>
<li id="CR29">
<span class="label">29.</span><cite>Zhu, Z. et al. Asymmetric non-local neural networks for semantic segmentation[C]//Proceedings of the IEEE/CVF international conference on computer vision. : 593–602. (2019).</cite>
</li>
<li id="CR30">
<span class="label">30.</span><cite>Zheng, N. et al. Probing synergistic High-order interaction in infrared and visible image fusion[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 26384–26395. (2024).</cite>
</li>
<li id="CR31">
<span class="label">31.</span><cite>Han, Y. et al. A new image fusion performance metric based on visual information fidelity[J]. <em>Inform. Fusion</em>. <strong>14</strong> (2), 127–135 (2013).</cite> [<a href="https://scholar.google.com/scholar_lookup?Han,%20Y.%20et%20al.%20A%20new%20image%20fusion%20performance%20metric%20based%20on%20visual%20information%20fidelity%5BJ%5D.%20Inform.%20Fusion.%2014%20(2),%20127%E2%80%93135%20(2013)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Zhang, Y. &amp; Wang, H. Diverse embedding expansion network and low-light cross-modality benchmark for visible-infrared person re-identification[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 2153–2162. (2023).</cite>
</li>
<li id="CR33">
<span class="label">33.</span><cite>Finder, S. E. et al. Wavelet convolutions for large receptive fields[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, : 363–380. (2024).</cite>
</li>
<li id="CR34">
<span class="label">34.</span><cite>Nguyen, D. T. et al. Person recognition system based on a combination of body images from visible light and thermal cameras[J]. <em>Sensors</em><strong>17</strong> (3), 605 (2017).
</cite> [<a href="https://doi.org/10.3390/s17030605" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC5375891/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28300783/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Nguyen,%20D.%20T.%20et%20al.%20Person%20recognition%20system%20based%20on%20a%20combination%20of%20body%20images%20from%20visible%20light%20and%20thermal%20cameras%5BJ%5D.%20Sensors17%20(3),%20605%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Prosser, B. J. et al. Person re-identification by support vector ranking[C]//Bmvc. <strong>2</strong>(5): 6. (2010).</cite>
</li>
<li id="CR36">
<span class="label">36.</span><cite>Zheng, L. et al. Scalable person re-identification: A benchmark[C]//Proceedings of the IEEE international conference on computer vision. : 1116–1124. (2015).</cite>
</li>
<li id="CR37">
<span class="label">37.</span><cite>Lu, H., Zou, X. &amp; Zhang, P. Learning progressive modality-shared transformers for effective visible-infrared person re-identification[C]//Proceedings of the AAAI conference on artificial intelligence. 37(2): 1835–1843. (2023).</cite>
</li>
<li id="CR38">
<span class="label">38.</span><cite>Liu, H., Xia, D. &amp; Jiang, W. Towards homogeneous modality learning and multi-granularity information exploration for visible-infrared person re-identification[J]. <em>IEEE J. Selec. Topics Signal Process.</em><strong>17</strong> (3), 545–559 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20H.,%20Xia,%20D.%20&amp;%20Jiang,%20W.%20Towards%20homogeneous%20modality%20learning%20and%20multi-granularity%20information%20exploration%20for%20visible-infrared%20person%20re-identification%5BJ%5D.%20IEEE%20J.%20Selec.%20Topics%20Signal%20Process.17%20(3),%20545%E2%80%93559%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR39">
<span class="label">39.</span><cite>Yang, M., Huang, Z. &amp; Peng, X. Robust object re-identification with coupled noisy labels[J]. <em>Int. J. Comput. Vision</em>. <strong>132</strong> (7), 2511–2529 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yang,%20M.,%20Huang,%20Z.%20&amp;%20Peng,%20X.%20Robust%20object%20re-identification%20with%20coupled%20noisy%20labels%5BJ%5D.%20Int.%20J.%20Comput.%20Vision.%20132%20(7),%202511%E2%80%932529%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Liang, T. et al. <em>Bridging the Gap: multi-level cross-modality Joint Alignment for visible-infrared Person re-identification[J]</em> (IEEE Transactions on Circuits and Systems for Video Technology, 2024).</cite>
</li>
<li id="CR41">
<span class="label">41.</span><cite>Chan, S. et al. <em>Diverse-feature Collaborative Progressive Learning for visible-infrared Person re-identification[J]</em> (IEEE Transactions on Industrial Informatics, 2024).</cite>
</li>
<li id="CR42">
<span class="label">42.</span><cite>Yang, X. et al. Cooperative separation of modality shared-specific features for visible-infrared person re-identification[J]. <em>IEEE Trans. Multimedia</em>, (2024).</cite>
</li>
<li id="CR43">
<span class="label">43.</span><cite>Qian, Z., Lin, Y. &amp; Du, B. Visible–infrared person re-identification via patch-mixed cross-modality learning[J]. <em>Pattern Recogn.</em><strong>157</strong>, 110873 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Qian,%20Z.,%20Lin,%20Y.%20&amp;%20Du,%20B.%20Visible%E2%80%93infrared%20person%20re-identification%20via%20patch-mixed%20cross-modality%20learning%5BJ%5D.%20Pattern%20Recogn.157,%20110873%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR44">
<span class="label">44.</span><cite>Yu, X. et al. <em>Clip-driven Semantic Discovery Network for visible-infrared Person re-identification[J]</em> (IEEE Transactions on Multimedia, 2025).</cite>
</li>
<li id="CR45">
<span class="label">45.</span><cite>Wu, R. et al. Enhancing Visible-Infrared Person Re-identification with Modality-and Instance-aware Visual Prompt Learning[C]//Proceedings of the 2024 International Conference on Multimedia Retrieval. : 579–588. (2024).</cite>
</li>
<li id="CR46">
<span class="label">46.</span><cite>Yu, H. et al. Discovering attention-guided cross-modality correlation for visible–infrared person re-identification[J]. <em>Pattern Recogn.</em><strong>155</strong>, 110643 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yu,%20H.%20et%20al.%20Discovering%20attention-guided%20cross-modality%20correlation%20for%20visible%E2%80%93infrared%20person%20re-identification%5BJ%5D.%20Pattern%20Recogn.155,%20110643%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR47">
<span class="label">47.</span><cite>Lin, Z. &amp; Wang, B. Visible-Infrared Person Re-Indentification via Feature Fusion and Deep Mutual Learning[C]//The 16th Asian Conference on Machine Learning (Conference Track). (2024).</cite>
</li>
<li id="CR48">
<span class="label">48.</span><cite>Jiang, Y. et al. Domain shifting: A generalized solution for heterogeneous cross-modality person re-identification[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, : 289–306. (2024).</cite>
</li>
<li id="CR49">
<span class="label">49.</span><cite>Schroff, F., Kalenichenko, D., Philbin, J. &amp; Facenet A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. : 815–823. (2015).</cite>
</li>
<li id="CR50">
<span class="label">50.</span><cite>Ranasinghe, K. et al. Orthogonal projection loss[C]//Proceedings of the IEEE/CVF international conference on computer vision. : 12333–12343. (2021).</cite>
</li>
<li id="CR51">
<span class="label">51.</span><cite>Mao, A., Mohri, M. &amp; Zhong, Y. Cross-entropy loss functions: Theoretical analysis and applications[C]//International conference on Machine learning. PMLR, : 23803–23828. (2023).</cite>
</li>
<li id="CR52">
<span class="label">52.</span><cite>Hua, X. et al. MSCMNet: Multi-scale semantic correlation mining for Visible-Infrared person Re-Identification[J]. <em>Pattern Recogn.</em><strong>159</strong>, 111090 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Hua,%20X.%20et%20al.%20MSCMNet:%20Multi-scale%20semantic%20correlation%20mining%20for%20Visible-Infrared%20person%20Re-Identification%5BJ%5D.%20Pattern%20Recogn.159,%20111090%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR53">
<span class="label">53.</span><cite>Hua, X. et al. Local-Aware residual attention vision transformer for Visible-Infrared person Re-Identification[J]. <em>ACM Trans. Multimedia Comput. Commun. Appl.</em><strong>21</strong> (5), 1–24 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Hua,%20X.%20et%20al.%20Local-Aware%20residual%20attention%20vision%20transformer%20for%20Visible-Infrared%20person%20Re-Identification%5BJ%5D.%20ACM%20Trans.%20Multimedia%20Comput.%20Commun.%20Appl.21%20(5),%201%E2%80%9324%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR54">
<span class="label">54.</span><cite>Yang, X. et al. Bidirectional modality information interaction for Visible–Infrared person Re-identification[J]. <em>Pattern Recogn.</em><strong>161</strong>, 111301 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Yang,%20X.%20et%20al.%20Bidirectional%20modality%20information%20interaction%20for%20Visible%E2%80%93Infrared%20person%20Re-identification%5BJ%5D.%20Pattern%20Recogn.161,%20111301%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR55">
<span class="label">55.</span><cite>Zheng, H. et al. Visible-infrared person re-identification: A comprehensive survey and a new setting[J]. <em>Electronics</em><strong>11</strong> (3), 454 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zheng,%20H.%20et%20al.%20Visible-infrared%20person%20re-identification:%20A%20comprehensive%20survey%20and%20a%20new%20setting%5BJ%5D.%20Electronics11%20(3),%20454%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The SYSU-MM01 dataset and RegDB dataset used in the paper are publicly available and can be accessed through https://github.com/wuancong/SYSU-MM01 and https://github.com/WSR-001/HIW.The RegDB_Shape dataset is created based on RegDB and can be accessed through https://github.com/WSR-001/HIW.</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-14978-x"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_14978.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (4.1 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12370930/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12370930/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370930%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370930/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12370930/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12370930/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40841746/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12370930/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40841746/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12370930/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12370930/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="DMNq0U1uRoxr5JuJcQYXP6e1Ay6HrAAJ7JjOrROYdJPRu7QSgIqbgdsyX4sb7FKw">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
