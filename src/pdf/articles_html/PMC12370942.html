
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Dynamic atrous attention and dual branch context fusion for cross scale Building segmentation in high resolution remote sensing imagery - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="D2E4A6A68AF2552305A6A6002F7E6D95.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370942/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Dynamic atrous attention and dual branch context fusion for cross scale Building segmentation in high resolution remote sensing imagery">
<meta name="citation_author" content="Yaohui Liu">
<meta name="citation_author_institution" content="School of Surveying and Geo-Informatics, Shandong Jianzhu University, Jinan, 250101 China">
<meta name="citation_author" content="Shuzhe Zhang">
<meta name="citation_author_institution" content="School of Surveying and Geo-Informatics, Shandong Jianzhu University, Jinan, 250101 China">
<meta name="citation_author" content="Xinkai Wang">
<meta name="citation_author_institution" content="School of Surveying and Geo-Informatics, Shandong Jianzhu University, Jinan, 250101 China">
<meta name="citation_author" content="Rui Zhai">
<meta name="citation_author_institution" content="China Unicom Shandong Branch, Jinan, 250101 China">
<meta name="citation_author" content="Hu Jiang">
<meta name="citation_author_institution" content="School of Surveying and Geo-Informatics, Shandong Jianzhu University, Jinan, 250101 China">
<meta name="citation_author_institution" content="State Key Laboratory of Precision Geodesy, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, 430077 China">
<meta name="citation_author" content="Lingjia Kong">
<meta name="citation_author_institution" content="School of Mining Engineering, Heilongjiang University of Science and Technology, Haerbin, 150000 China">
<meta name="citation_publication_date" content="2025 Aug 21">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30800">
<meta name="citation_doi" content="10.1038/s41598-025-14751-0">
<meta name="citation_pmid" content="40841730">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370942/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370942/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370942/pdf/41598_2025_Article_14751.pdf">
<meta name="description" content="Building segmentation of high-resolution remote sensing images using deep learning effectively reduces labor costs, but still faces the key challenges of effectively modeling cross-scale contextual relationships and preserving fine spatial details. ...">
<meta name="og:title" content="Dynamic atrous attention and dual branch context fusion for cross scale Building segmentation in high resolution remote sensing imagery">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Building segmentation of high-resolution remote sensing images using deep learning effectively reduces labor costs, but still faces the key challenges of effectively modeling cross-scale contextual relationships and preserving fine spatial details. ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370942/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12370942">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-14751-0"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_14751.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370942%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12370942/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12370942/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370942/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 21;15:30800. doi: <a href="https://doi.org/10.1038/s41598-025-14751-0" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-14751-0</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Dynamic atrous attention and dual branch context fusion for cross scale Building segmentation in high resolution remote sensing imagery</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20Y%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Yaohui Liu</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Yaohui Liu</span></h3>
<div class="p">
<sup>1</sup>School of Surveying and Geo-Informatics, Shandong Jianzhu University, Jinan, 250101 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Liu%20Y%22%5BAuthor%5D" class="usa-link"><span class="name western">Yaohui Liu</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Shuzhe Zhang</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Shuzhe Zhang</span></h3>
<div class="p">
<sup>1</sup>School of Surveying and Geo-Informatics, Shandong Jianzhu University, Jinan, 250101 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Shuzhe Zhang</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20X%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Xinkai Wang</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Xinkai Wang</span></h3>
<div class="p">
<sup>1</sup>School of Surveying and Geo-Informatics, Shandong Jianzhu University, Jinan, 250101 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Wang%20X%22%5BAuthor%5D" class="usa-link"><span class="name western">Xinkai Wang</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhai%20R%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Rui Zhai</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Rui Zhai</span></h3>
<div class="p">
<sup>2</sup>China Unicom Shandong Branch, Jinan, 250101 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhai%20R%22%5BAuthor%5D" class="usa-link"><span class="name western">Rui Zhai</span></a>
</div>
</div>
<sup>2,</sup><sup>✉</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Jiang%20H%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Hu Jiang</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Hu Jiang</span></h3>
<div class="p">
<sup>1</sup>School of Surveying and Geo-Informatics, Shandong Jianzhu University, Jinan, 250101 China </div>
<div class="p">
<sup>3</sup>State Key Laboratory of Precision Geodesy, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, 430077 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Jiang%20H%22%5BAuthor%5D" class="usa-link"><span class="name western">Hu Jiang</span></a>
</div>
</div>
<sup>1,</sup><sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kong%20L%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Lingjia Kong</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Lingjia Kong</span></h3>
<div class="p">
<sup>4</sup>School of Mining Engineering, Heilongjiang University of Science and Technology, Haerbin, 150000 China </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Kong%20L%22%5BAuthor%5D" class="usa-link"><span class="name western">Lingjia Kong</span></a>
</div>
</div>
<sup>4</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>School of Surveying and Geo-Informatics, Shandong Jianzhu University, Jinan, 250101 China </div>
<div id="Aff2">
<sup>2</sup>China Unicom Shandong Branch, Jinan, 250101 China </div>
<div id="Aff3">
<sup>3</sup>State Key Laboratory of Precision Geodesy, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, 430077 China </div>
<div id="Aff4">
<sup>4</sup>School of Mining Engineering, Heilongjiang University of Science and Technology, Haerbin, 150000 China </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 Mar 3; Accepted 2025 Aug 4; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12370942  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40841730/" class="usa-link">40841730</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Building segmentation of high-resolution remote sensing images using deep learning effectively reduces labor costs, but still faces the key challenges of effectively modeling cross-scale contextual relationships and preserving fine spatial details. Current Transformer-based approaches demonstrate superior long-range dependency modeling, but still suffer from the problem of progressive information loss during hierarchical feature encoding. Therefore, this study proposed a new semantic segmentation network named SegTDformer to extract buildings in remote sensing images. We designed a Dynamic Atrous Attention (DAA) fusion module that integrated multi-scale features from Transformer, constructing an information exchange between global information and local representational information. Among them, we introduced the Shift Operation module and the Self-Attention module, which adopt a dual-branch structure to respectively capture local spatial dependencies and global correlations, and perform weight coupling to achieve highly complementary contextual information fusion. Furthermore, we fused triplet attention with depth-wise separable convolutions, reducing computational requirements and mitigating potential overfitting scenarios. We benchmarked the model on three different datasets, including Massachusetts, INRIA, and WHU, and the results show that the model consistently outperforms existing models. Notably, on the Massachusetts dataset, the SegTDformer model achieved benchmark in mIoU, F1-score, and Overall Accuracy of 75.47%, 84.7%, and 94.61%, respectively, superseding other deep learning models. The proposed SegTDformer model exhibits enhanced precision in the extraction of urban structures from intricate environments and manifests a diminished rate of both omission and internal misclassification errors, particularly within the context of diminutive and expansive edifices.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Remote sensing, Building segmentation, Transformer, Attention mechanism, Dynamic atrous attention</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Aerospace engineering, Computer science</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">In recent years, with the rapid advancement of deep learning technologies and the continuous refinement of remote sensing techniques, the use of deep learning to extract building information from high-resolution remote sensing (HRS) imagery has become a highly efficient and accurate method<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>. This technology is applied in various fields including urban planning<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>urban dynamic monitoring<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>and disaster detection<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>. Although research on the extraction of buildings from HRS images using deep learning has achieved some results, there remain several challenges and issues to be addressed. For instance, the diversity and complex shapes of buildings in HRS images, coupled with the effects of occlusions and variations in illumination, can impact the accuracy and robustness of building extraction. Consequently, further optimization of deep learning algorithms to enhance the precision and efficiency of building extraction has become one of the significant research topics of the present.</p>
<p id="Par3">In recent years, the Convolutional Neural Network (CNN) model, as a type of deep learning neural network, has achieved notable success, especially with the Fully Convolutional Network (FCN)<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup> model demonstrating exceptional performance. FCN is an end-to-end network model that exclusively utilizes convolutional layers, avoiding the fully connected layers typical of traditional CNN models. This adaptation allows for better accommodation of input images of various sizes. With ongoing innovation by researchers, many models with outstanding structures have emerged, such as UNet<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup> and DeeplabV3+<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>. UNet features a U-shaped structure, processing images through an encode-decode mechanism to output segmentation results. The DeeplabV3 + model incorporates the Atrous Spatial Pyramid Pooling (ASPP) module, facilitating better integration of objects across different scales and enhancing segmentation accuracy.</p>
<p id="Par4">HRS imagery presents unique challenges for building segmentation due to two inherent characteristics: small-scale building structures and complex environmental contexts<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>. Conventional convolutional neural networks often exhibit limited capacity in capturing fine-grained semantic information at smaller scales<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>particularly when vegetation occlusion introduces ambiguous spatial relationships. These challenges necessitate architectural designs that enhance contextual reasoning and local dependency modeling within segmentation frameworks<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a>,<a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>. Critically, the effective utilization of multi-scale information remains a fundamental yet under-addressed requirement in HRS building segmentation. Small-scale targets<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup> (e.g., compact residential buildings) suffer severe feature degradation in standard encoder-decoder architectures due to repeated downsampling<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>while complex scene compositions demand adaptive fusion of hierarchical semantic features<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. Recent studies have demonstrated that static multi-scale fusion strategies in existing methods (e.g., ASPP, FPN) inadequately resolve the scale variance problem in HRS scenarios, where optimal receptive fields vary significantly across spatial locations. This limitation motivates our investigation into dynamic multi-scale feature interaction mechanisms tailored for HRS-specific challenges.</p>
<p id="Par5">Attention mechanisms have emerged as pivotal solutions to address these challenges through learnable feature recalibration. The research evolution progresses along three technical pathways: (1) Channel-spatial recalibration: SENet<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup> pioneered channel attention through squeeze-excitation blocks, while GENet<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup> extended this to spatial dimensions. Subsequent innovations like SKNet<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup> introduced multi-branch architectures with adaptive kernel selection, and CBAM<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup> systematically combined channel and spatial attention through sequential modules. TLSTM networks<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup> based on spatial attention increase transduction of long and short-term memory for time series detection (2) Global context modeling: Non-local networks<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup> established global self-attention for long-range dependencies, inspiring GCNet’s<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup> efficient channel-wise attention and DANet’s<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup> dual attention mechanism for spatial-channel context fusion. Practical implementations like OCRNet<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup> demonstrated effective class-conditional attention for urban scene parsing. In addition, to reduce information redundancy, FDNet<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup> optimizes spectral feature learning by compressing redundant information while highlighting information-rich spectral bands. MCKTNet<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup> combines channel discrete loss and spatial discrete loss to facilitate cross-modal migration of geometrical and semantic features, and reduces redundancy by minimizing differences in feature distribution. (3) Computational efficiency optimization: Recent works address computational overhead through dimension-aware interactions (Triplet Attention<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>, energy-based importance weighting (SimAM<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>, and hybrid convolution-attention designs (ACmix<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>. Notably, while these advancements enhance feature discrimination, their capacity for multi-scale context integration remains constrained by fixed-scale attention computations, particularly ineffective for resolving the scale-confusion issues prevalent in HRS building segmentation tasks. LWGANet<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup> cleverly utilizes redundant features to extract a wide range of spatial information from local to global scales without introducing additional complexity or computational overhead. This facilitates accurate feature extraction across multiple scales within an efficient framework. Facing the small object problem in remote sensing tasks, DecoupleNet<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup> preserves small object features during downsampling and enhances small and multi-scale object feature representation through a novel decoupling approach.</p>
<p id="Par6">The recent paradigm shift towards Vision Transformers (ViT)<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup> has further expanded architectural possibilities. Initial adaptations like DETR<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> introduced end-to-end detection through transformer encoders-decoders, while ViT established patch-based sequence processing for classification. Subsequent architectural innovations addressed computational scalability: Swin Transformer<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup> employed hierarchical feature learning with shifted windows, and SegFormer<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup> introduced an efficient pyramid transformer for semantic segmentation. However, current transformer-based approaches primarily focus on modeling long-range dependencies, often resulting in a loss of fine-grained spatial details of small-scale structures during patch embedding operations. Additionally, these methods exhibit limited dynamic adaptability to scale variations in complex urban landscapes. To accurately capture semantic information in remote sensing images, FTransDeepLab<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup> extends the encoder by stacking multi-scale Segformers to encode the input images into highly representative spatial features. DSymFuser<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup> fully exploits and exploits remote sensing images by adaptively aggregating local and global features extracted with CNN-Transformer network depth information. The above articles have made significant contributions to information mining. However, due to the richness of information in remote sensing images, redundancy and conflicting information are easily generated in the process of multi-scale fusion.</p>
<p id="Par7">To improve the efficiency of multi-scale information utilization, we designed a building extraction network named SegTDformer. Dynamic Atrous Attention (DAA) designed to focus on increasing the fusion weight of key information by considering the information distribution, thereby strengthening the relevance of the information context in the multi-scale fusion process in depth. To verify the validity of the model, we conducted comparative experiments with PSPNet<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>DeepLabV3 + , HRNet<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>B-FGC-Net<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>Segformer<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>TDNet<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup>DSymFuser<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>GDGNet<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup> and SparseFormer<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup> on the three datasets Massachusetts, INRIA, and WHU.</p>
<p id="Par8">The main contributions of this article are given as follows.</p>
<p id="Par9">(1) This study designed a DAA feature fusion module. The module filters each feature source during the fusion process, suppressing erroneous information while enhancing the weight of critical information, thereby improving the utilization efficiency of feature information.</p>
<p id="Par10">(2) By introducing the Shift operation model to dynamically obtain the distribution of feature fusion information, it can be combined with the Self attention model to improve the dependency between information.</p>
<p id="Par11">(3) Before features are extracted at each layer and fed into the main trunk, a Triplet Attention (TA) mechanism is introduced, and a residual link structure is used to connect them. This approach not only enhances the backbone network’s ability to identify and capture target points but also effectively accelerates the convergence speed of the network.</p>
<p id="Par12">(4) To enhance the applicability of the model to remote sensing image tasks, we maintain the computational load and the number of parameters within a controllable range, achieving a balance between accuracy and computational cost.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Related work</h2>
<section id="Sec3"><h3 class="pmc_sec_title">Remote sensing semantic segmentation based on CNN</h3>
<p id="Par13">The FCN is a classic model of CNN, which replaces the fully connected layers of the VGG configuration with convolutional layers. This adaptation permits the size of image input to be selected flexibly, giving CNN models an advantageous position for application in HRS imagery<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a>,<a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup>. Based on the FCN, Pan et al. devised a universal automated method for road network extraction from HRS<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup>. The success of the FCN-based deep learning model in the extraction of HRS imagery has gradually led to a recognition of the convenience of AI. Consequently, an increasing number of more advanced deep learning models are being applied in the field of HRS imagery. ResNet<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup> is a network structure with a residual connection, which lessens the negative impact of additional modules in experiments on the experimental results while mitigating the gradient explosion problem. This permits the model to have a deeper network structure and improves segmentation accuracy. ResNeXt<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup> replaced the residual module with separable convolutions, reducing the model parameters effectively and preventing overfitting in the model. Liu et al. proposed ARC-Net<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup> containing residual blocks with asymmetric convolution, which utilizes dilated convolution and multi-scale pyramid pooling modules to expand the perceptual field and improve accuracy at a small computational cost. Sun et al. proposed HRNet<sup><a href="#CR49" class="usa-link" aria-describedby="CR49">49</a></sup> which integrates multiple branches of different resolutions in parallel, accompanied by continuous information exchange between the different branches. This approach achieves the simultaneous targeting of strong semantic information and precise positional information. The Multilayer Perceptron (MLP)<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup> presented by Maggiori et al. offers a fresh connection form for the CNN models.</p>
<p id="Par14">However, CNNs still possess shortcomings, such as the inability to capture global dependencies and a limited receptive field. Many researchers have attempted to integrate attention mechanisms into CNN models to address these issues. Chen et al. developed a change detection model based on a Dual Attention Fully Convolutional Siamese Network, DASNet<sup><a href="#CR50" class="usa-link" aria-describedby="CR50">50</a></sup>which addresses the lack of robustness to pseudo-information changes (such as noise) and the problem of class imbalance in samples. Zhou et al. proposed EDSANet<sup><a href="#CR51" class="usa-link" aria-describedby="CR51">51</a></sup> to aggregate multilevel semantics and enhance the accuracy of building extraction. Li et al. proposed A2-FPN<sup><a href="#CR52" class="usa-link" aria-describedby="CR52">52</a></sup>which improves multi-scale feature learning and mitigates the loss of semantic information caused by the drastic reduction of channels through attention-guided feature fusion. Iqbal et al. introduced MSANet<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>utilizing multiple feature maps from a support image and a query image to accurately estimate semantic relationships. Chen et al. developed a HRS image change detection model based on spatiotemporal self-attention, STANet<sup><a href="#CR54" class="usa-link" aria-describedby="CR54">54</a></sup>which designed a CD self-attention mechanism to model spatiotemporal relations, enabling the model to capture spatiotemporal dependencies at different scales. Zhao et al.‘s SSAtNet utilizes a pyramid attention pooling module, incorporating the attention mechanism into multiscale modules for adaptive feature refinement<sup><a href="#CR55" class="usa-link" aria-describedby="CR55">55</a></sup>. Li et al.‘s MAResUNet introduced a Linear Attention Mechanism (LAM), enabling a more flexible and diverse integration of the attention mechanism with deep networks<sup><a href="#CR56" class="usa-link" aria-describedby="CR56">56</a></sup>.</p></section><section id="Sec4"><h3 class="pmc_sec_title">Remote sensing semantic segmentation based on transformer</h3>
<p id="Par15">In recent years, CNN models have been increasingly replaced by Transformers, a trend evident in the field of semantic segmentation of HRS imagery as well. Transformers can effectively model the global context of an image, which is crucial for understanding the comprehensive scene depicted in HRS images. Zhang et al. designed a U-structured Transformer network, SwinSUNet<sup><a href="#CR57" class="usa-link" aria-describedby="CR57">57</a></sup>inspired by the UNet network architecture. Similarly, He et al. combined Swin Transformer with UNet, proposing a dual encoder network: ST-UNet<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>. This setup reduces the loss of information and thereby increases the segmentation accuracy of small-scale ground objects. Xu et al., proposed a lightweight Transformer model that accelerates training times on small size datasets and reduces computational consumption. Wu et al., introduced a novel encoder-decoder structured Transformer fused network, CMTFNet<sup><a href="#CR58" class="usa-link" aria-describedby="CR58">58</a></sup>aimed at improving the efficiency of extracting and fusing local information and multi-scale global context information from HRS images. Zhou et al., designed a dual-branch convolutional Transformer method with an Efficient Interactive Self-Attention (EISA), which enables a full convergence of local and global spatial spectral features<sup><a href="#CR59" class="usa-link" aria-describedby="CR59">59</a></sup>. Sun et al. suggested a Spectral–Spatial Feature Tokenization Transformer, introducing a Gaussian-weighted feature tokenizer to capture spectral spatial features and high-level semantic features<sup><a href="#CR60" class="usa-link" aria-describedby="CR60">60</a></sup>. Because of the bird’s-eye view perspective of HRS images, scenes can be complex with densely distributed targets. To address these challenges, Li et al. designed a pyramid-structured Transformer<sup><a href="#CR61" class="usa-link" aria-describedby="CR61">61</a></sup>leveraging the pyramid structure to facilitate context information interaction and enhancing the capacity to handle multi-scale information.</p></section></section><section id="Sec5"><h2 class="pmc_sec_title">Methodology</h2>
<p id="Par16">The proposed SegTDformer architecture employs ViT backbone to capture low-level feature representations, followed by multi-dimensional feature enhancement through residual pathways in the TA module. Subsequently, the DAA dynamically integrates these refined features across scales, establishing a hierarchical information refinement pipeline that progressively resolves spatial-semantic ambiguities in complex environments.</p>
<section id="Sec6"><h3 class="pmc_sec_title">Backbone overview</h3>
<p id="Par17">Our model employs a Transformer as the Backbone for extracting features from images, as illustrated in Fig. <a href="#Fig1" class="usa-link">1</a>. The Transformer architecture is composed of multiple identical layers in both its encoder and decoder. Each encoder layer consists of two sub-layers: a self-attention mechanism and a feed-forward neural network. The self-attention layer enables the model to focus on other positions within the sequence when processing each position in the sequence. This is achieved through three vectors: Query, Key, and Value. These vectors are low-dimensional representations of the input data. the self-attention is estimated as:</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/59006291ed5e/d33e543.gif" loading="lazy" id="d33e543" alt="graphic file with name d33e543.gif"></td>
<td class="label">1</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq1"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/c948549a739d/d33e550.gif" loading="lazy" id="d33e550" alt="Inline graphic"></span> are the Query, Key, and Value matrices, respectively, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq2"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/e9f49f146a28/d33e556.gif" loading="lazy" id="d33e556" alt="Inline graphic"></span> is the dimension of the key vectors, introduced to scale the dot products for more stable gradients. The dot product shows the similarity between the queries and the keys; the softmax function is then applied to obtain the weights.</p>
<figure class="fig xbox font-sm" id="Fig1"><h4 class="obj_head">Fig. 1.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/78ddea0eb448/41598_2025_14751_Fig1_HTML.jpg" loading="lazy" id="d33e736" height="397" width="669" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Framework overview of SegTDformer model. The images used in this figure are sourced from the Massachusetts dataset, and the dataset can be accessed at: <a href="https://www.cs.toronto.edu/~vmnih/data/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.cs.toronto.edu/~vmnih/data/</a>. Images were generated by Python software [version number 3.10, URL: <a href="https://www.python.org/downloads/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.python.org/downloads/</a>].</p></figcaption></figure><p id="Par19">The model computes the correlation of each pixel with all others, a process that will significantly increase the computational load. To address this, our model employs a multi-head attention mechanism. Where traditional attention mechanisms use only one set of Query, Key, Value weights, multi-head attention uses multiple sets, each learning different parts of the sequence. Each “head” operates independently, capturing information in different subspaces, and then the outputs are concatenated. This enables the model to capture a more diverse range of information from the input data. Multi-head attention projects <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq3"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/b0c20064acbb/d33e564.gif" loading="lazy" id="d33e564" alt="Inline graphic"></span> into <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq4"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/eab8a4e955a3/d33e570.gif" loading="lazy" id="d33e570" alt="Inline graphic"></span> different sets of weight matrices <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq5"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/c4deeac5a025/d33e576.gif" loading="lazy" id="d33e576" alt="Inline graphic"></span> and performs parallel attention calculations:</p>
<table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/51ec108304fc/d33e582.gif" loading="lazy" id="d33e582" alt="graphic file with name d33e582.gif"></td>
<td class="label">2</td>
</tr></table>
<table class="disp-formula p" id="Equ3"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/b3efc4440a3b/d33e588.gif" loading="lazy" id="d33e588" alt="graphic file with name d33e588.gif"></td>
<td class="label">3</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq6"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/8f80ad58be07/d33e595.gif" loading="lazy" id="d33e595" alt="Inline graphic"></span> is another weight matrix used to combine the information from the different heads.</p>
<p id="Par21">In order to better adapt to the task of building extraction in HRS images, considering both accuracy and training cost, we ultimately selected MiT-B0 as the backbone of our model. The specific parameters and other main components are detailed in Table <a href="#Tab1" class="usa-link">1</a>. Many researchers have demonstrated that the features extracted by Transformers are insufficient for effectively segmenting buildings. We introduce the TA module to perform residual connections after each Transformer block to enhance the correlation of information at different scales. Subsequently, the output features at different scales are input into the DAA module for the fusion and coupling of feature information, thereby effectively capturing detailed information for efficient segmentation of target contours.</p>
<section class="tw xbox font-sm" id="Tab1"><h4 class="obj_head">Table 1.</h4>
<div class="caption p"><p>Detailed architecture of each module of segtdformer.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Framework</th>
<th align="left" colspan="1" rowspan="1">Module</th>
<th align="left" colspan="1" rowspan="1">Name</th>
<th align="left" colspan="1" rowspan="1">Number</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="5" colspan="1">Backbone</td>
<td align="left" rowspan="5" colspan="1">MiT-B0</td>
<td align="left" colspan="1" rowspan="1">patch_size</td>
<td align="left" colspan="1" rowspan="1">4</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">embed_dims</td>
<td align="left" colspan="1" rowspan="1">[32, 64, 160, 256]</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">num_heads</td>
<td align="left" colspan="1" rowspan="1">[1, 2, 5, 8]</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">mlp_ratios</td>
<td align="left" colspan="1" rowspan="1">[4, 4, 4, 4]</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">sr_ratios</td>
<td align="left" colspan="1" rowspan="1">[8, 4, 2, 1]</td>
</tr>
<tr>
<td align="left" rowspan="5" colspan="1">Neck</td>
<td align="left" rowspan="2" colspan="1">DAA</td>
<td align="left" colspan="1" rowspan="1">dim_in</td>
<td align="left" colspan="1" rowspan="1">[32, 64, 160, 256]</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">dim_out</td>
<td align="left" colspan="1" rowspan="1">[32, 64, 160, 256]</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">TA</td>
<td align="left" colspan="1" rowspan="1">in_channels</td>
<td align="left" colspan="1" rowspan="1">[32, 64, 160, 256]</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">out_channels</td>
<td align="left" colspan="1" rowspan="1">256</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">num_outs</td>
<td align="left" colspan="1" rowspan="1">4</td>
</tr>
<tr>
<td align="left" rowspan="3" colspan="1">Decoder_Head</td>
<td align="left" rowspan="3" colspan="1">Segformer_head</td>
<td align="left" colspan="1" rowspan="1">in_channels</td>
<td align="left" colspan="1" rowspan="1">[32, 64,160, 256]</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">in_index</td>
<td align="left" colspan="1" rowspan="1">[0, 1, 2, 3]</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">feature_strides</td>
<td align="left" colspan="1" rowspan="1">[4, 8, 16, 32]</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p22"><p>Where ‘mlp_ratios’ represents the size of the hidden layer in the MLP; ‘sr_ratio’ controls the size of the <em>K</em>, <em>V</em> parameter matrix.</p></div></div></section></section><section id="Sec7"><h3 class="pmc_sec_title">DAA module</h3>
<p id="Par24">Similarly, Segformer also utilizes a Transformer to extract semantic information for performing semantic segmentation tasks. However, the multi-scale semantic information generated by the Transformer is not fully integrated, which limits its effectiveness in complex HRS imagery environments. To address this issue, we developed a DAA feature fusion module. This module integrates multi-level semantic information, establishing an interaction between representational information and global spatial information. Additionally, it employs two branches—the Shift Operation and the Self-Attention—to evaluate the cross-scale weight relationships and to construct dependencies across multiple spatial hierarchies. This design can compensate for the problem that the multi-scale feature information output by Transformer is not well correlated and the detail information is not efficiently learnt during training. In consideration of the increased computational demand introduced by the attention mechanism, we replaced standard convolutions with Depthwise Separable Convolution (DSC)<sup><a href="#CR62" class="usa-link" aria-describedby="CR62">62</a></sup> to enhance operational efficiency. This is demonstrated in Fig. <a href="#Fig2" class="usa-link">2</a>.</p>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig2_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/c4e02c125f66/41598_2025_14751_Fig2_HTML.jpg" loading="lazy" id="d33e757" height="385" width="669" alt="Fig. 2"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Illustration of the DAA module.</p></figcaption></figure><p id="Par26">First, perform Depthwise Dilated Convolutions (DDC) for each input channel and then apply pointwise convolutions (1 × 1) to combine the features across channels. Given <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq7"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/c8efe146729f/d33e761.gif" loading="lazy" id="d33e761" alt="Inline graphic"></span> as the input feature map channel <em>c</em>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq8"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/23b7a05db54f/d33e770.gif" loading="lazy" id="d33e770" alt="Inline graphic"></span> as the result of the DDC with kernel <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq9"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/da9fdfef1c91/d33e776.gif" loading="lazy" id="d33e776" alt="Inline graphic"></span> and dilation rate <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq10"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/0a33f142b04d/d33e782.gif" loading="lazy" id="d33e782" alt="Inline graphic"></span>:</p>
<table class="disp-formula p" id="Equ4"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/78c3742c7cd7/d33e789.gif" loading="lazy" id="d33e789" alt="graphic file with name d33e789.gif"></td>
<td class="label">4</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq11"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/ab95a625aef2/d33e796.gif" loading="lazy" id="d33e796" alt="Inline graphic"></span> represent convolution kernel offset indices, denoting spatial offsets within the kernel relative to the output feature map location <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq12"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/4101d0415d6d/d33e802.gif" loading="lazy" id="d33e802" alt="Inline graphic"></span>. r represents the dilation rate.</p>
<p id="Par28">Then, the pointwise convolution <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq13"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/792eb41a8f8e/d33e810.gif" loading="lazy" id="d33e810" alt="Inline graphic"></span> is computed using 1 × 1 convolution kernel <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq14"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/7aa7210ee3cc/d33e816.gif" loading="lazy" id="d33e816" alt="Inline graphic"></span>:</p>
<table class="disp-formula p" id="Equ5"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/e0cebcc98e5e/d33e822.gif" loading="lazy" id="d33e822" alt="graphic file with name d33e822.gif"></td>
<td class="label">5</td>
</tr></table>
<p id="Par29">In addition, the module employs a global average pooling (GAP) branch to extract global contextual information, which compresses the spatial dimensions of the input feature map to a single vector <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq15"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/15551f0b9301/d33e830.gif" loading="lazy" id="d33e830" alt="Inline graphic"></span>:</p>
<table class="disp-formula p" id="Equ6"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/45c3c1aff0a5/d33e836.gif" loading="lazy" id="d33e836" alt="graphic file with name d33e836.gif"></td>
<td class="label">6</td>
</tr></table>
<p id="Par30">This vector is subsequently subjected to a channel-wise expansion via an upscaling 1 × 1 convolution to attain a feature map <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq16"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/bde458fd0e91/d33e844.gif" loading="lazy" id="d33e844" alt="Inline graphic"></span>:</p>
<table class="disp-formula p" id="Equ7"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/377f449701e0/d33e850.gif" loading="lazy" id="d33e850" alt="graphic file with name d33e850.gif"></td>
<td class="label">7</td>
</tr></table>
<p id="Par31">The culmination of this approach is the concatenation of the multi-dilated convolutional features with the upscaled global features, yielding a multidimensional feature representation:</p>
<table class="disp-formula p" id="Equ8"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/def091d69d82/d33e858.gif" loading="lazy" id="d33e858" alt="graphic file with name d33e858.gif"></td>
<td class="label">8</td>
</tr></table>
<p id="Par32">The feature fusion information <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq17"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/d2b355283c52/d33e866.gif" loading="lazy" id="d33e866" alt="Inline graphic"></span> is imported into the attention module for information significance assessment. Fig. <a href="#Fig3" class="usa-link">3</a> shows the two main stages of information processing, the first stage shares the same 1 × 1 convolution operations between self-attention and convolution, with the output from this stage being a set of intermediate features obtained through 1 × 1 convolutions applied to the input features. The second stage then aggregates these intermediate features according to the respective mechanisms of shift operation and convolution. The equations of attention module are roughly as follows:</p>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/127597ed11be/41598_2025_14751_Fig3_HTML.jpg" loading="lazy" id="d33e963" height="302" width="669" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>(<strong>a</strong>) Shift Operation model. Each feature value is composed of a 3 × 3 convolutional sliding window. Each feature map is obtained by performing a 1 × 1 convolution on the convolutional kernel weights from a certain location. (<strong>b</strong>) Self-Attention model. The feature information is divided into three parts: <em>Q</em>, <em>K</em>, and <em>V</em>, to compute self-correlation information.</p></figcaption></figure><p id="Par33">Shared by self-attention and convolution in Stage I:</p>
<table class="disp-formula p" id="Equ9"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/e0e29b336e54/d33e877.gif" loading="lazy" id="d33e877" alt="graphic file with name d33e877.gif"></td>
<td class="label">9</td>
</tr></table>
<table class="disp-formula p" id="Equ10"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/4aacb0f46236/d33e883.gif" loading="lazy" id="d33e883" alt="graphic file with name d33e883.gif"></td>
<td class="label">10</td>
</tr></table>
<table class="disp-formula p" id="Equ11"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/71ac02b72fc1/d33e889.gif" loading="lazy" id="d33e889" alt="graphic file with name d33e889.gif"></td>
<td class="label">11</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq18"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/d2b355283c52/d33e896.gif" loading="lazy" id="d33e896" alt="Inline graphic"></span> is the input feature, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq19"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/5aaefa3f69f2/d33e902.gif" loading="lazy" id="d33e902" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq20"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/2aad2085578d/d33e908.gif" loading="lazy" id="d33e908" alt="Inline graphic"></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq21"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/6a20b2b5bdae/d33e915.gif" loading="lazy" id="d33e915" alt="Inline graphic"></span> are the queries, keys, and values matrices in the self-attention mechanism. <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq22"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/920af115d5dc/d33e921.gif" loading="lazy" id="d33e921" alt="Inline graphic"></span>, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq23"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/169a08331eff/d33e927.gif" loading="lazy" id="d33e927" alt="Inline graphic"></span>, and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq24"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/4f527b6a2165/d33e933.gif" loading="lazy" id="d33e933" alt="Inline graphic"></span> are the corresponding weight matrices.</p>
<p id="Par36">Convolution in Stage II:</p>
<table class="disp-formula p" id="Equ12"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/3cf62a4cdb01/d33e968.gif" loading="lazy" id="d33e968" alt="graphic file with name d33e968.gif"></td>
<td class="label">12</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq25"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/d08d8b5295a4/d33e975.gif" loading="lazy" id="d33e975" alt="Inline graphic"></span> is the convolutional kernel, <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq26"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/7a5561799652/d33e981.gif" loading="lazy" id="d33e981" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq27"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/d8ff3d93bbee/d33e987.gif" loading="lazy" id="d33e987" alt="Inline graphic"></span> are the displacements in the horizontal and vertical directions, the operator <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq28"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/fee187df0f98/d33e993.gif" loading="lazy" id="d33e993" alt="Inline graphic"></span> represents the convolution operation, and Sum and Shift represent the summation and shifting operations, respectively.</p>
<p id="Par38">Self-attention in Stage II:</p>
<table class="disp-formula p" id="Equ13"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/a1022592c081/d33e1001.gif" loading="lazy" id="d33e1001" alt="graphic file with name d33e1001.gif"></td>
<td class="label">13</td>
</tr></table>
<p id="Par39">The final output is a linear combination of the aggregated features from these two paths:</p>
<table class="disp-formula p" id="Equ14"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/8f4947c60426/d33e1009.gif" loading="lazy" id="d33e1009" alt="graphic file with name d33e1009.gif"></td>
<td class="label">14</td>
</tr></table>
<p>where <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq29"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/73975112fc4a/d33e1016.gif" loading="lazy" id="d33e1016" alt="Inline graphic"></span> and <span xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IEq30"><img class="inline" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/f5e0e8aa5fab/d33e1022.gif" loading="lazy" id="d33e1022" alt="Inline graphic"></span> are learned parameters used to balance the relative contributions of self-attention and convolution output feature maps.</p></section><section id="Sec8"><h3 class="pmc_sec_title">Reduced parameter method</h3>
<p id="Par41">The number of parameters is also a standard by which the excellence of a model is judged. Usually, the effect is obvious when further processing feature information by adding modules. However, the computational overhead introduced by supplementary modules must be mitigated. Consequently, low-parameter modules are prioritized to achieve significant performance gains without inflating computational burden. One such module is the TA. As shown in Fig. <a href="#Fig4" class="usa-link">4</a>, The depicted triplet attention mechanism comprises three distinct branches. The uppermost branch calculates attention across the channel dimension <em>C</em> and the spatial width dimension <em>W</em>. The central branch likewise attends to channel dimension <em>C</em> and spatial height dimension <em>H</em>. The concluding branch at the base captures spatial interdependencies between height <em>H</em> and width <em>W</em>. For the initial two branches, a rotation operation is employed to forge links between channel dimension and a single spatial dimension. Subsequently, these attention weights are consolidated by means of a straightforward averaging process. We use this module residual to connect two adjacent transformer blocks, making the output strongly correlated.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/11b82c2fe689/41598_2025_14751_Fig4_HTML.jpg" loading="lazy" id="d33e1062" height="281" width="669" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Illustration of the TA module.</p></figcaption></figure></section></section><section id="Sec9"><h2 class="pmc_sec_title">Experimental datasets and evaluation</h2>
<section id="Sec10"><h3 class="pmc_sec_title">Datasets</h3>
<p id="Par43">The Massachusetts dataset, proposed by Mnih<sup><a href="#CR63" class="usa-link" aria-describedby="CR63">63</a></sup>comprises 151 aerial images of the Boston area. Each image covers approximately 2.25 square kilometers and has a resolution of 1500 × 1500 with a spatial resolution of one meter. The dataset is divided into 137 training images, 4 validation images, and 10 test images. Every image is composed of three RGB channels, and it includes original images along with their respective labels, as illustrated in Fig. <a href="#Fig5" class="usa-link">5</a>.</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/c3a40c5d9dd8/41598_2025_14751_Fig5_HTML.jpg" loading="lazy" id="d33e1096" height="356" width="661" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Image and label example from the Massachusetts dataset. (<strong>a</strong>) Aerial image; (<strong>b</strong>) Label image; red for buildings and black for the background. The images used in this figure are sourced from the Massachusetts dataset, and the dataset can be accessed at: <a href="https://www.cs.toronto.edu/~vmnih/data/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.cs.toronto.edu/~vmnih/data/</a>.</p></figcaption></figure><p id="Par45">The INRIA dataset, proposed by Maggiori<sup><a href="#CR64" class="usa-link" aria-describedby="CR64">64</a></sup>includes building data from various cities worldwide. Each image in this dataset has a resolution of 5000 × 5000 and a spatial resolution of 0.3 m. For this study, the dataset is split into three segments: training set and validation set, with a division ratio of 7:3 The aerial images and labels of the INRIA dataset can be observed in Fig. <a href="#Fig6" class="usa-link">6</a>.</p>
<figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/f8914e605fcd/41598_2025_14751_Fig6_HTML.jpg" loading="lazy" id="d33e1126" height="351" width="661" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Image and label example selected from the INRIA dataset: (<strong>a</strong>) Aerial Image; (<strong>b</strong>) Label Image. Black and red pixels mark non-building and building, respectively. The images used in this figure are sourced from the INRIA dataset, and the dataset can be accessed at: <a href="https://project.inria.fr/aerialimagelabeling/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://project.inria.fr/aerialimagelabeling/</a>.</p></figcaption></figure><p id="Par47">The WHU Building Dataset, developed by Wuhan University<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup>consists of building samples extracted manually from aerial and satellite images. It encompasses a land area of about 450 square kilometers in Christchurch, New Zealand. This dataset comprises 8189 images sized at 512 × 512 pixels with a spatial resolution of 0.3 m. It is partitioned into training, validation, and test sets, with 4736, 1036, and 2416 images, respectively. Fig. <a href="#Fig7" class="usa-link">7</a> displays the original images alongside their associated labels.</p>
<figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/7eec359a527d/41598_2025_14751_Fig7_HTML.jpg" loading="lazy" id="d33e1156" height="356" width="661" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Image and label example selected from the WHU dataset: (<strong>a</strong>) Aerial Image; (<strong>b</strong>) Label Image. Black and red pixels mark non-building and building, respectively. The images used in this figure are sourced from the WHU dataset, and the dataset can be accessed at: <a href="http://gpcv.whu.edu.cn/data/building_dataset.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://gpcv.whu.edu.cn/data/building_dataset.html</a>.</p></figcaption></figure></section><section id="Sec11"><h3 class="pmc_sec_title">Data processing</h3>
<p id="Par49">To improve the model’s performance, this study requires a larger dataset to help it recognize a wider range of features for better decision-making. Data augmentation, which creates new data from existing samples, is an effective method for enhancing model accuracy and reducing overfitting.</p>
<p id="Par50">In this experiment, each HRS image in the dataset is divided into multiple 512 × 512 segments due to the high resolution of the images. These segmented images are then randomly horizontally flipped with a 50% probability when loaded into the Dataloader. It’s important to note that the SegTDformer model needs single-channel labels, while the dataset labels are in the form of RGB three-channel images. In this study, each pixel’s gray value of 255 is assigned a value of 1, and any other gray value is assigned a value of 0. As a result, pixels with a mask value of 1 in the dataset represent buildings, while pixels with a mask value of 0 indicate background areas.</p></section><section id="Sec12"><h3 class="pmc_sec_title">Experimental settings</h3>
<p id="Par51">The building segmentation experiments were conducted using the PyTorch deep learning framework, Version 1.9.0. The experiments ran on a computer server equipped with two NVIDIA GeForce RTX 3060 (12GB) GPUs to accelerate computation.</p>
<p id="Par52">The training used the AdamW optimization function with an initial learning rate of 0.00006, and weight decay was employed to aid convergence and prevent overfitting. Each dataset underwent 40,000 iterations of training due to the significant computational load associated with segmented images based on transformers. A batch size of 2 was set to accommodate the GPU configuration. Analysis of the training progress over 40,000 iterations for the three datasets revealed a gradual increase in overall accuracy and a progressive decrease in loss, with localized fluctuations observed within a small range.</p>
<p id="Par53">This comparative study will evaluate the accuracy of the models using five assessment criteria: Overall Accuracy (OA), Precision, Recall, F1-score, and Intersection over Union (IoU). OA measures the ratio of correctly classified pixels to the total number of tested pixels. Precision is the percentage of correctly predicted positive samples out of all samples predicted as positive by the model. Recall is the percentage of all samples with positive true labels that were correctly predicted. F1-score is a metric that balances the Precision and Recall. IoU measures the ratio of the intersection and union of the true and predicted values. Typically, IoU is calculated for individual classes, and the IoU for each class is aggregated and then averaged to provide an overall assessment.</p>
<table class="disp-formula p" id="Equ15"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/ee49576c933b/d33e1172.gif" loading="lazy" id="d33e1172" alt="graphic file with name d33e1172.gif"></td>
<td class="label">15</td>
</tr></table>
<table class="disp-formula p" id="Equ16"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/02f930d5295a/d33e1178.gif" loading="lazy" id="d33e1178" alt="graphic file with name d33e1178.gif"></td>
<td class="label">16</td>
</tr></table>
<table class="disp-formula p" id="Equ17"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/3ef56483eb59/d33e1184.gif" loading="lazy" id="d33e1184" alt="graphic file with name d33e1184.gif"></td>
<td class="label">17</td>
</tr></table>
<table class="disp-formula p" id="Equ18"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/dbe27665c26b/d33e1190.gif" loading="lazy" id="d33e1190" alt="graphic file with name d33e1190.gif"></td>
<td class="label">18</td>
</tr></table>
<table class="disp-formula p" id="Equ19"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/d687f37a9285/d33e1196.gif" loading="lazy" id="d33e1196" alt="graphic file with name d33e1196.gif"></td>
<td class="label">19</td>
</tr></table>
<p>where <em>TP</em> is the number of true positives, <em>TN</em> is the number of true negatives, <em>FP</em> is the number of false positives, and <em>FN</em> is the number of false negatives.</p></section></section><section id="Sec13"><h2 class="pmc_sec_title">Results</h2>
<p id="Par55">In order to thoroughly evaluate the performance of the model proposed in this study, we will conduct experiments on multiple datasets. These datasets are of various learning difficulty levels, ensuring the broad applicability of the results. Simultaneously, we have selected several currently leading models as benchmarks for comparison, including both some classic models and some of the latest models. The experiment design aims to comprehensively measure the accuracy and robustness of the model through multiple metrics.</p>
<section id="Sec14"><h3 class="pmc_sec_title">Experimental results on the Massachusetts dataset</h3>
<p id="Par56">The extraction results of various models on the Massachusetts dataset are illustrated in Fig. <a href="#Fig8" class="usa-link">8</a>. The dataset features densely packed and irregular buildings, with numerous obstructions around them, posing substantial challenges to model learning and making extraction particularly difficult. As shown in Fig. <a href="#Fig8" class="usa-link">8</a>, classical models (e.g., DeepLabV3 + and HRNet) perform poorly on this challenging dataset, with uneven segmentation and large misclassification at the building edges, whereas the newly proposed models show a significant improvement compared to them. Due to the presence of attention fusion mechanisms, SegTDformer demonstrates better discrimination capability, especially in complex environments with occluded buildings. When compared to other models, SegTDformer provides more complete extraction of buildings. TDNet, DSymFuser, Sparsefoemer and Segforemer all exhibit varying degrees of missed detection within the interior areas of the buildings extracted.</p>
<figure class="fig xbox font-sm" id="Fig8"><h4 class="obj_head">Fig. 8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig8_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/105c28a7cd24/41598_2025_14751_Fig8_HTML.jpg" loading="lazy" id="d33e1247" height="233" width="669" alt="Fig. 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Building extraction results of each method in Massachusetts dataset: The green, red, blue, and black pixels of the maps represent the predictions of true positive, false positive, false negative, and true negative, respectively. The images used in this figure are sourced from the Massachusetts dataset, and the dataset can be accessed at: <a href="https://www.cs.toronto.edu/~vmnih/data/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.cs.toronto.edu/~vmnih/data/</a>. Images were generated by Python software [version number 3.10, URL: <a href="https://www.python.org/downloads/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.python.org/downloads/</a>].</p></figcaption></figure><p id="Par58">The performance metrics for each model are presented in Table <a href="#Tab2" class="usa-link">2</a>. The SegTDformer model shows comprehensive and balanced performance advantages. Its overall accuracy (OA) reaches 94.61%, which ranks first among the compared models, and improves 0.13% points over the next best SparseFormer (94.48%), the effectiveness of DAA module in multi-scale feature fusion. The module significantly improves the semantic understanding of the model by enhancing the weights of key information and suppressing misinformation. Particularly outstanding is the model’s Recall metric, which breaks the highest record of the compared models with 83.63%, an improvement of 0.74% over SparseFormer (82.89%), reducing the leakage rate of buildings, which is crucial for the complete extraction of complex targets in remote sensing scenes. In terms of comprehensive performance indicators, SegTDformer’s F1-score (84.74%) and mIoU (75.47%) are firmly at the top of the list. Among them, mIoU is improved by 0.43% compared to SparseFormer (75.04%), and compared to the benchmark model TDNet, SegTDformer surpasses comprehensively in OA, F1 and mIoU. Meanwhile, its significant improvement in OA and mIoU compared to the Transformer class model Segformer highlights the advantages of the dynamic feature screening mechanism over traditional self-attention. Although SparseFormer maintains a slight lead in Precision, SegTDformer achieves better overall performance with higher Recall and mIoU. SegTDformer achieves the current state-of-the-art building extraction accuracy at a controllable computational cost through its original attention mechanism and feature fusion strategy.</p>
<section class="tw xbox font-sm" id="Tab2"><h4 class="obj_head">Table 2.</h4>
<div class="caption p"><p>Quantitative comparisons with classical models on the Massachusetts dataset.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1"></th>
<th align="left" colspan="1" rowspan="1">OA(%)</th>
<th align="left" colspan="1" rowspan="1">Precision(%)</th>
<th align="left" colspan="1" rowspan="1">Recall(%)</th>
<th align="left" colspan="1" rowspan="1">F1-score(%)</th>
<th align="left" colspan="1" rowspan="1">mIoU(%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">PSPNet</td>
<td align="center" colspan="1" rowspan="1">92.64</td>
<td align="center" colspan="1" rowspan="1">80.22</td>
<td align="center" colspan="1" rowspan="1">77.65</td>
<td align="center" colspan="1" rowspan="1">78.91</td>
<td align="center" colspan="1" rowspan="1">68.44</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeepLabV3+</td>
<td align="center" colspan="1" rowspan="1">93.34</td>
<td align="center" colspan="1" rowspan="1">84.73</td>
<td align="center" colspan="1" rowspan="1">79.10</td>
<td align="center" colspan="1" rowspan="1">81.82</td>
<td align="center" colspan="1" rowspan="1">69.23</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">HRNet</td>
<td align="center" colspan="1" rowspan="1">94.12</td>
<td align="center" colspan="1" rowspan="1">
<strong>88.78</strong>
</td>
<td align="center" colspan="1" rowspan="1">75.85</td>
<td align="center" colspan="1" rowspan="1">81.80</td>
<td align="center" colspan="1" rowspan="1">70.71</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">B-FGC-Net</td>
<td align="center" colspan="1" rowspan="1">93.45</td>
<td align="center" colspan="1" rowspan="1">85.62</td>
<td align="center" colspan="1" rowspan="1">81.25</td>
<td align="center" colspan="1" rowspan="1">83.38</td>
<td align="center" colspan="1" rowspan="1">73.28</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Segformer</td>
<td align="center" colspan="1" rowspan="1">94.36</td>
<td align="center" colspan="1" rowspan="1">86.93</td>
<td align="center" colspan="1" rowspan="1">82.38</td>
<td align="center" colspan="1" rowspan="1">84.59</td>
<td align="center" colspan="1" rowspan="1">73.92</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">TDNet</td>
<td align="center" colspan="1" rowspan="1">94.46</td>
<td align="center" colspan="1" rowspan="1">87.79</td>
<td align="center" colspan="1" rowspan="1">80.82</td>
<td align="center" colspan="1" rowspan="1">84.16</td>
<td align="center" colspan="1" rowspan="1">74.44</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DSymFuser</td>
<td align="center" colspan="1" rowspan="1">94.36</td>
<td align="center" colspan="1" rowspan="1">87.35</td>
<td align="center" colspan="1" rowspan="1">81.42</td>
<td align="center" colspan="1" rowspan="1">84.05</td>
<td align="center" colspan="1" rowspan="1">74.68</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">GDGNet</td>
<td align="center" colspan="1" rowspan="1">94.41</td>
<td align="center" colspan="1" rowspan="1">87.73</td>
<td align="center" colspan="1" rowspan="1">82.11</td>
<td align="center" colspan="1" rowspan="1">84.29</td>
<td align="center" colspan="1" rowspan="1">74.87</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SparseFormer</td>
<td align="center" colspan="1" rowspan="1">94.48</td>
<td align="center" colspan="1" rowspan="1">86.47</td>
<td align="center" colspan="1" rowspan="1">82.89</td>
<td align="center" colspan="1" rowspan="1">84.64</td>
<td align="center" colspan="1" rowspan="1">75.04</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SegTDformer</td>
<td align="center" colspan="1" rowspan="1">
<strong>94.61</strong>
</td>
<td align="center" colspan="1" rowspan="1">85.87</td>
<td align="center" colspan="1" rowspan="1">
<strong>83.63</strong>
</td>
<td align="center" colspan="1" rowspan="1">
<strong>84.74</strong>
</td>
<td align="center" colspan="1" rowspan="1">
<strong>75.47</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p64"><p>The highest value for each metric is marked as bold.</p></div></div></section></section><section id="Sec15"><h3 class="pmc_sec_title">Experimental results on the INRIA dataset</h3>
<p id="Par60">Compared to the Massachusetts dataset, training on the INRIA dataset presents a slightly reduced difficulty; however, the presence of buildings with non-uniform contour shapes still affects the model’s segmentation precision. Figure <a href="#Fig9" class="usa-link">9</a> displays the segmentation results of various models on the INRIA dataset. The results show that DeepLabV3 + and HRNet perform moderately, experiencing instances of missed classification within large buildings. More complex CNN models (HRNet, TDNet) and Transformer models (Segformer, SparseFormer, SegTDformer) tend to produce more complete segmentation of large buildings, likely due to both types of models considering the association between local appearances and global information. The model we propose not only excels in the integrity of building interiors but also provides a better match to the actual contours in the segmentation of buildings with complex shapes compared to other models.</p>
<figure class="fig xbox font-sm" id="Fig9"><h4 class="obj_head">Fig. 9.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig9_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/0c609c929017/41598_2025_14751_Fig9_HTML.jpg" loading="lazy" id="d33e1459" height="236" width="669" alt="Fig. 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Building extraction results of each method in INRIA dataset. The images used in this figure are sourced from the INRIA dataset, and the dataset can be accessed at: <a href="https://project.inria.fr/aerialimagelabeling/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://project.inria.fr/aerialimagelabeling/</a>. Images were generated by Python software [version number 3.10, URL: <a href="https://www.python.org/downloads/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.python.org/downloads/</a>].</p></figcaption></figure><p id="Par62">As shown in Table <a href="#Tab3" class="usa-link">3</a>, the OA of SegTDformer ranks first with 95.39%, an improvement of 0.27% over the best-in-class SparseFormer (95.12%), which verifies the robustness of the model architecture. On mIoU, the core metric of segmentation quality, SegTDformer sets a new record with 82.54%, a 0.45% improvement over the next best DSymFuser (82.09%). Although its Precision (91.54%) is slightly lower than that of Segformer (91.89%) and TDNet (91.62%), it is still better than SparseFormer (91.35%) and on par with GDGNet (91.56%), reflecting the precise control of Precision-Recall balance in the design. This balance is even more pronounced in the comparison of HRNet: although HRNet has the highest Precision (89.62%), it lags behind F1-score (88.83%) and mIoU (80.85%) due to low Recall. In addition, compared to the Transformer benchmark Segformer, SegTDformer has an F1 advantage of 0.6% and an mIoU advantage of 1.1%, confirming that the dynamic feature screening mechanism outperforms the standard self-attentive structure. Reflecting the coordinated reliability of the SegTDformer metrics.</p>
<section class="tw xbox font-sm" id="Tab3"><h4 class="obj_head">Table 3.</h4>
<div class="caption p"><p>Quantitative comparisons with classical models on the INRIA dataset.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1"></th>
<th align="left" colspan="1" rowspan="1">OA (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">F1-score (%)</th>
<th align="left" colspan="1" rowspan="1">mIoU (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">DeepLabV3+</td>
<td align="center" colspan="1" rowspan="1">93.28</td>
<td align="center" colspan="1" rowspan="1">87.35</td>
<td align="center" colspan="1" rowspan="1">86.4</td>
<td align="center" colspan="1" rowspan="1">86.87</td>
<td align="center" colspan="1" rowspan="1">76.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PSPNet</td>
<td align="center" colspan="1" rowspan="1">94.24</td>
<td align="center" colspan="1" rowspan="1">90.81</td>
<td align="center" colspan="1" rowspan="1">83.7</td>
<td align="center" colspan="1" rowspan="1">87.11</td>
<td align="center" colspan="1" rowspan="1">77.99</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">HRNet</td>
<td align="center" colspan="1" rowspan="1">94.78</td>
<td align="center" colspan="1" rowspan="1">89.62</td>
<td align="center" colspan="1" rowspan="1">88.05</td>
<td align="center" colspan="1" rowspan="1">88.83</td>
<td align="center" colspan="1" rowspan="1">80.85</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">B-FGC-Net</td>
<td align="center" colspan="1" rowspan="1">94.70</td>
<td align="center" colspan="1" rowspan="1">87.82</td>
<td align="center" colspan="1" rowspan="1">88.12</td>
<td align="center" colspan="1" rowspan="1">87.97</td>
<td align="center" colspan="1" rowspan="1">79.31</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Segformer</td>
<td align="center" colspan="1" rowspan="1">95.16</td>
<td align="center" colspan="1" rowspan="1">
<strong>91.89</strong>
</td>
<td align="center" colspan="1" rowspan="1">86.94</td>
<td align="center" colspan="1" rowspan="1">89.34</td>
<td align="center" colspan="1" rowspan="1">81.44</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">TDNet</td>
<td align="center" colspan="1" rowspan="1">95.11</td>
<td align="center" colspan="1" rowspan="1">91.62</td>
<td align="center" colspan="1" rowspan="1">87.02</td>
<td align="center" colspan="1" rowspan="1">89.26</td>
<td align="center" colspan="1" rowspan="1">81.34</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DSymFuser</td>
<td align="center" colspan="1" rowspan="1">95.11</td>
<td align="center" colspan="1" rowspan="1">91.44</td>
<td align="center" colspan="1" rowspan="1">88.25</td>
<td align="center" colspan="1" rowspan="1">89.81</td>
<td align="center" colspan="1" rowspan="1">82.09</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">GDGNet</td>
<td align="center" colspan="1" rowspan="1">95.09</td>
<td align="center" colspan="1" rowspan="1">91.56</td>
<td align="center" colspan="1" rowspan="1">87.93</td>
<td align="center" colspan="1" rowspan="1">89.71</td>
<td align="center" colspan="1" rowspan="1">81.64</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SparseFormer</td>
<td align="center" colspan="1" rowspan="1">95.12</td>
<td align="center" colspan="1" rowspan="1">91.35</td>
<td align="center" colspan="1" rowspan="1">88.14</td>
<td align="center" colspan="1" rowspan="1">89.71</td>
<td align="center" colspan="1" rowspan="1">81.98</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SegTDformer</td>
<td align="center" colspan="1" rowspan="1">
<strong>95.39</strong>
</td>
<td align="center" colspan="1" rowspan="1">91.54</td>
<td align="center" colspan="1" rowspan="1">
<strong>88.51</strong>
</td>
<td align="center" colspan="1" rowspan="1">
<strong>89.94</strong>
</td>
<td align="center" colspan="1" rowspan="1">
<strong>82.54</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p71"><p>The highest value for each metric is marked as bold.</p></div></div></section></section><section id="Sec16"><h3 class="pmc_sec_title">Experimental results on the WHU dataset</h3>
<p id="Par64">The WHU dataset features buildings that are more scattered in distribution, with uniform shapes and no large structures, and is devoid of obstructions, providing a rather simplistic environment. Hence, this dataset presents a relatively low training difficulty. As illustrated in Fig. <a href="#Fig10" class="usa-link">10</a>, classic CNN models such as DeepLabV3+, TDNet, and HRNet all exhibit commendable performance, while emerging models like Segformer, TDNet, and SegTDformer demonstrate more precise edge detail processing. Notably, SegTDformer performs well in detecting small buildings, a task where other models tend to exhibit missed classifications.</p>
<figure class="fig xbox font-sm" id="Fig10"><h4 class="obj_head">Fig. 10.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig10_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/cfd2ed6a538f/41598_2025_14751_Fig10_HTML.jpg" loading="lazy" id="d33e1671" height="235" width="669" alt="Fig. 10"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig10/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Building extraction results of each method in WHU dataset. The images used in this figure are sourced from the WHU dataset, and the dataset can be accessed at: <a href="http://gpcv.whu.edu.cn/data/building_dataset.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://gpcv.whu.edu.cn/data/building_dataset.html</a>. Images were generated by Python software [version number 3.10, URL: <a href="https://www.python.org/downloads/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.python.org/downloads/</a>].</p></figcaption></figure><p id="Par66">As demonstrated in Table <a href="#Tab4" class="usa-link">4</a>, under the high-precision benchmark of WHU dataset, SegTDformer still shows significant advantages, with its OA (98.51%), F1 (96.21%) and mIoU (92.85%) at the top of the list: the OA improves by 0.12% compared with that of TDNet (98.39%), and the mIoU surpasses that of SparseFormer (92.27%) by 0.58%, for the first time, the core metrics achieved the overall lead. The model verifies the ability of dynamic void attention to capture complex building edges with Recall of 95.9% (0.58% improvement over TDNet), and at the same time sets a new record with Precision of 96.52% (better than SparseFormer’s 96.11%), reaching a Precision-Recall double breakthrough on high-end datasets for the first time. Especially critical is that SegTDformer becomes the only model whose F1 breaks through 96%, and the mIoU increase highlights the optimization effect of the triple-attention mechanism on boundary details, confirming its robustness and generalization ability in ultra-high precision scenarios.</p>
<section class="tw xbox font-sm" id="Tab4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>Quantitative comparisons with classical models on the INRIA dataset.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1"></th>
<th align="left" colspan="1" rowspan="1">OA (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">F1-score (%)</th>
<th align="left" colspan="1" rowspan="1">mIoU (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">DeepLabV3+</td>
<td align="center" colspan="1" rowspan="1">97.84</td>
<td align="center" colspan="1" rowspan="1">95.46</td>
<td align="center" colspan="1" rowspan="1">93.42</td>
<td align="center" colspan="1" rowspan="1">94.43</td>
<td align="center" colspan="1" rowspan="1">89.74</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PSPNet</td>
<td align="center" colspan="1" rowspan="1">97.42</td>
<td align="center" colspan="1" rowspan="1">94.90</td>
<td align="center" colspan="1" rowspan="1">
<strong>96.33</strong>
</td>
<td align="center" colspan="1" rowspan="1">95.61</td>
<td align="center" colspan="1" rowspan="1">91.61</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">HRNet</td>
<td align="center" colspan="1" rowspan="1">98.22</td>
<td align="center" colspan="1" rowspan="1">95.55</td>
<td align="center" colspan="1" rowspan="1">95.43</td>
<td align="center" colspan="1" rowspan="1">95.49</td>
<td align="center" colspan="1" rowspan="1">91.58</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">B-FGC-Net</td>
<td align="center" colspan="1" rowspan="1">96.98</td>
<td align="center" colspan="1" rowspan="1">95.32</td>
<td align="center" colspan="1" rowspan="1">94.75</td>
<td align="center" colspan="1" rowspan="1">95.03</td>
<td align="center" colspan="1" rowspan="1">90.04</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Segformer</td>
<td align="center" colspan="1" rowspan="1">98.31</td>
<td align="center" colspan="1" rowspan="1">95.78</td>
<td align="center" colspan="1" rowspan="1">95.66</td>
<td align="center" colspan="1" rowspan="1">95.72</td>
<td align="center" colspan="1" rowspan="1">91.99</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">TDNet</td>
<td align="center" colspan="1" rowspan="1">98.39</td>
<td align="center" colspan="1" rowspan="1">96.44</td>
<td align="center" colspan="1" rowspan="1">95.32</td>
<td align="center" colspan="1" rowspan="1">95.88</td>
<td align="center" colspan="1" rowspan="1">92.25</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DSymFuser</td>
<td align="center" colspan="1" rowspan="1">98.30</td>
<td align="center" colspan="1" rowspan="1">95.79</td>
<td align="center" colspan="1" rowspan="1">95.58</td>
<td align="center" colspan="1" rowspan="1">95.68</td>
<td align="center" colspan="1" rowspan="1">91.93</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">GDGNet</td>
<td align="center" colspan="1" rowspan="1">97.95</td>
<td align="center" colspan="1" rowspan="1">96.21</td>
<td align="center" colspan="1" rowspan="1">95.51</td>
<td align="center" colspan="1" rowspan="1">95.86</td>
<td align="center" colspan="1" rowspan="1">92.15</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SparseFormer</td>
<td align="center" colspan="1" rowspan="1">98.16</td>
<td align="center" colspan="1" rowspan="1">96.11</td>
<td align="center" colspan="1" rowspan="1">95.67</td>
<td align="center" colspan="1" rowspan="1">95.89</td>
<td align="center" colspan="1" rowspan="1">92.27</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SegTDformer</td>
<td align="center" colspan="1" rowspan="1">
<strong>98.51</strong>
</td>
<td align="center" colspan="1" rowspan="1">
<strong>96.52</strong>
</td>
<td align="center" colspan="1" rowspan="1">95.90</td>
<td align="center" colspan="1" rowspan="1">
<strong>96.21</strong>
</td>
<td align="center" colspan="1" rowspan="1">
<strong>92.85</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<div class="tw-foot p"><div class="fn" id="_fn_p78"><p>The highest value for each metric is marked as bold.</p></div></div></section></section></section><section id="Sec17"><h2 class="pmc_sec_title">Discussion</h2>
<section id="Sec18"><h3 class="pmc_sec_title">Ablation study</h3>
<p id="Par68">To systematically validate the contribution of each proposed module, we conducted an in-depth ablation study on the Massachusetts dataset with comprehensive quantitative and qualitative analyses. As quantitatively demonstrated in Table <a href="#Tab5" class="usa-link">5</a>, the progressive integration of our novel components reveals their synergistic effects: the DAA module alone contributes a 0.8% mIoU improvement by establishing adaptive feature fusion pathways between global contexts and local details. This enhancement stems from its dual-branch architecture: the Shift Operation branch preserves fine-grained spatial patterns through controlled feature shifting, while the parallel Self-Attention branch captures long-range dependencies, with their dynamic weight coupling mechanism enabling context-aware feature recombination. Further analysis shows that the TA augmentation brings an additional 0.5% performance gain by implementing cross-dimensional interaction gates. Through its lightweight channel-spatial co-attention mechanism combined with depth-wise separable convolutions, TA effectively suppresses feature conflicts between scales while maintaining computational efficiency.</p>
<section class="tw xbox font-sm" id="Tab5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>Quantitative analysis of ablation experiments in Massachusetts dataset.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1"></th>
<th align="left" colspan="1" rowspan="1">OA (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">F1-score (%)</th>
<th align="left" colspan="1" rowspan="1">mIoU (%)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Baseline</td>
<td align="center" colspan="1" rowspan="1">94.36</td>
<td align="center" colspan="1" rowspan="1">86.93</td>
<td align="center" colspan="1" rowspan="1">82.38</td>
<td align="center" colspan="1" rowspan="1">84.59</td>
<td align="center" colspan="1" rowspan="1">73.92</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Baseline + DAA</td>
<td align="center" colspan="1" rowspan="1">94.68</td>
<td align="center" colspan="1" rowspan="1">87.44</td>
<td align="center" colspan="1" rowspan="1">81.41</td>
<td align="center" colspan="1" rowspan="1">84.08</td>
<td align="center" colspan="1" rowspan="1">74.72</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Baseline + TA</td>
<td align="center" colspan="1" rowspan="1">94.66</td>
<td align="center" colspan="1" rowspan="1">87.79</td>
<td align="center" colspan="1" rowspan="1">80.82</td>
<td align="center" colspan="1" rowspan="1">83.85</td>
<td align="center" colspan="1" rowspan="1">74.44</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Beseline + DAA + TA</td>
<td align="center" colspan="1" rowspan="1">94.61</td>
<td align="center" colspan="1" rowspan="1">85.87</td>
<td align="center" colspan="1" rowspan="1">83.63</td>
<td align="center" colspan="1" rowspan="1">84.7</td>
<td align="center" colspan="1" rowspan="1">75.47</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par69">As shown in Fig. <a href="#Fig11" class="usa-link">11</a>, qualitative comparisons further reveal that the baseline model tends to produce fragmented predictions around building boundaries, whereas our full model demonstrates remarkable improvements in capturing structural details and maintaining topological consistency, particularly for large-area buildings with complex shapes. These visual evidences align with our quantitative metrics, jointly validating that the proposed modules effectively address the information degradation problem in hierarchical feature encoding through their collaborative multi-scale modeling framework.</p>
<figure class="fig xbox font-sm" id="Fig11"><h4 class="obj_head">Fig. 11.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370942_41598_2025_14751_Fig11_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b4f3/12370942/0998a5694b12/41598_2025_14751_Fig11_HTML.jpg" loading="lazy" id="d33e1971" height="120" width="669" alt="Fig. 11"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig11/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Visualization and analysis of ablation experiments. The images used in this figure are sourced from the Massachusetts dataset, and the dataset can be accessed at: <a href="https://www.cs.toronto.edu/~vmnih/data/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.cs.toronto.edu/~vmnih/data/</a>. Images were generated by Python software [version number 3.10, URL: <a href="https://www.python.org/downloads/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.python.org/downloads/</a>].</p></figcaption></figure></section><section id="Sec19"><h3 class="pmc_sec_title">Total parameters of different networks</h3>
<p id="Par72">The applicability of a model is not solely reflected in the enhancement of its precision. Parameters and computational cost are equally crucial metrics for evaluating a model. Nowadays, the trend towards lightweight models is also a significant aspect of model improvement. Table <a href="#Tab6" class="usa-link">6</a> presents the complexity of various models when training images of 512 × 512 resolution. Both TDNet and SegTDformer lead in terms of precision, yet the parameters and calculations of SegTDformer are substantially lower than those of TDNet. SegTDformer achieves the highest segmentation precision with relatively fewer parameters and computational requirements, primarily due to the integration of the parameter-free TA attention mechanism and DSC, effectively fulfilling critical functions with a minimal number of parameters.</p>
<section class="tw xbox font-sm" id="Tab6"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>Comparison of the complexities of our model with other models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1"></th>
<th align="left" colspan="1" rowspan="1">Params (M)</th>
<th align="left" colspan="1" rowspan="1">FLOPs (GFLOPS)</th>
<th align="left" colspan="1" rowspan="1">mIoU</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">DeepLabV3+</td>
<td align="center" colspan="1" rowspan="1">58.6</td>
<td align="center" colspan="1" rowspan="1">23.99</td>
<td align="center" colspan="1" rowspan="1">69.23</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">HRNet</td>
<td align="center" colspan="1" rowspan="1">15.86</td>
<td align="center" colspan="1" rowspan="1">19.96</td>
<td align="center" colspan="1" rowspan="1">70.71</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">PSPNet</td>
<td align="center" colspan="1" rowspan="1">53.32</td>
<td align="center" colspan="1" rowspan="1">28.00</td>
<td align="center" colspan="1" rowspan="1">68.44</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Segformer</td>
<td align="center" colspan="1" rowspan="1">3.71</td>
<td align="center" colspan="1" rowspan="1">7.79</td>
<td align="center" colspan="1" rowspan="1">73.92</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">TDNet</td>
<td align="center" colspan="1" rowspan="1">35.86</td>
<td align="center" colspan="1" rowspan="1">68.79</td>
<td align="center" colspan="1" rowspan="1">74.44</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">GDGNet</td>
<td align="center" colspan="1" rowspan="1">19.60</td>
<td align="center" colspan="1" rowspan="1">221.12</td>
<td align="center" colspan="1" rowspan="1">74.87</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SegTDformer</td>
<td align="center" colspan="1" rowspan="1">12.15</td>
<td align="center" colspan="1" rowspan="1">14.85</td>
<td align="center" colspan="1" rowspan="1">
<strong>75.47</strong>
</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section><section id="Sec20"><h3 class="pmc_sec_title">Generalization ability of SegTDformer</h3>
<p id="Par74">The generalization ability of the model indicates whether a well-trained model is effective only on specific types of images on which it was trained. An experiment was conducted to test the segmentation precision of models trained on the Massachusetts dataset applied to the test set of the INRIA dataset. According to Table <a href="#Tab7" class="usa-link">7</a>, the precision of SegTDformer on the INRIA test set reached 72.4%, which is 8.14% and 7.27% higher in mIoU compared to the TDNet, Segformer, and other models, respectively. Given that the two datasets differ in shape characteristics and building density, the experimental data demonstrate that SegTDformer’s generalization ability is significantly superior to other models.</p>
<section class="tw xbox font-sm" id="Tab7"><h4 class="obj_head">Table 7.</h4>
<div class="caption p"><p>Transfer learning results of different models.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" rowspan="2" colspan="1">Model</th>
<th align="left" colspan="2" rowspan="1">Massachusetts</th>
<th align="left" colspan="2" rowspan="1">INRIA</th>
</tr>
<tr>
<th align="left" colspan="1" rowspan="1">IoU</th>
<th align="left" colspan="1" rowspan="1">F1-score</th>
<th align="left" colspan="1" rowspan="1">IoU</th>
<th align="left" colspan="1" rowspan="1">F1-score</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">PSPNet</td>
<td align="center" colspan="1" rowspan="1">68.23</td>
<td align="center" colspan="1" rowspan="1">78.81</td>
<td align="center" colspan="1" rowspan="1">29.87</td>
<td align="center" colspan="1" rowspan="1">41.10</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">DeepLabV3+</td>
<td align="center" colspan="1" rowspan="1">69.23</td>
<td align="center" colspan="1" rowspan="1">81.82</td>
<td align="center" colspan="1" rowspan="1">33.22</td>
<td align="center" colspan="1" rowspan="1">47.69</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Segformer</td>
<td align="center" colspan="1" rowspan="1">73.92</td>
<td align="center" colspan="1" rowspan="1">84.59</td>
<td align="center" colspan="1" rowspan="1">65.13</td>
<td align="center" colspan="1" rowspan="1">74.94</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">TDNet</td>
<td align="center" colspan="1" rowspan="1">74.44</td>
<td align="center" colspan="1" rowspan="1">83.85</td>
<td align="center" colspan="1" rowspan="1">64.26</td>
<td align="center" colspan="1" rowspan="1">75.24</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">GDGNet</td>
<td align="center" colspan="1" rowspan="1">74.87</td>
<td align="center" colspan="1" rowspan="1">84.26</td>
<td align="center" colspan="1" rowspan="1">70.86</td>
<td align="center" colspan="1" rowspan="1">80.22</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">SegTDformer</td>
<td align="center" colspan="1" rowspan="1">75.47</td>
<td align="center" colspan="1" rowspan="1">84.7</td>
<td align="center" colspan="1" rowspan="1">72.4</td>
<td align="center" colspan="1" rowspan="1">82.54</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section></section></section><section id="Sec21"><h2 class="pmc_sec_title">Conclusion</h2>
<p id="Par76">In this study, we proposed a novel model named SegTDformer, which leveraged an attention mechanism and atrous convolution to construct DAA feature fusion module, enhancing the model’s ability to interact with multi-level information. The integration of the Shift Operation and the Self-Attention has effectively improved the model’s capability to capture information and the correlation of edge information in HRS images. In addition, the SegTDformer model integrated the TA attention mechanism with DSC, highlighting the key information of low-level features while achieving the goal of optimizing computational efficiency. The SegTDformer model’s performance was evaluated across three different datasets: Massachusetts, INRIA, and WHU. In these evaluations, it consistently outperformed existing models. Remarkably, within the Massachusetts dataset, the SegTDformer model secured superior metrics, achieving an mIoU of 75.47%, an F1-score of 84.7%, and an OA of 94.61%, eclipsing the benchmarks set by its counterparts. These outcomes attest to the efficacy of the SegTDformer model.</p>
<p id="Par77">This study demonstrates the great potential of deep learning techniques in analyzing HRS images and extracting architectural information. Although the SegTDformer model has achieved remarkable results, future work can explore more feature fusion methods and attention mechanisms to further improve the performance and adaptability of the model. In addition, we also plan to use multiple modal features to jointly express building information and realize to achieve complementary information.</p></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>This study was supported in part by the National Natural Science Foundation of China [grant number 42201077]; the Shandong Provincial Natural Science Foundation [grant number ZR2024QD279 and ZR2021QD074]; the China Postdoctoral Science Foundation [grant number 2023M732105]; the Youth Innovation Team Project of Higher School in Shandong Province, China [grant number 2024KJH087]; the State Key Laboratory of Precision Geodesy, Innovation Academy for Precision Measurement Science and Technology, CAS [grant number SKLGED2024-3-4].</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>Y.L.; Conceptualization, Methodology, Software; S.Z.: Investigation, Formal Analysis, Writing - Original Draft; X.W.: Data Curation, Writing - Original Draft; R.Z.: Visualization, Investigation; J.H.: Resources; L.K.: Supervision. All authors reviewed the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Data availability</h2>
<p>The datasets generated and analyzed during the current study are available in the [WHU building datasets] repository, [<a href="http://gpcv.whu.edu.cn/data/building_dataset.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://gpcv.whu.edu.cn/data/building_dataset.html</a>]; [Massachusetts Buildings Dataset], [<a href="https://www.cs.toronto.edu/~vmnih/data/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.cs.toronto.edu/~vmnih/data/</a>] and [INRIA Buildings Dataset], [<a href="https://project.inria.fr/aerialimagelabeling/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://project.inria.fr/aerialimagelabeling/</a>].</p></section><section id="notes3"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par78">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Liu, Y. et al. Automatic Building extraction on High-Resolution remote sensing imagery using deep convolutional Encoder-Decoder with Spatial pyramid pooling. <em>IEEE Access.</em><strong>7</strong>, 128774–128786 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20Y.%20et%20al.%20Automatic%20Building%20extraction%20on%20High-Resolution%20remote%20sensing%20imagery%20using%20deep%20convolutional%20Encoder-Decoder%20with%20Spatial%20pyramid%20pooling.%20IEEE%20Access.7,%20128774%E2%80%93128786%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Rathore, M. M., Ahmad, A., Paul, A. &amp; Rho, S. Urban planning and Building smart cities based on the internet of things using big data analytics. <em>Comput. Netw.</em><strong>101</strong>, 63–80 (2016).</cite> [<a href="https://scholar.google.com/scholar_lookup?Rathore,%20M.%20M.,%20Ahmad,%20A.,%20Paul,%20A.%20&amp;%20Rho,%20S.%20Urban%20planning%20and%20Building%20smart%20cities%20based%20on%20the%20internet%20of%20things%20using%20big%20data%20analytics.%20Comput.%20Netw.101,%2063%E2%80%9380%20(2016)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Xu, S. et al. Automatic Building rooftop extraction from aerial images via hierarchical RGB-D priors. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>56</strong>, 7369–7387 (2018).</cite> [<a href="https://scholar.google.com/scholar_lookup?Xu,%20S.%20et%20al.%20Automatic%20Building%20rooftop%20extraction%20from%20aerial%20images%20via%20hierarchical%20RGB-D%20priors.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.56,%207369%E2%80%937387%20(2018)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR4">
<span class="label">4.</span><cite>Zheng, Z., Zhong, Y., Wang, J., Ma, A. &amp; Zhang, L. Building damage assessment for rapid disaster response with a deep object-based semantic change detection framework: from natural disasters to man-made disasters. <em>Remote Sens. Environ.</em><strong>265</strong>, 112636 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zheng,%20Z.,%20Zhong,%20Y.,%20Wang,%20J.,%20Ma,%20A.%20&amp;%20Zhang,%20L.%20Building%20damage%20assessment%20for%20rapid%20disaster%20response%20with%20a%20deep%20object-based%20semantic%20change%20detection%20framework:%20from%20natural%20disasters%20to%20man-made%20disasters.%20Remote%20Sens.%20Environ.265,%20112636%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR5">
<span class="label">5.</span><cite>Ji, S., Wei, S. &amp; Lu, M. Fully convolutional networks for multisource Building extraction from an open aerial and satellite imagery data set. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>57</strong>, 574–586 (2019).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ji,%20S.,%20Wei,%20S.%20&amp;%20Lu,%20M.%20Fully%20convolutional%20networks%20for%20multisource%20Building%20extraction%20from%20an%20open%20aerial%20and%20satellite%20imagery%20data%20set.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.57,%20574%E2%80%93586%20(2019)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR6">
<span class="label">6.</span><cite>Ronneberger, O., Fischer, P. &amp; Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation. Preprint at (2015). <a href="http://arxiv.org/abs/1505.04597" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1505.04597</a>.</cite>
</li>
<li id="CR7">
<span class="label">7.</span><cite>Chen, L. C. et al. Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. Preprint at (2017). <a href="http://arxiv.org/abs/1606.00915" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1606.00915</a>.</cite> [<a href="https://doi.org/10.1109/TPAMI.2017.2699184" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/28463186/" class="usa-link">PubMed</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>He, X. et al. Swin transformer embedding UNet for remote sensing image semantic segmentation. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>60</strong>, 1–15 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?He,%20X.%20et%20al.%20Swin%20transformer%20embedding%20UNet%20for%20remote%20sensing%20image%20semantic%20segmentation.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.60,%201%E2%80%9315%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR9">
<span class="label">9.</span><cite>Chen, X. et al. Adaptive effective receptive field Convolution for semantic segmentation of VHR remote sensing images. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>59</strong>, 3532–3546 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20X.%20et%20al.%20Adaptive%20effective%20receptive%20field%20Convolution%20for%20semantic%20segmentation%20of%20VHR%20remote%20sensing%20images.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.59,%203532%E2%80%933546%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR10">
<span class="label">10.</span><cite>Zhao, W., Zhao, Z., Xu, M., Ding, Y. &amp; Gong, J. Differential multimodal fusion algorithm for remote sensing object detection through multi-branch feature extraction. <em>Expert Syst. Appl.</em><strong>265</strong>, 125826 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhao,%20W.,%20Zhao,%20Z.,%20Xu,%20M.,%20Ding,%20Y.%20&amp;%20Gong,%20J.%20Differential%20multimodal%20fusion%20algorithm%20for%20remote%20sensing%20object%20detection%20through%20multi-branch%20feature%20extraction.%20Expert%20Syst.%20Appl.265,%20125826%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>Zheng, K. et al. Enhancing remote sensing semantic segmentation accuracy and efficiency through transformer and knowledge distillation. <em>IEEE J. Sel. Top. Appl. Earth Observ Remote Sens.</em><strong>18</strong>, 4074–4092 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zheng,%20K.%20et%20al.%20Enhancing%20remote%20sensing%20semantic%20segmentation%20accuracy%20and%20efficiency%20through%20transformer%20and%20knowledge%20distillation.%20IEEE%20J.%20Sel.%20Top.%20Appl.%20Earth%20Observ%20Remote%20Sens.18,%204074%E2%80%934092%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Lu, W., Chen, S. B., Tang, J., Ding, C. H. Q. &amp; Luo, B. A robust feature downsampling module for Remote-Sensing visual tasks. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>61</strong>, 1–12 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Lu,%20W.,%20Chen,%20S.%20B.,%20Tang,%20J.,%20Ding,%20C.%20H.%20Q.%20&amp;%20Luo,%20B.%20A%20robust%20feature%20downsampling%20module%20for%20Remote-Sensing%20visual%20tasks.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.61,%201%E2%80%9312%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>He, X., Zhou, Y., Liu, B., Zhao, J. &amp; Yao, R. Remote sensing image semantic segmentation via class-guided structural interaction and boundary perception. <em>Expert Syst. Appl.</em><strong>252</strong>, 124019 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?He,%20X.,%20Zhou,%20Y.,%20Liu,%20B.,%20Zhao,%20J.%20&amp;%20Yao,%20R.%20Remote%20sensing%20image%20semantic%20segmentation%20via%20class-guided%20structural%20interaction%20and%20boundary%20perception.%20Expert%20Syst.%20Appl.252,%20124019%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Tang, K. et al. The ClearSCD model: comprehensively leveraging semantics and change relationships for semantic change detection in high Spatial resolution remote sensing imagery. <em>ISPRS J. Photogrammetry Remote Sens.</em><strong>211</strong>, 299–317 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Tang,%20K.%20et%20al.%20The%20ClearSCD%20model:%20comprehensively%20leveraging%20semantics%20and%20change%20relationships%20for%20semantic%20change%20detection%20in%20high%20Spatial%20resolution%20remote%20sensing%20imagery.%20ISPRS%20J.%20Photogrammetry%20Remote%20Sens.211,%20299%E2%80%93317%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR15">
<span class="label">15.</span><cite>Hu, J., Shen, L., Albanie, S., Sun, G. &amp; Wu, E. Squeeze-and-Excitation Networks. Preprint at (2019). <a href="http://arxiv.org/abs/1709.01507" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1709.01507</a>.</cite> [<a href="https://doi.org/10.1109/TPAMI.2019.2913372" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/31034408/" class="usa-link">PubMed</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>Hu, J., Shen, L., Albanie, S., Sun, G. &amp; Vedaldi, A. Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks.</cite>
</li>
<li id="CR17">
<span class="label">17.</span><cite>Li, X., Wang, W., Hu, X. &amp; Yang, J. Selective Kernel Networks. in <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> 510–519 (IEEE, Long Beach, CA, USA, 2019). 510–519 (IEEE, Long Beach, CA, USA, 2019). 10.1109/CVPR.2019.00060 (2019).</cite>
</li>
<li id="CR18">
<span class="label">18.</span><cite>Woo, S., Park, J., Lee, J. Y. &amp; Kweon, I. S. CBAM: Convolutional Block Attention Module. Preprint at (2018). <a href="http://arxiv.org/abs/1807.06521" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1807.06521</a>.</cite>
</li>
<li id="CR19">
<span class="label">19.</span><cite>Merikhipour, M., Khanmohammadidoustani, S. &amp; Abbasi, M. Transportation mode detection through Spatial attention-based transductive long short-term memory and off-policy feature selection. <em>Expert Syst. Appl.</em><strong>267</strong>, 126196 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Merikhipour,%20M.,%20Khanmohammadidoustani,%20S.%20&amp;%20Abbasi,%20M.%20Transportation%20mode%20detection%20through%20Spatial%20attention-based%20transductive%20long%20short-term%20memory%20and%20off-policy%20feature%20selection.%20Expert%20Syst.%20Appl.267,%20126196%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Wang, X., Girshick, R., Gupta, A. &amp; He, K. Non-local Neural Networks. in <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 7794–7803 (IEEE, Salt Lake City, UT, USA, 2018). 7794–7803 (IEEE, Salt Lake City, UT, USA, 2018). (2018). 10.1109/CVPR.2018.00813.</cite>
</li>
<li id="CR21">
<span class="label">21.</span><cite>Cao, Y. et al. Non-Local Networks Meet Squeeze-Excitation Networks and Beyond. In <em>IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em> 1971–1980 (IEEE, Seoul, Korea (South), 2019). 1971–1980 (IEEE, Seoul, Korea (South), 2019). (2019). 10.1109/ICCVW.2019.00246.</cite>
</li>
<li id="CR22">
<span class="label">22.</span><cite>Fu, J. et al. Dual Attention Network for Scene Segmentation. Preprint at (2019). <a href="http://arxiv.org/abs/1809.02983" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1809.02983</a>.</cite>
</li>
<li id="CR23">
<span class="label">23.</span><cite>Yuan, Y., Chen, X., Chen, X. &amp; Wang, J. Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation. Preprint at (2021). <a href="http://arxiv.org/abs/1909.11065" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1909.11065</a>.</cite>
</li>
<li id="CR24">
<span class="label">24.</span><cite>Li, X. et al. A frequency decoupling network for semantic segmentation of remote sensing images. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>63</strong>, 1–21 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20X.%20et%20al.%20A%20frequency%20decoupling%20network%20for%20semantic%20segmentation%20of%20remote%20sensing%20images.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.63,%201%E2%80%9321%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR25">
<span class="label">25.</span><cite>Cui, J. et al. Multiscale Cross-Modal knowledge transfer network for semantic segmentation of remote sensing images. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>63</strong>, 1–15 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Cui,%20J.%20et%20al.%20Multiscale%20Cross-Modal%20knowledge%20transfer%20network%20for%20semantic%20segmentation%20of%20remote%20sensing%20images.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.63,%201%E2%80%9315%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR26">
<span class="label">26.</span><cite>Misra, D., Nalamada, T., Arasanipalai, A. U. &amp; Hou, Q. Rotate to Attend: Convolutional Triplet Attention Module. Preprint at (2020). <a href="http://arxiv.org/abs/2010.03045" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2010.03045</a>.</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Yang, L., Zhang, R. Y., Li, L. &amp; Xie, X. SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks.</cite>
</li>
<li id="CR28">
<span class="label">28.</span><cite>Pan, X. et al. On the Integration of Self-Attention and Convolution.</cite>
</li>
<li id="CR29">
<span class="label">29.</span><cite>Lu, W., Chen, S. B., Ding, C. H. Q., Tang, J. &amp; Luo, B. LWGANet: A Lightweight Group Attention Backbone for Remote Sensing Visual Tasks. Preprint at (2025). 10.48550/arXiv.2501.10040.</cite>
</li>
<li id="CR30">
<span class="label">30.</span><cite>Lu, W., Chen, S. B., Shu, Q. L., Tang, J. &amp; Luo, B. DecoupleNet: A lightweight backbone network with efficient feature decoupling for remote sensing visual tasks. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>62</strong>, 1–13 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Lu,%20W.,%20Chen,%20S.%20B.,%20Shu,%20Q.%20L.,%20Tang,%20J.%20&amp;%20Luo,%20B.%20DecoupleNet:%20A%20lightweight%20backbone%20network%20with%20efficient%20feature%20decoupling%20for%20remote%20sensing%20visual%20tasks.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.62,%201%E2%80%9313%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Dosovitskiy, A. et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Preprint at (2021). <a href="http://arxiv.org/abs/2010.11929" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2010.11929</a>.</cite>
</li>
<li id="CR32">
<span class="label">32.</span><cite>Carion, N. et al. End-to-End object detection with Transformers. in Computer Vision – ECCV 2020 (eds Vedaldi, A., Bischof, H., Brox, T. &amp; Frahm, J. M.) vol. 12346 213–229 (Springer International Publishing, Cham, (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Carion,%20N.%20et%20al.%20End-to-End%20object%20detection%20with%20Transformers.%20in%20Computer%20Vision%20%E2%80%93%20ECCV%202020%20(eds%20Vedaldi,%20A.,%20Bischof,%20H.,%20Brox,%20T.%20&amp;%20Frahm,%20J.%20M.)%20vol.%2012346%20213%E2%80%93229%20(Springer%20International%20Publishing,%20Cham,%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33">
<span class="label">33.</span><cite>Liu, Z. et al. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. Preprint at (2021). <a href="http://arxiv.org/abs/2103.14030" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2103.14030</a>.</cite>
</li>
<li id="CR34">
<span class="label">34.</span><cite>Xie, E. et al. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. Preprint at (2021). <a href="http://arxiv.org/abs/2105.15203" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2105.15203</a>.</cite>
</li>
<li id="CR35">
<span class="label">35.</span><cite>Feng, H. et al. FTransDeepLab: multimodal fusion Transformer-Based DeepLabv3 + for remote sensing semantic segmentation. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>63</strong>, 1–18 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Feng,%20H.%20et%20al.%20FTransDeepLab:%20multimodal%20fusion%20Transformer-Based%20DeepLabv3%E2%80%89+%E2%80%89for%20remote%20sensing%20semantic%20segmentation.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.63,%201%E2%80%9318%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<span class="label">36.</span><cite>Chang, H. et al. Deep symmetric fusion transformer for multimodal remote sensing data classification. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>62</strong>, 1–15 (2024).</cite> [<a href="https://doi.org/10.1109/TNNLS.2022.3189994" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35839200/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Chang,%20H.%20et%20al.%20Deep%20symmetric%20fusion%20transformer%20for%20multimodal%20remote%20sensing%20data%20classification.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.62,%201%E2%80%9315%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR37">
<span class="label">37.</span><cite>Zhao, H., Shi, J., Qi, X., Wang, X. &amp; Jia, J. Pyramid Scene Parsing Network. Preprint at (2016). 10.48550/ARXIV.1612.01105.</cite>
</li>
<li id="CR38">
<span class="label">38.</span><cite>Maggiori, E., Tarabalka, Y., Charpiat, G. &amp; Alliez, P. High-Resolution semantic labeling with convolutional neural networks. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>55</strong>, 7092–7103 (2017).</cite> [<a href="https://scholar.google.com/scholar_lookup?Maggiori,%20E.,%20Tarabalka,%20Y.,%20Charpiat,%20G.%20&amp;%20Alliez,%20P.%20High-Resolution%20semantic%20labeling%20with%20convolutional%20neural%20networks.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.55,%207092%E2%80%937103%20(2017)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR39">
<span class="label">39.</span><cite>Wang, Y., Zeng, X., Liao, X. &amp; Zhuang, D. B-FGC-Net: A Building extraction network from high resolution remote sensing imagery. <em>Remote Sens.</em><strong>14</strong>, 269 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20Y.,%20Zeng,%20X.,%20Liao,%20X.%20&amp;%20Zhuang,%20D.%20B-FGC-Net:%20A%20Building%20extraction%20network%20from%20high%20resolution%20remote%20sensing%20imagery.%20Remote%20Sens.14,%20269%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Zhang, F., Yang, G., Sun, J., Wan, W. &amp; Zhang, K. Triple disentangled network with dual attention for remote sensing image fusion. <em>Expert Syst. Appl.</em><strong>245</strong>, 123093 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20F.,%20Yang,%20G.,%20Sun,%20J.,%20Wan,%20W.%20&amp;%20Zhang,%20K.%20Triple%20disentangled%20network%20with%20dual%20attention%20for%20remote%20sensing%20image%20fusion.%20Expert%20Syst.%20Appl.245,%20123093%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR41">
<span class="label">41.</span><cite>Wang, K., Zhang, X., Wang, X. &amp; Yu, L. Gradient decoupling guided network for High-Resolution remote sensing segmentation. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>63</strong>, 1–18 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20K.,%20Zhang,%20X.,%20Wang,%20X.%20&amp;%20Yu,%20L.%20Gradient%20decoupling%20guided%20network%20for%20High-Resolution%20remote%20sensing%20segmentation.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.63,%201%E2%80%9318%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR42">
<span class="label">42.</span><cite>Chen, Y. et al. SparseFormer: A credible Dual-CNN Expert-Guided transformer for remote sensing image segmentation with sparse point annotation. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>63</strong>, 1–16 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20Y.%20et%20al.%20SparseFormer:%20A%20credible%20Dual-CNN%20Expert-Guided%20transformer%20for%20remote%20sensing%20image%20segmentation%20with%20sparse%20point%20annotation.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.63,%201%E2%80%9316%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR43">
<span class="label">43.</span><cite>Wang, Z., Wang, B., Zhang, C., Liu, Y. &amp; Guo, J. Defending against poisoning attacks in aerial image semantic segmentation with robust invariant feature enhancement. <em>Remote Sens.</em><strong>15</strong>, 3157 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20Z.,%20Wang,%20B.,%20Zhang,%20C.,%20Liu,%20Y.%20&amp;%20Guo,%20J.%20Defending%20against%20poisoning%20attacks%20in%20aerial%20image%20semantic%20segmentation%20with%20robust%20invariant%20feature%20enhancement.%20Remote%20Sens.15,%203157%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR44">
<span class="label">44.</span><cite>Wang, Z., Wang, B., Zhang, C. &amp; Liu, Y. Defense against adversarial patch attacks for aerial image semantic segmentation by robust feature extraction. <em>Remote Sens.</em><strong>15</strong>, 1690 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wang,%20Z.,%20Wang,%20B.,%20Zhang,%20C.%20&amp;%20Liu,%20Y.%20Defense%20against%20adversarial%20patch%20attacks%20for%20aerial%20image%20semantic%20segmentation%20by%20robust%20feature%20extraction.%20Remote%20Sens.15,%201690%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR45">
<span class="label">45.</span><cite>Pan, D., Zhang, M., Zhang, B. A. &amp; Generic FCN-Based approach for the Road-Network extraction from VHR remote sensing Images – Using openstreetmap as benchmarks. <em>IEEE J. Sel. Top. Appl. Earth Observations Remote Sens.</em><strong>14</strong>, 2662–2673 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Pan,%20D.,%20Zhang,%20M.,%20Zhang,%20B.%20A.%20&amp;%20Generic%20FCN-Based%20approach%20for%20the%20Road-Network%20extraction%20from%20VHR%20remote%20sensing%20Images%20%E2%80%93%20Using%20openstreetmap%20as%20benchmarks.%20IEEE%20J.%20Sel.%20Top.%20Appl.%20Earth%20Observations%20Remote%20Sens.14,%202662%E2%80%932673%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR46">
<span class="label">46.</span><cite>He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep Residual Learning for Image Recognition. in <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> 770–778 (IEEE, Las Vegas, NV, USA, 2016). 770–778 (IEEE, Las Vegas, NV, USA, 2016). 10.1109/CVPR.2016.90 (2016).</cite>
</li>
<li id="CR47">
<span class="label">47.</span><cite>Xie, S., Girshick, R., Dollár, P., Tu, Z. &amp; He, K. Aggregated Residual Transformations for Deep Neural Networks. Preprint at (2017). <a href="http://arxiv.org/abs/1611.05431" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1611.05431</a>.</cite>
</li>
<li id="CR48">
<span class="label">48.</span><cite>Liu, Y. et al. ARC-Net: an efficient network for Building extraction from High-Resolution aerial images. <em>IEEE Access.</em><strong>8</strong>, 154997–155010 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20Y.%20et%20al.%20ARC-Net:%20an%20efficient%20network%20for%20Building%20extraction%20from%20High-Resolution%20aerial%20images.%20IEEE%20Access.8,%20154997%E2%80%93155010%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR49">
<span class="label">49.</span><cite>Sun, K., Xiao, B., Liu, D. &amp; Wang, J. Deep High-Resolution Representation Learning for Human Pose Estimation. Preprint at (2019). <a href="http://arxiv.org/abs/1902.09212" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/1902.09212</a>.</cite>
</li>
<li id="CR50">
<span class="label">50.</span><cite>Chen, J. et al. DASNet: dual attentive fully convolutional Siamese networks for change detection in High-Resolution satellite images. <em>IEEE J. Sel. Top. Appl. Earth Observations Remote Sens.</em><strong>14</strong>, 1194–1206 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20J.%20et%20al.%20DASNet:%20dual%20attentive%20fully%20convolutional%20Siamese%20networks%20for%20change%20detection%20in%20High-Resolution%20satellite%20images.%20IEEE%20J.%20Sel.%20Top.%20Appl.%20Earth%20Observations%20Remote%20Sens.14,%201194%E2%80%931206%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR51">
<span class="label">51.</span><cite>Zhou, J. et al. Building extraction and floor area Estimation at the village level in rural China via a comprehensive method integrating UAV photogrammetry and the novel EDSANet. <em>Remote Sens.</em><strong>14</strong>, 5175 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhou,%20J.%20et%20al.%20Building%20extraction%20and%20floor%20area%20Estimation%20at%20the%20village%20level%20in%20rural%20China%20via%20a%20comprehensive%20method%20integrating%20UAV%20photogrammetry%20and%20the%20novel%20EDSANet.%20Remote%20Sens.14,%205175%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR52">
<span class="label">52.</span><cite>Li, R., Zheng, S., Zhang, C., Duan, C. &amp; Wang, L. A2-FPN for semantic segmentation of Fine-Resolution remotely sensed images. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>60</strong>, 1–13 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20R.,%20Zheng,%20S.,%20Zhang,%20C.,%20Duan,%20C.%20&amp;%20Wang,%20L.%20A2-FPN%20for%20semantic%20segmentation%20of%20Fine-Resolution%20remotely%20sensed%20images.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.60,%201%E2%80%9313%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR53">
<span class="label">53.</span><cite>Iqbal, E., Safarov, S., Bang, S. &amp; MSANet Multi-Similarity and Attention Guidance for Boosting Few-Shot Segmentation. Preprint at (2022). <a href="http://arxiv.org/abs/2206.09667" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2206.09667</a>.</cite>
</li>
<li id="CR54">
<span class="label">54.</span><cite>Chen, H., Shi, Z. A. &amp; Spatial-Temporal Attention-Based method and a new dataset for remote sensing image change detection. <em>Remote Sens.</em><strong>12</strong>, 1662 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20H.,%20Shi,%20Z.%20A.%20&amp;%20Spatial-Temporal%20Attention-Based%20method%20and%20a%20new%20dataset%20for%20remote%20sensing%20image%20change%20detection.%20Remote%20Sens.12,%201662%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR55">
<span class="label">55.</span><cite>Zhao, Q., Liu, J., Li, Y. &amp; Zhang, H. Semantic segmentation with attention mechanism for remote sensing images. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>60</strong>, 1–13 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhao,%20Q.,%20Liu,%20J.,%20Li,%20Y.%20&amp;%20Zhang,%20H.%20Semantic%20segmentation%20with%20attention%20mechanism%20for%20remote%20sensing%20images.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.60,%201%E2%80%9313%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR56">
<span class="label">56.</span><cite>Li, R., Zheng, S., Duan, C., Su, J. &amp; Zhang, C. Multistage attention ResU-Net for semantic segmentation of Fine-Resolution remote sensing images. <em>IEEE Geosci. Remote Sens. Lett.</em><strong>19</strong>, 1–5 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20R.,%20Zheng,%20S.,%20Duan,%20C.,%20Su,%20J.%20&amp;%20Zhang,%20C.%20Multistage%20attention%20ResU-Net%20for%20semantic%20segmentation%20of%20Fine-Resolution%20remote%20sensing%20images.%20IEEE%20Geosci.%20Remote%20Sens.%20Lett.19,%201%E2%80%935%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR57">
<span class="label">57.</span><cite>Zhang, C., Wang, L., Cheng, S., Li, Y. &amp; SwinSUNet Pure transformer network for remote sensing image change detection. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>60</strong>, 1–13 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhang,%20C.,%20Wang,%20L.,%20Cheng,%20S.,%20Li,%20Y.%20&amp;%20SwinSUNet%20Pure%20transformer%20network%20for%20remote%20sensing%20image%20change%20detection.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.60,%201%E2%80%9313%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR58">
<span class="label">58.</span><cite>Wu, H., Huang, P., Zhang, M., Tang, W. &amp; Yu, X. CMTFNet: CNN and multiscale transformer fusion network for Remote-Sensing image semantic segmentation. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>61</strong>, 1–12 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wu,%20H.,%20Huang,%20P.,%20Zhang,%20M.,%20Tang,%20W.%20&amp;%20Yu,%20X.%20CMTFNet:%20CNN%20and%20multiscale%20transformer%20fusion%20network%20for%20Remote-Sensing%20image%20semantic%20segmentation.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.61,%201%E2%80%9312%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR59">
<span class="label">59.</span><cite>Zhou, Y., Huang, X., Yang, X., Peng, J. &amp; Ban, Y. D. C. T. N. Dual-Branch convolutional transformer network with efficient interactive Self-Attention for hyperspectral image classification. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>62</strong>, 1–16 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Zhou,%20Y.,%20Huang,%20X.,%20Yang,%20X.,%20Peng,%20J.%20&amp;%20Ban,%20Y.%20D.%20C.%20T.%20N.%20Dual-Branch%20convolutional%20transformer%20network%20with%20efficient%20interactive%20Self-Attention%20for%20hyperspectral%20image%20classification.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.62,%201%E2%80%9316%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR60">
<span class="label">60.</span><cite>Sun, L., Zhao, G., Zheng, Y. &amp; Wu, Z. Spectral–Spatial feature tokenization transformer for hyperspectral image classification. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>60</strong>, 1–14 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Sun,%20L.,%20Zhao,%20G.,%20Zheng,%20Y.%20&amp;%20Wu,%20Z.%20Spectral%E2%80%93Spatial%20feature%20tokenization%20transformer%20for%20hyperspectral%20image%20classification.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.60,%201%E2%80%9314%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR61">
<span class="label">61.</span><cite>Li, J. et al. PCViT: A pyramid convolutional vision transformer detector for object detection in Remote-Sensing imagery. <em>IEEE Trans. Geosci. Remote Sens.</em><strong>62</strong>, 1–15 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20J.%20et%20al.%20PCViT:%20A%20pyramid%20convolutional%20vision%20transformer%20detector%20for%20object%20detection%20in%20Remote-Sensing%20imagery.%20IEEE%20Trans.%20Geosci.%20Remote%20Sens.62,%201%E2%80%9315%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR62">
<span class="label">62.</span><cite>Dang, L., Pang, P. &amp; Lee, J. Depth-Wise separable Convolution neural network with residual connection for hyperspectral image classification. <em>Remote Sens.</em><strong>12</strong>, 3408 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Dang,%20L.,%20Pang,%20P.%20&amp;%20Lee,%20J.%20Depth-Wise%20separable%20Convolution%20neural%20network%20with%20residual%20connection%20for%20hyperspectral%20image%20classification.%20Remote%20Sens.12,%203408%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR63">
<span class="label">63.</span><cite>Mnih, V. Machine Learning for Aerial Image Labeling.</cite>
</li>
<li id="CR64">
<span class="label">64.</span><cite>Maggiori, E., Tarabalka, Y., Charpiat, G. &amp; Alliez, P. Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark. In <em>IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</em> 3226–3229 (IEEE, Fort Worth, TX, 2017). 3226–3229 (IEEE, Fort Worth, TX, 2017). 10.1109/IGARSS.2017.8127684 (2017).</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The datasets generated and analyzed during the current study are available in the [WHU building datasets] repository, [<a href="http://gpcv.whu.edu.cn/data/building_dataset.html" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://gpcv.whu.edu.cn/data/building_dataset.html</a>]; [Massachusetts Buildings Dataset], [<a href="https://www.cs.toronto.edu/~vmnih/data/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.cs.toronto.edu/~vmnih/data/</a>] and [INRIA Buildings Dataset], [<a href="https://project.inria.fr/aerialimagelabeling/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://project.inria.fr/aerialimagelabeling/</a>].</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-14751-0"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_14751.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (12.1 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12370942/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12370942/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370942%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370942/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12370942/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12370942/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40841730/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12370942/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40841730/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12370942/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12370942/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="FIuSSxRUEa0NNLfWE6eSxLJTcKpn09vBKzqolJqOqyd4fWFISY7L8ySmXoJ8qD4K">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
