
<!DOCTYPE html>
<html lang="en" >
    <head >

        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="MobileOptimized" content="320" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        
        

        
        
  <link rel="stylesheet" href="/static/assets/style-8258669d.css" />
<script type="module" crossorigin="" src="/static/assets/base_style-f4a99799.js"></script>

  <link rel="stylesheet" href="/static/assets/style-ef962842.css" />
<link rel="stylesheet" href="/static/assets/style-3ade8b5c.css" />
<script type="module" crossorigin="" src="/static/assets/article_style-d757a0dd.js"></script>

  
  
    <style>
  
  
  @media screen and (min-width: 64em) {
    div.pmc-wm {
      background: repeat-y;
      background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='20' height='350' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Cdefs%3E%3Cfilter x='-.02' y='0' width='1.05' height='1' id='c'%3E%3CfeFlood flood-color='%23FFF'/%3E%3CfeComposite in='SourceGraphic'/%3E%3C/filter%3E%3Ctext id='b' font-family='Helvetica' font-size='11pt' style='opacity:1;fill:%23005ea2;stroke:none;text-anchor:middle' x='175' y='14'%3E%3C/text%3E%3Cpath id='a' style='fill:%23005ea2' d='M0 8h350v3H0z'/%3E%3C/defs%3E%3Cuse xlink:href='%23a' transform='rotate(90 10 10)'/%3E%3Cuse xlink:href='%23b' transform='rotate(90 10 10)' filter='url(%23c)'/%3E%3C/svg%3E");
      padding-left: 3rem;
    }
  }
</style>

  



        
            <link rel="apple-touch-icon"
                  sizes="180x180"
                  href="/static/img/favicons/apple-touch-icon.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="48x48"
                  href="/static/img/favicons/favicon-48x48.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="32x32"
                  href="/static/img/favicons/favicon-32x32.png" />
            <link rel="icon"
                  type="image/png"
                  sizes="16x16"
                  href="/static/img/favicons/favicon-16x16.png" />
            <link rel="manifest" href="/static/img/favicons/site.webmanifest" />
            <link rel="mask-icon"
                  href="/static/img/favicons/safari-pinned-tab.svg"
                  color="#0071bc" />
            <meta name="msapplication-config"
                  content="/static/img/favicons/browserconfig.xml" />
            <meta name="theme-color" content="#ffffff" />
        

        <title>
            Hybrid pre trained model based feature extraction for enhanced indoor scene classification in federated learning environments - PMC
        </title>

        
        
  
  <!-- Logging params: Pinger defaults -->
<meta name="ncbi_app" content="cloudpmc-viewer" />
<meta name="ncbi_db" content="pmc" />
<meta name="ncbi_phid" content="15253DF98AF27613053DF90010135DC4.m_1" />
<meta name="ncbi_pinger_stat_url" content="https://pmc.ncbi.nlm.nih.gov/stat" />
<!-- Logging params: Pinger custom -->
<meta name="ncbi_pdid" content="article" />
  
    <link rel="preconnect" href="https://www.google-analytics.com" />

    
        <link rel="preconnect" href="https://cdn.ncbi.nlm.nih.gov" />
    

    <!-- Include USWDS Init Script -->
    <script src="/static/assets/uswds-init.js"></script>


    <meta name="ncbi_domain" content="scirep">
<meta name="ncbi_type" content="fulltext">
<meta name="ncbi_pcid" content="journal">
<meta name="ncbi_feature" content="associated_data">
<link rel="canonical" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370940/">
<meta name="robots" content="INDEX,NOFOLLOW,NOARCHIVE">
<meta name="citation_journal_title" content="Scientific Reports">
<meta name="citation_title" content="Hybrid pre trained model based feature extraction for enhanced indoor scene classification in federated learning environments">
<meta name="citation_author" content="Monica Dutta">
<meta name="citation_author_institution" content="Department of Computer Engineering &amp; Applications, Institute of Engineering &amp; Technology, GLA University, Mathura, India">
<meta name="citation_author" content="Deepali Gupta">
<meta name="citation_author_institution" content="Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India">
<meta name="citation_author" content="Vikas Khullar">
<meta name="citation_author_institution" content="Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India">
<meta name="citation_author" content="Sapna Juneja">
<meta name="citation_author_institution" content="KIET Group of Institutions, Delhi- NCR, Ghaziabad, India">
<meta name="citation_author" content="Roobaea Alroobaea">
<meta name="citation_author_institution" content="Department of Computer Science, College of Computers and Information Technology, Taif University, P.O. Box 11099, Taif, 21944 Saudi Arabia">
<meta name="citation_author" content="Pooja Sapra">
<meta name="citation_author_institution" content="Parul Institute of Engineering and Technology, Faculty of Engineering and Technology, Parul University, Vadodara, Gujarat India">
<meta name="citation_publication_date" content="2025 Aug 21">
<meta name="citation_volume" content="15">
<meta name="citation_firstpage" content="30803">
<meta name="citation_doi" content="10.1038/s41598-025-16673-3">
<meta name="citation_pmid" content="40841736">
<meta name="citation_abstract_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370940/">
<meta name="citation_fulltext_html_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370940/">
<meta name="citation_pdf_url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370940/pdf/41598_2025_Article_16673.pdf">
<meta name="description" content="Classification of indoor scenes is a crucial task of computer vision. It has widespread applications like smart homes, smart cities, robotics, etc. Primitive classification methods like Support Vector Machines (SVM) and K-Nearest Neighbors (KNN), ...">
<meta name="og:title" content="Hybrid pre trained model based feature extraction for enhanced indoor scene classification in federated learning environments">
<meta name="og:type" content="article">
<meta name="og:site_name" content="PubMed Central (PMC)">
<meta name="og:description" content="Classification of indoor scenes is a crucial task of computer vision. It has widespread applications like smart homes, smart cities, robotics, etc. Primitive classification methods like Support Vector Machines (SVM) and K-Nearest Neighbors (KNN), ...">
<meta name="og:url" content="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370940/">
<meta name="og:image" content="https://cdn.ncbi.nlm.nih.gov/pmc/cms/images/pmc-card-share.jpg?_=0">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@ncbi">
    
    

    </head>
    <body >
        
    <a class="usa-skipnav " href="#main-content">
      Skip to main content
    </a>


        
            

<section class="usa-banner " aria-label="Official website of the United States government" >
    <div class="usa-accordion">
        <header class="usa-banner__header">
            <div class="usa-banner__inner">
                <div class="grid-col-auto">
                    <img aria-hidden="true"
                         class="usa-banner__header-flag"
                         src="/static/img/us_flag.svg"
                         alt="" />
                </div>

                <div class="grid-col-fill tablet:grid-col-auto" aria-hidden="true">
                    <p class="usa-banner__header-text">
                        An official website of the United States government
                    </p>
                    <span class="usa-banner__header-action">Here's how you know</span>
                </div>

                



















    
        <button
            type="button"
        
    
    class="usa-accordion__button usa-banner__button
           

           
               
               
               
               
            

           
           
           
           "
    aria-expanded="false"
    aria-controls="gov-banner-default"
    
    data-testid="storybook-django-banner"
    
    >
    
        

        
                    <span class="usa-banner__button-text">Here's how you know</span>
                

        
    
        
            </button>
        


            </div>
        </header>

        <div class="usa-banner__content usa-accordion__content"
             id="gov-banner-default"
             hidden>
            <div class="grid-row grid-gap-lg">
                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-dot-gov.svg"
                         alt=""
                         aria-hidden="true" />
                    <div class="usa-media-block__body">
                        <p>
                            <strong>Official websites use .gov</strong>
                            <br />
                            A
                            <strong>.gov</strong> website belongs to an official
                            government organization in the United States.
                        </p>
                    </div>
                </div>

                <div class="usa-banner__guidance tablet:grid-col-6">
                    <img class="usa-banner__icon usa-media-block__img"
                         src="/static/img/icon-https.svg"
                         alt=""
                         aria-hidden="true" />

                    <div class="usa-media-block__body">
                        <p>
                            <strong>Secure .gov websites use HTTPS</strong>
                            <br />
                            A <strong>lock</strong> (
                            <span class="icon-lock">
                                <svg xmlns="http://www.w3.org/2000/svg"
                                     width="52"
                                     height="64"
                                     viewBox="0 0 52 64"
                                     class="usa-banner__lock-image"
                                     role="graphics-symbol"
                                     aria-labelledby="banner-lock-description"
                                     focusable="false">
                                    <title id="banner-lock-title">Lock</title>
                                    <desc id="banner-lock-description">
                                    Locked padlock icon
                                    </desc>
                                    <path fill="#000000"
                                          fill-rule="evenodd"
                                          d="M26 0c10.493 0 19 8.507 19 19v9h3a4 4 0 0 1 4 4v28a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V32a4 4 0 0 1 4-4h3v-9C7 8.507 15.507 0 26 0zm0 8c-5.979 0-10.843 4.77-10.996 10.712L15 19v9h22v-9c0-6.075-4.925-11-11-11z" />
                                </svg>
</span>) or <strong>https://</strong> means you've safely
                                connected to the .gov website. Share sensitive
                                information only on official, secure websites.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        

        
    
    
    

<div class="usa-overlay">
</div>



<header class="usa-header usa-header--extended usa-header--wide" data-header data-testid="header"    >
    <div class="ncbi-header">
        <div class="ncbi-header__container">
            <a class="ncbi-header__logo-container"
               href="https://www.ncbi.nlm.nih.gov/">
                <img alt="NCBI home page"
                     class="ncbi-header__logo-image"
                     src="/static/img/ncbi-logos/nih-nlm-ncbi--white.svg"
                     width="410"
                     height="100" />
            </a>

            <!-- Mobile menu hamburger button -->
            



















    
        <button
            type="button"
        
    
    class="usa-menu-btn ncbi-header__hamburger-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Show menu"
    data-testid="navMenuButton"
    
    >
    
        

        
                <svg aria-hidden="true"
                     class="ncbi-hamburger-icon"
                     fill="none"
                     focusable="false"
                     height="21"
                     viewBox="0 0 31 21"
                     width="31"
                     xmlns="http://www.w3.org/2000/svg">
                    <path clip-rule="evenodd"
                          d="M0.125 20.75H30.875V17.3333H0.125V20.75ZM0.125 12.2083H30.875V8.79167H0.125V12.2083ZM0.125 0.25V3.66667H30.875V0.25H0.125Z"
                          fill="#F1F1F1"
                          fill-rule="evenodd" />
                </svg>
            

        
    
        
            </button>
        



            
                <!-- Desktop buttons-->
                <div class="ncbi-header__desktop-buttons">
                    
                        <!-- Desktop search button -->
                        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button
           "
    aria-expanded="false"
    aria-controls="search-field-desktop-navigation"
    aria-label="Show search overlay"
    data-testid="toggleSearchPanelButton"
    data-toggle-search-panel-button
    >
    
        

        
                            



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#search" />
    </svg>


                            Search
                        

        
    
        
            </button>
        


                    

                    <!-- Desktop login dropdown -->
                    
                        <div class="ncbi-header__login-dropdown">
                            



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--unstyled ncbi-header__desktop-button ncbi-header__login-dropdown-button
           "
    aria-expanded="false"
    aria-controls="login-dropdown-menu"
    aria-label="Show login menu"
    data-testid="toggleLoginMenuDropdown"
    data-desktop-login-button
    >
    
        

        
                                



    <svg class="usa-icon " role="graphics-symbol" aria-hidden="true" >
        
        <use xlink:href="/static/img/sprite.svg#person" />
    </svg>



                                <span data-login-dropdown-text>Log in</span>

                                <!-- Dropdown icon pointing up -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-less ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-up-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_less" />
    </svg>



                                <!-- Dropdown icon pointing down -->
                                



    <svg class="usa-icon ncbi-header__login-dropdown-icon ncbi-header__login-dropdown-icon--expand-more ncbi-header__login-dropdown-icon--hidden" role="graphics-symbol" aria-hidden="true" data-login-dropdown-down-arrow>
        
        <use xlink:href="/static/img/sprite.svg#expand_more" />
    </svg>


                            

        
    
        
            </button>
        



                            <!-- Login dropdown menu -->
                            <ul class="usa-nav__submenu ncbi-header__login-dropdown-menu"
                                id="login-dropdown-menu"
                                data-desktop-login-menu-dropdown
                                hidden>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                                    </li>
                                
                                    <li class="usa-nav__submenu-item">
                                        <!-- Uses custom style overrides to render external and document links. -->
                                        









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                                    </li>
                                
                                <li class="usa-nav__submenu-item">
                                    



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           usa-button--outline ncbi-header__login-dropdown-logout-button
           "
    
    
    
    data-testid="desktopLogoutButton"
    data-desktop-logout-button
    >
    
        

        Log out

        
    
        
            </button>
        


                                </li>
                            </ul>
                        </div>
                    
                </div>
            
        </div>
    </div>

    <!-- Search panel -->
    
        <div class="ncbi-search-panel ncbi--show-only-at-desktop"
             data-header-search-panel
             hidden>
            <div class="ncbi-search-panel__container">
                <form action="https://www.ncbi.nlm.nih.gov/search/all/"
                      
                      autocomplete="off"
                      class="usa-search usa-search--big ncbi-search-panel__form"
                      data-testid="desktop-navigation-search-form"
                      method="GET"
                      role="search">
                    <label class="usa-sr-only" for="search-field-desktop-navigation">
                        Search…
                    </label>
                    <input class="usa-input"
                           id="search-field-desktop-navigation"
                           name="term"
                           
                               placeholder="Search NCBI"
                           
                           type="search"
                           value="" />
                    



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                        <span class="usa-search__submit-text">
                            Search NCBI
                        </span>
                    

        
    
        
            </button>
        


                </form>

                
            </div>
        </div>
    

    <nav aria-label="Primary navigation" class="usa-nav">
        <p class="usa-sr-only" id="primary-navigation-sr-only-title">
            Primary site navigation
        </p>

        <!-- Mobile menu close button -->
        



















    
        <button
            type="button"
        
    
    class="usa-nav__close ncbi-nav__close-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    aria-label="Close navigation menu"
    data-testid="navCloseButton"
    
    >
    
        

        
            <img src="/static/img/usa-icons/close.svg" alt="Close" />
        

        
    
        
            </button>
        



        
            <!-- Mobile search component -->
            <form class="usa-search usa-search--small ncbi--hide-at-desktop margin-top-6"
                  action="https://www.ncbi.nlm.nih.gov/search/all/"
                  
                  autocomplete="off"
                  data-testid="mobile-navigation-search-form"
                  method="GET"
                  role="search">
                <label class="usa-sr-only" for="search-field-mobile-navigation">
                    Search
                </label>

                <input class="usa-input"
                       id="search-field-mobile-navigation"
                       type="search"
                       
                           placeholder="Search NCBI"
                       
                       name="term" />

                



















    
        <button
            type="submit"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           
           "
    
    
    
    
    
    >
    
        

        
                    <!-- This SVG should be kept inline and not replaced with a link to the icon as otherwise it will render in the wrong color -->
                    <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjI0Ij48cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTE1LjUgMTRoLS43OWwtLjI4LS4yN0E2LjQ3MSA2LjQ3MSAwIDAgMCAxNiA5LjUgNi41IDYuNSAwIDEgMCA5LjUgMTZjMS42MSAwIDMuMDktLjU5IDQuMjMtMS41N2wuMjcuMjh2Ljc5bDUgNC45OUwyMC40OSAxOWwtNC45OS01em0tNiAwQzcuMDEgMTQgNSAxMS45OSA1IDkuNVM3LjAxIDUgOS41IDUgMTQgNy4wMSAxNCA5LjUgMTEuOTkgMTQgOS41IDE0eiIvPjwvc3ZnPg=="
                         class="usa-search__submit-icon"
                         alt="Search" />
                

        
    
        
            </button>
        


            </form>

            
        

        <!-- Primary navigation menu items -->
        <!-- This usa-nav__inner wrapper is required to correctly style the navigation items on Desktop -->
        

        
            <div class="ncbi-nav__mobile-login-menu ncbi--hide-at-desktop"
                 data-mobile-login-menu
                 hidden>
                <p class="ncbi-nav__mobile-login-menu-status">
                    Logged in as:
                    <strong class="ncbi-nav__mobile-login-menu-email"
                            data-mobile-login-email-text></strong>
                </p>
                <ul class="usa-nav__primary usa-accordion">
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/" class="usa-link  "  >
        

        
            Dashboard
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/" class="usa-link  "  >
        

        
            Publications
        

        
    </a>


                        </li>
                    
                        <li class="usa-nav__primary-item">
                            









    <a href="https://www.ncbi.nlm.nih.gov/account/settings/" class="usa-link  "  >
        

        
            Account settings
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        

        



















    
        <button
            type="button"
        
    
    class="usa-button
           

           
               
               
               
               
            

           
           
           ncbi-nav__mobile-login-button ncbi--hide-at-desktop
           "
    
    
    
    data-testid="mobileLoginButton"
    data-mobile-login-button
    >
    
        

        Log in

        
    
        
            </button>
        


    </nav>
</header>

    
    
        

<section class="pmc-header pmc-header--basic" aria-label="PMC Header with search box">
    <div class="pmc-nav-container">
        <div class="pmc-header__bar">
           <div class="pmc-header__logo">
               <a href="/" title="Home" aria-label="PMC Home"></a>
           </div>
            <button
                    type="button"
                    class="usa-button usa-button--unstyled pmc-header__search__button"
                    aria-label="Open search"
                    data-ga-category="search"
                    data-ga-action="PMC"
                    data-ga-label="pmc_search_panel_mobile"
            >
                <svg class="usa-icon width-4 height-4 pmc-icon__open" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#search"></use>
                </svg>
                <svg class="usa-icon width-4 height-4 pmc-icon__close" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
            </button>
        </div>
        <div class="pmc-header__search">
            


<form class="usa-search usa-search--extra usa-search--article-right-column  pmc-header__search__form" id="pmc-search-form" autocomplete="off" role="search">
<label class="usa-sr-only" for="pmc-search">Search PMC Full-Text Archive</label>
<span class="autoComplete_wrapper flex-1">
<input class="usa-input width-full maxw-none" required="required" placeholder="Search PMC Full-Text Archive" id="pmc-search"  type="search" name="term" data-autocomplete-url="https://pmc.ncbi.nlm.nih.gov/autocomp/search/autocomp/"/>
</span>
<button
class="usa-button"
type="submit"
formaction="https://www.ncbi.nlm.nih.gov/pmc/"
data-ga-category="search"
data-ga-action="PMC"
data-ga-label="PMC_search_button"
>
<span class="usa-search__submit-text">Search in PMC</span>
<img
src="/static/img/usa-icons-bg/search--white.svg"
class="usa-search__submit-icon"
alt="Search"
/>
</button>
</form>
            <div class="display-flex flex-column tablet:flex-row tablet:flex-justify flex-justify-center flex-align-center width-full desktop:maxw-44">
                <ul class="pmc-header__search__menu">
                    <li>
                        
                            <a class="usa-link" href="/journals/" data-ga-action="featured_link" data-ga-label="journal list">
                                Journal List
                            </a>
                        
                    </li>
                    <li>
                        
                            <a class="usa-link" href="/about/userguide/" data-ga-action="featured_link"
                            data-ga-label="user guide">
                                User Guide
                            </a>
                        
                    </li>
                </ul>
                
                    <button form="pmc-search-form" formaction="https://pmc.ncbi.nlm.nih.gov/search/" type="submit" class="usa-button usa-button--unstyled hover:text-no-underline text-no-underline width-auto margin-top-1 tablet:margin-top-0">
     <span class="bg-green-label padding-05 text-white">New</span><span class="text-underline text-primary">Try this search in PMC Beta Search</span>
</button>
                
            </div>
        </div>
    </div>
</section>

    


        
        

       
  <div class="usa-section padding-top-0 desktop:padding-top-6 pmc-article-section" data-article-db="pmc" data-article-id="12370940">

    

   



<div class="grid-container pmc-actions-bar" aria-label="Actions bar" role="complementary">
    <div class="grid-row">
        <div class="grid-col-fill display-flex">
             <div class="display-flex">
                <ul class="usa-list usa-list--unstyled usa-list--horizontal">
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <button
                                type="button"
                                class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                                aria-label="Open resources"
                                data-extra-class="is-visible-resources"
                                data-ga-category="resources_accordion"
                                data-ga-action="click"
                                data-ga-label="mobile_icon"
                        >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#more_vert"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex mob">
                        <a
                                href="https://doi.org/10.1038/s41598-025-16673-3"
                                class="usa-link display-flex usa-tooltip"
                                role="button"
                                target="_blank"
                                rel="noreferrer noopener"
                                title="View on publisher site"
                                data-position="bottom"
                                aria-label="View on publisher site"
                                data-ga-category="actions"
                                data-ga-action="click"
                                data-ga-label="publisher_link_mobile"
                        >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#launch"></use>
                                </svg>
                        </a>
                    </li>
                    
                    
                        <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                             <a
                                     href="pdf/41598_2025_Article_16673.pdf"
                                     class="usa-link display-flex usa-tooltip"
                                     role="button"
                                     title="Download PDF"
                                     data-position="bottom"
                                     aria-label="Download PDF"
                                     data-ga-category="actions"
                                     data-ga-action="click"
                                     data-ga-label="pdf_download_mobile"
                             >
                                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                                </svg>
                            </a>
                        </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button
                            class="usa-button usa-button--unstyled  usa-tooltip collections-dialog-trigger collections-button display-flex collections-button-empty"
                            title="Add to Collections"
                            data-position="bottom"
                            aria-label="Save article in MyNCBI collections."
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="collections_button_mobile"
                            data-collections-open-dialog-enabled="false"
                            data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370940%2F%3Freport%3Dclassic%23open-collections-dialog"
                            data-in-collections="false"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="margin-right-2 mobile-lg:margin-right-4 display-flex">
                        <button role="button" class="usa-button usa-button--unstyled usa-tooltip citation-dialog-trigger display-flex"
                            aria-label="Open dialog with citation text in different styles"
                            title="Cite"
                            data-position="bottom"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="cite_mobile"
                            data-all-citations-url="/resources/citations/12370940/"
                            data-citation-style="nlm"
                            data-download-format-link="/resources/citations/12370940/export/"
                        >
                            <svg class="usa-icon width-4 height-4 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                            </svg>
                        </button>
                    </li>
                    
                    <li class="pmc-permalink display-flex" >
                         <button
                                 type="button"
                                 title="Permalink"
                                 data-position="bottom"
                                 class="usa-button usa-button--unstyled display-flex usa-tooltip"
                                 aria-label="Show article permalink"
                                 aria-expanded="false"
                                 aria-haspopup="true"
                                 data-ga-category="actions"
                                 data-ga-action="open"
                                 data-ga-label="permalink_mobile"
                         >
                            <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                                <use xlink:href="/static/img/sprite.svg#share"></use>
                            </svg>
                        </button>
                        

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370940/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
                    </li>
                </ul>
            </div>
            <button
                    type="button"
                    class="usa-button pmc-sidenav__container__open usa-button--unstyled width-auto display-flex"
                    aria-label="Open article navigation"
                    data-extra-class="is-visible-in-page"
                    data-ga-category="actions"
                    data-ga-action="open"
                    data-ga-label="article_nav_mobile"
            >
                <svg class="usa-icon width-4 height-4" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#list"></use>
                </svg>
            </button>
        </div>
    </div>
</div>
    <div class="grid-container desktop:padding-left-6">
      <div id="article-container" class="grid-row grid-gap">
        <div class="grid-col-12 desktop:grid-col-8 order-2 pmc-layout__content">
            <div class="grid-container padding-left-0 padding-right-0">
                <div class="grid-row desktop:margin-left-neg-6">
                    <div class="grid-col-12">
                        <div class="pmc-layout__disclaimer" role="complementary" aria-label="Disclaimer note">
    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
    the contents by NLM or the National Institutes of Health.<br/>
    Learn more:
    <a class="usa-link" data-ga-category="Link click" data-ga-action="Disclaimer" data-ga-label="New disclaimer box" href="/about/disclaimer/">PMC Disclaimer</a>
    |
    <a class="usa-link" data-ga-category="Link click" data-ga-action="PMC Copyright Notice" data-ga-label="New disclaimer box" href="/about/copyright/">
        PMC Copyright Notice
    </a>
</div>
                    </div>
                </div>
                <div class="grid-row pmc-wm desktop:margin-left-neg-6">
                    <!-- Main content -->
                    <main
                      id="main-content"
                      class="usa-layout-docs__main usa-layout-docs grid-col-12 pmc-layout pmc-prose padding-0"
                    >

                      
                        <section class="pmc-journal-banner text-center line-height-none" aria-label="Journal banner"><img src="https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-scirep.jpg" alt="Scientific Reports logo" usemap="#pmc-banner-imagemap" width="500" height="75"><map name="pmc-banner-imagemap"><area alt="Link to Scientific Reports" title="Link to Scientific Reports" shape="default" href="http://www.nature.com/srep" target="_blank" rel="noopener noreferrer"></map></section><article lang="en"><section aria-label="Article citation and metadata"><section class="pmc-layout__citation font-secondary font-xs"><div>
<div class="display-inline-block"><button type="button" class="cursor-pointer text-no-underline bg-transparent border-0 padding-0 text-left margin-0 text-normal text-primary" aria-controls="journal_context_menu">Sci Rep</button></div>. 2025 Aug 21;15:30803. doi: <a href="https://doi.org/10.1038/s41598-025-16673-3" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-16673-3</a>
</div>
<nav id="journal_context_menu" hidden="hidden"><ul class="menu-list font-family-ui" role="menu">
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem">Search in PMC</a></li>
<li role="presentation"><a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sci%20Rep%22%5Bjour%5D" lang="en" class="usa-link" role="menuitem">Search in PubMed</a></li>
<li role="presentation"><a href="https://www.ncbi.nlm.nih.gov/nlmcatalog?term=%22Sci%20Rep%22%5BTitle%20Abbreviation%5D" class="usa-link" role="menuitem">View in NLM Catalog</a></li>
<li role="presentation"><a href="?term=%22Sci%20Rep%22%5Bjour%5D" class="usa-link" role="menuitem" data-add-to-search="true">Add to search</a></li>
</ul></nav></section><section class="front-matter"><div class="ameta p font-secondary font-xs">
<hgroup><h1>Hybrid pre trained model based feature extraction for enhanced indoor scene classification in federated learning environments</h1></hgroup><div class="cg p">
<a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Dutta%20M%22%5BAuthor%5D" class="usa-link" aria-describedby="id1"><span class="name western">Monica Dutta</span></a><div hidden="hidden" id="id1">
<h3><span class="name western">Monica Dutta</span></h3>
<div class="p">
<sup>1</sup>Department of Computer Engineering &amp; Applications, Institute of Engineering &amp; Technology, GLA University, Mathura, India </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Dutta%20M%22%5BAuthor%5D" class="usa-link"><span class="name western">Monica Dutta</span></a>
</div>
</div>
<sup>1</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gupta%20D%22%5BAuthor%5D" class="usa-link" aria-describedby="id2"><span class="name western">Deepali Gupta</span></a><div hidden="hidden" id="id2">
<h3><span class="name western">Deepali Gupta</span></h3>
<div class="p">
<sup>2</sup>Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Gupta%20D%22%5BAuthor%5D" class="usa-link"><span class="name western">Deepali Gupta</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khullar%20V%22%5BAuthor%5D" class="usa-link" aria-describedby="id3"><span class="name western">Vikas Khullar</span></a><div hidden="hidden" id="id3">
<h3><span class="name western">Vikas Khullar</span></h3>
<div class="p">
<sup>2</sup>Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Khullar%20V%22%5BAuthor%5D" class="usa-link"><span class="name western">Vikas Khullar</span></a>
</div>
</div>
<sup>2</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Juneja%20S%22%5BAuthor%5D" class="usa-link" aria-describedby="id4"><span class="name western">Sapna Juneja</span></a><div hidden="hidden" id="id4">
<h3><span class="name western">Sapna Juneja</span></h3>
<div class="p">
<sup>3</sup>KIET Group of Institutions, Delhi- NCR, Ghaziabad, India </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Juneja%20S%22%5BAuthor%5D" class="usa-link"><span class="name western">Sapna Juneja</span></a>
</div>
</div>
<sup>3</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Alroobaea%20R%22%5BAuthor%5D" class="usa-link" aria-describedby="id5"><span class="name western">Roobaea Alroobaea</span></a><div hidden="hidden" id="id5">
<h3><span class="name western">Roobaea Alroobaea</span></h3>
<div class="p">
<sup>4</sup>Department of Computer Science, College of Computers and Information Technology, Taif University, P.O. Box 11099, Taif, 21944 Saudi Arabia </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Alroobaea%20R%22%5BAuthor%5D" class="usa-link"><span class="name western">Roobaea Alroobaea</span></a>
</div>
</div>
<sup>4</sup>, <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sapra%20P%22%5BAuthor%5D" class="usa-link" aria-describedby="id6"><span class="name western">Pooja Sapra</span></a><div hidden="hidden" id="id6">
<h3><span class="name western">Pooja Sapra</span></h3>
<div class="p">
<sup>5</sup>Parul Institute of Engineering and Technology, Faculty of Engineering and Technology, Parul University, Vadodara, Gujarat India </div>
<div class="p">Find articles by <a href="https://pubmed.ncbi.nlm.nih.gov/?term=%22Sapra%20P%22%5BAuthor%5D" class="usa-link"><span class="name western">Pooja Sapra</span></a>
</div>
</div>
<sup>5,</sup><sup>✉</sup>
</div>
<ul class="d-buttons inline-list">
<li><button class="d-button" aria-controls="aip_a" aria-expanded="false">Author information</button></li>
<li><button class="d-button" aria-controls="anp_a" aria-expanded="false">Article notes</button></li>
<li><button class="d-button" aria-controls="clp_a" aria-expanded="false">Copyright and License information</button></li>
</ul>
<div class="d-panels font-secondary-light">
<div id="aip_a" class="d-panel p" style="display: none">
<div class="p" id="Aff1">
<sup>1</sup>Department of Computer Engineering &amp; Applications, Institute of Engineering &amp; Technology, GLA University, Mathura, India </div>
<div id="Aff2">
<sup>2</sup>Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India </div>
<div id="Aff3">
<sup>3</sup>KIET Group of Institutions, Delhi- NCR, Ghaziabad, India </div>
<div id="Aff4">
<sup>4</sup>Department of Computer Science, College of Computers and Information Technology, Taif University, P.O. Box 11099, Taif, 21944 Saudi Arabia </div>
<div id="Aff5">
<sup>5</sup>Parul Institute of Engineering and Technology, Faculty of Engineering and Technology, Parul University, Vadodara, Gujarat India </div>
<div class="author-notes p"><div class="fn" id="_fncrsp93pmc__">
<sup>✉</sup><p class="display-inline">Corresponding author.</p>
</div></div>
</div>
<div id="anp_a" class="d-panel p" style="display: none"><div class="notes p"><section id="historyarticle-meta1" class="history"><p>Received 2025 May 31; Accepted 2025 Aug 18; Collection date 2025.</p></section></div></div>
<div id="clp_a" class="d-panel p" style="display: none">
<div>© The Author(s) 2025</div>
<p><strong>Open Access</strong> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
<div class="p"><a href="/about/copyright/" class="usa-link">PMC Copyright notice</a></div>
</div>
</div>
<div>PMCID: PMC12370940  PMID: <a href="https://pubmed.ncbi.nlm.nih.gov/40841736/" class="usa-link">40841736</a>
</div>
</div></section></section><section aria-label="Article content"><section class="body main-article-body"><section class="abstract" id="Abs1"><h2>Abstract</h2>
<p id="Par1">Classification of indoor scenes is a crucial task of computer vision. It has widespread applications like smart homes, smart cities, robotics, etc. Primitive classification methods like Support Vector Machines (SVM) and K-Nearest Neighbors (KNN), provide a compromised performance with complex indoor environments due to light variations, intra-class similarities, and occlusions. Deep Learning (DL) models, especially Convolutional Neural Networks (CNNs), have improved classification accuracy significantly by extracting the image features. This study proposes and implements a novel MultiData model, which integrates DL with Linear Discriminant Analysis (LDA) and Federated Learning (FL) to enhance classification performance while preserving data privacy. Comparative performance analysis of the MultiData model with VGG16, VGG19, and ResNet152 shows that MultiData achieves near-perfect accuracy (99.99%) and minimal validation loss (0%). The performance of the proposed model was further compared with FL-based training across four clients using IID and non-IID datasets. The results further confirm the model’s robustness, achieving 100% accuracy in training and over 95% in validation times respectively. This research is beneficial for stakeholders in healthcare, smart infrastructure, surveillance, and other IoT-based automation, aligning with SDG 9 (Industry, Innovation, and Infrastructure), SDG 11 (Sustainable Cities and Communities), and SDG 12 (Responsible Consumption and Production). By enabling efficient and privacy-preserving scene classification, this work contributes significantly to the development of smart and sustainable environments enhancing AI-based decision-making across various sectors.</p>
<section id="kwd-group1" lang="en" class="kwd-group"><p><strong>Keywords:</strong> Indoor scene classification, Computer vision, Deep learning, Federated learning</p></section><section id="kwd-group2" class="kwd-group"><p><strong>Subject terms:</strong> Mathematics and computing, Computer science</p></section></section><section id="Sec1"><h2 class="pmc_sec_title">Introduction</h2>
<p id="Par2">Classification of objects is a typical topic of research in the image domain. Classifying or categorizing indoor images into predefined classes is termed indoor scene classification<sup><a href="#CR1" class="usa-link" aria-describedby="CR1">1</a></sup>. The images are focused on indoor settings or enclosed spaces. It is a subdomain of image processing, computer vision, and scene recognition where the classification is majorly done into various classes like office, kitchen, library, living room, gym, bedroom, etc., based on the visual content of the images. Monitoring and analyzing domestic activities also make understanding indoor environment behaviors and human activities easier<sup><a href="#CR2" class="usa-link" aria-describedby="CR2">2</a></sup>.</p>
<p id="Par3">The performance of Machine Learning (ML) models is highly dependent on the quality of the input training data, and proper organization of the dataset. An enhanced output is obtained when the training data is of high quality. The high cost and time-consuming process of data collection often results in low-quality samples or an imbalanced dataset which further creates performance gaps and degrades the performance of ML models<sup><a href="#CR3" class="usa-link" aria-describedby="CR3">3</a></sup>. Diverse fields are explored to provide a personal service of recognizing the activities and surroundings of users. Data augmentation has been applied in various fields to address data-related issues. Various operations like rotating, cropping, or adding noise to the image are used in the image domain and acoustic studies. One of the computer vision tasks that analyze background and target objects and predicts the scene category is called ‘scene classification’<sup><a href="#CR4" class="usa-link" aria-describedby="CR4">4</a></sup>. The same scene can have multiple points of view or scene areas, which makes it difficult to obtain feature space. Indoor scene classification is also integrated with robotic applications to perceive and recognize the background environment<sup><a href="#CR5" class="usa-link" aria-describedby="CR5">5</a></sup>.</p>
<p id="Par4">DL models have also emerged as effective methods to classify indoor images<sup><a href="#CR6" class="usa-link" aria-describedby="CR6">6</a></sup>. Before DL models emerged, primitive methods like traditional image processing and feature extraction techniques were used to categorize indoor images into predefined classes. Primitive methods heavily depended on basic classification techniques like SVM, KNN, and other manual feature extraction techniques<sup><a href="#CR7" class="usa-link" aria-describedby="CR7">7</a></sup>. Though such primitive methods were somewhat effective, they were inefficient with complex scenes with cluttered, overlapped, or dimly lit objects. The DL models emerged with significant advancements in this field. Several challenges related to the variability and complexity of the object have been overcome with the use of DL in this domain. Approaches like DL-based CNNs can automatically learn from robust features of raw images and have replaced primitive classification methods<sup><a href="#CR8" class="usa-link" aria-describedby="CR8">8</a></sup>.</p>
<p id="Par5">FL offers many newly developed services and applications that can revolutionize existing IoT systems completely. It plays a major role in classifying indoor images by enabling collaborative learning, addressing privacy concerns, and enhancing performance without centralized data storage<sup><a href="#CR9" class="usa-link" aria-describedby="CR9">9</a></sup>. The common applications of FL in indoor image classification are in smart homes, retail, security systems, and healthcare, where sensitive information cannot be shared among devices<sup><a href="#CR10" class="usa-link" aria-describedby="CR10">10</a></sup>. The foremost advantage of FL is preserving the privacy of indoor images as they may contain confidential information. The data is kept localized on user devices like cameras, smartphones, etc., and each local device trains its local ML model on its images<sup><a href="#CR11" class="usa-link" aria-describedby="CR11">11</a></sup>. The devices then share the mathematical representations of their model performances, called gradients, to the central server where the aggregates are combined into a global model which is redistributed to the devices. These global models enhance knowledge without data sharing which is effective across various scenarios. This methodology reduces data breaches and enforces data privacy preservation by not sharing the raw data and collaborative training of ML models on multiple local devices<sup><a href="#CR12" class="usa-link" aria-describedby="CR12">12</a></sup>. FL also allows the training of models on diverse datasets collected across diverse locations further enhancing the robustness of the classification models. It can minimize the computational load by distributing them across the local devices<sup><a href="#CR13" class="usa-link" aria-describedby="CR13">13</a></sup>.</p>
<p id="Par6">Several factors like high intra-class variability handling, addressing similar inter-class objects, feature extraction, occlusion and cluttered objects, and consistent lighting conditions affect the classification of indoor scenes. High intra-class variability handling is an essential factor that affects indoor scene classification<sup><a href="#CR14" class="usa-link" aria-describedby="CR14">14</a></sup>. Significant visual variations are exhibited by indoor scenes of the same class or category. This can be caused by varied structure, object placement, and lighting. Therefore, a robust and reliable classification method must be able to handle data variations. Another major factor affecting the classification of indoor scenes is addressing the similarity of inter-class objects<sup><a href="#CR15" class="usa-link" aria-describedby="CR15">15</a></sup>. Many classes may exhibit inter-class visual similarity of objects, e.g., chairs and tables in living rooms and offices. A reliable classifier must be able to differentiate scenes based on their contextual information and spatial arrangements rather than just the objects.</p>
<p id="Par7">The classification method must be sufficiently resilient to ensure accuracy. Occluded and cluttered images often add complexity to indoor objects therefore, feature extraction is a comprehensive understanding of the scene necessary for capturing the spatial and object-specific details<sup><a href="#CR16" class="usa-link" aria-describedby="CR16">16</a></sup>. Lighting conditions are another major aspect that affects the classification of indoor scenes. Indoor images may be subject to inconsistent lighting conditions caused by shadows, reflections, etc. The classification methods must perform efficiently in diverse lighting conditions.</p>
<p id="Par8">Classification of indoor scenes plays an important role in understanding the real-world indoor scenario. Some domains where indoor scene classification is important are: robotics for context-aware navigation, IoT-enabled smart homes for personalized automation, drones and CCTV for surveillance and disaster management, Augmented and virtual realities for interior design simulation, and assistive technologies for the elderly or the visually impaired<sup><a href="#CR17" class="usa-link" aria-describedby="CR17">17</a></sup>. This way ML and DL have revolutionized image classification tasks by accurately identifying and classifying objects.</p>
<p id="Par9">The present research work addresses various key objectives regarding the classification of images. The key objectives of the research can be enlisted as follows:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par11">A basic DL algorithm is implemented.</p></li>
<li><p id="Par12">A new approach named LDA, which combines ML and DL models, has been implemented as shown in Fig 3.</p></li>
<li><p id="Par13">A new approach implemented on FL as depicted in Fig 4.</p></li>
</ul>
<p id="Par14">The rest of the manuscript is organized in 5 sections. Section 2 comprises the background literature study in various sections relevant to the experiment. The third section comprises the materials and methods required for the implementation of this study. Section 4 comprises the results and discussions of the study, followed by the conclusion section.</p></section><section id="Sec2"><h2 class="pmc_sec_title">Literature survey</h2>
<p id="Par15">Indoor image classification is becoming increasingly important in applications like smart cities, smart healthcare, robotics, and security systems. Several research works have been done to propose and implement robust computational approaches and facilitate indoor scene classifications in various research domains<sup><a href="#CR18" class="usa-link" aria-describedby="CR18">18</a></sup>.</p>
<p id="Par16">Traditional indoor image classification methods were performed using feature extraction methods like Scale-Invariant Feature Transform (SIFT) and Histogram of Oriented Gradients (HOG)<sup><a href="#CR19" class="usa-link" aria-describedby="CR19">19</a></sup>. Such techniques implemented manually designed algorithms for detecting textures and edges in an image. Though effective, these methods seemed ineffective in complex indoor environments like variable lighting or occlusions and had several limitations such as reliance and expertise on the domain.</p>
<p id="Par17">The advent of ML algorithms like SVMs and Random Forest (RF) significantly enhanced the learning patterns from the features extracted using traditional methods<sup><a href="#CR20" class="usa-link" aria-describedby="CR20">20</a></sup>. Though being comparatively more effective, the quality of these models was constrained which was eventually removed by making a transition to DL models<sup><a href="#CR21" class="usa-link" aria-describedby="CR21">21</a></sup>. CNNs improved the performance of tasks involving diverse and complex indoor scenes by ensuring automatic feature learning from the raw images. Such evolution allows better generalization across various application domains and datasets by minimizing the dependence on manually extracting features.</p>
<ol class="list" style="list-style-type:lower-alpha"><li><p id="Par19">Applications of ML in indoor image classification.</p></li></ol>
<p id="Par20">Various researchers have understood the importance of classifying indoor images in diverse application areas. Several ML methods have efficiently been used to classify indoor scenes into distinct classes. The lightweight architecture of the GenericConv model is designed and proposed in<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup> to classify images using max-pooling, dense layers, integration of convolutional layers, dropout, and few-shot learning which helps in the prevention of overfitting and efficiently extracts features. MiniSun, MiniPlaces, and MIT-Indoor 67 datasets were used to test the model results, where superior model performances were highlighted. In<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup>, the authors used S3DIS dataset to classify 3D indoor point clouds using XGBoost, Multi-Layer Perceptron (MLP), Random Forest (RF), and TabNet al.gorithms, with RF achieving 86% accuracy. A novel ML model named RepConv is introduced in<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup> to classify scenes on the Intel scenes dataset which was recategorized for binary and multi-class classification. The authors demonstrated a comparative performance of RepConv on ResNet 50 and SE ResNext 101 using fewer parameters and epochs. For multi-task classification, 93.55% and 75.54% accuracies are achieved on training and validation data, respectively. Whereas, for binary classification, 98.08% and 92.70% are achieved on training and validation data, respectively.</p>
<p id="Par21">A comprehensive review of ML is provided in various image processing techniques, highlighting the challenges of foggy images and enhancing, segmenting, and denoising images using ML. The advantages of transfer learning for small datasets are emphasized and the need for better datasets and lightweight architectures is also emphasized for real-time defogging applications. Scene-Aware Label Graph Learning (SALGL) framework to classify multi-label images is introduced in<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup> which captures label co-occurrence dynamically by associating them with scene categories. A semantic attention module is incorporated for aligning visual label features, a graph propagation mechanism is used to refine label predictions, and a scene-aware co-occurrence module is implemented to model label interactions. Robust label handling dependencies are implemented in diverse datasets, which outperforms the existing methods significantly. In<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>, the authors present a semantic classification of LiDAR-based sensory data of indoor robot navigation. A cost and memory-efficient framework is introduced to distinguish between doorways, rooms, halls, and corridors and missing or infinite LiDAR values are handled using the preprocessing techniques. SVM achieved the best testing accuracy of 97.21%. Table <a href="#Tab1" class="usa-link">1</a> summarizes the various ML approaches used for classifying indoor images.</p>
<section class="tw xbox font-sm" id="Tab1"><h3 class="obj_head">Table 1.</h3>
<div class="caption p"><p>ML approaches and applications in indoor image classification.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Refs. / Year</th>
<th align="left" colspan="1" rowspan="1">Domain</th>
<th align="left" colspan="1" rowspan="1">Datasets Used</th>
<th align="left" colspan="1" rowspan="1">Techniques/Models</th>
<th align="left" colspan="1" rowspan="1">Results</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR22" class="usa-link" aria-describedby="CR22">22</a></sup>/ 2024</td>
<td align="left" colspan="1" rowspan="1">Few-shot image classification</td>
<td align="left" colspan="1" rowspan="1">MiniSun, MiniPlaces, MIT-Indoor 67</td>
<td align="left" colspan="1" rowspan="1">GenericConv (Convolutional layers, dropout, max-pooling, dense layers)</td>
<td align="left" colspan="1" rowspan="1">52.16% (five-shot on MiniSun), 37.26% (five-shot on MIT-Indoor 67)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR23" class="usa-link" aria-describedby="CR23">23</a></sup> / 2023</td>
<td align="left" colspan="1" rowspan="1">Indoor 3D point cloud classification</td>
<td align="left" colspan="1" rowspan="1">S3DIS</td>
<td align="left" colspan="1" rowspan="1">XGBoost, MLP, Random Forest (RF), TabNet</td>
<td align="left" colspan="1" rowspan="1">RF achieved 86% accuracy</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR24" class="usa-link" aria-describedby="CR24">24</a></sup> / 2022</td>
<td align="left" colspan="1" rowspan="1">Scene classification (binary &amp; multi-class)</td>
<td align="left" colspan="1" rowspan="1">Intel Scenes</td>
<td align="left" colspan="1" rowspan="1">RepConv, ResNet-50, SE-ResNeXt-101</td>
<td align="left" colspan="1" rowspan="1">Binary: 98.08% (train), 92.70% (validation); Multi-class: 93.55% (train), 75.54% (validation)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR25" class="usa-link" aria-describedby="CR25">25</a></sup> / 2023</td>
<td align="left" colspan="1" rowspan="1">Multi-label image classification</td>
<td align="left" colspan="1" rowspan="1">Pascal VOC 2007, NUS-WIDE, MS-COCO</td>
<td align="left" colspan="1" rowspan="1">SALGL framework (Scene-Aware Label Graph Learning, semantic attention module)</td>
<td align="left" colspan="1" rowspan="1">mAP: 96.7% (VOC 2007), 66.3% (NUS-WIDE), 87.3% (MS-COCO)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR26" class="usa-link" aria-describedby="CR26">26</a></sup>/ 2022</td>
<td align="left" colspan="1" rowspan="1">Indoor robot navigation using semantic classification</td>
<td align="left" colspan="1" rowspan="1">LiDAR sensory data</td>
<td align="left" colspan="1" rowspan="1">SVM with preprocessing (merging classes, handling missing data)</td>
<td align="left" colspan="1" rowspan="1">97.21% testing accuracy</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par23">ML uses algorithms and statistical methods that can be trained using data patterns without explicit programming. ML models extract features to classify images into identified classes. Some of the most widely used ML methods for image classification are:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par25">SVMs classify images by segregating features into specific classes using hyperplanes. They can classify indoor images from outdoor images using edges, colors, or textures<sup><a href="#CR27" class="usa-link" aria-describedby="CR27">27</a></sup>.</p></li>
<li><p id="Par26">RF and DT classify images by learning decision rules based on their extracted features. They generally detect indoor objects based on their shapes<sup><a href="#CR28" class="usa-link" aria-describedby="CR28">28</a></sup>.</p></li>
<li><p id="Par27">KNN is capable of identifying the image class based on the nearest neighbor of the image. They can classify images based on their visual similarity<sup><a href="#CR29" class="usa-link" aria-describedby="CR29">29</a></sup>.</p></li>
</ul>
<p id="Par28">Though ML algorithms are efficient in classification methods, they are encountered with several challenges. Some of the challenges encountered by ML algorithms are that they require manual feature extraction thereby making them more time-consuming. They also exhibit limited performance of large-scale, complex datasets<sup><a href="#CR30" class="usa-link" aria-describedby="CR30">30</a></sup>.</p>
<ul class="list" style="list-style-type:none"><li>
<span class="label">b.</span><p class="display-inline" id="Par30">Applications of DL in indoor image classification.</p>
</li></ul>
<p id="Par31">Various DL approaches and frameworks are also used in indoor scene classification. The extensive literature review demonstrates diverse applications of DL in this domain. A novel DL framework for multi-label image classification is presented in<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup>, where the label-specific pooling (LSP) is introduced. MS-COCO and PASCAL VOC datasets were used to evaluate the methods and achieve better classification results, mean average precision (mAP), recall, and precision compared to the existing models. A multi-scale CNN and LSTM-based scene classification method is proposed and optimized by a Whale Optimization Algorithm (WOA). 98.91% and 94.35% classification accuracies were obtained on lab and FR079 public datasets. On optimizing the learning rate and regularization, the WOA elevated the accuracy to 99.76<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup>.</p>
<p id="Par32">The authors proposed a fusion of hand-crafted methods for indoor scene classification, with DL features from the EfficientNet-B7 model. An enhanced accuracy of 93.87% is obtained using the fusion model which outperformed the state-of-the-art methods<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup>. The authors present a lightweight DL model for big data applications in indoor scene classification. IRDA-YOLOv3, an enhanced lightweight architecture is proposed which improves the YOLOv3-Tiny model for scene classification and object detection. IRDA-YOLOv3 demonstrated improved efficiency and forward computation time by 0.2 ms, reduced parameters by 56.2%, and computation by 46.3%<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup>. Multimodal DL techniques for indoor and outdoor scene classification and recognition are reviewed, demonstrating advancements in CNN and computer vision. The integration of transfer learning and CNNs is highlighted to enhance parameter optimization and feature extraction. Significantly enhanced accuracy was attained on Kaggle’s Indoor Scene dataset<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup>. The various DL models used for indoor scene classification are summarized in Table <a href="#Tab2" class="usa-link">2</a>.</p>
<section class="tw xbox font-sm" id="Tab2"><h3 class="obj_head">Table 2.</h3>
<div class="caption p"><p>DL approaches and applications in indoor image classification.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Refs. / Year</th>
<th align="left" colspan="1" rowspan="1">Domain</th>
<th align="left" colspan="1" rowspan="1">Datasets Used</th>
<th align="left" colspan="1" rowspan="1">Techniques/Models</th>
<th align="left" colspan="1" rowspan="1">Results</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR31" class="usa-link" aria-describedby="CR31">31</a></sup> / 2020</td>
<td align="left" colspan="1" rowspan="1">Multi-label image classification</td>
<td align="left" colspan="1" rowspan="1">MS-COCO, PASCAL VOC</td>
<td align="left" colspan="1" rowspan="1">Joint Input-Output Space Learning, Label-Specific Pooling (LSP)</td>
<td align="left" colspan="1" rowspan="1">Improved mAP, recall, and precision compared to existing models</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR32" class="usa-link" aria-describedby="CR32">32</a></sup> / 2023</td>
<td align="left" colspan="1" rowspan="1">Scene classification with optimization</td>
<td align="left" colspan="1" rowspan="1">Lab dataset, FR079</td>
<td align="left" colspan="1" rowspan="1">Multi-scale CNN, LSTM, Whale Optimization Algorithm (WOA)</td>
<td align="left" colspan="1" rowspan="1">98.91% (lab), 94.35% (FR079); Optimized accuracy: 99.76%</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR33" class="usa-link" aria-describedby="CR33">33</a></sup> / 2023</td>
<td align="left" colspan="1" rowspan="1">Fusion of hand-crafted &amp; DL features</td>
<td align="left" colspan="1" rowspan="1">Not specified</td>
<td align="left" colspan="1" rowspan="1">EfficientNet-B7, Feature Fusion Model</td>
<td align="left" colspan="1" rowspan="1">Enhanced accuracy: 93.87%, outperformed state-of-the-art</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR34" class="usa-link" aria-describedby="CR34">34</a></sup> / 2022</td>
<td align="left" colspan="1" rowspan="1">Lightweight DL for big data scene classification</td>
<td align="left" colspan="1" rowspan="1">Not specified</td>
<td align="left" colspan="1" rowspan="1">IRDA-YOLOv3 (improved YOLOv3-Tiny)</td>
<td align="left" colspan="1" rowspan="1">Reduced parameters: 56.2%, computation: 46.3%, forward computation time: 0.2 ms</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR35" class="usa-link" aria-describedby="CR35">35</a></sup> / 2022</td>
<td align="left" colspan="1" rowspan="1">Multi-modal scene classification &amp; recognition</td>
<td align="left" colspan="1" rowspan="1">Kaggle Indoor Scene dataset</td>
<td align="left" colspan="1" rowspan="1">CNN, Transfer Learning</td>
<td align="left" colspan="1" rowspan="1">Enhanced parameter optimization and accuracy</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par34">These methods are capable of automating feature extraction and achieving higher accuracy while classifying images. Some of the commonly used DL methods are enlisted as follows:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par36">CNNs can classify large datasets with thousands of categories. They use convolutional layers to learn hierarchical features like shapes, edges, or textures for classification. The most popular CNN models are AlexNet, VGG, ResNet, and DenseNet<sup><a href="#CR36" class="usa-link" aria-describedby="CR36">36</a></sup>.</p></li>
<li><p id="Par37">RNNs with CNNs: In sequence-based or video classifications like indoor surveillance videos, RNNs capture temporal relationships while CNNs perform feature extraction<sup><a href="#CR37" class="usa-link" aria-describedby="CR37">37</a></sup>.</p></li>
<li><p id="Par38">Generative Adversarial Networks (GANs): These networks augment training datasets on synthetic data to improve classification accuracy. It is generally used in training models with limited indoor images by generating realistic samples<sup><a href="#CR38" class="usa-link" aria-describedby="CR38">38</a></sup>.</p></li>
<li><p id="Par39">Transfer Learning: Transfer learning leverages pre-trained models such as ResNet or VGG. These models are fine-tuned for indoor image classification tasks with limited datasets. This approach also enhances accuracy by reducing training time and requiring fewer computational resources<sup><a href="#CR39" class="usa-link" aria-describedby="CR39">39</a></sup>.</p></li>
</ul>
<p id="Par40">Though ML methods like SVM, KNN, etc., have laid the basis for image classification, DL methods like CNNs have improved their scalability and accuracy. These aspects help in diverse applications across various fields like healthcare, robotics, security and surveillance, drones, autonomous vehicles, and real-time indoor and outdoor classifications.</p>
<ul class="list" style="list-style-type:none"><li>
<span class="label">c.</span><p class="display-inline" id="Par42">Applications of FL in indoor image classification.</p>
</li></ul>
<p id="Par43">FL can be applied to classify indoor scenes to address challenges in distributed data across multiple devices. It creates and empowers robust and privacy-preserving models, especially in distributed environments<sup><a href="#CR40" class="usa-link" aria-describedby="CR40">40</a></sup>. The indoor images are collected from different devices and classified locally without transferring the data to a centralized server. Some of the major applications of FL in indoor scene classification are in smart homes, indoor robotics, healthcare, retail, and smart public places where there are multifold benefits like preservation of data privacy, scalability, and learning from diverse heterogeneous data<sup><a href="#CR41" class="usa-link" aria-describedby="CR41">41</a></sup>. Another crucial aspect that affects the model performance and training process in FL is Independent and Identically Distributed (IID) and Not Independent and/or Not Identically Distributed (Non-IID) data<sup><a href="#CR42" class="usa-link" aria-describedby="CR42">42</a></sup>.</p>
<p id="Par44">IID refers to the identically distributed independent data across various devices. It has the additional advantages of faster convergence during training and lower model bias risk. Whereas, non-IID refers to the non-uniform data distributed across various clients. Unlike IID, the non-IID exhibits challenges like comparatively slower convergence, biases in the model, and challenges in aggregation.</p>
<p id="Par45">In<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup>, the authors have designed a blockchain-based FL scheme, namely FedBG, to address privacy concerns and insufficient medical image classification data challenges. It incorporates EC-GAN, an enhanced classification-GAN to improve image diversity along with optimizing classification accuracy. Results after experimentation demonstrate that FedBG exhibits a 27–38% reduced training time and 0.9-2% better accuracy along with data privacy preservation while high-quality medical images are generated. An FL-based indoor image recognition positioning system is proposed in<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup> to address privacy concerns in image-based indoor positioning methods. The system exhibits client privacy and is 94% accurate by implementing the MobileNet model with FedAvg and FedOpt algorithms. Whereas, in<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup>, four FL algorithms, namely FedBABU, FedAvg, FedProto, and APPLE in non-IID data in the Fashion MNIST dataset. 76.04% and 74.62% accuracies were obtained from FedAvg and FedBABU algorithms respectively which were surpassed by APPLE and FedProto algorithms attaining 99.21% and 99.40% accuracies. Personalized FL (P-FL) and generalized FL (G-FL) were bridged for image classification and FED-ROD framework was proposed which outperformed the other methods with non-IID data in diverse datasets with superior scalability and balanced accuracy<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>. A FL-based MRI brain tumor classification was proposed leveraging the FedAvg algorithm and the EfficientNet-B0 model to minimize privacy breach issue and enhance accuracy in diagnosis. ResNet-50 was outperformed by EfficientNet-50, with a maximum of 80.17% test accuracy and minimum of 0.612 loss as compared to ResNet-50 which attained 65.32% accuracy and 1.017 loss. The FL model was resilient to heterogeneous data and achieved 99% accuracy<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup>. In the research done<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup>, the transformative impact of FL on medical imaging is highlighted while addressing data privacy concerns. The results demonstrate that FL showed comparable results as centralized models in diagnostic conditions, disease classification, etc. Along with active ongoing optimization improvements, there are several prevalent issues like communication bottlenecks, heterogeneity, and computational costs.</p>
<p id="Par46">The effectiveness of FL in analyzing histopathological images of breast cancer in a secure manner using BreakHis dataset is done in<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup>. The research compares federated, independent, and centralized training models where FL achieves results comparable to centralized learning with negligible differences in F1 scores, accuracy, and precision. DenseNet-201 and ResNet-152 achieved high diagnostic odds ratios and Kappa values and outperformed other models in reliability and consistency, thus validating FL as a solution to address privacy concerns in the collaborative analysis of medical images. In another research work<sup><a href="#CR49" class="usa-link" aria-describedby="CR49">49</a></sup>, the authors introduce the FLBIC-CUAV framework as a combination of blockchain and FL for efficient and secure classification of UAV network images in Industrial IoT (IIoT) environments. The proposed framework leverages FL with ResNet for classification, beetle swarm optimization (BSO) for UAV clustering, and blockchain for secure data transmission. The results showed that the proposed framework produced superior outcomes as compared to existing methods with up to 99.15% accuracy, reduced delay, improved throughput, and lower energy consumption. A concise representation of various FL approaches used to classify indoor images is represented in Table <a href="#Tab3" class="usa-link">3</a>.</p>
<section class="tw xbox font-sm" id="Tab3"><h3 class="obj_head">Table 3.</h3>
<div class="caption p"><p>FL approaches and applications in indoor image classification.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Refs. / Year</th>
<th align="left" colspan="1" rowspan="1">Domain</th>
<th align="left" colspan="1" rowspan="1">Datasets Used</th>
<th align="left" colspan="1" rowspan="1">Techniques/Models</th>
<th align="left" colspan="1" rowspan="1">Results</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR43" class="usa-link" aria-describedby="CR43">43</a></sup> / 2025</td>
<td align="left" colspan="1" rowspan="1">Medical image classification using blockchain-based FL</td>
<td align="left" colspan="1" rowspan="1">Not specified</td>
<td align="left" colspan="1" rowspan="1">FedBG (blockchain-based FL, EC-GAN)</td>
<td align="left" colspan="1" rowspan="1">27–38% reduced training time, 0.9–2% better accuracy, privacy preserved</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR44" class="usa-link" aria-describedby="CR44">44</a></sup> / 2024</td>
<td align="left" colspan="1" rowspan="1">Indoor image-based positioning using FL</td>
<td align="left" colspan="1" rowspan="1">Not specified</td>
<td align="left" colspan="1" rowspan="1">MobileNet, FedAvg, FedOpt</td>
<td align="left" colspan="1" rowspan="1">94% accuracy, client privacy preserved</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR45" class="usa-link" aria-describedby="CR45">45</a></sup> / 2024</td>
<td align="left" colspan="1" rowspan="1">Efficiency of FL algorithms on image classification</td>
<td align="left" colspan="1" rowspan="1">Fashion MNIST</td>
<td align="left" colspan="1" rowspan="1">FedAvg, FedBABU, FedProto, APPLE</td>
<td align="left" colspan="1" rowspan="1">APPLE: 99.21%, FedProto: 99.40% accuracy (non-IID data)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR46" class="usa-link" aria-describedby="CR46">46</a></sup> / 2021</td>
<td align="left" colspan="1" rowspan="1">Bridging P-FL and G-FL for image classification</td>
<td align="left" colspan="1" rowspan="1">Diverse datasets</td>
<td align="left" colspan="1" rowspan="1">FED-ROD framework</td>
<td align="left" colspan="1" rowspan="1">Superior scalability, balanced accuracy with non-IID data</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR47" class="usa-link" aria-describedby="CR47">47</a></sup> / 2023</td>
<td align="left" colspan="1" rowspan="1">FL for medical image analysis</td>
<td align="left" colspan="1" rowspan="1">Not specified</td>
<td align="left" colspan="1" rowspan="1">Deep Neural Networks</td>
<td align="left" colspan="1" rowspan="1">Comparable results to centralized models, privacy preserved</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR48" class="usa-link" aria-describedby="CR48">48</a></sup> / 2022</td>
<td align="left" colspan="1" rowspan="1">FL for breast cancer histopathological image classification</td>
<td align="left" colspan="1" rowspan="1">BreakHis</td>
<td align="left" colspan="1" rowspan="1">DenseNet-201, ResNet-152</td>
<td align="left" colspan="1" rowspan="1">Comparable to centralized models, high reliability and consistency</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">
<sup><a href="#CR49" class="usa-link" aria-describedby="CR49">49</a></sup> / 2022</td>
<td align="left" colspan="1" rowspan="1">FL with blockchain for UAV image classification</td>
<td align="left" colspan="1" rowspan="1">Not specified</td>
<td align="left" colspan="1" rowspan="1">FLBIC-CUAV (FL, blockchain, ResNet, BSO)</td>
<td align="left" colspan="1" rowspan="1">99.15% accuracy, reduced delay, improved throughput, lower energy consumption</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><section id="Sec3"><h3 class="pmc_sec_title">Problem description</h3>
<p id="Par48">The extensive literature survey found that the existing research does not address several aspects in various domains. ML and DL algorithms play a pivotal role in classifying indoor images providing reliable and robust techniques for essential decision-making based on pattern recognition and feature extraction. The roles of ML and DL algorithms in indoor image classification include better feature extraction, enhanced classification accuracy, and integration with multimodal data. Apart from these, DL proves to be a dominant paradigm in this domain, providing further advantages over ML in terms of scalability and generalization, where DL models can scale up to larger datasets and provide better generalization as compared to ML models, which have limited capacity to learn from large datasets<sup><a href="#CR50" class="usa-link" aria-describedby="CR50">50</a></sup>. ML also lags in using contextual information in classification, which is overcome by using DL models, along with handling occlusions, cluttering, and lighting variations<sup><a href="#CR51" class="usa-link" aria-describedby="CR51">51</a></sup>. The pre-trained DL models like ResNet, MobileNet, and VGG can help in reducing the indoor classification time and improving performance.</p>
<p id="Par49">Various smart domains like smart homes, indoor robotic navigation, and augmented reality (AR) require privacy preservation as the foremost context of scene classification. Multiple devices are used in the data collection process, where either the data distribution may be IID or Non-IID<sup><a href="#CR52" class="usa-link" aria-describedby="CR52">52</a></sup>. FL models are effective in these cases where privacy preservation and data integrity are the key concerns. Unlike the centralized data, in FL models, the data is trained in their respective devices and the gradients are shared with the central server<sup><a href="#CR53" class="usa-link" aria-describedby="CR53">53</a></sup>.</p>
<p id="Par50">The key contributions of this work are:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par52">To analyze machine learning and DL algorithms for indoor scene classification.</p></li>
<li><p id="Par53">To design and develop hybrid pre-trained model-based feature extraction for better classification.</p></li>
<li><p id="Par54">To implement the proposed hybrid pre-trained model-based feature extraction technique in an FL environment with identical and non-identical data distribution.</p></li>
</ul></section></section><section id="Sec4"><h2 class="pmc_sec_title">Materials and methods</h2>
<p id="Par55">The various materials and the corresponding method used in this study are explained in this section. The various materials include the dataset, the algorithms used, and the performance metrics, whereas the methodology includes a detailed description of the process flow for the entire study.</p>
<section id="Sec5"><h3 class="pmc_sec_title">Dataset description</h3>
<p id="Par56">The Indoor Scene Recognition Dataset, introduced in the CVPR 2009 paper by MIT, is a standard benchmark dataset designed for the challenging task of indoor scene classification. It is taken from Kaggle<sup><a href="#CR54" class="usa-link" aria-describedby="CR54">54</a></sup> and is widely used for evaluating scene recognition methods in computer vision. Due to the increasing complexity of the indoor scenes, this dataset was created to broaden the scope of existing scene recognition algorithms, helping the researchers to develop DL methods and evaluate them for indoor scene classification. It has also inspired advancements in DL architectures, feature extraction, and object detection.</p>
<p id="Par58">Figure <a href="#Fig1" class="usa-link">1</a> demonstrates the screenshot of the dataset. The noteworthy features of the dataset can be enlisted under several classifications. The dataset contains 15,620 images and 67 categories of indoor scenes. The categories of indoor images include airport with 66 images, art studio with 63 images, auditorium with 4 images, bakery with 58 images, bar with 57 images, bathroom with 197 images, bedroom with 350 images, bookstore with 380 images, bowling 213 images, buffet 111 images, casino 515 images, children room 112 images, church 180 images, classroom 113 images, cloister 120 images, closet 135 images, clothing store 106 images, computer room 114 images, concert hall 103 images, corridor 346 images, deli 258 images, dental office 131 images, dining room 274 images, elevator 101 images, fast food restaurant 116 images, florist 103 images, game room 127 images, garage 103 images, greenhouse 101 images, grocery store 213 images, gym 231 images, hair salon 239 images, hospital room 101 images, inside bus 102 images, inside subway 457 images, jewellery shop 157 images, kindergarten 127 images, kitchen 734 images, laboratory 125 images, laundromat 276 images, library 107 images, living room 706 images, lobby 101 images, locker room 249 images, mall 176 images, meeting room 233 images, movie theatre 175 images, museum168 images, nursery 144 images, office 109 images, operating room 135 images, pantry 384 images, inside pool 174 images, prison cell 103 images, restaurant 513 images, restaurant kitchen 107 images, shoe shop 116 images, staircase 155 images, studio music 108 images, subway 539 images, toystore 347 images, train station 153 images, tv studio 166 images, video store 110 images, waiting room 151 images, warehouse 506 images and wine cellar 269 images. Each category where each image belongs to one image. All the images in this dataset are provided explicitly for research purposes only. Clutter and complex spatial layouts of the images cause high intra-class variability and similarity, making the classification problem more challenging.</p>
<figure class="fig xbox font-sm" id="Fig1"><h4 class="obj_head">Fig. 1.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370940_41598_2025_16673_Fig1_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/489d3ed39379/41598_2025_16673_Fig1_HTML.jpg" loading="lazy" id="d33e962" height="640" width="768" alt="Fig. 1"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig1/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Screenshot of the dataset.</p></figcaption></figure><p id="Par59">The dataset consists of IID and non-IID data from four different clients. Initially, the image data is equally divided among four clients in 67 different classes (IID). In this case, the image distribution is of IID type. The image features are extracted using pre-trained models and the best model is identified. The extracted features are then implemented in the FL environment for collaborative training and model improvement using LDA. The same process is repeated using an uneven and random distribution of images in 67 different classes across all 4 clients.</p>
<p id="Par60">LDA is a statistical method commonly used to separate various classes in ML and DL. It finds a linear combination of features that separates the classes in the best way, and is mainly used for dimensionality reduction and classification applications<sup><a href="#CR55" class="usa-link" aria-describedby="CR55">55</a></sup>. It has two pivot goals:</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par62">To maximize the inter-class separation while minimizing the intra-class variation (high inter-class scatter).</p></li>
<li><p id="Par63">To ensure maximum class-discriminatory information while reducing feature dimensions (low intra-class scatter).</p></li>
</ul>
<p id="Par64">These goals are achieved using the optimization demonstrated in Eq. <a href="#Equ1" class="usa-link">1</a> which maximizes the ratio of inter-class scatter to intra-class scatter:</p>
<table class="disp-formula p" id="Equ1"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/a42b26b8ae7c/d33e992.gif" loading="lazy" id="d33e992" alt="graphic file with name d33e992.gif"></td>
<td class="label">1</td>
</tr></table>
<p id="Par65">where,</p>
<p id="Par66">O’ - optimization objective formula of LDA.</p></section><section id="Sec6"><h3 class="pmc_sec_title">Methodology</h3>
<p id="Par67">The methodology followed to carry out this study is demonstrated in Fig. <a href="#Fig2" class="usa-link">2</a>. Various images are collected in a dataset and an efficient classification is carried out by initially cleaning the data and extracting their features using some pre-trained models. The privacy and integrity of the images are preserved and the aggregates are then implemented in a FL environment on IID and non-IID datasets separately.</p>
<figure class="fig xbox font-sm" id="Fig2"><h4 class="obj_head">Fig. 2.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/3c63a40ee79b/41598_2025_16673_Fig2_HTML.jpg" loading="lazy" id="d33e1017" height="1108" width="711" alt="Fig. 2"></p>
<div class="p text-right font-secondary"><a href="figure/Fig2/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Workflow of the study.</p></figcaption></figure><p id="Par69">It begins with collecting indoor images to form a dataset. Then, the data is preprocessed to prepare the raw data for training and model evaluation by cleaning, and transforming the data. Pretrained DL models like EfficientNet-B3, DenseNet-201, InceptionResNetV2, EfficientNet V2S, VGG16, and MobileNet are implemented to extract features of the preprocessed data. The best DL algorithm is identified based on various evaluation parameters like accuracy, recall, precision, F-1 score, loss, etc. The extracted features form compact representations of the images, reducing their size and preserving privacy, which are implemented in the FL environment. FL is implemented on IID and non-IID datasets separately to evaluate and analyze the resulting outcomes.</p>
<p id="Par71">The process of performance analysis of the models is depicted in Fig. <a href="#Fig3" class="usa-link">3</a>. The feature extraction of the image data is done using various DL models mentioned above. After the feature extraction step, EfficientNetB3 extracted 1536 features, DenseNet201 extracted 1920 features, InceptionResnetV2 extracted 1536 features, EfficientNetV2S extracted 1280 features, VGG16 extracted 4096 features, and MobileNet extracted 1000 features. The corresponding extracted features are applied upon using the LDA approach of image classification. The performance analysis of each of the models is computed and compared with the proposed MultiData model, where it is found to outperform the other models with superior accuracy, precision, recall, and F1-Score and minimal loss.</p>
<figure class="fig xbox font-sm" id="Fig3"><h4 class="obj_head">Fig. 3.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370940_41598_2025_16673_Fig3_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/76f89109fa9e/41598_2025_16673_Fig3_HTML.jpg" loading="lazy" id="d33e1029" height="418" width="668" alt="Fig. 3"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig3/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Performance analysis of various models for indoor image classification.</p></figcaption></figure><p id="Par72">This method is followed across four different clients to attain an average performance of all the clients and get a consolidated performance of the same. The implementation is done across 10 communication rounds comprising 10 epochs each, summing to a total of 100 epochs. The average performance from all four clients is collected by the server and the model performance is analyzed as depicted in Fig. <a href="#Fig4" class="usa-link">4</a>.</p>
<figure class="fig xbox font-sm" id="Fig4"><h4 class="obj_head">Fig. 4.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370940_41598_2025_16673_Fig4_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/760073337096/41598_2025_16673_Fig4_HTML.jpg" loading="lazy" id="d33e1049" height="626" width="778" alt="Fig. 4"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Client-wise training and validation time performance analysis.</p></figcaption></figure><p id="Par74">The classifiers used for each model were evaluated on various performance metrics like accuracy, precision, recall, F1-Score, Zero-One Loss (ZOL), Methews Correlation Coefficient (MCC), and Kappa Statistics.</p>
<p id="Par75">Where,</p>
<ul class="list" style="list-style-type:disc"><li><p id="Par77">Accuracy is the proportion of correct instances that are classified. It can be defined as in Eq. <a href="#Equ2" class="usa-link">2</a>.</p></li></ul>
<table class="disp-formula p" id="Equ2"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/7bec148f8472/d33e1065.gif" loading="lazy" id="d33e1065" alt="graphic file with name d33e1065.gif"></td>
<td class="label">2</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p id="Par79">Validation accuracy measures how well a trained DNN generalizes to unseen data by evaluating performance on a separate validation set. A high validation accuracy indicates good generalization, while a large gap from training accuracy suggests overfitting.</p></li></ul>
<blockquote class="text-italic"><p id="Par80">It can be represented by Eq. <a href="#Equ3" class="usa-link">3</a>.</p></blockquote>
<table class="disp-formula p" id="Equ3"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/d30ec8d23035/d33e1087.gif" loading="lazy" id="d33e1087" alt="graphic file with name d33e1087.gif"></td>
<td class="label">3</td>
</tr></table>
<blockquote class="text-italic">
<p id="Par82">where,</p>
<p id="Par83">N = Total validation samples.</p>
<p id="Par84">y​ = True label for the i<sup>th</sup> sample.</p>
<p id="Par85">z​ = Predicted output for the i<sup>th</sup> sample.</p>
<p id="Par86">L(y, z) = Selected loss function.</p>
</blockquote>
<ul class="list" style="list-style-type:disc"><li><p id="Par88">Precision is the proportion of the true positive predictions among all the positive predictions. It can be calculated according to the Eq. <a href="#Equ4" class="usa-link">4</a>.</p></li></ul>
<table class="disp-formula p" id="Equ4"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/302a1e646db3/d33e1122.gif" loading="lazy" id="d33e1122" alt="graphic file with name d33e1122.gif"></td>
<td class="label">4</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p id="Par90">Recall is the proportion of the true positives among all the actual positives. Equation <a href="#Equ5" class="usa-link">5</a> describes the loss calculation.</p></li></ul>
<table class="disp-formula p" id="Equ5"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/1405c0d19ef8/d33e1139.gif" loading="lazy" id="d33e1139" alt="graphic file with name d33e1139.gif"></td>
<td class="label">5</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p id="Par92">F1-Score is the harmonic mean of precision and recall. F1-Score can be evaluated using the formula in Eq. <a href="#Equ6" class="usa-link">6</a>.</p></li></ul>
<table class="disp-formula p" id="Equ6"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/757f6e079f6c/d33e1156.gif" loading="lazy" id="d33e1156" alt="graphic file with name d33e1156.gif"></td>
<td class="label">6</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p id="Par94">ZOL is the measure of incorrect predictions. The lower the ZOL, the better it is. Equation <a href="#Equ7" class="usa-link">7</a> defines the calculation of ZOL.</p></li></ul>
<table class="disp-formula p" id="Equ7"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/487ac6f442de/d33e1173.gif" loading="lazy" id="d33e1173" alt="graphic file with name d33e1173.gif"></td>
<td class="label">7</td>
</tr></table>
<blockquote class="text-italic">
<p id="Par96">N = Total samples.</p>
<p id="Par97">y = True label of the i<sup>th</sup> sample.</p>
<p id="Par98">z​ = Predicted label of the i<sup>th</sup> sample.</p>
<p>1(y ≠ z) = Indicator function: 1 - prediction is incorrect and</p>
<p id="Par100">0 - prediction is correct.</p>
</blockquote>
<ul class="list" style="list-style-type:disc"><li><p id="Par102">Validation loss is the measure of the performance of a trained DNN on unknown data. The lower the validation loss, the better the generalization. Equation <a href="#Equ8" class="usa-link">8</a> demonstrates the formula for the calculation of validation loss.</p></li></ul>
<table class="disp-formula p" id="Equ8"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/44fb82d4d2fe/d33e1207.gif" loading="lazy" id="d33e1207" alt="graphic file with name d33e1207.gif"></td>
<td class="label">8</td>
</tr></table>
<ul class="list" style="list-style-type:disc"><li><p id="Par104">MCC is a balanced metric that considers false positives, true positives, false negatives, and true negatives. It can be derived using Eq. <a href="#Equ9" class="usa-link">9</a>.</p></li></ul>
<table class="disp-formula p" id="Equ9"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/85c3cca13199/d33e1224.gif" loading="lazy" id="d33e1224" alt="graphic file with name d33e1224.gif"></td>
<td class="label">9</td>
</tr></table>
<blockquote class="text-italic">
<p id="Par106">Where,</p>
<p id="Par107">TP (True Positives) = Correctly predicted positive cases.</p>
<p id="Par108">TN (True Negatives) = Correctly predicted negative cases.</p>
<p id="Par109">FP (False Positives) = Incorrectly predicted positive cases.</p>
<p id="Par110">FN (False Negatives) = Incorrectly predicted negative cases.</p>
</blockquote>
<ul class="list" style="list-style-type:disc"><li><p id="Par112">Kappa Statistics is a measure of correlation between predicted and actual classifications. It is measured using Eq. <a href="#Equ10" class="usa-link">10</a>.</p></li></ul>
<table class="disp-formula p" id="Equ10"><tr>
<td class="formula"><img class="graphic" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/95797f824d5c/d33e1255.gif" loading="lazy" id="d33e1255" alt="graphic file with name d33e1255.gif"></td>
<td class="label">10</td>
</tr></table></section></section><section id="Sec7"><h2 class="pmc_sec_title">Results and discussions</h2>
<p id="Par113">The results of the indoor image classification can be divided into four subsections. Initially, the features of indoor images are extracted, and various DL models are used to check their performances on various parameters, and their performances are compared with the proposed MultiModel architecture. Subsequently, the results of the DNN architecture are compared based on various parameters as compared to the proposed MultiData model. Furthermore, the client-wise training and validation time plots for IID and Non-IID datasets are demonstrated and explained, followed by the client-wise training and validation time plots for NIID datasets. The entire results are combined in a table showing the comparative results.</p>
<section id="Sec8"><h3 class="pmc_sec_title">Performance analysis of the proposed vs. other DL models</h3>
<p id="Par114">The feature extraction of indoor scenes is done using various DL models like VGG16, VGG19, ResNet152, InceptionResNet, MobileNet, DenseNet, and the proposed multi-data model. The models were applied to the extracted features to classify indoor images. The various classifiers used in the experiment were Decision Tree (DT), Logistic Regression (LR), Random Forest (RF), Extra Trees (ET), Histogram-Based Gradient Boosting Technique, and Multi Layered Perceptron Model.</p>
<ul class="list" style="list-style-type:disc">
<li><p id="Par116">VGG16 - a deep convolutional neural network (CNN) architecture that has 16 layers and uniform 3 × 3 convolutional filters. It is effective for image feature extraction and is widely used in transfer learning for various computer vision tasks.</p></li>
<li><p id="Par117">VGG19 - an extended VGG16 architecture with 19 layers. It maintains the 3 × 3 convolutional filter architecture with increased depth for enhanced feature extraction.</p></li>
<li><p id="Par118">ResNet152 - a deep convolutional neural network with 152 layers that utilize the remaining connections to solve the vanishing gradient problem. It is a highly effective architecture for image recognition and transfer learning with a greater depth and optimal efficiency of training.</p></li>
<li><p id="Par119">InceptionResNet - combines the Inception architecture with residual connections to enhance the training accuracy and efficiency in deep networks. It attains enhanced performance in image recognition.</p></li>
<li><p id="Par120">MobileNet - a lightweight CNN designed for efficient device applications. It reduces model sizes and computational complexities.</p></li>
<li><p id="Par121">DenseNet - a deep CNN with every layer inter-connected helping in the reuse of features, better efficiency, and enhanced performance. It mitigates the vanishing gradient problem and also reduces the parameters.</p></li>
</ul>
<p id="Par122">A concise representation of the performance comparison of various models is mentioned in Table <a href="#Tab4" class="usa-link">4</a>. The proposed MultiData model outperforms all the other models in all the performance parameters.</p>
<section class="tw xbox font-sm" id="Tab4"><h4 class="obj_head">Table 4.</h4>
<div class="caption p"><p>Performance comparison of DL models with the proposed model.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Feature Extractor</th>
<th align="left" colspan="1" rowspan="1">Classifier</th>
<th align="left" colspan="1" rowspan="1">Accuracy (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">F1-Score (0–1)</th>
<th align="left" colspan="1" rowspan="1">ZOL<br>(0–1)</th>
<th align="left" colspan="1" rowspan="1">Methews Coefficient</th>
<th align="left" colspan="1" rowspan="1">Kappa Statistics</th>
</tr></thead>
<tbody>
<tr>
<td align="left" rowspan="6" colspan="1">VGG16</td>
<td align="left" colspan="1" rowspan="1">Decision Tree</td>
<td align="center" colspan="1" rowspan="1">52</td>
<td align="center" colspan="1" rowspan="1">53</td>
<td align="center" colspan="1" rowspan="1">52</td>
<td align="left" colspan="1" rowspan="1">0.52</td>
<td align="center" colspan="1" rowspan="1">0.481433</td>
<td align="center" colspan="1" rowspan="1">0.504781</td>
<td align="center" colspan="1" rowspan="1">0.504561</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Logistic Regression</td>
<td align="center" colspan="1" rowspan="1">89</td>
<td align="center" colspan="1" rowspan="1">92</td>
<td align="center" colspan="1" rowspan="1">91</td>
<td align="left" colspan="1" rowspan="1">0.91</td>
<td align="center" colspan="1" rowspan="1">0.107471</td>
<td align="center" colspan="1" rowspan="1">0.889452</td>
<td align="center" colspan="1" rowspan="1">0.8894</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Random Forest</td>
<td align="center" colspan="1" rowspan="1">82</td>
<td align="center" colspan="1" rowspan="1">91</td>
<td align="center" colspan="1" rowspan="1">82</td>
<td align="left" colspan="1" rowspan="1">0.86</td>
<td align="center" colspan="1" rowspan="1">0.175623</td>
<td align="center" colspan="1" rowspan="1">0.819558</td>
<td align="center" colspan="1" rowspan="1">0.818744</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Extra Trees</td>
<td align="center" colspan="1" rowspan="1">82</td>
<td align="center" colspan="1" rowspan="1">92</td>
<td align="center" colspan="1" rowspan="1">81</td>
<td align="left" colspan="1" rowspan="1">0.86</td>
<td align="center" colspan="1" rowspan="1">0.177807</td>
<td align="center" colspan="1" rowspan="1">0.817684</td>
<td align="center" colspan="1" rowspan="1">0.816322</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Hist Gradient Boosting</td>
<td align="center" colspan="1" rowspan="1">86</td>
<td align="center" colspan="1" rowspan="1">91</td>
<td align="center" colspan="1" rowspan="1">86</td>
<td align="left" colspan="1" rowspan="1">0.88</td>
<td align="center" colspan="1" rowspan="1">0.139799</td>
<td align="center" colspan="1" rowspan="1">0.856322</td>
<td align="center" colspan="1" rowspan="1">0.855894</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Multi Layered Perceptron</td>
<td align="center" colspan="1" rowspan="1">63</td>
<td align="center" colspan="1" rowspan="1">63</td>
<td align="center" colspan="1" rowspan="1">60</td>
<td align="left" colspan="1" rowspan="1">0.61</td>
<td align="center" colspan="1" rowspan="1">0.371341</td>
<td align="center" colspan="1" rowspan="1">0.617851</td>
<td align="center" colspan="1" rowspan="1">0.61765</td>
</tr>
<tr>
<td align="left" rowspan="6" colspan="1">VGG19</td>
<td align="left" colspan="1" rowspan="1">Decision Tree</td>
<td align="center" colspan="1" rowspan="1">51</td>
<td align="center" colspan="1" rowspan="1">51</td>
<td align="center" colspan="1" rowspan="1">50</td>
<td align="left" colspan="1" rowspan="1">0.5</td>
<td align="center" colspan="1" rowspan="1">0.492355</td>
<td align="center" colspan="1" rowspan="1">0.493621</td>
<td align="center" colspan="1" rowspan="1">0.493427</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Logistic Regression</td>
<td align="center" colspan="1" rowspan="1">88</td>
<td align="center" colspan="1" rowspan="1">90</td>
<td align="center" colspan="1" rowspan="1">89</td>
<td align="left" colspan="1" rowspan="1">0.89</td>
<td align="center" colspan="1" rowspan="1">0.118829</td>
<td align="center" colspan="1" rowspan="1">0.877794</td>
<td align="center" colspan="1" rowspan="1">0.877713</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Random Forest</td>
<td align="center" colspan="1" rowspan="1">79</td>
<td align="center" colspan="1" rowspan="1">88</td>
<td align="center" colspan="1" rowspan="1">78</td>
<td align="left" colspan="1" rowspan="1">0.82</td>
<td align="center" colspan="1" rowspan="1">0.207951</td>
<td align="center" colspan="1" rowspan="1">0.786128</td>
<td align="center" colspan="1" rowspan="1">0.785316</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Extra Trees</td>
<td align="center" colspan="1" rowspan="1">80</td>
<td align="center" colspan="1" rowspan="1">90</td>
<td align="center" colspan="1" rowspan="1">77</td>
<td align="left" colspan="1" rowspan="1">0.82</td>
<td align="center" colspan="1" rowspan="1">0.203145</td>
<td align="center" colspan="1" rowspan="1">0.791253</td>
<td align="center" colspan="1" rowspan="1">0.790073</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Hist Gradient Boosting</td>
<td align="center" colspan="1" rowspan="1">23</td>
<td align="center" colspan="1" rowspan="1">27</td>
<td align="center" colspan="1" rowspan="1">26</td>
<td align="left" colspan="1" rowspan="1">0.23</td>
<td align="center" colspan="1" rowspan="1">0.773263</td>
<td align="center" colspan="1" rowspan="1">0.210248</td>
<td align="center" colspan="1" rowspan="1">0.207227</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Multi Layered Perceptron</td>
<td align="center" colspan="1" rowspan="1">64</td>
<td align="center" colspan="1" rowspan="1">64</td>
<td align="center" colspan="1" rowspan="1">59</td>
<td align="left" colspan="1" rowspan="1">0.6</td>
<td align="center" colspan="1" rowspan="1">0.36173</td>
<td align="center" colspan="1" rowspan="1">0.627556</td>
<td align="center" colspan="1" rowspan="1">0.627236</td>
</tr>
<tr>
<td align="left" rowspan="6" colspan="1">ResNet 152</td>
<td align="left" colspan="1" rowspan="1">Decision Tree</td>
<td align="center" colspan="1" rowspan="1">28</td>
<td align="center" colspan="1" rowspan="1">26</td>
<td align="center" colspan="1" rowspan="1">26</td>
<td align="left" colspan="1" rowspan="1">0.26</td>
<td align="center" colspan="1" rowspan="1">0.721979</td>
<td align="center" colspan="1" rowspan="1">0.25679</td>
<td align="center" colspan="1" rowspan="1">0.256733</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Logistic Regression</td>
<td align="center" colspan="1" rowspan="1">67</td>
<td align="center" colspan="1" rowspan="1">71</td>
<td align="center" colspan="1" rowspan="1">69</td>
<td align="left" colspan="1" rowspan="1">0.7</td>
<td align="center" colspan="1" rowspan="1">0.327058</td>
<td align="center" colspan="1" rowspan="1">0.663184</td>
<td align="center" colspan="1" rowspan="1">0.66305</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Random Forest</td>
<td align="center" colspan="1" rowspan="1">56</td>
<td align="center" colspan="1" rowspan="1">74</td>
<td align="center" colspan="1" rowspan="1">51</td>
<td align="left" colspan="1" rowspan="1">0.57</td>
<td align="center" colspan="1" rowspan="1">0.440455</td>
<td align="center" colspan="1" rowspan="1">0.544877</td>
<td align="center" colspan="1" rowspan="1">0.542807</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Extra Trees</td>
<td align="center" colspan="1" rowspan="1">55</td>
<td align="center" colspan="1" rowspan="1">75</td>
<td align="center" colspan="1" rowspan="1">48</td>
<td align="left" colspan="1" rowspan="1">0.55</td>
<td align="center" colspan="1" rowspan="1">0.453152</td>
<td align="center" colspan="1" rowspan="1">0.531971</td>
<td align="center" colspan="1" rowspan="1">0.529031</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Hist Gradient Boosting</td>
<td align="center" colspan="1" rowspan="1">61</td>
<td align="center" colspan="1" rowspan="1">75</td>
<td align="center" colspan="1" rowspan="1">58</td>
<td align="left" colspan="1" rowspan="1">0.63</td>
<td align="center" colspan="1" rowspan="1">0.388792</td>
<td align="center" colspan="1" rowspan="1">0.598489</td>
<td align="center" colspan="1" rowspan="1">0.596998</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Multi Layered Perceptron</td>
<td align="center" colspan="1" rowspan="1">42</td>
<td align="center" colspan="1" rowspan="1">41</td>
<td align="center" colspan="1" rowspan="1">36</td>
<td align="left" colspan="1" rowspan="1">0.37</td>
<td align="center" colspan="1" rowspan="1">0.578371</td>
<td align="center" colspan="1" rowspan="1">0.402491</td>
<td align="center" colspan="1" rowspan="1">0.401788</td>
</tr>
<tr>
<td align="left" rowspan="6" colspan="1">InceptionResNet</td>
<td align="left" colspan="1" rowspan="1">Decision Tree</td>
<td align="center" colspan="1" rowspan="1">25</td>
<td align="center" colspan="1" rowspan="1">22</td>
<td align="center" colspan="1" rowspan="1">22</td>
<td align="left" colspan="1" rowspan="1">0.22</td>
<td align="center" colspan="1" rowspan="1">0.74574</td>
<td align="center" colspan="1" rowspan="1">0.233011</td>
<td align="center" colspan="1" rowspan="1">0.232868</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Logistic Regression</td>
<td align="center" colspan="1" rowspan="1">65</td>
<td align="center" colspan="1" rowspan="1">69</td>
<td align="center" colspan="1" rowspan="1">67</td>
<td align="left" colspan="1" rowspan="1">0.68</td>
<td align="center" colspan="1" rowspan="1">0.349061</td>
<td align="center" colspan="1" rowspan="1">0.6406</td>
<td align="center" colspan="1" rowspan="1">0.640503</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Random Forest</td>
<td align="center" colspan="1" rowspan="1">51</td>
<td align="center" colspan="1" rowspan="1">66</td>
<td align="center" colspan="1" rowspan="1">44</td>
<td align="left" colspan="1" rowspan="1">0.49</td>
<td align="center" colspan="1" rowspan="1">0.487112</td>
<td align="center" colspan="1" rowspan="1">0.496484</td>
<td align="center" colspan="1" rowspan="1">0.49406</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Extra Trees</td>
<td align="center" colspan="1" rowspan="1">50</td>
<td align="center" colspan="1" rowspan="1">70</td>
<td align="center" colspan="1" rowspan="1">43</td>
<td align="left" colspan="1" rowspan="1">0.49</td>
<td align="center" colspan="1" rowspan="1">0.503713</td>
<td align="center" colspan="1" rowspan="1">0.479194</td>
<td align="center" colspan="1" rowspan="1">0.475773</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Hist Gradient Boosting</td>
<td align="center" colspan="1" rowspan="1">50</td>
<td align="center" colspan="1" rowspan="1">40</td>
<td align="center" colspan="1" rowspan="1">20</td>
<td align="left" colspan="1" rowspan="1">0.01</td>
<td align="center" colspan="1" rowspan="1">0.95107</td>
<td align="center" colspan="1" rowspan="1">0.023288</td>
<td align="center" colspan="1" rowspan="1">0.008262</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Multi Layered Perceptron</td>
<td align="center" colspan="1" rowspan="1">40</td>
<td align="center" colspan="1" rowspan="1">36</td>
<td align="center" colspan="1" rowspan="1">32</td>
<td align="left" colspan="1" rowspan="1">0.32</td>
<td align="center" colspan="1" rowspan="1">0.60332</td>
<td align="center" colspan="1" rowspan="1">0.376451</td>
<td align="center" colspan="1" rowspan="1">0.375507</td>
</tr>
<tr>
<td align="left" rowspan="6" colspan="1">Mobile Net</td>
<td align="left" colspan="1" rowspan="1">Decision Tree</td>
<td align="center" colspan="1" rowspan="1">21</td>
<td align="center" colspan="1" rowspan="1">19</td>
<td align="center" colspan="1" rowspan="1">18</td>
<td align="left" colspan="1" rowspan="1">0.18</td>
<td align="center" colspan="1" rowspan="1">0.790738</td>
<td align="center" colspan="1" rowspan="1">0.187306</td>
<td align="center" colspan="1" rowspan="1">0.187208</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Logistic Regression</td>
<td align="center" colspan="1" rowspan="1">56</td>
<td align="center" colspan="1" rowspan="1">58</td>
<td align="center" colspan="1" rowspan="1">57</td>
<td align="left" colspan="1" rowspan="1">0.57</td>
<td align="center" colspan="1" rowspan="1">0.442551</td>
<td align="center" colspan="1" rowspan="1">0.544893</td>
<td align="center" colspan="1" rowspan="1">0.544743</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Random Forest</td>
<td align="center" colspan="1" rowspan="1">44</td>
<td align="center" colspan="1" rowspan="1">59</td>
<td align="center" colspan="1" rowspan="1">37</td>
<td align="left" colspan="1" rowspan="1">0.41</td>
<td align="center" colspan="1" rowspan="1">0.563565</td>
<td align="center" colspan="1" rowspan="1">0.41797</td>
<td align="center" colspan="1" rowspan="1">0.415247</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Extra Trees</td>
<td align="center" colspan="1" rowspan="1">42</td>
<td align="center" colspan="1" rowspan="1">61</td>
<td align="center" colspan="1" rowspan="1">36</td>
<td align="left" colspan="1" rowspan="1">0.4</td>
<td align="center" colspan="1" rowspan="1">0.576234</td>
<td align="center" colspan="1" rowspan="1">0.404824</td>
<td align="center" colspan="1" rowspan="1">0.401376</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Hist Gradient Boosting</td>
<td align="center" colspan="1" rowspan="1">48</td>
<td align="center" colspan="1" rowspan="1">63</td>
<td align="center" colspan="1" rowspan="1">44</td>
<td align="left" colspan="1" rowspan="1">0.49</td>
<td align="center" colspan="1" rowspan="1">0.515946</td>
<td align="center" colspan="1" rowspan="1">0.467625</td>
<td align="center" colspan="1" rowspan="1">0.465104</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Multi Layered Perceptron</td>
<td align="center" colspan="1" rowspan="1">35</td>
<td align="center" colspan="1" rowspan="1">33</td>
<td align="center" colspan="1" rowspan="1">28</td>
<td align="left" colspan="1" rowspan="1">0.29</td>
<td align="center" colspan="1" rowspan="1">0.649192</td>
<td align="center" colspan="1" rowspan="1">0.329995</td>
<td align="center" colspan="1" rowspan="1">0.329104</td>
</tr>
<tr>
<td align="left" rowspan="6" colspan="1">Dense Net</td>
<td align="left" colspan="1" rowspan="1">Decision Tree</td>
<td align="center" colspan="1" rowspan="1">34</td>
<td align="center" colspan="1" rowspan="1">32</td>
<td align="center" colspan="1" rowspan="1">31</td>
<td align="left" colspan="1" rowspan="1">0.31</td>
<td align="center" colspan="1" rowspan="1">0.656743</td>
<td align="center" colspan="1" rowspan="1">0.324706</td>
<td align="center" colspan="1" rowspan="1">0.324571</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Logistic Regression</td>
<td align="center" colspan="1" rowspan="1">77</td>
<td align="center" colspan="1" rowspan="1">80</td>
<td align="center" colspan="1" rowspan="1">78</td>
<td align="left" colspan="1" rowspan="1">0.79</td>
<td align="center" colspan="1" rowspan="1">0.231173</td>
<td align="center" colspan="1" rowspan="1">0.762368</td>
<td align="center" colspan="1" rowspan="1">0.762249</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Random Forest</td>
<td align="center" colspan="1" rowspan="1">64</td>
<td align="center" colspan="1" rowspan="1">79</td>
<td align="center" colspan="1" rowspan="1">59</td>
<td align="left" colspan="1" rowspan="1">0.64</td>
<td align="center" colspan="1" rowspan="1">0.359457</td>
<td align="center" colspan="1" rowspan="1">0.630222</td>
<td align="center" colspan="1" rowspan="1">0.628265</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Extra Trees</td>
<td align="center" colspan="1" rowspan="1">63</td>
<td align="center" colspan="1" rowspan="1">80</td>
<td align="center" colspan="1" rowspan="1">58</td>
<td align="left" colspan="1" rowspan="1">0.64</td>
<td align="center" colspan="1" rowspan="1">0.366025</td>
<td align="center" colspan="1" rowspan="1">0.62359</td>
<td align="center" colspan="1" rowspan="1">0.621119</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Hist Gradient Boosting</td>
<td align="center" colspan="1" rowspan="1">90</td>
<td align="center" colspan="1" rowspan="1">16</td>
<td align="center" colspan="1" rowspan="1">60</td>
<td align="left" colspan="1" rowspan="1">0.06</td>
<td align="center" colspan="1" rowspan="1">0.911559</td>
<td align="center" colspan="1" rowspan="1">0.068907</td>
<td align="center" colspan="1" rowspan="1">0.050004</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Multi Layered Perceptron</td>
<td align="center" colspan="1" rowspan="1">49</td>
<td align="center" colspan="1" rowspan="1">45</td>
<td align="center" colspan="1" rowspan="1">43</td>
<td align="left" colspan="1" rowspan="1">0.43</td>
<td align="center" colspan="1" rowspan="1">0.507881</td>
<td align="center" colspan="1" rowspan="1">0.476921</td>
<td align="center" colspan="1" rowspan="1">0.476427</td>
</tr>
<tr>
<td align="left" rowspan="6" colspan="1">Multi Data</td>
<td align="left" colspan="1" rowspan="1">Decision Tree</td>
<td align="center" colspan="1" rowspan="1">99</td>
<td align="center" colspan="1" rowspan="1">99</td>
<td align="center" colspan="1" rowspan="1">98</td>
<td align="left" colspan="1" rowspan="1">0.99</td>
<td align="center" colspan="1" rowspan="1">0.009632</td>
<td align="center" colspan="1" rowspan="1">0.99009</td>
<td align="center" colspan="1" rowspan="1">0.990083</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Logistic Regression</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="center" colspan="1" rowspan="1">0.001313</td>
<td align="center" colspan="1" rowspan="1">0.998648</td>
<td align="center" colspan="1" rowspan="1">0.998648</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Random Forest</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="center" colspan="1" rowspan="1">0.000876</td>
<td align="center" colspan="1" rowspan="1">0.999099</td>
<td align="center" colspan="1" rowspan="1">0.999099</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Extra Trees</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="center" colspan="1" rowspan="1">0.000438</td>
<td align="center" colspan="1" rowspan="1">0.999549</td>
<td align="center" colspan="1" rowspan="1">0.999549</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Hist Gradient Boosting</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="center" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="center" colspan="1" rowspan="1">0.003065</td>
<td align="center" colspan="1" rowspan="1">0.996846</td>
<td align="center" colspan="1" rowspan="1">0.996845</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Multi-Layered Perceptron</td>
<td align="center" colspan="1" rowspan="1">98</td>
<td align="center" colspan="1" rowspan="1">97</td>
<td align="center" colspan="1" rowspan="1">97</td>
<td align="left" colspan="1" rowspan="1">0.97</td>
<td align="center" colspan="1" rowspan="1">0.0162</td>
<td align="center" colspan="1" rowspan="1">0.983429</td>
<td align="center" colspan="1" rowspan="1">0.983323</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab4/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par124">From the analysis of the tabular results, it is observed that the performance of the VGG16 and VGG19 feature extractors was good, with an accuracy of more than 85%. ResNet152 and InceptionResNet showed lower overall performance with some classifiers, attaining around 60% accuracy. MobileNet and DenseNet depicted moderate performance, with LR and RF attaining better results. The proposed MultiData performs exceptionally better across classifiers, attaining an accuracy of close to 100%.</p>
<p id="Par125">An analysis of the best classifier for each feature extractor shows that the LR, RF, ET, and Histogram-Based Gradient Boosting models perform consistently well with all the feature extractors. Whereas the DT and Multi-Layer Perceptron (MLP) are less effective with a lower accuracy. Overall, exceptionally high accuracy is obtained from the MultiData which achieves near-perfect accuracy across all classifiers.</p></section><section id="Sec9"><h3 class="pmc_sec_title">Performance analysis of the proposed vs. other DNN models</h3>
<p id="Par126">After comparing the MultiData model with various DL models, various DNN models were compared with the proposed MultiData model concerning various performance metrics like accuracy, validation accuracy, loss, and validation loss. Various DL models used for performance comparison with the proposed MultiData model are DenseNet201, InceptionResNetV2, MobileNetV2, ResNet152V2, VGG16, VGG19, and MultiData, and their performances were calculated on various metrics like accuracy, validation accuracy, loss, and validation loss. Performance comparison of the proposed model with various DNN models is represented in Table <a href="#Tab5" class="usa-link">5</a>.</p>
<section class="tw xbox font-sm" id="Tab5"><h4 class="obj_head">Table 5.</h4>
<div class="caption p"><p>Performance comparison of DNN models vs. the proposed model.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Models</th>
<th align="left" colspan="1" rowspan="1">Accuracy<br>(%)</th>
<th align="left" colspan="1" rowspan="1">Validation Accuracy (%)</th>
<th align="left" colspan="1" rowspan="1">Loss<br>(0–1)</th>
<th align="left" colspan="1" rowspan="1">Validation Loss<br>(0–1)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">DenseNet201</td>
<td align="center" colspan="1" rowspan="1">92.14</td>
<td align="center" colspan="1" rowspan="1">72.92</td>
<td align="left" colspan="1" rowspan="1">0.23</td>
<td align="center" colspan="1" rowspan="1">1.54</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">InceptionResNetV2</td>
<td align="center" colspan="1" rowspan="1">83.9</td>
<td align="center" colspan="1" rowspan="1">59.36</td>
<td align="left" colspan="1" rowspan="1">0.52</td>
<td align="center" colspan="1" rowspan="1">2.1</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MobileNetV2</td>
<td align="center" colspan="1" rowspan="1">74.08</td>
<td align="center" colspan="1" rowspan="1">51.93</td>
<td align="left" colspan="1" rowspan="1">0.83</td>
<td align="center" colspan="1" rowspan="1">2.22</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">ResNet152V2</td>
<td align="center" colspan="1" rowspan="1">85.65</td>
<td align="center" colspan="1" rowspan="1">60.15</td>
<td align="left" colspan="1" rowspan="1">0.48</td>
<td align="center" colspan="1" rowspan="1">2.07</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VGG16</td>
<td align="center" colspan="1" rowspan="1">98.79</td>
<td align="center" colspan="1" rowspan="1">87.47</td>
<td align="left" colspan="1" rowspan="1">0.04</td>
<td align="center" colspan="1" rowspan="1">0.8</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">VGG19</td>
<td align="center" colspan="1" rowspan="1">98.37</td>
<td align="center" colspan="1" rowspan="1">85.21</td>
<td align="left" colspan="1" rowspan="1">0.05</td>
<td align="center" colspan="1" rowspan="1">1.08</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">MultiData</td>
<td align="center" colspan="1" rowspan="1">99.99</td>
<td align="center" colspan="1" rowspan="1">99.93</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="center" colspan="1" rowspan="1">0.01</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par128">The inference drawn from the table can be explained in terms of various observations on each of the models. The VGG16 and VGG19 attained the maximum validation accuracy of 87.47% and 85.21% respectively. They also exhibit strong generalization due to a minimized validation loss. DenseNet201 and ResNet152V2 demonstrate a moderate validation accuracy of 72.92% and 60.15% respectively but show a relatively high validation loss. The InceptionResNetV2 and MobileNetV2 perform the least, attaining a validation accuracy below 60% and moderately high validation loss values. Conversely, the MultiData model achieves the best accuracy and validation accuracy of 99.99% and 99.93% respectively, with a validation loss close to zero. The performances of various models obtained are plotted in graphs for a clearer insight as in Fig. <a href="#Fig5" class="usa-link">5</a>.</p>
<figure class="fig xbox font-sm" id="Fig5"><h4 class="obj_head">Fig. 5.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370940_41598_2025_16673_Fig5_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/69dfdccdf99e/41598_2025_16673_Fig5_HTML.jpg" loading="lazy" id="d33e2199" height="655" width="768" alt="Fig. 5"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig5/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The performances of different DL models on varying Epochs.</p></figcaption></figure><p id="Par130">Figure <a href="#Fig6" class="usa-link">6</a> demonstrates the performances of DenseNet201, InceptionResNetV2, MobileNetV2, ResNet152V2, VGG16, VGG19, and the proposed MultiData model in training time. The MultiData is found to perform the best among all others with higher accuracy, precision, and recall. The reduced loss in the MultiData also adds to the superior performance of the proposed model. Figure <a href="#Fig4" class="usa-link">4</a> is a graphical depiction of the validation results of the models as compared to the proposed MultiData model.</p>
<figure class="fig xbox font-sm" id="Fig6"><h4 class="obj_head">Fig. 6.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370940_41598_2025_16673_Fig6_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/213a6f0157cf/41598_2025_16673_Fig6_HTML.jpg" loading="lazy" id="d33e2217" height="731" width="761" alt="Fig. 6"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>The validation performances of different DL models on varying Epochs.</p></figcaption></figure><p id="Par132">After training, the performance comparison of various models is done in the validation time. The proposed model’s accuracy, precision, and recall are close to 100% in validation and other performances. The loss and validation loss are almost negligible. Thus, the proposed model is the most efficient among all the DL models.</p></section><section id="Sec10"><h3 class="pmc_sec_title">Federated learning based proposed ecosystem for IID dataset</h3>
<p id="Par133">After the exceptionally better performance of the proposed model, while comparing it with various DL and DNN models, the model performance was then compared with four clients in a federated learning ecosystem. The performance was first compared with an IID dataset and then with a non-IID dataset.</p>
<p id="Par134">For IID datasets, the accuracy, precision, and recall during the training time are approximately 100% in the case of all four clients. On the other hand, the loss functions of the clients are found to be 0%. All the training time performance parameters of the clients are demonstrated in Fig. <a href="#Fig7" class="usa-link">7</a>.</p>
<figure class="fig xbox font-sm" id="Fig7"><h4 class="obj_head">Fig. 7.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370940_41598_2025_16673_Fig7_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/310f61e5eea6/41598_2025_16673_Fig7_HTML.jpg" loading="lazy" id="d33e2238" height="595" width="709" alt="Fig. 7"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig7/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Client-wise training time plot.</p></figcaption></figure><p id="Par136">The blue bar represents the accuracy, the green bar represents the precision, the yellow line represents the recall, and the red line represents the loss function of the various clients. During the validation, the clients’ performances are evaluated based on the same parameters, i.e. accuracy, precision, recall, and loss.</p>
<p id="Par138">Figure <a href="#Fig8" class="usa-link">8</a> demonstrates the client-wise validation plots for IID datasets. After a certain number of epochs, all four clients attain a state of convergence in accuracy, precision, and recall. Though the parameters vary differently in each client, the final average of all four clients after 100 epochs shows very high accuracy, precision, and recall. In this case, 10 epochs comprise one communication round, and 10 communication rounds are used, summing up to 100 epochs. This way, the federated learning based proposed ecosystem performs exceptionally well in case of IID datasets.</p>
<figure class="fig xbox font-sm" id="Fig8"><h4 class="obj_head">Fig. 8.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370940_41598_2025_16673_Fig8_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/a6763b0eddb0/41598_2025_16673_Fig8_HTML.jpg" loading="lazy" id="d33e2250" height="549" width="752" alt="Fig. 8"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig8/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Client wise validation plots.</p></figcaption></figure></section><section id="Sec11"><h3 class="pmc_sec_title">Federated learning based proposed ecosystem for non-IID dataset</h3>
<p id="Par139">In case of non-IID datasets, the client-wise training time plots are generated based on the same evaluation parameters. Where, the blue line represents the accuracy, the green line represents the precision, the yellow line represents the recall, and the red line represents the loss percentages respectively.</p>
<p id="Par140">While training, all four clients attain exceptionally high accuracy, precision, and recall convergences after 10 communication rounds comprising 10 epochs each. Among all the parameters, the loss function varies the most, but despite that, the average loss of all the four clients is close to zero as shown in Fig. <a href="#Fig9" class="usa-link">9</a>.</p>
<figure class="fig xbox font-sm" id="Fig9"><h4 class="obj_head">Fig. 9.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370940_41598_2025_16673_Fig9_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/022b895cda92/41598_2025_16673_Fig9_HTML.jpg" loading="lazy" id="d33e2274" height="422" width="764" alt="Fig. 9"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig9/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Client wise training time plot.</p></figcaption></figure><p id="Par142">The performance evaluation of the clients while the validation process is evaluated and plotted in Fig. <a href="#Fig10" class="usa-link">10</a>. All the clients perform in unison, depicting the convergence in various performance parameters, like accuracy, precision, and recall.</p>
<figure class="fig xbox font-sm" id="Fig10"><h4 class="obj_head">Fig. 10.</h4>
<p class="img-box line-height-none margin-x-neg-2 tablet:margin-x-0 text-center"><a class="tileshop" target="_blank" href="https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=12370940_41598_2025_16673_Fig10_HTML.jpg"><img class="graphic zoom-in" src="https://cdn.ncbi.nlm.nih.gov/pmc/blobs/3085/12370940/bcacd6243f0c/41598_2025_16673_Fig10_HTML.jpg" loading="lazy" id="d33e2289" height="480" width="776" alt="Fig. 10"></a></p>
<div class="p text-right font-secondary"><a href="figure/Fig10/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div>
<figcaption><p>Client wise validation plots.</p></figcaption></figure></section><section id="Sec12"><h3 class="pmc_sec_title">Discussions</h3>
<p id="Par144">The final results obtained after taking the mean of all the performances, and consolidating all the performance of the proposed model, are depicted in Table <a href="#Tab6" class="usa-link">6</a>.</p>
<section class="tw xbox font-sm" id="Tab6"><h4 class="obj_head">Table 6.</h4>
<div class="caption p"><p>Performance analysis of the proposed model.</p></div>
<div class="tbl-box p" tabindex="0"><table class="content" frame="hsides" rules="groups">
<thead><tr>
<th align="left" colspan="1" rowspan="1">Phase</th>
<th align="left" colspan="1" rowspan="1">Data</th>
<th align="left" colspan="1" rowspan="1">Accuracy (%)</th>
<th align="left" colspan="1" rowspan="1">Precision (%)</th>
<th align="left" colspan="1" rowspan="1">Recall (%)</th>
<th align="left" colspan="1" rowspan="1">Loss (0–1)</th>
</tr></thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Training</td>
<td align="left" colspan="1" rowspan="1">IID</td>
<td align="left" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">0</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Training</td>
<td align="left" colspan="1" rowspan="1">Non-IID</td>
<td align="left" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">100</td>
<td align="left" colspan="1" rowspan="1">0</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Validation</td>
<td align="left" colspan="1" rowspan="1">IID</td>
<td align="left" colspan="1" rowspan="1">98.86</td>
<td align="left" colspan="1" rowspan="1">98.86</td>
<td align="left" colspan="1" rowspan="1">96.59</td>
<td align="left" colspan="1" rowspan="1">0.1</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Validation</td>
<td align="left" colspan="1" rowspan="1">Non-IID</td>
<td align="left" colspan="1" rowspan="1">95.01</td>
<td align="left" colspan="1" rowspan="1">97.99</td>
<td align="left" colspan="1" rowspan="1">95.01</td>
<td align="left" colspan="1" rowspan="1">0.18</td>
</tr>
</tbody>
</table></div>
<div class="p text-right font-secondary"><a href="table/Tab6/" class="usa-link" target="_blank" rel="noopener noreferrer">Open in a new tab</a></div></section><p id="Par146">The results demonstrate that the proposed Multidata model attains near perfect accuracy, precision, and recall in training and validation phases. The performances of all the four clients are excellent in cases of IID as well as non-IID datasets, which shows the superior ability of the model in classifying indoor images in any scenario.</p></section></section><section id="Sec13"><h2 class="pmc_sec_title">Conclusion and future research directions</h2>
<p id="Par147">The necessity to classify indoor images is important in many aspects, and several research has been done to identify the best method of doing it. This study proposes and implements a MultiModel architecture, which is tested and compared with various DL models, DNN models, as well as FE environment. The diversity of the experiment also includes testing its performance on IID as well as non-IID datasets. After the implementation, it has been observed that the proposed MultiModel data performs the best when compared with various DL models. It also outperforms various DNN models in terms of all the performance metrics.</p>
<p id="Par148">Not only in DL and DNN models, the proposed model is also checked for its performance in a federated learning environment. The client-wise performance of the proposed model is checked in the training as well as the validation phases, whose average shows the best result of the proposed model in case of IID as well as non-IID datasets. In all the cases of comparison, the proposed MultiData model achieves an accuracy, precision, and recall close to 100%, with negligible loss.</p>
<p id="Par149">This work is a computation-based experiment where the performance is calculated and analyzed through computational methods and not in real time. Analyzing the real-time performance of the model can be performed as the future prospect of this research work. This model can be deployed on some low memory devices to check for its real-time performance.</p></section><section id="ack1" class="ack"><h2 class="pmc_sec_title">Acknowledgements</h2>
<p>The authors extend their appreciation to Taif University, Saudi Arabia, for supporting this work through project number (TU-DSPP-2024-17).</p></section><section id="notes1"><h2 class="pmc_sec_title">Author contributions</h2>
<p>M.D. and D.G. wrote the initial draft, V.K. performed the supervision, S.J. and R.A. wrote the final draft, P.J. wrote the methodology and provided the software. All authors reviewed the manuscript.</p></section><section id="notes2"><h2 class="pmc_sec_title">Funding</h2>
<p>Open access funding provided by Parul University.</p></section><section id="notes3"><h2 class="pmc_sec_title">Data availability</h2>
<p>The dataset is available at [https://www.kaggle.com/datasets/itsahmad/indoor-scenes-cvpr-2019].</p></section><section id="notes4"><h2 class="pmc_sec_title">Declarations</h2>
<section id="FPar1"><h3 class="pmc_sec_title">Competing interests</h3>
<p id="Par150">The authors declare no competing interests.</p></section></section><section id="fn-group1" class="fn-group"><h2 class="pmc_sec_title">Footnotes</h2>
<div class="fn-group p font-secondary-light font-sm"><div class="fn p" id="fn1">
<p><strong>Publisher’s note</strong></p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</div></div></section><section id="Bib1" class="ref-list"><h2 class="pmc_sec_title">References</h2>
<section id="Bib1_sec2"><ul class="ref-list font-sm" style="list-style-type:none">
<li id="CR1">
<span class="label">1.</span><cite>Khan, S. D. &amp; Othman, K. M. Indoor scene classification through dual-stream deep learning: a framework for improved scene Understanding in robotics. <em>Computers</em><strong>13</strong> (5), 121 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Khan,%20S.%20D.%20&amp;%20Othman,%20K.%20M.%20Indoor%20scene%20classification%20through%20dual-stream%20deep%20learning:%20a%20framework%20for%20improved%20scene%20Understanding%20in%20robotics.%20Computers13%20(5),%20121%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR2">
<span class="label">2.</span><cite>Soroush, R. &amp; Baleghi, Y. NIR/RGB image fusion for scene classification using deep neural networks. <em>Visual Comput.</em><strong>39</strong> (7), 2725–2739 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Soroush,%20R.%20&amp;%20Baleghi,%20Y.%20NIR/RGB%20image%20fusion%20for%20scene%20classification%20using%20deep%20neural%20networks.%20Visual%20Comput.39%20(7),%202725%E2%80%932739%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR3">
<span class="label">3.</span><cite>Thepade, S. D. &amp; Idhate, M. E. Machine Learning-Based Scene Classification Using Thepade’s SBTC, LBP, and GLCM. in Futuristic Trends in Networks and Computing Technologies: Select Proceedings of Fourth International Conference on FTNCT 2021. Springer. (2022).</cite>
</li>
<li id="CR4">
<span class="label">4.</span><cite>Lei, Y. et al. Research on indoor robot navigation algorithm based on deep reinforcement learning. in 2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML). IEEE. (2023).</cite>
</li>
<li id="CR5">
<span class="label">5.</span><cite>Ahmed, M. W. &amp; Jalal, A. Indoor Scene Classification Using RGB-D Data: A Vision Transformer and Conditional Random Field Approach.</cite>
</li>
<li id="CR6">
<span class="label">6.</span><cite>Pereira, R. et al. A deep learning-based global and segmentation-based semantic feature fusion approach for indoor scene classification. <em>Pattern Recognit. Lett.</em><strong>179</strong>, 24–30 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Pereira,%20R.%20et%20al.%20A%20deep%20learning-based%20global%20and%20segmentation-based%20semantic%20feature%20fusion%20approach%20for%20indoor%20scene%20classification.%20Pattern%20Recognit.%20Lett.179,%2024%E2%80%9330%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR7">
<span class="label">7.</span><cite>Tran, H. N. et al. Enhancing semantic scene segmentation for indoor autonomous systems using advanced attention-supported improved UNet. <em>Signal. Image Video Process.</em><strong>19</strong> (2), 190 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Tran,%20H.%20N.%20et%20al.%20Enhancing%20semantic%20scene%20segmentation%20for%20indoor%20autonomous%20systems%20using%20advanced%20attention-supported%20improved%20UNet.%20Signal.%20Image%20Video%20Process.19%20(2),%20190%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR8">
<span class="label">8.</span><cite>Yue, H. et al. <em>Recognition of Indoor Scenes Using 3D Scene Graphs</em> (IEEE Transactions on Geoscience and Remote Sensing, 2024).</cite>
</li>
<li id="CR9">
<span class="label">9.</span><cite>Govea, J., Gaibor-Naranjo, W. &amp; Villegas-Ch, W. Securing critical infrastructure with blockchain technology: an approach to Cyber-Resilience. <em>Computers</em><strong>13</strong> (5), 122 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Govea,%20J.,%20Gaibor-Naranjo,%20W.%20&amp;%20Villegas-Ch,%20W.%20Securing%20critical%20infrastructure%20with%20blockchain%20technology:%20an%20approach%20to%20Cyber-Resilience.%20Computers13%20(5),%20122%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR10">
<span class="label">10.</span><cite>Wen, J. et al. A survey on federated learning: challenges and applications. <em>Int. J. Mach. Learn. Cybernet.</em><strong>14</strong> (2), 513–535 (2023).</cite> [<a href="https://doi.org/10.1007/s13042-022-01647-y" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9650178/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/36407495/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Wen,%20J.%20et%20al.%20A%20survey%20on%20federated%20learning:%20challenges%20and%20applications.%20Int.%20J.%20Mach.%20Learn.%20Cybernet.14%20(2),%20513%E2%80%93535%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR11">
<span class="label">11.</span><cite>Banabilah, S. et al. Federated learning review: fundamentals, enabling technologies, and future applications. <em>Inf. Process. Manag.</em><strong>59</strong> (6), 103061 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Banabilah,%20S.%20et%20al.%20Federated%20learning%20review:%20fundamentals,%20enabling%20technologies,%20and%20future%20applications.%20Inf.%20Process.%20Manag.59%20(6),%20103061%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR12">
<span class="label">12.</span><cite>Blanco-Justicia, A. et al. Achieving security and privacy in federated learning systems: survey, research challenges and future directions. <em>Eng. Appl. Artif. Intell.</em><strong>106</strong>, 104468 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Blanco-Justicia,%20A.%20et%20al.%20Achieving%20security%20and%20privacy%20in%20federated%20learning%20systems:%20survey,%20research%20challenges%20and%20future%20directions.%20Eng.%20Appl.%20Artif.%20Intell.106,%20104468%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR13">
<span class="label">13.</span><cite>Chen, R. et al. Service delay minimization for federated learning over mobile devices. <em>IEEE J. Sel. Areas Commun.</em><strong>41</strong> (4), 990–1006 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20R.%20et%20al.%20Service%20delay%20minimization%20for%20federated%20learning%20over%20mobile%20devices.%20IEEE%20J.%20Sel.%20Areas%20Commun.41%20(4),%20990%E2%80%931006%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR14">
<span class="label">14.</span><cite>Bharati, S. et al. Federated learning: applications, challenges and future directions. <em>Int. J. Hybrid. Intell. Syst.</em><strong>18</strong> (1–2), 19–35 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Bharati,%20S.%20et%20al.%20Federated%20learning:%20applications,%20challenges%20and%20future%20directions.%20Int.%20J.%20Hybrid.%20Intell.%20Syst.18%20(1%E2%80%932),%2019%E2%80%9335%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR15">
<span class="label">15.</span><cite>Song, C. et al. Semantic-embedded similarity prototype for scene recognition. <em>Pattern Recogn.</em><strong>155</strong>, 110725 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Song,%20C.%20et%20al.%20Semantic-embedded%20similarity%20prototype%20for%20scene%20recognition.%20Pattern%20Recogn.155,%20110725%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR16">
<span class="label">16.</span><cite>Alazeb, A. et al. Remote intelligent perception system for multi-object detection. <em>Front. Neurorobotics.</em><strong>18</strong>, 1398703 (2024).</cite> [<a href="https://doi.org/10.3389/fnbot.2024.1398703" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11144911/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38831877/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Alazeb,%20A.%20et%20al.%20Remote%20intelligent%20perception%20system%20for%20multi-object%20detection.%20Front.%20Neurorobotics.18,%201398703%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR17">
<span class="label">17.</span><cite>Qiao, D. et al. AMFL: Resource-efficient adaptive metaverse-based federated learning for the human-centric augmented reality applications. <em>IEEE Trans. Neural Networks Learn. Syst.</em><strong>36</strong>(5), 7888–7902 (2024).</cite> [<a href="https://doi.org/10.1109/TNNLS.2024.3409446" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38896514/" class="usa-link">PubMed</a>]</li>
<li id="CR18">
<span class="label">18.</span><cite>Fadhel, M. A. et al. Comprehensive systematic review of information fusion methods in smart cities and urban environments. <em>Inform. Fusion</em>. <strong>102317</strong>, p (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Fadhel,%20M.%20A.%20et%20al.%20Comprehensive%20systematic%20review%20of%20information%20fusion%20methods%20in%20smart%20cities%20and%20urban%20environments.%20Inform.%20Fusion.%20102317,%20p%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR19">
<span class="label">19.</span><cite>Chang, Y. et al. SAR image matching based on rotation-invariant description. <em>Sci. Rep.</em><strong>13</strong> (1), 14510 (2023).
</cite> [<a href="https://doi.org/10.1038/s41598-023-41592-6" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10477315/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37666967/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Chang,%20Y.%20et%20al.%20SAR%20image%20matching%20based%20on%20rotation-invariant%20description.%20Sci.%20Rep.13%20(1),%2014510%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR20">
<span class="label">20.</span><cite>Li, P. Machine Learning Techniques for Pattern Recognition in High-Dimensional Data Mining. arXiv preprint arXiv:2412.15593, (2024).</cite>
</li>
<li id="CR21">
<span class="label">21.</span><cite>Liu, T., Zheng, P. &amp; Bao, J. Deep learning-based welding image recognition: A comprehensive review. <em>J. Manuf. Syst.</em><strong>68</strong>, 601–625 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20T.,%20Zheng,%20P.%20&amp;%20Bao,%20J.%20Deep%20learning-based%20welding%20image%20recognition:%20A%20comprehensive%20review.%20J.%20Manuf.%20Syst.68,%20601%E2%80%93625%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR22">
<span class="label">22.</span><cite>Lee, J. &amp; Kim, M. Rare data image classification system using Few-Shot learning. <em>Electronics</em><strong>13</strong> (19), 3923 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Lee,%20J.%20&amp;%20Kim,%20M.%20Rare%20data%20image%20classification%20system%20using%20Few-Shot%20learning.%20Electronics13%20(19),%203923%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR23">
<span class="label">23.</span><cite>Sen, A. &amp; Bilgili, A. Indoor Mapping Using Machine Learning Based Classification of 3D Point Clouds. in 18th International Conference on Location Based Services. (2023).</cite>
</li>
<li id="CR24">
<span class="label">24.</span><cite>Soudy, M., Afify, Y. &amp; Badr, N. RepConv: A novel architecture for image scene classification on intel scenes dataset. <em>Int. J. Intell. Comput. Inform. Sci.</em><strong>22</strong> (2), 63–73 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Soudy,%20M.,%20Afify,%20Y.%20&amp;%20Badr,%20N.%20RepConv:%20A%20novel%20architecture%20for%20image%20scene%20classification%20on%20intel%20scenes%20dataset.%20Int.%20J.%20Intell.%20Comput.%20Inform.%20Sci.22%20(2),%2063%E2%80%9373%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR25">
<span class="label">25.</span><cite>Zhu, X. et al. Scene-aware label graph learning for multi-label image classification. in Proceedings of the IEEE/CVF International Conference on Computer Vision. (2023).</cite>
</li>
<li id="CR26">
<span class="label">26.</span><cite>Alenzi, Z. et al. A semantic classification approach for indoor robot navigation. Electronics, 11(13): p. 2063. (2022).</cite>
</li>
<li id="CR27">
<span class="label">27.</span><cite>Roy, A. &amp; Chakraborty, S. Support vector machine in structural reliability analysis: A review. <em>Reliab. Eng. Syst. Saf.</em><strong>233</strong>, 109126 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Roy,%20A.%20&amp;%20Chakraborty,%20S.%20Support%20vector%20machine%20in%20structural%20reliability%20analysis:%20A%20review.%20Reliab.%20Eng.%20Syst.%20Saf.233,%20109126%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR28">
<span class="label">28.</span><cite>Ibrahim, H. B. et al. Smart monitoring of road pavement deformations from UAV images by using machine learning. <em>Innovative Infrastructure Solutions</em>. <strong>9</strong> (1), 16 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ibrahim,%20H.%20B.%20et%20al.%20Smart%20monitoring%20of%20road%20pavement%20deformations%20from%20UAV%20images%20by%20using%20machine%20learning.%20Innovative%20Infrastructure%20Solutions.%209%20(1),%2016%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR29">
<span class="label">29.</span><cite>Cheng, K. Prediction of emotion distribution of images based on weighted K-nearest neighbor-attention mechanism. <em>Front. Comput. Neurosci.</em><strong>18</strong>, 1350916 (2024).
</cite> [<a href="https://doi.org/10.3389/fncom.2024.1350916" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11061417/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/38694951/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Cheng,%20K.%20Prediction%20of%20emotion%20distribution%20of%20images%20based%20on%20weighted%20K-nearest%20neighbor-attention%20mechanism.%20Front.%20Comput.%20Neurosci.18,%201350916%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR30">
<span class="label">30.</span><cite>Salehin, I. et al. AutoML: A systematic review on automated machine learning with neural architecture search. <em>J. Inform. Intell.</em><strong>2</strong> (1), 52–81 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Salehin,%20I.%20et%20al.%20AutoML:%20A%20systematic%20review%20on%20automated%20machine%20learning%20with%20neural%20architecture%20search.%20J.%20Inform.%20Intell.2%20(1),%2052%E2%80%9381%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR31">
<span class="label">31.</span><cite>Xu, J. et al. Joint input and output space learning for multi-label image classification. <em>IEEE Trans. Multimedia</em>. <strong>23</strong>, 1696–1707 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Xu,%20J.%20et%20al.%20Joint%20input%20and%20output%20space%20learning%20for%20multi-label%20image%20classification.%20IEEE%20Trans.%20Multimedia.%2023,%201696%E2%80%931707%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR32">
<span class="label">32.</span><cite>Ran, Y. et al. Scene classification method based on multi-scale convolutional neural network with long short-term memory and Whale optimization algorithm. <em>Remote Sens.</em><strong>16</strong> (1), 174 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ran,%20Y.%20et%20al.%20Scene%20classification%20method%20based%20on%20multi-scale%20convolutional%20neural%20network%20with%20long%20short-term%20memory%20and%20Whale%20optimization%20algorithm.%20Remote%20Sens.16%20(1),%20174%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR33">
<span class="label">33.</span><cite>Anami, B. S. &amp; Sagarnal, C. V. A fusion of hand-crafted features and deep neural network for indoor scene classification. <em>Malaysian J. Comput. Sci.</em><strong>36</strong> (2), 193–207 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Anami,%20B.%20S.%20&amp;%20Sagarnal,%20C.%20V.%20A%20fusion%20of%20hand-crafted%20features%20and%20deep%20neural%20network%20for%20indoor%20scene%20classification.%20Malaysian%20J.%20Comput.%20Sci.36%20(2),%20193%E2%80%93207%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR34">
<span class="label">34.</span><cite>Liu, L. Scene classification in the environmental Art design by using the lightweight deep learning model under the background of big data. <em>Comput. Intell. Neurosci.</em><strong>2022</strong> (1), 9066648 (2022).
</cite> [<a href="https://doi.org/10.1155/2022/9066648" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC9208967/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/35733573/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Liu,%20L.%20Scene%20classification%20in%20the%20environmental%20Art%20design%20by%20using%20the%20lightweight%20deep%20learning%20model%20under%20the%20background%20of%20big%20data.%20Comput.%20Intell.%20Neurosci.2022%20(1),%209066648%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR35">
<span class="label">35.</span><cite>Tyagi, B., Nigam, S. &amp; Singh, R. A review of deep learning techniques for crowd behavior analysis. <em>Arch. Comput. Methods Eng.</em><strong>29</strong> (7), 5427–5455 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Tyagi,%20B.,%20Nigam,%20S.%20&amp;%20Singh,%20R.%20A%20review%20of%20deep%20learning%20techniques%20for%20crowd%20behavior%20analysis.%20Arch.%20Comput.%20Methods%20Eng.29%20(7),%205427%E2%80%935455%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR36">
<span class="label">36.</span><cite>Chen, L. et al. Review of image classification algorithms based on convolutional neural networks. <em>Remote Sens.</em><strong>13</strong> (22), 4712 (2021).</cite> [<a href="https://scholar.google.com/scholar_lookup?Chen,%20L.%20et%20al.%20Review%20of%20image%20classification%20algorithms%20based%20on%20convolutional%20neural%20networks.%20Remote%20Sens.13%20(22),%204712%20(2021)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR37">
<span class="label">37.</span><cite>Alam, M. S. et al. A review of recurrent neural network based camera localization for indoor environments. <em>IEEE Access.</em><strong>11</strong>, 43985–44009 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Alam,%20M.%20S.%20et%20al.%20A%20review%20of%20recurrent%20neural%20network%20based%20camera%20localization%20for%20indoor%20environments.%20IEEE%20Access.11,%2043985%E2%80%9344009%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR38">
<span class="label">38.</span><cite>Lu, Y. et al. Generative adversarial networks (GANs) for image augmentation in agriculture: A systematic review. <em>Comput. Electron. Agric.</em><strong>200</strong>, 107208 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Lu,%20Y.%20et%20al.%20Generative%20adversarial%20networks%20(GANs)%20for%20image%20augmentation%20in%20agriculture:%20A%20systematic%20review.%20Comput.%20Electron.%20Agric.200,%20107208%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR39">
<span class="label">39.</span><cite>Sayed, A. N., Himeur, Y. &amp; Bensaali, F. Deep and transfer learning for Building occupancy detection: A review and comparative analysis. <em>Eng. Appl. Artif. Intell.</em><strong>115</strong>, 105254 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Sayed,%20A.%20N.,%20Himeur,%20Y.%20&amp;%20Bensaali,%20F.%20Deep%20and%20transfer%20learning%20for%20Building%20occupancy%20detection:%20A%20review%20and%20comparative%20analysis.%20Eng.%20Appl.%20Artif.%20Intell.115,%20105254%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR40">
<span class="label">40.</span><cite>Himeur, Y. et al. Federated learning for computer vision. arXiv preprint arXiv:2308.13558, (2023).</cite>
</li>
<li id="CR41">
<span class="label">41.</span><cite>Wu, J. et al. Topology-aware federated learning in edge computing: A comprehensive survey. <em>ACM Comput. Surveys</em>. <strong>56</strong> (10), 1–41 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Wu,%20J.%20et%20al.%20Topology-aware%20federated%20learning%20in%20edge%20computing:%20A%20comprehensive%20survey.%20ACM%20Comput.%20Surveys.%2056%20(10),%201%E2%80%9341%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR42">
<span class="label">42.</span><cite>Abdel-Basset, M. et al. Privacy-preserved learning from non-iid data in fog-assisted iot: A federated learning approach. <em>Digit. Commun. Networks</em>. <strong>10</strong> (2), 404–415 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Abdel-Basset,%20M.%20et%20al.%20Privacy-preserved%20learning%20from%20non-iid%20data%20in%20fog-assisted%20iot:%20A%20federated%20learning%20approach.%20Digit.%20Commun.%20Networks.%2010%20(2),%20404%E2%80%93415%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR43">
<span class="label">43.</span><cite>Liu, W. et al. An efficient federated learning method based on enhanced classification-GAN for medical image classification. <em>Multimedia Syst.</em><strong>31</strong> (1), 1–17 (2025).</cite> [<a href="https://scholar.google.com/scholar_lookup?Liu,%20W.%20et%20al.%20An%20efficient%20federated%20learning%20method%20based%20on%20enhanced%20classification-GAN%20for%20medical%20image%20classification.%20Multimedia%20Syst.31%20(1),%201%E2%80%9317%20(2025)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR44">
<span class="label">44.</span><cite>Yu, H. S. &amp; Jhuang, Y. C. and J.-S. Leu. Image Recognition-Based Indoor Positioning System Using Federated Learning. in 2024 IEEE 100th Vehicular Technology Conference (VTC2024-Fall). IEEE. (2024).</cite>
</li>
<li id="CR45">
<span class="label">45.</span><cite>Abir, M. R., Zaman, A. &amp; Mursalin, S. Efficiency measurement of FL algorithms for image classification. <em>GSC Adv. Res. Reviews</em>. <strong>18</strong> (3), 356–366 (2024).</cite> [<a href="https://scholar.google.com/scholar_lookup?Abir,%20M.%20R.,%20Zaman,%20A.%20&amp;%20Mursalin,%20S.%20Efficiency%20measurement%20of%20FL%20algorithms%20for%20image%20classification.%20GSC%20Adv.%20Res.%20Reviews.%2018%20(3),%20356%E2%80%93366%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR46">
<span class="label">46.</span><cite>Chen, H. Y. &amp; Chao, W. L. On bridging generic and personalized federated learning for image classification. arXiv preprint arXiv:2107.00778, (2021).</cite>
</li>
<li id="CR47">
<span class="label">47.</span><cite>Nazir, S. &amp; Kaleem, M. Federated learning for medical image analysis with deep neural networks. <em>Diagnostics</em><strong>13</strong> (9), 1532 (2023).
</cite> [<a href="https://doi.org/10.3390/diagnostics13091532" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC10177193/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/37174925/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Nazir,%20S.%20&amp;%20Kaleem,%20M.%20Federated%20learning%20for%20medical%20image%20analysis%20with%20deep%20neural%20networks.%20Diagnostics13%20(9),%201532%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR48">
<span class="label">48.</span><cite>Li, L., Xie, N. &amp; Yuan, S. A federated learning framework for breast cancer histopathological image classification. <em>Electronics</em><strong>11</strong> (22), 3767 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Li,%20L.,%20Xie,%20N.%20&amp;%20Yuan,%20S.%20A%20federated%20learning%20framework%20for%20breast%20cancer%20histopathological%20image%20classification.%20Electronics11%20(22),%203767%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR49">
<span class="label">49.</span><cite>Abunadi, I. et al. Federated learning with blockchain assisted image classification for clustered UAV networks. <em>Comput. Mater. Contin</em>. <strong>72</strong>, 1195–1212 (2022).</cite> [<a href="https://scholar.google.com/scholar_lookup?Abunadi,%20I.%20et%20al.%20Federated%20learning%20with%20blockchain%20assisted%20image%20classification%20for%20clustered%20UAV%20networks.%20Comput.%20Mater.%20Contin.%2072,%201195%E2%80%931212%20(2022)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR50">
<span class="label">50.</span><cite>Ahmed, S. F. et al. Deep learning modelling techniques: current progress, applications, advantages, and challenges. <em>Artif. Intell. Rev.</em><strong>56</strong> (11), 13521–13617 (2023).</cite> [<a href="https://scholar.google.com/scholar_lookup?Ahmed,%20S.%20F.%20et%20al.%20Deep%20learning%20modelling%20techniques:%20current%20progress,%20applications,%20advantages,%20and%20challenges.%20Artif.%20Intell.%20Rev.56%20(11),%2013521%E2%80%9313617%20(2023)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR51">
<span class="label">51.</span><cite>Gupta, P. Presence detection for lighting control with ML models using RADAR data in an indoor environment. (2021).</cite>
</li>
<li id="CR52">
<span class="label">52.</span><cite>Qamar, F. et al. Federated learning for millimeter-wave spectrum in 6G networks: applications, challenges, way forward and open research issues. <em>PeerJ Comput. Sci.</em><strong>10</strong>, e2360 (2024).
</cite> [<a href="https://doi.org/10.7717/peerj-cs.2360" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">DOI</a>] [<a href="/articles/PMC11623056/" class="usa-link">PMC free article</a>] [<a href="https://pubmed.ncbi.nlm.nih.gov/39650377/" class="usa-link">PubMed</a>] [<a href="https://scholar.google.com/scholar_lookup?Qamar,%20F.%20et%20al.%20Federated%20learning%20for%20millimeter-wave%20spectrum%20in%206G%20networks:%20applications,%20challenges,%20way%20forward%20and%20open%20research%20issues.%20PeerJ%20Comput.%20Sci.10,%20e2360%20(2024)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR53">
<span class="label">53.</span><cite>Savazzi, S., Nicoli, M. &amp; Rampa, V. Federated learning with cooperating devices: A consensus approach for massive IoT networks. <em>IEEE Internet Things J.</em><strong>7</strong> (5), 4641–4654 (2020).</cite> [<a href="https://scholar.google.com/scholar_lookup?Savazzi,%20S.,%20Nicoli,%20M.%20&amp;%20Rampa,%20V.%20Federated%20learning%20with%20cooperating%20devices:%20A%20consensus%20approach%20for%20massive%20IoT%20networks.%20IEEE%20Internet%20Things%20J.7%20(5),%204641%E2%80%934654%20(2020)." class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">Google Scholar</a>]</li>
<li id="CR54">
<span class="label">54.</span><cite><a href="https://www.kaggle.com/datasets/itsahmad/indoor-scenes-cvpr-2019" class="usa-link usa-link--external" data-ga-action="click_feat_suppl" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/datasets/itsahmad/indoor-scenes-cvpr-2019</a>. accessed 3 Apr 2025.</cite>
</li>
<li id="CR55">
<span class="label">55.</span><cite>Balakrishnama, S. &amp; Ganapathiraju, A. Linear discriminant analysis-a brief tutorial. Institute for Signal and Information Processing, 18(1998): pp. 1–8. (1998).</cite>
</li>
</ul></section></section><section id="_ad93_" lang="en" class="associated-data"><h2 class="pmc_sec_title">Associated Data</h2>
<p class="font-secondary"><em>This section collects any data citations, data availability statements, or supplementary materials included in this article.</em></p>
<section id="_adda93_" lang="en" class="data-availability-statement"><h3 class="pmc_sec_title">Data Availability Statement</h3>
<p>The dataset is available at [https://www.kaggle.com/datasets/itsahmad/indoor-scenes-cvpr-2019].</p></section></section></section><footer class="p courtesy-note font-secondary font-sm text-center"><hr class="headless">
<p>Articles from Scientific Reports are provided here courtesy of <strong>Nature Publishing Group</strong></p></footer></section></article>

                      

                    </main>
                </div>
            </div>
        </div>

        



<!-- Secondary navigation placeholder -->
<div class="pmc-sidenav desktop:grid-col-4 display-flex">
    <section class="pmc-sidenav__container" aria-label="Article resources and navigation">
        <button type="button" class="usa-button pmc-sidenav__container__close usa-button--unstyled">
            <img src="/static/img/usa-icons/close.svg" role="img" alt="Close" />
        </button>
    <div class="display-none desktop:display-block">
       <section class="margin-top-4 desktop:margin-top-0">
              <h2 class="margin-top-0">ACTIONS</h2>
           <ul class="usa-list usa-list--unstyled usa-list--actions">
               
               <li>
                     <a
                             href="https://doi.org/10.1038/s41598-025-16673-3"
                             class="usa-button usa-button--outline width-24 font-xs display-inline-flex flex-align-center flex-justify-start padding-left-1"
                             target="_blank"
                             rel="noreferrer noopener"
                             data-ga-category="actions"
                             data-ga-action="click"
                             data-ga-label="publisher_link_desktop"
                     >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#launch"></use>
                         </svg>
                         <span class="display-inline-flex flex-justify-center flex-1 padding-right-2">View on publisher site</span>
                     </a>
               </li>
               
               
               <li>
                    <a
                            href="pdf/41598_2025_Article_16673.pdf"
                            class="usa-button usa-button--outline width-24 display-inline-flex flex-align-center flex-justify-start padding-left-1"
                            data-ga-category="actions"
                            data-ga-action="click"
                            data-ga-label="pdf_download_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#file_download"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1">PDF (5.7 MB)</span>
                    </a>
               </li>
               
                
               <li>
                   <button role="button" class="usa-button width-24 citation-dialog-trigger display-inline-flex flex-align-center flex-justify-start padding-left-1"
                        aria-label="Open dialog with citation text in different styles"
                        data-ga-category="actions"
                        data-ga-action="open"
                        data-ga-label="cite_desktop"
                        data-all-citations-url="/resources/citations/12370940/"
                        data-citation-style="nlm"
                        data-download-format-link="/resources/citations/12370940/export/"
                    >
                        <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#format_quote"></use>
                        </svg>
                       <span class="display-inline-flex flex-justify-center flex-1 button-label">Cite</span>
                    </button>
               </li>
                
               <li>

                        <button class="usa-button width-24 collections-dialog-trigger collections-button display-inline-flex flex-align-center flex-justify-start padding-left-1 collections-button-empty"
                              aria-label="Save article in MyNCBI collections."
                              data-ga-category="actions"
                              data-ga-action="click"
                              data-ga-label="collections_button_desktop"
                              data-collections-open-dialog-enabled="false"
                              data-collections-open-dialog-url="https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC12370940%2F%3Freport%3Dclassic%23open-collections-dialog"
                              data-in-collections="false">
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-full" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-full.svg#icon"></use>
                            </svg>
                            <svg class="usa-icon width-3 height-3 usa-icon--bookmark-empty" aria-hidden="true" focusable="false" role="img" hidden>
                                <use xlink:href="/static/img/action-bookmark-empty.svg#icon"></use>
                            </svg>
                            <span class="display-inline-flex flex-justify-center flex-1">Collections</span>
                       </button>
               </li>
               <li class="pmc-permalink">
                    <button
                            type="button"
                            class="usa-button width-24 display-inline-flex flex-align-center flex-justify padding-left-1 shadow-none"
                            aria-label="Show article permalink"
                            aria-expanded="false"
                            aria-haspopup="true"
                            data-ga-category="actions"
                            data-ga-action="open"
                            data-ga-label="permalink_desktop"
                    >
                         <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img" hidden>
                            <use xlink:href="/static/img/sprite.svg#share"></use>
                        </svg>
                        <span class="display-inline-flex flex-justify-center flex-1 button-label">Permalink</span>
                    </button>
                   

<div class="pmc-permalink__dropdown" hidden>
    <div class="pmc-permalink__dropdown__container">
          <h2 class="usa-modal__heading margin-top-0 margin-bottom-2 text-uppercase font-sans-xs">PERMALINK</h2>
          <div class="pmc-permalink__dropdown__content">
              <input type="text" class="usa-input" value="https://pmc.ncbi.nlm.nih.gov/articles/PMC12370940/" aria-label="Article permalink">
              <button class="usa-button display-inline-flex pmc-permalink__dropdown__copy__btn margin-right-0" title="Copy article permalink" data-ga-category="save_share" data-ga-action="link" data-ga-label="copy_link">
                  <svg class="usa-icon" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#content_copy"></use>
                  </svg>
                  <span class="margin-left-1">Copy</span>
              </button>
          </div>
    </div>
</div>
               </li>
           </ul>
       </section>
     </div>

        <section class="pmc-resources margin-top-6 desktop:margin-top-4" data-page-path="/articles/PMC12370940/">
            <h2 class="margin-top-0">RESOURCES</h2>
            
                <div class="usa-accordion usa-accordion--multiselectable" data-allow-multiple>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-similar-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_similar_articles"
                        data-ga-label="/articles/PMC12370940/"
                        data-action-open="open_similar_articles"
                        data-action-close="close_similar_articles"
                        >
                            Similar articles
                        </button>
                    </h3>
                    <div
                            id="resources-similar-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/similar-article-links/40841736/"
                            
                    >
                        
                    </div>
                    <h3 class="usa-accordion__heading">
                        <button
                        type="button"
                        class="usa-accordion__button"
                        aria-expanded="false"
                        aria-controls="resources-cited-by-other-articles"
                        data-ga-category="resources_accordion"
                        data-ga-action="open_cited_by"
                        data-ga-label="/articles/PMC12370940/"
                        data-action-open="open_cited_by"
                        data-action-close="close_cited_by"
                        >
                             Cited by other articles
                        </button>
                    </h3>
                    <div
                            id="resources-cited-by-other-articles"
                            class="usa-accordion__content usa-prose"
                            
                                data-source-url="/resources/cited-by-links/40841736/"
                            
                    >
                          
                    </div>
                    
                        <h3 class="usa-accordion__heading">
                            <button
                            type="button"
                            class="usa-accordion__button"
                            aria-expanded="false"
                            aria-controls="resources-links-to-ncbi-databases"
                            data-ga-category="resources_accordion"
                            data-ga-action="open_NCBI_links"
                            data-ga-label="/articles/PMC12370940/"
                            data-action-open="open_NCBI_links"
                            data-action-close="close_NCBI_link"
                            >
                                 Links to NCBI Databases
                            </button>
                        </h3>
                        <div
                                id="resources-links-to-ncbi-databases"
                                class="usa-accordion__content usa-prose"
                                data-source-url="/resources/db-links/12370940/"
                        >
                        </div>
                    
                    
                </div>
            
        </section>


        <section
        class="usa-in-page-nav usa-in-page-nav--wide margin-top-6 desktop:margin-top-4"
        data-title-text="On this page"
        data-title-heading-level="h2"
        data-scroll-offset="0"
        data-root-margin="-10% 0px -80% 0px"
        data-main-content-selector="main"
        data-threshold="1"
        hidden
        ></section>
    </section>
</div>


        

<div class="overlay" role="dialog" aria-label="Citation Dialog" hidden>
    <div class="dialog citation-dialog" aria-hidden="true">
        <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
            <h2 class="usa-modal__heading margin-0">Cite</h2>
             <button type="button" class="usa-button usa-button--unstyled close-overlay text-black width-auto"  tabindex="1">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#close"></use>
                </svg>
             </button>
        </div>

        

<div class="citation-text-block">
  <div class="citation-text margin-bottom-2"></div>
  <ul class="usa-list usa-list--unstyled display-inline-flex flex-justify width-full flex-align-center">
      <li>
        <button
          class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center copy-button dialog-focus"
          data-ga-category="save_share"
          data-ga-action="cite"
          data-ga-label="copy"
          tabindex="2">
            <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                <use xlink:href="/static/img/sprite.svg#content_copy"></use>
            </svg>
            <span>Copy</span>
        </button>
      </li>
      <li>
          <a
              href="#"
              role="button"
              class="usa-button usa-button--unstyled text-no-underline display-flex flex-align-center export-button"
              data-ga-category="save_share"
              data-ga-action="cite"
              data-ga-label="download"
              title="Download a file for external citation management software"
              tabindex="3">
                <svg class="usa-icon width-3 height-3" aria-hidden="true" focusable="false" role="img">
                    <use xlink:href="/static/img/sprite.svg#file_download"></use>
                </svg>
                <span class="display-none mobile-lg:display-inline">Download .nbib</span>
                <span class="display-inline mobile-lg:display-none">.nbib</span>
            </a>
      </li>
      <li>
          

<div class="display-inline-flex flex-align-center">
  <label class="usa-label margin-top-0">Format:</label>
  <select aria-label="Format" class="usa-select citation-style-selector padding-1 margin-top-0 border-0 padding-right-4" tabindex="4" >
    
      <option data-style-url-name="ama"
              value="AMA"
              >
        AMA
      </option>
    
      <option data-style-url-name="apa"
              value="APA"
              >
        APA
      </option>
    
      <option data-style-url-name="mla"
              value="MLA"
              >
        MLA
      </option>
    
      <option data-style-url-name="nlm"
              value="NLM"
              selected="selected">
        NLM
      </option>
    
  </select>
</div>
      </li>
  </ul>
</div>
    </div>
</div>

        <div class="overlay" role="dialog" hidden>
  <div id="collections-action-dialog" class="dialog collections-dialog" aria-hidden="true">
   <div class="display-inline-flex flex-align-center flex-justify width-full margin-bottom-2">
        <h2 class="usa-modal__heading margin-0">Add to Collections</h2>
    </div>
    <div class="collections-action-panel action-panel">
      


<form id="collections-action-dialog-form"
      class="usa-form maxw-full collections-action-panel-form action-panel-content action-form action-panel-smaller-selectors"
      data-existing-collections-url="/list-existing-collections/"
      data-add-to-existing-collection-url="/add-to-existing-collection/"
      data-create-and-add-to-new-collection-url="/create-and-add-to-new-collection/"
      data-myncbi-max-collection-name-length="100"
      data-collections-root-url="https://www.ncbi.nlm.nih.gov/myncbi/collections/">

    <input type="hidden" name="csrfmiddlewaretoken" value="7owIZAJ62ibcyrPoomxxEBwsVzImXY6k7I7S4w14yl3ibSDfNWeUCBX2w9ERp1Tl">

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-new"
            class="usa-radio__input usa-radio__input--tile collections-new  margin-top-0"
            name="collections"
            value="new"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_new" />
            <label class="usa-radio__label" for="collections-action-dialog-new">Create a new collection</label>
        </div>
        <div class="usa-radio">
            <input type="radio"
            id="collections-action-dialog-existing"
            class="usa-radio__input usa-radio__input--tile collections-existing"
            name="collections"
            value="existing"
            checked="true"
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="collections_radio_existing" />
            <label class="usa-radio__label" for="collections-action-dialog-existing">Add to an existing collection</label>
        </div>
    </fieldset>

    <fieldset class="usa-fieldset margin-bottom-2">
        <div class="action-panel-control-wrap new-collections-controls">
           <label for="collections-action-dialog-add-to-new" class="usa-label margin-top-0">
                Name your collection
               <abbr title="required" class="usa-hint usa-hint--required text-no-underline">*</abbr>
          </label>
          <input
            type="text"
            name="add-to-new-collection"
            id="collections-action-dialog-add-to-new"
            class="usa-input collections-action-add-to-new"
            pattern="[^&quot;&amp;=&lt;&gt;/]*" title="The following characters are not allowed in the Name field: &quot;&amp;=&lt;&gt;/"
            maxlength=""
            data-ga-category="collections_button"
            data-ga-action="create_collection"
            data-ga-label="non_favorties_collection"
            required
          />
        </div>
        <div class="action-panel-control-wrap existing-collections-controls">
             <label for="collections-action-dialog-add-to-existing" class="usa-label margin-top-0">
                Choose a collection
              </label>
              <select id="collections-action-dialog-add-to-existing"
                      class="usa-select collections-action-add-to-existing"
                      data-ga-category="collections_button"
                      data-ga-action="select_collection"
                      data-ga-label="($('.collections-action-add-to-existing').val() === 'Favorites') ? 'Favorites' : 'non_favorites_collection'">
              </select>
              <div class="collections-retry-load-on-error usa-input-error-message selection-validation-message">
                Unable to load your collection due to an error<br>
                <a href="#">Please try again</a>
              </div>
        </div>
    </fieldset>

    <div class="display-inline-flex">
        <button class="usa-button margin-top-0 action-panel-submit"
            type="submit"
            data-loading-label="Adding..."
            data-pinger-ignore
            data-ga-category="collections_button"
            data-ga-action="click"
            data-ga-label="add">
          Add
        </button>
        <button class="usa-button usa-button--outline margin-top-0 action-panel-cancel"
                aria-label="Close 'Add to Collections' panel"
                ref="linksrc=close_collections_panel"
                data-ga-category="collections_button"
                data-ga-action="click"
                data-ga-label="cancel">
          Cancel
        </button>
    </div>
</form>
    </div>
  </div>
</div>

        

      </div>
    </div>
  </div>



        
    
    

<footer class="ncbi-footer ncbi-dark-background " >
    
        <div class="ncbi-footer__icon-section">
            <div class="ncbi-footer__social-header">
                Follow NCBI
            </div>

            <div class="grid-container ncbi-footer__ncbi-social-icons-container">
                
                    <a href="https://twitter.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="40"
                             height="40"
                             viewBox="0 0 40 40"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on X (formerly known as Twitter)</span>
                    </a>
                

                
                    <a href="https://www.facebook.com/ncbi.nlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="16"
                             height="29"
                             focusable="false"
                             aria-hidden="true"
                             viewBox="0 0 16 29"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg">
                            <path d="M3.8809 21.4002C3.8809 19.0932 3.8809 16.7876 3.8809 14.478C3.8809 14.2117 3.80103 14.1452 3.54278 14.1492C2.53372 14.1638 1.52334 14.1492 0.514288 14.1598C0.302626 14.1598 0.248047 14.0972 0.248047 13.8936C0.256034 12.4585 0.256034 11.0239 0.248047 9.58978C0.248047 9.37013 0.302626 9.30224 0.528931 9.3049C1.53798 9.31688 2.54837 9.3049 3.55742 9.31555C3.80103 9.31555 3.8809 9.26097 3.87957 9.00272C3.87158 8.00565 3.85428 7.00592 3.90753 6.00884C3.97142 4.83339 4.31487 3.73115 5.04437 2.78467C5.93095 1.63318 7.15699 1.09005 8.56141 0.967577C10.5582 0.79319 12.555 0.982221 14.5518 0.927641C14.7102 0.927641 14.7462 0.99287 14.7449 1.13664C14.7449 2.581 14.7449 4.02668 14.7449 5.47104C14.7449 5.67604 14.6517 5.68669 14.4946 5.68669C13.4523 5.68669 12.4113 5.68669 11.3703 5.68669C10.3506 5.68669 9.92057 6.10868 9.90593 7.13904C9.89661 7.7647 9.91525 8.39303 9.89794 9.01869C9.88995 9.26364 9.96583 9.31822 10.2015 9.31688C11.7204 9.30623 13.2393 9.31688 14.7595 9.3049C15.0257 9.3049 15.0723 9.3728 15.0444 9.62439C14.89 10.9849 14.7515 12.3467 14.6144 13.7085C14.5691 14.1571 14.5785 14.1585 14.1458 14.1585C12.8386 14.1585 11.5313 14.1665 10.2254 14.1518C9.95119 14.1518 9.89794 14.2317 9.89794 14.4899C9.90593 19.0799 9.89794 23.6752 9.91125 28.2612C9.91125 28.5674 9.8407 28.646 9.53186 28.6433C7.77866 28.6273 6.02414 28.6366 4.27094 28.634C3.82499 28.634 3.87158 28.6992 3.87158 28.22C3.87602 25.9472 3.87913 23.6739 3.8809 21.4002Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on Facebook</span>
                    </a>
                

                
                    <a href="https://www.linkedin.com/company/ncbinlm"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="25"
                             height="23"
                             viewBox="0 0 26 24"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M14.6983 9.98423C15.6302 9.24808 16.5926 8.74754 17.6762 8.51991C19.673 8.09126 21.554 8.30824 23.1262 9.7526C24.2351 10.7723 24.7529 12.1115 25.0165 13.5612C25.1486 14.3363 25.2105 15.1218 25.2015 15.9081C25.2015 18.3043 25.2015 20.6898 25.2082 23.0806C25.2082 23.3468 25.1549 23.444 24.8621 23.4414C23.1297 23.4272 21.3992 23.4272 19.6704 23.4414C19.4041 23.4414 19.3429 23.3588 19.3442 23.1019C19.3535 20.5194 19.3442 17.9368 19.3442 15.3543C19.3442 14.0005 18.3258 12.9448 17.0266 12.9488C15.7273 12.9528 14.6983 14.0071 14.6983 15.361C14.6983 17.9328 14.6917 20.5047 14.6983 23.0753C14.6983 23.3708 14.6198 23.444 14.3296 23.4427C12.6185 23.4294 10.9079 23.4294 9.19779 23.4427C8.93155 23.4427 8.86099 23.3735 8.86232 23.1086C8.8783 19.7619 8.88628 16.4144 8.88628 13.066C8.88628 11.5688 8.87874 10.0708 8.86365 8.57182C8.86365 8.3575 8.90758 8.27896 9.14054 8.28029C10.9048 8.29094 12.6687 8.29094 14.4321 8.28029C14.6464 8.28029 14.6983 8.34818 14.6983 8.54653C14.6903 9.00047 14.6983 9.45441 14.6983 9.98423Z">
                            </path>
                            <path d="M6.55316 15.8443C6.55316 18.2564 6.55316 20.6699 6.55316 23.082C6.55316 23.3629 6.48127 23.4388 6.19906 23.4374C4.47737 23.4241 2.75568 23.4241 1.03399 23.4374C0.767751 23.4374 0.69986 23.3629 0.701191 23.1006C0.709178 18.2648 0.709178 13.4281 0.701191 8.59053C0.701191 8.34026 0.765089 8.27237 1.01669 8.2737C2.74991 8.28435 4.48048 8.28435 6.20838 8.2737C6.47462 8.2737 6.5465 8.33627 6.54517 8.6065C6.54783 11.0186 6.55316 13.4308 6.55316 15.8443Z">
                            </path>
                            <path d="M3.65878 0.243898C5.36804 0.243898 6.58743 1.45529 6.58743 3.1406C6.58743 4.75801 5.32145 5.95742 3.60819 5.96807C3.22177 5.97614 2.83768 5.90639 2.47877 5.76299C2.11985 5.61959 1.79344 5.40546 1.51897 5.13334C1.24449 4.86123 1.02755 4.53668 0.881058 4.17902C0.734563 3.82136 0.661505 3.43788 0.666231 3.05141C0.67555 1.42601 1.9362 0.242566 3.65878 0.243898Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on LinkedIn</span>
                    </a>
                

                
                    <a href="https://github.com/ncbi"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="28"
                             height="27"
                             viewBox="0 0 28 28"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M16.7228 20.6334C17.5057 20.5527 18.2786 20.3944 19.0301 20.1608C21.3108 19.4193 22.5822 17.8259 22.963 15.4909C23.1228 14.5112 23.1814 13.5287 22.9883 12.5437C22.8106 11.6423 22.4013 10.8028 21.8007 10.1076C21.7526 10.0605 21.7197 10 21.7064 9.934C21.6931 9.86799 21.7 9.79952 21.7262 9.73748C22.0856 8.6206 21.9711 7.51969 21.601 6.42677C21.582 6.3497 21.5345 6.2827 21.468 6.23923C21.4016 6.19577 21.3211 6.17906 21.2429 6.19248C20.7329 6.21649 20.2313 6.33051 19.7611 6.52928C19.1103 6.7908 18.4899 7.12198 17.9104 7.51703C17.84 7.56996 17.7581 7.60551 17.6713 7.62078C17.5846 7.63605 17.4954 7.6306 17.4112 7.60489C15.2596 7.05882 13.0054 7.06203 10.8554 7.61421C10.7806 7.63586 10.7018 7.63967 10.6253 7.62534C10.5487 7.611 10.4766 7.57892 10.4148 7.53167C9.64788 7.03247 8.85171 6.58918 7.96368 6.33359C7.65781 6.24338 7.34123 6.19458 7.02239 6.18849C6.94879 6.17986 6.87462 6.19893 6.81432 6.242C6.75402 6.28507 6.71191 6.34904 6.69621 6.42145C6.32342 7.51437 6.2209 8.61527 6.56307 9.73348C6.59635 9.84264 6.64694 9.93316 6.54177 10.0516C5.47666 11.2604 5.09988 12.6834 5.19574 14.2676C5.2663 15.4244 5.46201 16.5466 6.01454 17.5769C6.84399 19.1171 8.21664 19.9119 9.85158 20.3352C10.3938 20.4706 10.9444 20.5698 11.4998 20.632C11.5384 20.7492 11.4506 20.7798 11.408 20.8291C11.1734 21.1179 10.9894 21.4441 10.8634 21.7942C10.7622 22.0458 10.8315 22.4039 10.6065 22.5516C10.263 22.7766 9.83827 22.8485 9.42421 22.8871C8.17936 23.0056 7.26471 22.4877 6.6283 21.4348C6.25552 20.8184 5.76956 20.3325 5.08523 20.0663C4.76981 19.9325 4.42139 19.8967 4.08537 19.9638C3.7898 20.029 3.73788 20.1901 3.93891 20.4111C4.03639 20.5234 4.14989 20.6207 4.27575 20.6999C4.9796 21.1318 5.51717 21.7884 5.80152 22.5636C6.37002 23.9973 7.48039 24.5697 8.93825 24.6323C9.43741 24.6575 9.93768 24.615 10.4254 24.5058C10.5892 24.4672 10.6531 24.4872 10.6517 24.6762C10.6451 25.4936 10.6637 26.3123 10.6517 27.131C10.6517 27.6635 10.1684 27.9297 9.58663 27.7393C8.17396 27.2671 6.84977 26.5631 5.66838 25.656C2.59555 23.2891 0.720966 20.1861 0.217704 16.3376C-0.357453 11.9127 0.911353 8.00824 3.98551 4.73881C6.11909 2.42656 8.99932 0.939975 12.1203 0.540191C16.5351 -0.0601815 20.4347 1.14323 23.7232 4.16373C26.2449 6.47869 27.724 9.37672 28.1048 12.7726C28.5828 17.0325 27.3686 20.7945 24.4768 23.9827C22.9762 25.6323 21.0956 26.8908 18.9982 27.6488C18.8783 27.6927 18.7585 27.738 18.636 27.7726C18.0356 27.9404 17.6189 27.6395 17.6189 27.0098C17.6189 25.7452 17.6308 24.4806 17.6295 23.2159C17.6329 22.9506 17.6128 22.6856 17.5696 22.4238C17.4325 21.6664 17.3419 21.484 16.7228 20.6334Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI on GitHub</span>
                    </a>
                

                
                    <a href="https://ncbiinsights.ncbi.nlm.nih.gov/"
                       class="ncbi-footer__social-icon ncbi-footer__social-icon--gray"
                       target="_blank"
                       rel="noreferrer noopener">
                        <svg width="26"
                             height="26"
                             viewBox="0 0 27 27"
                             fill="none"
                             xmlns="http://www.w3.org/2000/svg"
                             focusable="false"
                             aria-hidden="true">
                            <path d="M23.7778 26.4574C23.1354 26.3913 22.0856 26.8024 21.636 26.3087C21.212 25.8444 21.4359 24.8111 21.324 24.0347C19.9933 14.8323 14.8727 8.80132 6.09057 5.85008C4.37689 5.28406 2.58381 4.99533 0.779072 4.99481C0.202773 4.99481 -0.0229751 4.83146 0.00455514 4.21479C0.0660406 3.08627 0.0660406 1.95525 0.00455514 0.826734C-0.0413285 0.0815827 0.259669 -0.0193618 0.896534 0.00266238C6.96236 0.222904 12.3693 2.24179 16.9889 6.16209C22.9794 11.2478 26.1271 17.7688 26.4372 25.648C26.4629 26.294 26.3179 26.5271 25.6609 26.4684C25.0827 26.417 24.4991 26.4574 23.7778 26.4574Z">
                            </path>
                            <path d="M14.8265 26.441C14.0924 26.441 13.2371 26.6795 12.6626 26.3786C12.0092 26.0372 12.3781 25.0644 12.246 24.378C11.1154 18.5324 6.6849 14.5497 0.74755 14.1001C0.217135 14.0615 -0.0104482 13.9422 0.0134113 13.3659C0.0519536 12.1454 0.0482829 10.9213 0.0134113 9.69524C-0.00127145 9.14464 0.196946 9.03268 0.703502 9.04736C9.21217 9.27128 16.5994 16.2511 17.2804 24.7231C17.418 26.4446 17.418 26.4446 15.6579 26.4446H14.832L14.8265 26.441Z">
                            </path>
                            <path d="M3.58928 26.4555C2.64447 26.4618 1.73584 26.0925 1.06329 25.4289C0.39073 24.7653 0.00933763 23.8617 0.0030097 22.9169C-0.00331824 21.9721 0.365937 21.0635 1.02954 20.3909C1.69315 19.7184 2.59675 19.337 3.54156 19.3306C4.48637 19.3243 5.39499 19.6936 6.06755 20.3572C6.7401 21.0208 7.1215 21.9244 7.12782 22.8692C7.13415 23.814 6.7649 24.7226 6.10129 25.3952C5.43768 26.0677 4.53409 26.4491 3.58928 26.4555Z">
                            </path>
                        </svg>
                        <span class="usa-sr-only">NCBI RSS feed</span>
                    </a>
                
            </div>
        </div>
    

    <div data-testid="gridContainer"
         class="grid-container ncbi-footer__container">
        <div class="grid-row ncbi-footer__main-content-container"
             data-testid="grid">
            
                <div class="ncbi-footer__column">
                    
                        <p class="ncbi-footer__circled-icons-heading">
                            Connect with NLM
                        </p>
                    

                    <div class="ncbi-footer__circled-icons-list">
                        
                            <a href=https://twitter.com/nlm_nih class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="32"
                                     height="32"
                                     viewBox="0 0 40 40"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="m6.067 8 10.81 13.9L6 33.2h4.2l8.4-9.1 7.068 9.1H34L22.8 18.5 31.9 8h-3.5l-7.7 8.4L14.401 8H6.067Zm3.6 1.734h3.266l16.8 21.732H26.57L9.668 9.734Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on X (formerly known as Twitter)</span>
                            </a>
                        

                        
                            <a href=https://www.facebook.com/nationallibraryofmedicine class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="13"
                                     height="24"
                                     viewBox="0 0 13 24"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M4.11371 23.1369C4.11371 23.082 4.11371 23.0294 4.11371 22.9745V12.9411H0.817305C0.6709 12.9411 0.670898 12.9411 0.670898 12.8016C0.670898 11.564 0.670898 10.3287 0.670898 9.09341C0.670898 8.97903 0.705213 8.95158 0.815017 8.95158C1.8673 8.95158 2.91959 8.95158 3.97417 8.95158H4.12057V8.83263C4.12057 7.8055 4.12057 6.7738 4.12057 5.74897C4.1264 4.92595 4.31387 4.11437 4.66959 3.37217C5.12916 2.38246 5.94651 1.60353 6.95717 1.1921C7.64827 0.905008 8.3913 0.764035 9.13953 0.778051C10.0019 0.791777 10.8644 0.830666 11.7268 0.860404C11.8869 0.860404 12.047 0.894717 12.2072 0.90158C12.2964 0.90158 12.3261 0.940469 12.3261 1.02968C12.3261 1.5421 12.3261 2.05452 12.3261 2.56465C12.3261 3.16857 12.3261 3.7725 12.3261 4.37642C12.3261 4.48165 12.2964 4.51367 12.1912 4.51138C11.5369 4.51138 10.8804 4.51138 10.2261 4.51138C9.92772 4.51814 9.63058 4.5526 9.33855 4.61433C9.08125 4.6617 8.84537 4.78881 8.66431 4.97766C8.48326 5.16652 8.3662 5.40755 8.32972 5.66661C8.28476 5.89271 8.26027 6.1224 8.25652 6.35289C8.25652 7.19014 8.25652 8.02969 8.25652 8.86923C8.25652 8.89439 8.25652 8.91955 8.25652 8.95615H12.0219C12.1797 8.95615 12.182 8.95616 12.1614 9.10714C12.0768 9.76596 11.9876 10.4248 11.9029 11.0813C11.8312 11.6319 11.7626 12.1824 11.697 12.733C11.6719 12.9434 11.6787 12.9434 11.4683 12.9434H8.26338V22.899C8.26338 22.979 8.26338 23.0591 8.26338 23.1392L4.11371 23.1369Z">
                                    </path>
                                </svg>
                                <span class="usa-sr-only">NLM on Facebook</span>
                            </a>
                        

                        
                            <a href=https://www.youtube.com/user/NLMNIH class="ncbi-footer__social-icon ncbi-footer__social-icon--circled" target="_blank" rel="noreferrer noopener">
                                <svg width="21"
                                     height="15"
                                     viewBox="0 0 21 15"
                                     fill="none"
                                     xmlns="http://www.w3.org/2000/svg"
                                     focusable="false"
                                     aria-hidden="true">
                                    <path d="M19.2561 1.47914C18.9016 1.15888 18.5699 0.957569 17.2271 0.834039C15.5503 0.678484 13.2787 0.655608 11.563 0.65332H9.43556C7.71987 0.65332 5.4483 0.678484 3.77151 0.834039C2.43098 0.957569 2.097 1.15888 1.74242 1.47914C0.813665 2.32097 0.619221 4.62685 0.598633 6.89384C0.598633 7.31781 0.598633 7.74101 0.598633 8.16345C0.626084 10.4121 0.827391 12.686 1.74242 13.521C2.097 13.8412 2.4287 14.0425 3.77151 14.1661C5.4483 14.3216 7.71987 14.3445 9.43556 14.3468H11.563C13.2787 14.3468 15.5503 14.3216 17.2271 14.1661C18.5676 14.0425 18.9016 13.8412 19.2561 13.521C20.1712 12.6929 20.3725 10.451 20.3999 8.22064C20.3999 7.74025 20.3999 7.25986 20.3999 6.77946C20.3725 4.54907 20.1689 2.30724 19.2561 1.47914ZM8.55942 10.5311V4.65201L13.5601 7.50005L8.55942 10.5311Z"
                                          fill="white" />
                                </svg>
                                <span class="usa-sr-only">NLM on YouTube</span>
                            </a>
                        
                    </div>
                </div>
            

            
                <address class="ncbi-footer__address ncbi-footer__column">
                    
        <p>
            <a class="usa-link usa-link--external"
            href="https://www.google.com/maps/place/8600+Rockville+Pike,+Bethesda,+MD+20894/%4038.9959508,
            -77.101021,17z/data%3D!3m1!4b1!4m5!3m4!1s0x89b7c95e25765ddb%3A0x19156f88b27635b8!8m2!3d38.9959508!
            4d-77.0988323"
            rel="noopener noreferrer" target="_blank">National Library of Medicine
            <br/> 8600 Rockville Pike<br/> Bethesda, MD 20894</a>
        </p>
    
                </address>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/web_policies.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Web Policies
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nih.gov/institutes-nih/nih-office-director/office-communications-public-liaison/freedom-information-act-office" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            FOIA
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.hhs.gov/vulnerability-disclosure-policy/index.html" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS Vulnerability Disclosure
        

        
    </a>


                        </li>
                    
                </ul>
            

            
                <ul class="usa-list usa-list--unstyled ncbi-footer__vertical-list ncbi-footer__column">
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://support.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Help
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/accessibility.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Accessibility
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__vertical-list-item">
                            









    <a href="https://www.nlm.nih.gov/careers/careers.html" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            Careers
        

        
    </a>


                        </li>
                    
                </ul>
            
        </div>

        
            <div class="grid-row grid-col-12" data-testid="grid">
                <ul class="ncbi-footer__bottom-links-list">
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nlm.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NLM
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.nih.gov/" class="usa-link  usa-link--alt ncbi-footer__link"  >
        

        
            NIH
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.hhs.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            HHS
        

        
    </a>


                        </li>
                    
                        <li class="ncbi-footer__bottom-list-item">
                            









    <a href="https://www.usa.gov/" class="usa-link usa-link--external usa-link--alt ncbi-footer__link" rel="noreferrer noopener" target='_blank' >
        

        
            USA.gov
        

        
    </a>


                        </li>
                    
                </ul>
            </div>
        
    </div>
</footer>

    


        
        
    
  <script  type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/pinger/pinger.js"> </script>


    
        

<button class="back-to-top" data-ga-category="pagination" data-ga-action="back_to_top">
    <label>Back to Top</label>
    <svg class="usa-icon order-0" aria-hidden="true" focusable="false" role="img">
        <use xlink:href="/static/img/sprite.svg#arrow_upward"></use>
    </svg>
</button>
    


        
    
    
    
        
    <script type="application/javascript">
    window.ncbi = window.ncbi || {};
    window.ncbi.pmc = window.ncbi.pmc || {};
    window.ncbi.pmc.options = {
        logLevel: 'INFO',
        
        staticEndpoint: '/static/',
        
        citeCookieName: 'pmc-cf',
    };
</script>
    <script type="module" crossorigin="" src="/static/assets/base-133c6271.js"></script>
    
    <script type="text/javascript" src="https://cdn.ncbi.nlm.nih.gov/core/jquery/jquery-3.6.0.min.js">&#xA0;</script>
    <script type="text/javascript">
        jQuery.getScript("https://cdn.ncbi.nlm.nih.gov/core/alerts/alerts.js", function () {
            galert(['div.nav_and_browser', 'div.header', '#universal_header', '.usa-banner', 'body > *:nth-child(1)'])
        });
    </script>


    <script type="text/javascript">var exports = {};</script>
     <script src="/static/CACHE/js/output.13b077bc3ffd.js"></script>

    <script type="module" crossorigin="" src="/static/assets/article-44d04358.js"></script>
    
    

    </body>
</html>
